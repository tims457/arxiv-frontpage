{"created":"2024-05-16 17:59:52","title":"Probing Hidden Dimensions via Muon Lifetime Measurements","abstract":"In the context of Kaluza-Klein theories, the time dilation of charged particles in an external field depends on the charge in a specific way. Experimental tests are proposed to search for extra dimensions using this distinctive feature.","sentences":["In the context of Kaluza-Klein theories, the time dilation of charged particles in an external field depends on the charge in a specific way.","Experimental tests are proposed to search for extra dimensions using this distinctive feature."],"url":"http://arxiv.org/abs/2405.10321v1","category":"gr-qc"}
{"created":"2024-05-16 17:59:42","title":"Fast simulation mapping: from standard to modified gravity cosmologies using the bias assignment method","abstract":"We assess the effectiveness of a non-parametric bias model in generating mock halo catalogues for modified gravity (MG) cosmologies, relying on the distribution of dark matter from either MG or $\\Lambda$CDM. We aim to generate halo catalogues that effectively capture the distinct impact of MG, ensuring high accuracy in both two- and three-point statistics for comprehensive analysis of large-scale structures. As part of this study we aim at investigating the inclusion of MG into non-local bias to directly map the tracers onto $\\Lambda$CDM fields, which would save many computational costs. We employ the bias assignment method (BAM) to model halo distribution statistics by leveraging seven high-resolution COLA simulations of MG cosmologies. Taking into account cosmic-web dependencies when learning the bias relations, we design two experiments to map the MG effects: one utilising the consistent MG density fields and the other employing the benchmark $\\Lambda$CDM density field. BAM generates MG halo catalogues from both calibrations experiments excelling in summary statistics, achieving a $\\sim 1\\%$ accuracy in the power spectrum across a wide range of $k$-modes, with only minimal differences well below 10\\% at modes subject to cosmic variance, particularly below $k<0.07$ $h$Mpc$^{-1}$. The reduced bispectrum remains consistent with the reference catalogues within 10\\% for the studied configuration. Our results demonstrate that a non-linear and non-local bias description can model the effects of MG starting from a $\\Lambda$CDM dark matter field.","sentences":["We assess the effectiveness of a non-parametric bias model in generating mock halo catalogues for modified gravity (MG) cosmologies, relying on the distribution of dark matter from either MG or $\\Lambda$CDM.","We aim to generate halo catalogues that effectively capture the distinct impact of MG, ensuring high accuracy in both two- and three-point statistics for comprehensive analysis of large-scale structures.","As part of this study we aim at investigating the inclusion of MG into non-local bias to directly map the tracers onto $\\Lambda$CDM fields, which would save many computational costs.","We employ the bias assignment method (BAM) to model halo distribution statistics by leveraging seven high-resolution COLA simulations of MG cosmologies.","Taking into account cosmic-web dependencies when learning the bias relations, we design two experiments to map the MG effects: one utilising the consistent MG density fields and the other employing the benchmark $\\Lambda$CDM density field.","BAM generates MG halo catalogues from both calibrations experiments excelling in summary statistics, achieving a $\\sim 1\\%$ accuracy in the power spectrum across a wide range of $k$-modes, with only minimal differences well below 10\\% at modes subject to cosmic variance, particularly below $k<0.07$ $h$Mpc$^{-1}$. The reduced bispectrum remains consistent with the reference catalogues within 10\\% for the studied configuration.","Our results demonstrate that a non-linear and non-local bias description can model the effects of MG starting from a $\\Lambda$CDM dark matter field."],"url":"http://arxiv.org/abs/2405.10319v1","category":"astro-ph.CO"}
{"created":"2024-05-16 17:59:36","title":"Gauge theory of giant phonon magnetic moment in doped Dirac semimetals","abstract":"We present a quantum theory for phonon magnetic moment in doped Dirac semimetals. Our theory is based on an emergent gauge field approach to the electron-phonon coupling, applicable for both gapless and gapped systems. We find that the magnetic moment is directly proportional to the electrical Hall conductivity through the phonon Hall viscosity. Our theory is combined with the first-principles calculations, allowing us to quantitatively implement it to realistic materials. Magnetic moments are found to be on the order of Bohr magneton for certain phonon modes in graphene and $\\text{Cd}_3 \\text{As}_2$. Our results provide practical guidance for the dynamic generation of large magnetization in the topological quantum materials.","sentences":["We present a quantum theory for phonon magnetic moment in doped Dirac semimetals.","Our theory is based on an emergent gauge field approach to the electron-phonon coupling, applicable for both gapless and gapped systems.","We find that the magnetic moment is directly proportional to the electrical Hall conductivity through the phonon Hall viscosity.","Our theory is combined with the first-principles calculations, allowing us to quantitatively implement it to realistic materials.","Magnetic moments are found to be on the order of Bohr magneton for certain phonon modes in graphene and $\\text{Cd}_3 \\text{As}_2$.","Our results provide practical guidance for the dynamic generation of large magnetization in the topological quantum materials."],"url":"http://arxiv.org/abs/2405.10318v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-16 17:59:22","title":"Text-to-Vector Generation with Neural Path Representation","abstract":"Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties. However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task. Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible. However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints. To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities. By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs. Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs. In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process. In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure. We demonstrate the effectiveness of our method through extensive experiments and showcase various applications. The project page is https://intchous.github.io/T2V-NPR.","sentences":["Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties.","However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task.","Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible.","However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints.","To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities.","By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs.","Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs.","In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process.","In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure.","We demonstrate the effectiveness of our method through extensive experiments and showcase various applications.","The project page is https://intchous.github.io/T2V-NPR."],"url":"http://arxiv.org/abs/2405.10317v1","category":"cs.CV"}
{"created":"2024-05-16 17:59:21","title":"Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model","abstract":"Visual In-Context Learning (ICL) has emerged as a promising research area due to its capability to accomplish various tasks with limited example pairs through analogical reasoning. However, training-based visual ICL has limitations in its ability to generalize to unseen tasks and requires the collection of a diverse task dataset. On the other hand, existing methods in the inference-based visual ICL category solely rely on textual prompts, which fail to capture fine-grained contextual information from given examples and can be time-consuming when converting from images to text prompts. To address these challenges, we propose Analogist, a novel inference-based visual ICL approach that exploits both visual and textual prompting techniques using a text-to-image diffusion model pretrained for image inpainting. For visual prompting, we propose a self-attention cloning (SAC) method to guide the fine-grained structural-level analogy between image examples. For textual prompting, we leverage GPT-4V's visual reasoning capability to efficiently generate text prompts and introduce a cross-attention masking (CAM) operation to enhance the accuracy of semantic-level analogy guided by text prompts. Our method is out-of-the-box and does not require fine-tuning or optimization. It is also generic and flexible, enabling a wide range of visual tasks to be performed in an in-context manner. Extensive experiments demonstrate the superiority of our method over existing approaches, both qualitatively and quantitatively.","sentences":["Visual In-Context Learning (ICL) has emerged as a promising research area due to its capability to accomplish various tasks with limited example pairs through analogical reasoning.","However, training-based visual ICL has limitations in its ability to generalize to unseen tasks and requires the collection of a diverse task dataset.","On the other hand, existing methods in the inference-based visual ICL category solely rely on textual prompts, which fail to capture fine-grained contextual information from given examples and can be time-consuming when converting from images to text prompts.","To address these challenges, we propose Analogist, a novel inference-based visual ICL approach that exploits both visual and textual prompting techniques using a text-to-image diffusion model pretrained for image inpainting.","For visual prompting, we propose a self-attention cloning (SAC) method to guide the fine-grained structural-level analogy between image examples.","For textual prompting, we leverage GPT-4V's visual reasoning capability to efficiently generate text prompts and introduce a cross-attention masking (CAM) operation to enhance the accuracy of semantic-level analogy guided by text prompts.","Our method is out-of-the-box and does not require fine-tuning or optimization.","It is also generic and flexible, enabling a wide range of visual tasks to be performed in an in-context manner.","Extensive experiments demonstrate the superiority of our method over existing approaches, both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.10316v1","category":"cs.CV"}
{"created":"2024-05-16 17:59:07","title":"TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction","abstract":"Learning in simulation and transferring the learned policy to the real world has the potential to enable generalist robots. The key challenge of this approach is to address simulation-to-reality (sim-to-real) gaps. Previous methods often require domain-specific knowledge a priori. We argue that a straightforward way to obtain such knowledge is by asking humans to observe and assist robot policy execution in the real world. The robots can then learn from humans to close various sim-to-real gaps. We propose TRANSIC, a data-driven approach to enable successful sim-to-real transfer based on a human-in-the-loop framework. TRANSIC allows humans to augment simulation policies to overcome various unmodeled sim-to-real gaps holistically through intervention and online correction. Residual policies can be learned from human corrections and integrated with simulation policies for autonomous execution. We show that our approach can achieve successful sim-to-real transfer in complex and contact-rich manipulation tasks such as furniture assembly. Through synergistic integration of policies learned in simulation and from humans, TRANSIC is effective as a holistic approach to addressing various, often coexisting sim-to-real gaps. It displays attractive properties such as scaling with human effort. Videos and code are available at https://transic-robot.github.io/","sentences":["Learning in simulation and transferring the learned policy to the real world has the potential to enable generalist robots.","The key challenge of this approach is to address simulation-to-reality (sim-to-real) gaps.","Previous methods often require domain-specific knowledge a priori.","We argue that a straightforward way to obtain such knowledge is by asking humans to observe and assist robot policy execution in the real world.","The robots can then learn from humans to close various sim-to-real gaps.","We propose TRANSIC, a data-driven approach to enable successful sim-to-real transfer based on a human-in-the-loop framework.","TRANSIC allows humans to augment simulation policies to overcome various unmodeled sim-to-real gaps holistically through intervention and online correction.","Residual policies can be learned from human corrections and integrated with simulation policies for autonomous execution.","We show that our approach can achieve successful sim-to-real transfer in complex and contact-rich manipulation tasks such as furniture assembly.","Through synergistic integration of policies learned in simulation and from humans, TRANSIC is effective as a holistic approach to addressing various, often coexisting sim-to-real gaps.","It displays attractive properties such as scaling with human effort.","Videos and code are available at https://transic-robot.github.io/"],"url":"http://arxiv.org/abs/2405.10315v1","category":"cs.RO"}
{"created":"2024-05-16 17:59:05","title":"CAT3D: Create Anything in 3D with Multi-View Diffusion Models","abstract":"Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene. We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model. Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene. These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time. CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation. See our project page for results and interactive demos at https://cat3d.github.io .","sentences":["Advances in 3D reconstruction have enabled high-quality 3D capture, but require a user to collect hundreds to thousands of images to create a 3D scene.","We present CAT3D, a method for creating anything in 3D by simulating this real-world capture process with a multi-view diffusion model.","Given any number of input images and a set of target novel viewpoints, our model generates highly consistent novel views of a scene.","These generated views can be used as input to robust 3D reconstruction techniques to produce 3D representations that can be rendered from any viewpoint in real-time.","CAT3D can create entire 3D scenes in as little as one minute, and outperforms existing methods for single image and few-view 3D scene creation.","See our project page for results and interactive demos at https://cat3d.github.io ."],"url":"http://arxiv.org/abs/2405.10314v1","category":"cs.CV"}
{"created":"2024-05-16 17:59:02","title":"How Far Are We From AGI","abstract":"The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors. Yet, the escalating demands on AI have highlighted the limitations of AI's current offerings, catalyzing a movement towards Artificial General Intelligence (AGI). AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution. While existing works have summarized specific recent advancements of AI, they lack a comprehensive discussion of AGI's definitions, goals, and developmental trajectories. Different from existing survey papers, this paper delves into the pivotal questions of our proximity to AGI and the strategies necessary for its realization through extensive surveys, discussions, and original perspectives. We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions. As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors. Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status-quo, and finally giving our roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains. In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI.","sentences":["The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors.","Yet, the escalating demands on AI have highlighted the limitations of AI's current offerings, catalyzing a movement towards Artificial General Intelligence (AGI).","AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution.","While existing works have summarized specific recent advancements of AI, they lack a comprehensive discussion of AGI's definitions, goals, and developmental trajectories.","Different from existing survey papers, this paper delves into the pivotal questions of our proximity to AGI and the strategies necessary for its realization through extensive surveys, discussions, and original perspectives.","We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions.","As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors.","Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status-quo, and finally giving our roadmap of how to reach the pinnacle of AGI.","Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains.","In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI."],"url":"http://arxiv.org/abs/2405.10313v1","category":"cs.AI"}
{"created":"2024-05-16 17:58:45","title":"UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models","abstract":"Recently, Multi-Modal(MM) Large Language Models(LLMs) have unlocked many complex use-cases that require MM understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities. To further improve the output fidelity of MM-LLMs we introduce the model-agnostic UniRAG technique that adds relevant retrieved information to prompts as few-shot examples during inference. Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT4 and Gemini-Pro and smaller open-source models like Llava, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by MM retrievers like UniIR models.","sentences":["Recently, Multi-Modal(MM) Large Language Models(LLMs) have unlocked many complex use-cases that require MM understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities.","To further improve the output fidelity of MM-LLMs we introduce the model-agnostic UniRAG technique that adds relevant retrieved information to prompts as few-shot examples during inference.","Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT4 and Gemini-Pro and smaller open-source models like Llava, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by MM retrievers like UniIR models."],"url":"http://arxiv.org/abs/2405.10311v1","category":"cs.IR"}
{"created":"2024-05-16 17:58:44","title":"Stochastic Q-learning for Large Discrete Action Spaces","abstract":"In complex environments with large discrete action spaces, effective decision-making is critical in reinforcement learning (RL). Despite the widespread use of value-based RL approaches like Q-learning, they come with a computational burden, necessitating the maximization of a value function over all actions in each iteration. This burden becomes particularly challenging when addressing large-scale problems and using deep neural networks as function approximators. In this paper, we present stochastic value-based RL approaches which, in each iteration, as opposed to optimizing over the entire set of $n$ actions, only consider a variable stochastic set of a sublinear number of actions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic value-based RL methods include, among others, Stochastic Q-learning, StochDQN, and StochDDQN, all of which integrate this stochastic approach for both value-function updates and action selection. The theoretical convergence of Stochastic Q-learning is established, while an analysis of stochastic maximization is provided. Moreover, through empirical validation, we illustrate that the various proposed approaches outperform the baseline methods across diverse environments, including different control problems, achieving near-optimal average returns in significantly reduced time.","sentences":["In complex environments with large discrete action spaces, effective decision-making is critical in reinforcement learning (RL).","Despite the widespread use of value-based RL approaches like Q-learning, they come with a computational burden, necessitating the maximization of a value function over all actions in each iteration.","This burden becomes particularly challenging when addressing large-scale problems and using deep neural networks as function approximators.","In this paper, we present stochastic value-based RL approaches which, in each iteration, as opposed to optimizing over the entire set of $n$ actions, only consider a variable stochastic set of a sublinear number of actions, possibly as small as $\\mathcal{O}(\\log(n))$. The presented stochastic value-based RL methods include, among others, Stochastic Q-learning, StochDQN, and StochDDQN, all of which integrate this stochastic approach for both value-function updates and action selection.","The theoretical convergence of Stochastic Q-learning is established, while an analysis of stochastic maximization is provided.","Moreover, through empirical validation, we illustrate that the various proposed approaches outperform the baseline methods across diverse environments, including different control problems, achieving near-optimal average returns in significantly reduced time."],"url":"http://arxiv.org/abs/2405.10310v1","category":"cs.LG"}
{"created":"2024-05-16 17:58:16","title":"On the lapse contour","abstract":"The gravitational path integral is usually implemented with a covariant action by analogy with other gauge field theories, but the gravitational case is different in important ways. A key difference is that the integrand has an essential singularity, which occurs at zero lapse where the spacetime metric degenerates. The lapse integration contour required to impose the local time reparametrization constraints must run from $-\\infty$ to $+\\infty$, yet must not pass through zero. This raises the question: what is the correct integration contour, and why? We study that question by starting with the reduced phase space path integral, which involves no essential singularity. We observe that if the momenta are to be integrated before the lapse, to obtain a configuration space path integral, the lapse contour should pass below the origin in the complex lapse plane. This contour is also consistent with the requirement that quantum field fluctuation amplitudes have the usual short distance vacuum form, and with obtaining the Bekenstein-Hawking horizon entropy from a Lorentzian path integral. We close with a discussion of related issues, including potential obstacles to deriving a nonperturbative covariant gravitational path integral.","sentences":["The gravitational path integral is usually implemented with a covariant action by analogy with other gauge field theories, but the gravitational case is different in important ways.","A key difference is that the integrand has an essential singularity, which occurs at zero lapse where the spacetime metric degenerates.","The lapse integration contour required to impose the local time reparametrization constraints must run from $-\\infty$ to $+\\infty$, yet must not pass through zero.","This raises the question: what is the correct integration contour, and why?","We study that question by starting with the reduced phase space path integral, which involves no essential singularity.","We observe that if the momenta are to be integrated before the lapse, to obtain a configuration space path integral, the lapse contour should pass below the origin in the complex lapse plane.","This contour is also consistent with the requirement that quantum field fluctuation amplitudes have the usual short distance vacuum form, and with obtaining the Bekenstein-Hawking horizon entropy from a Lorentzian path integral.","We close with a discussion of related issues, including potential obstacles to deriving a nonperturbative covariant gravitational path integral."],"url":"http://arxiv.org/abs/2405.10307v1","category":"hep-th"}
{"created":"2024-05-16 17:56:55","title":"4D Panoptic Scene Graph Generation","abstract":"We are living in a three-dimensional space while moving forward through a fourth dimension: time. To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce 4D Panoptic Scene Graph (PSG-4D), a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding. Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status information, and edges, which capture the temporal relations. To facilitate research in this new area, we build a richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of 1M frames, each of which is labeled with 4D panoptic segmentation masks as well as fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer, a Transformer-based model that can predict panoptic segmentation masks, track masks along the time axis, and generate the corresponding scene graphs via a relation component. Extensive experiments on the new dataset show that our method can serve as a strong baseline for future research on PSG-4D. In the end, we provide a real-world application example to demonstrate how we can achieve dynamic scene understanding by integrating a large language model into our PSG-4D system.","sentences":["We are living in a three-dimensional space while moving forward through a fourth dimension: time.","To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce 4D Panoptic Scene Graph (PSG-4D), a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding.","Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status information, and edges, which capture the temporal relations.","To facilitate research in this new area, we build a richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of 1M frames, each of which is labeled with 4D panoptic segmentation masks as well as fine-grained, dynamic scene graphs.","To solve PSG-4D, we propose PSG4DFormer, a Transformer-based model that can predict panoptic segmentation masks, track masks along the time axis, and generate the corresponding scene graphs via a relation component.","Extensive experiments on the new dataset show that our method can serve as a strong baseline for future research on PSG-4D. In the end, we provide a real-world application example to demonstrate how we can achieve dynamic scene understanding by integrating a large language model into our PSG-4D system."],"url":"http://arxiv.org/abs/2405.10305v1","category":"cs.CV"}
{"created":"2024-05-16 17:56:22","title":"Towards Unpolarized GPDs from Pseudo-Distributions","abstract":"We present an exploration of the unpolarized isovector proton generalized parton distributions (GPDs) $H^{u-d}(x, \\xi, t)$ and $E^{u-d}(x, \\xi, t)$ in the pseudo-distribution formalism using distillation. Taking advantage of the large kinematic coverage made possible by this approach, we present results on the moments of GPDs up to the order $x^3$ -- including their skewness dependence -- at a pion mass $m_\\pi = 358$ MeV and a lattice spacing $a = 0.094$ fm.","sentences":["We present an exploration of the unpolarized isovector proton generalized parton distributions (GPDs) $H^{u-d}(x, \\xi, t)$ and $E^{u-d}(x, \\xi, t)$ in the pseudo-distribution formalism using distillation.","Taking advantage of the large kinematic coverage made possible by this approach, we present results on the moments of GPDs up to the order $x^3$ -- including their skewness dependence -- at a pion mass $m_\\pi = 358$ MeV and a lattice spacing $a = 0.094$ fm."],"url":"http://arxiv.org/abs/2405.10304v1","category":"hep-lat"}
{"created":"2024-05-16 17:56:16","title":"Asymmetric Warm Dark Matter: from Cosmological Asymmetry to Chirality of Life","abstract":"We investigate a novel scenario involving asymmetric keV-range dark matter (DM) in the form of right-handed (sterile) neutrinos. Based on the Fermi-Dirac distribution, we demonstrate that asymmetric fermionic DM forms a Fermi degenerate gas, making it potentially colder than symmetric fermionic DM. This setup simultaneously accounts for the Universe's baryon asymmetry through tiny Yukawa interactions with Standard Model leptons and the Higgs field, and the homochirality of amino acids via decay into circularly polarized photons. This scenario can be investigated through soft X-ray searches conducted by current and upcoming space missions. The helical X-rays is a smoking-gun signal of our scenario. Additionally, we propose a new mechanism to suppress DM thermal production by introducing a light modulus, which may also benefit cosmology involving generic right-handed neutrinos with large mixing.","sentences":["We investigate a novel scenario involving asymmetric keV-range dark matter (DM) in the form of right-handed (sterile) neutrinos.","Based on the Fermi-Dirac distribution, we demonstrate that asymmetric fermionic DM forms a Fermi degenerate gas, making it potentially colder than symmetric fermionic DM.","This setup simultaneously accounts for the Universe's baryon asymmetry through tiny Yukawa interactions with Standard Model leptons and the Higgs field, and the homochirality of amino acids via decay into circularly polarized photons.","This scenario can be investigated through soft X-ray searches conducted by current and upcoming space missions.","The helical X-rays is a smoking-gun signal of our scenario.","Additionally, we propose a new mechanism to suppress DM thermal production by introducing a light modulus, which may also benefit cosmology involving generic right-handed neutrinos with large mixing."],"url":"http://arxiv.org/abs/2405.10303v1","category":"hep-ph"}
{"created":"2024-05-16 17:55:42","title":"Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift","abstract":"As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts. A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance. The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain. Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through a real-world dataset, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.","sentences":["As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts.","A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance.","The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution.","In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain.","Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation.","Our proposed methodologies are computationally efficient and easy to implement.","Beyond illustrating the performance of our method through a real-world dataset, we also delve into the theoretical details.","This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals.","Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts."],"url":"http://arxiv.org/abs/2405.10302v1","category":"stat.ME"}
{"created":"2024-05-16 17:55:24","title":"Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees","abstract":"Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values. For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.","sentences":["Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values.","For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making.","This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion.","It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution.","Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor.","It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy.","Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data.","En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor."],"url":"http://arxiv.org/abs/2405.10301v1","category":"stat.ML"}
{"created":"2024-05-16 17:54:15","title":"Grounding DINO 1.5: Advance the \"Edge\" of Open-Set Object Detection","abstract":"This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the \"Edge\" of open-set object detection. The suite encompasses two models: Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model optimized for faster speed demanded in many applications requiring edge deployment. The Grounding DINO 1.5 Pro model advances its predecessor by scaling up the model architecture, integrating an enhanced vision backbone, and expanding the training dataset to over 20 million images with grounding annotations, thereby achieving a richer semantic understanding. The Grounding DINO 1.5 Edge model, while designed for efficiency with reduced feature scales, maintains robust detection capabilities by being trained on the same comprehensive dataset. Empirical results demonstrate the effectiveness of Grounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP on the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot transfer benchmark, setting new records for open-set object detection. Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT, achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP on the LVIS-minival benchmark, making it more suitable for edge computing scenarios. Model examples and demos with API will be released at https://github.com/IDEA-Research/Grounding-DINO-1.5-API","sentences":["This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the \"Edge\" of open-set object detection.","The suite encompasses two models:","Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model optimized for faster speed demanded in many applications requiring edge deployment.","The Grounding DINO 1.5 Pro model advances its predecessor by scaling up the model architecture, integrating an enhanced vision backbone, and expanding the training dataset to over 20 million images with grounding annotations, thereby achieving a richer semantic understanding.","The Grounding DINO 1.5 Edge model, while designed for efficiency with reduced feature scales, maintains robust detection capabilities by being trained on the same comprehensive dataset.","Empirical results demonstrate the effectiveness of Grounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP on the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot transfer benchmark, setting new records for open-set object detection.","Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT, achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP on the LVIS-minival benchmark, making it more suitable for edge computing scenarios.","Model examples and demos with API will be released at https://github.com/IDEA-Research/Grounding-DINO-1.5-API"],"url":"http://arxiv.org/abs/2405.10300v1","category":"cs.CV"}
{"created":"2024-05-16 17:53:32","title":"HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models","abstract":"The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency, energy consumption, GPU memory usage, and performance. There is a growing interest in establishing Pareto frontiers for different language model configurations to identify optimal models with specified hardware constraints. Notably, architectures that excel in latency on one device may not perform optimally on another. However, exhaustive training and evaluation of numerous architectures across diverse hardware configurations is computationally prohibitive. To this end, we propose HW-GPT-Bench, a hardware-aware language model surrogate benchmark, where we leverage weight-sharing techniques from Neural Architecture Search (NAS) to efficiently train a supernet proxy, encompassing language models of varying scales in a single model. We conduct profiling of these models across 13 devices, considering 5 hardware metrics and 3 distinct model scales. Finally, we showcase the usability of HW-GPT-Bench using 8 different multi-objective NAS algorithms and evaluate the quality of the resultant Pareto fronts. Through this benchmark, our objective is to propel and expedite research in the advancement of multi-objective methods for NAS and structural pruning in large language models.","sentences":["The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency, energy consumption, GPU memory usage, and performance.","There is a growing interest in establishing Pareto frontiers for different language model configurations to identify optimal models with specified hardware constraints.","Notably, architectures that excel in latency on one device may not perform optimally on another.","However, exhaustive training and evaluation of numerous architectures across diverse hardware configurations is computationally prohibitive.","To this end, we propose HW-GPT-Bench, a hardware-aware language model surrogate benchmark, where we leverage weight-sharing techniques from Neural Architecture Search (NAS) to efficiently train a supernet proxy, encompassing language models of varying scales in a single model.","We conduct profiling of these models across 13 devices, considering 5 hardware metrics and 3 distinct model scales.","Finally, we showcase the usability of HW-GPT-Bench using 8 different multi-objective NAS algorithms and evaluate the quality of the resultant Pareto fronts.","Through this benchmark, our objective is to propel and expedite research in the advancement of multi-objective methods for NAS and structural pruning in large language models."],"url":"http://arxiv.org/abs/2405.10299v1","category":"cs.LG"}
{"created":"2024-05-16 17:52:36","title":"Low-Degree Polynomials Are Good Extractors","abstract":"We prove that random low-degree polynomials (over $\\mathbb{F}_2$) are unbiased, in an extremely general sense. That is, we show that random low-degree polynomials are good randomness extractors for a wide class of distributions. Prior to our work, such results were only known for the small families of (1) uniform sources, (2) affine sources, and (3) local sources. We significantly generalize these results, and prove the following.   1. Low-degree polynomials extract from small families. We show that a random low-degree polynomial is a good low-error extractor for any small family of sources. In particular, we improve the positive result of Alrabiah, Chattopadhyay, Goodman, Li, and Ribeiro (ICALP 2022) for local sources, and give new results for polynomial sources and variety sources via a single unified approach.   2. Low-degree polynomials extract from sumset sources. We show that a random low-degree polynomial is a good extractor for sumset sources, which are the most general large family of sources (capturing independent sources, interleaved sources, small-space sources, and more). This extractor achieves polynomially small error, and its min-entropy requirement is tight up to a square.   Our results on sumset extractors imply new complexity separations for linear ROBPs, and the tools that go into its proof have further applications, as well. The two main tools we use are a new structural result on sumset-punctured Reed-Muller codes, paired with a novel type of reduction between randomness extractors. Using the first new tool, we strengthen and generalize the extractor impossibility results of Chattopadhyay, Goodman, and Gurumukhani (ITCS 2024). Using the second, we show the existence of sumset extractors for min-entropy $k=O(\\log(n/\\varepsilon))$, resolving an open problem of Chattopadhyay and Liao (STOC 2022).","sentences":["We prove that random low-degree polynomials (over $\\mathbb{F}_2$) are unbiased, in an extremely general sense.","That is, we show that random low-degree polynomials are good randomness extractors for a wide class of distributions.","Prior to our work, such results were only known for the small families of (1) uniform sources, (2) affine sources, and (3) local sources.","We significantly generalize these results, and prove the following.   ","1. Low-degree polynomials extract from small families.","We show that a random low-degree polynomial is a good low-error extractor for any small family of sources.","In particular, we improve the positive result of Alrabiah, Chattopadhyay, Goodman, Li, and Ribeiro (ICALP 2022) for local sources, and give new results for polynomial sources and variety sources via a single unified approach.   ","2. Low-degree polynomials extract from sumset sources.","We show that a random low-degree polynomial is a good extractor for sumset sources, which are the most general large family of sources (capturing independent sources, interleaved sources, small-space sources, and more).","This extractor achieves polynomially small error, and its min-entropy requirement is tight up to a square.   ","Our results on sumset extractors imply new complexity separations for linear ROBPs, and the tools that go into its proof have further applications, as well.","The two main tools we use are a new structural result on sumset-punctured Reed-Muller codes, paired with a novel type of reduction between randomness extractors.","Using the first new tool, we strengthen and generalize the extractor impossibility results of Chattopadhyay, Goodman, and Gurumukhani (ITCS 2024).","Using the second, we show the existence of sumset extractors for min-entropy $k=O(\\log(n/\\varepsilon))$, resolving an open problem of Chattopadhyay and Liao (STOC 2022)."],"url":"http://arxiv.org/abs/2405.10297v1","category":"cs.CC"}
{"created":"2024-05-16 17:52:26","title":"Verifying Unboundedness via Amalgamation","abstract":"Well-structured transition systems (WSTS) are an abstract family of systems that encompasses a vast landscape of infinite-state systems. By requiring a well-quasi-ordering (wqo) on the set of states, a WSTS enables generic algorithms for classic verification tasks such as coverability and termination. However, even for systems that are WSTS like vector addition systems (VAS), the framework is notoriously ill-equipped to analyse reachability (as opposed to coverability). Moreover, some important types of infinite-state systems fall out of WSTS' scope entirely, such as pushdown systems (PDS).   Inspired by recent algorithmic techniques on VAS, we propose an abstract notion of systems where the set of runs is equipped with a wqo and supports amalgamation of runs. We show that it subsumes a large class of infinite-state systems, including (reachability languages of) VAS and PDS, and even all systems from the abstract framework of valence systems, except for those already known to be Turing-complete.   Moreover, this abstract setting enables simple and general algorithmic solutions to unboundedness problems, which have received much attention in recent years. We present algorithms for the (i) simultaneous unboundedness problem (which implies computability of downward closures and decidability of separability by piecewise testable languages), (ii) computing priority downward closures, (iii) deciding whether a language is bounded, meaning included in $w_1^*\\cdots w_k^*$ for some words $w_1,\\ldots,w_k$, and (iv)~effective regularity of unary languages. This leads to either drastically simpler proofs or new decidability results for a rich variety of systems.","sentences":["Well-structured transition systems (WSTS) are an abstract family of systems that encompasses a vast landscape of infinite-state systems.","By requiring a well-quasi-ordering (wqo) on the set of states, a WSTS enables generic algorithms for classic verification tasks such as coverability and termination.","However, even for systems that are WSTS like vector addition systems (VAS), the framework is notoriously ill-equipped to analyse reachability (as opposed to coverability).","Moreover, some important types of infinite-state systems fall out of WSTS' scope entirely, such as pushdown systems (PDS).   ","Inspired by recent algorithmic techniques on VAS, we propose an abstract notion of systems where the set of runs is equipped with a wqo and supports amalgamation of runs.","We show that it subsumes a large class of infinite-state systems, including (reachability languages of) VAS and PDS, and even all systems from the abstract framework of valence systems, except for those already known to be Turing-complete.   ","Moreover, this abstract setting enables simple and general algorithmic solutions to unboundedness problems, which have received much attention in recent years.","We present algorithms for the (i) simultaneous unboundedness problem (which implies computability of downward closures and decidability of separability by piecewise testable languages), (ii) computing priority downward closures, (iii) deciding whether a language is bounded, meaning included in $w_1^*\\cdots w_k^*$ for some words $w_1,\\ldots,w_k$, and (iv)~effective regularity of unary languages.","This leads to either drastically simpler proofs or new decidability results for a rich variety of systems."],"url":"http://arxiv.org/abs/2405.10296v1","category":"cs.FL"}
{"created":"2024-05-16 17:52:12","title":"Societal Adaptation to Advanced AI","abstract":"Existing strategies for managing risks from advanced AI systems often focus on affecting what AI systems are developed and how they diffuse. However, this approach becomes less feasible as the number of developers of advanced AI grows, and impedes beneficial use-cases as well as harmful ones. In response, we urge a complementary approach: increasing societal adaptation to advanced AI, that is, reducing the expected negative impacts from a given level of diffusion of a given AI capability. We introduce a conceptual framework which helps identify adaptive interventions that avoid, defend against and remedy potentially harmful uses of AI systems, illustrated with examples in election manipulation, cyberterrorism, and loss of control to AI decision-makers. We discuss a three-step cycle that society can implement to adapt to AI. Increasing society's ability to implement this cycle builds its resilience to advanced AI. We conclude with concrete recommendations for governments, industry, and third-parties.","sentences":["Existing strategies for managing risks from advanced AI systems often focus on affecting what AI systems are developed and how they diffuse.","However, this approach becomes less feasible as the number of developers of advanced AI grows, and impedes beneficial use-cases as well as harmful ones.","In response, we urge a complementary approach: increasing societal adaptation to advanced AI, that is, reducing the expected negative impacts from a given level of diffusion of a given AI capability.","We introduce a conceptual framework which helps identify adaptive interventions that avoid, defend against and remedy potentially harmful uses of AI systems, illustrated with examples in election manipulation, cyberterrorism, and loss of control to AI decision-makers.","We discuss a three-step cycle that society can implement to adapt to AI.","Increasing society's ability to implement this cycle builds its resilience to advanced AI.","We conclude with concrete recommendations for governments, industry, and third-parties."],"url":"http://arxiv.org/abs/2405.10295v1","category":"cs.CY"}
{"created":"2024-05-16 17:52:09","title":"Corrections to adiabatic behavior for long paths","abstract":"The cost and the error of the adiabatic theorem for preparing the final eigenstate are discussed in terms of path length. Previous studies in terms of the norm of the Hamiltonian and its derivatives with the spectral gap are limited to describe the cost of adiabatic state preparation for large systems. We argue that total time is not a good measure for determining the computational difficulty of adiabatic quantum computation by developing a no-go theorem. From the result of time-periodic Hamiltonian cases, we suggest that there are proxies for computational cost which typically grow as path length increases when the error is kept fixed and small and consider possible conjectures on how general the behavior is.","sentences":["The cost and the error of the adiabatic theorem for preparing the final eigenstate are discussed in terms of path length.","Previous studies in terms of the norm of the Hamiltonian and its derivatives with the spectral gap are limited to describe the cost of adiabatic state preparation for large systems.","We argue that total time is not a good measure for determining the computational difficulty of adiabatic quantum computation by developing a no-go theorem.","From the result of time-periodic Hamiltonian cases, we suggest that there are proxies for computational cost which typically grow as path length increases when the error is kept fixed and small and consider possible conjectures on how general the behavior is."],"url":"http://arxiv.org/abs/2405.10294v1","category":"quant-ph"}
{"created":"2024-05-16 17:51:38","title":"Charged Black Holes from Interacting Vacuum","abstract":"In this paper charged black holes are obtained assuming that a Born-Infeld electrodynamics may arise from an interaction between the electromagnetic field and a vacuum component. In this context Cauchy horizons do not appear in the maximal analytical extension once an event horizon is formed so that the interior spacetime does not suffer from any sort of instabilities which are well known in the literature. On the contrary, the causal structure exhibits an event horizon -- encapsulating a spacelike singularity -- and a cosmological horizon. We show that the strong cosmic censorship is then restored for a wide range of the parameters including configurations in which the black hole charge is much larger than its mass. We also show that the black hole thus formed described by our solution exhibits an unstable photon sphere analogous to that of the Schwarzschild metric.","sentences":["In this paper charged black holes are obtained assuming that a Born-Infeld electrodynamics may arise from an interaction between the electromagnetic field and a vacuum component.","In this context Cauchy horizons do not appear in the maximal analytical extension once an event horizon is formed so that the interior spacetime does not suffer from any sort of instabilities which are well known in the literature.","On the contrary, the causal structure exhibits an event horizon -- encapsulating a spacelike singularity -- and a cosmological horizon.","We show that the strong cosmic censorship is then restored for a wide range of the parameters including configurations in which the black hole charge is much larger than its mass.","We also show that the black hole thus formed described by our solution exhibits an unstable photon sphere analogous to that of the Schwarzschild metric."],"url":"http://arxiv.org/abs/2405.10293v1","category":"gr-qc"}
{"created":"2024-05-16 17:50:19","title":"Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning","abstract":"Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.","sentences":["Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios.","However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments.","To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL).","Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action.","Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards.","Finally, our framework uses these task rewards to fine-tune the entire VLM with RL.","Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini.","Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method."],"url":"http://arxiv.org/abs/2405.10292v1","category":"cs.AI"}
{"created":"2024-05-16 17:50:05","title":"Interaction induced splitting of Dirac monopoles in the topological Thouless pumping of strongly interacting Bosons and SU($N$) Fermions","abstract":"Motivated by the observation of the breakdown of quantization for the Thouless pump in the presence of strong interaction by ETH [Walter et. al. Nat. Phys. 19, 1471 (2023), Viebahn et. al. arXiv:2308.03756], we study the interplay of strong interaction and topology in the (1+1)-dimensional interacting Rice-Mele model. We point out that the quantization of the interacting Thouless pump is dictated by the Chern number, i.e., the Dirac monopoles enclosed by the generalized Brillouin zone of the many-body wave function. By analyzing the change of location monopoles due to interaction, we predict the Thouless charge pump for strongly interacting Bose and SU($N$) Fermi gases in optical lattices and explain the ETH experiment.","sentences":["Motivated by the observation of the breakdown of quantization for the Thouless pump in the presence of strong interaction by ETH [Walter et.","al.","Nat.","Phys. 19, 1471 (2023), Viebahn et.","al. arXiv:2308.03756], we study the interplay of strong interaction and topology in the (1+1)-dimensional interacting Rice-Mele model.","We point out that the quantization of the interacting Thouless pump is dictated by the Chern number, i.e., the Dirac monopoles enclosed by the generalized Brillouin zone of the many-body wave function.","By analyzing the change of location monopoles due to interaction, we predict the Thouless charge pump for strongly interacting Bose and SU($N$)","Fermi gases in optical lattices and explain the ETH experiment."],"url":"http://arxiv.org/abs/2405.10291v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-16 17:48:21","title":"Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction","abstract":"Facts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets.","sentences":["Facts extraction is pivotal for constructing knowledge graphs.","Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction.","In this paper, we specifically address the extraction of temporal facts from natural language text.","Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences.","To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts.","In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results.","To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs).","To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset.","Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets."],"url":"http://arxiv.org/abs/2405.10288v1","category":"cs.CL"}
{"created":"2024-05-16 17:47:49","title":"Exotic compact objects and light bosonic fields","abstract":"In this note, we discuss the effect of light, non-gauge, bosonic degrees of freedom on the exterior spacetime of an exotic compact object. We show that such fields generically introduce large deviations from spacetimes of vacuum General Relativity near and outside the surfaces of ultra-compact exotic objects unless one assumes they totally decouple from the standard model or new heavy fields. Hence, using solutions of vacuum General Relativity to model ultra-compact exotic objects and their perturbations relies implicitly on this assumption or on the absence of such fields.","sentences":["In this note, we discuss the effect of light, non-gauge, bosonic degrees of freedom on the exterior spacetime of an exotic compact object.","We show that such fields generically introduce large deviations from spacetimes of vacuum General Relativity near and outside the surfaces of ultra-compact exotic objects unless one assumes they totally decouple from the standard model or new heavy fields.","Hence, using solutions of vacuum General Relativity to model ultra-compact exotic objects and their perturbations relies implicitly on this assumption or on the absence of such fields."],"url":"http://arxiv.org/abs/2405.10287v1","category":"gr-qc"}
{"created":"2024-05-16 17:46:54","title":"FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models","abstract":"Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized. Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity. Then, we devise effective solutions for addressing both problems, which essentially require training with multiple true positive pairs. Finally, we propose training with sigmoid loss to address such a requirement. We show very large gains over the current state-of-the-art for both image recognition ($\\sim +6\\%$ on average over 11 datasets) and image retrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO).","sentences":["Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized.","Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity.","Then, we devise effective solutions for addressing both problems, which essentially require training with multiple true positive pairs.","Finally, we propose training with sigmoid loss to address such a requirement.","We show very large gains over the current state-of-the-art for both image recognition ($\\sim +6\\%$ on average over 11 datasets) and image retrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO)."],"url":"http://arxiv.org/abs/2405.10286v1","category":"cs.CV"}
{"created":"2024-05-16 17:46:12","title":"Interacting chiral fermions on the lattice with matrix product operator norms","abstract":"We develop a formalism for simulating one-dimensional interacting chiral fermions on the lattice without breaking any local symmetries by defining a Fock space endowed with a semi-definite norm defined in terms of matrix product operators. This formalism can be understood as a second-quantized form of Stacey fermions, hence providing a possible solution for the fermion doubling problem and circumventing the Nielsen-Ninomiya theorem. We prove that the emerging theory is hermitian by virtue of the fact that it gives rise to a hermitian generalized eigenvalue problem and that it has local features as it can be simulated using tensor network methods similar to the ones used for simulating local quantum Hamiltonians. We also show that the scaling limit of the free model recovers the chiral fermion field. As a proof of principle, we consider a single Weyl fermion on a periodic ring with Hubbard-type nearest-neighbor interactions and construct a variational generalized DMRG code demonstrating that the ground states of the system for large system sizes can be determined efficiently.","sentences":["We develop a formalism for simulating one-dimensional interacting chiral fermions on the lattice without breaking any local symmetries by defining a Fock space endowed with a semi-definite norm defined in terms of matrix product operators.","This formalism can be understood as a second-quantized form of Stacey fermions, hence providing a possible solution for the fermion doubling problem and circumventing the Nielsen-Ninomiya theorem.","We prove that the emerging theory is hermitian by virtue of the fact that it gives rise to a hermitian generalized eigenvalue problem and that it has local features as it can be simulated using tensor network methods similar to the ones used for simulating local quantum Hamiltonians.","We also show that the scaling limit of the free model recovers the chiral fermion field.","As a proof of principle, we consider a single Weyl fermion on a periodic ring with Hubbard-type nearest-neighbor interactions and construct a variational generalized DMRG code demonstrating that the ground states of the system for large system sizes can be determined efficiently."],"url":"http://arxiv.org/abs/2405.10285v1","category":"hep-lat"}
{"created":"2024-05-16 17:41:09","title":"GKLS Vector Field Dynamics for Gaussian States","abstract":"We construct the vector field associated with the GKLS generator for systems described by Gaussian states. This vector field is defined on the dual space of the algebra of operators, restricted to operators quadratic in position and momentum. It is shown that the GKLS dynamics accepts a decomposition principle, that is, this vector field can be decomposed in three parts, a conservative Hamiltonian component, a gradient-like and a Choi-Krauss vector field. The last two terms are considered a \"perturbation\" associated with dissipation. Examples are presented for a harmonic oscillator with different dissipation terms.","sentences":["We construct the vector field associated with the GKLS generator for systems described by Gaussian states.","This vector field is defined on the dual space of the algebra of operators, restricted to operators quadratic in position and momentum.","It is shown that the GKLS dynamics accepts a decomposition principle, that is, this vector field can be decomposed in three parts, a conservative Hamiltonian component, a gradient-like and a Choi-Krauss vector field.","The last two terms are considered a \"perturbation\" associated with dissipation.","Examples are presented for a harmonic oscillator with different dissipation terms."],"url":"http://arxiv.org/abs/2405.10282v1","category":"quant-ph"}
{"created":"2024-05-16 17:36:03","title":"Photon emission from macroscopic currents","abstract":"Coherent states are a well-established tool of quantum optics to describe electromagnetic waves in terms of photons. However, they do not describe the near-field regime of radiation sources. Instead, we generically use classical solutions of Maxwell's equations to describe radiation in the near-field regime. The classical solutions provide linear relations between currents and emitted electromagnetic fields, whereas evolution of states at the quantum level proceeds through unitary time evolution operators involving photon operators. This begs questions how the classical radiation equations relate to unitary quantum evolution, and how we can describe macroscopic fields from antennas or magnetic coils in terms of elementary photons. The present paper answers both questions through the construction of generalized Glauber states for radiation emitters.","sentences":["Coherent states are a well-established tool of quantum optics to describe electromagnetic waves in terms of photons.","However, they do not describe the near-field regime of radiation sources.","Instead, we generically use classical solutions of Maxwell's equations to describe radiation in the near-field regime.","The classical solutions provide linear relations between currents and emitted electromagnetic fields, whereas evolution of states at the quantum level proceeds through unitary time evolution operators involving photon operators.","This begs questions how the classical radiation equations relate to unitary quantum evolution, and how we can describe macroscopic fields from antennas or magnetic coils in terms of elementary photons.","The present paper answers both questions through the construction of generalized Glauber states for radiation emitters."],"url":"http://arxiv.org/abs/2405.10279v1","category":"quant-ph"}
{"created":"2024-05-16 17:34:37","title":"Hilbert Functions and Low-Degree Randomness Extractors","abstract":"For $S\\subseteq \\mathbb{F}^n$, consider the linear space of restrictions of degree-$d$ polynomials to $S$. The Hilbert function of $S$, denoted $\\mathrm{h}_S(d,\\mathbb{F})$, is the dimension of this space. We obtain a tight lower bound on the smallest value of the Hilbert function of subsets $S$ of arbitrary finite grids in $\\mathbb{F}^n$ with a fixed size $|S|$. We achieve this by proving that this value coincides with a combinatorial quantity, namely the smallest number of low Hamming weight points in a down-closed set of size $|S|$.   Understanding the smallest values of Hilbert functions is closely related to the study of degree-$d$ closure of sets, a notion introduced by Nie and Wang (Journal of Combinatorial Theory, Series A, 2015). We use bounds on the Hilbert function to obtain a tight bound on the size of degree-$d$ closures of subsets of $\\mathbb{F}_q^n$, which answers a question posed by Doron, Ta-Shma, and Tell (Computational Complexity, 2022).   We use the bounds on the Hilbert function and degree-$d$ closure of sets to prove that a random low-degree polynomial is an extractor for samplable randomness sources. Most notably, we prove the existence of low-degree extractors and dispersers for sources generated by constant-degree polynomials and polynomial-size circuits. Until recently, even the existence of arbitrary deterministic extractors for such sources was not known.","sentences":["For $S\\subseteq \\mathbb{F}^n$, consider the linear space of restrictions of degree-$d$ polynomials to $S$. The Hilbert function of $S$, denoted $\\mathrm{h}_S(d,\\mathbb{F})$, is the dimension of this space.","We obtain a tight lower bound on the smallest value of the Hilbert function of subsets $S$ of arbitrary finite grids in $\\mathbb{F}^n$ with a fixed size $|S|$.","We achieve this by proving that this value coincides with a combinatorial quantity, namely the smallest number of low Hamming weight points in a down-closed set of size $|S|$.   Understanding the smallest values of Hilbert functions is closely related to the study of degree-$d$ closure of sets, a notion introduced by Nie and Wang (Journal of Combinatorial Theory, Series A, 2015).","We use bounds on the Hilbert function to obtain a tight bound on the size of degree-$d$ closures of subsets of $\\mathbb{F}_q^n$, which answers a question posed by Doron, Ta-Shma, and Tell (Computational Complexity, 2022).   ","We use the bounds on the Hilbert function and degree-$d$ closure of sets to prove that a random low-degree polynomial is an extractor for samplable randomness sources.","Most notably, we prove the existence of low-degree extractors and dispersers for sources generated by constant-degree polynomials and polynomial-size circuits.","Until recently, even the existence of arbitrary deterministic extractors for such sources was not known."],"url":"http://arxiv.org/abs/2405.10277v1","category":"cs.CC"}
{"created":"2024-05-16 17:29:37","title":"Faces that Speak: Jointly Synthesising Talking Face and Speech from Text","abstract":"The goal of this work is to simultaneously generate natural talking faces and speech outputs from text. We achieve this by integrating Talking Face Generation (TFG) and Text-to-Speech (TTS) systems into a unified framework. We address the main challenges of each task: (1) generating a range of head poses representative of real-world scenarios, and (2) ensuring voice consistency despite variations in facial motion for the same identity. To tackle these issues, we introduce a motion sampler based on conditional flow matching, which is capable of high-quality motion code generation in an efficient way. Moreover, we introduce a novel conditioning method for the TTS system, which utilises motion-removed features from the TFG model to yield uniform speech outputs. Our extensive experiments demonstrate that our method effectively creates natural-looking talking faces and speech that accurately match the input text. To our knowledge, this is the first effort to build a multimodal synthesis system that can generalise to unseen identities.","sentences":["The goal of this work is to simultaneously generate natural talking faces and speech outputs from text.","We achieve this by integrating Talking Face Generation (TFG) and Text-to-Speech (TTS) systems into a unified framework.","We address the main challenges of each task: (1) generating a range of head poses representative of real-world scenarios, and (2) ensuring voice consistency despite variations in facial motion for the same identity.","To tackle these issues, we introduce a motion sampler based on conditional flow matching, which is capable of high-quality motion code generation in an efficient way.","Moreover, we introduce a novel conditioning method for the TTS system, which utilises motion-removed features from the TFG model to yield uniform speech outputs.","Our extensive experiments demonstrate that our method effectively creates natural-looking talking faces and speech that accurately match the input text.","To our knowledge, this is the first effort to build a multimodal synthesis system that can generalise to unseen identities."],"url":"http://arxiv.org/abs/2405.10272v1","category":"cs.CV"}
{"created":"2024-05-16 17:27:41","title":"Automated Federated Learning via Informed Pruning","abstract":"Federated learning (FL) represents a pivotal shift in machine learning (ML) as it enables collaborative training of local ML models coordinated by a central aggregator, all without the need to exchange local data. However, its application on edge devices is hindered by limited computational capabilities and data communication challenges, compounded by the inherent complexity of Deep Learning (DL) models. Model pruning is identified as a key technique for compressing DL models on devices with limited resources. Nonetheless, conventional pruning techniques typically rely on manually crafted heuristics and demand human expertise to achieve a balance between model size, speed, and accuracy, often resulting in sub-optimal solutions.   In this study, we introduce an automated federated learning approach utilizing informed pruning, called AutoFLIP, which dynamically prunes and compresses DL models within both the local clients and the global server. It leverages a federated loss exploration phase to investigate model gradient behavior across diverse datasets and losses, providing insights into parameter significance. Our experiments showcase notable enhancements in scenarios with strong non-IID data, underscoring AutoFLIP's capacity to tackle computational constraints and achieve superior global convergence.","sentences":["Federated learning (FL) represents a pivotal shift in machine learning (ML) as it enables collaborative training of local ML models coordinated by a central aggregator, all without the need to exchange local data.","However, its application on edge devices is hindered by limited computational capabilities and data communication challenges, compounded by the inherent complexity of Deep Learning (DL) models.","Model pruning is identified as a key technique for compressing DL models on devices with limited resources.","Nonetheless, conventional pruning techniques typically rely on manually crafted heuristics and demand human expertise to achieve a balance between model size, speed, and accuracy, often resulting in sub-optimal solutions.   ","In this study, we introduce an automated federated learning approach utilizing informed pruning, called AutoFLIP, which dynamically prunes and compresses DL models within both the local clients and the global server.","It leverages a federated loss exploration phase to investigate model gradient behavior across diverse datasets and losses, providing insights into parameter significance.","Our experiments showcase notable enhancements in scenarios with strong non-IID data, underscoring AutoFLIP's capacity to tackle computational constraints and achieve superior global convergence."],"url":"http://arxiv.org/abs/2405.10271v1","category":"cs.LG"}
{"created":"2024-05-16 17:25:26","title":"Quadratic quasi-normal mode dependence on linear mode parity","abstract":"Quasi-normal modes (QNMs) uniquely describe the gravitational-wave ringdown of post-merger black holes. While the linear QNM regime has been extensively studied, recent work has highlighted the importance of second-perturbative-order, quadratic QNMs (QQNMs) arising from the nonlinear coupling of linear QNMs. Previous attempts to quantify the magnitude of these QQNMs have shown discrepant results. Using a new hyperboloidal framework, we resolve the discrepancy by showing that the QQNM/QNM ratio is a function not only of the black hole parameters but also of the ratio between even- and odd-parity linear QNMs: the ratio QQNM/QNM depends on what created the ringing black hole, but only through this ratio of even- to odd-parity linear perturbations.","sentences":["Quasi-normal modes (QNMs) uniquely describe the gravitational-wave ringdown of post-merger black holes.","While the linear QNM regime has been extensively studied, recent work has highlighted the importance of second-perturbative-order, quadratic QNMs (QQNMs) arising from the nonlinear coupling of linear QNMs.","Previous attempts to quantify the magnitude of these QQNMs have shown discrepant results.","Using a new hyperboloidal framework, we resolve the discrepancy by showing that the QQNM/QNM ratio is a function not only of the black hole parameters but also of the ratio between even- and odd-parity linear QNMs: the ratio QQNM/QNM depends on what created the ringing black hole, but only through this ratio of even- to odd-parity linear perturbations."],"url":"http://arxiv.org/abs/2405.10270v1","category":"gr-qc"}
{"created":"2024-05-16 17:19:58","title":"Sharpness-Aware Minimization in Genetic Programming","abstract":"Sharpness-Aware Minimization (SAM) was recently introduced as a regularization procedure for training deep neural networks. It simultaneously minimizes the fitness (or loss) function and the so-called fitness sharpness. The latter serves as a %connection between the geometry of the fitness landscape measure of the nonlinear behavior of a solution %and generalization and does so by finding solutions that lie in neighborhoods having uniformly similar loss values across all fitness cases. In this contribution, we adapt SAM for tree Genetic Programming (TGP) by exploring the semantic neighborhoods of solutions using two simple approaches By capitalizing upon perturbing input and output of program trees, sharpness can be estimated and used as a second optimization criterion during the evolution. To better understand the impact of this variant of SAM on TGP, we collect numerous indicators of the evolutionary process, including generalization ability, complexity, diversity, and a recently proposed genotype-phenotype mapping to study the amount of redundancy in trees. The experimental results demonstrate that using any of the two proposed SAM adaptations in TGP allows (i) a significant reduction of tree sizes in the population and (ii) a decrease in redundancy of the trees. When assessed on real-world benchmarks, the generalization ability of the elite solutions does not deteriorate.","sentences":["Sharpness-Aware Minimization (SAM) was recently introduced as a regularization procedure for training deep neural networks.","It simultaneously minimizes the fitness (or loss) function and the so-called fitness sharpness.","The latter serves as a %connection between the geometry of the fitness landscape measure of the nonlinear behavior of a solution %and generalization and does so by finding solutions that lie in neighborhoods having uniformly similar loss values across all fitness cases.","In this contribution, we adapt SAM for tree Genetic Programming (TGP) by exploring the semantic neighborhoods of solutions using two simple approaches By capitalizing upon perturbing input and output of program trees, sharpness can be estimated and used as a second optimization criterion during the evolution.","To better understand the impact of this variant of SAM on TGP, we collect numerous indicators of the evolutionary process, including generalization ability, complexity, diversity, and a recently proposed genotype-phenotype mapping to study the amount of redundancy in trees.","The experimental results demonstrate that using any of the two proposed SAM adaptations in TGP allows (i) a significant reduction of tree sizes in the population and (ii) a decrease in redundancy of the trees.","When assessed on real-world benchmarks, the generalization ability of the elite solutions does not deteriorate."],"url":"http://arxiv.org/abs/2405.10267v1","category":"cs.NE"}
{"created":"2024-05-16 17:15:39","title":"Architectures and random properties of symplectic quantum circuits","abstract":"Parametrized and random unitary (or orthogonal) $n$-qubit circuits play a central role in quantum information. As such, one could naturally assume that circuits implementing symplectic transformation would attract similar attention. However, this is not the case, as $\\mathbb{SP}(d/2)$ -- the group of $d\\times d$ unitary symplectic matrices -- has thus far been overlooked. In this work, we aim at starting to right this wrong. We begin by presenting a universal set of generators $\\mathcal{G}$ for the symplectic algebra $i\\mathfrak{sp}(d/2)$, consisting of one- and two-qubit Pauli operators acting on neighboring sites in a one-dimensional lattice. Here, we uncover two critical differences between such set, and equivalent ones for unitary and orthogonal circuits. Namely, we find that the operators in $\\mathcal{G}$ cannot generate arbitrary local symplectic unitaries and that they are not translationally invariant. We then review the Schur-Weyl duality between the symplectic group and the Brauer algebra, and use tools from Weingarten calculus to prove that Pauli measurements at the output of Haar random symplectic circuits can converge to Gaussian processes. As a by-product, such analysis provides us with concentration bounds for Pauli measurements in circuits that form $t$-designs over $\\mathbb{SP}(d/2)$. To finish, we present tensor-network tools to analyze shallow random symplectic circuits, and we use these to numerically show that computational-basis measurements anti-concentrate at logarithmic depth.","sentences":["Parametrized and random unitary (or orthogonal) $n$-qubit circuits play a central role in quantum information.","As such, one could naturally assume that circuits implementing symplectic transformation would attract similar attention.","However, this is not the case, as $\\mathbb{SP}(d/2)$ -- the group of $d\\times d$ unitary symplectic matrices -- has thus far been overlooked.","In this work, we aim at starting to right this wrong.","We begin by presenting a universal set of generators $\\mathcal{G}$ for the symplectic algebra $i\\mathfrak{sp}(d/2)$, consisting of one- and two-qubit Pauli operators acting on neighboring sites in a one-dimensional lattice.","Here, we uncover two critical differences between such set, and equivalent ones for unitary and orthogonal circuits.","Namely, we find that the operators in $\\mathcal{G}$ cannot generate arbitrary local symplectic unitaries and that they are not translationally invariant.","We then review the Schur-Weyl duality between the symplectic group and the Brauer algebra, and use tools from Weingarten calculus to prove that Pauli measurements at the output of Haar random symplectic circuits can converge to Gaussian processes.","As a by-product, such analysis provides us with concentration bounds for Pauli measurements in circuits that form $t$-designs over $\\mathbb{SP}(d/2)$. To finish, we present tensor-network tools to analyze shallow random symplectic circuits, and we use these to numerically show that computational-basis measurements anti-concentrate at logarithmic depth."],"url":"http://arxiv.org/abs/2405.10264v1","category":"quant-ph"}
{"created":"2024-05-16 17:13:55","title":"On Partially Unitary Learning","abstract":"The problem of an optimal mapping between Hilbert spaces $IN$ of $\\left|\\psi\\right\\rangle$ and $OUT$ of $\\left|\\phi\\right\\rangle$ based on a set of wavefunction measurements (within a phase) $\\psi_l \\to \\phi_l$, $l=1\\dots M$, is formulated as an optimization problem maximizing the total fidelity $\\sum_{l=1}^{M} \\omega^{(l)} \\left|\\langle\\phi_l|\\mathcal{U}|\\psi_l\\rangle\\right|^2$ subject to probability preservation constraints on $\\mathcal{U}$ (partial unitarity). Constructed operator $\\mathcal{U}$ can be considered as a $IN$ to $OUT$ quantum channel; it is a partially unitary rectangular matrix of the dimension $\\dim(OUT) \\times \\dim(IN)$ transforming operators as $A^{OUT}=\\mathcal{U} A^{IN} \\mathcal{U}^{\\dagger}$. An iteration algorithm finding the global maximum of this optimization problem is developed and it's application to a number of problems is demonstrated. A software product implementing the algorithm is available from the authors.","sentences":["The problem of an optimal mapping between Hilbert spaces $IN$ of $\\left|\\psi\\right\\rangle$ and $OUT$ of $\\left|\\phi\\right\\rangle$ based on a set of wavefunction measurements (within a phase) $\\psi_l \\to \\phi_l$, $l=1\\dots M$, is formulated as an optimization problem maximizing the total fidelity $\\sum_{l=1}^{M} \\omega^{(l)} \\left|\\langle\\phi_l|\\mathcal{U}|\\psi_l\\rangle\\right|^2$ subject to probability preservation constraints on $\\mathcal{U}$ (partial unitarity).","Constructed operator $\\mathcal{U}$ can be considered as a $IN$ to $OUT$ quantum channel; it is a partially unitary rectangular matrix of the dimension $\\dim(OUT) \\times \\dim(IN)$ transforming operators as $A^{OUT}=\\mathcal{U} A^{IN} \\mathcal{U}^{\\dagger}$.","An iteration algorithm finding the global maximum of this optimization problem is developed and it's application to a number of problems is demonstrated.","A software product implementing the algorithm is available from the authors."],"url":"http://arxiv.org/abs/2405.10263v1","category":"cs.LG"}
{"created":"2024-05-16 17:13:25","title":"Two-Phase Dynamics of Interactions Explains the Starting Point of a DNN Learning Over-Fitted Features","abstract":"This paper investigates the dynamics of a deep neural network (DNN) learning interactions. Previous studies have discovered and mathematically proven that given each input sample, a well-trained DNN usually only encodes a small number of interactions (non-linear relationships) between input variables in the sample. A series of theorems have been derived to prove that we can consider the DNN's inference equivalent to using these interactions as primitive patterns for inference. In this paper, we discover the DNN learns interactions in two phases. The first phase mainly penalizes interactions of medium and high orders, and the second phase mainly learns interactions of gradually increasing orders. We can consider the two-phase phenomenon as the starting point of a DNN learning over-fitted features. Such a phenomenon has been widely shared by DNNs with various architectures trained for different tasks. Therefore, the discovery of the two-phase dynamics provides a detailed mechanism for how a DNN gradually learns different inference patterns (interactions). In particular, we have also verified the claim that high-order interactions have weaker generalization power than low-order interactions. Thus, the discovered two-phase dynamics also explains how the generalization power of a DNN changes during the training process.","sentences":["This paper investigates the dynamics of a deep neural network (DNN) learning interactions.","Previous studies have discovered and mathematically proven that given each input sample, a well-trained DNN usually only encodes a small number of interactions (non-linear relationships) between input variables in the sample.","A series of theorems have been derived to prove that we can consider the DNN's inference equivalent to using these interactions as primitive patterns for inference.","In this paper, we discover the DNN learns interactions in two phases.","The first phase mainly penalizes interactions of medium and high orders, and the second phase mainly learns interactions of gradually increasing orders.","We can consider the two-phase phenomenon as the starting point of a DNN learning over-fitted features.","Such a phenomenon has been widely shared by DNNs with various architectures trained for different tasks.","Therefore, the discovery of the two-phase dynamics provides a detailed mechanism for how a DNN gradually learns different inference patterns (interactions).","In particular, we have also verified the claim that high-order interactions have weaker generalization power than low-order interactions.","Thus, the discovered two-phase dynamics also explains how the generalization power of a DNN changes during the training process."],"url":"http://arxiv.org/abs/2405.10262v1","category":"cs.LG"}
{"created":"2024-05-16 17:12:18","title":"Keep It Private: Unsupervised Privatization of Online Text","abstract":"Authorship obfuscation techniques hold the promise of helping people protect their privacy in online communications by automatically rewriting text to hide the identity of the original author. However, obfuscation has been evaluated in narrow settings in the NLP literature and has primarily been addressed with superficial edit operations that can lead to unnatural outputs. In this work, we introduce an automatic text privatization framework that fine-tunes a large language model via reinforcement learning to produce rewrites that balance soundness, sense, and privacy. We evaluate it extensively on a large-scale test set of English Reddit posts by 68k authors composed of short-medium length texts. We study how the performance changes among evaluative conditions including authorial profile length and authorship detection strategy. Our method maintains high text quality according to both automated metrics and human evaluation, and successfully evades several automated authorship attacks.","sentences":["Authorship obfuscation techniques hold the promise of helping people protect their privacy in online communications by automatically rewriting text to hide the identity of the original author.","However, obfuscation has been evaluated in narrow settings in the NLP literature and has primarily been addressed with superficial edit operations that can lead to unnatural outputs.","In this work, we introduce an automatic text privatization framework that fine-tunes a large language model via reinforcement learning to produce rewrites that balance soundness, sense, and privacy.","We evaluate it extensively on a large-scale test set of English Reddit posts by 68k authors composed of short-medium length texts.","We study how the performance changes among evaluative conditions including authorial profile length and authorship detection strategy.","Our method maintains high text quality according to both automated metrics and human evaluation, and successfully evades several automated authorship attacks."],"url":"http://arxiv.org/abs/2405.10260v1","category":"cs.CL"}
{"created":"2024-05-16 17:12:00","title":"Energy-limited quantum dynamics","abstract":"We consider quantum systems with energy constraints. In general, quantum channels and continuous-time dynamics need not satisfy energy conservation. Physically meaningful channels, however, can only introduce a finite amount of energy to the system, and continuous-time dynamics may only increase the energy gradually over time. We systematically study such \"energy-limited\" channels and dynamics. For Markovian dynamics, energy-limitedness is equivalent to a single operator inequality in the Heisenberg picture. By tracking the output energy, we observe that the energy-constrained operator and diamond norms of Shirokov and Winter satisfy submultiplicativity estimates with respect to energy-limited channels. This makes for a powerful toolkit for quantitative analyses of dynamical problems in finite and infinite-dimensional systems. As an application, we derive state-dependent bounds for quantum speed limits and related problems that outperform the usual operator/diamond norm estimates, which have to account for fluctuations in high-energy states.","sentences":["We consider quantum systems with energy constraints.","In general, quantum channels and continuous-time dynamics need not satisfy energy conservation.","Physically meaningful channels, however, can only introduce a finite amount of energy to the system, and continuous-time dynamics may only increase the energy gradually over time.","We systematically study such \"energy-limited\" channels and dynamics.","For Markovian dynamics, energy-limitedness is equivalent to a single operator inequality in the Heisenberg picture.","By tracking the output energy, we observe that the energy-constrained operator and diamond norms of Shirokov and Winter satisfy submultiplicativity estimates with respect to energy-limited channels.","This makes for a powerful toolkit for quantitative analyses of dynamical problems in finite and infinite-dimensional systems.","As an application, we derive state-dependent bounds for quantum speed limits and related problems that outperform the usual operator/diamond norm estimates, which have to account for fluctuations in high-energy states."],"url":"http://arxiv.org/abs/2405.10259v1","category":"quant-ph"}
{"created":"2024-05-16 16:59:58","title":"When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models","abstract":"As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D.","sentences":["As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces.","This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data.","Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems.","Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs).","It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation.","The paper also includes a brief review of other methods that integrate 3D and language.","The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs.","Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world.","To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D."],"url":"http://arxiv.org/abs/2405.10255v1","category":"cs.CV"}
{"created":"2024-05-16 16:59:12","title":"PRISM: A Multi-Modal Generative Foundation Model for Slide-Level Histopathology","abstract":"Foundation models in computational pathology promise to unlock the development of new clinical decision support systems and models for precision medicine. However, there is a mismatch between most clinical analysis, which is defined at the level of one or more whole slide images, and foundation models to date, which process the thousands of image tiles contained in a whole slide image separately. The requirement to train a network to aggregate information across a large number of tiles in multiple whole slide images limits these models' impact. In this work, we present a slide-level foundation model for H&E-stained histopathology, PRISM, that builds on Virchow tile embeddings and leverages clinical report text for pre-training. Using the tile embeddings, PRISM produces slide-level embeddings with the ability to generate clinical reports, resulting in several modes of use. Using text prompts, PRISM achieves zero-shot cancer detection and sub-typing performance approaching and surpassing that of a supervised aggregator model. Using the slide embeddings with linear classifiers, PRISM surpasses supervised aggregator models. Furthermore, we demonstrate that fine-tuning of the PRISM slide encoder yields label-efficient training for biomarker prediction, a task that typically suffers from low availability of training data; an aggregator initialized with PRISM and trained on as little as 10% of the training data can outperform a supervised baseline that uses all of the data.","sentences":["Foundation models in computational pathology promise to unlock the development of new clinical decision support systems and models for precision medicine.","However, there is a mismatch between most clinical analysis, which is defined at the level of one or more whole slide images, and foundation models to date, which process the thousands of image tiles contained in a whole slide image separately.","The requirement to train a network to aggregate information across a large number of tiles in multiple whole slide images limits these models' impact.","In this work, we present a slide-level foundation model for H&E-stained histopathology, PRISM, that builds on Virchow tile embeddings and leverages clinical report text for pre-training.","Using the tile embeddings, PRISM produces slide-level embeddings with the ability to generate clinical reports, resulting in several modes of use.","Using text prompts, PRISM achieves zero-shot cancer detection and sub-typing performance approaching and surpassing that of a supervised aggregator model.","Using the slide embeddings with linear classifiers, PRISM surpasses supervised aggregator models.","Furthermore, we demonstrate that fine-tuning of the PRISM slide encoder yields label-efficient training for biomarker prediction, a task that typically suffers from low availability of training data; an aggregator initialized with PRISM and trained on as little as 10% of the training data can outperform a supervised baseline that uses all of the data."],"url":"http://arxiv.org/abs/2405.10254v1","category":"eess.IV"}
{"created":"2024-05-16 16:56:54","title":"A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks","abstract":"Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance of LLMs in natural language generation (NLG) tasks, a pivotal criterion for determining model excellence. Thus, this paper conducts a comprehensive evaluation of well-known and high-performing LLMs, namely ChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models, in the context of NLG tasks. We select English and Chinese datasets encompassing Dialogue Generation and Text Summarization. Moreover, we propose a common evaluation setting that incorporates input templates and post-processing strategies. Our study reports both automatic results, accompanied by a detailed analysis.","sentences":["Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation.","However, to the best of our knowledge, no work has specifically investigated the performance of LLMs in natural language generation (NLG) tasks, a pivotal criterion for determining model excellence.","Thus, this paper conducts a comprehensive evaluation of well-known and high-performing LLMs, namely ChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models, in the context of NLG tasks.","We select English and Chinese datasets encompassing Dialogue Generation and Text Summarization.","Moreover, we propose a common evaluation setting that incorporates input templates and post-processing strategies.","Our study reports both automatic results, accompanied by a detailed analysis."],"url":"http://arxiv.org/abs/2405.10251v1","category":"cs.CL"}
{"created":"2024-05-16 16:55:06","title":"IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers","abstract":"Large language models (LLMs) have exhibited a strong promise in automatically generating executable code from natural language descriptions, particularly with interactive features that allow users to engage in the code-generation process by instructing the LLM with iterative feedback. However, existing interaction paradigms often assume that users have expert knowledge to debug source code and are not optimized for non-professional programmers' use. This raises challenges in making interactive code generation more accessible for individuals with varying levels of programming expertise. To tackle these challenges, we present IntelliExplain, which offers a novel human-LLM interaction paradigm to enhance non-professional programmers' experience by enabling them to interact with source code via natural language explanations. Users interact with IntelliExplain by providing natural language corrective feedback on errors they identify from the explanations. Feedback is used by the system to revise the code, until the user is satisfied with explanations by the system of the code. Our user study demonstrates that users with IntelliExplain achieve a significantly higher success rate 11.6% and 25.3% better than with vanilla GPT-3.5, while also requiring 39.0% and 15.6% less time in Text-to-SQL and Python code generation tasks, respectively.","sentences":["Large language models (LLMs) have exhibited a strong promise in automatically generating executable code from natural language descriptions, particularly with interactive features that allow users to engage in the code-generation process by instructing the LLM with iterative feedback.","However, existing interaction paradigms often assume that users have expert knowledge to debug source code and are not optimized for non-professional programmers' use.","This raises challenges in making interactive code generation more accessible for individuals with varying levels of programming expertise.","To tackle these challenges, we present IntelliExplain, which offers a novel human-LLM interaction paradigm to enhance non-professional programmers' experience by enabling them to interact with source code via natural language explanations.","Users interact with IntelliExplain by providing natural language corrective feedback on errors they identify from the explanations.","Feedback is used by the system to revise the code, until the user is satisfied with explanations by the system of the code.","Our user study demonstrates that users with IntelliExplain achieve a significantly higher success rate 11.6% and 25.3% better than with vanilla GPT-3.5, while also requiring 39.0% and 15.6% less time in Text-to-SQL and Python code generation tasks, respectively."],"url":"http://arxiv.org/abs/2405.10250v1","category":"cs.HC"}
{"created":"2024-05-16 16:49:20","title":"A Foundation Model for Brain Lesion Segmentation with Mixture of Modality Experts","abstract":"Brain lesion segmentation plays an essential role in neurological research and diagnosis. As brain lesions can be caused by various pathological alterations, different types of brain lesions tend to manifest with different characteristics on different imaging modalities. Due to this complexity, brain lesion segmentation methods are often developed in a task-specific manner. A specific segmentation model is developed for a particular lesion type and imaging modality. However, the use of task-specific models requires predetermination of the lesion type and imaging modality, which complicates their deployment in real-world scenarios. In this work, we propose a universal foundation model for 3D brain lesion segmentation, which can automatically segment different types of brain lesions for input data of various imaging modalities. We formulate a novel Mixture of Modality Experts (MoME) framework with multiple expert networks attending to different imaging modalities. A hierarchical gating network combines the expert predictions and fosters expertise collaboration. Furthermore, we introduce a curriculum learning strategy during training to avoid the degeneration of each expert network and preserve their specialization. We evaluated the proposed method on nine brain lesion datasets, encompassing five imaging modalities and eight lesion types. The results show that our model outperforms state-of-the-art universal models and provides promising generalization to unseen datasets.","sentences":["Brain lesion segmentation plays an essential role in neurological research and diagnosis.","As brain lesions can be caused by various pathological alterations, different types of brain lesions tend to manifest with different characteristics on different imaging modalities.","Due to this complexity, brain lesion segmentation methods are often developed in a task-specific manner.","A specific segmentation model is developed for a particular lesion type and imaging modality.","However, the use of task-specific models requires predetermination of the lesion type and imaging modality, which complicates their deployment in real-world scenarios.","In this work, we propose a universal foundation model for 3D brain lesion segmentation, which can automatically segment different types of brain lesions for input data of various imaging modalities.","We formulate a novel Mixture of Modality Experts (MoME) framework with multiple expert networks attending to different imaging modalities.","A hierarchical gating network combines the expert predictions and fosters expertise collaboration.","Furthermore, we introduce a curriculum learning strategy during training to avoid the degeneration of each expert network and preserve their specialization.","We evaluated the proposed method on nine brain lesion datasets, encompassing five imaging modalities and eight lesion types.","The results show that our model outperforms state-of-the-art universal models and provides promising generalization to unseen datasets."],"url":"http://arxiv.org/abs/2405.10246v1","category":"eess.IV"}
{"created":"2024-05-16 16:46:46","title":"DocuMint: Docstring Generation for Python using Small Language Models","abstract":"Effective communication, specifically through documentation, is the beating heart of collaboration among contributors in software development. Recent advancements in language models (LMs) have enabled the introduction of a new type of actor in that ecosystem: LM-powered assistants capable of code generation, optimization, and maintenance. Our study investigates the efficacy of small language models (SLMs) for generating high-quality docstrings by assessing accuracy, conciseness, and clarity, benchmarking performance quantitatively through mathematical formulas and qualitatively through human evaluation using Likert scale. Further, we introduce DocuMint, as a large-scale supervised fine-tuning dataset with 100,000 samples. In quantitative experiments, Llama 3 8B achieved the best performance across all metrics, with conciseness and clarity scores of 0.605 and 64.88, respectively. However, under human evaluation, CodeGemma 7B achieved the highest overall score with an average of 8.3 out of 10 across all metrics. Fine-tuning the CodeGemma 2B model using the DocuMint dataset led to significant improvements in performance across all metrics, with gains of up to 22.5% in conciseness. The fine-tuned model and the dataset can be found in HuggingFace and the code can be found in the repository.","sentences":["Effective communication, specifically through documentation, is the beating heart of collaboration among contributors in software development.","Recent advancements in language models (LMs) have enabled the introduction of a new type of actor in that ecosystem: LM-powered assistants capable of code generation, optimization, and maintenance.","Our study investigates the efficacy of small language models (SLMs) for generating high-quality docstrings by assessing accuracy, conciseness, and clarity, benchmarking performance quantitatively through mathematical formulas and qualitatively through human evaluation using Likert scale.","Further, we introduce DocuMint, as a large-scale supervised fine-tuning dataset with 100,000 samples.","In quantitative experiments, Llama 3 8B achieved the best performance across all metrics, with conciseness and clarity scores of 0.605 and 64.88, respectively.","However, under human evaluation, CodeGemma 7B achieved the highest overall score with an average of 8.3 out of 10 across all metrics.","Fine-tuning the CodeGemma 2B model using the DocuMint dataset led to significant improvements in performance across all metrics, with gains of up to 22.5% in conciseness.","The fine-tuned model and the dataset can be found in HuggingFace and the code can be found in the repository."],"url":"http://arxiv.org/abs/2405.10243v1","category":"cs.SE"}
{"created":"2024-05-16 16:39:03","title":"An introduction to map-making for CMB experiments","abstract":"The cosmic microwave background (CMB) anisotropies are a powerful probe of the early universe, and have largely contributed to establishing the current standard cosmological model. To extract the information encoded in those tiny variations, one must first compress the raw, time-domain data collected by a telescope into maps of the sky at the observed frequencies, in a procedure known as map-making. I provide a general introduction to this problem, and highlight a few specificities of the MAPPRAISER implementation.","sentences":["The cosmic microwave background (CMB) anisotropies are a powerful probe of the early universe, and have largely contributed to establishing the current standard cosmological model.","To extract the information encoded in those tiny variations, one must first compress the raw, time-domain data collected by a telescope into maps of the sky at the observed frequencies, in a procedure known as map-making.","I provide a general introduction to this problem, and highlight a few specificities of the MAPPRAISER implementation."],"url":"http://arxiv.org/abs/2405.10239v1","category":"astro-ph.CO"}
{"created":"2024-05-16 16:34:06","title":"Boone--Higman Embeddings for Contracting Self-Similar Groups","abstract":"We give a short proof that every contracting self-similar group embeds into a finitely presented simple group. In particular, any contracting self-similar group embeds into the corresponding R\\\"over--Nekrashevych group, and this in turn embeds into one of the twisted Brin--Thompson groups introduced by the first author and Matthew Zaremsky. The proof here is a simplification of a more general argument given by the authors, Collin Bleak, and Matthew Zaremsky for contracting rational similarity groups.","sentences":["We give a short proof that every contracting self-similar group embeds into a finitely presented simple group.","In particular, any contracting self-similar group embeds into the corresponding R\\\"over--Nekrashevych group, and this in turn embeds into one of the twisted Brin--Thompson groups introduced by the first author and Matthew Zaremsky.","The proof here is a simplification of a more general argument given by the authors, Collin Bleak, and Matthew Zaremsky for contracting rational similarity groups."],"url":"http://arxiv.org/abs/2405.10234v1","category":"math.GR"}
{"created":"2024-05-16 16:34:03","title":"iDRAMA-Scored-2024: A Dataset of the Scored Social Media Platform from 2020 to 2023","abstract":"Online web communities often face bans for violating platform policies, encouraging their migration to alternative platforms. This migration, however, can result in increased toxicity and unforeseen consequences on the new platform. In recent years, researchers have collected data from many alternative platforms, indicating coordinated efforts leading to offline events, conspiracy movements, hate speech propagation, and harassment. Thus, it becomes crucial to characterize and understand these alternative platforms. To advance research in this direction, we collect and release a large-scale dataset from Scored -- an alternative Reddit platform that sheltered banned fringe communities, for example, c/TheDonald (a prominent right-wing community) and c/GreatAwakening (a conspiratorial community). Over four years, we collected approximately 57M posts from Scored, with at least 58 communities identified as migrating from Reddit and over 950 communities created since the platform's inception. Furthermore, we provide sentence embeddings of all posts in our dataset, generated through a state-of-the-art model, to further advance the field in characterizing the discussions within these communities. We aim to provide these resources to facilitate their investigations without the need for extensive data collection and processing efforts.","sentences":["Online web communities often face bans for violating platform policies, encouraging their migration to alternative platforms.","This migration, however, can result in increased toxicity and unforeseen consequences on the new platform.","In recent years, researchers have collected data from many alternative platforms, indicating coordinated efforts leading to offline events, conspiracy movements, hate speech propagation, and harassment.","Thus, it becomes crucial to characterize and understand these alternative platforms.","To advance research in this direction, we collect and release a large-scale dataset from Scored -- an alternative Reddit platform that sheltered banned fringe communities, for example, c/TheDonald (a prominent right-wing community) and c/GreatAwakening (a conspiratorial community).","Over four years, we collected approximately 57M posts from Scored, with at least 58 communities identified as migrating from Reddit and over 950 communities created since the platform's inception.","Furthermore, we provide sentence embeddings of all posts in our dataset, generated through a state-of-the-art model, to further advance the field in characterizing the discussions within these communities.","We aim to provide these resources to facilitate their investigations without the need for extensive data collection and processing efforts."],"url":"http://arxiv.org/abs/2405.10233v1","category":"cs.SI"}
{"created":"2024-05-16 16:33:34","title":"Beyond Static Calibration: The Impact of User Preference Dynamics on Calibrated Recommendation","abstract":"Calibration in recommender systems is an important performance criterion that ensures consistency between the distribution of user preference categories and that of recommendations generated by the system. Standard methods for mitigating miscalibration typically assume that user preference profiles are static, and they measure calibration relative to the full history of user's interactions, including possibly outdated and stale preference categories. We conjecture that this approach can lead to recommendations that, while appearing calibrated, in fact, distort users' true preferences. In this paper, we conduct a preliminary investigation of recommendation calibration at a more granular level, taking into account evolving user preferences. By analyzing differently sized training time windows from the most recent interactions to the oldest, we identify the most relevant segment of user's preferences that optimizes the calibration metric. We perform an exploratory analysis with datasets from different domains with distinctive user-interaction characteristics. We demonstrate how the evolving nature of user preferences affects recommendation calibration, and how this effect is manifested differently depending on the characteristics of the data in a given domain. Datasets, codes, and more detailed experimental results are available at: https://github.com/nicolelin13/DynamicCalibrationUMAP.","sentences":["Calibration in recommender systems is an important performance criterion that ensures consistency between the distribution of user preference categories and that of recommendations generated by the system.","Standard methods for mitigating miscalibration typically assume that user preference profiles are static, and they measure calibration relative to the full history of user's interactions, including possibly outdated and stale preference categories.","We conjecture that this approach can lead to recommendations that, while appearing calibrated, in fact, distort users' true preferences.","In this paper, we conduct a preliminary investigation of recommendation calibration at a more granular level, taking into account evolving user preferences.","By analyzing differently sized training time windows from the most recent interactions to the oldest, we identify the most relevant segment of user's preferences that optimizes the calibration metric.","We perform an exploratory analysis with datasets from different domains with distinctive user-interaction characteristics.","We demonstrate how the evolving nature of user preferences affects recommendation calibration, and how this effect is manifested differently depending on the characteristics of the data in a given domain.","Datasets, codes, and more detailed experimental results are available at: https://github.com/nicolelin13/DynamicCalibrationUMAP."],"url":"http://arxiv.org/abs/2405.10232v1","category":"cs.IR"}
{"created":"2024-05-16 16:29:49","title":"Influencer Cartels","abstract":"Social media influencers account for a growing share of marketing worldwide. We demonstrate the existence of a novel form of market failure in this advertising market: influencer cartels, where groups of influencers collude to increase their advertising revenue by inflating their engagement. Our theoretical model shows that influencer cartels can improve consumer welfare if they expand social media engagement to the target audience, or reduce welfare if they divert engagement to less relevant audiences. We validate the model empirically using novel data on influencer cartels combined with machine learning tools, and derive policy implications for how to maximize consumer welfare.","sentences":["Social media influencers account for a growing share of marketing worldwide.","We demonstrate the existence of a novel form of market failure in this advertising market: influencer cartels, where groups of influencers collude to increase their advertising revenue by inflating their engagement.","Our theoretical model shows that influencer cartels can improve consumer welfare if they expand social media engagement to the target audience, or reduce welfare if they divert engagement to less relevant audiences.","We validate the model empirically using novel data on influencer cartels combined with machine learning tools, and derive policy implications for how to maximize consumer welfare."],"url":"http://arxiv.org/abs/2405.10231v1","category":"econ.GN"}
{"created":"2024-05-16 16:27:13","title":"The fermionic double smeared null energy condition","abstract":"Energy conditions are crucial for understanding why exotic phenomena such as traversable wormholes and closed timelike curves remain elusive. In this paper, we prove the Double Smeared Null Energy Condition (DSNEC) for the fermionic free theory in 4-dimensional flat Minkowski space-time, extending previous work on the same energy condition for the bosonic case [1][2] by adapting Fewster and Mistry's method [3] to the energy-momentum tensor $T_{++}$. A notable difference from previous works lies in the presence of the $\\gamma_0 \\gamma_+$ matrix in $T_{++}$, causing a loss of symmetry. This challenge is addressed by making use of its square-root matrix. We provide explicit analytic results for the massless case as well as numerical insights for the mass-dependence of the bound in the case of Gaussian smearing.","sentences":["Energy conditions are crucial for understanding why exotic phenomena such as traversable wormholes and closed timelike curves remain elusive.","In this paper, we prove the Double Smeared Null Energy Condition (DSNEC) for the fermionic free theory in 4-dimensional flat Minkowski space-time, extending previous work on the same energy condition for the bosonic case [1][2] by adapting Fewster and Mistry's method [3] to the energy-momentum tensor $T_{++}$. A notable difference from previous works lies in the presence of the $\\gamma_0 \\gamma_+$ matrix in $T_{++}$, causing a loss of symmetry.","This challenge is addressed by making use of its square-root matrix.","We provide explicit analytic results for the massless case as well as numerical insights for the mass-dependence of the bound in the case of Gaussian smearing."],"url":"http://arxiv.org/abs/2405.10228v1","category":"gr-qc"}
{"created":"2024-05-16 16:25:14","title":"Experimental Validation of Collision-Radiation Dataset for Molecular Hydrogen in Plasmas","abstract":"Quantitative spectroscopy of molecular hydrogen has generated substantial demand, leading to the accumulation of diverse elementary-process data encompassing radiative transitions, electron-impact transitions, predissociations, and quenching. However, their rates currently available are still sparse and there are inconsistencies among those proposed by different authors. In this study, we demonstrate an experimental validation of such molecular dataset by composing a collisional-radiative model (CRM) for molecular hydrogen and comparing experimentally-obtained vibronic populations across multiple levels. From the population kinetics of molecular hydrogen, the importance of each elementary process in various parameter space is studied. In low-density plasmas (electron density $n_\\mathrm{e} \\lesssim 10^{17}\\;\\mathrm{m^{-3}}$) the excitation rates from the ground states and radiative decay rates, both of which have been reported previously, determines the excited state population. The inconsistency in the excitation rates affects the population distribution the most significantly in this parameter space. On the other hand, in higher density plasmas ($n_\\mathrm{e} \\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$), the excitation rates \\textit{from} excited states become important, which have never been reported in the literature, and may need to be approximated in some way. In order to validate these molecular datasets and approximated rates, we carried out experimental observations for two different hydrogen plasmas; a low-density radio-frequency (RF) heated plasma ($n_\\mathrm{e}\\approx 10^{16}\\;\\mathrm{m^{-3}}$) and the Large Helical Device (LHD) divertor plasma ($n_\\mathrm{e}\\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$)... [continued]","sentences":["Quantitative spectroscopy of molecular hydrogen has generated substantial demand, leading to the accumulation of diverse elementary-process data encompassing radiative transitions, electron-impact transitions, predissociations, and quenching.","However, their rates currently available are still sparse and there are inconsistencies among those proposed by different authors.","In this study, we demonstrate an experimental validation of such molecular dataset by composing a collisional-radiative model (CRM) for molecular hydrogen and comparing experimentally-obtained vibronic populations across multiple levels.","From the population kinetics of molecular hydrogen, the importance of each elementary process in various parameter space is studied.","In low-density plasmas (electron density $n_\\mathrm{e} \\lesssim 10^{17}\\;\\mathrm{m^{-3}}$) the excitation rates from the ground states and radiative decay rates, both of which have been reported previously, determines the excited state population.","The inconsistency in the excitation rates affects the population distribution the most significantly in this parameter space.","On the other hand, in higher density plasmas ($n_\\mathrm{e} \\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$), the excitation rates \\textit{from} excited states become important, which have never been reported in the literature, and may need to be approximated in some way.","In order to validate these molecular datasets and approximated rates, we carried out experimental observations for two different hydrogen plasmas; a low-density radio-frequency (RF) heated plasma ($n_\\mathrm{e}\\approx 10^{16}\\;\\mathrm{m^{-3}}$) and the Large Helical Device (LHD) divertor plasma ($n_\\mathrm{e}\\gtrsim 10^{18}\\;\\mathrm{m^{-3}}$)...","[continued]"],"url":"http://arxiv.org/abs/2405.10227v1","category":"physics.plasm-ph"}
{"created":"2024-05-16 16:24:36","title":"Geometric phase amplification in a clock interferometer for enhanced metrology","abstract":"High-precision measurements are crucial for testing the fundamental laws of nature and for advancing the technological frontier. Clock interferometry, where particles with an internal clock are coherently split and recombined along two spatial paths, has sparked significant interest due to its fundamental implications, especially at the intersection of quantum mechanics and general relativity. Here, we demonstrate that a clock interferometer provides metrological improvement with respect to its technical-noise-limited counterpart employing a single internal quantum state. This enhancement around a critical working point can be interpreted as a geometric-phase-induced signal-to-noise ratio gain. In our experimental setup, we infer a precision enhancement of 8.8 decibels when measuring a small difference between external fields. We estimate that tens of decibels of precision enhancement could be attained for measurements with a higher atom flux. This opens the door to the development of a superior probe for fundamental physics as well as a high-performance sensor for various technological applications.","sentences":["High-precision measurements are crucial for testing the fundamental laws of nature and for advancing the technological frontier.","Clock interferometry, where particles with an internal clock are coherently split and recombined along two spatial paths, has sparked significant interest due to its fundamental implications, especially at the intersection of quantum mechanics and general relativity.","Here, we demonstrate that a clock interferometer provides metrological improvement with respect to its technical-noise-limited counterpart employing a single internal quantum state.","This enhancement around a critical working point can be interpreted as a geometric-phase-induced signal-to-noise ratio gain.","In our experimental setup, we infer a precision enhancement of 8.8 decibels when measuring a small difference between external fields.","We estimate that tens of decibels of precision enhancement could be attained for measurements with a higher atom flux.","This opens the door to the development of a superior probe for fundamental physics as well as a high-performance sensor for various technological applications."],"url":"http://arxiv.org/abs/2405.10226v1","category":"quant-ph"}
{"created":"2024-05-16 17:19:06","title":"A Tale of Two Languages: Large-Vocabulary Continuous Sign Language Recognition from Spoken Language Supervision","abstract":"In this work, our goals are two fold: large-vocabulary continuous sign language recognition (CSLR), and sign language retrieval. To this end, we introduce a multi-task Transformer model, CSLR2, that is able to ingest a signing sequence and output in a joint embedding space between signed language and spoken language text. To enable CSLR evaluation in the large-vocabulary setting, we introduce new dataset annotations that have been manually collected. These provide continuous sign-level annotations for six hours of test videos, and will be made publicly available. We demonstrate that by a careful choice of loss functions, training the model for both the CSLR and retrieval tasks is mutually beneficial in terms of performance -- retrieval improves CSLR performance by providing context, while CSLR improves retrieval with more fine-grained supervision. We further show the benefits of leveraging weak and noisy supervision from large-vocabulary datasets such as BOBSL, namely sign-level pseudo-labels, and English subtitles. Our model significantly outperforms the previous state of the art on both tasks.","sentences":["In this work, our goals are two fold: large-vocabulary continuous sign language recognition (CSLR), and sign language retrieval.","To this end, we introduce a multi-task Transformer model, CSLR2, that is able to ingest a signing sequence and output in a joint embedding space between signed language and spoken language text.","To enable CSLR evaluation in the large-vocabulary setting, we introduce new dataset annotations that have been manually collected.","These provide continuous sign-level annotations for six hours of test videos, and will be made publicly available.","We demonstrate that by a careful choice of loss functions, training the model for both the CSLR and retrieval tasks is mutually beneficial in terms of performance -- retrieval improves CSLR performance by providing context, while CSLR improves retrieval with more fine-grained supervision.","We further show the benefits of leveraging weak and noisy supervision from large-vocabulary datasets such as BOBSL, namely sign-level pseudo-labels, and English subtitles.","Our model significantly outperforms the previous state of the art on both tasks."],"url":"http://arxiv.org/abs/2405.10266v1","category":"cs.CV"}
{"created":"2024-05-16 16:36:16","title":"Novel Data Models for Inter-operable LCA Frameworks","abstract":"Life cycle assessment (LCA) plays a critical role in assessing the environmental impacts of a product, technology, or service throughout its entire life cycle. Nonetheless, many existing LCA tools and methods lack adequate metadata management, which can hinder their further development and wide adoption. In the example of LCA for clean energy technologies, metadata helps monitor data and the environment that holds the integrity of the energy assets and sustainability of the materials sources across their entire value chains. Ontologizing metadata, i.e. a common vocabulary and language to connect multiple data sources, as well as implementing AI-aware data management, can have long-lasting, positive, and accelerating effects along with collecting and utilizing quality data from different sources and across the entire data lifecycle. The integration of ontologies in life cycle assessments has garnered significant attention in recent years. We synthesized the existing literature on ontologies for LCAs, providing insights into this interdisciplinary field's evolution, current state, and future directions. We also proposed the framework for a suitable data model and the workflow thereof to warrant the alignment with existing ontologies, practical frameworks, and industry standards.","sentences":["Life cycle assessment (LCA) plays a critical role in assessing the environmental impacts of a product, technology, or service throughout its entire life cycle.","Nonetheless, many existing LCA tools and methods lack adequate metadata management, which can hinder their further development and wide adoption.","In the example of LCA for clean energy technologies, metadata helps monitor data and the environment that holds the integrity of the energy assets and sustainability of the materials sources across their entire value chains.","Ontologizing metadata, i.e. a common vocabulary and language to connect multiple data sources, as well as implementing AI-aware data management, can have long-lasting, positive, and accelerating effects along with collecting and utilizing quality data from different sources and across the entire data lifecycle.","The integration of ontologies in life cycle assessments has garnered significant attention in recent years.","We synthesized the existing literature on ontologies for LCAs, providing insights into this interdisciplinary field's evolution, current state, and future directions.","We also proposed the framework for a suitable data model and the workflow thereof to warrant the alignment with existing ontologies, practical frameworks, and industry standards."],"url":"http://arxiv.org/abs/2405.10235v1","category":"cs.DB"}
{"created":"2024-05-16 17:58:39","title":"Hydrodynamic Edge Modes and Fragile Surface States of Symmetry Protected Integer Quantum Hall Effect of Bosons","abstract":"We adapt the fluid description of Fractional Quantum Hall (FQH) states, as seen in (arXiv:2203.06516), to model a system of interacting two-component bosons. This system represents the simplest physical realization of an interacting bosonic Symmetry-Protected Topological (SPT) phase, also known as the integer quantum Hall effect (IQHE) of bosons. In particular, we demonstrate how the fluid dynamical boundary conditions of no-penetration and no-stress at a hard wall naturally give rise to the two counter-propagating boundary modes expected in these SPT phases. Moreover, we identify energy-conserving hydro boundary conditions that can either create a gap in these edge modes or completely isolate the edge states from the bulk, as described in (Physical Review X 14, 011057 (2024)), where they are termed fragile surface states. These fragile surface states are typically absent in K-matrix edge theories and require bulk dynamics to manifest. By leveraging insights from hydrodynamical boundary dynamics, we can further elucidate the intricate surface properties of SPTs beyond the usual topological quantum field theory based approaches.","sentences":["We adapt the fluid description of Fractional Quantum Hall (FQH) states, as seen in (arXiv:2203.06516), to model a system of interacting two-component bosons.","This system represents the simplest physical realization of an interacting bosonic Symmetry-Protected Topological (SPT) phase, also known as the integer quantum Hall effect (IQHE) of bosons.","In particular, we demonstrate how the fluid dynamical boundary conditions of no-penetration and no-stress at a hard wall naturally give rise to the two counter-propagating boundary modes expected in these SPT phases.","Moreover, we identify energy-conserving hydro boundary conditions that can either create a gap in these edge modes or completely isolate the edge states from the bulk, as described in (Physical Review X 14, 011057 (2024)), where they are termed fragile surface states.","These fragile surface states are typically absent in K-matrix edge theories and require bulk dynamics to manifest.","By leveraging insights from hydrodynamical boundary dynamics, we can further elucidate the intricate surface properties of SPTs beyond the usual topological quantum field theory based approaches."],"url":"http://arxiv.org/abs/2405.10309v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 17:58:19","title":"Efficient Implementation of an Abstract Domain of Quantified First-Order Formulas","abstract":"This paper lays a practical foundation for using abstract interpretation with an abstract domain that consists of sets of quantified first-order logic formulas. This abstract domain seems infeasible at first sight due to the complexity of the formulas involved and the enormous size of sets of formulas (abstract elements). We introduce an efficient representation of abstract elements, which eliminates redundancies based on a novel syntactic subsumption relation that under-approximates semantic entailment. We develop algorithms and data-structures to efficiently compute the join of an abstract element with the abstraction of a concrete state, operating on the representation of abstract elements. To demonstrate feasibility of the domain, we use our data structures and algorithms to implement a symbolic abstraction algorithm that computes the least fixpoint of the best abstract transformer of a transition system, which corresponds to the strongest inductive invariant. We succeed at finding, for example, the least fixpoint for Paxos (which in our representation has 1,438 formulas with forall-exists-forall quantification) in time comparable to state-of-the-art property-directed approaches.","sentences":["This paper lays a practical foundation for using abstract interpretation with an abstract domain that consists of sets of quantified first-order logic formulas.","This abstract domain seems infeasible at first sight due to the complexity of the formulas involved and the enormous size of sets of formulas (abstract elements).","We introduce an efficient representation of abstract elements, which eliminates redundancies based on a novel syntactic subsumption relation that under-approximates semantic entailment.","We develop algorithms and data-structures to efficiently compute the join of an abstract element with the abstraction of a concrete state, operating on the representation of abstract elements.","To demonstrate feasibility of the domain, we use our data structures and algorithms to implement a symbolic abstraction algorithm that computes the least fixpoint of the best abstract transformer of a transition system, which corresponds to the strongest inductive invariant.","We succeed at finding, for example, the least fixpoint for Paxos (which in our representation has 1,438 formulas with forall-exists-forall quantification) in time comparable to state-of-the-art property-directed approaches."],"url":"http://arxiv.org/abs/2405.10308v1","category":"cs.LO"}
{"created":"2024-05-16 17:50:04","title":"On Sample Selection for Continual Learning: a Video Streaming Case Study","abstract":"Machine learning (ML) is a powerful tool to model the complexity of communication networks. As networks evolve, we cannot only train once and deploy. Retraining models, known as continual learning, is necessary. Yet, to date, there is no established methodology to answer the key questions: With which samples to retrain? When should we retrain?   We address these questions with the sample selection system Memento, which maintains a training set with the \"most useful\" samples to maximize sample space coverage. Memento particularly benefits rare patterns -- the notoriously long \"tail\" in networking -- and allows assessing rationally when retraining may help, i.e., when the coverage changes.   We deployed Memento on Puffer, the live-TV streaming project, and achieved a 14% reduction of stall time, 3.5x the improvement of random sample selection. Finally, Memento does not depend on a specific model architecture; it is likely to yield benefits in other ML-based networking applications.","sentences":["Machine learning (ML) is a powerful tool to model the complexity of communication networks.","As networks evolve, we cannot only train once and deploy.","Retraining models, known as continual learning, is necessary.","Yet, to date, there is no established methodology to answer the key questions: With which samples to retrain?","When should we retrain?   ","We address these questions with the sample selection system Memento, which maintains a training set with the \"most useful\" samples to maximize sample space coverage.","Memento particularly benefits rare patterns -- the notoriously long \"tail\" in networking -- and allows assessing rationally when retraining may help, i.e., when the coverage changes.   ","We deployed Memento on Puffer, the live-TV streaming project, and achieved a 14% reduction of stall time, 3.5x the improvement of random sample selection.","Finally, Memento does not depend on a specific model architecture; it is likely to yield benefits in other ML-based networking applications."],"url":"http://arxiv.org/abs/2405.10290v1","category":"cs.NI"}
{"created":"2024-05-16 17:37:01","title":"Evaluation of a Multi-Molecule Molecular Communication Testbed Based on Spectral Sensing","abstract":"This work presents a novel flow-based molecular communication (MC) testbed using spectral sensing and ink concentration estimation to enable real-time multi-molecule (MUMO) transmission. MUMO communication opens up crucial opportunities for increased throughput as well as implementing more complex coding, modulation, and resource allocation strategies for MC testbeds. A concentration estimator using non-invasive spectral sensing at the receiver is proposed based on a simple absorption model. We conduct in-depth channel impulse response (CIR) measurements and a preliminary communication performance evaluation. Additionally, a simple analytical model is used to check the consistency of the CIRs. The results indicate that by utilizing MUMO transmission, on-off-keying, and a simple difference detector, the testbed can achieve up to 3 bits per second for near-error-free communication, which is on par with comparable testbeds that utilize more sophisticated coding or detection methods. Our platform lays the ground for implementing MUMO communication and evaluating various physical layer and networking techniques based on multiple molecule types in future MC testbeds in real time.","sentences":["This work presents a novel flow-based molecular communication (MC) testbed using spectral sensing and ink concentration estimation to enable real-time multi-molecule (MUMO) transmission.","MUMO communication opens up crucial opportunities for increased throughput as well as implementing more complex coding, modulation, and resource allocation strategies for MC testbeds.","A concentration estimator using non-invasive spectral sensing at the receiver is proposed based on a simple absorption model.","We conduct in-depth channel impulse response (CIR) measurements and a preliminary communication performance evaluation.","Additionally, a simple analytical model is used to check the consistency of the CIRs.","The results indicate that by utilizing MUMO transmission, on-off-keying, and a simple difference detector, the testbed can achieve up to 3 bits per second for near-error-free communication, which is on par with comparable testbeds that utilize more sophisticated coding or detection methods.","Our platform lays the ground for implementing MUMO communication and evaluating various physical layer and networking techniques based on multiple molecule types in future MC testbeds in real time."],"url":"http://arxiv.org/abs/2405.10280v1","category":"cs.ET"}
{"created":"2024-05-16 17:35:28","title":"Locating the critical point for the hadron to quark-gluon plasma phase transition from finite-size scaling of proton cumulants in heavy-ion collisions","abstract":"We perform a finite-size scaling analysis of net-proton number cumulants in Au+Au collisions at center-of-mass energies between $\\sqrt{s_{\\rm{NN}}} = 2.4$ GeV and 54.4 GeV to search for evidence of a critical point in the QCD phase diagram. In our analysis, we use both susceptibility and Binder cumulants which we extract from the second and fourth moments of the net-proton number distributions. We take measurements in different rapidity bin widths, corresponding to different subvolumes of the system, as probes of different length scales. We use model simulations to verify the applicability of this approach, then apply it to data and find evidence for a critical point near the baryon chemical potential of $\\mu_{B} \\approx 625$ MeV and temperature of $T \\approx 140$ MeV. The Binder cumulants, also analyzed in varying rapidity bin widths, provide complementary evidence for a critical point in a similar region. This is the first analysis of experimental data to locate the critical point in a range consistent with theoretical predictions.","sentences":["We perform a finite-size scaling analysis of net-proton number cumulants in Au+Au collisions at center-of-mass energies between $\\sqrt{s_{\\rm{NN}}} = 2.4$ GeV and 54.4 GeV to search for evidence of a critical point in the QCD phase diagram.","In our analysis, we use both susceptibility and Binder cumulants which we extract from the second and fourth moments of the net-proton number distributions.","We take measurements in different rapidity bin widths, corresponding to different subvolumes of the system, as probes of different length scales.","We use model simulations to verify the applicability of this approach, then apply it to data and find evidence for a critical point near the baryon chemical potential of $\\mu_{B} \\approx 625$ MeV and temperature of $T \\approx 140$ MeV.","The Binder cumulants, also analyzed in varying rapidity bin widths, provide complementary evidence for a critical point in a similar region.","This is the first analysis of experimental data to locate the critical point in a range consistent with theoretical predictions."],"url":"http://arxiv.org/abs/2405.10278v1","category":"nucl-th"}
{"created":"2024-05-16 17:29:43","title":"A note on the equivalence of Gromov boundary and metric boundary","abstract":"In this paper, we introduce the concept of quasihyperbolically visible spaces. As a tool, we study the connection between the Gromov boundary and the metric boundary.","sentences":["In this paper, we introduce the concept of quasihyperbolically visible spaces.","As a tool, we study the connection between the Gromov boundary and the metric boundary."],"url":"http://arxiv.org/abs/2405.10273v1","category":"math.MG"}
{"created":"2024-05-16 17:25:07","title":"Direct magnetic imaging of fractional Chern insulators in twisted MoTe$_2$ with a superconducting sensor","abstract":"In the absence of time reversal symmetry, orbital magnetization provides a sensitive probe of topology and interactions, with particularly rich phenomenology in Chern insulators where topological edge states carry large equilibrium currents. Here, we use a nanoscale superconducting sensor to map the magnetic fringe fields in twisted bilayers of MoTe$_2$, where transport and optical sensing experiments have revealed the formation of fractional Chern insulator (FCI) states at zero magnetic field. At a temperature of 1.6K, we observe oscillations in the local magnetic field associated with fillings $\\nu=-1,-2/3,-3/5,-4/7$ and $-5/9$ of the first moir\\'e hole band, consistent with the formation of FCIs at these fillings. By quantitatively reconstructing the magnetization, we determine the local thermodynamic gaps of the most robust FCI state at $\\nu=-2/3$, finding $^{-2/3}\\Delta$ as large as 7 meV. Spatial mapping of the charge density- and displacement field-tuned magnetic phase diagram further allows us to characterize sample disorder, which we find to be dominated by both inhomogeneity in the effective unit cell area as well as inhomogeneity in the band edge offset and bound dipole moment. Our results highlight both the challenges posed by structural disorder in the study of twisted homobilayer moir\\'e systems and the opportunities afforded by the remarkably robust nature of the underlying correlated topological states.","sentences":["In the absence of time reversal symmetry, orbital magnetization provides a sensitive probe of topology and interactions, with particularly rich phenomenology in Chern insulators where topological edge states carry large equilibrium currents.","Here, we use a nanoscale superconducting sensor to map the magnetic fringe fields in twisted bilayers of MoTe$_2$, where transport and optical sensing experiments have revealed the formation of fractional Chern insulator (FCI) states at zero magnetic field.","At a temperature of 1.6K, we observe oscillations in the local magnetic field associated with fillings $\\nu=-1,-2/3,-3/5,-4/7$ and $-5/9$ of the first moir\\'e hole band, consistent with the formation of FCIs at these fillings.","By quantitatively reconstructing the magnetization, we determine the local thermodynamic gaps of the most robust FCI state at $\\nu=-2/3$, finding $^{-2/3}\\Delta$ as large as 7 meV. Spatial mapping of the charge density- and displacement field-tuned magnetic phase diagram further allows us to characterize sample disorder, which we find to be dominated by both inhomogeneity in the effective unit cell area as well as inhomogeneity in the band edge offset and bound dipole moment.","Our results highlight both the challenges posed by structural disorder in the study of twisted homobilayer moir\\'e systems and the opportunities afforded by the remarkably robust nature of the underlying correlated topological states."],"url":"http://arxiv.org/abs/2405.10269v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-16 17:05:03","title":"The 3He(\\vec n,p)3H parity-conserving asymmetry","abstract":"Recently, the n$^3$He collaboration reported a measurement of the parity-violating (PV) proton directional asymmetry $A_{\\mathrm {PV}} = (1.55\\pm 0.97~\\mathrm {(st\\ at)} \\pm 0.24~\\mathrm {(sys)})\\times 10^{-8}$ in the capture reaction of ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H at meV incident neutron energies. The result increased the limited inventory of precisely measured and calculable PV observables in few-body systems required to further understand the structure of hadronic weak interaction. In this letter, we report the experimental and theoretical investigation of a parity conserving (PC) asymmetry $A_{\\mathrm {PC}}$ in the same reaction (the first ever measured PC observable at meV neutron energies). As a result of S- and P-wave mixing in the reaction, the $A_{\\mathrm {PC}}$ is inversely proportional to the neutron wavelength $\\lambda$. The experimental value is $(\\lambda\\times A_{\\mathrm {PC}})\\equiv\\beta= (-1.97 \\pm 0.28~\\mathrm{(stat)}\\pm 0.12~\\mathrm{(sys)}) \\times 10^{-6}$ Amstrongs. We present results for a theoretical analysis of this reaction by solving the four-body scattering problem within the hyperspherical harmonic method. We find that in the ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H reaction, $A_{\\mathrm {PC}}$ depends critically on the energy and width of the close $0^-$ resonant state of ${}^4$He, resulting in a large sensitivity to the spin-orbit components of the nucleon-nucleon force and even to the three-nucleon force. The analysis of the accurately measured $A_{\\mathrm {PC}}$ and $A_{\\mathrm {PV}}$ using the same few-body theoretical models gives essential information needed to interpret the PV asymmetry in the ${}^3$He$(\\vec {n}, {\\mathrm p}){}^3$H reaction.","sentences":["Recently, the n$^3$He collaboration reported a measurement of the parity-violating (PV) proton directional asymmetry $A_{\\mathrm {PV}} = (1.55\\pm 0.97~\\mathrm {(st\\ at)} \\pm 0.24~\\mathrm {(sys)})\\times 10^{-8}$ in the capture reaction of ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H at meV incident neutron energies.","The result increased the limited inventory of precisely measured and calculable PV observables in few-body systems required to further understand the structure of hadronic weak interaction.","In this letter, we report the experimental and theoretical investigation of a parity conserving (PC) asymmetry $A_{\\mathrm {PC}}$ in the same reaction (the first ever measured PC observable at meV neutron energies).","As a result of S- and P-wave mixing in the reaction, the $A_{\\mathrm {PC}}$ is inversely proportional to the neutron wavelength $\\lambda$.","The experimental value is $(\\lambda\\times A_{\\mathrm {PC}})\\equiv\\beta= (-1.97 \\pm 0.28~\\mathrm{(stat)}\\pm 0.12~\\mathrm{(sys)})","\\times 10^{-6}$","Amstrongs.","We present results for a theoretical analysis of this reaction by solving the four-body scattering problem within the hyperspherical harmonic method.","We find that in the ${}^3$He$(\\vec {n},{\\mathrm p}){}^3$H reaction, $A_{\\mathrm {PC}}$ depends critically on the energy and width of the close $0^-$ resonant state of ${}^4$He, resulting in a large sensitivity to the spin-orbit components of the nucleon-nucleon force and even to the three-nucleon force.","The analysis of the accurately measured $A_{\\mathrm {PC}}$ and $A_{\\mathrm {PV}}$ using the same few-body theoretical models gives essential information needed to interpret the PV asymmetry in the ${}^3$He$(\\vec {n}, {\\mathrm p}){}^3$H reaction."],"url":"http://arxiv.org/abs/2405.10258v1","category":"nucl-ex"}
{"created":"2024-05-16 16:58:14","title":"Adaptive Quotient Filters","abstract":"Adaptive filters, such as telescoping and adaptive cuckoo filters, update their representation upon detecting a false positive to avoid repeating the same error in the future. Adaptive filters require an auxiliary structure, typically much larger than the main filter and often residing on slow storage, to facilitate adaptation. However, existing adaptive filters are not practical and have seen no adoption in real-world systems due to two main reasons. Firstly, they offer weak adaptivity guarantees, meaning that fixing a new false positive can cause a previously fixed false positive to come back. Secondly, the sub-optimal design of the auxiliary structure results in adaptivity overheads so substantial that they can actually diminish the overall system performance compared to a traditional filter.   In this paper, we design and implement AdaptiveQF, the first practical adaptive filter with minimal adaptivity overhead and strong adaptivity guarantees, which means that the performance and false-positive guarantees continue to hold even for adversarial workloads. The AdaptiveQF is based on the state-of-the-art quotient filter design and preserves all the critical features of the quotient filter such as cache efficiency and mergeability. Furthermore, we employ a new auxiliary structure design which results in considerably low adaptivity overhead and makes the AdaptiveQF practical in real systems.","sentences":["Adaptive filters, such as telescoping and adaptive cuckoo filters, update their representation upon detecting a false positive to avoid repeating the same error in the future.","Adaptive filters require an auxiliary structure, typically much larger than the main filter and often residing on slow storage, to facilitate adaptation.","However, existing adaptive filters are not practical and have seen no adoption in real-world systems due to two main reasons.","Firstly, they offer weak adaptivity guarantees, meaning that fixing a new false positive can cause a previously fixed false positive to come back.","Secondly, the sub-optimal design of the auxiliary structure results in adaptivity overheads so substantial that they can actually diminish the overall system performance compared to a traditional filter.   ","In this paper, we design and implement AdaptiveQF, the first practical adaptive filter with minimal adaptivity overhead and strong adaptivity guarantees, which means that the performance and false-positive guarantees continue to hold even for adversarial workloads.","The AdaptiveQF is based on the state-of-the-art quotient filter design and preserves all the critical features of the quotient filter such as cache efficiency and mergeability.","Furthermore, we employ a new auxiliary structure design which results in considerably low adaptivity overhead and makes the AdaptiveQF practical in real systems."],"url":"http://arxiv.org/abs/2405.10253v1","category":"cs.DS"}
{"created":"2024-05-16 16:57:05","title":"Bass Note Spectra of Binary Forms","abstract":"We show that the spectrum of every $\\mathbb{R}-$isotropic homogeneous binary form $P$ of degree $n\\geq3$ is an interval of the form $[0,M_P],$ where $M_P$ is some positive constant. This completes the discussion around a conjecture of Mordell from 1940 (disproved by Davenport) regarding the existence of spectral gaps for binary cubic forms and further settles Mahler's program for binary forms of every degree.","sentences":["We show that the spectrum of every $\\mathbb{R}-$isotropic homogeneous binary form $P$ of degree $n\\geq3$ is an interval of the form $[0,M_P],$ where $M_P$ is some positive constant.","This completes the discussion around a conjecture of Mordell from 1940 (disproved by Davenport) regarding the existence of spectral gaps for binary cubic forms and further settles Mahler's program for binary forms of every degree."],"url":"http://arxiv.org/abs/2405.10252v1","category":"math.NT"}
{"created":"2024-05-16 16:46:27","title":"Quantum State Learning Implies Circuit Lower Bounds","abstract":"We establish connections between state tomography, pseudorandomness, quantum state synthesis, and circuit lower bounds. In particular, let $\\mathfrak{C}$ be a family of non-uniform quantum circuits of polynomial size and suppose that there exists an algorithm that, given copies of $|\\psi \\rangle$, distinguishes whether $|\\psi \\rangle$ is produced by $\\mathfrak{C}$ or is Haar random, promised one of these is the case. For arbitrary fixed constant $c$, we show that if the algorithm uses at most $O(2^{n^c})$ time and $2^{n^{0.99}}$ samples then $\\mathsf{stateBQE} \\not\\subset \\mathsf{state}\\mathfrak{C}$. Here $\\mathsf{stateBQE} := \\mathsf{stateBQTIME}[2^{O(n)}]$ and $\\mathsf{state}\\mathfrak{C}$ are state synthesis complexity classes as introduced by Rosenthal and Yuen (ITCS 2022), which capture problems with classical inputs but quantum output. Note that efficient tomography implies a similarly efficient distinguishing algorithm against Haar random states, even for nearly exponential-time algorithms. Because every state produced by a polynomial-size circuit can be learned with $2^{O(n)}$ samples and time, or $O(n^{\\omega(1)})$ samples and $2^{O(n^{\\omega(1)})}$ time, we show that even slightly non-trivial quantum state tomography algorithms would lead to new statements about quantum state synthesis. Finally, a slight modification of our proof shows that distinguishing algorithms for quantum states can imply circuit lower bounds for decision problems as well. This help sheds light on why time-efficient tomography algorithms for non-uniform quantum circuit classes has only had limited and partial progress. Our work parallels results by Arunachalam et al. (FOCS 2021) that revealed a similar connection between quantum learning of Boolean functions and circuit lower bounds for classical circuit classes, but modified for the purposes of state tomography and state synthesis.","sentences":["We establish connections between state tomography, pseudorandomness, quantum state synthesis, and circuit lower bounds.","In particular, let $\\mathfrak{C}$ be a family of non-uniform quantum circuits of polynomial size and suppose that there exists an algorithm that, given copies of $|\\psi \\rangle$, distinguishes whether $|\\psi \\rangle$ is produced by $\\mathfrak{C}$ or is Haar random, promised one of these is the case.","For arbitrary fixed constant $c$, we show that if the algorithm uses at most $O(2^{n^c})$ time and $2^{n^{0.99}}$ samples then $\\mathsf{stateBQE} \\not\\subset \\mathsf{state}\\mathfrak{C}$. Here $\\mathsf{stateBQE} := \\mathsf{stateBQTIME}[2^{O(n)}]$ and $\\mathsf{state}\\mathfrak{C}$ are state synthesis complexity classes as introduced by Rosenthal and Yuen (ITCS 2022), which capture problems with classical inputs but quantum output.","Note that efficient tomography implies a similarly efficient distinguishing algorithm against Haar random states, even for nearly exponential-time algorithms.","Because every state produced by a polynomial-size circuit can be learned with $2^{O(n)}$ samples and time, or $O(n^{\\omega(1)})$ samples and $2^{O(n^{\\omega(1)})}$ time, we show that even slightly non-trivial quantum state tomography algorithms would lead to new statements about quantum state synthesis.","Finally, a slight modification of our proof shows that distinguishing algorithms for quantum states can imply circuit lower bounds for decision problems as well.","This help sheds light on why time-efficient tomography algorithms for non-uniform quantum circuit classes has only had limited and partial progress.","Our work parallels results by Arunachalam et al.","(FOCS 2021) that revealed a similar connection between quantum learning of Boolean functions and circuit lower bounds for classical circuit classes, but modified for the purposes of state tomography and state synthesis."],"url":"http://arxiv.org/abs/2405.10242v1","category":"quant-ph"}
{"created":"2024-05-16 16:38:56","title":"Rounding Large Independent Sets on Expanders","abstract":"We develop a new approach for approximating large independent sets when the input graph is a one-sided spectral expander - that is, the uniform random walk matrix of the graph has the second eigenvalue bounded away from 1. Consequently, we obtain a polynomial time algorithm to find linear-sized independent sets in one-sided expanders that are almost $3$-colorable or are promised to contain an independent set of size $(1/2-\\epsilon)n$. Our second result above can be refined to require only a weaker vertex expansion property with an efficient certificate. Somewhat surprisingly, we observe that the analogous task of finding a linear-sized independent set in almost $4$-colorable one-sided expanders (even when the second eigenvalue is $o_n(1)$) is NP-hard, assuming the Unique Games Conjecture.   All prior algorithms that beat the worst-case guarantees for this problem rely on bottom eigenspace enumeration techniques (following the classical spectral methods of Alon and Kahale) and require two-sided expansion, meaning a bounded number of negative eigenvalues of magnitude $\\Omega(1)$. Such techniques naturally extend to almost $k$-colorable graphs for any constant $k$, in contrast to analogous guarantees on one-sided expanders, which are Unique Games-hard to achieve for $k \\geq 4$.   Our rounding builds on the method of simulating multiple samples from a pseudodistribution introduced by Barak et. al. for rounding Unique Games instances. The key to our analysis is a new clustering property of large independent sets in expanding graphs - every large independent set has a larger-than-expected intersection with some member of a small list - and its formalization in the low-degree sum-of-squares proof system.","sentences":["We develop a new approach for approximating large independent sets when the input graph is a one-sided spectral expander - that is, the uniform random walk matrix of the graph has the second eigenvalue bounded away from 1.","Consequently, we obtain a polynomial time algorithm to find linear-sized independent sets in one-sided expanders that are almost $3$-colorable or are promised to contain an independent set of size $(1/2-\\epsilon)n$. Our second result above can be refined to require only a weaker vertex expansion property with an efficient certificate.","Somewhat surprisingly, we observe that the analogous task of finding a linear-sized independent set in almost $4$-colorable one-sided expanders (even when the second eigenvalue is $o_n(1)$) is NP-hard, assuming the Unique Games Conjecture.   ","All prior algorithms that beat the worst-case guarantees for this problem rely on bottom eigenspace enumeration techniques (following the classical spectral methods of Alon and Kahale) and require two-sided expansion, meaning a bounded number of negative eigenvalues of magnitude $\\Omega(1)$. Such techniques naturally extend to almost $k$-colorable graphs for any constant $k$, in contrast to analogous guarantees on one-sided expanders, which are Unique Games-hard to achieve for $k \\geq 4$.   ","Our rounding builds on the method of simulating multiple samples from a pseudodistribution introduced by Barak et.","al. for rounding Unique Games instances.","The key to our analysis is a new clustering property of large independent sets in expanding graphs - every large independent set has a larger-than-expected intersection with some member of a small list - and its formalization in the low-degree sum-of-squares proof system."],"url":"http://arxiv.org/abs/2405.10238v1","category":"cs.DS"}
{"created":"2024-05-16 16:37:23","title":"A systematic path to non-Markovian dynamics II: Probabilistic response of nonlinear multidimensional systems to Gaussian colored noise excitation","abstract":"The probabilistic characterization of non-Markovian responses to nonlinear dynamical systems under colored excitation is an important issue, arising in many applications. Extending the Fokker-Planck-Kolmogorov equation, governing the first-order response probability density function (pdf), to this case is a complicated task calling for special treatment. In this work, a new pdf-evolution equation is derived for the response of nonlinear dynamical systems under additive colored Gaussian noise. The derivation is based on the Stochastic Liouville equation (SLE), transformed, by means of an extended version of the Novikov-Furutsu theorem, to an exact yet non-closed equation, involving averages over the history of the functional derivatives of the non-Markovian response with respect to the excitation. The latter are calculated exactly by means of the state-transition matrix of variational, time-varying systems. Subsequently, an approximation scheme is implemented, relying on a decomposition of the state-transition matrix in its instantaneous mean value and its fluctuation around it. By a current-time approximation to the latter, we obtain our final equation, in which the effect of the instantaneous mean value of the response is maintained, rendering it nonlinear and non-local in time. Numerical results for the response pdf are provided for a bistable Duffing oscillator, under Gaussian excitation. The pdfs obtained from the solution of the novel equation and a simpler small correlation time (SCT) pdf-evolution equation are compared to Monde Carlo (MC) simulations. The novel equation outperforms the SCT equation as the excitation correlation time increases, keeping good agreement with the MC simulations.","sentences":["The probabilistic characterization of non-Markovian responses to nonlinear dynamical systems under colored excitation is an important issue, arising in many applications.","Extending the Fokker-Planck-Kolmogorov equation, governing the first-order response probability density function (pdf), to this case is a complicated task calling for special treatment.","In this work, a new pdf-evolution equation is derived for the response of nonlinear dynamical systems under additive colored Gaussian noise.","The derivation is based on the Stochastic Liouville equation (SLE), transformed, by means of an extended version of the Novikov-Furutsu theorem, to an exact yet non-closed equation, involving averages over the history of the functional derivatives of the non-Markovian response with respect to the excitation.","The latter are calculated exactly by means of the state-transition matrix of variational, time-varying systems.","Subsequently, an approximation scheme is implemented, relying on a decomposition of the state-transition matrix in its instantaneous mean value and its fluctuation around it.","By a current-time approximation to the latter, we obtain our final equation, in which the effect of the instantaneous mean value of the response is maintained, rendering it nonlinear and non-local in time.","Numerical results for the response pdf are provided for a bistable Duffing oscillator, under Gaussian excitation.","The pdfs obtained from the solution of the novel equation and a simpler small correlation time (SCT) pdf-evolution equation are compared to Monde Carlo (MC) simulations.","The novel equation outperforms the SCT equation as the excitation correlation time increases, keeping good agreement with the MC simulations."],"url":"http://arxiv.org/abs/2405.10236v1","category":"math-ph"}
{"created":"2024-05-16 17:59:00","title":"KiDS-1000 and DES-Y1 combined: Cosmology from peak count statistics","abstract":"We analyse the fourth data release of the Kilo Degree Survey (KiDS-1000) and extract cosmological parameter constraints based on the cosmic shear peak count statistics. Peaks are identified in aperture mass maps in which the filter is maximally sensitive to angular scales in the range 2-4arcmin, probing deep into the non-linear regime of structure formation. We interpret our results with a simulation-based inference pipeline, sampling over a broad $w$CDM prior volume and marginalising over uncertainties on shape calibration, photometric redshift distribution, intrinsic alignment and baryonic feedback. Our measurements constrain the structure growth parameter and the amplitude of the non-linear intrinsic alignment model to $\\Sigma_8 \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.60}=0.765^{+0.030}_{-0.030}$ and $A_{\\rm IA}= 0.71^{+0.42}_{-0.42}$, respectively, in agreement with previous KiDS-1000 results based on two-point shear statistics. These results are robust against modelling of the non-linear physics, different scale cuts and selections of tomographic bins. The posterior is also consistent with that from the Dark Energy Survey Year-1 peak count analysis presented in Harnois-D\\'eraps et al (2021), and hence we jointly analyse both surveys. We obtain $\\Sigma_8^{\\rm joint} \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.57}=0.759^{+0.020}_{-0.017}$, in agreement with the Planck $w$CDM results. The shear-CMB tension on this parameter increases to $3.1\\sigma$ when forcing $w=-1.0$, and to $4.1\\sigma$ if comparing instead with $S_{8,\\Lambda{\\rm CDM}}^{\\rm joint} = 0.736^{+0.016}_{-0.018}$, one of the tightest constraints to date on this quantity. (abridged)","sentences":["We analyse the fourth data release of the Kilo Degree Survey (KiDS-1000) and extract cosmological parameter constraints based on the cosmic shear peak count statistics.","Peaks are identified in aperture mass maps in which the filter is maximally sensitive to angular scales in the range 2-4arcmin, probing deep into the non-linear regime of structure formation.","We interpret our results with a simulation-based inference pipeline, sampling over a broad $w$CDM prior volume and marginalising over uncertainties on shape calibration, photometric redshift distribution, intrinsic alignment and baryonic feedback.","Our measurements constrain the structure growth parameter and the amplitude of the non-linear intrinsic alignment model to $\\Sigma_8 \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.60}=0.765^{+0.030}_{-0.030}$ and $A_{\\rm IA}= 0.71^{+0.42}_{-0.42}$, respectively, in agreement with previous KiDS-1000 results based on two-point shear statistics.","These results are robust against modelling of the non-linear physics, different scale cuts and selections of tomographic bins.","The posterior is also consistent with that from the Dark Energy Survey Year-1 peak count analysis presented in Harnois-D\\'eraps et al (2021), and hence we jointly analyse both surveys.","We obtain $\\Sigma_8^{\\rm joint} \\equiv \\sigma_8\\left[\\Omega_{\\rm m}/0.3\\right]^{0.57}=0.759^{+0.020}_{-0.017}$, in agreement with the Planck $w$CDM results.","The shear-CMB tension on this parameter increases to $3.1\\sigma$ when forcing $w=-1.0$, and to $4.1\\sigma$ if comparing instead with $S_{8,\\Lambda{\\rm CDM}}^{\\rm joint} = 0.736^{+0.016}_{-0.018}$, one of the tightest constraints to date on this quantity.","(abridged)"],"url":"http://arxiv.org/abs/2405.10312v1","category":"astro-ph.CO"}
{"created":"2024-05-16 17:52:59","title":"Heavy-element damage seeding in proteins under X-ray free electron laser illumination conditions","abstract":"The emerging technique of serial femtosecond X-ray crystallography (SFX) can be used to study the structure and dynamics of biological macromolecules to high spatial and temporal resolutions. An ongoing challenge for SFX is the damage caused by the ultrabright X-ray free electron laser pulse. Though it is often assumed that sufficiently femtosecond pulses `outrun' radiation damage, in reality electronic damage processes commence during exposure. We model the electronic damage to protein nanocrystals using a plasma model that tracks the continuous changes to the energy distribution of the unbound electrons. Tracking the continuous energy distribution is of particular importance for distinguishing the influence of differing elements on secondary damage processes. Heavy atoms have a ubiquitous but small presence in protein targets - typically as integral components of the macromolecule and as salts in the solvent. We find that these atoms considerably influence the simulated ionization and scattering behavior of realistic targets due to their rapid seeding of secondary ionization processes. In lysozyme, even the presence of native sulfur atoms significantly contributes to theoretical measures of damage-induced noise for >= 6 keV, 15 fs pulses. Contributing to the effect is that heavy atoms seed `intermediate' energy electron cascades that are particularly effective at ionizing the target on the femtosecond timescale. In addition, the disproportionate effect of heavy atoms means the damage to a protein crystal can be sensitive to their presence in the solvent. Outside of reducing the concentration of heavy atoms in the target, these results suggest the dose limits of SFX targets will be higher where the ionization of deep >~ 6 keV absorption edges is minimized, or, to a lesser extent, when such edges are only ionized with X-rays >> 2 keV above their binding energy.","sentences":["The emerging technique of serial femtosecond X-ray crystallography (SFX) can be used to study the structure and dynamics of biological macromolecules to high spatial and temporal resolutions.","An ongoing challenge for SFX is the damage caused by the ultrabright X-ray free electron laser pulse.","Though it is often assumed that sufficiently femtosecond pulses `outrun' radiation damage, in reality electronic damage processes commence during exposure.","We model the electronic damage to protein nanocrystals using a plasma model that tracks the continuous changes to the energy distribution of the unbound electrons.","Tracking the continuous energy distribution is of particular importance for distinguishing the influence of differing elements on secondary damage processes.","Heavy atoms have a ubiquitous but small presence in protein targets - typically as integral components of the macromolecule and as salts in the solvent.","We find that these atoms considerably influence the simulated ionization and scattering behavior of realistic targets due to their rapid seeding of secondary ionization processes.","In lysozyme, even the presence of native sulfur atoms significantly contributes to theoretical measures of damage-induced noise for >= 6 keV, 15 fs pulses.","Contributing to the effect is that heavy atoms seed `intermediate' energy electron cascades that are particularly effective at ionizing the target on the femtosecond timescale.","In addition, the disproportionate effect of heavy atoms means the damage to a protein crystal can be sensitive to their presence in the solvent.","Outside of reducing the concentration of heavy atoms in the target, these results suggest the dose limits of SFX targets will be higher where the ionization of deep >~ 6 keV absorption edges is minimized, or, to a lesser extent, when such edges are only ionized with X-rays >> 2 keV above their binding energy."],"url":"http://arxiv.org/abs/2405.10298v1","category":"physics.plasm-ph"}
{"created":"2024-05-16 17:41:53","title":"Power-law relaxation of a confined diffusing particle subject to resetting with memory","abstract":"We study the relaxation of a Brownian particle with long range memory under confinement in one dimension. The particle diffuses in an arbitrary confining potential and resets at random times to previously visited positions, chosen with a probability proportional to the local time spend there by the particle since the initial time. This model mimics an animal which moves erratically in its home range and returns preferentially to familiar places from time to time. The steady state density of the position is given by the equilibrium Boltzmann-Gibbs distribution, as in standard diffusion, while the transient part of the density can be obtained through a mapping of the Fokker-Planck equation of the process to a Schr\\\"odinger eigenvalue problem. Due to memory, the approach at large time toward the steady state is critically self-organised, in the sense that it always follows a sluggish power-law form, in contrast to the exponential decay that characterises Markov processes. The exponent of this power-law depends in a simple way on the resetting rate and on the relaxation rate of the Brownian particle in the absence of resetting. We apply these findings to several exactly solvable examples, such as the harmonic, V-shaped and box potentials.","sentences":["We study the relaxation of a Brownian particle with long range memory under confinement in one dimension.","The particle diffuses in an arbitrary confining potential and resets at random times to previously visited positions, chosen with a probability proportional to the local time spend there by the particle since the initial time.","This model mimics an animal which moves erratically in its home range and returns preferentially to familiar places from time to time.","The steady state density of the position is given by the equilibrium Boltzmann-Gibbs distribution, as in standard diffusion, while the transient part of the density can be obtained through a mapping of the Fokker-Planck equation of the process to a Schr\\\"odinger eigenvalue problem.","Due to memory, the approach at large time toward the steady state is critically self-organised, in the sense that it always follows a sluggish power-law form, in contrast to the exponential decay that characterises Markov processes.","The exponent of this power-law depends in a simple way on the resetting rate and on the relaxation rate of the Brownian particle in the absence of resetting.","We apply these findings to several exactly solvable examples, such as the harmonic, V-shaped and box potentials."],"url":"http://arxiv.org/abs/2405.10283v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-16 17:02:23","title":"Biasing & Debiasing based Approach Towards Fair Knowledge Transfer for Equitable Skin Analysis","abstract":"Deep learning models, particularly Convolutional Neural Networks (CNNs), have demonstrated exceptional performance in diagnosing skin diseases, often outperforming dermatologists. However, they have also unveiled biases linked to specific demographic traits, notably concerning diverse skin tones or gender, prompting concerns regarding fairness and limiting their widespread deployment. Researchers are actively working to ensure fairness in AI-based solutions, but existing methods incur an accuracy loss when striving for fairness. To solve this issue, we propose a `two-biased teachers' (i.e., biased on different sensitive attributes) based approach to transfer fair knowledge into the student network. Our approach mitigates biases present in the student network without harming its predictive accuracy. In fact, in most cases, our approach improves the accuracy of the baseline model. To achieve this goal, we developed a weighted loss function comprising biasing and debiasing loss terms. We surpassed available state-of-the-art approaches to attain fairness and also improved the accuracy at the same time. The proposed approach has been evaluated and validated on two dermatology datasets using standard accuracy and fairness evaluation measures. We will make source code publicly available to foster reproducibility and future research.","sentences":["Deep learning models, particularly Convolutional Neural Networks (CNNs), have demonstrated exceptional performance in diagnosing skin diseases, often outperforming dermatologists.","However, they have also unveiled biases linked to specific demographic traits, notably concerning diverse skin tones or gender, prompting concerns regarding fairness and limiting their widespread deployment.","Researchers are actively working to ensure fairness in AI-based solutions, but existing methods incur an accuracy loss when striving for fairness.","To solve this issue, we propose a `two-biased teachers' (i.e., biased on different sensitive attributes) based approach to transfer fair knowledge into the student network.","Our approach mitigates biases present in the student network without harming its predictive accuracy.","In fact, in most cases, our approach improves the accuracy of the baseline model.","To achieve this goal, we developed a weighted loss function comprising biasing and debiasing loss terms.","We surpassed available state-of-the-art approaches to attain fairness and also improved the accuracy at the same time.","The proposed approach has been evaluated and validated on two dermatology datasets using standard accuracy and fairness evaluation measures.","We will make source code publicly available to foster reproducibility and future research."],"url":"http://arxiv.org/abs/2405.10256v1","category":"cs.CV"}
{"created":"2024-05-16 16:51:25","title":"Unifying Partial Synchrony","abstract":"The distributed computing literature considers multiple options for modeling communication. Most simply, communication is categorized as either synchronous or asynchronous. Synchronous communication assumes that messages get delivered within a publicly known timeframe and that parties' clocks are synchronized. Asynchronous communication, on the other hand, only assumes that messages get delivered eventually. A more nuanced approach, or a middle ground between the two extremes, is given by the partially synchronous model, which is arguably the most realistic option. This model comes in two commonly considered flavors:   (i) The Global Stabilization Time (GST) model: after an (unknown) amount of time, the network becomes synchronous. This captures scenarios where network issues are transient.   (ii) The Unknown Latency (UL) model: the network is, in fact, synchronous, but the message delay bound is unknown.   This work formally establishes that any time-agnostic property that can be achieved by a protocol in the UL model can also be achieved by a (possibly different) protocol in the GST model. By time-agnostic, we mean properties that can depend on the order in which events happen but not on time as measured by the parties. Most properties considered in distributed computing are time-agnostic. The converse was already known, even without the time-agnostic requirement, so our result shows that the two network conditions are, under one sensible assumption, equally demanding.","sentences":["The distributed computing literature considers multiple options for modeling communication.","Most simply, communication is categorized as either synchronous or asynchronous.","Synchronous communication assumes that messages get delivered within a publicly known timeframe and that parties' clocks are synchronized.","Asynchronous communication, on the other hand, only assumes that messages get delivered eventually.","A more nuanced approach, or a middle ground between the two extremes, is given by the partially synchronous model, which is arguably the most realistic option.","This model comes in two commonly considered flavors:   (i) The Global Stabilization Time (GST) model: after an (unknown) amount of time, the network becomes synchronous.","This captures scenarios where network issues are transient.   ","(ii) The Unknown Latency (UL) model: the network is, in fact, synchronous, but the message delay bound is unknown.   ","This work formally establishes that any time-agnostic property that can be achieved by a protocol in the UL model can also be achieved by a (possibly different) protocol in the GST model.","By time-agnostic, we mean properties that can depend on the order in which events happen but not on time as measured by the parties.","Most properties considered in distributed computing are time-agnostic.","The converse was already known, even without the time-agnostic requirement, so our result shows that the two network conditions are, under one sensible assumption, equally demanding."],"url":"http://arxiv.org/abs/2405.10249v1","category":"cs.DC"}
{"created":"2024-05-16 16:50:31","title":"Co-Matching: Towards Human-Machine Collaborative Legal Case Matching","abstract":"Recent efforts have aimed to improve AI machines in legal case matching by integrating legal domain knowledge. However, successful legal case matching requires the tacit knowledge of legal practitioners, which is difficult to verbalize and encode into machines. This emphasizes the crucial role of involving legal practitioners in high-stakes legal case matching. To address this, we propose a collaborative matching framework called Co-Matching, which encourages both the machine and the legal practitioner to participate in the matching process, integrating tacit knowledge. Unlike existing methods that rely solely on the machine, Co-Matching allows both the legal practitioner and the machine to determine key sentences and then combine them probabilistically. Co-Matching introduces a method called ProtoEM to estimate human decision uncertainty, facilitating the probabilistic combination. Experimental results demonstrate that Co-Matching consistently outperforms existing legal case matching methods, delivering significant performance improvements over human- and machine-based matching in isolation (on average, +5.51% and +8.71%, respectively). Further analysis shows that Co-Matching also ensures better human-machine collaboration effectiveness. Our study represents a pioneering effort in human-machine collaboration for the matching task, marking a milestone for future collaborative matching studies.","sentences":["Recent efforts have aimed to improve AI machines in legal case matching by integrating legal domain knowledge.","However, successful legal case matching requires the tacit knowledge of legal practitioners, which is difficult to verbalize and encode into machines.","This emphasizes the crucial role of involving legal practitioners in high-stakes legal case matching.","To address this, we propose a collaborative matching framework called Co-Matching, which encourages both the machine and the legal practitioner to participate in the matching process, integrating tacit knowledge.","Unlike existing methods that rely solely on the machine, Co-Matching allows both the legal practitioner and the machine to determine key sentences and then combine them probabilistically.","Co-Matching introduces a method called ProtoEM to estimate human decision uncertainty, facilitating the probabilistic combination.","Experimental results demonstrate that Co-Matching consistently outperforms existing legal case matching methods, delivering significant performance improvements over human- and machine-based matching in isolation (on average, +5.51% and +8.71%, respectively).","Further analysis shows that Co-Matching also ensures better human-machine collaboration effectiveness.","Our study represents a pioneering effort in human-machine collaboration for the matching task, marking a milestone for future collaborative matching studies."],"url":"http://arxiv.org/abs/2405.10248v1","category":"cs.HC"}
{"created":"2024-05-16 16:28:11","title":"Random ReLU Neural Networks as Non-Gaussian Processes","abstract":"We consider a large class of shallow neural networks with randomly initialized parameters and rectified linear unit activation functions. We prove that these random neural networks are well-defined non-Gaussian processes. As a by-product, we demonstrate that these networks are solutions to stochastic differential equations driven by impulsive white noise (combinations of random Dirac measures). These processes are parameterized by the law of the weights and biases as well as the density of activation thresholds in each bounded region of the input domain. We prove that these processes are isotropic and wide-sense self-similar with Hurst exponent $3/2$. We also derive a remarkably simple closed-form expression for their autocovariance function. Our results are fundamentally different from prior work in that we consider a non-asymptotic viewpoint: The number of neurons in each bounded region of the input domain (i.e., the width) is itself a random variable with a Poisson law with mean proportional to the density parameter. Finally, we show that, under suitable hypotheses, as the expected width tends to infinity, these processes can converge in law not only to Gaussian processes, but also to non-Gaussian processes depending on the law of the weights. Our asymptotic results provide a new take on several classical results (wide networks converge to Gaussian processes) as well as some new ones (wide networks can converge to non-Gaussian processes).","sentences":["We consider a large class of shallow neural networks with randomly initialized parameters and rectified linear unit activation functions.","We prove that these random neural networks are well-defined non-Gaussian processes.","As a by-product, we demonstrate that these networks are solutions to stochastic differential equations driven by impulsive white noise (combinations of random Dirac measures).","These processes are parameterized by the law of the weights and biases as well as the density of activation thresholds in each bounded region of the input domain.","We prove that these processes are isotropic and wide-sense self-similar with Hurst exponent $3/2$. We also derive a remarkably simple closed-form expression for their autocovariance function.","Our results are fundamentally different from prior work in that we consider a non-asymptotic viewpoint: The number of neurons in each bounded region of the input domain (i.e., the width) is itself a random variable with a Poisson law with mean proportional to the density parameter.","Finally, we show that, under suitable hypotheses, as the expected width tends to infinity, these processes can converge in law not only to Gaussian processes, but also to non-Gaussian processes depending on the law of the weights.","Our asymptotic results provide a new take on several classical results (wide networks converge to Gaussian processes) as well as some new ones (wide networks can converge to non-Gaussian processes)."],"url":"http://arxiv.org/abs/2405.10229v1","category":"stat.ML"}
{"created":"2024-05-16 17:16:47","title":"Production of electroweak gauge bosons at forward rapidities in the color - dipole $S$ - matrix framework","abstract":"The cross-section for the production of an electroweak gauge boson ($G = W^{\\pm}, Z^0, \\gamma$) at forward rapidities in $pp$ collisions is derived within the color - dipole $S$ - matrix framework. We present the full expressions for the differential cross-section of the $q p \\rightarrow G X$ process in the impact parameter and transverse momentum spaces, considering the longitudinal and transverse polarizations of the gauge boson. The particular cases associated with the Drell - Yan process and real photon production are discussed. We demonstrate that the final formulae are expressed in terms of the dipole - proton cross-section or the unintegrated gluon distribution, and can be used to estimate the impact of the saturation effects in the gauge boson production at the LHC and future colliders.","sentences":["The cross-section for the production of an electroweak gauge boson ($G = W^{\\pm}, Z^0, \\gamma$) at forward rapidities in $pp$ collisions is derived within the color - dipole $S$ - matrix framework.","We present the full expressions for the differential cross-section of the $q p \\rightarrow G X$ process in the impact parameter and transverse momentum spaces, considering the longitudinal and transverse polarizations of the gauge boson.","The particular cases associated with the Drell - Yan process and real photon production are discussed.","We demonstrate that the final formulae are expressed in terms of the dipole - proton cross-section or the unintegrated gluon distribution, and can be used to estimate the impact of the saturation effects in the gauge boson production at the LHC and future colliders."],"url":"http://arxiv.org/abs/2405.10265v1","category":"hep-ph"}
{"created":"2024-05-16 16:40:14","title":"Pentagon equations, Vorono\u00ef tilings and pure braid groups invariant","abstract":"In the present paper, we construct $(2n-4)\\times (2n-4)$ matrices corresponding to the motion of points on standard round sphere from the point of view of Delaunay triangulations. We define homomorphism from spherical pure braids on $n$ strands to the product of these matrices for $n>5$.","sentences":["In the present paper, we construct $(2n-4)\\times (2n-4)$ matrices corresponding to the motion of points on standard round sphere from the point of view of Delaunay triangulations.","We define homomorphism from spherical pure braids on $n$ strands to the product of these matrices for $n>5$."],"url":"http://arxiv.org/abs/2405.10240v1","category":"math.AT"}
{"created":"2024-05-16 17:49:46","title":"Subgradient Convergence Implies Subdifferential Convergence on Weakly Convex Functions: With Uniform Rates Guarantees","abstract":"In nonsmooth, nonconvex stochastic optimization, understanding the uniform convergence of subdifferential mappings is crucial for analyzing stationary points of sample average approximations of risk as they approach the population risk. Yet, characterizing this convergence remains a fundamental challenge.   This work introduces a novel perspective by connecting the uniform convergence of subdifferential mappings to that of subgradient mappings as empirical risk converges to the population risk. We prove that, for stochastic weakly-convex objectives, and within any open set, a uniform bound on the convergence of subgradients -- chosen arbitrarily from the corresponding subdifferential sets -- translates to a uniform bound on the convergence of the subdifferential sets itself, measured by the Hausdorff metric.   Using this technique, we derive uniform convergence rates for subdifferential sets of stochastic convex-composite objectives. Our results do not rely on key distributional assumptions in the literature, which require the population and finite sample subdifferentials to be continuous in the Hausdorff metric, yet still provide tight convergence rates. These guarantees lead to new insights into the nonsmooth landscapes of such objectives within finite samples.","sentences":["In nonsmooth, nonconvex stochastic optimization, understanding the uniform convergence of subdifferential mappings is crucial for analyzing stationary points of sample average approximations of risk as they approach the population risk.","Yet, characterizing this convergence remains a fundamental challenge.   ","This work introduces a novel perspective by connecting the uniform convergence of subdifferential mappings to that of subgradient mappings as empirical risk converges to the population risk.","We prove that, for stochastic weakly-convex objectives, and within any open set, a uniform bound on the convergence of subgradients -- chosen arbitrarily from the corresponding subdifferential sets -- translates to a uniform bound on the convergence of the subdifferential sets itself, measured by the Hausdorff metric.   ","Using this technique, we derive uniform convergence rates for subdifferential sets of stochastic convex-composite objectives.","Our results do not rely on key distributional assumptions in the literature, which require the population and finite sample subdifferentials to be continuous in the Hausdorff metric, yet still provide tight convergence rates.","These guarantees lead to new insights into the nonsmooth landscapes of such objectives within finite samples."],"url":"http://arxiv.org/abs/2405.10289v1","category":"math.OC"}
{"created":"2024-05-16 17:45:54","title":"Quantum Vision Transformers for Quark-Gluon Classification","abstract":"We introduce a hybrid quantum-classical vision transformer architecture, notable for its integration of variational quantum circuits within both the attention mechanism and the multi-layer perceptrons. The research addresses the critical challenge of computational efficiency and resource constraints in analyzing data from the upcoming High Luminosity Large Hadron Collider, presenting the architecture as a potential solution. In particular, we evaluate our method by applying the model to multi-detector jet images from CMS Open Data. The goal is to distinguish quark-initiated from gluon-initiated jets. We successfully train the quantum model and evaluate it via numerical simulations. Using this approach, we achieve classification performance almost on par with the one obtained with the completely classical architecture, considering a similar number of parameters.","sentences":["We introduce a hybrid quantum-classical vision transformer architecture, notable for its integration of variational quantum circuits within both the attention mechanism and the multi-layer perceptrons.","The research addresses the critical challenge of computational efficiency and resource constraints in analyzing data from the upcoming High Luminosity Large Hadron Collider, presenting the architecture as a potential solution.","In particular, we evaluate our method by applying the model to multi-detector jet images from CMS Open Data.","The goal is to distinguish quark-initiated from gluon-initiated jets.","We successfully train the quantum model and evaluate it via numerical simulations.","Using this approach, we achieve classification performance almost on par with the one obtained with the completely classical architecture, considering a similar number of parameters."],"url":"http://arxiv.org/abs/2405.10284v1","category":"quant-ph"}
{"created":"2024-05-16 16:49:31","title":"Alternative ranking measures to predict international football results","abstract":"Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 World Cup and for the 2024 Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.","sentences":["Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities.","In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 World Cup and for the 2024 Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams.","More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football.","The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs."],"url":"http://arxiv.org/abs/2405.10247v1","category":"stat.AP"}
{"created":"2024-05-16 16:47:46","title":"Towards Task-Compatible Compressible Representations","abstract":"We identify an issue in multi-task learnable compression, in which a representation learned for one task does not positively contribute to the rate-distortion performance of a different task as much as expected, given the estimated amount of information available in it. We interpret this issue using the predictive $\\mathcal{V}$-information framework. In learnable scalable coding, previous work increased the utilization of side-information for input reconstruction by also rewarding input reconstruction when learning this shared representation. We evaluate the impact of this idea in the context of input reconstruction more rigorously and extended it to other computer vision tasks. We perform experiments using representations trained for object detection on COCO 2017 and depth estimation on the Cityscapes dataset, and use them to assist in image reconstruction and semantic segmentation tasks. The results show considerable improvements in the rate-distortion performance of the assisted tasks. Moreover, using the proposed representations, the performance of the base tasks are also improved. Results suggest that the proposed method induces simpler representations that are more compatible with downstream processes.","sentences":["We identify an issue in multi-task learnable compression, in which a representation learned for one task does not positively contribute to the rate-distortion performance of a different task as much as expected, given the estimated amount of information available in it.","We interpret this issue using the predictive $\\mathcal{V}$-information framework.","In learnable scalable coding, previous work increased the utilization of side-information for input reconstruction by also rewarding input reconstruction when learning this shared representation.","We evaluate the impact of this idea in the context of input reconstruction more rigorously and extended it to other computer vision tasks.","We perform experiments using representations trained for object detection on COCO 2017 and depth estimation on the Cityscapes dataset, and use them to assist in image reconstruction and semantic segmentation tasks.","The results show considerable improvements in the rate-distortion performance of the assisted tasks.","Moreover, using the proposed representations, the performance of the base tasks are also improved.","Results suggest that the proposed method induces simpler representations that are more compatible with downstream processes."],"url":"http://arxiv.org/abs/2405.10244v1","category":"cs.CV"}
{"created":"2024-05-16 17:57:15","title":"Fault Tolerance Embedded in a Quantum-Gap-Estimation Algorithm with Trial-State Optimization","abstract":"We construct a hybrid quantum algorithm to estimate gaps in many-body energy spectra and prove that it is inherently fault-tolerant to global multi-qubit depolarizing noise. Using trial-state optimization without active error correction, we show that the spectral peak of an exact target gap can be amplified beyond the noise threshold, thereby reducing gap-estimate error. We numerically verify fault tolerance using the Qiskit Aer simulator with a model of common mid-circuit noise channels. Our results reveal the potential for accurate quantum simulations on near-term noisy quantum computers.","sentences":["We construct a hybrid quantum algorithm to estimate gaps in many-body energy spectra and prove that it is inherently fault-tolerant to global multi-qubit depolarizing noise.","Using trial-state optimization without active error correction, we show that the spectral peak of an exact target gap can be amplified beyond the noise threshold, thereby reducing gap-estimate error.","We numerically verify fault tolerance using the Qiskit Aer simulator with a model of common mid-circuit noise channels.","Our results reveal the potential for accurate quantum simulations on near-term noisy quantum computers."],"url":"http://arxiv.org/abs/2405.10306v1","category":"quant-ph"}
{"created":"2024-05-16 17:33:50","title":"Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers","abstract":"Numerous recent works aim to enhance the efficacy of Large Language Models (LLMs) through strategic prompting. In particular, the Optimization by PROmpting (OPRO) approach provides state-of-the-art performance by leveraging LLMs as optimizers where the optimization task is to find instructions that maximize the task accuracy. In this paper, we revisit OPRO for automated prompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral 7B. Our investigation reveals that OPRO shows limited effectiveness in small-scale LLMs, with limited inference capabilities constraining optimization ability. We suggest future automatic prompting engineering to consider both model capabilities and computational costs. Additionally, for small-scale LLMs, we recommend direct instructions that clearly outline objectives and methodologies as robust prompt baselines, ensuring efficient and effective prompt engineering in ongoing research.","sentences":["Numerous recent works aim to enhance the efficacy of Large Language Models (LLMs) through strategic prompting.","In particular, the Optimization by PROmpting (OPRO) approach provides state-of-the-art performance by leveraging LLMs as optimizers where the optimization task is to find instructions that maximize the task accuracy.","In this paper, we revisit OPRO for automated prompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral 7B.","Our investigation reveals that OPRO shows limited effectiveness in small-scale LLMs, with limited inference capabilities constraining optimization ability.","We suggest future automatic prompting engineering to consider both model capabilities and computational costs.","Additionally, for small-scale LLMs, we recommend direct instructions that clearly outline objectives and methodologies as robust prompt baselines, ensuring efficient and effective prompt engineering in ongoing research."],"url":"http://arxiv.org/abs/2405.10276v1","category":"cs.CL"}
{"created":"2024-05-16 17:02:43","title":"End-to-End Optimization of Directly Modulated Laser Links using Chirp-Aware Modeling","abstract":"The rate and reach of directly-modulated laser links is often limited by the interplay between chirp and fiber chromatic dispersion. We address this by optimizing the transmitter, receiver, bias and peak-to-peak current to the laser jointly. Our approach outperforms Volterra post-equalization at various symbol rates.","sentences":["The rate and reach of directly-modulated laser links is often limited by the interplay between chirp and fiber chromatic dispersion.","We address this by optimizing the transmitter, receiver, bias and peak-to-peak current to the laser jointly.","Our approach outperforms Volterra post-equalization at various symbol rates."],"url":"http://arxiv.org/abs/2405.10257v1","category":"eess.SP"}
{"created":"2024-05-16 17:38:38","title":"The dynamics and electromagnetic signatures of accretion in unequal mass binary black hole inspirals","abstract":"We present a theoretical study of the gravitational wave (GW) driven inspirals of accreting black hole binaries with mass $M = 10^7 M_\\odot$ and mass ratios between $10^{-3}$ and $10^{-1}$. Our results are based on analytic estimates, and grid-based hydrodynamics simulations run for many thousands of binary orbits before the merger. We show that the GW inspiral is evident in the light curves and color evolution of a binary-hosting quasar, over years to decades before a merger. The long-term electromagnetic (EM) signature is characterized by a gradual UV brightening, and X-ray dimming, followed by an X-ray disappearance hours to days before the GW burst, and finally a years-like re-brightening as the disk relaxes and refuels the remnant black hole. These timescales are surprisingly insensitive to the amplitude of viscous stress in the disk. The spectrum of quasi-thermal disk emission shows two peaks: one in the UV, and another in the X-ray, associated with the outer and circum-secondary disks respectively; emission from the inner disk is suppressed because the secondary consumes most of the inflowing gas. We discuss implications for real-time and archival EM followup of GW bursts detected by LISA.","sentences":["We present a theoretical study of the gravitational wave (GW) driven inspirals of accreting black hole binaries with mass $M = 10^7 M_\\odot$ and mass ratios between $10^{-3}$ and $10^{-1}$. Our results are based on analytic estimates, and grid-based hydrodynamics simulations run for many thousands of binary orbits before the merger.","We show that the GW inspiral is evident in the light curves and color evolution of a binary-hosting quasar, over years to decades before a merger.","The long-term electromagnetic (EM) signature is characterized by a gradual UV brightening, and X-ray dimming, followed by an X-ray disappearance hours to days before the GW burst, and finally a years-like re-brightening as the disk relaxes and refuels the remnant black hole.","These timescales are surprisingly insensitive to the amplitude of viscous stress in the disk.","The spectrum of quasi-thermal disk emission shows two peaks: one in the UV, and another in the X-ray, associated with the outer and circum-secondary disks respectively; emission from the inner disk is suppressed because the secondary consumes most of the inflowing gas.","We discuss implications for real-time and archival EM followup of GW bursts detected by LISA."],"url":"http://arxiv.org/abs/2405.10281v1","category":"astro-ph.HE"}
{"created":"2024-05-16 17:30:55","title":"Simultaneous Haar Indistinguishability with Applications to Unclonable Cryptography","abstract":"Unclonable cryptography is concerned with leveraging the no-cloning principle to build cryptographic primitives that are otherwise impossible to achieve classically. Understanding the feasibility of unclonable encryption, one of the key unclonable primitives, satisfying indistinguishability security in the plain model has been a major open question in the area. So far, the existing constructions of unclonable encryption are either in the quantum random oracle model or are based on new conjectures.   We present a new approach to unclonable encryption via a reduction to a novel question about nonlocal quantum state discrimination: how well can non-communicating -- but entangled -- players distinguish between different distributions over quantum states? We call this task simultaneous state indistinguishability. Our main technical result is showing that the players cannot distinguish between each player receiving independently-chosen Haar random states versus all players receiving the same Haar random state.   We leverage this result to present the first construction of unclonable encryption satisfying indistinguishability security, with quantum decryption keys, in the plain model. We also show other implications to single-decryptor encryption and leakage-resilient secret sharing.","sentences":["Unclonable cryptography is concerned with leveraging the no-cloning principle to build cryptographic primitives that are otherwise impossible to achieve classically.","Understanding the feasibility of unclonable encryption, one of the key unclonable primitives, satisfying indistinguishability security in the plain model has been a major open question in the area.","So far, the existing constructions of unclonable encryption are either in the quantum random oracle model or are based on new conjectures.   ","We present a new approach to unclonable encryption via a reduction to a novel question about nonlocal quantum state discrimination: how well can non-communicating -- but entangled -- players distinguish between different distributions over quantum states?","We call this task simultaneous state indistinguishability.","Our main technical result is showing that the players cannot distinguish between each player receiving independently-chosen Haar random states versus all players receiving the same Haar random state.   ","We leverage this result to present the first construction of unclonable encryption satisfying indistinguishability security, with quantum decryption keys, in the plain model.","We also show other implications to single-decryptor encryption and leakage-resilient secret sharing."],"url":"http://arxiv.org/abs/2405.10274v1","category":"quant-ph"}
{"created":"2024-05-16 17:24:21","title":"The Magic in Nuclear and Hypernuclear Forces","abstract":"Toward an improved understanding of the role of quantum information in nuclei and exotic matter, we examine the magic (non-stabilizerness) in low-energy strong interaction processes. As stabilizer states can be prepared efficiently using classical computers, and include classes of entangled states, it is magic and fluctuations in magic, along with entanglement, that determine resource requirements for quantum simulations. As a measure of fluctuations in magic induced by scattering, the \"magic power\" of the S-matrix is introduced. Using experimentally-determined scattering phase shifts and mixing parameters, the magic power in nucleon-nucleon and hyperon-nucleon scattering, along with the magic in the deuteron, are found to exhibit interesting features. The $\\Sigma^-$-baryon is identified as a potential candidate catalyst for enhanced spreading of magic and entanglement in dense matter, depending on in-medium decoherence.","sentences":["Toward an improved understanding of the role of quantum information in nuclei and exotic matter, we examine the magic (non-stabilizerness) in low-energy strong interaction processes.","As stabilizer states can be prepared efficiently using classical computers, and include classes of entangled states, it is magic and fluctuations in magic, along with entanglement, that determine resource requirements for quantum simulations.","As a measure of fluctuations in magic induced by scattering, the \"magic power\" of the S-matrix is introduced.","Using experimentally-determined scattering phase shifts and mixing parameters, the magic power in nucleon-nucleon and hyperon-nucleon scattering, along with the magic in the deuteron, are found to exhibit interesting features.","The $\\Sigma^-$-baryon is identified as a potential candidate catalyst for enhanced spreading of magic and entanglement in dense matter, depending on in-medium decoherence."],"url":"http://arxiv.org/abs/2405.10268v1","category":"nucl-th"}
{"created":"2024-05-16 17:13:06","title":"Summary of CKM 2023 working group 5: Direct CP violation (DCPV) including $\u03c6_{3}/\u03b3$ from $B\\to DK$, DCPV effects, branching fractions and polarisation in charmless $B_{(s)}$ decays","abstract":"In this contribution a summary of the activities of Working Group 5 (WG5) presented during the 12th International Workshop on the CKM Unitarity Triangle (CKM2023) is reported. This includes new results on $\\phi_{3}/\\gamma$ measurements using $B\\to DK$ decays, search for $CP$ violation using charmless $B$ decays and $b$-Baryon decays, measurement of branching ratios in hadronic $B$ to charm decays, and theory of three-body nonleptonic $B$ decays.","sentences":["In this contribution a summary of the activities of Working Group 5 (WG5) presented during the 12th International Workshop on the CKM Unitarity Triangle (CKM2023) is reported.","This includes new results on $\\phi_{3}/\\gamma$ measurements using $B\\to DK$ decays, search for $CP$ violation using charmless $B$ decays and $b$-Baryon decays, measurement of branching ratios in hadronic $B$ to charm decays, and theory of three-body nonleptonic $B$ decays."],"url":"http://arxiv.org/abs/2405.10261v1","category":"hep-ex"}
