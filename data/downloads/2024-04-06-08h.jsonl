{"created":"2024-04-04 17:59:59","title":"Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning","abstract":"Recovering the 3D scene geometry from a single view is a fundamental yet ill-posed problem in computer vision. While classical depth estimation methods infer only a 2.5D scene representation limited to the image plane, recent approaches based on radiance fields reconstruct a full 3D representation. However, these methods still struggle with occluded regions since inferring geometry without visual observation requires (i) semantic knowledge of the surroundings, and (ii) reasoning about spatial context. We propose KYN, a novel method for single-view scene reconstruction that reasons about semantic and spatial context to predict each point's density. We introduce a vision-language modulation module to enrich point features with fine-grained semantic information. We aggregate point representations across the scene through a language-guided spatial attention mechanism to yield per-point density predictions aware of the 3D semantic context. We show that KYN improves 3D shape recovery compared to predicting density for each 3D point in isolation. We achieve state-of-the-art results in scene and object reconstruction on KITTI-360, and show improved zero-shot generalization compared to prior work. Project page: https://ruili3.github.io/kyn.","sentences":["Recovering the 3D scene geometry from a single view is a fundamental yet ill-posed problem in computer vision.","While classical depth estimation methods infer only a 2.5D scene representation limited to the image plane, recent approaches based on radiance fields reconstruct a full 3D representation.","However, these methods still struggle with occluded regions since inferring geometry without visual observation requires (i) semantic knowledge of the surroundings, and (ii) reasoning about spatial context.","We propose KYN, a novel method for single-view scene reconstruction that reasons about semantic and spatial context to predict each point's density.","We introduce a vision-language modulation module to enrich point features with fine-grained semantic information.","We aggregate point representations across the scene through a language-guided spatial attention mechanism to yield per-point density predictions aware of the 3D semantic context.","We show that KYN improves 3D shape recovery compared to predicting density for each 3D point in isolation.","We achieve state-of-the-art results in scene and object reconstruction on KITTI-360, and show improved zero-shot generalization compared to prior work.","Project page: https://ruili3.github.io/kyn."],"url":"http://arxiv.org/abs/2404.03658v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:58","title":"OW-VISCap: Open-World Video Instance Segmentation and Captioning","abstract":"Open-world video instance segmentation is an important video understanding task. Yet most methods either operate in a closed-world setting, require an additional user-input, or use classic region-based proposals to identify never before seen objects. Further, these methods only assign a one-word label to detected objects, and don't generate rich object-centric descriptions. They also often suffer from highly overlapping predictions. To address these issues, we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap), an approach to jointly segment, track, and caption previously seen or unseen objects in a video. For this, we introduce open-world object queries to discover never before seen objects without additional user-input. We generate rich and descriptive object-centric captions for each detected object via a masked attention augmented LLM input. We introduce an inter-query contrastive loss to ensure that the object queries differ from one another. Our generalized approach matches or surpasses state-of-the-art on three tasks: open-world video instance segmentation on the BURST dataset, dense video object captioning on the VidSTG dataset, and closed-world video instance segmentation on the OVIS dataset.","sentences":["Open-world video instance segmentation is an important video understanding task.","Yet most methods either operate in a closed-world setting, require an additional user-input, or use classic region-based proposals to identify never before seen objects.","Further, these methods only assign a one-word label to detected objects, and don't generate rich object-centric descriptions.","They also often suffer from highly overlapping predictions.","To address these issues, we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap), an approach to jointly segment, track, and caption previously seen or unseen objects in a video.","For this, we introduce open-world object queries to discover never before seen objects without additional user-input.","We generate rich and descriptive object-centric captions for each detected object via a masked attention augmented LLM input.","We introduce an inter-query contrastive loss to ensure that the object queries differ from one another.","Our generalized approach matches or surpasses state-of-the-art on three tasks: open-world video instance segmentation on the BURST dataset, dense video object captioning on the VidSTG dataset, and closed-world video instance segmentation on the OVIS dataset."],"url":"http://arxiv.org/abs/2404.03657v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:57","title":"MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation","abstract":"We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches.","sentences":["We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images.","While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output.","We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency.","Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency.","We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints.","We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods.","We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches."],"url":"http://arxiv.org/abs/2404.03656v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:52","title":"Magnetic fields from small-scale primordial perturbations","abstract":"Weak magnetic fields must have existed in the early Universe, as they were sourced by the cross product of electron density and temperature gradients through the Biermann-battery mechanism. In this paper we calculate the magnetic fields generated at cosmic dawn by a variety of small-scale primordial perturbations, carefully computing the evolution of electron density and temperature fluctuations, and consistently accounting for relative velocities between baryons and dark matter. We first compute the magnetic field resulting from standard, nearly scale-invariant primordial adiabatic perturbations, making significant improvements to previous calculations. This \"standard\" primordial field has a root mean square (rms) of $\\sim10^{-15}$ nG at $20\\lesssim z \\lesssim 100$, with fluctuations on $\\sim$ kpc comoving scales, and could serve as the seed of present-day magnetic fields observed in galaxies and galaxy clusters. In addition, we consider early-Universe magnetic fields as a possible probe of non-standard initial conditions of the Universe on small scales $k \\sim 1-10^3$ Mpc$^{-1}$. To this end, we compute the maximally-allowed magnetic fields within current upper limits on small-scale adiabatic and isocurvature perturbations. Under the current Cosmic Microwave Background spectral-distortion constraints magnetic fields could be produced with a rms of $\\sim 5\\times 10^{-11}$ nG at $z = 20$. Uncorrelated small-scale isocurvature perturbations within current Big-Bang Nucleosynthesis bounds could potentially enhance the magnetic field to $\\sim 10^{-14}-10^{-10}$ nG at $z = 20$, depending on the specific isocurvature mode considered. While these very weak fields remain well below current observational capabilities, our work points out that magnetic fields could potentially provide an interesting window into the poorly constrained small-scale initial conditions of the Universe.","sentences":["Weak magnetic fields must have existed in the early Universe, as they were sourced by the cross product of electron density and temperature gradients through the Biermann-battery mechanism.","In this paper we calculate the magnetic fields generated at cosmic dawn by a variety of small-scale primordial perturbations, carefully computing the evolution of electron density and temperature fluctuations, and consistently accounting for relative velocities between baryons and dark matter.","We first compute the magnetic field resulting from standard, nearly scale-invariant primordial adiabatic perturbations, making significant improvements to previous calculations.","This \"standard\" primordial field has a root mean square (rms) of $\\sim10^{-15}$ nG at $20\\lesssim z \\lesssim 100$, with fluctuations on $\\sim$ kpc comoving scales, and could serve as the seed of present-day magnetic fields observed in galaxies and galaxy clusters.","In addition, we consider early-Universe magnetic fields as a possible probe of non-standard initial conditions of the Universe on small scales $k \\sim 1-10^3$ Mpc$^{-1}$. To this end, we compute the maximally-allowed magnetic fields within current upper limits on small-scale adiabatic and isocurvature perturbations.","Under the current Cosmic Microwave Background spectral-distortion constraints magnetic fields could be produced with a rms of $\\sim 5\\times 10^{-11}$ nG at $z = 20$. Uncorrelated small-scale isocurvature perturbations within current Big-Bang Nucleosynthesis bounds could potentially enhance the magnetic field to $\\sim 10^{-14}-10^{-10}$ nG at $z = 20$, depending on the specific isocurvature mode considered.","While these very weak fields remain well below current observational capabilities, our work points out that magnetic fields could potentially provide an interesting window into the poorly constrained small-scale initial conditions of the Universe."],"url":"http://arxiv.org/abs/2404.03655v1","category":"astro-ph.CO"}
{"created":"2024-04-04 17:59:50","title":"RaFE: Generative Radiance Fields Restoration","abstract":"NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website https://zkaiwu.github.io/RaFE-Project/.","sentences":["NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints.","Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration.","To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations.","Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually.","Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images.","Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration.","We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task.","Please see our project website https://zkaiwu.github.io/RaFE-Project/."],"url":"http://arxiv.org/abs/2404.03654v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:46","title":"CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching","abstract":"Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion model's insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance.","sentences":["Diffusion models have demonstrated great success in the field of text-to-image generation.","However, alleviating the misalignment between the text prompts and images is still challenging.","The root reason behind the misalignment has not been extensively investigated.","We observe that the misalignment is caused by inadequate token attention activation.","We further attribute this phenomenon to the diffusion model's insufficient condition utilization, which is caused by its training paradigm.","To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism.","We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens.","A novel attribute concentration module is also proposed to address the attribute binding problem.","Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.","Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance."],"url":"http://arxiv.org/abs/2404.03653v1","category":"cs.CV"}
{"created":"2024-04-04 17:59:05","title":"Toric Promotion with Reflections and Refractions","abstract":"Inspired by recent work on refraction billiards in dynamics, we introduce a notion of refraction for combinatorial billiards. This allows us to define a generalization of toric promotion that we call toric promotion with reflections and refractions, which is a dynamical system defined via a graph $G$ whose edges are partitioned into a set of reflection edges and a set of refraction edges. This system is a discretization of a billiards system in which a beam of light can pass through, reflect off of, or refract through each toric hyperplane in a toric arrangement. Vastly generalizing the main theorem known about toric promotion, we give a simple formula for the orbit structure of toric promotion with reflections and refractions when $G$ is a forest. We also completely describe the orbit sizes when $G$ is a cycle with an even number of refraction edges; this result is new even for ordinary toric promotion (i.e., when there are no refraction edges). When $G$ is a cycle of even size with no reflection edges, we obtain an interesting instance of the cyclic sieving phenomenon.","sentences":["Inspired by recent work on refraction billiards in dynamics, we introduce a notion of refraction for combinatorial billiards.","This allows us to define a generalization of toric promotion that we call toric promotion with reflections and refractions, which is a dynamical system defined via a graph $G$ whose edges are partitioned into a set of reflection edges and a set of refraction edges.","This system is a discretization of a billiards system in which a beam of light can pass through, reflect off of, or refract through each toric hyperplane in a toric arrangement.","Vastly generalizing the main theorem known about toric promotion, we give a simple formula for the orbit structure of toric promotion with reflections and refractions when $G$ is a forest.","We also completely describe the orbit sizes when $G$ is a cycle with an even number of refraction edges; this result is new even for ordinary toric promotion (i.e., when there are no refraction edges).","When $G$ is a cycle of even size with no reflection edges, we obtain an interesting instance of the cyclic sieving phenomenon."],"url":"http://arxiv.org/abs/2404.03649v1","category":"math.CO"}
{"created":"2024-04-04 17:58:40","title":"AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent","abstract":"Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at \\url{https://github.com/THUDM/AutoWebGLM}.","sentences":["Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web.","In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly.","We employ a hybrid human-AI method to build web browsing data for curriculum training.","Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself.","For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks.","We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments.","Related code, model, and data will be released at \\url{https://github.com/THUDM/AutoWebGLM}."],"url":"http://arxiv.org/abs/2404.03648v1","category":"cs.CL"}
{"created":"2024-04-04 17:58:38","title":"Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra","abstract":"In this paper, we explore the capabilities of state-of-the-art large language models (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving undergraduate-level control problems. Controls provides an interesting case study for LLM reasoning due to its combination of mathematical theory and engineering design. We introduce ControlBench, a benchmark dataset tailored to reflect the breadth, depth, and complexity of classical control design. We use this dataset to study and evaluate the problem-solving abilities of these LLMs in the context of control engineering. We present evaluations conducted by a panel of human experts, providing insights into the accuracy, reasoning, and explanatory prowess of LLMs in control engineering. Our analysis reveals the strengths and limitations of each LLM in the context of classical control, and our results imply that Claude 3 Opus has become the state-of-the-art LLM for solving undergraduate control problems. Our study serves as an initial step towards the broader goal of employing artificial general intelligence in control engineering.","sentences":["In this paper, we explore the capabilities of state-of-the-art large language models (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving undergraduate-level control problems.","Controls provides an interesting case study for LLM reasoning due to its combination of mathematical theory and engineering design.","We introduce ControlBench, a benchmark dataset tailored to reflect the breadth, depth, and complexity of classical control design.","We use this dataset to study and evaluate the problem-solving abilities of these LLMs in the context of control engineering.","We present evaluations conducted by a panel of human experts, providing insights into the accuracy, reasoning, and explanatory prowess of LLMs in control engineering.","Our analysis reveals the strengths and limitations of each LLM in the context of classical control, and our results imply that Claude 3 Opus has become the state-of-the-art LLM for solving undergraduate control problems.","Our study serves as an initial step towards the broader goal of employing artificial general intelligence in control engineering."],"url":"http://arxiv.org/abs/2404.03647v1","category":"math.OC"}
{"created":"2024-04-04 17:58:01","title":"Hamiltonian simulation for low-energy states with optimal time dependence","abstract":"We consider the task of simulating time evolution under a Hamiltonian $H$ within its low-energy subspace. Assuming access to a block-encoding of $H'=(H-E)/\\lambda$ for some $E \\in \\mathbb R$, the goal is to implement an $\\epsilon$-approximation to $e^{-itH}$ when the initial state is confined to the subspace corresponding to eigenvalues $[-1, -1+\\Delta/\\lambda]$ of $H'$. We present a quantum algorithm that uses $O(t\\sqrt{\\lambda\\Gamma} + \\sqrt{\\lambda/\\Gamma}\\log(1/\\epsilon))$ queries to the block-encoding for any $\\Gamma$ such that $\\Delta \\leq \\Gamma \\leq \\lambda$. When $\\log(1/\\epsilon) = o(t\\lambda)$ and $\\Delta/\\lambda = o(1)$, this result improves over generic methods with query complexity $\\Omega(t\\lambda)$. Our quantum algorithm leverages spectral gap amplification and the quantum singular value transform. Using standard access models for $H$, we show that the ability to efficiently block-encode $H'$ is equivalent to $H$ being what we refer to as a \"gap-amplifiable\" Hamiltonian. This includes physically relevant examples such as frustration-free systems, and it encompasses all previously considered settings of low-energy simulation algorithms. We also provide lower bounds for low-energy simulation. In the worst case, we show that the low-energy condition cannot be used to improve the runtime of Hamiltonian simulation. For gap-amplifiable Hamiltonians, we prove that our algorithm is tight in the query model with respect to $t$, $\\Delta$, and $\\lambda$. In the practically relevant regime where $\\log (1/\\epsilon) = o(t\\Delta)$ and $\\Delta/\\lambda = o(1)$, we also prove a matching lower bound in gate complexity (up to log factors). To establish the query lower bounds, we consider $\\mathrm{PARITY}\\circ\\mathrm{OR}$ and degree bounds on trigonometric polynomials. To establish the lower bound on gate complexity, we use a circuit-to-Hamiltonian reduction acting on a low-energy state.","sentences":["We consider the task of simulating time evolution under a Hamiltonian $H$ within its low-energy subspace.","Assuming access to a block-encoding of $H'=(H-E)/\\lambda$ for some $E \\in \\mathbb R$, the goal is to implement an $\\epsilon$-approximation to $e^{-itH}$ when the initial state is confined to the subspace corresponding to eigenvalues $[-1, -1+\\Delta/\\lambda]$ of $H'$. We present a quantum algorithm that uses $O(t\\sqrt{\\lambda\\Gamma} + \\sqrt{\\lambda/\\Gamma}\\log(1/\\epsilon))$ queries to the block-encoding for any $\\Gamma$ such that $\\Delta \\leq \\Gamma","\\leq \\lambda$.","When $\\log(1/\\epsilon) = o(t\\lambda)$ and $\\Delta/\\lambda = o(1)$, this result improves over generic methods with query complexity $\\Omega(t\\lambda)$. Our quantum algorithm leverages spectral gap amplification and the quantum singular value transform.","Using standard access models for $H$, we show that the ability to efficiently block-encode $H'$ is equivalent to $H$ being what we refer to as a \"gap-amplifiable\" Hamiltonian.","This includes physically relevant examples such as frustration-free systems, and it encompasses all previously considered settings of low-energy simulation algorithms.","We also provide lower bounds for low-energy simulation.","In the worst case, we show that the low-energy condition cannot be used to improve the runtime of Hamiltonian simulation.","For gap-amplifiable Hamiltonians, we prove that our algorithm is tight in the query model with respect to $t$, $\\Delta$, and $\\lambda$. In the practically relevant regime where $\\log (1/\\epsilon) = o(t\\Delta)$ and $\\Delta/\\lambda = o(1)$, we also prove a matching lower bound in gate complexity (up to log factors).","To establish the query lower bounds, we consider $\\mathrm{PARITY}\\circ\\mathrm{OR}$ and degree bounds on trigonometric polynomials.","To establish the lower bound on gate complexity, we use a circuit-to-Hamiltonian reduction acting on a low-energy state."],"url":"http://arxiv.org/abs/2404.03644v1","category":"quant-ph"}
{"created":"2024-04-04 17:57:25","title":"DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior","abstract":"Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion model's focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods.","sentences":["Human body restoration plays a vital role in various applications related to the human body.","Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs.","Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance.","Specifically, we employ a pretrained body attention module to guide the diffusion model's focus on the foreground, addressing issues caused by blending between the subject and background.","We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details.","Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions.","Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration.","Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods."],"url":"http://arxiv.org/abs/2404.03642v1","category":"cs.CV"}
{"created":"2024-04-04 17:57:22","title":"Amortized Analysis via Coalgebra","abstract":"Amortized analysis is a cost analysis technique for data structures in which cost is studied in aggregate, rather than considering the maximum cost of a single operation. Traditionally, amortized analysis has been phrased inductively, in terms of finite sequences of operations. Connecting to prior work on coalgebraic semantics for data structures, we develop the perspective that amortized analysis is naturally viewed coalgebraically in the category of algebras for a cost monad, where a morphism of coalgebras serves as a first-class generalization of potential function suitable for integrating cost and behavior. Using this simple definition, we consider amortization of other effects, such as randomization, and we compose amortization arguments in the indexed category of coalgebras. We generalize this to parallel data structure usage patterns by using coalgebras for an endoprofunctor instead of an endofunctor, combining potential using a monoidal structure on the underlying category. Finally, we adapt our discussion to the bicategorical setting, supporting imprecise amortized upper bounds.","sentences":["Amortized analysis is a cost analysis technique for data structures in which cost is studied in aggregate, rather than considering the maximum cost of a single operation.","Traditionally, amortized analysis has been phrased inductively, in terms of finite sequences of operations.","Connecting to prior work on coalgebraic semantics for data structures, we develop the perspective that amortized analysis is naturally viewed coalgebraically in the category of algebras for a cost monad, where a morphism of coalgebras serves as a first-class generalization of potential function suitable for integrating cost and behavior.","Using this simple definition, we consider amortization of other effects, such as randomization, and we compose amortization arguments in the indexed category of coalgebras.","We generalize this to parallel data structure usage patterns by using coalgebras for an endoprofunctor instead of an endofunctor, combining potential using a monoidal structure on the underlying category.","Finally, we adapt our discussion to the bicategorical setting, supporting imprecise amortized upper bounds."],"url":"http://arxiv.org/abs/2404.03641v1","category":"cs.PL"}
{"created":"2024-04-04 17:55:49","title":"On-demand higher-harmonic generation through nonlinear Hall effects in curved nanomembranes","abstract":"The high-order Hall effects, which go beyond the ordinary, unlock more possibilities of electronic transport properties and functionalities. Pioneer works focus on the manufacture of complex nanostructures with low lattice symmetry to produce them. In this paper, we theoretically show that such high-order Hall effects can alternatively be generated by curving a conducting nanomembrane which is highly tunable and also enables anisotropy. Its Hall response can be tuned from first to fourth order by simply varying the direction and magnitude of the applied magnetic field. The dominant Hall current frequency can also be altered from zero to double, or even four times that of the applied alternating electric field. This phenomenon is critically dependent on the occurrence of high-order snake orbits associated with the effective magnetic-field dipoles and quadruples induced by the curved geometry. Our results offer pathways for spatially engineering magnetotransport, current rectification, and frequency multiplication in the bent conducting nanomembrane.","sentences":["The high-order Hall effects, which go beyond the ordinary, unlock more possibilities of electronic transport properties and functionalities.","Pioneer works focus on the manufacture of complex nanostructures with low lattice symmetry to produce them.","In this paper, we theoretically show that such high-order Hall effects can alternatively be generated by curving a conducting nanomembrane which is highly tunable and also enables anisotropy.","Its Hall response can be tuned from first to fourth order by simply varying the direction and magnitude of the applied magnetic field.","The dominant Hall current frequency can also be altered from zero to double, or even four times that of the applied alternating electric field.","This phenomenon is critically dependent on the occurrence of high-order snake orbits associated with the effective magnetic-field dipoles and quadruples induced by the curved geometry.","Our results offer pathways for spatially engineering magnetotransport, current rectification, and frequency multiplication in the bent conducting nanomembrane."],"url":"http://arxiv.org/abs/2404.03639v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 17:54:33","title":"WorDepth: Variational Language Prior for Monocular Depth Estimation","abstract":"Three-dimensional (3D) reconstruction from a single image is an ill-posed problem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text description(s) is similarly ill-posed, i.e. spatial arrangements of objects described. We investigate the question of whether two inherently ambiguous modalities can be used in conjunction to produce metric-scaled reconstructions. To test this, we focus on monocular depth estimation, the problem of predicting a dense depth map from a single image, but with an additional text caption describing the scene. To this end, we begin by encoding the text caption as a mean and standard deviation; using a variational framework, we learn the distribution of the plausible metric reconstructions of 3D scenes corresponding to the text captions as a prior. To \"select\" a specific reconstruction or depth map, we encode the given image through a conditional sampler that samples from the latent space of the variational text encoder, which is then decoded to the output depth map. Our approach is trained alternatingly between the text and image branches: in one optimization step, we predict the mean and standard deviation from the text description and sample from a standard Gaussian, and in the other, we sample using a (image) conditional sampler. Once trained, we directly predict depth from the encoded text using the conditional sampler. We demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where we show that language can consistently improve performance in both.","sentences":["Three-dimensional (3D) reconstruction from a single image is an ill-posed problem with inherent ambiguities, i.e. scale.","Predicting a 3D scene from text description(s) is similarly ill-posed, i.e. spatial arrangements of objects described.","We investigate the question of whether two inherently ambiguous modalities can be used in conjunction to produce metric-scaled reconstructions.","To test this, we focus on monocular depth estimation, the problem of predicting a dense depth map from a single image, but with an additional text caption describing the scene.","To this end, we begin by encoding the text caption as a mean and standard deviation; using a variational framework, we learn the distribution of the plausible metric reconstructions of 3D scenes corresponding to the text captions as a prior.","To \"select\" a specific reconstruction or depth map, we encode the given image through a conditional sampler that samples from the latent space of the variational text encoder, which is then decoded to the output depth map.","Our approach is trained alternatingly between the text and image branches: in one optimization step, we predict the mean and standard deviation from the text description and sample from a standard Gaussian, and in the other, we sample using a (image) conditional sampler.","Once trained, we directly predict depth from the encoded text using the conditional sampler.","We demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where we show that language can consistently improve performance in both."],"url":"http://arxiv.org/abs/2404.03635v1","category":"cs.CV"}
{"created":"2024-04-04 17:53:33","title":"Reference-Based 3D-Aware Image Editing with Triplane","abstract":"Generative Adversarial Networks (GANs) have emerged as powerful tools not only for high-quality image generation but also for real image editing through manipulation of their interpretable latent spaces. Recent advancements in GANs include the development of 3D-aware models such as EG3D, characterized by efficient triplane-based architectures enabling the reconstruction of 3D geometry from single images. However, scant attention has been devoted to providing an integrated framework for high-quality reference-based 3D-aware image editing within this domain. This study addresses this gap by exploring and demonstrating the effectiveness of EG3D's triplane space for achieving advanced reference-based edits, presenting a unique perspective on 3D-aware image editing through our novel pipeline. Our approach integrates the encoding of triplane features, spatial disentanglement and automatic localization of features in the triplane domain, and fusion learning for desired image editing. Moreover, our framework demonstrates versatility across domains, extending its effectiveness to animal face edits and partial stylization of cartoon portraits. The method shows significant improvements over relevant 3D-aware latent editing and 2D reference-based editing methods, both qualitatively and quantitatively. Project page: https://three-bee.github.io/triplane_edit","sentences":["Generative Adversarial Networks (GANs) have emerged as powerful tools not only for high-quality image generation but also for real image editing through manipulation of their interpretable latent spaces.","Recent advancements in GANs include the development of 3D-aware models such as EG3D, characterized by efficient triplane-based architectures enabling the reconstruction of 3D geometry from single images.","However, scant attention has been devoted to providing an integrated framework for high-quality reference-based 3D-aware image editing within this domain.","This study addresses this gap by exploring and demonstrating the effectiveness of EG3D's triplane space for achieving advanced reference-based edits, presenting a unique perspective on 3D-aware image editing through our novel pipeline.","Our approach integrates the encoding of triplane features, spatial disentanglement and automatic localization of features in the triplane domain, and fusion learning for desired image editing.","Moreover, our framework demonstrates versatility across domains, extending its effectiveness to animal face edits and partial stylization of cartoon portraits.","The method shows significant improvements over relevant 3D-aware latent editing and 2D reference-based editing methods, both qualitatively and quantitatively.","Project page: https://three-bee.github.io/triplane_edit"],"url":"http://arxiv.org/abs/2404.03632v1","category":"cs.CV"}
{"created":"2024-04-04 17:52:13","title":"Robust Concept Erasure Using Task Vectors","abstract":"With the rapid growth of text-to-image models, a variety of techniques have been suggested to prevent undesirable image generations. Yet, these methods often only protect against specific user prompts and have been shown to allow unsafe generations with other inputs. Here we focus on unconditionally erasing a concept from a text-to-image model rather than conditioning the erasure on the user's prompt. We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training. However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown. To this end, we propose a method called Diverse Inversion, which we use to estimate the required strength of the TV edit. Diverse Inversion finds within the model input space a large set of word embeddings, each of which induces the generation of the target concept. We find that encouraging diversity in the set makes our estimation more robust to unexpected prompts. Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining the core functionality of the model.","sentences":["With the rapid growth of text-to-image models, a variety of techniques have been suggested to prevent undesirable image generations.","Yet, these methods often only protect against specific user prompts and have been shown to allow unsafe generations with other inputs.","Here we focus on unconditionally erasing a concept from a text-to-image model rather than conditioning the erasure on the user's prompt.","We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training.","However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown.","To this end, we propose a method called Diverse Inversion, which we use to estimate the required strength of the TV edit.","Diverse Inversion finds within the model input space a large set of word embeddings, each of which induces the generation of the target concept.","We find that encouraging diversity in the set makes our estimation more robust to unexpected prompts.","Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining the core functionality of the model."],"url":"http://arxiv.org/abs/2404.03631v1","category":"cs.CV"}
{"created":"2024-04-04 17:52:05","title":"Lie-algebraic K\u00e4hler sigma models with the U(1) isotropy","abstract":"We discuss various questions which emerge in connection with the Lie-algebraic deformation of $\\mathbb{CP}^1$ sigma model in two dimensions. First we supersymmetrize the original model endowing it with the minimal ${\\cal N}=(0,2)$ and extended ${\\cal N}=(2,2)$ supersymmetries. Then we derive the general hypercurrent anomaly in the both cases. In the latter case this anomaly is one-loop but is somewhat different from the standard expressions one can find in the literature because the target manifold is non-symmetric. We also show how to introduce the twisted masses and the $\\theta$ term, and study the BPS equation for instantons, in particular the value of the topological charge. Then we demonstrate that the second loop in the $\\beta$ function of the non-supersymmetric Lie-algebraic sigma model is due to an infrared effect. To this end we use a supersymmetric regularization. We also conjecture that the above statement is valid for higher loops too, similar to the parallel phenomenon in four-dimensional ${\\cal N}=1$ super-Yang-Mills. In the second part of the paper we develop a special dimensional reduction -- namely, starting from the two-dimensional Lie-algebraic model we arrive at a quasi-exactly solvable quantum-mechanical problem of the Lam\\'e type.","sentences":["We discuss various questions which emerge in connection with the Lie-algebraic deformation of $\\mathbb{CP}^1$ sigma model in two dimensions.","First we supersymmetrize the original model endowing it with the minimal ${\\cal N}=(0,2)$ and extended ${\\cal N}=(2,2)$ supersymmetries.","Then we derive the general hypercurrent anomaly in the both cases.","In the latter case this anomaly is one-loop but is somewhat different from the standard expressions one can find in the literature because the target manifold is non-symmetric.","We also show how to introduce the twisted masses and the $\\theta$ term, and study the BPS equation for instantons, in particular the value of the topological charge.","Then we demonstrate that the second loop in the $\\beta$ function of the non-supersymmetric Lie-algebraic sigma model is due to an infrared effect.","To this end we use a supersymmetric regularization.","We also conjecture that the above statement is valid for higher loops too, similar to the parallel phenomenon in four-dimensional ${\\cal N}=1$ super-Yang-Mills.","In the second part of the paper we develop a special dimensional reduction -- namely, starting from the two-dimensional Lie-algebraic model we arrive at a quasi-exactly solvable quantum-mechanical problem of the Lam\\'e type."],"url":"http://arxiv.org/abs/2404.03630v1","category":"hep-th"}
{"created":"2024-04-04 17:49:23","title":"Injective norm of real and complex random tensors I: From spin glasses to geometric entanglement","abstract":"The injective norm is a natural generalization to tensors of the operator norm of a matrix. In quantum information, the injective norm is one important measure of genuine multipartite entanglement of quantum states, where it is known as the geometric entanglement. In this paper, we give a high-probability upper bound on the injective norm of real and complex Gaussian random tensors, corresponding to a lower bound on the geometric entanglement of random quantum states, and to a bound on the ground-state energy of a particular multispecies spherical spin glass model. For some cases of our model, previous work used $\\epsilon$-net techniques to identify the correct order of magnitude; in the present work, we use the Kac--Rice formula to give a one-sided bound on the constant which we believe to be tight.","sentences":["The injective norm is a natural generalization to tensors of the operator norm of a matrix.","In quantum information, the injective norm is one important measure of genuine multipartite entanglement of quantum states, where it is known as the geometric entanglement.","In this paper, we give a high-probability upper bound on the injective norm of real and complex Gaussian random tensors, corresponding to a lower bound on the geometric entanglement of random quantum states, and to a bound on the ground-state energy of a particular multispecies spherical spin glass model.","For some cases of our model, previous work used $\\epsilon$-net techniques to identify the correct order of magnitude; in the present work, we use the Kac--Rice formula to give a one-sided bound on the constant which we believe to be tight."],"url":"http://arxiv.org/abs/2404.03627v1","category":"math.PR"}
{"created":"2024-04-04 17:48:28","title":"Training LLMs over Neurally Compressed Text","abstract":"In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\\\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.","sentences":["In this paper, we explore the idea of training large language models (LLMs) over highly compressed text.","While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression.","If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans.","The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning.","In particular, we find that text na\\\"ively compressed via Arithmetic Coding is not readily learnable by LLMs.","To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length.","Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks.","While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths.","Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency.","Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers."],"url":"http://arxiv.org/abs/2404.03626v1","category":"cs.CL"}
{"created":"2024-04-04 17:47:52","title":"Universal Time-Entanglement Trade-off in Open Quantum Systems","abstract":"We demonstrate a surprising connection between pure steady state entanglement and relaxation timescales in an extremely broad class of Markovian open systems, where two (possibly many-body) systems $A$ and $B$ interact locally with a common dissipative environment. This setup also encompases a broad class of adaptive quantum dynamics based on continuous measurement and feedback. As steady state entanglement increases, there is generically an emergent strong symmetry that leads to a dynamical slow down. Using this we can prove rigorous bounds on relaxation times set by steady state entanglement. We also find that this time must necessarily diverge for maximal entanglement. To test our bound, we consider the dynamics of a random ensemble of local Lindbladians that support pure steady states, finding that the bound does an excellent job of predicting how the dissipative gap varies with the amount of entanglement. Our work provides general insights into how dynamics and entanglement are connected in open systems, and has specific relevance to quantum reservoir engineering.","sentences":["We demonstrate a surprising connection between pure steady state entanglement and relaxation timescales in an extremely broad class of Markovian open systems, where two (possibly many-body) systems $A$ and $B$ interact locally with a common dissipative environment.","This setup also encompases a broad class of adaptive quantum dynamics based on continuous measurement and feedback.","As steady state entanglement increases, there is generically an emergent strong symmetry that leads to a dynamical slow down.","Using this we can prove rigorous bounds on relaxation times set by steady state entanglement.","We also find that this time must necessarily diverge for maximal entanglement.","To test our bound, we consider the dynamics of a random ensemble of local Lindbladians that support pure steady states, finding that the bound does an excellent job of predicting how the dissipative gap varies with the amount of entanglement.","Our work provides general insights into how dynamics and entanglement are connected in open systems, and has specific relevance to quantum reservoir engineering."],"url":"http://arxiv.org/abs/2404.03625v1","category":"quant-ph"}
{"created":"2024-04-04 17:46:32","title":"Standardizing Knowledge Engineering Practices with a Reference Architecture","abstract":"Knowledge engineering is the process of creating and maintaining knowledge-producing systems. Throughout the history of computer science and AI, knowledge engineering workflows have been widely used given the importance of high-quality knowledge for reliable intelligent agents. Meanwhile, the scope of knowledge engineering, as apparent from its target tasks and use cases, has been shifting, together with its paradigms such as expert systems, semantic web, and language modeling. The intended use cases and supported user requirements between these paradigms have not been analyzed globally, as new paradigms often satisfy prior pain points while possibly introducing new ones. The recent abstraction of systemic patterns into a boxology provides an opening for aligning the requirements and use cases of knowledge engineering with the systems, components, and software that can satisfy them best. This paper proposes a vision of harmonizing the best practices in the field of knowledge engineering by leveraging the software engineering methodology of creating reference architectures. We describe how a reference architecture can be iteratively designed and implemented to associate user needs with recurring systemic patterns, building on top of existing knowledge engineering workflows and boxologies. We provide a six-step roadmap that can enable the development of such an architecture, providing an initial design and outcome of the definition of architectural scope, selection of information sources, and analysis. We expect that following through on this vision will lead to well-grounded reference architectures for knowledge engineering, will advance the ongoing initiatives of organizing the neurosymbolic knowledge engineering space, and will build new links to the software architectures and data science communities.","sentences":["Knowledge engineering is the process of creating and maintaining knowledge-producing systems.","Throughout the history of computer science and AI, knowledge engineering workflows have been widely used given the importance of high-quality knowledge for reliable intelligent agents.","Meanwhile, the scope of knowledge engineering, as apparent from its target tasks and use cases, has been shifting, together with its paradigms such as expert systems, semantic web, and language modeling.","The intended use cases and supported user requirements between these paradigms have not been analyzed globally, as new paradigms often satisfy prior pain points while possibly introducing new ones.","The recent abstraction of systemic patterns into a boxology provides an opening for aligning the requirements and use cases of knowledge engineering with the systems, components, and software that can satisfy them best.","This paper proposes a vision of harmonizing the best practices in the field of knowledge engineering by leveraging the software engineering methodology of creating reference architectures.","We describe how a reference architecture can be iteratively designed and implemented to associate user needs with recurring systemic patterns, building on top of existing knowledge engineering workflows and boxologies.","We provide a six-step roadmap that can enable the development of such an architecture, providing an initial design and outcome of the definition of architectural scope, selection of information sources, and analysis.","We expect that following through on this vision will lead to well-grounded reference architectures for knowledge engineering, will advance the ongoing initiatives of organizing the neurosymbolic knowledge engineering space, and will build new links to the software architectures and data science communities."],"url":"http://arxiv.org/abs/2404.03624v1","category":"cs.AI"}
{"created":"2024-04-04 17:45:59","title":"Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph","abstract":"Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of common factual knowledge information. However, unravelling the underlying reasoning of LLMs and explaining their internal mechanisms of exploiting this factual knowledge remain active areas of investigation. Our work analyzes the factual knowledge encoded in the latent representation of LLMs when prompted to assess the truthfulness of factual claims. We propose an end-to-end framework that jointly decodes the factual knowledge embedded in the latent space of LLMs from a vector space to a set of ground predicates and represents its evolution across the layers using a temporal knowledge graph. Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations. Consequently, we neither rely on external models nor training processes. We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The local interpretability analysis exposes different latent errors from representation to multi-hop reasoning errors. On the other hand, the global analysis uncovered patterns in the underlying evolution of the model's factual knowledge (e.g., store-and-seek factual information). By enabling graph-based analyses of the latent representations, this work represents a step towards the mechanistic interpretability of LLMs.","sentences":["Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of common factual knowledge information.","However, unravelling the underlying reasoning of LLMs and explaining their internal mechanisms of exploiting this factual knowledge remain active areas of investigation.","Our work analyzes the factual knowledge encoded in the latent representation of LLMs when prompted to assess the truthfulness of factual claims.","We propose an end-to-end framework that jointly decodes the factual knowledge embedded in the latent space of LLMs from a vector space to a set of ground predicates and represents its evolution across the layers using a temporal knowledge graph.","Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations.","Consequently, we neither rely on external models nor training processes.","We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER.","The local interpretability analysis exposes different latent errors from representation to multi-hop reasoning errors.","On the other hand, the global analysis uncovered patterns in the underlying evolution of the model's factual knowledge (e.g., store-and-seek factual information).","By enabling graph-based analyses of the latent representations, this work represents a step towards the mechanistic interpretability of LLMs."],"url":"http://arxiv.org/abs/2404.03623v1","category":"cs.CL"}
{"created":"2024-04-04 17:45:08","title":"Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models","abstract":"Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as \\textbf{the Mind's Eye}, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (\\textbf{VoT}) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate \\textit{mental images} to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.","sentences":["Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks.","However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored.","Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as \\textbf{the Mind's Eye}, enabling the imagination of the unseen world.","Inspired by this cognitive capacity, we propose Visualization-of-Thought (\\textbf{VoT}) prompting.","VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps.","We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds.","Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs.","Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks.","While VoT works surprisingly well on LLMs, the ability to generate \\textit{mental images} to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs."],"url":"http://arxiv.org/abs/2404.03622v1","category":"cs.CL"}
{"created":"2024-04-04 17:43:06","title":"LCM-Lookahead for Encoder-based Text-to-Image Personalization","abstract":"Recent advancements in diffusion models have introduced fast sampling methods that can effectively produce high-quality images in just one or a few denoising steps. Interestingly, when these are distilled from existing diffusion models, they often maintain alignment with the original model, retaining similar outputs for similar prompts and seeds. These properties present opportunities to leverage fast sampling methods as a shortcut-mechanism, using them to create a preview of denoised outputs through which we can backpropagate image-space losses. In this work, we explore the potential of using such shortcut-mechanisms to guide the personalization of text-to-image models to specific facial identities. We focus on encoder-based personalization approaches, and demonstrate that by tuning them with a lookahead identity loss, we can achieve higher identity fidelity, without sacrificing layout diversity or prompt alignment. We further explore the use of attention sharing mechanisms and consistent data generation for the task of personalization, and find that encoder training can benefit from both.","sentences":["Recent advancements in diffusion models have introduced fast sampling methods that can effectively produce high-quality images in just one or a few denoising steps.","Interestingly, when these are distilled from existing diffusion models, they often maintain alignment with the original model, retaining similar outputs for similar prompts and seeds.","These properties present opportunities to leverage fast sampling methods as a shortcut-mechanism, using them to create a preview of denoised outputs through which we can backpropagate image-space losses.","In this work, we explore the potential of using such shortcut-mechanisms to guide the personalization of text-to-image models to specific facial identities.","We focus on encoder-based personalization approaches, and demonstrate that by tuning them with a lookahead identity loss, we can achieve higher identity fidelity, without sacrificing layout diversity or prompt alignment.","We further explore the use of attention sharing mechanisms and consistent data generation for the task of personalization, and find that encoder training can benefit from both."],"url":"http://arxiv.org/abs/2404.03620v1","category":"cs.CV"}
{"created":"2024-04-04 17:41:13","title":"Circuit Knitting Faces Exponential Sampling Overhead Scaling Bounded by Entanglement Cost","abstract":"Circuit knitting, a method for connecting quantum circuits across multiple processors to simulate nonlocal quantum operations, is a promising approach for distributed quantum computing. While various techniques have been developed for circuit knitting, we uncover fundamental limitations to the scalability of this technology. We prove that the sampling overhead of circuit knitting is exponentially lower bounded by the exact entanglement cost of the target bipartite dynamic, even for asymptotic overhead in the parallel cut regime. Specifically, we prove that the regularized sampling overhead assisted with local operations and classical communication (LOCC), of any bipartite quantum channel is lower bounded by the exponential of its exact entanglement cost under separable preserving operations. Furthermore, we show that the regularized sampling overhead for simulating a general bipartite channel via LOCC is lower bounded by $\\kappa$-entanglement and max-Rains information, providing efficiently computable benchmarks. Our work reveals a profound connection between virtual quantum information processing via quasi-probability decomposition and quantum Shannon theory, highlighting the critical role of entanglement in distributed quantum computing.","sentences":["Circuit knitting, a method for connecting quantum circuits across multiple processors to simulate nonlocal quantum operations, is a promising approach for distributed quantum computing.","While various techniques have been developed for circuit knitting, we uncover fundamental limitations to the scalability of this technology.","We prove that the sampling overhead of circuit knitting is exponentially lower bounded by the exact entanglement cost of the target bipartite dynamic, even for asymptotic overhead in the parallel cut regime.","Specifically, we prove that the regularized sampling overhead assisted with local operations and classical communication (LOCC), of any bipartite quantum channel is lower bounded by the exponential of its exact entanglement cost under separable preserving operations.","Furthermore, we show that the regularized sampling overhead for simulating a general bipartite channel via LOCC is lower bounded by $\\kappa$-entanglement and max-Rains information, providing efficiently computable benchmarks.","Our work reveals a profound connection between virtual quantum information processing via quasi-probability decomposition and quantum Shannon theory, highlighting the critical role of entanglement in distributed quantum computing."],"url":"http://arxiv.org/abs/2404.03619v1","category":"quant-ph"}
{"created":"2024-04-04 17:40:06","title":"DeViDe: Faceted medical knowledge for improved medical vision-language pre-training","abstract":"Vision-language pre-training for chest X-rays has made significant strides, primarily by utilizing paired radiographs and radiology reports. However, existing approaches often face challenges in encoding medical knowledge effectively. While radiology reports provide insights into the current disease manifestation, medical definitions (as used by contemporary methods) tend to be overly abstract, creating a gap in knowledge. To address this, we propose DeViDe, a novel transformer-based method that leverages radiographic descriptions from the open web. These descriptions outline general visual characteristics of diseases in radiographs, and when combined with abstract definitions and radiology reports, provide a holistic snapshot of knowledge. DeViDe incorporates three key features for knowledge-augmented vision language alignment: First, a large-language model-based augmentation is employed to homogenise medical knowledge from diverse sources. Second, this knowledge is aligned with image information at various levels of granularity. Third, a novel projection layer is proposed to handle the complexity of aligning each image with multiple descriptions arising in a multi-label setting. In zero-shot settings, DeViDe performs comparably to fully supervised models on external datasets and achieves state-of-the-art results on three large-scale datasets. Additionally, fine-tuning DeViDe on four downstream tasks and six segmentation tasks showcases its superior performance across data from diverse distributions.","sentences":["Vision-language pre-training for chest X-rays has made significant strides, primarily by utilizing paired radiographs and radiology reports.","However, existing approaches often face challenges in encoding medical knowledge effectively.","While radiology reports provide insights into the current disease manifestation, medical definitions (as used by contemporary methods) tend to be overly abstract, creating a gap in knowledge.","To address this, we propose DeViDe, a novel transformer-based method that leverages radiographic descriptions from the open web.","These descriptions outline general visual characteristics of diseases in radiographs, and when combined with abstract definitions and radiology reports, provide a holistic snapshot of knowledge.","DeViDe incorporates three key features for knowledge-augmented vision language alignment: First, a large-language model-based augmentation is employed to homogenise medical knowledge from diverse sources.","Second, this knowledge is aligned with image information at various levels of granularity.","Third, a novel projection layer is proposed to handle the complexity of aligning each image with multiple descriptions arising in a multi-label setting.","In zero-shot settings, DeViDe performs comparably to fully supervised models on external datasets and achieves state-of-the-art results on three large-scale datasets.","Additionally, fine-tuning DeViDe on four downstream tasks and six segmentation tasks showcases its superior performance across data from diverse distributions."],"url":"http://arxiv.org/abs/2404.03618v1","category":"cs.CV"}
{"created":"2024-04-04 17:38:28","title":"On algebras of Dirichlet series invariant under permutations of coefficients","abstract":"Let $\\mathscr O_u$ be the algebra of holomorphic functions on ${\\bf C}_+:=\\{s\\in{\\bf C}:\\text{Re }s>0\\}$ that are limits of Dirichlet series $D=\\sum_{n=1}^\\infty a_n n^{-s}$, $s\\in \\bf{C}_+$, that converge uniformly on proper half-planes of $\\bf{C}_+$. We study algebraic-topological properties of subalgebras of $\\mathscr O_u$: the Banach algebras $\\mathscr W, \\mathscr A, \\mathscr H^\\infty$ and the Frechet algebra $\\mathscr O_b$. Here $\\mathscr W$ consists of functions in $\\mathscr O_u$ of absolutely convergent Dirichlet series on the closure of $\\bf{C}_+$, $\\mathscr A$ is the uniform closure of $\\mathscr W$, $\\mathscr H^\\infty$ is the algebra of all bounded functions in $\\mathscr O_u$, and $\\mathscr O_b$ is set of all $f(s)=\\sum_{n=1}^\\infty a_n n^{-s}$ in $\\mathscr O_u$ so that $f_r\\in \\mathscr H^\\infty$, $r\\in (0,1)$, where $f_r(s):=\\sum_{n=1}^\\infty a_n r^{\\Omega(n)} n^{-s}$ and $\\Omega(n)$ is the number of prime factors of $n$. Let $S_\\bf{N}$ be the group of permutations of $\\bf{N}$. Each $\\sigma\\in S_\\bf{N}$ determines a permutation $\\hat\\sigma\\in S_\\bf{N}$ (i.e., such that $\\hat\\sigma(mn)=\\hat\\sigma(n)\\hat\\sigma(m)$ for all $m,n\\in \\bf{N}$) via the fundamental theorem of arithmetic. For a Dirichlet series $D=\\sum_{n=1}^\\infty a_n n^{-s}$, and $\\sigma \\in S_\\bf{N}$, $S_\\sigma(D)=\\sum_{n=1}^\\infty a_{\\hat{\\sigma}^{-1}(n)} n^{-s}$ determines an action of $S_\\bf{N}$ on the set of all Dirichlet series. It is shown that each of the algebras above is invariant with respect to this action. Given a subgroup $G$ of $S_\\bf{N}$, the set of $G$-invariant subalgebras of these algebras are studied, and their maximal ideal spaces are described, and used to characterise groups of units and of invertible elements having logarithms, find the stable rank, show projective freeness, and describe when the special linear group is generated by elementary matrices, with bounds on the number of factors.","sentences":["Let $\\mathscr O_u$ be the algebra of holomorphic functions on ${\\bf C}_+:=\\{s\\in{\\bf C}:\\text{Re }s>0\\}$ that are limits of Dirichlet series $D=\\sum_{n=1}^\\infty a_n n^{-s}$, $s\\in \\bf{C}_+$, that converge uniformly on proper half-planes of $\\bf{C}_+$. We study algebraic-topological properties of subalgebras of $\\mathscr O_u$: the Banach algebras $\\mathscr W, \\mathscr A, \\mathscr H^\\infty$ and the Frechet algebra $\\mathscr O_b$. Here $\\mathscr W$ consists of functions in $\\mathscr O_u$ of absolutely convergent Dirichlet series on the closure of $\\bf{C}_+$, $\\mathscr A$ is the uniform closure of $\\mathscr W$, $\\mathscr H^\\infty$ is the algebra of all bounded functions in $\\mathscr O_u$, and $\\mathscr O_b$ is set of all $f(s)=\\sum_{n=1}^\\infty a_n n^{-s}$ in $\\mathscr O_u$ so that $f_r\\in \\mathscr H^\\infty$, $r\\in (0,1)$, where $f_r(s):=\\sum_{n=1}^\\infty a_n r^{\\Omega(n)} n^{-s}$ and $\\Omega(n)$ is the number of prime factors of $n$. Let $S_\\bf{N}$ be the group of permutations of $\\bf{N}$. Each $\\sigma\\in S_\\bf{N}$ determines a permutation $\\hat\\sigma\\in S_\\bf{N}$ (i.e., such that $\\hat\\sigma(mn)=\\hat\\sigma(n)\\hat\\sigma(m)$ for all $m,n\\in \\bf{N}$) via the fundamental theorem of arithmetic.","For a Dirichlet series $D=\\sum_{n=1}^\\infty a_n n^{-s}$, and $\\sigma \\in S_\\bf{N}$, $S_\\sigma(D)=\\sum_{n=1}^\\infty a_{\\hat{\\sigma}^{-1}(n)} n^{-s}$ determines an action of $S_\\bf{N}$ on the set of all Dirichlet series.","It is shown that each of the algebras above is invariant with respect to this action.","Given a subgroup $G$ of $S_\\bf{N}$, the set of $G$-invariant subalgebras of these algebras are studied, and their maximal ideal spaces are described, and used to characterise groups of units and of invertible elements having logarithms, find the stable rank, show projective freeness, and describe when the special linear group is generated by elementary matrices, with bounds on the number of factors."],"url":"http://arxiv.org/abs/2404.03616v1","category":"math.CV"}
{"created":"2024-04-04 17:36:20","title":"Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language (extended version)","abstract":"Automated program verifiers are typically implemented using an intermediate verification language (IVL), such as Boogie or Why3. A verifier front-end translates the input program and specification into an IVL program, while the back-end generates proof obligations for the IVL program and employs an SMT solver to discharge them. Soundness of such verifiers therefore requires that the front-end translation faithfully captures the semantics of the input program and specification in the IVL program, and that the back-end reports success only if the IVL program is actually correct. For a verification tool to be trustworthy, these soundness conditions must be satisfied by its actual implementation, not just the program logic it uses.   In this paper, we present a novel validation methodology that, given a formal semantics for the input language and IVL, provides formal soundness guarantees for front-end implementations. For each run of the verifier, we automatically generate a proof in Isabelle showing that the correctness of the produced IVL program implies the correctness of the input program. This proof can be checked independently from the verifier in Isabelle and can be combined with existing work on validating back-ends to obtain an end-to-end soundness result. Our methodology based on forward simulation employs several modularisation strategies to handle the large semantic gap between the input language and the IVL, as well as the intricacies of practical, optimised translations. We present our methodology for the widely-used Viper and Boogie languages. Our evaluation shows that it is effective in validating the translations performed by the existing Viper implementation.","sentences":["Automated program verifiers are typically implemented using an intermediate verification language (IVL), such as Boogie or Why3.","A verifier front-end translates the input program and specification into an IVL program, while the back-end generates proof obligations for the IVL program and employs an SMT solver to discharge them.","Soundness of such verifiers therefore requires that the front-end translation faithfully captures the semantics of the input program and specification in the IVL program, and that the back-end reports success only if the IVL program is actually correct.","For a verification tool to be trustworthy, these soundness conditions must be satisfied by its actual implementation, not just the program logic it uses.   ","In this paper, we present a novel validation methodology that, given a formal semantics for the input language and IVL, provides formal soundness guarantees for front-end implementations.","For each run of the verifier, we automatically generate a proof in Isabelle showing that the correctness of the produced IVL program implies the correctness of the input program.","This proof can be checked independently from the verifier in Isabelle and can be combined with existing work on validating back-ends to obtain an end-to-end soundness result.","Our methodology based on forward simulation employs several modularisation strategies to handle the large semantic gap between the input language and the IVL, as well as the intricacies of practical, optimised translations.","We present our methodology for the widely-used Viper and Boogie languages.","Our evaluation shows that it is effective in validating the translations performed by the existing Viper implementation."],"url":"http://arxiv.org/abs/2404.03614v1","category":"cs.PL"}
{"created":"2024-04-04 17:34:21","title":"InsectMamba: Insect Pest Classification with State Space Model","abstract":"The classification of insect pests is a critical task in agricultural technology, vital for ensuring food security and environmental sustainability. However, the complexity of pest identification, due to factors like high camouflage and species diversity, poses significant obstacles. Existing methods struggle with the fine-grained feature extraction needed to distinguish between closely related pest species. Although recent advancements have utilized modified network structures and combined deep learning approaches to improve accuracy, challenges persist due to the similarity between pests and their surroundings. To address this problem, we introduce InsectMamba, a novel approach that integrates State Space Models (SSMs), Convolutional Neural Networks (CNNs), Multi-Head Self-Attention mechanism (MSA), and Multilayer Perceptrons (MLPs) within Mix-SSM blocks. This integration facilitates the extraction of comprehensive visual features by leveraging the strengths of each encoding strategy. A selective module is also proposed to adaptively aggregate these features, enhancing the model's ability to discern pest characteristics. InsectMamba was evaluated against strong competitors across five insect pest classification datasets. The results demonstrate its superior performance and verify the significance of each model component by an ablation study.","sentences":["The classification of insect pests is a critical task in agricultural technology, vital for ensuring food security and environmental sustainability.","However, the complexity of pest identification, due to factors like high camouflage and species diversity, poses significant obstacles.","Existing methods struggle with the fine-grained feature extraction needed to distinguish between closely related pest species.","Although recent advancements have utilized modified network structures and combined deep learning approaches to improve accuracy, challenges persist due to the similarity between pests and their surroundings.","To address this problem, we introduce InsectMamba, a novel approach that integrates State Space Models (SSMs), Convolutional Neural Networks (CNNs), Multi-Head Self-Attention mechanism (MSA), and Multilayer Perceptrons (MLPs) within Mix-SSM blocks.","This integration facilitates the extraction of comprehensive visual features by leveraging the strengths of each encoding strategy.","A selective module is also proposed to adaptively aggregate these features, enhancing the model's ability to discern pest characteristics.","InsectMamba was evaluated against strong competitors across five insect pest classification datasets.","The results demonstrate its superior performance and verify the significance of each model component by an ablation study."],"url":"http://arxiv.org/abs/2404.03611v1","category":"cs.CV"}
{"created":"2024-04-04 17:34:08","title":"Deriving Compact QUBO Models via Multilevel Constraint Transformation","abstract":"With the advances in customized hardware for quantum annealing and digital/CMOS Annealing, Quadratic Unconstrained Binary Optimization (QUBO) models have received growing attention in the optimization literature. Motivated by an existing general-purpose approach that derives QUBO models from binary linear programs (BLP), we propose a novel Multilevel Constraint Transformation Scheme (MLCTS) that derives QUBO models with fewer ancillary binary variables. We formulate sufficient conditions for the existence of a compact QUBO formulation (i.e., in the original BLP decision space) in terms of constraint levelness and demonstrate the flexibility and applicability of MLCTS on synthetic examples and several well-known combinatorial optimization problems, i.e., the Maximum 2-Satisfiability Problem, the Linear Ordering Problem, the Community Detection Problem, and the Maximum Independence Set Problem. For a proof-of-concept, we compare the performance of two QUBO models for the latter problem on both a general-purpose software-based solver and a hardware-based QUBO solver. The MLCTS-derived models demonstrate significantly better performance for both solvers, in particular, solving up to seven times more instances with the hardware-based approach.","sentences":["With the advances in customized hardware for quantum annealing and digital/CMOS Annealing, Quadratic Unconstrained Binary Optimization (QUBO) models have received growing attention in the optimization literature.","Motivated by an existing general-purpose approach that derives QUBO models from binary linear programs (BLP), we propose a novel Multilevel Constraint Transformation Scheme (MLCTS) that derives QUBO models with fewer ancillary binary variables.","We formulate sufficient conditions for the existence of a compact QUBO formulation (i.e., in the original BLP decision space) in terms of constraint levelness and demonstrate the flexibility and applicability of MLCTS on synthetic examples and several well-known combinatorial optimization problems, i.e., the Maximum 2-Satisfiability Problem, the Linear Ordering Problem, the Community Detection Problem, and the Maximum Independence Set Problem.","For a proof-of-concept, we compare the performance of two QUBO models for the latter problem on both a general-purpose software-based solver and a hardware-based QUBO solver.","The MLCTS-derived models demonstrate significantly better performance for both solvers, in particular, solving up to seven times more instances with the hardware-based approach."],"url":"http://arxiv.org/abs/2404.03610v1","category":"quant-ph"}
{"created":"2024-04-04 17:31:32","title":"Sailor: Open Language Models for South-East Asia","abstract":"We present Sailor, a family of open language models ranging from 0.5B to 7B parameters, tailored for South-East Asian (SEA) languages. These models are continually pre-trained from Qwen1.5, a great language model for multilingual use cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily covering the languages of English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao. The training leverages several techniques, including BPE dropout for improving the model robustness, aggressive data cleaning and deduplication, and small proxy models to optimize data mixture. Experimental results on four typical tasks indicate that Sailor models demonstrate strong performance across different benchmarks, including commonsense reasoning, question answering, reading comprehension and examination. Embracing the open-source spirit, we share our insights through this report to spark a wider interest in developing large language models for multilingual use cases.","sentences":["We present Sailor, a family of open language models ranging from 0.5B to 7B parameters, tailored for South-East Asian (SEA) languages.","These models are continually pre-trained from Qwen1.5, a great language model for multilingual use cases.","From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily covering the languages of English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao.","The training leverages several techniques, including BPE dropout for improving the model robustness, aggressive data cleaning and deduplication, and small proxy models to optimize data mixture.","Experimental results on four typical tasks indicate that Sailor models demonstrate strong performance across different benchmarks, including commonsense reasoning, question answering, reading comprehension and examination.","Embracing the open-source spirit, we share our insights through this report to spark a wider interest in developing large language models for multilingual use cases."],"url":"http://arxiv.org/abs/2404.03608v1","category":"cs.CL"}
{"created":"2024-04-04 17:28:43","title":"Testing the Boundary-to-Bound Correspondence with Numerical Relativity","abstract":"The Boundary-to-Bound (B2B) correspondence, which connects orbital and radiative observables between bound and unbound orbits, has recently been introduced and demonstrated in the perturbative regime. We produce a large number of numerical relativity simulations of bound and unbound encounters between two nonspinning equal mass black holes in order to test this correspondence in the non-perturbative regime. We focus on testing the radiated energy and angular momentum, as well as orbital parameters such as the period and periastron advance. We find that, across a wide range of eccentricities, the B2B relationships do not hold in the non-perturbative regime, thereby placing a clear limit on the applicability of these relationships. We also approximate the separatrix between bound and unbound relativistic encounters as a function of their initial energies and angular momenta.","sentences":["The Boundary-to-Bound (B2B) correspondence, which connects orbital and radiative observables between bound and unbound orbits, has recently been introduced and demonstrated in the perturbative regime.","We produce a large number of numerical relativity simulations of bound and unbound encounters between two nonspinning equal mass black holes in order to test this correspondence in the non-perturbative regime.","We focus on testing the radiated energy and angular momentum, as well as orbital parameters such as the period and periastron advance.","We find that, across a wide range of eccentricities, the B2B relationships do not hold in the non-perturbative regime, thereby placing a clear limit on the applicability of these relationships.","We also approximate the separatrix between bound and unbound relativistic encounters as a function of their initial energies and angular momenta."],"url":"http://arxiv.org/abs/2404.03607v1","category":"gr-qc"}
{"created":"2024-04-04 17:25:31","title":"Analyzing Musical Characteristics of National Anthems in Relation to Global Indices","abstract":"Music plays a huge part in shaping peoples' psychology and behavioral patterns. This paper investigates the connection between national anthems and different global indices with computational music analysis and statistical correlation analysis. We analyze national anthem musical data to determine whether certain musical characteristics are associated with peace, happiness, suicide rate, crime rate, etc. To achieve this, we collect national anthems from 169 countries and use computational music analysis techniques to extract pitch, tempo, beat, and other pertinent audio features. We then compare these musical characteristics with data on different global indices to ascertain whether a significant correlation exists. Our findings indicate that there may be a correlation between the musical characteristics of national anthems and the indices we investigated. The implications of our findings for music psychology and policymakers interested in promoting social well-being are discussed. This paper emphasizes the potential of musical data analysis in social research and offers a novel perspective on the relationship between music and social indices. The source code and data are made open-access for reproducibility and future research endeavors. It can be accessed at http://bit.ly/na_code.","sentences":["Music plays a huge part in shaping peoples' psychology and behavioral patterns.","This paper investigates the connection between national anthems and different global indices with computational music analysis and statistical correlation analysis.","We analyze national anthem musical data to determine whether certain musical characteristics are associated with peace, happiness, suicide rate, crime rate, etc.","To achieve this, we collect national anthems from 169 countries and use computational music analysis techniques to extract pitch, tempo, beat, and other pertinent audio features.","We then compare these musical characteristics with data on different global indices to ascertain whether a significant correlation exists.","Our findings indicate that there may be a correlation between the musical characteristics of national anthems and the indices we investigated.","The implications of our findings for music psychology and policymakers interested in promoting social well-being are discussed.","This paper emphasizes the potential of musical data analysis in social research and offers a novel perspective on the relationship between music and social indices.","The source code and data are made open-access for reproducibility and future research endeavors.","It can be accessed at http://bit.ly/na_code."],"url":"http://arxiv.org/abs/2404.03606v1","category":"cs.SD"}
{"created":"2024-04-04 17:19:19","title":"Trimming Five Generated Gorenstein Ideals","abstract":"Let $(R,\\mathfrak{m},\\Bbbk)$ be a regular local ring of dimension 3. Let $I$ be a Gorenstein ideal of $R$ of grade 3. It follows from a result of Buchsbaum and Eisenbud that there is a skew-symmetric matrix of odd size such that $I$ is generated by the sub-maximal pfaffians of this matrix. Let $J$ be the ideal obtained by multiplying some of the pfaffian generators of $I$ by $\\mathfrak{m}$; we say that $J$ is a trimming of $I$. In a previous work, the first author and A. Hardesty constructed an explicit free resolution of $R/J$ and computed a DG algebra structure on this resolution. They utilized these products to analyze the Tor algebra of such trimmed ideals. Missing from their result was the case where $I$ is five generated. In this paper we address this case.","sentences":["Let $(R,\\mathfrak{m},\\Bbbk)$ be a regular local ring of dimension 3.","Let $I$ be a Gorenstein ideal of $R$ of grade 3.","It follows from a result of Buchsbaum and Eisenbud that there is a skew-symmetric matrix of odd size such that $I$ is generated by the sub-maximal pfaffians of this matrix.","Let $J$ be the ideal obtained by multiplying some of the pfaffian generators of $I$ by $\\mathfrak{m}$; we say that $J$ is a trimming of $I$. In a previous work, the first author and A. Hardesty constructed an explicit free resolution of $R/J$ and computed a DG algebra structure on this resolution.","They utilized these products to analyze the Tor algebra of such trimmed ideals.","Missing from their result was the case where $I$ is five generated.","In this paper we address this case."],"url":"http://arxiv.org/abs/2404.03601v1","category":"math.AC"}
{"created":"2024-04-04 17:18:38","title":"Deformation-induced CPT violation in entangled pairs of neutral kaons","abstract":"In this paper we consider description of kaon -- anti-kaon interference in the context of a theory with deformed $\\cal CPT$ symmetry. In the case of such theoretical models, deviations from the standard $\\cal CPT$ invariance is related to the momentum carried by the particles; in particular the rest masses of particles and antiparticles are equal. We find that the decay intensity of kaon -- anti-kaon pair has three contributing terms: the correct-parity, the wrong-parity, and the interference between them, all of which are affected by deformation. Using the fact that the presence of such terms were not observed we estimate the magnitude of deformation parameter $\\kappa \\gtrsim 10^{18}$ GeV, very close to the expected Planck mass scale. This raises hopes that the effect, if exists, could be detected in a near future accelerator experiments. An uncertainty of the energy measurement is crucial for accuracy of these predictions.","sentences":["In this paper we consider description of kaon -- anti-kaon interference in the context of a theory with deformed $\\cal CPT$ symmetry.","In the case of such theoretical models, deviations from the standard $\\cal CPT$ invariance is related to the momentum carried by the particles; in particular the rest masses of particles and antiparticles are equal.","We find that the decay intensity of kaon -- anti-kaon pair has three contributing terms: the correct-parity, the wrong-parity, and the interference between them, all of which are affected by deformation.","Using the fact that the presence of such terms were not observed we estimate the magnitude of deformation parameter $\\kappa \\gtrsim 10^{18}$ GeV, very close to the expected Planck mass scale.","This raises hopes that the effect, if exists, could be detected in a near future accelerator experiments.","An uncertainty of the energy measurement is crucial for accuracy of these predictions."],"url":"http://arxiv.org/abs/2404.03600v1","category":"hep-ph"}
{"created":"2024-04-04 17:10:32","title":"A mini-course on Generalized Symmetries and a relation to Cobordisms and K-theory","abstract":"This mini-course, conducted at the XI School on Geometric, Algebraic, and Topological Methods in Quantum Field Theory held in Villa de Leyva, Colombia, provides an overview of the interconnection between generalized symmetries and cohomology. It is designed for advanced undergraduate students with a background in physics or mathematics. Additionally, we describe a method for connecting generalized symmetries with cobordisms and K-theory within the framework of string theory. We assume basic knowledge in quantum field theory and differential forms.","sentences":["This mini-course, conducted at the XI School on Geometric, Algebraic, and Topological Methods in Quantum Field Theory held in Villa de Leyva, Colombia, provides an overview of the interconnection between generalized symmetries and cohomology.","It is designed for advanced undergraduate students with a background in physics or mathematics.","Additionally, we describe a method for connecting generalized symmetries with cobordisms and K-theory within the framework of string theory.","We assume basic knowledge in quantum field theory and differential forms."],"url":"http://arxiv.org/abs/2404.03599v1","category":"hep-th"}
{"created":"2024-04-04 17:09:52","title":"Intent Detection and Entity Extraction from BioMedical Literature","abstract":"Biomedical queries have become increasingly prevalent in web searches, reflecting the growing interest in accessing biomedical literature. Despite recent research on large-language models (LLMs) motivated by endeavours to attain generalized intelligence, their efficacy in replacing task and domain-specific natural language understanding approaches remains questionable. In this paper, we address this question by conducting a comprehensive empirical evaluation of intent detection and named entity recognition (NER) tasks from biomedical text. We show that Supervised Fine Tuned approaches are still relevant and more effective than general-purpose LLMs. Biomedical transformer models such as PubMedBERT can surpass ChatGPT on NER task with only 5 supervised examples.","sentences":["Biomedical queries have become increasingly prevalent in web searches, reflecting the growing interest in accessing biomedical literature.","Despite recent research on large-language models (LLMs) motivated by endeavours to attain generalized intelligence, their efficacy in replacing task and domain-specific natural language understanding approaches remains questionable.","In this paper, we address this question by conducting a comprehensive empirical evaluation of intent detection and named entity recognition (NER) tasks from biomedical text.","We show that Supervised Fine Tuned approaches are still relevant and more effective than general-purpose LLMs.","Biomedical transformer models such as PubMedBERT can surpass ChatGPT on NER task with only 5 supervised examples."],"url":"http://arxiv.org/abs/2404.03598v1","category":"cs.CL"}
{"created":"2024-04-04 17:08:47","title":"Chain union closures","abstract":"We study spherical completeness of ball spaces and its stability under expansions. We give some criteria for ball spaces that guarantee that spherical completeness is preserved when the ball space is closed under unions of chains. This applies in particular to the spaces of closed ultrametric balls in ultrametric spaces with linearly ordered value sets, or more generally, with countable narrow value sets. We show that in general, chain union closures of ultrametric spaces with partially ordered value sets do not preserve spherical completeness. Further, we introduce and study the notions of chain union stability and of chain union rank, which measure how often the process of closing a ball space under all unions of chains has to be iterated until a ball space is obtained that is closed under unions of chains.","sentences":["We study spherical completeness of ball spaces and its stability under expansions.","We give some criteria for ball spaces that guarantee that spherical completeness is preserved when the ball space is closed under unions of chains.","This applies in particular to the spaces of closed ultrametric balls in ultrametric spaces with linearly ordered value sets, or more generally, with countable narrow value sets.","We show that in general, chain union closures of ultrametric spaces with partially ordered value sets do not preserve spherical completeness.","Further, we introduce and study the notions of chain union stability and of chain union rank, which measure how often the process of closing a ball space under all unions of chains has to be iterated until a ball space is obtained that is closed under unions of chains."],"url":"http://arxiv.org/abs/2404.03597v1","category":"math.LO"}
{"created":"2024-04-04 17:05:42","title":"Laser Learning Environment: A new environment for coordination-critical multi-agent tasks","abstract":"We introduce the Laser Learning Environment (LLE), a collaborative multi-agent reinforcement learning environment in which coordination is central. In LLE, agents depend on each other to make progress (interdependence), must jointly take specific sequences of actions to succeed (perfect coordination), and accomplishing those joint actions does not yield any intermediate reward (zero-incentive dynamics). The challenge of such problems lies in the difficulty of escaping state space bottlenecks caused by interdependence steps since escaping those bottlenecks is not rewarded. We test multiple state-of-the-art value-based MARL algorithms against LLE and show that they consistently fail at the collaborative task because of their inability to escape state space bottlenecks, even though they successfully achieve perfect coordination. We show that Q-learning extensions such as prioritized experience replay and n-steps return hinder exploration in environments with zero-incentive dynamics, and find that intrinsic curiosity with random network distillation is not sufficient to escape those bottlenecks. We demonstrate the need for novel methods to solve this problem and the relevance of LLE as cooperative MARL benchmark.","sentences":["We introduce the Laser Learning Environment (LLE), a collaborative multi-agent reinforcement learning environment in which coordination is central.","In LLE, agents depend on each other to make progress (interdependence), must jointly take specific sequences of actions to succeed (perfect coordination), and accomplishing those joint actions does not yield any intermediate reward (zero-incentive dynamics).","The challenge of such problems lies in the difficulty of escaping state space bottlenecks caused by interdependence steps since escaping those bottlenecks is not rewarded.","We test multiple state-of-the-art value-based MARL algorithms against LLE and show that they consistently fail at the collaborative task because of their inability to escape state space bottlenecks, even though they successfully achieve perfect coordination.","We show that Q-learning extensions such as prioritized experience replay and n-steps return hinder exploration in environments with zero-incentive dynamics, and find that intrinsic curiosity with random network distillation is not sufficient to escape those bottlenecks.","We demonstrate the need for novel methods to solve this problem and the relevance of LLE as cooperative MARL benchmark."],"url":"http://arxiv.org/abs/2404.03596v1","category":"cs.LG"}
{"created":"2024-04-04 17:02:08","title":"Setpoint control of bilinear systems from noisy data","abstract":"We consider the problem of designing a controller for an unknown bilinear system using only noisy input-states data points generated by it. The controller should achieve regulation to a given state setpoint and provide a guaranteed basin of attraction. Determining the equilibrium input to achieve that setpoint is not trivial in a data-based setting and we propose the design of a controller in two scenarios. The design takes the form of linear matrix inequalities and is validated numerically for a Cuk converter.","sentences":["We consider the problem of designing a controller for an unknown bilinear system using only noisy input-states data points generated by it.","The controller should achieve regulation to a given state setpoint and provide a guaranteed basin of attraction.","Determining the equilibrium input to achieve that setpoint is not trivial in a data-based setting and we propose the design of a controller in two scenarios.","The design takes the form of linear matrix inequalities and is validated numerically for a Cuk converter."],"url":"http://arxiv.org/abs/2404.03594v1","category":"eess.SY"}
{"created":"2024-04-04 17:00:37","title":"ReFT: Representation Finetuning for Language Models","abstract":"Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. Here, we pursue this hypothesis by developing a family of $\\textbf{Representation Finetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.","sentences":["Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights.","However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative.","Here, we pursue this hypothesis by developing a family of $\\textbf{Representation Finetuning (ReFT)}$ methods.","ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations.","We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT).","LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs.","We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE.","In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs.","We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft."],"url":"http://arxiv.org/abs/2404.03592v1","category":"cs.CL"}
{"created":"2024-04-04 16:58:26","title":"SemGrasp: Semantic Grasp Generation via Language Aligned Discretization","abstract":"Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed SemGrasp, which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of SemGrasp, we have compiled a large-scale, grasp-text-aligned dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse grasps. Experimental findings demonstrate that SemGrasp efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset are available publicly at: https://kailinli.github.io/SemGrasp.","sentences":["Generating natural human grasps necessitates consideration of not just object geometry but also semantic information.","Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks.","This paper presents a novel semantic-based grasp generation method, termed SemGrasp, which generates a static human grasp pose by incorporating semantic information into the grasp representation.","We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions.","A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space.","To facilitate the training of SemGrasp, we have compiled a large-scale, grasp-text-aligned dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse grasps.","Experimental findings demonstrate that SemGrasp efficiently generates natural human grasps in alignment with linguistic intentions.","Our code, models, and dataset are available publicly at: https://kailinli.github.io/SemGrasp."],"url":"http://arxiv.org/abs/2404.03590v1","category":"cs.CV"}
{"created":"2024-04-04 16:52:48","title":"Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration","abstract":"An agent assisting humans in daily living activities can collaborate more effectively by anticipating upcoming tasks. Data-driven methods represent the state of the art in task anticipation, planning, and related problems, but these methods are resource-hungry and opaque. Our prior work introduced a proof of concept framework that used an LLM to anticipate 3 high-level tasks that served as goals for a classical planning system that computed a sequence of low-level actions for the agent to achieve these goals. This paper describes DaTAPlan, our framework that significantly extends our prior work toward human-robot collaboration. Specifically, DaTAPlan planner computes actions for an agent and a human to collaboratively and jointly achieve the tasks anticipated by the LLM, and the agent automatically adapts to unexpected changes in human action outcomes and preferences. We evaluate DaTAPlan capabilities in a realistic simulation environment, demonstrating accurate task anticipation, effective human-robot collaboration, and the ability to adapt to unexpected changes. Project website: https://dataplan-hrc.github.io","sentences":["An agent assisting humans in daily living activities can collaborate more effectively by anticipating upcoming tasks.","Data-driven methods represent the state of the art in task anticipation, planning, and related problems, but these methods are resource-hungry and opaque.","Our prior work introduced a proof of concept framework that used an LLM to anticipate 3 high-level tasks that served as goals for a classical planning system that computed a sequence of low-level actions for the agent to achieve these goals.","This paper describes DaTAPlan, our framework that significantly extends our prior work toward human-robot collaboration.","Specifically, DaTAPlan planner computes actions for an agent and a human to collaboratively and jointly achieve the tasks anticipated by the LLM, and the agent automatically adapts to unexpected changes in human action outcomes and preferences.","We evaluate DaTAPlan capabilities in a realistic simulation environment, demonstrating accurate task anticipation, effective human-robot collaboration, and the ability to adapt to unexpected changes.","Project website: https://dataplan-hrc.github.io"],"url":"http://arxiv.org/abs/2404.03587v1","category":"cs.RO"}
{"created":"2024-04-04 16:52:17","title":"Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning","abstract":"Effective verification and validation techniques for modern scientific machine learning workflows are challenging to devise. Statistical methods are abundant and easily deployed, but often rely on speculative assumptions about the data and methods involved. Error bounds for classical interpolation techniques can provide mathematically rigorous estimates of accuracy, but often are difficult or impractical to determine computationally. In this work, we present a best-of-both-worlds approach to verifiable scientific machine learning by demonstrating that (1) multiple standard interpolation techniques have informative error bounds that can be computed or estimated efficiently; (2) comparative performance among distinct interpolants can aid in validation goals; (3) deploying interpolation methods on latent spaces generated by deep learning techniques enables some interpretability for black-box models. We present a detailed case study of our approach for predicting lift-drag ratios from airfoil images. Code developed for this work is available in a public Github repository.","sentences":["Effective verification and validation techniques for modern scientific machine learning workflows are challenging to devise.","Statistical methods are abundant and easily deployed, but often rely on speculative assumptions about the data and methods involved.","Error bounds for classical interpolation techniques can provide mathematically rigorous estimates of accuracy, but often are difficult or impractical to determine computationally.","In this work, we present a best-of-both-worlds approach to verifiable scientific machine learning by demonstrating that (1) multiple standard interpolation techniques have informative error bounds that can be computed or estimated efficiently; (2) comparative performance among distinct interpolants can aid in validation goals; (3) deploying interpolation methods on latent spaces generated by deep learning techniques enables some interpretability for black-box models.","We present a detailed case study of our approach for predicting lift-drag ratios from airfoil images.","Code developed for this work is available in a public Github repository."],"url":"http://arxiv.org/abs/2404.03586v1","category":"cs.LG"}
{"created":"2024-04-04 16:43:40","title":"Consumer Behavior under Benevolent Price Discrimination","abstract":"Extensive research shows that consumers are generally averse to price discrimination. However, instruments of differential pricing can benefit consumer surplus and alleviate inequity through targeted price discounts. This paper examines how these outcome considerations influence consumer reactions to price discrimination. Six studies with 3951 participants show that a large share of consumers is willing to costly switch away from a store that introduces a discount for low-income consumers. This happens irrespective of whether income differences are due to luck or merit. While the price-discriminating store does attract some new high-income consumers, it cannot compensate the loss of existing consumers. Allowing for altruistic preferences by simulating a market mechanism increases costly support for price discounts, but does not alleviate consumer aversions. Finally, we provide evidence that warm glow drives costly support for price discounts.","sentences":["Extensive research shows that consumers are generally averse to price discrimination.","However, instruments of differential pricing can benefit consumer surplus and alleviate inequity through targeted price discounts.","This paper examines how these outcome considerations influence consumer reactions to price discrimination.","Six studies with 3951 participants show that a large share of consumers is willing to costly switch away from a store that introduces a discount for low-income consumers.","This happens irrespective of whether income differences are due to luck or merit.","While the price-discriminating store does attract some new high-income consumers, it cannot compensate the loss of existing consumers.","Allowing for altruistic preferences by simulating a market mechanism increases costly support for price discounts, but does not alleviate consumer aversions.","Finally, we provide evidence that warm glow drives costly support for price discounts."],"url":"http://arxiv.org/abs/2404.03581v1","category":"econ.GN"}
{"created":"2024-04-04 16:40:22","title":"Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm","abstract":"The sim-to-real gap, which represents the disparity between training and testing environments, poses a significant challenge in reinforcement learning (RL). A promising approach to addressing this challenge is distributionally robust RL, often framed as a robust Markov decision process (RMDP). In this framework, the objective is to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between the training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation (TV) distance robust set, postulating that the minimal value of the optimal robust value function is zero. We prove that such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for designing a sample-efficient algorithm accompanied by sharp sample complexity analysis.","sentences":["The sim-to-real gap, which represents the disparity between training and testing environments, poses a significant challenge in reinforcement learning (RL).","A promising approach to addressing this challenge is distributionally robust RL, often framed as a robust Markov decision process (RMDP).","In this framework, the objective is to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment.","Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error.","In this robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a balance between exploration and exploitation during data collection.","Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between the training and testing environments.","To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation (TV) distance robust set, postulating that the minimal value of the optimal robust value function is zero.","We prove that such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and present an algorithm with a provable sample complexity guarantee.","Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for designing a sample-efficient algorithm accompanied by sharp sample complexity analysis."],"url":"http://arxiv.org/abs/2404.03578v1","category":"cs.LG"}
{"created":"2024-04-04 16:38:57","title":"DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling","abstract":"Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io .","sentences":["Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors.","Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility.","In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies.","First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations.","FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures.","Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency.","Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments.","Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications.","Code and demos will be released at https://dreamscene-project.github.io ."],"url":"http://arxiv.org/abs/2404.03575v1","category":"cs.CV"}
{"created":"2024-04-04 16:38:49","title":"TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices","abstract":"Traditional machine learning models often require powerful hardware, making them unsuitable for deployment on resource-limited devices. Tiny Machine Learning (tinyML) has emerged as a promising approach for running machine learning models on these devices, but integrating multiple data modalities into tinyML models still remains a challenge due to increased complexity, latency, and power consumption. This paper proposes TinyVQA, a novel multimodal deep neural network for visual question answering tasks that can be deployed on resource-constrained tinyML hardware. TinyVQA leverages a supervised attention-based model to learn how to answer questions about images using both vision and language modalities. Distilled knowledge from the supervised attention-based VQA model trains the memory aware compact TinyVQA model and low bit-width quantization technique is employed to further compress the model for deployment on tinyML devices. The TinyVQA model was evaluated on the FloodNet dataset, which is used for post-disaster damage assessment. The compact model achieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA for real-world applications. Additionally, the model was deployed on a Crazyflie 2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA model achieved low latencies of 56 ms and consumes 693 mW power while deployed on the tiny drone, showcasing its suitability for resource-constrained embedded systems.","sentences":["Traditional machine learning models often require powerful hardware, making them unsuitable for deployment on resource-limited devices.","Tiny Machine Learning (tinyML) has emerged as a promising approach for running machine learning models on these devices, but integrating multiple data modalities into tinyML models still remains a challenge due to increased complexity, latency, and power consumption.","This paper proposes TinyVQA, a novel multimodal deep neural network for visual question answering tasks that can be deployed on resource-constrained tinyML hardware.","TinyVQA leverages a supervised attention-based model to learn how to answer questions about images using both vision and language modalities.","Distilled knowledge from the supervised attention-based VQA model trains the memory aware compact TinyVQA model and low bit-width quantization technique is employed to further compress the model for deployment on tinyML devices.","The TinyVQA model was evaluated on the FloodNet dataset, which is used for post-disaster damage assessment.","The compact model achieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA for real-world applications.","Additionally, the model was deployed on a Crazyflie 2.0 drone, equipped with an AI deck and GAP8 microprocessor.","The TinyVQA model achieved low latencies of 56 ms and consumes 693 mW power while deployed on the tiny drone, showcasing its suitability for resource-constrained embedded systems."],"url":"http://arxiv.org/abs/2404.03574v1","category":"cs.CV"}
{"created":"2024-04-04 16:25:23","title":"Factored Task and Motion Planning with Combined Optimization, Sampling and Learning","abstract":"In this thesis, we aim to improve the performance of TAMP algorithms from three complementary perspectives. First, we investigate the integration of discrete task planning with continuous trajectory optimization. Our main contribution is a conflict-based solver that automatically discovers why a task plan might fail when considering the constraints of the physical world. This information is then fed back into the task planner, resulting in an efficient, bidirectional, and intuitive interface between task and motion, capable of solving TAMP problems with multiple objects, robots, and tight physical constraints. In the second part, we first illustrate that, given the wide range of tasks and environments within TAMP, neither sampling nor optimization is superior in all settings. To combine the strengths of both approaches, we have designed meta-solvers for TAMP, adaptive solvers that automatically select which algorithms and computations to use and how to best decompose each problem to find a solution faster. In the third part, we combine deep learning architectures with model-based reasoning to accelerate computations within our TAMP solver. Specifically, we target infeasibility detection and nonlinear optimization, focusing on generalization, accuracy, compute time, and data efficiency. At the core of our contributions is a refined, factored representation of the trajectory optimization problems inside TAMP. This structure not only facilitates more efficient planning, encoding of geometric infeasibility, and meta-reasoning but also provides better generalization in neural architectures.","sentences":["In this thesis, we aim to improve the performance of TAMP algorithms from three complementary perspectives.","First, we investigate the integration of discrete task planning with continuous trajectory optimization.","Our main contribution is a conflict-based solver that automatically discovers why a task plan might fail when considering the constraints of the physical world.","This information is then fed back into the task planner, resulting in an efficient, bidirectional, and intuitive interface between task and motion, capable of solving TAMP problems with multiple objects, robots, and tight physical constraints.","In the second part, we first illustrate that, given the wide range of tasks and environments within TAMP, neither sampling nor optimization is superior in all settings.","To combine the strengths of both approaches, we have designed meta-solvers for TAMP, adaptive solvers that automatically select which algorithms and computations to use and how to best decompose each problem to find a solution faster.","In the third part, we combine deep learning architectures with model-based reasoning to accelerate computations within our TAMP solver.","Specifically, we target infeasibility detection and nonlinear optimization, focusing on generalization, accuracy, compute time, and data efficiency.","At the core of our contributions is a refined, factored representation of the trajectory optimization problems inside TAMP.","This structure not only facilitates more efficient planning, encoding of geometric infeasibility, and meta-reasoning but also provides better generalization in neural architectures."],"url":"http://arxiv.org/abs/2404.03567v1","category":"cs.RO"}
{"created":"2024-04-04 16:24:32","title":"PointInfinity: Resolution-Invariant Point Diffusion Models","abstract":"We present PointInfinity, an efficient family of point cloud diffusion models. Our core idea is to use a transformer-based architecture with a fixed-size, resolution-invariant latent representation. This enables efficient training with low-resolution point clouds, while allowing high-resolution point clouds to be generated during inference. More importantly, we show that scaling the test-time resolution beyond the training resolution improves the fidelity of generated point clouds and surfaces. We analyze this phenomenon and draw a link to classifier-free guidance commonly used in diffusion models, demonstrating that both allow trading off fidelity and variability during inference. Experiments on CO3D show that PointInfinity can efficiently generate high-resolution point clouds (up to 131k points, 31 times more than Point-E) with state-of-the-art quality.","sentences":["We present PointInfinity, an efficient family of point cloud diffusion models.","Our core idea is to use a transformer-based architecture with a fixed-size, resolution-invariant latent representation.","This enables efficient training with low-resolution point clouds, while allowing high-resolution point clouds to be generated during inference.","More importantly, we show that scaling the test-time resolution beyond the training resolution improves the fidelity of generated point clouds and surfaces.","We analyze this phenomenon and draw a link to classifier-free guidance commonly used in diffusion models, demonstrating that both allow trading off fidelity and variability during inference.","Experiments on CO3D show that PointInfinity can efficiently generate high-resolution point clouds (up to 131k points, 31 times more than Point-E) with state-of-the-art quality."],"url":"http://arxiv.org/abs/2404.03566v1","category":"cs.CV"}
{"created":"2024-04-04 16:20:34","title":"Personalized LLM Response Generation with Parameterized Memory Injection","abstract":"Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \\textbf{L}LM \\textbf{P}ersonalization(\\textbf{MiLP}).","sentences":["Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language.","On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical.","Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries.","We contend that such paradigm is unable to perceive fine-granularity information.","In this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \\textbf{L}LM \\textbf{P}ersonalization(\\textbf{MiLP})."],"url":"http://arxiv.org/abs/2404.03565v1","category":"cs.CL"}
{"created":"2024-04-04 16:20:07","title":"Polytope symmetries of Feynman integrals","abstract":"Feynman integrals appropriately generalized are $\\mathsf A$-hypergeometric functions. Among the properties of $\\mathsf A$-hypergeometric functions are symmetries associated with the Newton polytope. In ordinary hypergeometric functions these symmetries lead to linear transformations. Combining tools of $\\mathsf A$-hypergeometric systems and the computation of symmetries of polytopes, we consider the associated symmetries of Feynman integrals in the Lee-Pomeransky representation. We compute the symmetries of $\\mathtt n$-gon integrals up to $\\mathtt n=8$, massive banana integrals up to 5-loop, and on-shell ladders up to 3-loop. We apply these symmetries to study finite on-shell ladder integrals up to 3-loop.","sentences":["Feynman integrals appropriately generalized are $\\mathsf A$-hypergeometric functions.","Among the properties of $\\mathsf A$-hypergeometric functions are symmetries associated with the Newton polytope.","In ordinary hypergeometric functions these symmetries lead to linear transformations.","Combining tools of $\\mathsf A$-hypergeometric systems and the computation of symmetries of polytopes, we consider the associated symmetries of Feynman integrals in the Lee-Pomeransky representation.","We compute the symmetries of $\\mathtt n$-gon integrals up to $\\mathtt n=8$, massive banana integrals up to 5-loop, and on-shell ladders up to 3-loop.","We apply these symmetries to study finite on-shell ladder integrals up to 3-loop."],"url":"http://arxiv.org/abs/2404.03564v1","category":"hep-th"}
{"created":"2024-04-04 16:16:53","title":"Select and Summarize: Scene Saliency for Movie Script Summarization","abstract":"Abstractive summarization for long-form narrative texts such as movie scripts is challenging due to the computational and memory constraints of current language models. A movie script typically comprises a large number of scenes; however, only a fraction of these scenes are salient, i.e., important for understanding the overall narrative. The salience of a scene can be operationalized by considering it as salient if it is mentioned in the summary. Automatically identifying salient scenes is difficult due to the lack of suitable datasets. In this work, we introduce a scene saliency dataset that consists of human-annotated salient scenes for 100 movies. We propose a two-stage abstractive summarization approach which first identifies the salient scenes in script and then generates a summary using only those scenes. Using QA-based evaluation, we show that our model outperforms previous state-of-the-art summarization methods and reflects the information content of a movie more accurately than a model that takes the whole movie script as input.","sentences":["Abstractive summarization for long-form narrative texts such as movie scripts is challenging due to the computational and memory constraints of current language models.","A movie script typically comprises a large number of scenes; however, only a fraction of these scenes are salient, i.e., important for understanding the overall narrative.","The salience of a scene can be operationalized by considering it as salient if it is mentioned in the summary.","Automatically identifying salient scenes is difficult due to the lack of suitable datasets.","In this work, we introduce a scene saliency dataset that consists of human-annotated salient scenes for 100 movies.","We propose a two-stage abstractive summarization approach which first identifies the salient scenes in script and then generates a summary using only those scenes.","Using QA-based evaluation, we show that our model outperforms previous state-of-the-art summarization methods and reflects the information content of a movie more accurately than a model that takes the whole movie script as input."],"url":"http://arxiv.org/abs/2404.03561v1","category":"cs.CL"}
{"created":"2024-04-04 16:15:44","title":"A quantum Pascal pyramid and an extended de Moivre-Laplace theorem","abstract":"Pascal's triangle is widely used as a pedagogical tool to explain the \"first-order\" multiplet patterns that arise in the spectra of $I_N S$ coupled spin-1/2 systems in magnetic resonance. Various other combinatorial structures, which may be well-known in the broader field of quantum dynamics, appear to have largely escaped the attention of the magnetic resonance community with a few exceptions, despite potential usefulness.   In this brief set of lecture notes, we describe a \"quantum Pascal pyramid\" (OEIS https://oeis.org/A268533) as a generalization of Pascal's triangle, which is shown to directly map the relationship between multispin operators of arbitrary spin product rank $q$ ($\\hat{Z}_N^q$) and population operators for states with magnetic quantum number $m$ ($\\hat{S}_N^m$), and - as a consequence - obtain the general form of the intensity ratios of multiplets associated with antiphase single-quantum coherences, with an expression given in terms of the Jacobi polynomials.   An extension of the de Moivre-Laplace theorem, beyond the trivial case $q=0$, is applied to the $q$-th columns of the quantum Pascal pyramid, and is given in terms of a product of the $q$-th order Hermite polynomials and a Gaussian distribution, reproducing the well-known functional forms of the solutions of the quantum harmonic oscillator and the classical limit of Hermite-Gaussian modes in laser physics (Allen et al., $\\textit{Phys. Rev. A.}$, $\\textbf{45}$, 1992). This is used to approximate the Fourier-transformed spectra of $\\hat{Z}_N^q$-associated multiplets of arbitrary complexity.   Finally, an exercise is shown in which the first two columns of the quantum Pascal pyramid are used to calculate the previously known symmetry-constrained upper bound on $I_z \\rightarrow S_z$ polarization transfer in $I_N S$ spin systems.","sentences":["Pascal's triangle is widely used as a pedagogical tool to explain the \"first-order\" multiplet patterns that arise in the spectra of $I_N S$ coupled spin-1/2 systems in magnetic resonance.","Various other combinatorial structures, which may be well-known in the broader field of quantum dynamics, appear to have largely escaped the attention of the magnetic resonance community with a few exceptions, despite potential usefulness.   ","In this brief set of lecture notes, we describe a \"quantum Pascal pyramid\" (OEIS https://oeis.org/A268533) as a generalization of Pascal's triangle, which is shown to directly map the relationship between multispin operators of arbitrary spin product rank $q$ ($\\hat{Z}_N^q$) and population operators for states with magnetic quantum number $m$ ($\\hat{S}_N^m$), and - as a consequence - obtain the general form of the intensity ratios of multiplets associated with antiphase single-quantum coherences, with an expression given in terms of the Jacobi polynomials.   ","An extension of the de Moivre-Laplace theorem, beyond the trivial case $q=0$, is applied to the $q$-th columns of the quantum Pascal pyramid, and is given in terms of a product of the $q$-th order Hermite polynomials and a Gaussian distribution, reproducing the well-known functional forms of the solutions of the quantum harmonic oscillator and the classical limit of Hermite-Gaussian modes in laser physics (Allen et al., $\\textit{Phys. Rev. A.}$, $\\textbf{45}$, 1992).","This is used to approximate the Fourier-transformed spectra of $\\hat{Z}_N^q$-associated multiplets of arbitrary complexity.   ","Finally, an exercise is shown in which the first two columns of the quantum Pascal pyramid are used to calculate the previously known symmetry-constrained upper bound on $I_z \\rightarrow S_z$ polarization transfer in $I_N S$ spin systems."],"url":"http://arxiv.org/abs/2404.03560v1","category":"quant-ph"}
{"created":"2024-04-04 16:15:23","title":"How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes","abstract":"Large language models (LLM) have recently shown the extraordinary ability to perform unseen tasks based on few-shot examples provided as text, also known as in-context learning (ICL). While recent works have attempted to understand the mechanisms driving ICL, few have explored training strategies that incentivize these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist models is a promising direction that offers transfer learning potential, enabling large parameterized models to be trained from simpler, related tasks. In this work, we investigate the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples. We propose several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence. Our experiments reveal that ICL models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed curriculum in this work. Our code and models are available at https://github.com/harmonbhasin/curriculum_learning_icl .","sentences":["Large language models (LLM) have recently shown the extraordinary ability to perform unseen tasks based on few-shot examples provided as text, also known as in-context learning (ICL).","While recent works have attempted to understand the mechanisms driving ICL, few have explored training strategies that incentivize these models to generalize to multiple tasks.","Multi-task learning (MTL) for generalist models is a promising direction that offers transfer learning potential, enabling large parameterized models to be trained from simpler, related tasks.","In this work, we investigate the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples.","We propose several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence.","Our experiments reveal that ICL models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed curriculum in this work.","Our code and models are available at https://github.com/harmonbhasin/curriculum_learning_icl ."],"url":"http://arxiv.org/abs/2404.03558v1","category":"cs.CL"}
{"created":"2024-04-04 16:07:06","title":"From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization","abstract":"Training summarization models requires substantial amounts of training data. However for less resourceful languages like Hungarian, openly available models and datasets are notably scarce. To address this gap our paper introduces HunSum-2 an open-source Hungarian corpus suitable for training abstractive and extractive summarization models. The dataset is assembled from segments of the Common Crawl corpus undergoing thorough cleaning, preprocessing and deduplication. In addition to abstractive summarization we generate sentence-level labels for extractive summarization using sentence similarity. We train baseline models for both extractive and abstractive summarization using the collected dataset. To demonstrate the effectiveness of the trained models, we perform both quantitative and qualitative evaluation. Our dataset, models and code are publicly available, encouraging replication, further research, and real-world applications across various domains.","sentences":["Training summarization models requires substantial amounts of training data.","However for less resourceful languages like Hungarian, openly available models and datasets are notably scarce.","To address this gap our paper introduces HunSum-2 an open-source Hungarian corpus suitable for training abstractive and extractive summarization models.","The dataset is assembled from segments of the Common Crawl corpus undergoing thorough cleaning, preprocessing and deduplication.","In addition to abstractive summarization we generate sentence-level labels for extractive summarization using sentence similarity.","We train baseline models for both extractive and abstractive summarization using the collected dataset.","To demonstrate the effectiveness of the trained models, we perform both quantitative and qualitative evaluation.","Our dataset, models and code are publicly available, encouraging replication, further research, and real-world applications across various domains."],"url":"http://arxiv.org/abs/2404.03555v1","category":"cs.CL"}
{"created":"2024-04-04 16:06:39","title":"No Panacea in Planning: Algorithm Selection for Suboptimal Multi-Agent Path Finding","abstract":"Since more and more algorithms are proposed for multi-agent path finding (MAPF) and each of them has its strengths, choosing the correct one for a specific scenario that fulfills some specified requirements is an important task. Previous research in algorithm selection for MAPF built a standard workflow and showed that machine learning can help. In this paper, we study general solvers for MAPF, which further include suboptimal algorithms. We propose different groups of optimization objectives and learning tasks to handle the new tradeoff between runtime and solution quality. We conduct extensive experiments to show that the same loss can not be used for different groups of optimization objectives, and that standard computer vision models are no worse than customized architecture. We also provide insightful discussions on how feature-sensitive pre-processing is needed for learning for MAPF, and how different learning metrics are correlated to different learning tasks.","sentences":["Since more and more algorithms are proposed for multi-agent path finding (MAPF) and each of them has its strengths, choosing the correct one for a specific scenario that fulfills some specified requirements is an important task.","Previous research in algorithm selection for MAPF built a standard workflow and showed that machine learning can help.","In this paper, we study general solvers for MAPF, which further include suboptimal algorithms.","We propose different groups of optimization objectives and learning tasks to handle the new tradeoff between runtime and solution quality.","We conduct extensive experiments to show that the same loss can not be used for different groups of optimization objectives, and that standard computer vision models are no worse than customized architecture.","We also provide insightful discussions on how feature-sensitive pre-processing is needed for learning for MAPF, and how different learning metrics are correlated to different learning tasks."],"url":"http://arxiv.org/abs/2404.03554v1","category":"cs.MA"}
{"created":"2024-04-04 16:05:01","title":"Bringing memory to Boolean networks: a unifying framework","abstract":"Boolean networks are extensively applied as models of complex dynamical systems, aiming at capturing essential features related to causality and synchronicity of the state changes of components along time. Dynamics of Boolean networks result from the application of their Boolean map according to a so-called update mode, specifying the possible transitions between network configurations. In this paper, we explore update modes that possess a memory on past configurations, and provide a generic framework to define them. We show that recently introduced modes such as the most permissive and interval modes can be naturally expressed in this framework. We propose novel update modes, the history-based and trapping modes, and provide a comprehensive comparison between them. Furthermore, we show that trapping dynamics, which further generalize the most permissive mode, correspond to a rich class of networks related to transitive dynamics and encompassing commutative networks. Finally, we provide a thorough characterization of the structure of minimal and principal trapspaces, bringing a combinatorial and algebraic understanding of these objects.","sentences":["Boolean networks are extensively applied as models of complex dynamical systems, aiming at capturing essential features related to causality and synchronicity of the state changes of components along time.","Dynamics of Boolean networks result from the application of their Boolean map according to a so-called update mode, specifying the possible transitions between network configurations.","In this paper, we explore update modes that possess a memory on past configurations, and provide a generic framework to define them.","We show that recently introduced modes such as the most permissive and interval modes can be naturally expressed in this framework.","We propose novel update modes, the history-based and trapping modes, and provide a comprehensive comparison between them.","Furthermore, we show that trapping dynamics, which further generalize the most permissive mode, correspond to a rich class of networks related to transitive dynamics and encompassing commutative networks.","Finally, we provide a thorough characterization of the structure of minimal and principal trapspaces, bringing a combinatorial and algebraic understanding of these objects."],"url":"http://arxiv.org/abs/2404.03553v1","category":"cs.LO"}
{"created":"2024-04-04 15:56:23","title":"Alzheimer's disease detection in PSG signals","abstract":"Alzheimer's disease (AD) and sleep disorders exhibit a close association, where disruptions in sleep patterns often precede the onset of Mild Cognitive Impairment (MCI) and early-stage AD. This study delves into the potential of utilizing sleep-related electroencephalography (EEG) signals acquired through polysomnography (PSG) for the early detection of AD. Our primary focus is on exploring semi-supervised Deep Learning techniques for the classification of EEG signals due to the clinical scenario characterized by the limited data availability. The methodology entails testing and comparing the performance of semi-supervised SMATE and TapNet models, benchmarked against the supervised XCM model, and unsupervised Hidden Markov Models (HMMs). The study highlights the significance of spatial and temporal analysis capabilities, conducting independent analyses of each sleep stage. Results demonstrate the effectiveness of SMATE in leveraging limited labeled data, achieving stable metrics across all sleep stages, and reaching 90% accuracy in its supervised form. Comparative analyses reveal SMATE's superior performance over TapNet and HMM, while XCM excels in supervised scenarios with an accuracy range of 92 - 94%. These findings underscore the potential of semi-supervised models in early AD detection, particularly in overcoming the challenges associated with the scarcity of labeled data. Ablation tests affirm the critical role of spatio-temporal feature extraction in semi-supervised predictive performance, and t-SNE visualizations validate the model's proficiency in distinguishing AD patterns. Overall, this research contributes to the advancement of AD detection through innovative Deep Learning approaches, highlighting the crucial role of semi-supervised learning in addressing data limitations.","sentences":["Alzheimer's disease (AD) and sleep disorders exhibit a close association, where disruptions in sleep patterns often precede the onset of Mild Cognitive Impairment (MCI) and early-stage AD.","This study delves into the potential of utilizing sleep-related electroencephalography (EEG) signals acquired through polysomnography (PSG) for the early detection of AD.","Our primary focus is on exploring semi-supervised Deep Learning techniques for the classification of EEG signals due to the clinical scenario characterized by the limited data availability.","The methodology entails testing and comparing the performance of semi-supervised SMATE and TapNet models, benchmarked against the supervised XCM model, and unsupervised Hidden Markov Models (HMMs).","The study highlights the significance of spatial and temporal analysis capabilities, conducting independent analyses of each sleep stage.","Results demonstrate the effectiveness of SMATE in leveraging limited labeled data, achieving stable metrics across all sleep stages, and reaching 90% accuracy in its supervised form.","Comparative analyses reveal SMATE's superior performance over TapNet and HMM, while XCM excels in supervised scenarios with an accuracy range of 92 - 94%.","These findings underscore the potential of semi-supervised models in early AD detection, particularly in overcoming the challenges associated with the scarcity of labeled data.","Ablation tests affirm the critical role of spatio-temporal feature extraction in semi-supervised predictive performance, and t-SNE visualizations validate the model's proficiency in distinguishing AD patterns.","Overall, this research contributes to the advancement of AD detection through innovative Deep Learning approaches, highlighting the crucial role of semi-supervised learning in addressing data limitations."],"url":"http://arxiv.org/abs/2404.03549v1","category":"eess.SP"}
{"created":"2024-04-04 15:55:04","title":"Generalized R\u00e9nyi statistics","abstract":"In R\\'enyi's representation for exponential order statistics, we change the iid exponential sequence to any iid sequence, and call the resulting order statistic \\emph{generalized R\\'enyi statistic}. We prove that randomly reordering the variables in the generalized R\\'enyi statistic, in the limit we obtain a sequence of iid exponentials. This result allows us to propose a new model for heavy-tailed data. We investigate the problem of estimation of the tail index in the new model.","sentences":["In R\\'enyi's representation for exponential order statistics, we change the iid exponential sequence to any iid sequence, and call the resulting order statistic \\emph{generalized R\\'enyi statistic}.","We prove that randomly reordering the variables in the generalized R\\'enyi statistic, in the limit we obtain a sequence of iid exponentials.","This result allows us to propose a new model for heavy-tailed data.","We investigate the problem of estimation of the tail index in the new model."],"url":"http://arxiv.org/abs/2404.03548v1","category":"math.ST"}
{"created":"2024-04-04 15:52:23","title":"Constraints on the Dirac spectrum from chiral symmetry restoration","abstract":"I derive constraints on the Dirac spectrum in the chirally symmetric phase of a gauge theory with two massless fermion flavors. Using only general properties of correlation functions of scalar and pseudoscalar bilinears, I prove that in the chiral limit of vanishing quark mass $m$ all the corresponding susceptibilities must be power series in $m^2$ with finite coefficients, from which spectral constraints follow. I then use them to show that effective breaking of the anomalous $\\mathrm{U}(1)_A$ symmetry is allowed in the $\\mathrm{SU}(2)_A$ symmetric phase in the chiral limit, and leads to distinctive spectral features: (i) the spectral density must develop a singular $m^4/\\lambda$ peak as $m\\to 0$, (ii) the two-point eigenvalue correlator of near-zero modes must be singular, and (iii) near-zero modes cannot be localized. Moreover, in the symmetric phase the topological charge distribution must be indistinguishable from that of an ideal gas of instantons and anti-instantons of vanishing density, to leading order in the fermion mass.","sentences":["I derive constraints on the Dirac spectrum in the chirally symmetric phase of a gauge theory with two massless fermion flavors.","Using only general properties of correlation functions of scalar and pseudoscalar bilinears, I prove that in the chiral limit of vanishing quark mass $m$ all the corresponding susceptibilities must be power series in $m^2$ with finite coefficients, from which spectral constraints follow.","I then use them to show that effective breaking of the anomalous $\\mathrm{U}(1)_A$ symmetry is allowed in the $\\mathrm{SU}(2)_A$ symmetric phase in the chiral limit, and leads to distinctive spectral features: (i) the spectral density must develop a singular $m^4/\\lambda$ peak as $m\\to 0$, (ii) the two-point eigenvalue correlator of near-zero modes must be singular, and (iii) near-zero modes cannot be localized.","Moreover, in the symmetric phase the topological charge distribution must be indistinguishable from that of an ideal gas of instantons and anti-instantons of vanishing density, to leading order in the fermion mass."],"url":"http://arxiv.org/abs/2404.03546v1","category":"hep-lat"}
{"created":"2024-04-04 15:52:03","title":"Temperature-dependent optical and magneto-optical spectra of ferromagnetic BCC Fe","abstract":"Optical and magneto-optical properties of magnetic materials have been widely exploited to characterize magnetic structures and phenomena, however, their temperature dependence is not well understood. This study implements the supercell approach with thermal lattice and magnetic disorders to obtain optical and magneto-optical spectra at finite temperatures based on Williams-Lax theory. Our results show that large optical spectrum signals are generated at photon energies below 1 eV, originating from the phonon- and magnon-assisted intraband transitions as lattice and magnetic temperatures increase. In addition, the prominent peak near 2.7 eV is redshifted proportionally to magnetic temperature but depends much less on lattice temperature. By analyzing unfolded bands, we show that the reduction of exchange splitting due to the thermal demagnetization causes this redshift. Our unfolded electronic band structure with magnetic disorder shows band kinks, which are characteristic evidence of the coupling between electrons and magnetic excitations. First-order magneto-optical spectra at finite temperature are also predicted, but due to their small magnitude suffer more from sampling errors. We discuss the effect of zero-point vibrations and the connection of these simulations to the Drude model for intraband transitions.","sentences":["Optical and magneto-optical properties of magnetic materials have been widely exploited to characterize magnetic structures and phenomena, however, their temperature dependence is not well understood.","This study implements the supercell approach with thermal lattice and magnetic disorders to obtain optical and magneto-optical spectra at finite temperatures based on Williams-Lax theory.","Our results show that large optical spectrum signals are generated at photon energies below 1 eV, originating from the phonon- and magnon-assisted intraband transitions as lattice and magnetic temperatures increase.","In addition, the prominent peak near 2.7 eV is redshifted proportionally to magnetic temperature but depends much less on lattice temperature.","By analyzing unfolded bands, we show that the reduction of exchange splitting due to the thermal demagnetization causes this redshift.","Our unfolded electronic band structure with magnetic disorder shows band kinks, which are characteristic evidence of the coupling between electrons and magnetic excitations.","First-order magneto-optical spectra at finite temperature are also predicted, but due to their small magnitude suffer more from sampling errors.","We discuss the effect of zero-point vibrations and the connection of these simulations to the Drude model for intraband transitions."],"url":"http://arxiv.org/abs/2404.03545v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 15:49:49","title":"CodeEditorBench: Evaluating Code Editing Capability of Large Language Models","abstract":"Large Language Models (LLMs) for code are rapidly evolving, with code editing emerging as a critical capability. We introduce CodeEditorBench, an evaluation framework designed to rigorously assess the performance of LLMs in code editing tasks, including debugging, translating, polishing, and requirement switching. Unlike existing benchmarks focusing solely on code generation, CodeEditorBench emphasizes real-world scenarios and practical aspects of software development. We curate diverse coding challenges and scenarios from five sources, covering various programming languages, complexity levels, and editing tasks. Evaluation of 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and GPT-4), outperform open-source models in CodeEditorBench, highlighting differences in model performance based on problem types and prompt sensitivities. CodeEditorBench aims to catalyze advancements in LLMs by providing a robust platform for assessing code editing capabilities. We will release all prompts and datasets to enable the community to expand the dataset and benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to the advancement of LLMs in code editing and provide a valuable resource for researchers and practitioners.","sentences":["Large Language Models (LLMs) for code are rapidly evolving, with code editing emerging as a critical capability.","We introduce CodeEditorBench, an evaluation framework designed to rigorously assess the performance of LLMs in code editing tasks, including debugging, translating, polishing, and requirement switching.","Unlike existing benchmarks focusing solely on code generation, CodeEditorBench emphasizes real-world scenarios and practical aspects of software development.","We curate diverse coding challenges and scenarios from five sources, covering various programming languages, complexity levels, and editing tasks.","Evaluation of 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and GPT-4), outperform open-source models in CodeEditorBench, highlighting differences in model performance based on problem types and prompt sensitivities.","CodeEditorBench aims to catalyze advancements in LLMs by providing a robust platform for assessing code editing capabilities.","We will release all prompts and datasets to enable the community to expand the dataset and benchmark emerging LLMs.","By introducing CodeEditorBench, we contribute to the advancement of LLMs in code editing and provide a valuable resource for researchers and practitioners."],"url":"http://arxiv.org/abs/2404.03543v1","category":"cs.SE"}
{"created":"2024-04-04 15:49:01","title":"Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models","abstract":"Deep learning-based medical image processing algorithms require representative data during development. In particular, surgical data might be difficult to obtain, and high-quality public datasets are limited. To overcome this limitation and augment datasets, a widely adopted solution is the generation of synthetic images. In this work, we employ conditional diffusion models to generate knee radiographs from contour and bone segmentations. Remarkably, two distinct strategies are presented by incorporating the segmentation as a condition into the sampling and training process, namely, conditional sampling and conditional training. The results demonstrate that both methods can generate realistic images while adhering to the conditioning segmentation. The conditional training method outperforms the conditional sampling method and the conventional U-Net.","sentences":["Deep learning-based medical image processing algorithms require representative data during development.","In particular, surgical data might be difficult to obtain, and high-quality public datasets are limited.","To overcome this limitation and augment datasets, a widely adopted solution is the generation of synthetic images.","In this work, we employ conditional diffusion models to generate knee radiographs from contour and bone segmentations.","Remarkably, two distinct strategies are presented by incorporating the segmentation as a condition into the sampling and training process, namely, conditional sampling and conditional training.","The results demonstrate that both methods can generate realistic images while adhering to the conditioning segmentation.","The conditional training method outperforms the conditional sampling method and the conventional U-Net."],"url":"http://arxiv.org/abs/2404.03541v1","category":"eess.IV"}
{"created":"2024-04-04 15:47:30","title":"Is CLIP the main roadblock for fine-grained open-world perception?","abstract":"Modern applications increasingly demand flexible computer vision models that adapt to novel concepts not encountered during training. This necessity is pivotal in emerging domains like extended reality, robotics, and autonomous driving, which require the ability to respond to open-world stimuli. A key ingredient is the ability to identify objects based on free-form textual queries defined at inference time - a task known as open-vocabulary object detection. Multimodal backbones like CLIP are the main enabling technology for current open-world perception solutions. Despite performing well on generic queries, recent studies highlighted limitations on the fine-grained recognition capabilities in open-vocabulary settings - i.e., for distinguishing subtle object features like color, shape, and material. In this paper, we perform a detailed examination of these open-vocabulary object recognition limitations to find the root cause. We evaluate the performance of CLIP, the most commonly used vision-language backbone, against a fine-grained object-matching benchmark, revealing interesting analogies between the limitations of open-vocabulary object detectors and their backbones. Experiments suggest that the lack of fine-grained understanding is caused by the poor separability of object characteristics in the CLIP latent space. Therefore, we try to understand whether fine-grained knowledge is present in CLIP embeddings but not exploited at inference time due, for example, to the unsuitability of the cosine similarity matching function, which may discard important object characteristics. Our preliminary experiments show that simple CLIP latent-space re-projections help separate fine-grained concepts, paving the way towards the development of backbones inherently able to process fine-grained details. The code for reproducing these experiments is available at https://github.com/lorebianchi98/FG-CLIP.","sentences":["Modern applications increasingly demand flexible computer vision models that adapt to novel concepts not encountered during training.","This necessity is pivotal in emerging domains like extended reality, robotics, and autonomous driving, which require the ability to respond to open-world stimuli.","A key ingredient is the ability to identify objects based on free-form textual queries defined at inference time - a task known as open-vocabulary object detection.","Multimodal backbones like CLIP are the main enabling technology for current open-world perception solutions.","Despite performing well on generic queries, recent studies highlighted limitations on the fine-grained recognition capabilities in open-vocabulary settings - i.e., for distinguishing subtle object features like color, shape, and material.","In this paper, we perform a detailed examination of these open-vocabulary object recognition limitations to find the root cause.","We evaluate the performance of CLIP, the most commonly used vision-language backbone, against a fine-grained object-matching benchmark, revealing interesting analogies between the limitations of open-vocabulary object detectors and their backbones.","Experiments suggest that the lack of fine-grained understanding is caused by the poor separability of object characteristics in the CLIP latent space.","Therefore, we try to understand whether fine-grained knowledge is present in CLIP embeddings but not exploited at inference time due, for example, to the unsuitability of the cosine similarity matching function, which may discard important object characteristics.","Our preliminary experiments show that simple CLIP latent-space re-projections help separate fine-grained concepts, paving the way towards the development of backbones inherently able to process fine-grained details.","The code for reproducing these experiments is available at https://github.com/lorebianchi98/FG-CLIP."],"url":"http://arxiv.org/abs/2404.03539v1","category":"cs.CV"}
{"created":"2024-04-04 15:46:26","title":"Quantum Science and Technologies in K-12: Supporting Teachers to Integrate Quantum in STEM Classrooms","abstract":"Quantum science and computing represent a vital intersection between science and technology, gaining increasing importance in modern society. There is a pressing need to incorporate these concepts into the K-12 curriculum, equipping new generations with the tools to navigate and thrive in an evolving technological landscape. This study explores the professional learning of K-12 teachers (n = 49) related to quantum concepts and pedagogy. We used open-ended surveys, field notes, workshop artifacts, and interviews to examine teachers' perceptions of quantum and how they made connections between quantum and their curriculum. Our data reveal that most teachers were excited and interested in teaching quantum but were aware of potential barriers and concerns that might get in the way of teaching quantum. We found that teachers readily identified connections to math and science in their curriculum, but only a few made connections to computing. Enthusiasm for teaching quantum concepts was found in both elementary and secondary educators, suggesting a widespread recognition of its importance in preparing students for a future where quantum technology is a fundamental aspect of their lives and careers.","sentences":["Quantum science and computing represent a vital intersection between science and technology, gaining increasing importance in modern society.","There is a pressing need to incorporate these concepts into the K-12 curriculum, equipping new generations with the tools to navigate and thrive in an evolving technological landscape.","This study explores the professional learning of K-12 teachers (n = 49) related to quantum concepts and pedagogy.","We used open-ended surveys, field notes, workshop artifacts, and interviews to examine teachers' perceptions of quantum and how they made connections between quantum and their curriculum.","Our data reveal that most teachers were excited and interested in teaching quantum but were aware of potential barriers and concerns that might get in the way of teaching quantum.","We found that teachers readily identified connections to math and science in their curriculum, but only a few made connections to computing.","Enthusiasm for teaching quantum concepts was found in both elementary and secondary educators, suggesting a widespread recognition of its importance in preparing students for a future where quantum technology is a fundamental aspect of their lives and careers."],"url":"http://arxiv.org/abs/2404.03538v1","category":"physics.ed-ph"}
{"created":"2024-04-04 15:42:02","title":"The dilaton gravity hologram of double-scaled SYK","abstract":"We work out a precise holographic duality between sine dilaton gravity, and DSSYK. More precisely, canonical quantization of sine dilaton gravity reproduces q-Schwarzian quantum mechanics, which is the auxiliary system that arises from the chord diagrams of DSSYK. The role of the chord number in DSSYK is played by the (Weyl rescaled) geodesic length in the bulk. The most puzzling aspect of reconciling DSSYK with a simple gravitational dual at the classical level is the distinction between temperature and \"fake temperature\". At the q-Schwarzian level, we clarify how this arises from the constraint that the chord number is positive. The on-shell q-Schwarzian action with the constraint reproduces the thermodynamics of DSSYK. Semi-classically, in sine dilaton gravity this translates to the insertion of a defect, from which we deduce that fake temperature is the Hawking temperature of a smooth Lorentzian black hole. We comment on several relations with dS space. One remarkable feature is that in sine dilaton gravity quantization discretizes spacetime, therefore the Hilbert space is discrete.","sentences":["We work out a precise holographic duality between sine dilaton gravity, and DSSYK.","More precisely, canonical quantization of sine dilaton gravity reproduces q-Schwarzian quantum mechanics, which is the auxiliary system that arises from the chord diagrams of DSSYK.","The role of the chord number in DSSYK is played by the (Weyl rescaled) geodesic length in the bulk.","The most puzzling aspect of reconciling DSSYK with a simple gravitational dual at the classical level is the distinction between temperature and \"fake temperature\".","At the q-Schwarzian level, we clarify how this arises from the constraint that the chord number is positive.","The on-shell q-Schwarzian action with the constraint reproduces the thermodynamics of DSSYK.","Semi-classically, in sine dilaton gravity this translates to the insertion of a defect, from which we deduce that fake temperature is the Hawking temperature of a smooth Lorentzian black hole.","We comment on several relations with dS space.","One remarkable feature is that in sine dilaton gravity quantization discretizes spacetime, therefore the Hilbert space is discrete."],"url":"http://arxiv.org/abs/2404.03535v1","category":"hep-th"}
{"created":"2024-04-04 15:36:53","title":"Evaluating Generative Language Models in Information Extraction as Subjective Question Correction","abstract":"Modern Large Language Models (LLMs) have showcased remarkable prowess in various tasks necessitating sophisticated cognitive behaviors. Nevertheless, a paradoxical performance discrepancy is observed, where these models underperform in seemingly elementary tasks like relation extraction and event extraction due to two issues in conventional evaluation. (1) The imprecision of existing evaluation metrics that struggle to effectively gauge semantic consistency between model outputs and ground truth, and (2) The inherent incompleteness of evaluation benchmarks, primarily due to restrictive human annotation schemas, resulting in underestimated LLM performances. Inspired by the principles in subjective question correction, we propose a new evaluation method, SQC-Score. This method innovatively utilizes LLMs, fine-tuned through subjective question correction data, to refine matching between model outputs and golden labels. Additionally, by incorporating a Natural Language Inference (NLI) model, SQC-Score enriches golden labels, addressing benchmark incompleteness by acknowledging correct yet previously omitted answers. Results on three information extraction tasks show that SQC-Score is more preferred by human annotators than the baseline metrics. Utilizing SQC-Score, we conduct a comprehensive evaluation of the state-of-the-art LLMs and provide insights for future research for information extraction. Dataset and associated codes can be accessed at https://github.com/THU-KEG/SQC-Score.","sentences":["Modern Large Language Models (LLMs) have showcased remarkable prowess in various tasks necessitating sophisticated cognitive behaviors.","Nevertheless, a paradoxical performance discrepancy is observed, where these models underperform in seemingly elementary tasks like relation extraction and event extraction due to two issues in conventional evaluation.","(1) The imprecision of existing evaluation metrics that struggle to effectively gauge semantic consistency between model outputs and ground truth, and (2) The inherent incompleteness of evaluation benchmarks, primarily due to restrictive human annotation schemas, resulting in underestimated LLM performances.","Inspired by the principles in subjective question correction, we propose a new evaluation method, SQC-Score.","This method innovatively utilizes LLMs, fine-tuned through subjective question correction data, to refine matching between model outputs and golden labels.","Additionally, by incorporating a Natural Language Inference (NLI) model, SQC-Score enriches golden labels, addressing benchmark incompleteness by acknowledging correct yet previously omitted answers.","Results on three information extraction tasks show that SQC-Score is more preferred by human annotators than the baseline metrics.","Utilizing SQC-Score, we conduct a comprehensive evaluation of the state-of-the-art LLMs and provide insights for future research for information extraction.","Dataset and associated codes can be accessed at https://github.com/THU-KEG/SQC-Score."],"url":"http://arxiv.org/abs/2404.03532v1","category":"cs.CL"}
{"created":"2024-04-04 15:35:41","title":"The solving degrees for computing Gr\u00f6bner bases of affine semi-regular polynomial sequences","abstract":"Determining the complexity of computing Gr\\\"{o}bner bases is an important problem both in theory and in practice, and for that the solving degree plays a key role. In this paper, we study the solving degrees of affine semi-regular sequences and their homogenized sequences. Some of our results are considered to give mathematically rigorous proofs of the correctness of methods for computing Gr\\\"{o}bner bases of the ideal generated by an affine semi-regular sequence. This paper is a sequel of the authors' previous work and gives additional results on the solving degrees and important behaviors of Gr\\\"obner basis computation.","sentences":["Determining the complexity of computing Gr\\\"{o}bner bases is an important problem both in theory and in practice, and for that the solving degree plays a key role.","In this paper, we study the solving degrees of affine semi-regular sequences and their homogenized sequences.","Some of our results are considered to give mathematically rigorous proofs of the correctness of methods for computing Gr\\\"{o}bner bases of the ideal generated by an affine semi-regular sequence.","This paper is a sequel of the authors' previous work and gives additional results on the solving degrees and important behaviors of Gr\\\"obner basis computation."],"url":"http://arxiv.org/abs/2404.03530v1","category":"math.AC"}
{"created":"2024-04-04 15:31:11","title":"HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion","abstract":"Data-fusion networks have shown significant promise for RGB-thermal scene parsing. However, the majority of existing studies have relied on symmetric duplex encoders for heterogeneous feature extraction and fusion, paying inadequate attention to the inherent differences between RGB and thermal modalities. Recent progress in vision foundation models (VFMs) trained through self-supervision on vast amounts of unlabeled data has proven their ability to extract informative, general-purpose features. However, this potential has yet to be fully leveraged in the domain. In this study, we take one step toward this new research area by exploring a feasible strategy to fully exploit VFM features for RGB-thermal scene parsing. Specifically, we delve deeper into the unique characteristics of RGB and thermal modalities, thereby designing a hybrid, asymmetric encoder that incorporates both a VFM and a convolutional neural network. This design allows for more effective extraction of complementary heterogeneous features, which are subsequently fused in a dual-path, progressive manner. Moreover, we introduce an auxiliary task to further enrich the local semantics of the fused features, thereby improving the overall performance of RGB-thermal scene parsing. Our proposed HAPNet, equipped with all these components, demonstrates superior performance compared to all other state-of-the-art RGB-thermal scene parsing networks, achieving top ranks across three widely used public RGB-thermal scene parsing datasets. We believe this new paradigm has opened up new opportunities for future developments in data-fusion scene parsing approaches.","sentences":["Data-fusion networks have shown significant promise for RGB-thermal scene parsing.","However, the majority of existing studies have relied on symmetric duplex encoders for heterogeneous feature extraction and fusion, paying inadequate attention to the inherent differences between RGB and thermal modalities.","Recent progress in vision foundation models (VFMs) trained through self-supervision on vast amounts of unlabeled data has proven their ability to extract informative, general-purpose features.","However, this potential has yet to be fully leveraged in the domain.","In this study, we take one step toward this new research area by exploring a feasible strategy to fully exploit VFM features for RGB-thermal scene parsing.","Specifically, we delve deeper into the unique characteristics of RGB and thermal modalities, thereby designing a hybrid, asymmetric encoder that incorporates both a VFM and a convolutional neural network.","This design allows for more effective extraction of complementary heterogeneous features, which are subsequently fused in a dual-path, progressive manner.","Moreover, we introduce an auxiliary task to further enrich the local semantics of the fused features, thereby improving the overall performance of RGB-thermal scene parsing.","Our proposed HAPNet, equipped with all these components, demonstrates superior performance compared to all other state-of-the-art RGB-thermal scene parsing networks, achieving top ranks across three widely used public RGB-thermal scene parsing datasets.","We believe this new paradigm has opened up new opportunities for future developments in data-fusion scene parsing approaches."],"url":"http://arxiv.org/abs/2404.03527v1","category":"cs.CV"}
{"created":"2024-04-04 15:30:11","title":"AMC-backed Twin Arrow Antenna for Wearable Electronic Travel Aid System at 24 GHz","abstract":"An ultra-compact wearable antenna, for electronic travel aid (ETA) applications, is presented. An AMC-backed twin arrow antenna, operative in the 24.05-24.25 GHz frequency band, has been designed for imaging systems supporting ETA. Artificial Magnetic Conductor (AMC) is combined with the antenna with the aim of reducing the backward radiation to the wearing person while improving its radiation properties and bandwidth, all this without increasing the initial area of the antenna. Prototypes of the AMC-antenna have been fabricated and measured. In order to test its performance for the application, imaging have been conducted by means of synthetic aperture radar (SAR) techniques by placing the antenna in the arm of a user to take advantage of natural body movement. Electromagnetic images have been obtained and the target has been identified, demonstrating the suitability of the AMC-antenna for the ETA system.","sentences":["An ultra-compact wearable antenna, for electronic travel aid (ETA) applications, is presented.","An AMC-backed twin arrow antenna, operative in the 24.05-24.25 GHz frequency band, has been designed for imaging systems supporting ETA.","Artificial Magnetic Conductor (AMC) is combined with the antenna with the aim of reducing the backward radiation to the wearing person while improving its radiation properties and bandwidth, all this without increasing the initial area of the antenna.","Prototypes of the AMC-antenna have been fabricated and measured.","In order to test its performance for the application, imaging have been conducted by means of synthetic aperture radar (SAR) techniques by placing the antenna in the arm of a user to take advantage of natural body movement.","Electromagnetic images have been obtained and the target has been identified, demonstrating the suitability of the AMC-antenna for the ETA system."],"url":"http://arxiv.org/abs/2404.03525v1","category":"eess.SY"}
{"created":"2024-04-04 15:26:26","title":"Integrating Generative AI into Financial Market Prediction for Improved Decision Making","abstract":"This study provides an in-depth analysis of the model architecture and key technologies of generative artificial intelligence, combined with specific application cases, and uses conditional generative adversarial networks ( cGAN ) and time series analysis methods to simulate and predict dynamic changes in financial markets. The research results show that the cGAN model can effectively capture the complexity of financial market data, and the deviation between the prediction results and the actual market performance is minimal, showing a high degree of accuracy.","sentences":["This study provides an in-depth analysis of the model architecture and key technologies of generative artificial intelligence, combined with specific application cases, and uses conditional generative adversarial networks ( cGAN ) and time series analysis methods to simulate and predict dynamic changes in financial markets.","The research results show that the cGAN model can effectively capture the complexity of financial market data, and the deviation between the prediction results and the actual market performance is minimal, showing a high degree of accuracy."],"url":"http://arxiv.org/abs/2404.03523v1","category":"cs.CE"}
{"created":"2024-04-04 15:25:40","title":"Experimental studies of black holes: status and future prospects","abstract":"More than a century ago, Albert Einstein presented his general theory of gravitation (GR) to the Prussian Academy of Sciences. One of the predictions of the theory is that not only particles and objects with mass, but also the quanta of light, photons, are tied to the curvature of space-time, and thus to gravity. There must be a critical mass density, above which photons cannot escape. These are black holes (henceforth BH). It took fifty years after the theory was announced before possible candidate objects were identified by observational astronomy. And another fifty years have passed, until we finally have in hand detailed and credible experimental evidence that BHs of 10 to 10^10 times the mass of the Sun exist in the Universe. Three very different experimental techniques, but all based on Michelson interferometry or Fourier-inversion spatial interferometry have enabled the critical experimental breakthroughs. It has now become possible to investigate the space-time structure in the vicinity of the event horizons of BHs. We briefly summarize these interferometric techniques, and discuss the spectacular recent improvements achieved with all three techniques. Finally, we sketch where the path of exploration and inquiry may go on in the next decades.","sentences":["More than a century ago, Albert Einstein presented his general theory of gravitation (GR) to the Prussian Academy of Sciences.","One of the predictions of the theory is that not only particles and objects with mass, but also the quanta of light, photons, are tied to the curvature of space-time, and thus to gravity.","There must be a critical mass density, above which photons cannot escape.","These are black holes (henceforth BH).","It took fifty years after the theory was announced before possible candidate objects were identified by observational astronomy.","And another fifty years have passed, until we finally have in hand detailed and credible experimental evidence that BHs of 10 to 10^10 times the mass of the Sun exist in the Universe.","Three very different experimental techniques, but all based on Michelson interferometry or Fourier-inversion spatial interferometry have enabled the critical experimental breakthroughs.","It has now become possible to investigate the space-time structure in the vicinity of the event horizons of BHs.","We briefly summarize these interferometric techniques, and discuss the spectacular recent improvements achieved with all three techniques.","Finally, we sketch where the path of exploration and inquiry may go on in the next decades."],"url":"http://arxiv.org/abs/2404.03522v1","category":"astro-ph.GA"}
{"created":"2024-04-04 15:21:22","title":"Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach","abstract":"Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval. Aiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs' pretraining data. However, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pretraining data. Moreover, these methods offer limited adaptability under fine-tuning or continual learning settings. We hypothesize that token embeddings are able to capture the model's intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data. Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings. Extensive experiments and in-depth analyses demonstrate the superiority of our model-aware approach.","sentences":["Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks.","Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval.","Aiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs' pretraining data.","However, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pretraining data.","Moreover, these methods offer limited adaptability under fine-tuning or continual learning settings.","We hypothesize that token embeddings are able to capture the model's intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data.","Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings.","Extensive experiments and in-depth analyses demonstrate the superiority of our model-aware approach."],"url":"http://arxiv.org/abs/2404.03514v1","category":"cs.CL"}
{"created":"2024-04-04 15:14:40","title":"Privacy-Enhancing Technologies for Artificial Intelligence-Enabled Systems","abstract":"Artificial intelligence (AI) models introduce privacy vulnerabilities to systems. These vulnerabilities may impact model owners or system users; they exist during model development, deployment, and inference phases, and threats can be internal or external to the system. In this paper, we investigate potential threats and propose the use of several privacy-enhancing technologies (PETs) to defend AI-enabled systems. We then provide a framework for PETs evaluation for a AI-enabled systems and discuss the impact PETs may have on system-level variables.","sentences":["Artificial intelligence (AI) models introduce privacy vulnerabilities to systems.","These vulnerabilities may impact model owners or system users; they exist during model development, deployment, and inference phases, and threats can be internal or external to the system.","In this paper, we investigate potential threats and propose the use of several privacy-enhancing technologies (PETs) to defend AI-enabled systems.","We then provide a framework for PETs evaluation for a AI-enabled systems and discuss the impact PETs may have on system-level variables."],"url":"http://arxiv.org/abs/2404.03509v1","category":"cs.CR"}
{"created":"2024-04-04 15:14:18","title":"The economic consequences of geopolitical fragmentation: Evidence from the Cold War","abstract":"The Cold War was the defining episode of geopolitical fragmentation in the twentieth century. Trade between East and West across the Iron Curtain (a symbolical and physical barrier dividing Europe into two distinct areas) was restricted, but the severity of these restrictions varied over time. We quantify the trade and welfare effects of the Iron Curtain and show how the difficulty of trading across the Iron Curtain fluctuated throughout the Cold War. Using a novel dataset on trade between the two economic blocs and a quantitative trade model, we find that while the Iron Curtain at its height represented a tariff equivalent of 48% in 1951, trade between East and West gradually became easier until the fall of the Berlin Wall in 1989. Despite the easing of trade restrictions, we estimate that the Iron Curtain roughly halved East-West trade flows and caused substantial welfare losses in the Eastern bloc countries that persisted until the end of the Cold War. Conversely, the Iron Curtain led to an increase in intra-bloc trade, especially in the Eastern bloc, which outpaced the integration of Western Europe in the run-up to the formation of the European Union.","sentences":["The Cold War was the defining episode of geopolitical fragmentation in the twentieth century.","Trade between East and West across the Iron Curtain (a symbolical and physical barrier dividing Europe into two distinct areas) was restricted, but the severity of these restrictions varied over time.","We quantify the trade and welfare effects of the Iron Curtain and show how the difficulty of trading across the Iron Curtain fluctuated throughout the Cold War.","Using a novel dataset on trade between the two economic blocs and a quantitative trade model, we find that while the Iron Curtain at its height represented a tariff equivalent of 48% in 1951, trade between East and West gradually became easier until the fall of the Berlin Wall in 1989.","Despite the easing of trade restrictions, we estimate that the Iron Curtain roughly halved East-West trade flows and caused substantial welfare losses in the Eastern bloc countries that persisted until the end of the Cold War.","Conversely, the Iron Curtain led to an increase in intra-bloc trade, especially in the Eastern bloc, which outpaced the integration of Western Europe in the run-up to the formation of the European Union."],"url":"http://arxiv.org/abs/2404.03508v1","category":"econ.GN"}
{"created":"2024-04-04 15:10:24","title":"DQ-DETR: DETR with Dynamic Query for Tiny Object Detection","abstract":"Despite previous DETR-like methods having performed successfully in generic object detection, tiny object detection is still a challenging task for them since the positional information of object queries is not customized for detecting tiny objects, whose scale is extraordinarily smaller than general objects. Also, DETR-like methods using a fixed number of queries make them unsuitable for aerial datasets, which only contain tiny objects, and the numbers of instances are imbalanced between different images. Thus, we present a simple yet effective model, named DQ-DETR, which consists of three different components: categorical counting module, counting-guided feature enhancement, and dynamic query selection to solve the above-mentioned problems. DQ-DETR uses the prediction and density maps from the categorical counting module to dynamically adjust the number of object queries and improve the positional information of queries. Our model DQ-DETR outperforms previous CNN-based and DETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2 dataset, which mostly consists of tiny objects.","sentences":["Despite previous DETR-like methods having performed successfully in generic object detection, tiny object detection is still a challenging task for them since the positional information of object queries is not customized for detecting tiny objects, whose scale is extraordinarily smaller than general objects.","Also, DETR-like methods using a fixed number of queries make them unsuitable for aerial datasets, which only contain tiny objects, and the numbers of instances are imbalanced between different images.","Thus, we present a simple yet effective model, named DQ-DETR, which consists of three different components: categorical counting module, counting-guided feature enhancement, and dynamic query selection to solve the above-mentioned problems.","DQ-DETR uses the prediction and density maps from the categorical counting module to dynamically adjust the number of object queries and improve the positional information of queries.","Our model DQ-DETR outperforms previous CNN-based and DETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2 dataset, which mostly consists of tiny objects."],"url":"http://arxiv.org/abs/2404.03507v1","category":"cs.CV"}
{"created":"2024-04-04 15:10:13","title":"CountARFactuals -- Generating plausible model-agnostic counterfactual explanations with adversarial random forests","abstract":"Counterfactual explanations elucidate algorithmic decisions by pointing to scenarios that would have led to an alternative, desired outcome. Giving insight into the model's behavior, they hint users towards possible actions and give grounds for contesting decisions. As a crucial factor in achieving these goals, counterfactuals must be plausible, i.e., describing realistic alternative scenarios within the data manifold. This paper leverages a recently developed generative modeling technique -- adversarial random forests (ARFs) -- to efficiently generate plausible counterfactuals in a model-agnostic way. ARFs can serve as a plausibility measure or directly generate counterfactual explanations. Our ARF-based approach surpasses the limitations of existing methods that aim to generate plausible counterfactual explanations: It is easy to train and computationally highly efficient, handles continuous and categorical data naturally, and allows integrating additional desiderata such as sparsity in a straightforward manner.","sentences":["Counterfactual explanations elucidate algorithmic decisions by pointing to scenarios that would have led to an alternative, desired outcome.","Giving insight into the model's behavior, they hint users towards possible actions and give grounds for contesting decisions.","As a crucial factor in achieving these goals, counterfactuals must be plausible, i.e., describing realistic alternative scenarios within the data manifold.","This paper leverages a recently developed generative modeling technique -- adversarial random forests (ARFs) -- to efficiently generate plausible counterfactuals in a model-agnostic way.","ARFs can serve as a plausibility measure or directly generate counterfactual explanations.","Our ARF-based approach surpasses the limitations of existing methods that aim to generate plausible counterfactual explanations: It is easy to train and computationally highly efficient, handles continuous and categorical data naturally, and allows integrating additional desiderata such as sparsity in a straightforward manner."],"url":"http://arxiv.org/abs/2404.03506v1","category":"stat.ML"}
{"created":"2024-04-04 15:06:23","title":"AI and the Problem of Knowledge Collapse","abstract":"While artificial intelligence has the potential to process vast amounts of data, generate new insights, and unlock greater productivity, its widespread adoption may entail unforeseen consequences. We identify conditions under which AI, by reducing the cost of access to certain modes of knowledge, can paradoxically harm public understanding. While large language models are trained on vast amounts of diverse data, they naturally generate output towards the 'center' of the distribution. This is generally useful, but widespread reliance on recursive AI systems could lead to a process we define as \"knowledge collapse\", and argue this could harm innovation and the richness of human understanding and culture. However, unlike AI models that cannot choose what data they are trained on, humans may strategically seek out diverse forms of knowledge if they perceive them to be worthwhile. To investigate this, we provide a simple model in which a community of learners or innovators choose to use traditional methods or to rely on a discounted AI-assisted process and identify conditions under which knowledge collapse occurs. In our default model, a 20% discount on AI-generated content generates public beliefs 2.3 times further from the truth than when there is no discount. Finally, based on the results, we consider further research directions to counteract such outcomes.","sentences":["While artificial intelligence has the potential to process vast amounts of data, generate new insights, and unlock greater productivity, its widespread adoption may entail unforeseen consequences.","We identify conditions under which AI, by reducing the cost of access to certain modes of knowledge, can paradoxically harm public understanding.","While large language models are trained on vast amounts of diverse data, they naturally generate output towards the 'center' of the distribution.","This is generally useful, but widespread reliance on recursive AI systems could lead to a process we define as \"knowledge collapse\", and argue this could harm innovation and the richness of human understanding and culture.","However, unlike AI models that cannot choose what data they are trained on, humans may strategically seek out diverse forms of knowledge if they perceive them to be worthwhile.","To investigate this, we provide a simple model in which a community of learners or innovators choose to use traditional methods or to rely on a discounted AI-assisted process and identify conditions under which knowledge collapse occurs.","In our default model, a 20% discount on AI-generated content generates public beliefs 2.3 times further from the truth than when there is no discount.","Finally, based on the results, we consider further research directions to counteract such outcomes."],"url":"http://arxiv.org/abs/2404.03502v1","category":"cs.AI"}
{"created":"2024-04-04 14:57:32","title":"Comprehensible Artificial Intelligence on Knowledge Graphs: A survey","abstract":"Artificial Intelligence applications gradually move outside the safe walls of research labs and invade our daily lives. This is also true for Machine Learning methods on Knowledge Graphs, which has led to a steady increase in their application since the beginning of the 21st century. However, in many applications, users require an explanation of the Artificial Intelligences decision. This led to increased demand for Comprehensible Artificial Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible Artificial Intelligence, due to their ability to display connected data, i.e. knowledge, in a human- as well as machine-readable way. This survey gives a short history to Comprehensible Artificial Intelligence on Knowledge Graphs. Furthermore, we contribute by arguing that the concept Explainable Artificial Intelligence is overloaded and overlapping with Interpretable Machine Learning. By introducing the parent concept Comprehensible Artificial Intelligence, we provide a clear-cut distinction of both concepts while accounting for their similarities. Thus, we provide in this survey a case for Comprehensible Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine Learning on Knowledge Graphs and Explainable Artificial Intelligence on Knowledge Graphs. This leads to the introduction of a novel taxonomy for Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a comprehensive overview of the research on Comprehensible Artificial Intelligence on Knowledge Graphs is presented and put into the context of the taxonomy. Finally, research gaps in the field of Comprehensible Artificial Intelligence on Knowledge Graphs are identified for future research.","sentences":["Artificial Intelligence applications gradually move outside the safe walls of research labs and invade our daily lives.","This is also true for Machine Learning methods on Knowledge Graphs, which has led to a steady increase in their application since the beginning of the 21st century.","However, in many applications, users require an explanation of the Artificial Intelligences decision.","This led to increased demand for Comprehensible Artificial Intelligence.","Knowledge Graphs epitomize fertile soil for Comprehensible Artificial Intelligence, due to their ability to display connected data, i.e. knowledge, in a human- as well as machine-readable way.","This survey gives a short history to Comprehensible Artificial Intelligence on Knowledge Graphs.","Furthermore, we contribute by arguing that the concept Explainable Artificial Intelligence is overloaded and overlapping with Interpretable Machine Learning.","By introducing the parent concept Comprehensible Artificial Intelligence, we provide a clear-cut distinction of both concepts while accounting for their similarities.","Thus, we provide in this survey a case for Comprehensible Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine Learning on Knowledge Graphs and Explainable Artificial Intelligence on Knowledge Graphs.","This leads to the introduction of a novel taxonomy for Comprehensible Artificial Intelligence on Knowledge Graphs.","In addition, a comprehensive overview of the research on Comprehensible Artificial Intelligence on Knowledge Graphs is presented and put into the context of the taxonomy.","Finally, research gaps in the field of Comprehensible Artificial Intelligence on Knowledge Graphs are identified for future research."],"url":"http://arxiv.org/abs/2404.03499v1","category":"cs.AI"}
{"created":"2024-04-04 14:53:17","title":"Self-Testing Graph States Permitting Bounded Classical Communication","abstract":"Self-testing identifies quantum states and correlations that exhibit non-locality, distinguishing them, up to local transformations, from other quantum states. Due to their strong non-locality, all graph states can be self-tested with strictly local measurement devices. Moreover, graph states display non-local correlations even when bounded classical communication on the underlying graph is permitted, a feature that has found applications in proving a circuit-depth separation between classical and quantum computing. In the framework of bounded classical communication, we show that certain graph states with appropriate symmetry can be robustly self-tested, by providing an explicit self-test for the circular graph state and the honeycomb cluster state. Since communication generally obstructs self-testing of graph states, we further provide a procedure to robustly self-test any graph state from larger ones that exhibit non-local correlations in the communication scenario. Furthermore, in the standard setup without classical communication, we demonstrate that any graph state from an underlying connected graph with at least three vertices can be robustly self-tested using only Pauli measurements.","sentences":["Self-testing identifies quantum states and correlations that exhibit non-locality, distinguishing them, up to local transformations, from other quantum states.","Due to their strong non-locality, all graph states can be self-tested with strictly local measurement devices.","Moreover, graph states display non-local correlations even when bounded classical communication on the underlying graph is permitted, a feature that has found applications in proving a circuit-depth separation between classical and quantum computing.","In the framework of bounded classical communication, we show that certain graph states with appropriate symmetry can be robustly self-tested, by providing an explicit self-test for the circular graph state and the honeycomb cluster state.","Since communication generally obstructs self-testing of graph states, we further provide a procedure to robustly self-test any graph state from larger ones that exhibit non-local correlations in the communication scenario.","Furthermore, in the standard setup without classical communication, we demonstrate that any graph state from an underlying connected graph with at least three vertices can be robustly self-tested using only Pauli measurements."],"url":"http://arxiv.org/abs/2404.03496v1","category":"quant-ph"}
{"created":"2024-04-04 14:49:47","title":"A topological reading of inductive and coinductive definitions in Dependent Type Theory","abstract":"In the context of dependent type theory, we show that coinductive predicates have an equivalent topological counterpart in terms of coinductively generated positivity relations, introduced by G. Sambin to represent closed subsets in point-free topology. Our work is complementary to a previous one with M.E. Maietti, where we showed that, in dependent type theory, the well-known concept of wellfounded trees has a topological equivalent counterpart in terms of proof-relevant inductively generated formal covers used to provide a predicative and constructive representation of complete suplattices. All proofs in Martin-L\\\"of's type theory are formalised in the Agda proof assistant.","sentences":["In the context of dependent type theory, we show that coinductive predicates have an equivalent topological counterpart in terms of coinductively generated positivity relations, introduced by G. Sambin to represent closed subsets in point-free topology.","Our work is complementary to a previous one with M.E. Maietti, where we showed that, in dependent type theory, the well-known concept of wellfounded trees has a topological equivalent counterpart in terms of proof-relevant inductively generated formal covers used to provide a predicative and constructive representation of complete suplattices.","All proofs in Martin-L\\\"of's type theory are formalised in the Agda proof assistant."],"url":"http://arxiv.org/abs/2404.03494v1","category":"math.LO"}
{"created":"2024-04-04 14:48:26","title":"A Methodology to Study the Impact of Spiking Neural Network Parameters considering Event-Based Automotive Data","abstract":"Autonomous Driving (AD) systems are considered as the future of human mobility and transportation. Solving computer vision tasks such as image classification and object detection/segmentation, with high accuracy and low power/energy consumption, is highly needed to realize AD systems in real life. These requirements can potentially be satisfied by Spiking Neural Networks (SNNs). However, the state-of-the-art works in SNN-based AD systems still focus on proposing network models that can achieve high accuracy, and they have not systematically studied the roles of SNN parameters when used for learning event-based automotive data. Therefore, we still lack understanding of how to effectively develop SNN models for AD systems. Toward this, we propose a novel methodology to systematically study and analyze the impact of SNN parameters considering event-based automotive data, then leverage this analysis for enhancing SNN developments. To do this, we first explore different settings of SNN parameters that directly affect the learning mechanism (i.e., batch size, learning rate, neuron threshold potential, and weight decay), then analyze the accuracy results. Afterward, we propose techniques that jointly improve SNN accuracy and reduce training time. Experimental results show that our methodology can improve the SNN models for AD systems than the state-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARS dataset, and it can also achieve iso-accuracy (i.e., ~85% with standard deviation less than 0.5%) while speeding up the training time by 1.9x. In this manner, our research work provides a set of guidelines for SNN parameter enhancements, thereby enabling the practical developments of SNN-based AD systems.","sentences":["Autonomous Driving (AD) systems are considered as the future of human mobility and transportation.","Solving computer vision tasks such as image classification and object detection/segmentation, with high accuracy and low power/energy consumption, is highly needed to realize AD systems in real life.","These requirements can potentially be satisfied by Spiking Neural Networks (SNNs).","However, the state-of-the-art works in SNN-based AD systems still focus on proposing network models that can achieve high accuracy, and they have not systematically studied the roles of SNN parameters when used for learning event-based automotive data.","Therefore, we still lack understanding of how to effectively develop SNN models for AD systems.","Toward this, we propose a novel methodology to systematically study and analyze the impact of SNN parameters considering event-based automotive data, then leverage this analysis for enhancing SNN developments.","To do this, we first explore different settings of SNN parameters that directly affect the learning mechanism (i.e., batch size, learning rate, neuron threshold potential, and weight decay), then analyze the accuracy results.","Afterward, we propose techniques that jointly improve SNN accuracy and reduce training time.","Experimental results show that our methodology can improve the SNN models for AD systems than the state-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARS dataset, and it can also achieve iso-accuracy (i.e., ~85% with standard deviation less than 0.5%) while speeding up the training time by 1.9x.","In this manner, our research work provides a set of guidelines for SNN parameter enhancements, thereby enabling the practical developments of SNN-based AD systems."],"url":"http://arxiv.org/abs/2404.03493v1","category":"cs.NE"}
{"created":"2024-04-04 14:45:28","title":"Three perspectives on entropy dynamics in a non-Hermitian two-state system","abstract":"A comparative study of entropy dynamics as an indicator of physical behavior in an open two-state system with balanced gain and loss is presented. We distinguish the perspective taken in utilizing the conventional framework of Hermitian-adjoint states from an approach that is based on biorthogonal-adjoint states and a third case based on an isospectral mapping. In this it is demonstrated that their differences are rooted in the treatment of the environmental coupling mode. For unbroken $\\mathcal{PT}$ symmetry of the system, a notable characteristic feature of the perspective taken is the presence or absence of purity oscillations, with an associated entropy revival. The description of the system is then continued from its $\\mathcal{PT}$-symmetric pseudo-Hermitian phase into the regime of spontaneously broken symmetry, in the latter two approaches through a non-analytic operator-based continuation, yielding a Lindblad master equation based on the $\\mathcal{PT}$ charge operator $\\mathcal{C}$. This phase transition indicates a general connection between the pseudo-Hermitian closed-system and the Lindbladian open-system formalism through a spontaneous breakdown of the underlying physical reflection symmetry.","sentences":["A comparative study of entropy dynamics as an indicator of physical behavior in an open two-state system with balanced gain and loss is presented.","We distinguish the perspective taken in utilizing the conventional framework of Hermitian-adjoint states from an approach that is based on biorthogonal-adjoint states and a third case based on an isospectral mapping.","In this it is demonstrated that their differences are rooted in the treatment of the environmental coupling mode.","For unbroken $\\mathcal{PT}$ symmetry of the system, a notable characteristic feature of the perspective taken is the presence or absence of purity oscillations, with an associated entropy revival.","The description of the system is then continued from its $\\mathcal{PT}$-symmetric pseudo-Hermitian phase into the regime of spontaneously broken symmetry, in the latter two approaches through a non-analytic operator-based continuation, yielding a Lindblad master equation based on the $\\mathcal{PT}$ charge operator $\\mathcal{C}$. This phase transition indicates a general connection between the pseudo-Hermitian closed-system and the Lindbladian open-system formalism through a spontaneous breakdown of the underlying physical reflection symmetry."],"url":"http://arxiv.org/abs/2404.03492v1","category":"quant-ph"}
{"created":"2024-04-04 14:45:26","title":"A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation","abstract":"Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.","sentences":["Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations.","However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses.","Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination.","Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue.","As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks.","In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods.","Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction.","Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models.","We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems."],"url":"http://arxiv.org/abs/2404.03491v1","category":"cs.CL"}
{"created":"2024-04-04 14:44:15","title":"SUSHI: An algorithm for source separation of hyperspectral images with non-stationary spectral variation","abstract":"Hyperspectral images are data cubes with two spatial dimensions and a third spectral dimension, providing a spectrum for each pixel, and thus allow the mapping of extended sources' physical properties. In this article, we present the Semi-blind Unmixing with Sparsity for Hyperspectral Images (SUSHI), an algorithm for non-stationary unmixing of hyperspectral images with spatial regularization of spectral parameters. The method allows for the disentangling of physical components without the assumption of a unique spectrum for each component. Thus, unlike most source separation methods used in astrophysics, all physical components obtained by SUSHI vary in spectral shape and in amplitude across the data cube.   Non-stationary source separation is an ill-posed inverse problem that needs to be constrained. We achieve this by training a spectral model and applying a spatial regularization constraint on its parameters. For the spectral model, we used an Interpolatory Auto-Encoder, a generative model that can be trained with limited samples. For spatial regularization, we applied a sparsity constraint on the wavelet transform of the model parameter maps.   We applied SUSHI to a toy model meant to resemble supernova remnants in X-ray astrophysics, though the method may be used on any extended source with any hyperspectral instrument. We compared this result to the one obtained by a classic 1D fit on each individual pixel. We find that SUSHI obtains more accurate results, particularly when it comes to reconstructing physical parameters. We applied SUSHI to real X-ray data from the supernova remnant Cassiopeia A and to the Crab Nebula. The results obtained are realistic and in accordance with past findings but have a much better spatial resolution. Thanks to spatial regularization, SUSHI can obtain reliable physical parameters at fine scales that are out of reach for pixel-by-pixel methods.","sentences":["Hyperspectral images are data cubes with two spatial dimensions and a third spectral dimension, providing a spectrum for each pixel, and thus allow the mapping of extended sources' physical properties.","In this article, we present the Semi-blind Unmixing with Sparsity for Hyperspectral Images (SUSHI), an algorithm for non-stationary unmixing of hyperspectral images with spatial regularization of spectral parameters.","The method allows for the disentangling of physical components without the assumption of a unique spectrum for each component.","Thus, unlike most source separation methods used in astrophysics, all physical components obtained by SUSHI vary in spectral shape and in amplitude across the data cube.   ","Non-stationary source separation is an ill-posed inverse problem that needs to be constrained.","We achieve this by training a spectral model and applying a spatial regularization constraint on its parameters.","For the spectral model, we used an Interpolatory Auto-Encoder, a generative model that can be trained with limited samples.","For spatial regularization, we applied a sparsity constraint on the wavelet transform of the model parameter maps.   ","We applied SUSHI to a toy model meant to resemble supernova remnants in X-ray astrophysics, though the method may be used on any extended source with any hyperspectral instrument.","We compared this result to the one obtained by a classic 1D fit on each individual pixel.","We find that SUSHI obtains more accurate results, particularly when it comes to reconstructing physical parameters.","We applied SUSHI to real X-ray data from the supernova remnant Cassiopeia A and to the Crab Nebula.","The results obtained are realistic and in accordance with past findings but have a much better spatial resolution.","Thanks to spatial regularization, SUSHI can obtain reliable physical parameters at fine scales that are out of reach for pixel-by-pixel methods."],"url":"http://arxiv.org/abs/2404.03490v1","category":"astro-ph.IM"}
{"created":"2024-04-04 14:40:07","title":"Generative AI and Teachers -- For Us or Against Us? A Case Study","abstract":"We present insightful results of a survey on the adoption of generative artificial intelligence (GenAI) by university teachers in their teaching activities. The transformation of education by GenAI, particularly large language models (LLMs), has been presenting both opportunities and challenges, including cheating by students. We prepared the online survey according to best practices and the questions were created by the authors, who have pedagogy experience. The survey contained 12 questions and a pilot study was first conducted. The survey was then sent to all teachers in multiple departments across different campuses of the university of interest in Sweden: Lule{\\aa} University of Technology. The survey was available in both Swedish and English. The results show that 35 teachers (more than half) use GenAI out of 67 respondents. Preparation is the teaching activity with the most frequency that GenAI is used for and ChatGPT is the most commonly used GenAI. 59% say it has impacted their teaching, however, 55% say there should be legislation around the use of GenAI, especially as inaccuracies and cheating are the biggest concerns.","sentences":["We present insightful results of a survey on the adoption of generative artificial intelligence (GenAI) by university teachers in their teaching activities.","The transformation of education by GenAI, particularly large language models (LLMs), has been presenting both opportunities and challenges, including cheating by students.","We prepared the online survey according to best practices and the questions were created by the authors, who have pedagogy experience.","The survey contained 12 questions and a pilot study was first conducted.","The survey was then sent to all teachers in multiple departments across different campuses of the university of interest in Sweden: Lule{\\aa} University of Technology.","The survey was available in both Swedish and English.","The results show that 35 teachers (more than half) use GenAI out of 67 respondents.","Preparation is the teaching activity with the most frequency that GenAI is used for and ChatGPT is the most commonly used GenAI.","59% say it has impacted their teaching, however, 55% say there should be legislation around the use of GenAI, especially as inaccuracies and cheating are the biggest concerns."],"url":"http://arxiv.org/abs/2404.03486v1","category":"cs.CL"}
{"created":"2024-04-04 14:35:49","title":"AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale","abstract":"Active Visual Exploration (AVE) is a task that involves dynamically selecting observations (glimpses), which is critical to facilitate comprehension and navigation within an environment. While modern AVE methods have demonstrated impressive performance, they are constrained to fixed-scale glimpses from rigid grids. In contrast, existing mobile platforms equipped with optical zoom capabilities can capture glimpses of arbitrary positions and scales. To address this gap between software and hardware capabilities, we introduce AdaGlimpse. It uses Soft Actor-Critic, a reinforcement learning algorithm tailored for exploration tasks, to select glimpses of arbitrary position and scale. This approach enables our model to rapidly establish a general awareness of the environment before zooming in for detailed analysis. Experimental results demonstrate that AdaGlimpse surpasses previous methods across various visual tasks while maintaining greater applicability in realistic AVE scenarios.","sentences":["Active Visual Exploration (AVE) is a task that involves dynamically selecting observations (glimpses), which is critical to facilitate comprehension and navigation within an environment.","While modern AVE methods have demonstrated impressive performance, they are constrained to fixed-scale glimpses from rigid grids.","In contrast, existing mobile platforms equipped with optical zoom capabilities can capture glimpses of arbitrary positions and scales.","To address this gap between software and hardware capabilities, we introduce AdaGlimpse.","It uses Soft Actor-Critic, a reinforcement learning algorithm tailored for exploration tasks, to select glimpses of arbitrary position and scale.","This approach enables our model to rapidly establish a general awareness of the environment before zooming in for detailed analysis.","Experimental results demonstrate that AdaGlimpse surpasses previous methods across various visual tasks while maintaining greater applicability in realistic AVE scenarios."],"url":"http://arxiv.org/abs/2404.03482v1","category":"cs.CV"}
{"created":"2024-04-04 14:34:42","title":"Escape from a metastable state in non-Markovian population dynamics","abstract":"We study the long-time dynamics in non-Markovian single-population stochastic models, where one or more reactions are modelled as a stochastic process with a fat-tailed non-exponential distribution of waiting times, mimicking long-term memory. We focus on three prototypical examples: genetic switching, population establishment and population extinction, all with non-exponential production rates. The system is studied in two regimes. In the first, the distribution of waiting times has a finite mean. Here, the system approaches a (quasi)stationary steady state at long times, and we develop a general WKB approach for these non-Markovian systems. We derive explicit results for the mean population size and mean escape time from the metastable state of the stochastic dynamics. In this realm, we reveal that for sufficiently strong memory, a memory-induced (meta)stable state can emerge in the system. In the second regime, the waiting time distribution is assumed to have an infinite mean. Here, for bistable systems we find two distinct scaling regimes, separated by an exponentially long time which may strongly depend on the initial conditions of the system.","sentences":["We study the long-time dynamics in non-Markovian single-population stochastic models, where one or more reactions are modelled as a stochastic process with a fat-tailed non-exponential distribution of waiting times, mimicking long-term memory.","We focus on three prototypical examples: genetic switching, population establishment and population extinction, all with non-exponential production rates.","The system is studied in two regimes.","In the first, the distribution of waiting times has a finite mean.","Here, the system approaches a (quasi)stationary steady state at long times, and we develop a general WKB approach for these non-Markovian systems.","We derive explicit results for the mean population size and mean escape time from the metastable state of the stochastic dynamics.","In this realm, we reveal that for sufficiently strong memory, a memory-induced (meta)stable state can emerge in the system.","In the second regime, the waiting time distribution is assumed to have an infinite mean.","Here, for bistable systems we find two distinct scaling regimes, separated by an exponentially long time which may strongly depend on the initial conditions of the system."],"url":"http://arxiv.org/abs/2404.03481v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-04 14:32:16","title":"Gibbs-preserving operations requiring infinite amount of quantum coherence","abstract":"Gibbs-preserving operations have been studied as one of the standard free processes in quantum thermodynamics. Although they admit a simple mathematical structure, their operational significance has been unclear due to the potential hidden cost to implement them using an operatioanlly motivated class of operations, such as thermal operations. Here, we show that this hidden cost can be infinite -- we present a family of Gibbs-preserving operations that cannot be implemented by thermal operations aided by any finite amount of quantum coherence. Our result implies that there are uncountably many Gibbs-preserving operations that require unbounded thermodynamic resources to implement, raising a question about employing Gibbs-preserving operations as available thermodynamics processes. This finding is a consequence of the general lower bounds we provide for the coherence cost of approximately implementing a certain class of Gibbs-preserving operations with a desired accuracy. We find that our lower bound is almost tight, identifying a quantity -- related to the energy change caused by the channel to implement -- as a fundamental quantifier characterizing the coherence cost for the approximate implementation of Gibbs-preserving operations.","sentences":["Gibbs-preserving operations have been studied as one of the standard free processes in quantum thermodynamics.","Although they admit a simple mathematical structure, their operational significance has been unclear due to the potential hidden cost to implement them using an operatioanlly motivated class of operations, such as thermal operations.","Here, we show that this hidden cost can be infinite -- we present a family of Gibbs-preserving operations that cannot be implemented by thermal operations aided by any finite amount of quantum coherence.","Our result implies that there are uncountably many Gibbs-preserving operations that require unbounded thermodynamic resources to implement, raising a question about employing Gibbs-preserving operations as available thermodynamics processes.","This finding is a consequence of the general lower bounds we provide for the coherence cost of approximately implementing a certain class of Gibbs-preserving operations with a desired accuracy.","We find that our lower bound is almost tight, identifying a quantity -- related to the energy change caused by the channel to implement -- as a fundamental quantifier characterizing the coherence cost for the approximate implementation of Gibbs-preserving operations."],"url":"http://arxiv.org/abs/2404.03479v1","category":"quant-ph"}
{"created":"2024-04-04 14:30:49","title":"Resolving Gilbert's Conjecture: Dimensional Dependencies in Hardy Spaces Valued in Clifford Modules","abstract":"This article provides a thorough investigation into Gilbert's Conjecture, pertaining to Hardy spaces in the upper half-space valued in Clifford modules. We explore the conjecture proposed by Gilbert in 1991, which seeks to extend the classical principle of representing real $L^p$ functions on the real line as boundary values of Hardy holomorphic functions to higher-dimensional Euclidean spaces valued in any Clifford module. We present a complete resolution to this conjecture, demonstrating that its validity is contingent upon the dimension $n$, specifically holding true when \\(n \\not\\equiv 6, 7 \\mod 8\\) and failing otherwise. The pivotal discovery that Gilbert's conjecture can be reformulated as a set of algebraic conditions is underscored in this work. To navigate these conditions, we employ a novel strategy that leverages the octonions, revealing their instrumental role in addressing issues related to Clifford modules and spinors. This innovative approach not only provides explicit realization through the generalization of the Hilbert transform to the Riesz transform but also establishes a significant advancement in the understanding of Hardy spaces within higher dimensions.","sentences":["This article provides a thorough investigation into Gilbert's Conjecture, pertaining to Hardy spaces in the upper half-space valued in Clifford modules.","We explore the conjecture proposed by Gilbert in 1991, which seeks to extend the classical principle of representing real $L^p$ functions on the real line as boundary values of Hardy holomorphic functions to higher-dimensional Euclidean spaces valued in any Clifford module.","We present a complete resolution to this conjecture, demonstrating that its validity is contingent upon the dimension $n$, specifically holding true when \\(n \\not\\equiv 6, 7 \\mod 8\\) and failing otherwise.","The pivotal discovery that Gilbert's conjecture can be reformulated as a set of algebraic conditions is underscored in this work.","To navigate these conditions, we employ a novel strategy that leverages the octonions, revealing their instrumental role in addressing issues related to Clifford modules and spinors.","This innovative approach not only provides explicit realization through the generalization of the Hilbert transform to the Riesz transform but also establishes a significant advancement in the understanding of Hardy spaces within higher dimensions."],"url":"http://arxiv.org/abs/2404.03478v1","category":"math.CV"}
{"created":"2024-04-04 14:28:34","title":"Towards Automated Movie Trailer Generation","abstract":"Movie trailers are an essential tool for promoting films and attracting audiences. However, the process of creating trailers can be time-consuming and expensive. To streamline this process, we propose an automatic trailer generation framework that generates plausible trailers from a full movie by automating shot selection and composition. Our approach draws inspiration from machine translation techniques and models the movies and trailers as sequences of shots, thus formulating the trailer generation problem as a sequence-to-sequence task. We introduce Trailer Generation Transformer (TGT), a deep-learning framework utilizing an encoder-decoder architecture. TGT movie encoder is tasked with contextualizing each movie shot representation via self-attention, while the autoregressive trailer decoder predicts the feature representation of the next trailer shot, accounting for the relevance of shots' temporal order in trailers. Our TGT significantly outperforms previous methods on a comprehensive suite of metrics.","sentences":["Movie trailers are an essential tool for promoting films and attracting audiences.","However, the process of creating trailers can be time-consuming and expensive.","To streamline this process, we propose an automatic trailer generation framework that generates plausible trailers from a full movie by automating shot selection and composition.","Our approach draws inspiration from machine translation techniques and models the movies and trailers as sequences of shots, thus formulating the trailer generation problem as a sequence-to-sequence task.","We introduce Trailer Generation Transformer (TGT), a deep-learning framework utilizing an encoder-decoder architecture.","TGT movie encoder is tasked with contextualizing each movie shot representation via self-attention, while the autoregressive trailer decoder predicts the feature representation of the next trailer shot, accounting for the relevance of shots' temporal order in trailers.","Our TGT significantly outperforms previous methods on a comprehensive suite of metrics."],"url":"http://arxiv.org/abs/2404.03477v1","category":"cs.CV"}
{"created":"2024-04-04 14:28:05","title":"A Reduction from Multi-Parameter to Single-Parameter Bayesian Contract Design","abstract":"The main result of this paper is an almost approximation-preserving polynomial-time reduction from the most general multi-parameter Bayesian contract design (BCD) to single-parameter BCD. That is, for any multi-parameter BCD instance $I^M$, we construct a single-parameter instance $I^S$ such that any $\\beta$-approximate contract (resp. menu of contracts) of $I^S$ can in turn be converted to a $(\\beta -\\epsilon)$-approximate contract (resp. menu of contracts) of $I^M$. The reduction is in time polynomial in the input size and $\\log(\\frac{1}{\\epsilon})$; moreover, when $\\beta = 1$ (i.e., the given single-parameter solution is exactly optimal), the dependence on $\\frac{1}{\\epsilon}$ can be removed, leading to a polynomial-time exact reduction. This efficient reduction is somewhat surprising because in the closely related problem of Bayesian mechanism design, a polynomial-time reduction from multi-parameter to single-parameter setting is believed to not exist. Our result demonstrates the intrinsic difficulty of addressing moral hazard in Bayesian contract design, regardless of being single-parameter or multi-parameter.   As byproducts, our reduction answers two open questions in recent literature of algorithmic contract design: (a) it implies that optimal contract design in single-parameter BCD is not in APX unless P=NP even when the agent's type distribution is regular, answering the open question of [Alon et al. 2021] in the negative; (b) it implies that the principal's (order-wise) tight utility gap between using a menu of contracts and a single contract is $\\Theta(n)$ where $n$ is the number of actions, answering the major open question of [Guruganesh et al. 2021] for the single-parameter case.","sentences":["The main result of this paper is an almost approximation-preserving polynomial-time reduction from the most general multi-parameter Bayesian contract design (BCD) to single-parameter BCD.","That is, for any multi-parameter BCD instance $I^M$, we construct a single-parameter instance $I^S$ such that any $\\beta$-approximate contract (resp.","menu of contracts) of $I^S$ can in turn be converted to a $(\\beta -\\epsilon)$-approximate contract (resp.","menu of contracts) of $I^M$. The reduction is in time polynomial in the input size and $\\log(\\frac{1}{\\epsilon})$; moreover, when $\\beta = 1$ (i.e., the given single-parameter solution is exactly optimal), the dependence on $\\frac{1}{\\epsilon}$ can be removed, leading to a polynomial-time exact reduction.","This efficient reduction is somewhat surprising because in the closely related problem of Bayesian mechanism design, a polynomial-time reduction from multi-parameter to single-parameter setting is believed to not exist.","Our result demonstrates the intrinsic difficulty of addressing moral hazard in Bayesian contract design, regardless of being single-parameter or multi-parameter.   ","As byproducts, our reduction answers two open questions in recent literature of algorithmic contract design: (a) it implies that optimal contract design in single-parameter BCD is not in APX unless P=NP even when the agent's type distribution is regular, answering the open question of [Alon et al. 2021] in the negative; (b) it implies that the principal's (order-wise) tight utility gap between using a menu of contracts and a single contract is $\\Theta(n)$ where $n$ is the number of actions, answering the major open question of [Guruganesh et al. 2021] for the single-parameter case."],"url":"http://arxiv.org/abs/2404.03476v1","category":"cs.GT"}
{"created":"2024-04-04 14:27:57","title":"Topology and monoid representations II: left regular bands of groups and Hsiao's monoid of ordered $G$-partitions","abstract":"The goal of this paper is to use topological methods to compute $\\mathrm{Ext}$ between irreducible representations of von Neumann regular monoids in which Green's $\\mathscr L$- and $\\mathscr J$-relations coincide (e.g., left regular bands). Our results subsume those of S.~Margolis, F.~Saliola, and B.~Steinberg, \\emph{Combinatorial topology and the global dimension of algebras arising in combinatorics}, J. Eur. Math. Soc. (JEMS), \\textbf{17}, 3037--3080 (2015).   Applications include computing $\\mathrm{Ext}$ between arbitrary simple modules and computing a quiver presentation for the algebra of Hsiao's monoid of ordered $G$-partitions (connected to the Mantaci-Reutenauer descent algebra for the wreath product $G\\wr S_n$). We show that this algebra is Koszul, compute its Koszul dual and compute minimal projective resolutions of all the simple modules using topology. More generally, these results work for CW left regular bands of abelian groups. These results generalize the results of S.~Margolis, F.~V. Saliola, and B.~Steinberg. \\emph{Cell complexes, poset topology and the representation theory of algebras arising in algebraic combinatorics and discrete geometry}, Mem. Amer. Math. Soc., \\textbf{274}, 1--135, (2021).","sentences":["The goal of this paper is to use topological methods to compute $\\mathrm{Ext}$ between irreducible representations of von Neumann regular monoids in which Green's $\\mathscr L$- and $\\mathscr J$-relations coincide (e.g., left regular bands).","Our results subsume those of S.~Margolis, F.~Saliola, and B.~Steinberg, \\emph{Combinatorial topology and the global dimension of algebras arising in combinatorics}, J. Eur.","Math.","Soc.","(JEMS), \\textbf{17}, 3037--3080 (2015).   ","Applications include computing $\\mathrm{Ext}$ between arbitrary simple modules and computing a quiver presentation for the algebra of Hsiao's monoid of ordered $G$-partitions (connected to the Mantaci-Reutenauer descent algebra for the wreath product $G\\wr S_n$).","We show that this algebra is Koszul, compute its Koszul dual and compute minimal projective resolutions of all the simple modules using topology.","More generally, these results work for CW left regular bands of abelian groups.","These results generalize the results of S.~Margolis, F.~V. Saliola, and B.~Steinberg.","\\emph{Cell complexes, poset topology and the representation theory of algebras arising in algebraic combinatorics and discrete geometry}, Mem.","Amer.","Math.","Soc., \\textbf{274}, 1--135, (2021)."],"url":"http://arxiv.org/abs/2404.03475v1","category":"math.RT"}
{"created":"2024-04-04 14:26:58","title":"Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images","abstract":"With fine-grained classification, we identify unique characteristics to distinguish among classes of the same super-class. We are focusing on species recognition in Insecta, as they are critical for biodiversity monitoring and at the base of many ecosystems. With citizen science campaigns, billions of images are collected in the wild. Once these are labelled, experts can use them to create distribution maps. However, the labelling process is time-consuming, which is where computer vision comes in. The field of computer vision offers a wide range of algorithms, each with its strengths and weaknesses; how do we identify the algorithm that is in line with our application? To answer this question, we provide a full and detailed evaluation of nine algorithms among deep convolutional networks (CNN), vision transformers (ViT), and locality-based vision transformers (LBVT) on 4 different aspects: classification performance, embedding quality, computational cost, and gradient activity. We offer insights that we haven't yet had in this domain proving to which extent these algorithms solve the fine-grained tasks in Insecta. We found that the ViT performs the best on inference speed and computational cost while the LBVT outperforms the others on performance and embedding quality; the CNN provide a trade-off among the metrics.","sentences":["With fine-grained classification, we identify unique characteristics to distinguish among classes of the same super-class.","We are focusing on species recognition in Insecta, as they are critical for biodiversity monitoring and at the base of many ecosystems.","With citizen science campaigns, billions of images are collected in the wild.","Once these are labelled, experts can use them to create distribution maps.","However, the labelling process is time-consuming, which is where computer vision comes in.","The field of computer vision offers a wide range of algorithms, each with its strengths and weaknesses; how do we identify the algorithm that is in line with our application?","To answer this question, we provide a full and detailed evaluation of nine algorithms among deep convolutional networks (CNN), vision transformers (ViT), and locality-based vision transformers (LBVT) on 4 different aspects: classification performance, embedding quality, computational cost, and gradient activity.","We offer insights that we haven't yet had in this domain proving to which extent these algorithms solve the fine-grained tasks in Insecta.","We found that the ViT performs the best on inference speed and computational cost while the LBVT outperforms the others on performance and embedding quality; the CNN provide a trade-off among the metrics."],"url":"http://arxiv.org/abs/2404.03474v1","category":"cs.CV"}
{"created":"2024-04-04 14:26:47","title":"Generalization Bounds for Message Passing Networks on Mixture of Graphons","abstract":"We study the generalization capabilities of Message Passing Neural Networks (MPNNs), a prevalent class of Graph Neural Networks (GNN). We derive generalization bounds specifically for MPNNs with normalized sum aggregation and mean aggregation. Our analysis is based on a data generation model incorporating a finite set of template graphons. Each graph within this framework is generated by sampling from one of the graphons with a certain degree of perturbation. In particular, we extend previous MPNN generalization results to a more realistic setting, which includes the following modifications: 1) we analyze simple random graphs with Bernoulli-distributed edges instead of weighted graphs; 2) we sample both graphs and graph signals from perturbed graphons instead of clean graphons; and 3) we analyze sparse graphs instead of dense graphs. In this more realistic and challenging scenario, we provide a generalization bound that decreases as the average number of nodes in the graphs increases. Our results imply that MPNNs with higher complexity than the size of the training set can still generalize effectively, as long as the graphs are sufficiently large.","sentences":["We study the generalization capabilities of Message Passing Neural Networks (MPNNs), a prevalent class of Graph Neural Networks (GNN).","We derive generalization bounds specifically for MPNNs with normalized sum aggregation and mean aggregation.","Our analysis is based on a data generation model incorporating a finite set of template graphons.","Each graph within this framework is generated by sampling from one of the graphons with a certain degree of perturbation.","In particular, we extend previous MPNN generalization results to a more realistic setting, which includes the following modifications: 1) we analyze simple random graphs with Bernoulli-distributed edges instead of weighted graphs; 2) we sample both graphs and graph signals from perturbed graphons instead of clean graphons; and 3) we analyze sparse graphs instead of dense graphs.","In this more realistic and challenging scenario, we provide a generalization bound that decreases as the average number of nodes in the graphs increases.","Our results imply that MPNNs with higher complexity than the size of the training set can still generalize effectively, as long as the graphs are sufficiently large."],"url":"http://arxiv.org/abs/2404.03473v1","category":"cs.LG"}
{"created":"2024-04-04 14:24:06","title":"Reevaluating Bias Detection in Language Models: The Role of Implicit Norm","abstract":"Large language models (LLMs), trained on vast datasets, can carry biases that manifest in various forms, from overt discrimination to implicit stereotypes. One facet of bias is performance disparities in LLMs, often harming underprivileged groups, such as racial minorities. A common approach to quantifying bias is to use template-based bias probes, which explicitly state group membership (e.g. White) and evaluate if the outcome of a task, sentiment analysis for instance, is invariant to the change of group membership (e.g. change White race to Black). This approach is widely used in bias quantification. However, in this work, we find evidence of an unexpectedly overlooked consequence of using template-based probes for LLM bias quantification. We find that in doing so, text examples associated with White ethnicities appear to be classified as exhibiting negative sentiment at elevated rates. We hypothesize that the scenario arises artificially through a mismatch between the pre-training text of LLMs and the templates used to measure bias through reporting bias, unstated norms that imply group membership without explicit statement. Our finding highlights the potential misleading impact of varying group membership through explicit mention in bias quantification","sentences":["Large language models (LLMs), trained on vast datasets, can carry biases that manifest in various forms, from overt discrimination to implicit stereotypes.","One facet of bias is performance disparities in LLMs, often harming underprivileged groups, such as racial minorities.","A common approach to quantifying bias is to use template-based bias probes, which explicitly state group membership (e.g. White) and evaluate if the outcome of a task, sentiment analysis for instance, is invariant to the change of group membership (e.g. change White race to Black).","This approach is widely used in bias quantification.","However, in this work, we find evidence of an unexpectedly overlooked consequence of using template-based probes for LLM bias quantification.","We find that in doing so, text examples associated with White ethnicities appear to be classified as exhibiting negative sentiment at elevated rates.","We hypothesize that the scenario arises artificially through a mismatch between the pre-training text of LLMs and the templates used to measure bias through reporting bias, unstated norms that imply group membership without explicit statement.","Our finding highlights the potential misleading impact of varying group membership through explicit mention in bias quantification"],"url":"http://arxiv.org/abs/2404.03471v1","category":"cs.CL"}
{"created":"2024-04-04 14:22:01","title":"Exponential decay of solutions to linear evolution equations with time-dependent time delay","abstract":"In this note, we analyze an abstract evolution equation with time-dependent time delay and time-dependent delay feedback coefficient. We assume that the operator corresponding to the nondelayed part of the model generates an exponentially stable semigroup. Under an appropriate assumption on the delay feedback, we prove the well-posedness and an exponential stability estimate for our model. Applications of our abstract results to concrete models are also illustrated.","sentences":["In this note, we analyze an abstract evolution equation with time-dependent time delay and time-dependent delay feedback coefficient.","We assume that the operator corresponding to the nondelayed part of the model generates an exponentially stable semigroup.","Under an appropriate assumption on the delay feedback, we prove the well-posedness and an exponential stability estimate for our model.","Applications of our abstract results to concrete models are also illustrated."],"url":"http://arxiv.org/abs/2404.03467v1","category":"math.OC"}
{"created":"2024-04-04 14:10:21","title":"Shell-type Tidal Features Are More Frequently Detected in Slowly Rotating Early-type Galaxies than Stream- and Tail-type Features","abstract":"To enhance our understanding of the impact of galaxy mergers on the kinematics of early-type galaxies (ETGs), we examine differences in specific stellar angular momentum within the half-light radius ($\\lambda_{R_e}$) among ETGs with different types of tidal features and those without such features. This is accomplished by categorizing tidal features, which serve as direct evidence of recent mergers, into shells, streams, and tails, through deep images from the DESI Legacy Survey, and by using MaNGA data for the analysis of the kinematics of 1244 ETGs at $z<0.055$. We find that ETGs with tidal features typically have reduced $\\lambda_{R_e}$ values that are lower by 0.12 dex than ETGs without tidal features. ETGs with shells contribute most to the reduction in $\\lambda_{R_e}$. Consequently, nearly half of ETGs with shells are classified as slow rotators, a fraction that is more than twice as high as that of ETGs with tails or streams, and over three times higher than that of ETGs without tidal features. These trends generally remain valid even when ETGs are divided into several mass bins. Our findings support the idea that radial mergers, which are more effective at reducing $\\lambda_{R_e}$ than circular mergers, are more closely associated with the formation of shells rather than streams or tails. The detection of shells in slightly more massive ETGs compared to streams and tails may be attributed to the fact that massive satellite galaxies are more likely to be accreted through radial orbits, due to the nature of dynamical friction.","sentences":["To enhance our understanding of the impact of galaxy mergers on the kinematics of early-type galaxies (ETGs), we examine differences in specific stellar angular momentum within the half-light radius ($\\lambda_{R_e}$) among ETGs with different types of tidal features and those without such features.","This is accomplished by categorizing tidal features, which serve as direct evidence of recent mergers, into shells, streams, and tails, through deep images from the DESI Legacy Survey, and by using MaNGA data for the analysis of the kinematics of 1244 ETGs at $z<0.055$. We find that ETGs with tidal features typically have reduced $\\lambda_{R_e}$ values that are lower by 0.12 dex than ETGs without tidal features.","ETGs with shells contribute most to the reduction in $\\lambda_{R_e}$. Consequently, nearly half of ETGs with shells are classified as slow rotators, a fraction that is more than twice as high as that of ETGs with tails or streams, and over three times higher than that of ETGs without tidal features.","These trends generally remain valid even when ETGs are divided into several mass bins.","Our findings support the idea that radial mergers, which are more effective at reducing $\\lambda_{R_e}$ than circular mergers, are more closely associated with the formation of shells rather than streams or tails.","The detection of shells in slightly more massive ETGs compared to streams and tails may be attributed to the fact that massive satellite galaxies are more likely to be accreted through radial orbits, due to the nature of dynamical friction."],"url":"http://arxiv.org/abs/2404.03459v1","category":"astro-ph.GA"}
{"created":"2024-04-04 13:57:44","title":"Conditioning of Banach Space Valued Gaussian Random Variables: An Approximation Approach Based on Martingales","abstract":"In this paper we investigate the conditional distributions of two Banach space valued, jointly Gaussian random variables. These conditional distributions are again Gaussian and their means and covariances are determined by a general approximation scheme based upon a martingale idea. We then apply our general results to the case of Gaussian processes with continuous paths conditioned to partial observations of their paths.","sentences":["In this paper we investigate the conditional distributions of two Banach space valued, jointly Gaussian random variables.","These conditional distributions are again Gaussian and their means and covariances are determined by a general approximation scheme based upon a martingale idea.","We then apply our general results to the case of Gaussian processes with continuous paths conditioned to partial observations of their paths."],"url":"http://arxiv.org/abs/2404.03453v1","category":"math.PR"}
{"created":"2024-04-04 13:50:46","title":"Integrating AI in NDE: Techniques, Trends, and Further Directions","abstract":"The digital transformation is fundamentally changing our industries, affecting planning, execution as well as monitoring of production processes in a wide range of application fields. With product line-ups becoming more and more versatile and diverse, the necessary inspection and monitoring sparks significant novel requirements on the corresponding Nondestructive Evaluation (NDE) systems. The establishment of increasingly powerful approaches to incorporate Artificial Intelligence (AI) may provide just the needed innovation to solve some of these challenges.   In this paper we provide a comprehensive survey about the usage of AI methods in NDE in light of the recent innovations towards NDE 4.0. Since we cannot discuss each NDE modality in one paper, we limit our attention to magnetic methods, ultrasound, thermography, as well as optical inspection. In addition to reviewing recent AI developments in each field, we draw common connections by pointing out NDE-related tasks that have a common underlying mathematical problem and categorizing the state of the art according to the corresponding sub-tasks. In so doing, interdisciplinary connections are drawn that provide a more complete overall picture.","sentences":["The digital transformation is fundamentally changing our industries, affecting planning, execution as well as monitoring of production processes in a wide range of application fields.","With product line-ups becoming more and more versatile and diverse, the necessary inspection and monitoring sparks significant novel requirements on the corresponding Nondestructive Evaluation (NDE) systems.","The establishment of increasingly powerful approaches to incorporate Artificial Intelligence (AI) may provide just the needed innovation to solve some of these challenges.   ","In this paper we provide a comprehensive survey about the usage of AI methods in NDE in light of the recent innovations towards NDE 4.0.","Since we cannot discuss each NDE modality in one paper, we limit our attention to magnetic methods, ultrasound, thermography, as well as optical inspection.","In addition to reviewing recent AI developments in each field, we draw common connections by pointing out NDE-related tasks that have a common underlying mathematical problem and categorizing the state of the art according to the corresponding sub-tasks.","In so doing, interdisciplinary connections are drawn that provide a more complete overall picture."],"url":"http://arxiv.org/abs/2404.03449v1","category":"eess.SP"}
{"created":"2024-04-04 13:47:53","title":"ZynqMP-based board-management mezzanines for Serenity ATCA-blades","abstract":"In the context of the CMS Phase-2 tracker back-end processing system, two mezzanines based on the Zynq Ultrascale+ Multi-Processor System-on-Chip (MPSoC) device have been developed to serve as centralized slow control and board management solution for the Serenity-family \\textcolor{black}{Advanced Telecommunications Computing Architecture (ATCA)} blades.   This paper presents the developments of the MPSoC mezzanines to execute the Intelligent Platform Management Controller (IPMC) software in the real-time capable processors of the MPSoC. In coordination with the Shelf Manager, once full-power is enabled, a CentOS-based Linux distribution is executed in the application processors of the MPSoC, on which EMPButler and the Serenity Management Shell (SMASH) are running.","sentences":["In the context of the CMS Phase-2 tracker back-end processing system, two mezzanines based on the Zynq Ultrascale+ Multi-Processor System-on-Chip (MPSoC) device have been developed to serve as centralized slow control and board management solution for the Serenity-family \\textcolor{black}{Advanced","Telecommunications Computing Architecture (ATCA)} blades.   ","This paper presents the developments of the MPSoC mezzanines to execute the Intelligent Platform Management Controller (IPMC) software in the real-time capable processors of the MPSoC. In coordination with the Shelf Manager, once full-power is enabled, a CentOS-based Linux distribution is executed in the application processors of the MPSoC, on which EMPButler and the Serenity Management Shell (SMASH) are running."],"url":"http://arxiv.org/abs/2404.03447v1","category":"physics.ins-det"}
{"created":"2024-04-04 13:46:52","title":"SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering","abstract":"Deep clustering, which learns representation and semantic clustering without labels information, poses a great challenge for deep learning-based approaches. Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the practical applicability of their methods. In this paper, we propose a more practical problem setting named deep imbalanced clustering, where the underlying classes exhibit an imbalance distribution. To address this challenge, we introduce a novel optimal transport-based pseudo-label learning framework. Our framework formulates pseudo-label generation as a Semantic-regularized Progressive Partial Optimal Transport (SP$^2$OT) problem, which progressively transports each sample to imbalanced clusters under several prior distribution and semantic relation constraints, thus generating high-quality and imbalance-aware pseudo-labels. To solve SP$^2$OT, we develop a Majorization-Minimization-based optimization algorithm. To be more precise, we employ the strategy of majorization to reformulate the SP$^2$OT problem into a Progressive Partial Optimal Transport problem, which can be transformed into an unbalanced optimal transport problem with augmented constraints and can be solved efficiently by a fast matrix scaling algorithm. Experiments on various datasets, including a human-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority of our method.","sentences":["Deep clustering, which learns representation and semantic clustering without labels information, poses a great challenge for deep learning-based approaches.","Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the practical applicability of their methods.","In this paper, we propose a more practical problem setting named deep imbalanced clustering, where the underlying classes exhibit an imbalance distribution.","To address this challenge, we introduce a novel optimal transport-based pseudo-label learning framework.","Our framework formulates pseudo-label generation as a Semantic-regularized Progressive Partial Optimal Transport (SP$^2$OT) problem, which progressively transports each sample to imbalanced clusters under several prior distribution and semantic relation constraints, thus generating high-quality and imbalance-aware pseudo-labels.","To solve SP$^2$OT, we develop a Majorization-Minimization-based optimization algorithm.","To be more precise, we employ the strategy of majorization to reformulate the SP$^2$OT problem into a Progressive Partial Optimal Transport problem, which can be transformed into an unbalanced optimal transport problem with augmented constraints and can be solved efficiently by a fast matrix scaling algorithm.","Experiments on various datasets, including a human-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority of our method."],"url":"http://arxiv.org/abs/2404.03446v1","category":"cs.CV"}
{"created":"2024-04-04 13:43:11","title":"Part-Attention Based Model Make Occluded Person Re-Identification Stronger","abstract":"The goal of occluded person re-identification (ReID) is to retrieve specific pedestrians in occluded situations. However, occluded person ReID still suffers from background clutter and low-quality local feature representations, which limits model performance. In our research, we introduce a new framework called PAB-ReID, which is a novel ReID model incorporating part-attention mechanisms to tackle the aforementioned issues effectively. Firstly, we introduce the human parsing label to guide the generation of more accurate human part attention maps. In addition, we propose a fine-grained feature focuser for generating fine-grained human local feature representations while suppressing background interference. Moreover, We also design a part triplet loss to supervise the learning of human local features, which optimizes intra/inter-class distance. We conducted extensive experiments on specialized occlusion and regular ReID datasets, showcasing that our approach outperforms the existing state-of-the-art methods.","sentences":["The goal of occluded person re-identification (ReID) is to retrieve specific pedestrians in occluded situations.","However, occluded person ReID still suffers from background clutter and low-quality local feature representations, which limits model performance.","In our research, we introduce a new framework called PAB-ReID, which is a novel ReID model incorporating part-attention mechanisms to tackle the aforementioned issues effectively.","Firstly, we introduce the human parsing label to guide the generation of more accurate human part attention maps.","In addition, we propose a fine-grained feature focuser for generating fine-grained human local feature representations while suppressing background interference.","Moreover, We also design a part triplet loss to supervise the learning of human local features, which optimizes intra/inter-class distance.","We conducted extensive experiments on specialized occlusion and regular ReID datasets, showcasing that our approach outperforms the existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.03443v1","category":"cs.CV"}
{"created":"2024-04-04 13:39:06","title":"Benchmarking ChatGPT on Algorithmic Reasoning","abstract":"We evaluate ChatGPT's ability to solve algorithm problems from the CLRS benchmark suite that is designed for GNNs. The benchmark requires the use of a specified classical algorithm to solve a given problem. We find that ChatGPT outperforms specialist GNN models, using Python to successfully solve these problems. This raises new points in the discussion about learning algorithms with neural networks.","sentences":["We evaluate ChatGPT's ability to solve algorithm problems from the CLRS benchmark suite that is designed for GNNs.","The benchmark requires the use of a specified classical algorithm to solve a given problem.","We find that ChatGPT outperforms specialist GNN models, using Python to successfully solve these problems.","This raises new points in the discussion about learning algorithms with neural networks."],"url":"http://arxiv.org/abs/2404.03441v1","category":"cs.AI"}
{"created":"2024-04-04 13:29:30","title":"Interpreting End-to-End Deep Learning Models for Speech Source Localization Using Layer-wise Relevance Propagation","abstract":"Deep learning models are widely applied in the signal processing community, yet their inner working procedure is often treated as a black box. In this paper, we investigate the use of eXplainable Artificial Intelligence (XAI) techniques to learning-based end-to-end speech source localization models. We consider the Layer-wise Relevance Propagation (LRP) technique, which aims to determine which parts of the input are more important for the output prediction. Using LRP we analyze two state-of-the-art models, of differing architectural complexity that map audio signals acquired by the microphones to the cartesian coordinates of the source. Specifically, we inspect the relevance associated with the input features of the two models and discover that both networks denoise and de-reverberate the microphone signals to compute more accurate statistical correlations between them and consequently localize the sources. To further demonstrate this fact, we estimate the Time-Difference of Arrivals (TDoAs) via the Generalized Cross Correlation with Phase Transform (GCC-PHAT) using both microphone signals and relevance signals extracted from the two networks and show that through the latter we obtain more accurate time-delay estimation results.","sentences":["Deep learning models are widely applied in the signal processing community, yet their inner working procedure is often treated as a black box.","In this paper, we investigate the use of eXplainable Artificial Intelligence (XAI) techniques to learning-based end-to-end speech source localization models.","We consider the Layer-wise Relevance Propagation (LRP) technique, which aims to determine which parts of the input are more important for the output prediction.","Using LRP we analyze two state-of-the-art models, of differing architectural complexity that map audio signals acquired by the microphones to the cartesian coordinates of the source.","Specifically, we inspect the relevance associated with the input features of the two models and discover that both networks denoise and de-reverberate the microphone signals to compute more accurate statistical correlations between them and consequently localize the sources.","To further demonstrate this fact, we estimate the Time-Difference of Arrivals (TDoAs) via the Generalized Cross Correlation with Phase Transform (GCC-PHAT) using both microphone signals and relevance signals extracted from the two networks and show that through the latter we obtain more accurate time-delay estimation results."],"url":"http://arxiv.org/abs/2404.03436v1","category":"eess.AS"}
{"created":"2024-04-04 13:28:32","title":"Size dependent photoemission study by electrochemical coarsening of nanoporous gold","abstract":"The generation and utilization of hot charge carriers in plasmonic materials have emerged as a topic of significant importance, with profound implications across multiple disciplines, including optoelectronics, photovoltaics, photocatalysis, and sensing. In this study, we investigate the hot electron transfer from nanoporous gold (npAu) in dependence of the structure size, utilizing both the nanoscale feature size and the interconnected nature of this material. We employ photoelectron injection from nanoporous gold into the electrolyte under UV illumination as a test electron transfer process. Nanoporous gold thin films with sub-10 nm initial ligament diameter are stepwise coarsened by potential cycles in a photoelectrochemical setup, thereby allowing us to precisely probe the influence of ligament diameter on the photocurrent response. The resulting ligament diameter variations are confirmed by scanning electron microscopy (SEM) analysis. As the ligament diameter increased from 8 to 16 nm, there was a corresponding decrease in quantum efficiency proportional to the inverse ligament diameter squared. Such dependency is expected for electrons excited by surface collisions. For the small ligament diameter of 10 nm we estimate an emission efficiency of excited 6sp electrons as 3.14%, reaching 23% for the surface excited electrons.","sentences":["The generation and utilization of hot charge carriers in plasmonic materials have emerged as a topic of significant importance, with profound implications across multiple disciplines, including optoelectronics, photovoltaics, photocatalysis, and sensing.","In this study, we investigate the hot electron transfer from nanoporous gold (npAu) in dependence of the structure size, utilizing both the nanoscale feature size and the interconnected nature of this material.","We employ photoelectron injection from nanoporous gold into the electrolyte under UV illumination as a test electron transfer process.","Nanoporous gold thin films with sub-10 nm initial ligament diameter are stepwise coarsened by potential cycles in a photoelectrochemical setup, thereby allowing us to precisely probe the influence of ligament diameter on the photocurrent response.","The resulting ligament diameter variations are confirmed by scanning electron microscopy (SEM) analysis.","As the ligament diameter increased from 8 to 16 nm, there was a corresponding decrease in quantum efficiency proportional to the inverse ligament diameter squared.","Such dependency is expected for electrons excited by surface collisions.","For the small ligament diameter of 10 nm we estimate an emission efficiency of excited 6sp electrons as 3.14%, reaching 23% for the surface excited electrons."],"url":"http://arxiv.org/abs/2404.03435v1","category":"physics.optics"}
{"created":"2024-04-04 13:27:02","title":"Piecemeal Quantum Telescope with Superresolution","abstract":"Detecting remote objects with higher precision and resolution takes a crucial role in many scientific tasks, such as astronomical observation. Compared with classical telescopes, quantum telescopes can detect more precise angle value for single-star target. The precision of existing quantum telescopes is improved in the scale of square root of incident single photons. Here we propose the piecemeal quantum telescope with bit-by-bit iteration. It improves precision exponentially with number of nincident single-photons in detecting the star angle. As a result, it requests to detect only a few hundreds of photons for a precision breaking classical limit by 4 to 5 magnitude orders. Moreover, it can detect a general astronomical target consisting of unknown number of stars.","sentences":["Detecting remote objects with higher precision and resolution takes a crucial role in many scientific tasks, such as astronomical observation.","Compared with classical telescopes, quantum telescopes can detect more precise angle value for single-star target.","The precision of existing quantum telescopes is improved in the scale of square root of incident single photons.","Here we propose the piecemeal quantum telescope with bit-by-bit iteration.","It improves precision exponentially with number of nincident single-photons in detecting the star angle.","As a result, it requests to detect only a few hundreds of photons for a precision breaking classical limit by 4 to 5 magnitude orders.","Moreover, it can detect a general astronomical target consisting of unknown number of stars."],"url":"http://arxiv.org/abs/2404.03432v1","category":"quant-ph"}
{"created":"2024-04-04 13:22:28","title":"Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions","abstract":"Intelligent tutoring systems (ITSs) that imitate human tutors and aim to provide immediate and customized instructions or feedback to learners have shown their effectiveness in education. With the emergence of generative artificial intelligence, large language models (LLMs) further entitle the systems to complex and coherent conversational interactions. These systems would be of great help in language education as it involves developing skills in communication, which, however, drew relatively less attention. Additionally, due to the complicated cognitive development at younger ages, more endeavors are needed for practical uses. Scaffolding refers to a teaching technique where teachers provide support and guidance to students for learning and developing new concepts or skills. It is an effective way to support diverse learning needs, goals, processes, and outcomes. In this work, we investigate how pedagogical instructions facilitate the scaffolding in ITSs, by conducting a case study on guiding children to describe images for language learning. We construct different types of scaffolding tutoring systems grounded in four fundamental learning theories: knowledge construction, inquiry-based learning, dialogic teaching, and zone of proximal development. For qualitative and quantitative analyses, we build and refine a seven-dimension rubric to evaluate the scaffolding process. In our experiment on GPT-4V, we observe that LLMs demonstrate strong potential to follow pedagogical instructions and achieve self-paced learning in different student groups. Moreover, we extend our evaluation framework from a manual to an automated approach, paving the way to benchmark various conversational tutoring systems.","sentences":["Intelligent tutoring systems (ITSs) that imitate human tutors and aim to provide immediate and customized instructions or feedback to learners have shown their effectiveness in education.","With the emergence of generative artificial intelligence, large language models (LLMs) further entitle the systems to complex and coherent conversational interactions.","These systems would be of great help in language education as it involves developing skills in communication, which, however, drew relatively less attention.","Additionally, due to the complicated cognitive development at younger ages, more endeavors are needed for practical uses.","Scaffolding refers to a teaching technique where teachers provide support and guidance to students for learning and developing new concepts or skills.","It is an effective way to support diverse learning needs, goals, processes, and outcomes.","In this work, we investigate how pedagogical instructions facilitate the scaffolding in ITSs, by conducting a case study on guiding children to describe images for language learning.","We construct different types of scaffolding tutoring systems grounded in four fundamental learning theories: knowledge construction, inquiry-based learning, dialogic teaching, and zone of proximal development.","For qualitative and quantitative analyses, we build and refine a seven-dimension rubric to evaluate the scaffolding process.","In our experiment on GPT-4V, we observe that LLMs demonstrate strong potential to follow pedagogical instructions and achieve self-paced learning in different student groups.","Moreover, we extend our evaluation framework from a manual to an automated approach, paving the way to benchmark various conversational tutoring systems."],"url":"http://arxiv.org/abs/2404.03429v1","category":"cs.CL"}
{"created":"2024-04-04 13:15:28","title":"Edisum: Summarizing and Explaining Wikipedia Edits at Scale","abstract":"An edit summary is a succinct comment written by a Wikipedia editor explaining the nature of, and reasons for, an edit to a Wikipedia page. Edit summaries are crucial for maintaining the encyclopedia: they are the first thing seen by content moderators and help them decide whether to accept or reject an edit. Additionally, edit summaries constitute a valuable data source for researchers. Unfortunately, as we show, for many edits, summaries are either missing or incomplete. To overcome this problem and help editors write useful edit summaries, we propose a model for recommending edit summaries generated by a language model trained to produce good edit summaries given the representation of an edit diff. This is a challenging task for multiple reasons, including mixed-quality training data, the need to understand not only what was changed in the article but also why it was changed, and efficiency requirements imposed by the scale of Wikipedia. We address these challenges by curating a mix of human and synthetically generated training data and fine-tuning a generative language model sufficiently small to be used on Wikipedia at scale. Our model performs on par with human editors. Commercial large language models are able to solve this task better than human editors, but would be too expensive to run on Wikipedia at scale. More broadly, this paper showcases how language modeling technology can be used to support humans in maintaining one of the largest and most visible projects on the Web.","sentences":["An edit summary is a succinct comment written by a Wikipedia editor explaining the nature of, and reasons for, an edit to a Wikipedia page.","Edit summaries are crucial for maintaining the encyclopedia: they are the first thing seen by content moderators and help them decide whether to accept or reject an edit.","Additionally, edit summaries constitute a valuable data source for researchers.","Unfortunately, as we show, for many edits, summaries are either missing or incomplete.","To overcome this problem and help editors write useful edit summaries, we propose a model for recommending edit summaries generated by a language model trained to produce good edit summaries given the representation of an edit diff.","This is a challenging task for multiple reasons, including mixed-quality training data, the need to understand not only what was changed in the article but also why it was changed, and efficiency requirements imposed by the scale of Wikipedia.","We address these challenges by curating a mix of human and synthetically generated training data and fine-tuning a generative language model sufficiently small to be used on Wikipedia at scale.","Our model performs on par with human editors.","Commercial large language models are able to solve this task better than human editors, but would be too expensive to run on Wikipedia at scale.","More broadly, this paper showcases how language modeling technology can be used to support humans in maintaining one of the largest and most visible projects on the Web."],"url":"http://arxiv.org/abs/2404.03428v1","category":"cs.CL"}
{"created":"2024-04-04 13:06:25","title":"ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model","abstract":"Convolutional neural networks (CNN) and Transformers have made impressive progress in the field of remote sensing change detection (CD). However, both architectures have their inherent shortcomings. Recently, the Mamba architecture, based on spatial state models, has shown remarkable performance in a series of natural language processing tasks, which can effectively compensate for the shortcomings of the above two architectures. In this paper, we explore for the first time the potential of the Mamba architecture for remote sensing change detection tasks. We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD), semantic change detection (SCD), and building damage assessment (BDA), respectively. All three frameworks adopt the cutting-edge visual Mamba architecture as the encoder, which allows full learning of global spatial contextual information from the input images. For the change decoder, which is available in all three architectures, we propose three spatio-temporal relationship modeling mechanisms, which can be naturally combined with the Mamba architecture and fully utilize its attribute to achieve spatio-temporal interaction of multi-temporal features and obtain accurate change information. On five benchmark datasets, our proposed frameworks outperform current CNN- and Transformer-based approaches without using any complex strategies or tricks, fully demonstrating the potential of the Mamba architecture. Specifically, we obtained 83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU, LEVIR-CD+, and WHU-CD; on the SCD dataset SECOND, we obtained 24.04% SeK; and on the xBD dataset, we obtained 81.41% overall F1 score. The source code will be available in https://github.com/ChenHongruixuan/MambaCD","sentences":["Convolutional neural networks (CNN) and Transformers have made impressive progress in the field of remote sensing change detection (CD).","However, both architectures have their inherent shortcomings.","Recently, the Mamba architecture, based on spatial state models, has shown remarkable performance in a series of natural language processing tasks, which can effectively compensate for the shortcomings of the above two architectures.","In this paper, we explore for the first time the potential of the Mamba architecture for remote sensing change detection tasks.","We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD), semantic change detection (SCD), and building damage assessment (BDA), respectively.","All three frameworks adopt the cutting-edge visual Mamba architecture as the encoder, which allows full learning of global spatial contextual information from the input images.","For the change decoder, which is available in all three architectures, we propose three spatio-temporal relationship modeling mechanisms, which can be naturally combined with the Mamba architecture and fully utilize its attribute to achieve spatio-temporal interaction of multi-temporal features and obtain accurate change information.","On five benchmark datasets, our proposed frameworks outperform current CNN- and Transformer-based approaches without using any complex strategies or tricks, fully demonstrating the potential of the Mamba architecture.","Specifically, we obtained 83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU, LEVIR-CD+, and WHU-CD; on the SCD dataset SECOND, we obtained 24.04% SeK; and on the xBD dataset, we obtained 81.41% overall F1 score.","The source code will be available in https://github.com/ChenHongruixuan/MambaCD"],"url":"http://arxiv.org/abs/2404.03425v1","category":"eess.IV"}
{"created":"2024-04-04 12:54:13","title":"Integrating Hyperparameter Search into GramML","abstract":"Automated Machine Learning (AutoML) has become increasingly popular in recent years due to its ability to reduce the amount of time and expertise required to design and develop machine learning systems. This is very important for the practice of machine learning, as it allows building strong baselines quickly, improving the efficiency of the data scientists, and reducing the time to production. However, despite the advantages of AutoML, it faces several challenges, such as defining the solutions space and exploring it efficiently. Recently, some approaches have been shown to be able to do it using tree-based search algorithms and context-free grammars. In particular, GramML presents a model-free reinforcement learning approach that leverages pipeline configuration grammars and operates using Monte Carlo tree search. However, one of the limitations of GramML is that it uses default hyperparameters, limiting the search problem to finding optimal pipeline structures for the available data preprocessors and models. In this work, we propose an extension to GramML that supports larger search spaces including hyperparameter search. We evaluated the approach using an OpenML benchmark and found significant improvements compared to other state-of-the-art techniques.","sentences":["Automated Machine Learning (AutoML) has become increasingly popular in recent years due to its ability to reduce the amount of time and expertise required to design and develop machine learning systems.","This is very important for the practice of machine learning, as it allows building strong baselines quickly, improving the efficiency of the data scientists, and reducing the time to production.","However, despite the advantages of AutoML, it faces several challenges, such as defining the solutions space and exploring it efficiently.","Recently, some approaches have been shown to be able to do it using tree-based search algorithms and context-free grammars.","In particular, GramML presents a model-free reinforcement learning approach that leverages pipeline configuration grammars and operates using Monte Carlo tree search.","However, one of the limitations of GramML is that it uses default hyperparameters, limiting the search problem to finding optimal pipeline structures for the available data preprocessors and models.","In this work, we propose an extension to GramML that supports larger search spaces including hyperparameter search.","We evaluated the approach using an OpenML benchmark and found significant improvements compared to other state-of-the-art techniques."],"url":"http://arxiv.org/abs/2404.03419v1","category":"cs.LG"}
{"created":"2024-04-04 12:51:28","title":"Permissible Knowledge Pooling","abstract":"Information pooling has been extensively formalised across various logical frameworks in distributed systems, characterized by diverse information-sharing patterns. These approaches generally adopt an intersection perspective, aggregating all possible information, regardless of whether it is known or unknown to the agents. In contrast, this work adopts a unique stance, emphasising that sharing knowledge means distributing what is known, rather than what remains uncertain. This paper introduces a dynamic logic for knowledge pooling or sharing and further discusses a potential framework for permissible knowledge pooling.","sentences":["Information pooling has been extensively formalised across various logical frameworks in distributed systems, characterized by diverse information-sharing patterns.","These approaches generally adopt an intersection perspective, aggregating all possible information, regardless of whether it is known or unknown to the agents.","In contrast, this work adopts a unique stance, emphasising that sharing knowledge means distributing what is known, rather than what remains uncertain.","This paper introduces a dynamic logic for knowledge pooling or sharing and further discusses a potential framework for permissible knowledge pooling."],"url":"http://arxiv.org/abs/2404.03418v1","category":"cs.LO"}
{"created":"2024-04-04 12:46:37","title":"Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought","abstract":"We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM in reasoning tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance.","sentences":["We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM in reasoning tasks.","Specifically, the lightweight LM first generates a rationale for each input instance.","The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM.","Our approach is resource-efficient in the sense that it only requires training the lightweight LM.","We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals.","We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA.","Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy.","We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance."],"url":"http://arxiv.org/abs/2404.03414v1","category":"cs.CL"}
{"created":"2024-04-04 12:29:45","title":"Electronic ferroelectricity in monolayer graphene for multifunctional neuromorphic electronics","abstract":"Ferroelectricity is intriguing for its spontaneous electric polarization, which is switchable by an external electric field. Expanding ferroelectric materials to two-dimensional limit will provide versatile applications for the development of next-generation nonvolatile devices. Conventional ferroelectricity requires the materials consisting of at least two constituent elements associated with polar crystalline structures. Monolayer graphene as an elementary two-dimensional material unlikely exhibits ferroelectric order due to its highly centrosymmetric hexagonal lattices. Nevertheless, two-dimensional moire superlattices offer a powerful way to engineer diverse electronic orders in non-polar materials. Here, we report the observations of electronic ferroelectricity in monolayer graphene by introducing asymmetric moire superlattice at the graphene/h-BN interface. Utilizing Hall measurements, the electric polarization is identified to stem from electron-hole dipoles, suggesting the electronic dynamics of the observed ferroelectricity. Standard polarization-electric field hysteresis loops, as well as unconventional multiple switchable polarization states, have been achieved. By in-situ comparing with control devices, we found that the electronic ferroelectricity in graphene moire systems is independent of layer number of graphene and the corresponding fine band structures. Furthermore, we demonstrate the applications of this ferroelectric moire structures in multi-state non-volatile data storage and the emulation of versatile synaptic behaviors, including short-term plasticity, long-term potentiation and long-term depression. This work not only enriches the fundamental understanding of ferroelectricity, but also demonstrates the promising applications of graphene in multi-state memories and neuromorphic computing.","sentences":["Ferroelectricity is intriguing for its spontaneous electric polarization, which is switchable by an external electric field.","Expanding ferroelectric materials to two-dimensional limit will provide versatile applications for the development of next-generation nonvolatile devices.","Conventional ferroelectricity requires the materials consisting of at least two constituent elements associated with polar crystalline structures.","Monolayer graphene as an elementary two-dimensional material unlikely exhibits ferroelectric order due to its highly centrosymmetric hexagonal lattices.","Nevertheless, two-dimensional moire superlattices offer a powerful way to engineer diverse electronic orders in non-polar materials.","Here, we report the observations of electronic ferroelectricity in monolayer graphene by introducing asymmetric moire superlattice at the graphene/h-BN interface.","Utilizing Hall measurements, the electric polarization is identified to stem from electron-hole dipoles, suggesting the electronic dynamics of the observed ferroelectricity.","Standard polarization-electric field hysteresis loops, as well as unconventional multiple switchable polarization states, have been achieved.","By in-situ comparing with control devices, we found that the electronic ferroelectricity in graphene moire systems is independent of layer number of graphene and the corresponding fine band structures.","Furthermore, we demonstrate the applications of this ferroelectric moire structures in multi-state non-volatile data storage and the emulation of versatile synaptic behaviors, including short-term plasticity, long-term potentiation and long-term depression.","This work not only enriches the fundamental understanding of ferroelectricity, but also demonstrates the promising applications of graphene in multi-state memories and neuromorphic computing."],"url":"http://arxiv.org/abs/2404.03410v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-04 12:25:21","title":"Analytical Characterization of Epileptic Dynamics in a Bistable System","abstract":"Epilepsy is one of the most common neurological disorders globally, affecting millions of individuals. Despite significant advancements, the precise mechanisms underlying this condition remain largely unknown, making accurately predicting and preventing epileptic seizures challenging. In this paper, we employ a bistable model, where a stable equilibrium and a stable limit cycle coexist, to describe epileptic dynamics. The equilibrium captures normal steady-state neural activity, while the stable limit cycle signifies seizure-like oscillations. The noise-driven switch from the equilibrium to the limit cycle characterizes the onset of seizures. The differences in the regions of attraction of these two stable states distinguish epileptic brain dynamics from healthy ones. We analytically construct the regions of attraction for both states. Further, using the notion of input-to-state stability, we theoretically show how the regions of attraction influence the stability of the system subject to external perturbations. Generalizing the bistable system into coupled networks, we also find the role of network parameters in shaping the regions of attraction. Our findings shed light on the intricate interplay between brain networks and epileptic activity, offering mechanistic insights into potential avenues for more predictable treatments.","sentences":["Epilepsy is one of the most common neurological disorders globally, affecting millions of individuals.","Despite significant advancements, the precise mechanisms underlying this condition remain largely unknown, making accurately predicting and preventing epileptic seizures challenging.","In this paper, we employ a bistable model, where a stable equilibrium and a stable limit cycle coexist, to describe epileptic dynamics.","The equilibrium captures normal steady-state neural activity, while the stable limit cycle signifies seizure-like oscillations.","The noise-driven switch from the equilibrium to the limit cycle characterizes the onset of seizures.","The differences in the regions of attraction of these two stable states distinguish epileptic brain dynamics from healthy ones.","We analytically construct the regions of attraction for both states.","Further, using the notion of input-to-state stability, we theoretically show how the regions of attraction influence the stability of the system subject to external perturbations.","Generalizing the bistable system into coupled networks, we also find the role of network parameters in shaping the regions of attraction.","Our findings shed light on the intricate interplay between brain networks and epileptic activity, offering mechanistic insights into potential avenues for more predictable treatments."],"url":"http://arxiv.org/abs/2404.03409v1","category":"eess.SY"}
{"created":"2024-04-04 12:12:24","title":"AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment","abstract":"With the rapid advancements in AI-Generated Content (AIGC), AI-Generated Images (AIGIs) have been widely applied in entertainment, education, and social media. However, due to the significant variance in quality among different AIGIs, there is an urgent need for models that consistently match human subjective ratings. To address this issue, we organized a challenge towards AIGC quality assessment on NTIRE 2024 that extensively considers 15 popular generative models, utilizing dynamic hyper-parameters (including classifier-free guidance, iteration epochs, and output image resolution), and gather subjective scores that consider perceptual quality and text-to-image alignment altogether comprehensively involving 21 subjects. This approach culminates in the creation of the largest fine-grained AIGI subjective quality database to date with 20,000 AIGIs and 420,000 subjective ratings, known as AIGIQA-20K. Furthermore, we conduct benchmark experiments on this database to assess the correspondence between 16 mainstream AIGI quality models and human perception. We anticipate that this large-scale quality database will inspire robust quality indicators for AIGIs and propel the evolution of AIGC for vision. The database is released on https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image.","sentences":["With the rapid advancements in AI-Generated Content (AIGC), AI-Generated Images (AIGIs) have been widely applied in entertainment, education, and social media.","However, due to the significant variance in quality among different AIGIs, there is an urgent need for models that consistently match human subjective ratings.","To address this issue, we organized a challenge towards AIGC quality assessment on NTIRE 2024 that extensively considers 15 popular generative models, utilizing dynamic hyper-parameters (including classifier-free guidance, iteration epochs, and output image resolution), and gather subjective scores that consider perceptual quality and text-to-image alignment altogether comprehensively involving 21 subjects.","This approach culminates in the creation of the largest fine-grained AIGI subjective quality database to date with 20,000 AIGIs and 420,000 subjective ratings, known as AIGIQA-20K.","Furthermore, we conduct benchmark experiments on this database to assess the correspondence between 16 mainstream AIGI quality models and human perception.","We anticipate that this large-scale quality database will inspire robust quality indicators for AIGIs and propel the evolution of AIGC for vision.","The database is released on https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image."],"url":"http://arxiv.org/abs/2404.03407v1","category":"cs.CV"}
{"created":"2024-04-04 12:12:04","title":"Non-trivial bundles and defect operators in $n$-form gauge theories","abstract":"In $(d+1)$-dimensional $1$-form non-Abelian gauge theories, we classify non-trivial $0$-form bundles in $ \\mathbb{R}^{d} $, which yield configurations of $D(d-2j)$-branes wrapping $(d-2j)$-cycles $c_{d-2j} $ in $Dd$-branes. We construct the related defect operators $ U^{(2j-1)} ( c_{d-2j} ) $, which are disorder operators carrying the $D(d-2j)$ charge. We compute the commutation relations between the defect operators and Chern-Simons operators on odd-dimensional closed manifolds, and derive the generalized Witten effect for $U^{(2j-1)} ( c_{d-2j} ) $. When $c_{d-2j}$ is not exact, $ U^{(2j-1)} ( c_{d-2j} ) $ and $ U^{(2j-1)} (- c_{d-2j} ) $ can also combine into an electric $(2j-1)$-form global symmetry operator, where the $(2j-1)$-form is the Chern-Simons form. The dual magnetic $(d-2j)$-form global symmetry is generated by the $D(d-2j)$ charge. We also study non-trivial $1$-form bundles in $(d+1)$-dimensional $2$-form non-Abelian gauge theories, where the defect operators are $\\mathcal{U}^{(2j)} ( c_{d-2j-1} ) $. With the field strength of the $1$-form taken as the flat connection of the $2$-form, we classify the topological sectors in $2$-form theories.","sentences":["In $(d+1)$-dimensional $1$-form non-Abelian gauge theories, we classify non-trivial $0$-form bundles in $ \\mathbb{R}^{d} $, which yield configurations of $D(d-2j)$-branes wrapping $(d-2j)$-cycles $c_{d-2j} $ in $Dd$-branes.","We construct the related defect operators $ U^{(2j-1)} ( c_{d-2j} ) $, which are disorder operators carrying the $D(d-2j)$ charge.","We compute the commutation relations between the defect operators and Chern-Simons operators on odd-dimensional closed manifolds, and derive the generalized Witten effect for $U^{(2j-1)} ( c_{d-2j} ) $.","When $c_{d-2j}$ is not exact, $ U^{(2j-1)} ( c_{d-2j} ) $ and $ U^{(2j-1)} (- c_{d-2j} ) $ can also combine into an electric $(2j-1)$-form global symmetry operator, where the $(2j-1)$-form is the Chern-Simons form.","The dual magnetic $(d-2j)$-form global symmetry is generated by the $D(d-2j)$ charge.","We also study non-trivial $1$-form bundles in $(d+1)$-dimensional $2$-form non-Abelian gauge theories, where the defect operators are $\\mathcal{U}^{(2j)} ( c_{d-2j-1} ) $.","With the field strength of the $1$-form taken as the flat connection of the $2$-form, we classify the topological sectors in $2$-form theories."],"url":"http://arxiv.org/abs/2404.03406v1","category":"hep-th"}
{"created":"2024-04-04 12:05:04","title":"Robust inference for linear regression models with possibly skewed error distribution","abstract":"Traditional methods for linear regression generally assume that the underlying error distribution, equivalently the distribution of the responses, is normal. Yet, sometimes real life response data may exhibit a skewed pattern, and assuming normality would not give reliable results in such cases. This is often observed in cases of some biomedical, behavioral, socio-economic and other variables. In this paper, we propose to use the class of skew normal (SN) distributions, which also includes the ordinary normal distribution as its special case, as the model for the errors in a linear regression setup and perform subsequent statistical inference using the popular and robust minimum density power divergence approach to get stable insights in the presence of possible data contamination (e.g., outliers). We provide the asymptotic distribution of the proposed estimator of the regression parameters and also propose robust Wald-type tests of significance for these parameters. We provide an influence function analysis of these estimators and test statistics, and also provide level and power influence functions. Numerical verification including simulation studies and real data analysis is provided to substantiate the theory developed.","sentences":["Traditional methods for linear regression generally assume that the underlying error distribution, equivalently the distribution of the responses, is normal.","Yet, sometimes real life response data may exhibit a skewed pattern, and assuming normality would not give reliable results in such cases.","This is often observed in cases of some biomedical, behavioral, socio-economic and other variables.","In this paper, we propose to use the class of skew normal (SN) distributions, which also includes the ordinary normal distribution as its special case, as the model for the errors in a linear regression setup and perform subsequent statistical inference using the popular and robust minimum density power divergence approach to get stable insights in the presence of possible data contamination (e.g., outliers).","We provide the asymptotic distribution of the proposed estimator of the regression parameters and also propose robust Wald-type tests of significance for these parameters.","We provide an influence function analysis of these estimators and test statistics, and also provide level and power influence functions.","Numerical verification including simulation studies and real data analysis is provided to substantiate the theory developed."],"url":"http://arxiv.org/abs/2404.03404v1","category":"stat.ME"}
{"created":"2024-04-04 11:59:06","title":"Scaling Up Video Summarization Pretraining with Large Language Models","abstract":"Long-form video content constitutes a significant portion of internet traffic, making automated video summarization an essential research problem. However, existing video summarization datasets are notably limited in their size, constraining the effectiveness of state-of-the-art methods for generalization. Our work aims to overcome this limitation by capitalizing on the abundance of long-form videos with dense speech-to-video alignment and the remarkable capabilities of recent large language models (LLMs) in summarizing long text. We introduce an automated and scalable pipeline for generating a large-scale video summarization dataset using LLMs as Oracle summarizers. By leveraging the generated dataset, we analyze the limitations of existing approaches and propose a new video summarization model that effectively addresses them. To facilitate further research in the field, our work also presents a new benchmark dataset that contains 1200 long videos each with high-quality summaries annotated by professionals. Extensive experiments clearly indicate that our proposed approach sets a new state-of-the-art in video summarization across several benchmarks.","sentences":["Long-form video content constitutes a significant portion of internet traffic, making automated video summarization an essential research problem.","However, existing video summarization datasets are notably limited in their size, constraining the effectiveness of state-of-the-art methods for generalization.","Our work aims to overcome this limitation by capitalizing on the abundance of long-form videos with dense speech-to-video alignment and the remarkable capabilities of recent large language models (LLMs) in summarizing long text.","We introduce an automated and scalable pipeline for generating a large-scale video summarization dataset using LLMs as Oracle summarizers.","By leveraging the generated dataset, we analyze the limitations of existing approaches and propose a new video summarization model that effectively addresses them.","To facilitate further research in the field, our work also presents a new benchmark dataset that contains 1200 long videos each with high-quality summaries annotated by professionals.","Extensive experiments clearly indicate that our proposed approach sets a new state-of-the-art in video summarization across several benchmarks."],"url":"http://arxiv.org/abs/2404.03398v1","category":"cs.CV"}
{"created":"2024-04-04 11:50:53","title":"A superconvergence result in the RBF-FD method","abstract":"Radial Basis Function-generated Finite Differences (RBF-FD) is a meshless method that can be used to numerically solve partial differential equations. The solution procedure consists of two steps. First, the differential operator is discretised on given scattered nodes and afterwards, a global sparse matrix is assembled and inverted to obtain an approximate solution. Focusing on Polyharmonic Splines as our Radial Basis Functions (RBFs) of choice, appropriately augmented with monomials, it is well known that the truncation error of the differential operator approximation is determined by the degree of monomial augmentation. Naively, one might think that the solution error will have the same order of convergence. We present a superconvergence result that shows otherwise - for some augmentation degrees, order of convergence is higher than expected.","sentences":["Radial Basis Function-generated Finite Differences (RBF-FD) is a meshless method that can be used to numerically solve partial differential equations.","The solution procedure consists of two steps.","First, the differential operator is discretised on given scattered nodes and afterwards, a global sparse matrix is assembled and inverted to obtain an approximate solution.","Focusing on Polyharmonic Splines as our Radial Basis Functions (RBFs) of choice, appropriately augmented with monomials, it is well known that the truncation error of the differential operator approximation is determined by the degree of monomial augmentation.","Naively, one might think that the solution error will have the same order of convergence.","We present a superconvergence result that shows otherwise - for some augmentation degrees, order of convergence is higher than expected."],"url":"http://arxiv.org/abs/2404.03393v1","category":"math.NA"}
{"created":"2024-04-04 11:45:50","title":"Peculiar magnetism and magneto-transport properties in a non-centrosymmetric self-intercalated van der Waals ferromagnet Cr5Te8","abstract":"Trigonal Cr5Te8, a self-intercalated van der Waals ferromagnet with an out of plane magnetic anisotropy, has long been known to crystallise in a centrosymmetric structure. Through detailed structural analysis together with second harmonic generation experiments, we show that the compound actually adopts a non-centrosymmetric structure. A large anomalous Hall conductivity of 102 {\\Omega}^(-1) cm^(-1) at low temperature stems from intrinsic origin, which is larger than any previously reported values in bulk Cr-Te system. In addition, we observe a hump-like feature in the field-dependent Hall resistivity data, resembling a typical topological Hall signal. We demonstrate that the feature is highly tunable and is not related to topological Hall effect even though we observe N\\'eel-type skyrmions by Lorentz transmission electron microscopy which is consistent with the non-centrosymmetric structure of the compound.","sentences":["Trigonal Cr5Te8, a self-intercalated van der Waals ferromagnet with an out of plane magnetic anisotropy, has long been known to crystallise in a centrosymmetric structure.","Through detailed structural analysis together with second harmonic generation experiments, we show that the compound actually adopts a non-centrosymmetric structure.","A large anomalous Hall conductivity of 102 {\\Omega}^(-1) cm^(-1) at low temperature stems from intrinsic origin, which is larger than any previously reported values in bulk Cr-Te system.","In addition, we observe a hump-like feature in the field-dependent Hall resistivity data, resembling a typical topological Hall signal.","We demonstrate that the feature is highly tunable and is not related to topological Hall effect even though we observe N\\'eel-type skyrmions by Lorentz transmission electron microscopy which is consistent with the non-centrosymmetric structure of the compound."],"url":"http://arxiv.org/abs/2404.03391v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 11:43:51","title":"Combinatorial Dyson-Schwinger Equations of Quartic Matrix Field Theory","abstract":"Matrix field theory is a combinatorially non-local field theory which has recently been found to be a non-trivial but solvable QFT example. To generalize such non-perturbative structures to other models, a more combinatorial understanding of Dyson-Schwinger equations and their solutions is of high interest. To this end we consider combinatorial Dyson-Schwinger equations manifestly relying on the Hopf-algebraic structure of perturbative renormalization. We find that these equations are fully compatible with renormalization, relying only on the superficially divergent diagrams which are planar ribbon graphs, i.e. decompleted dual combinatorial maps. Still, they are of a similar kind as in realistic models of local QFT, featuring in particular an infinite number of primitive diagrams as well as graph-dependent combinatorial factors.","sentences":["Matrix field theory is a combinatorially non-local field theory which has recently been found to be a non-trivial but solvable QFT example.","To generalize such non-perturbative structures to other models, a more combinatorial understanding of Dyson-Schwinger equations and their solutions is of high interest.","To this end we consider combinatorial Dyson-Schwinger equations manifestly relying on the Hopf-algebraic structure of perturbative renormalization.","We find that these equations are fully compatible with renormalization, relying only on the superficially divergent diagrams which are planar ribbon graphs, i.e. decompleted dual combinatorial maps.","Still, they are of a similar kind as in realistic models of local QFT, featuring in particular an infinite number of primitive diagrams as well as graph-dependent combinatorial factors."],"url":"http://arxiv.org/abs/2404.03389v1","category":"math-ph"}
{"created":"2024-04-04 11:43:39","title":"An observation concerning highly ramified $\u03b5$-factors","abstract":"In this note we prove a quantitative stability result for the $\\epsilon$-factors associated to generic irreducible representations of $\\textrm{GL}_n(F)$ under twists by highly ramified characters, where $F$ is a non-archimedean local field.","sentences":["In this note we prove a quantitative stability result for the $\\epsilon$-factors associated to generic irreducible representations of $\\textrm{GL}_n(F)$ under twists by highly ramified characters, where $F$ is a non-archimedean local field."],"url":"http://arxiv.org/abs/2404.03388v1","category":"math.RT"}
{"created":"2024-04-04 11:37:55","title":"SENSOR: Imitate Third-Person Expert's Behaviors via Active Sensoring","abstract":"In many real-world visual Imitation Learning (IL) scenarios, there is a misalignment between the agent's and the expert's perspectives, which might lead to the failure of imitation. Previous methods have generally solved this problem by domain alignment, which incurs extra computation and storage costs, and these methods fail to handle the \\textit{hard cases} where the viewpoint gap is too large. To alleviate the above problems, we introduce active sensoring in the visual IL setting and propose a model-based SENSory imitatOR (SENSOR) to automatically change the agent's perspective to match the expert's. SENSOR jointly learns a world model to capture the dynamics of latent states, a sensor policy to control the camera, and a motor policy to control the agent. Experiments on visual locomotion tasks show that SENSOR can efficiently simulate the expert's perspective and strategy, and outperforms most baseline methods.","sentences":["In many real-world visual Imitation Learning (IL) scenarios, there is a misalignment between the agent's and the expert's perspectives, which might lead to the failure of imitation.","Previous methods have generally solved this problem by domain alignment, which incurs extra computation and storage costs, and these methods fail to handle the \\textit{hard cases} where the viewpoint gap is too large.","To alleviate the above problems, we introduce active sensoring in the visual IL setting and propose a model-based SENSory imitatOR (SENSOR) to automatically change the agent's perspective to match the expert's.","SENSOR jointly learns a world model to capture the dynamics of latent states, a sensor policy to control the camera, and a motor policy to control the agent.","Experiments on visual locomotion tasks show that SENSOR can efficiently simulate the expert's perspective and strategy, and outperforms most baseline methods."],"url":"http://arxiv.org/abs/2404.03386v1","category":"cs.RO"}
{"created":"2024-04-04 11:33:38","title":"Signed eigenvalue/vector distribution of complex order-three random tensor","abstract":"We compute the signed distribution of the eigenvalues/vectors of the complex order-three random tensor by computing a partition function of a four-fermi theory, where signs are from a Hessian determinant associated to each eigenvector. The issue of the presence of a continuous degeneracy of the eigenvectors is properly treated by a gauge-fixing. The final expression is compactly represented by a generating function, which has an expansion whose powers are the dimensions of the tensor index spaces. A crosscheck is performed by Monte Carlo simulations. By taking the large-$N$ limit we obtain a critical point where the behavior of the signed distribution qualitatively changes, and also the end of the signed distribution. The expected agreement of the end of the signed distribution with that of the genuine distribution provides a few applications, such as computing the largest eigenvalue, the geometric measure of entanglement, and the best rank-one approximation in the large-$N$ limit.","sentences":["We compute the signed distribution of the eigenvalues/vectors of the complex order-three random tensor by computing a partition function of a four-fermi theory, where signs are from a Hessian determinant associated to each eigenvector.","The issue of the presence of a continuous degeneracy of the eigenvectors is properly treated by a gauge-fixing.","The final expression is compactly represented by a generating function, which has an expansion whose powers are the dimensions of the tensor index spaces.","A crosscheck is performed by Monte Carlo simulations.","By taking the large-$N$ limit we obtain a critical point where the behavior of the signed distribution qualitatively changes, and also the end of the signed distribution.","The expected agreement of the end of the signed distribution with that of the genuine distribution provides a few applications, such as computing the largest eigenvalue, the geometric measure of entanglement, and the best rank-one approximation in the large-$N$ limit."],"url":"http://arxiv.org/abs/2404.03385v1","category":"hep-th"}
{"created":"2024-04-04 11:33:29","title":"LongVLM: Efficient Long Video Understanding via Large Language Models","abstract":"Empowered by Large Language Models (LLMs), recent advancements in VideoLLMs have driven progress in various video understanding tasks. These models encode video representations through pooling or query aggregation over a vast number of visual tokens, making computational and memory costs affordable. Despite successfully providing an overall comprehension of video content, existing VideoLLMs still face challenges in achieving detailed understanding in videos due to overlooking local information in long-term videos. To tackle this challenge, we introduce LongVLM, a straightforward yet powerful VideoLLM for long video understanding, building upon the observation that long videos often consist of sequential key events, complex actions, and camera movements. Our approach proposes to decompose long videos into multiple short-term segments and encode local features for each local segment via a hierarchical token merging module. These features are concatenated in temporal order to maintain the storyline across sequential short-term segments. Additionally, we propose to integrate global semantics into each local feature to enhance context understanding. In this way, we encode video representations that incorporate both local and global information, enabling the LLM to generate comprehensive responses for long-term videos. Experimental results on the VideoChatGPT benchmark and zero-shot video question-answering datasets demonstrate the superior capabilities of our model over the previous state-of-the-art methods. Qualitative examples demonstrate that our model produces more precise responses for long videos understanding. Code is available at \\url{https://github.com/ziplab/LongVLM}.","sentences":["Empowered by Large Language Models (LLMs), recent advancements in VideoLLMs have driven progress in various video understanding tasks.","These models encode video representations through pooling or query aggregation over a vast number of visual tokens, making computational and memory costs affordable.","Despite successfully providing an overall comprehension of video content, existing VideoLLMs still face challenges in achieving detailed understanding in videos due to overlooking local information in long-term videos.","To tackle this challenge, we introduce LongVLM, a straightforward yet powerful VideoLLM for long video understanding, building upon the observation that long videos often consist of sequential key events, complex actions, and camera movements.","Our approach proposes to decompose long videos into multiple short-term segments and encode local features for each local segment via a hierarchical token merging module.","These features are concatenated in temporal order to maintain the storyline across sequential short-term segments.","Additionally, we propose to integrate global semantics into each local feature to enhance context understanding.","In this way, we encode video representations that incorporate both local and global information, enabling the LLM to generate comprehensive responses for long-term videos.","Experimental results on the VideoChatGPT benchmark and zero-shot video question-answering datasets demonstrate the superior capabilities of our model over the previous state-of-the-art methods.","Qualitative examples demonstrate that our model produces more precise responses for long videos understanding.","Code is available at \\url{https://github.com/ziplab/LongVLM}."],"url":"http://arxiv.org/abs/2404.03384v1","category":"cs.CV"}
{"created":"2024-04-04 11:29:05","title":"DIDA: Denoised Imitation Learning based on Domain Adaptation","abstract":"Imitating skills from low-quality datasets, such as sub-optimal demonstrations and observations with distractors, is common in real-world applications. In this work, we focus on the problem of Learning from Noisy Demonstrations (LND), where the imitator is required to learn from data with noise that often occurs during the processes of data collection or transmission. Previous IL methods improve the robustness of learned policies by injecting an adversarially learned Gaussian noise into pure expert data or utilizing additional ranking information, but they may fail in the LND setting. To alleviate the above problems, we propose Denoised Imitation learning based on Domain Adaptation (DIDA), which designs two discriminators to distinguish the noise level and expertise level of data, facilitating a feature encoder to learn task-related but domain-agnostic representations. Experiment results on MuJoCo demonstrate that DIDA can successfully handle challenging imitation tasks from demonstrations with various types of noise, outperforming most baseline methods.","sentences":["Imitating skills from low-quality datasets, such as sub-optimal demonstrations and observations with distractors, is common in real-world applications.","In this work, we focus on the problem of Learning from Noisy Demonstrations (LND), where the imitator is required to learn from data with noise that often occurs during the processes of data collection or transmission.","Previous IL methods improve the robustness of learned policies by injecting an adversarially learned Gaussian noise into pure expert data or utilizing additional ranking information, but they may fail in the LND setting.","To alleviate the above problems, we propose Denoised Imitation learning based on Domain Adaptation (DIDA), which designs two discriminators to distinguish the noise level and expertise level of data, facilitating a feature encoder to learn task-related but domain-agnostic representations.","Experiment results on MuJoCo demonstrate that DIDA can successfully handle challenging imitation tasks from demonstrations with various types of noise, outperforming most baseline methods."],"url":"http://arxiv.org/abs/2404.03382v1","category":"cs.LG"}
{"created":"2024-04-04 11:27:54","title":"Learning to Plan and Generate Text with Citations","abstract":"The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as blueprints of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an abstractive model where questions are generated from scratch, and an extractive model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.","sentences":["The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence.","In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text.","We conceptualize plans as a sequence of questions which serve as blueprints of the generated content and its organization.","We propose two attribution models that utilize different variants of blueprints, an abstractive model where questions are generated from scratch, and an extractive model where questions are copied from the input.","Experiments on long-form question-answering show that planning consistently improves attribution quality.","Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component."],"url":"http://arxiv.org/abs/2404.03381v1","category":"cs.CL"}
{"created":"2024-04-04 11:26:51","title":"On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers","abstract":"Graph transformers have recently received significant attention in graph learning, partly due to their ability to capture more global interaction via self-attention. Nevertheless, while higher-order graph neural networks have been reasonably well studied, the exploration of extending graph transformers to higher-order variants is just starting. Both theoretical understanding and empirical results are limited. In this paper, we provide a systematic study of the theoretical expressive power of order-$k$ graph transformers and sparse variants. We first show that, an order-$k$ graph transformer without additional structural information is less expressive than the $k$-Weisfeiler Lehman ($k$-WL) test despite its high computational cost. We then explore strategies to both sparsify and enhance the higher-order graph transformers, aiming to improve both their efficiency and expressiveness. Indeed, sparsification based on neighborhood information can enhance the expressive power, as it provides additional information about input graph structures. In particular, we show that a natural neighborhood-based sparse order-$k$ transformer model is not only computationally efficient, but also expressive -- as expressive as $k$-WL test. We further study several other sparse graph attention models that are computationally efficient and provide their expressiveness analysis. Finally, we provide experimental results to show the effectiveness of the different sparsification strategies.","sentences":["Graph transformers have recently received significant attention in graph learning, partly due to their ability to capture more global interaction via self-attention.","Nevertheless, while higher-order graph neural networks have been reasonably well studied, the exploration of extending graph transformers to higher-order variants is just starting.","Both theoretical understanding and empirical results are limited.","In this paper, we provide a systematic study of the theoretical expressive power of order-$k$ graph transformers and sparse variants.","We first show that, an order-$k$ graph transformer without additional structural information is less expressive than the $k$-Weisfeiler Lehman ($k$-WL) test despite its high computational cost.","We then explore strategies to both sparsify and enhance the higher-order graph transformers, aiming to improve both their efficiency and expressiveness.","Indeed, sparsification based on neighborhood information can enhance the expressive power, as it provides additional information about input graph structures.","In particular, we show that a natural neighborhood-based sparse order-$k$ transformer model is not only computationally efficient, but also expressive -- as expressive as $k$-WL test.","We further study several other sparse graph attention models that are computationally efficient and provide their expressiveness analysis.","Finally, we provide experimental results to show the effectiveness of the different sparsification strategies."],"url":"http://arxiv.org/abs/2404.03380v1","category":"cs.LG"}
{"created":"2024-04-04 11:25:38","title":"Alternating Quantifiers in Uniform One-Dimensional Fragments with an Excursion into Three-Variable Logic","abstract":"The uniform one-dimensional fragment of first-order logic was introduced a few years ago as a generalization of the two-variable fragment to contexts involving relations of arity greater than two. Quantifiers in this logic are used in blocks, each block consisting only of existential quantifiers or only of universal quantifiers. In this paper we consider the possibility of mixing both types of quantifiers in blocks. We show the finite (exponential) model property and NExpTime-completeness of the satisfiability problem for two restrictions of the resulting formalism: in the first we require that every block of quantifiers is either purely universal or ends with the existential quantifier, in the second we restrict the number of variables to three; in both equality is not allowed. We also extend the second variation to a rich subfragment of the three-variable fragment (without equality) that still has the finite model property and decidable, NExpTime{}-complete satisfiability.","sentences":["The uniform one-dimensional fragment of first-order logic was introduced a few years ago as a generalization of the two-variable fragment to contexts involving relations of arity greater than two.","Quantifiers in this logic are used in blocks, each block consisting only of existential quantifiers or only of universal quantifiers.","In this paper we consider the possibility of mixing both types of quantifiers in blocks.","We show the finite (exponential) model property and NExpTime-completeness of the satisfiability problem for two restrictions of the resulting formalism: in the first we require that every block of quantifiers is either purely universal or ends with the existential quantifier, in the second we restrict the number of variables to three; in both equality is not allowed.","We also extend the second variation to a rich subfragment of the three-variable fragment (without equality) that still has the finite model property and decidable, NExpTime{}-complete satisfiability."],"url":"http://arxiv.org/abs/2404.03377v1","category":"cs.LO"}
{"created":"2024-04-04 11:17:29","title":"On monogenic functions and the Dirac complex of two vector variables","abstract":"A monogenic function of two vector variables is a function annihilated by the operator consisting of two Dirac operators, which are associated to two variables, respectively. We give the explicit form of differential operators in the Dirac complex resolving this operator and prove its ellipticity directly. This open the door to apply the method of several complex variables to investigate this kind of monogenic functions. We prove the Poincar\\'e lemma for this complex, i.e. the non-homogeneous equations are solvable under the compatibility condition by solving the associated Hodge Laplacian equations of fourth order. As corollaries, we establish the Bochner--Martinelli integral representation formula for this differential operator and the Hartogs' extension phenomenon for monogenic functions. We also apply abstract duality theorem to the Dirac complex to obtain the generalization of Malgrange's vanishing theorem and establish the Hartogs--Bochner extension phenomenon for monogenic functions under the moment condition.","sentences":["A monogenic function of two vector variables is a function annihilated by the operator consisting of two Dirac operators, which are associated to two variables, respectively.","We give the explicit form of differential operators in the Dirac complex resolving this operator and prove its ellipticity directly.","This open the door to apply the method of several complex variables to investigate this kind of monogenic functions.","We prove the Poincar\\'e lemma for this complex, i.e. the non-homogeneous equations are solvable under the compatibility condition by solving the associated Hodge Laplacian equations of fourth order.","As corollaries, we establish the Bochner--Martinelli integral representation formula for this differential operator and the Hartogs' extension phenomenon for monogenic functions.","We also apply abstract duality theorem to the Dirac complex to obtain the generalization of Malgrange's vanishing theorem and establish the Hartogs--Bochner extension phenomenon for monogenic functions under the moment condition."],"url":"http://arxiv.org/abs/2404.03374v1","category":"math.CV"}
{"created":"2024-04-04 11:16:22","title":"Riemann-Hilbert problems, Toeplitz operators and ergosurfaces","abstract":"The Riemann-Hilbert approach, in conjunction with the canonical Wiener-Hopf factorisation of certain matrix functions called monodromy matrices, enables one to obtain explicit solutions to the non-linear field equations of some gravitational theories. These solutions are encoded in the elements of a matrix $M$ depending on the Weyl coordinates $\\rho$ and $v$, determined by that factorisation. We address here, for the first time, the underlying question of what happens when a canonical Wiener-Hopf factorisation does not exist, using the close connection of Wiener-Hopf factorisation with Toeplitz operators to study this question. For the case of rational monodromy matrices, we prove that the non-existence of a canonical Wiener-Hopf factorisation determines curves in the $(\\rho,v)$ plane on which some elements of $M(\\rho,v)$ tend to infinity, but where the space-time metric may still be well behaved. In the case of uncharged rotating black holes in four space-time dimensions and, for certain choices of coordinates, in five space-time dimensions, we show that these curves correspond to their ergosurfaces.","sentences":["The Riemann-Hilbert approach, in conjunction with the canonical Wiener-Hopf factorisation of certain matrix functions called monodromy matrices, enables one to obtain explicit solutions to the non-linear field equations of some gravitational theories.","These solutions are encoded in the elements of a matrix $M$ depending on the Weyl coordinates $\\rho$ and $v$, determined by that factorisation.","We address here, for the first time, the underlying question of what happens when a canonical Wiener-Hopf factorisation does not exist, using the close connection of Wiener-Hopf factorisation with Toeplitz operators to study this question.","For the case of rational monodromy matrices, we prove that the non-existence of a canonical Wiener-Hopf factorisation determines curves in the $(\\rho,v)$ plane on which some elements of $M(\\rho,v)$ tend to infinity, but where the space-time metric may still be well behaved.","In the case of uncharged rotating black holes in four space-time dimensions and, for certain choices of coordinates, in five space-time dimensions, we show that these curves correspond to their ergosurfaces."],"url":"http://arxiv.org/abs/2404.03373v1","category":"math-ph"}
{"created":"2024-04-04 11:07:15","title":"New fractional classifications of papers based on two generations of references and on the ASJC Scopus scheme","abstract":"This paper presents and evaluates a set of methods to classify individual Scopus publications using their references back to the second generation, where each publication can be assigned fractionally into up to five ASJC (All Science Journal Classifications) categories, excluding the Multidisciplinary area and the miscellaneous categories. Based on proposals by Glanzel et al. (1999a, 1999b, 2021), some additional parameters are established that allow different results to be obtained depending on how category membership is weighted or how the acceptance thresholds for multiple assignments are established. Various classifications are obtained, and then compared with each other, with the original ASJC Scopus journal classification, and with the AAC (Authors Assignation Collection) classification of a previous study (Alvarez-Llorente et al., 2023) in which the papers corresponding authors assign them the most appropriate categories. Classifications in which a high threshold is set for allowing assignments to multiple categories, combined with the use of first- and second-generation references and averaging over the number of references, provide the most promising results, improving over other reference-based reclassification proposals in terms of granularity, and over the Scopus classification itself in such aspects as the homogeneity of the publications assigned to a category. They also show greater coincidence with the AAC classification.","sentences":["This paper presents and evaluates a set of methods to classify individual Scopus publications using their references back to the second generation, where each publication can be assigned fractionally into up to five ASJC (All Science Journal Classifications) categories, excluding the Multidisciplinary area and the miscellaneous categories.","Based on proposals by Glanzel et al. (1999a, 1999b, 2021), some additional parameters are established that allow different results to be obtained depending on how category membership is weighted or how the acceptance thresholds for multiple assignments are established.","Various classifications are obtained, and then compared with each other, with the original ASJC Scopus journal classification, and with the AAC (Authors Assignation Collection) classification of a previous study (Alvarez-Llorente et al., 2023) in which the papers corresponding authors assign them the most appropriate categories.","Classifications in which a high threshold is set for allowing assignments to multiple categories, combined with the use of first- and second-generation references and averaging over the number of references, provide the most promising results, improving over other reference-based reclassification proposals in terms of granularity, and over the Scopus classification itself in such aspects as the homogeneity of the publications assigned to a category.","They also show greater coincidence with the AAC classification."],"url":"http://arxiv.org/abs/2404.03366v1","category":"cs.DL"}
{"created":"2024-04-04 11:04:59","title":"High energy head-on particle collisions near event horizons: classifcation of scenarios","abstract":"We consider head-on collisions of two particles near the event horizon. Particle 1 is outgoing, particle 2 is ingoing. We elucidate, in which case the energy $E_{c.m.}$ in the center of mass frame can grow unbounded. If the proper time between the horizon and an arbitrary point outside it for particle 1 is finite, we deal with a white hole. If it is infinite, we deal with a black hole. Particles can be either free or experience the action of a finite force. Our results are complementary to those for the standard BSW effect when particles move in the same direction. The results rely on classification of particles developed in our previous work H.V. Ovcharenko, O.B. Zaslavskii, Phys. Rev. D 108, 064029 (2023).","sentences":["We consider head-on collisions of two particles near the event horizon.","Particle 1 is outgoing, particle 2 is ingoing.","We elucidate, in which case the energy $E_{c.m.}$ in the center of mass frame can grow unbounded.","If the proper time between the horizon and an arbitrary point outside it for particle 1 is finite, we deal with a white hole.","If it is infinite, we deal with a black hole.","Particles can be either free or experience the action of a finite force.","Our results are complementary to those for the standard BSW effect when particles move in the same direction.","The results rely on classification of particles developed in our previous work H.V. Ovcharenko, O.B. Zaslavskii, Phys.","Rev. D 108, 064029 (2023)."],"url":"http://arxiv.org/abs/2404.03364v1","category":"gr-qc"}
{"created":"2024-04-04 10:56:30","title":"REACT: Revealing Evolutionary Action Consequence Trajectories for Interpretable Reinforcement Learning","abstract":"To enhance the interpretability of Reinforcement Learning (RL), we propose Revealing Evolutionary Action Consequence Trajectories (REACT). In contrast to the prevalent practice of validating RL models based on their optimal behavior learned during training, we posit that considering a range of edge-case trajectories provides a more comprehensive understanding of their inherent behavior. To induce such scenarios, we introduce a disturbance to the initial state, optimizing it through an evolutionary algorithm to generate a diverse population of demonstrations. To evaluate the fitness of trajectories, REACT incorporates a joint fitness function that encourages both local and global diversity in the encountered states and chosen actions. Through assessments with policies trained for varying durations in discrete and continuous environments, we demonstrate the descriptive power of REACT. Our results highlight its effectiveness in revealing nuanced aspects of RL models' behavior beyond optimal performance, thereby contributing to improved interpretability.","sentences":["To enhance the interpretability of Reinforcement Learning (RL), we propose Revealing Evolutionary Action Consequence Trajectories (REACT).","In contrast to the prevalent practice of validating RL models based on their optimal behavior learned during training, we posit that considering a range of edge-case trajectories provides a more comprehensive understanding of their inherent behavior.","To induce such scenarios, we introduce a disturbance to the initial state, optimizing it through an evolutionary algorithm to generate a diverse population of demonstrations.","To evaluate the fitness of trajectories, REACT incorporates a joint fitness function that encourages both local and global diversity in the encountered states and chosen actions.","Through assessments with policies trained for varying durations in discrete and continuous environments, we demonstrate the descriptive power of REACT.","Our results highlight its effectiveness in revealing nuanced aspects of RL models' behavior beyond optimal performance, thereby contributing to improved interpretability."],"url":"http://arxiv.org/abs/2404.03359v1","category":"cs.LG"}
{"created":"2024-04-04 10:51:22","title":"Degenerate Chenciner bifurcation revisited","abstract":"Generic results for degenerate Chenciner (generalized Neimark-Sacker) bifurcation are obtained in the present work. The bifurcation arises in two-dimensional discrete-time systems with two independent parameters. We define in this work a new transformation of parameters, which enables the study of the bifurcation when the degeneracy occurs. By the four bifurcation diagrams we obtain, new behaviors hidden by the degeneracy are brought to light.","sentences":["Generic results for degenerate Chenciner (generalized Neimark-Sacker) bifurcation are obtained in the present work.","The bifurcation arises in two-dimensional discrete-time systems with two independent parameters.","We define in this work a new transformation of parameters, which enables the study of the bifurcation when the degeneracy occurs.","By the four bifurcation diagrams we obtain, new behaviors hidden by the degeneracy are brought to light."],"url":"http://arxiv.org/abs/2404.03357v1","category":"math.DS"}
{"created":"2024-04-04 10:45:23","title":"A Comprehensive Survey on Self-Supervised Learning for Recommendation","abstract":"Recommender systems play a crucial role in tackling the challenge of information overload by delivering personalized recommendations based on individual user preferences. Deep learning techniques, such as RNNs, GNNs, and Transformer architectures, have significantly propelled the advancement of recommender systems by enhancing their comprehension of user behaviors and preferences. However, supervised learning methods encounter challenges in real-life scenarios due to data sparsity, resulting in limitations in their ability to learn representations effectively. To address this, self-supervised learning (SSL) techniques have emerged as a solution, leveraging inherent data structures to generate supervision signals without relying solely on labeled data. By leveraging unlabeled data and extracting meaningful representations, recommender systems utilizing SSL can make accurate predictions and recommendations even when confronted with data sparsity. In this paper, we provide a comprehensive review of self-supervised learning frameworks designed for recommender systems, encompassing a thorough analysis of over 170 papers. We conduct an exploration of nine distinct scenarios, enabling a comprehensive understanding of SSL-enhanced recommenders in different contexts. For each domain, we elaborate on different self-supervised learning paradigms, namely contrastive learning, generative learning, and adversarial learning, so as to present technical details of how SSL enhances recommender systems in various contexts. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-SSLRec-Papers.","sentences":["Recommender systems play a crucial role in tackling the challenge of information overload by delivering personalized recommendations based on individual user preferences.","Deep learning techniques, such as RNNs, GNNs, and Transformer architectures, have significantly propelled the advancement of recommender systems by enhancing their comprehension of user behaviors and preferences.","However, supervised learning methods encounter challenges in real-life scenarios due to data sparsity, resulting in limitations in their ability to learn representations effectively.","To address this, self-supervised learning (SSL) techniques have emerged as a solution, leveraging inherent data structures to generate supervision signals without relying solely on labeled data.","By leveraging unlabeled data and extracting meaningful representations, recommender systems utilizing SSL can make accurate predictions and recommendations even when confronted with data sparsity.","In this paper, we provide a comprehensive review of self-supervised learning frameworks designed for recommender systems, encompassing a thorough analysis of over 170 papers.","We conduct an exploration of nine distinct scenarios, enabling a comprehensive understanding of SSL-enhanced recommenders in different contexts.","For each domain, we elaborate on different self-supervised learning paradigms, namely contrastive learning, generative learning, and adversarial learning, so as to present technical details of how SSL enhances recommender systems in various contexts.","We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-SSLRec-Papers."],"url":"http://arxiv.org/abs/2404.03354v1","category":"cs.IR"}
{"created":"2024-04-04 10:40:24","title":"ToMCCA: A Toy Monte Carlo Coalescence Afterburner","abstract":"The study of antinuclei in cosmic rays provides a unique opportunity to probe physics beyond the Standard Model. Antinuclei in our Galaxy may stem either from annihilation or decay of dark matter, or from collisions of cosmic rays with the interstellar medium, which constitute the background of indirect dark matter searches. Understanding the formation mechanism of (anti)nuclei is crucial for setting limits on their production in space. Coalescence models, which describe the formation of light nuclei from final-state interaction of nucleons, have been widely employed in high-energy collisions. In this work, we introduce ToMCCA (Toy Monte Carlo Coalescence Afterburner), which allows for detailed studies of the nuclear formation processes without the overload of general-purpose event generators. ToMCCA contains parameterizations of the multiplicity dependence of the transverse momentum distributions of protons and of the baryon-emitting source size, extracted from ALICE measurements in pp collisions at $\\sqrt{s} = 5 - 13$ TeV, as well as of the event multiplicity distributions, taken from the EPOS event generator. ToMCCA provides predictions of the deuteron transverse momentum distributions, with an agreement of $\\sim5\\%$ with the experimental data. The results of ToMCCA show that the coalescence mechanism in pp collisions depends only on the event multiplicity rather than on the collision system or its energy. This allows the model to be utilized for predictions at lower center-of-mass collision energies, which are the most relevant for the production of antinuclei from processes related to dark matter. This model can also be extended to heavier nuclei as long as the target nucleus wave function and its Wigner function are known.","sentences":["The study of antinuclei in cosmic rays provides a unique opportunity to probe physics beyond the Standard Model.","Antinuclei in our Galaxy may stem either from annihilation or decay of dark matter, or from collisions of cosmic rays with the interstellar medium, which constitute the background of indirect dark matter searches.","Understanding the formation mechanism of (anti)nuclei is crucial for setting limits on their production in space.","Coalescence models, which describe the formation of light nuclei from final-state interaction of nucleons, have been widely employed in high-energy collisions.","In this work, we introduce ToMCCA (Toy Monte Carlo Coalescence Afterburner), which allows for detailed studies of the nuclear formation processes without the overload of general-purpose event generators.","ToMCCA contains parameterizations of the multiplicity dependence of the transverse momentum distributions of protons and of the baryon-emitting source size, extracted from ALICE measurements in pp collisions at $\\sqrt{s} = 5 - 13$ TeV, as well as of the event multiplicity distributions, taken from the EPOS event generator.","ToMCCA provides predictions of the deuteron transverse momentum distributions, with an agreement of $\\sim5\\%$ with the experimental data.","The results of ToMCCA show that the coalescence mechanism in pp collisions depends only on the event multiplicity rather than on the collision system or its energy.","This allows the model to be utilized for predictions at lower center-of-mass collision energies, which are the most relevant for the production of antinuclei from processes related to dark matter.","This model can also be extended to heavier nuclei as long as the target nucleus wave function and its Wigner function are known."],"url":"http://arxiv.org/abs/2404.03352v1","category":"hep-ph"}
{"created":"2024-04-04 10:33:21","title":"Analysis of degenerate Chenciner bifurcation","abstract":"Degenerate Chenciner bifurcation in generic discrete-time dynamical systems is studied in this work. While the non-degenerate Chenciner bifurcation can be described by 2 bifurcation diagrams, the degeneracy we studied in this work gives rise to 32 different bifurcation diagrams.","sentences":["Degenerate Chenciner bifurcation in generic discrete-time dynamical systems is studied in this work.","While the non-degenerate Chenciner bifurcation can be described by 2 bifurcation diagrams, the degeneracy we studied in this work gives rise to 32 different bifurcation diagrams."],"url":"http://arxiv.org/abs/2404.03350v1","category":"math.DS"}
{"created":"2024-04-04 10:28:55","title":"Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations","abstract":"In recent years, there has been a notable increase in the deployment of machine learning (ML) models as services (MLaaS) across diverse production software applications. In parallel, explainable AI (XAI) continues to evolve, addressing the necessity for transparency and trustworthiness in ML models. XAI techniques aim to enhance the transparency of ML models by providing insights, in terms of the model's explanations, into their decision-making process. Simultaneously, some MLaaS platforms now offer explanations alongside the ML prediction outputs. This setup has elevated concerns regarding vulnerabilities in MLaaS, particularly in relation to privacy leakage attacks such as model extraction attacks (MEA). This is due to the fact that explanations can unveil insights about the inner workings of the model which could be exploited by malicious users. In this work, we focus on investigating how model explanations, particularly Generative adversarial networks (GANs)-based counterfactual explanations (CFs), can be exploited for performing MEA within the MLaaS platform. We also delve into assessing the effectiveness of incorporating differential privacy (DP) as a mitigation strategy. To this end, we first propose a novel MEA methodology based on Knowledge Distillation (KD) to enhance the efficiency of extracting a substitute model of a target model exploiting CFs. Then, we advise an approach for training CF generators incorporating DP to generate private CFs. We conduct thorough experimental evaluations on real-world datasets and demonstrate that our proposed KD-based MEA can yield a high-fidelity substitute model with reduced queries with respect to baseline approaches. Furthermore, our findings reveal that the inclusion of a privacy layer impacts the performance of the explainer, the quality of CFs, and results in a reduction in the MEA performance.","sentences":["In recent years, there has been a notable increase in the deployment of machine learning (ML) models as services (MLaaS) across diverse production software applications.","In parallel, explainable AI (XAI) continues to evolve, addressing the necessity for transparency and trustworthiness in ML models.","XAI techniques aim to enhance the transparency of ML models by providing insights, in terms of the model's explanations, into their decision-making process.","Simultaneously, some MLaaS platforms now offer explanations alongside the ML prediction outputs.","This setup has elevated concerns regarding vulnerabilities in MLaaS, particularly in relation to privacy leakage attacks such as model extraction attacks (MEA).","This is due to the fact that explanations can unveil insights about the inner workings of the model which could be exploited by malicious users.","In this work, we focus on investigating how model explanations, particularly Generative adversarial networks (GANs)-based counterfactual explanations (CFs), can be exploited for performing MEA within the MLaaS platform.","We also delve into assessing the effectiveness of incorporating differential privacy (DP) as a mitigation strategy.","To this end, we first propose a novel MEA methodology based on Knowledge Distillation (KD) to enhance the efficiency of extracting a substitute model of a target model exploiting CFs.","Then, we advise an approach for training CF generators incorporating DP to generate private CFs.","We conduct thorough experimental evaluations on real-world datasets and demonstrate that our proposed KD-based MEA can yield a high-fidelity substitute model with reduced queries with respect to baseline approaches.","Furthermore, our findings reveal that the inclusion of a privacy layer impacts the performance of the explainer, the quality of CFs, and results in a reduction in the MEA performance."],"url":"http://arxiv.org/abs/2404.03348v1","category":"cs.LG"}
{"created":"2024-04-04 10:21:24","title":"Bose and Fermi gases in metric-affine gravity and linear Generalized Uncertainty Principle","abstract":"We examine the relationship between Palatini-like theories of gravity and models incorporating linear generalized uncertainty principles. Additionally, we delve into the thermodynamics of systems comprising both Bose and Fermi gases. Our analysis encompasses the equations of state for various systems, including general Fermi gases, degenerate Fermi gases, Boltzmann gases, Bose gases such as phonons and photons, as well as Bose-Einstein condensates and liquid helium.","sentences":["We examine the relationship between Palatini-like theories of gravity and models incorporating linear generalized uncertainty principles.","Additionally, we delve into the thermodynamics of systems comprising both Bose and Fermi gases.","Our analysis encompasses the equations of state for various systems, including general Fermi gases, degenerate Fermi gases, Boltzmann gases, Bose gases such as phonons and photons, as well as Bose-Einstein condensates and liquid helium."],"url":"http://arxiv.org/abs/2404.03345v1","category":"gr-qc"}
{"created":"2024-04-04 10:18:03","title":"Schroedinger's Threshold: When the AUC doesn't predict Accuracy","abstract":"The Area Under Curve measure (AUC) seems apt to evaluate and compare diverse models, possibly without calibration. An important example of AUC application is the evaluation and benchmarking of models that predict faithfulness of generated text. But we show that the AUC yields an academic and optimistic notion of accuracy that can misalign with the actual accuracy observed in application, yielding significant changes in benchmark rankings. To paint a more realistic picture of downstream model performance (and prepare a model for actual application), we explore different calibration modes, testing calibration data and method.","sentences":["The Area Under Curve measure (AUC) seems apt to evaluate and compare diverse models, possibly without calibration.","An important example of AUC application is the evaluation and benchmarking of models that predict faithfulness of generated text.","But we show that the AUC yields an academic and optimistic notion of accuracy that can misalign with the actual accuracy observed in application, yielding significant changes in benchmark rankings.","To paint a more realistic picture of downstream model performance (and prepare a model for actual application), we explore different calibration modes, testing calibration data and method."],"url":"http://arxiv.org/abs/2404.03344v1","category":"cs.CL"}
{"created":"2024-04-04 10:06:45","title":"Influence of Gameplay Duration, Hand Tracking, and Controller Based Control Methods on UX in VR","abstract":"Inside-out tracking is growing popular in consumer VR, enhancing accessibility. It uses HMD camera data and neural networks for effective hand tracking. However, limited user experience studies have compared this method to traditional controllers, with no consensus on the optimal control technique. This paper investigates the impact of control methods and gaming duration on VR user experience, hypothesizing hand tracking might be preferred for short sessions and by users new to VR due to its simplicity. Through a lab study with twenty participants, evaluating presence, emotional response, UX quality, and flow, findings revealed control type and session length affect user experience without significant interaction. Controllers were generally superior, attributed to their reliability, and longer sessions increased presence and realism. The study found that individuals with more VR experience were more inclined to recommend hand tracking to others, which contradicted predictions.","sentences":["Inside-out tracking is growing popular in consumer VR, enhancing accessibility.","It uses HMD camera data and neural networks for effective hand tracking.","However, limited user experience studies have compared this method to traditional controllers, with no consensus on the optimal control technique.","This paper investigates the impact of control methods and gaming duration on VR user experience, hypothesizing hand tracking might be preferred for short sessions and by users new to VR due to its simplicity.","Through a lab study with twenty participants, evaluating presence, emotional response, UX quality, and flow, findings revealed control type and session length affect user experience without significant interaction.","Controllers were generally superior, attributed to their reliability, and longer sessions increased presence and realism.","The study found that individuals with more VR experience were more inclined to recommend hand tracking to others, which contradicted predictions."],"url":"http://arxiv.org/abs/2404.03337v1","category":"cs.HC"}
{"created":"2024-04-04 10:00:35","title":"A classification of well-behaved graph clustering schemes","abstract":"Community detection in graphs is a problem that is likely to be relevant whenever network data appears, and consequently the problem has received much attention with many different methods and algorithms applied. However, many of these methods are hard to study theoretically, and they optimise for somewhat different goals. A general and rigorous account of the problem and possible methods remains elusive.   We study the class of all clustering methods that are monotone under addition of vertices and edges, phrasing this as a functoriality notion. We show that if additionally we require the methods to have no resolution limit in a strong sense, this is equivalent to a notion of representability, which requires them to be explainable and determined by a representing set of graphs. We show that representable clustering methods are always computable in polynomial time, and in any nowhere dense class they are computable in roughly quadratic time.   Finally, we extend our definitions to the case of hierarchical clustering, and give a notion of representability for hierarchical clustering schemes.","sentences":["Community detection in graphs is a problem that is likely to be relevant whenever network data appears, and consequently the problem has received much attention with many different methods and algorithms applied.","However, many of these methods are hard to study theoretically, and they optimise for somewhat different goals.","A general and rigorous account of the problem and possible methods remains elusive.   ","We study the class of all clustering methods that are monotone under addition of vertices and edges, phrasing this as a functoriality notion.","We show that if additionally we require the methods to have no resolution limit in a strong sense, this is equivalent to a notion of representability, which requires them to be explainable and determined by a representing set of graphs.","We show that representable clustering methods are always computable in polynomial time, and in any nowhere dense class they are computable in roughly quadratic time.   ","Finally, we extend our definitions to the case of hierarchical clustering, and give a notion of representability for hierarchical clustering schemes."],"url":"http://arxiv.org/abs/2404.03332v1","category":"math.CO"}
{"created":"2024-04-04 09:53:32","title":"Primordial Black Hole Interpretation in Subsolar Mass Gravitational Wave Candidate SSM200308","abstract":"In the recent second part of the third observation run by the LIGO-Virgo-KAGRA collaboration, a candidate with sub-solar mass components was reported, which we labelled as SSM200308. This study investigates the premise that primordial black holes (PBHs), arising from Gaussian perturbation collapses, could explain SSM200308. Through Bayesian analysis, we obtain the primordial curvature power spectrum that leads to the merger rate of PBHs aligning with observational data as long as they constitute $3.54^{+7.66}_{-2.82}\\times 10^{-3}$ of the dark matter. However, while the gravitational wave (GW) background from binary PBH mergers is within current observational limits, the scalar-induced GWs associated with PBH formation exceed the constraints imposed by pulsar timing arrays, challenging the Gaussian perturbation collapse PBH model as the source of SSM200308.","sentences":["In the recent second part of the third observation run by the LIGO-Virgo-KAGRA collaboration, a candidate with sub-solar mass components was reported, which we labelled as SSM200308.","This study investigates the premise that primordial black holes (PBHs), arising from Gaussian perturbation collapses, could explain SSM200308.","Through Bayesian analysis, we obtain the primordial curvature power spectrum that leads to the merger rate of PBHs aligning with observational data as long as they constitute $3.54^{+7.66}_{-2.82}\\times 10^{-3}$ of the dark matter.","However, while the gravitational wave (GW) background from binary PBH mergers is within current observational limits, the scalar-induced GWs associated with PBH formation exceed the constraints imposed by pulsar timing arrays, challenging the Gaussian perturbation collapse PBH model as the source of SSM200308."],"url":"http://arxiv.org/abs/2404.03328v1","category":"astro-ph.CO"}
{"created":"2024-04-04 09:52:22","title":"Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack","abstract":"Robotic technologies have been an indispensable part for improving human productivity since they have been helping humans in completing diverse, complex, and intensive tasks in a fast yet accurate and efficient way. Therefore, robotic technologies have been deployed in a wide range of applications, ranging from personal to industrial use-cases. However, current robotic technologies and their computing paradigm still lack embodied intelligence to efficiently interact with operational environments, respond with correct/expected actions, and adapt to changes in the environments. Toward this, recent advances in neuromorphic computing with Spiking Neural Networks (SNN) have demonstrated the potential to enable the embodied intelligence for robotics through bio-plausible computing paradigm that mimics how the biological brain works, known as \"neuromorphic artificial intelligence (AI)\". However, the field of neuromorphic AI-based robotics is still at an early stage, therefore its development and deployment for solving real-world problems expose new challenges in different design aspects, such as accuracy, adaptability, efficiency, reliability, and security. To address these challenges, this paper will discuss how we can enable embodied neuromorphic AI for robotic systems through our perspectives: (P1) Embodied intelligence based on effective learning rule, training mechanism, and adaptability; (P2) Cross-layer optimizations for energy-efficient neuromorphic computing; (P3) Representative and fair benchmarks; (P4) Low-cost reliability and safety enhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A synergistic development for energy-efficient and robust neuromorphic-based robotics. Furthermore, this paper identifies research challenges and opportunities, as well as elaborates our vision for future research development toward embodied neuromorphic AI for robotics.","sentences":["Robotic technologies have been an indispensable part for improving human productivity since they have been helping humans in completing diverse, complex, and intensive tasks in a fast yet accurate and efficient way.","Therefore, robotic technologies have been deployed in a wide range of applications, ranging from personal to industrial use-cases.","However, current robotic technologies and their computing paradigm still lack embodied intelligence to efficiently interact with operational environments, respond with correct/expected actions, and adapt to changes in the environments.","Toward this, recent advances in neuromorphic computing with Spiking Neural Networks (SNN) have demonstrated the potential to enable the embodied intelligence for robotics through bio-plausible computing paradigm that mimics how the biological brain works, known as \"neuromorphic artificial intelligence (AI)\".","However, the field of neuromorphic AI-based robotics is still at an early stage, therefore its development and deployment for solving real-world problems expose new challenges in different design aspects, such as accuracy, adaptability, efficiency, reliability, and security.","To address these challenges, this paper will discuss how we can enable embodied neuromorphic AI for robotic systems through our perspectives: (P1) Embodied intelligence based on effective learning rule, training mechanism, and adaptability; (P2) Cross-layer optimizations for energy-efficient neuromorphic computing; (P3) Representative and fair benchmarks; (P4) Low-cost reliability and safety enhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A synergistic development for energy-efficient and robust neuromorphic-based robotics.","Furthermore, this paper identifies research challenges and opportunities, as well as elaborates our vision for future research development toward embodied neuromorphic AI for robotics."],"url":"http://arxiv.org/abs/2404.03325v1","category":"cs.RO"}
{"created":"2024-04-04 09:43:43","title":"Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning","abstract":"We propose a novel architecture and method of explainable classification with Concept Bottleneck Models (CBMs). While SOTA approaches to Image Classification task work as a black box, there is a growing demand for models that would provide interpreted results. Such a models often learn to predict the distribution over class labels using additional description of this target instances, called concepts. However, existing Bottleneck methods have a number of limitations: their accuracy is lower than that of a standard model and CBMs require an additional set of concepts to leverage. We provide a framework for creating Concept Bottleneck Model from pre-trained multi-modal encoder and new CLIP-like architectures. By introducing a new type of layers known as Concept Bottleneck Layers, we outline three methods for training them: with $\\ell_1$-loss, contrastive loss and loss function based on Gumbel-Softmax distribution (Sparse-CBM), while final FC layer is still trained with Cross-Entropy. We show a significant increase in accuracy using sparse hidden layers in CLIP-based bottleneck models. Which means that sparse representation of concepts activation vector is meaningful in Concept Bottleneck Models. Moreover, with our Concept Matrix Search algorithm we can improve CLIP predictions on complex datasets without any additional training or fine-tuning. The code is available at: https://github.com/Andron00e/SparseCBM.","sentences":["We propose a novel architecture and method of explainable classification with Concept Bottleneck Models (CBMs).","While SOTA approaches to Image Classification task work as a black box, there is a growing demand for models that would provide interpreted results.","Such a models often learn to predict the distribution over class labels using additional description of this target instances, called concepts.","However, existing Bottleneck methods have a number of limitations: their accuracy is lower than that of a standard model and CBMs require an additional set of concepts to leverage.","We provide a framework for creating Concept Bottleneck Model from pre-trained multi-modal encoder and new CLIP-like architectures.","By introducing a new type of layers known as Concept Bottleneck Layers, we outline three methods for training them: with $\\ell_1$-loss, contrastive loss and loss function based on Gumbel-Softmax distribution (Sparse-CBM), while final FC layer is still trained with Cross-Entropy.","We show a significant increase in accuracy using sparse hidden layers in CLIP-based bottleneck models.","Which means that sparse representation of concepts activation vector is meaningful in Concept Bottleneck Models.","Moreover, with our Concept Matrix Search algorithm we can improve CLIP predictions on complex datasets without any additional training or fine-tuning.","The code is available at: https://github.com/Andron00e/SparseCBM."],"url":"http://arxiv.org/abs/2404.03323v1","category":"cs.CV"}
{"created":"2024-04-04 09:37:59","title":"Fusion of Mixture of Experts and Generative Artificial Intelligence in Mobile Edge Metaverse","abstract":"In the digital transformation era, Metaverse offers a fusion of virtual reality (VR), augmented reality (AR), and web technologies to create immersive digital experiences. However, the evolution of the Metaverse is slowed down by the challenges of content creation, scalability, and dynamic user interaction. Our study investigates an integration of Mixture of Experts (MoE) models with Generative Artificial Intelligence (GAI) for mobile edge computing to revolutionize content creation and interaction in the Metaverse. Specifically, we harness an MoE model's ability to efficiently manage complex data and complex tasks by dynamically selecting the most relevant experts running various sub-models to enhance the capabilities of GAI. We then present a novel framework that improves video content generation quality and consistency, and demonstrate its application through case studies. Our findings underscore the efficacy of MoE and GAI integration to redefine virtual experiences by offering a scalable, efficient pathway to harvest the Metaverse's full potential.","sentences":["In the digital transformation era, Metaverse offers a fusion of virtual reality (VR), augmented reality (AR), and web technologies to create immersive digital experiences.","However, the evolution of the Metaverse is slowed down by the challenges of content creation, scalability, and dynamic user interaction.","Our study investigates an integration of Mixture of Experts (MoE) models with Generative Artificial Intelligence (GAI) for mobile edge computing to revolutionize content creation and interaction in the Metaverse.","Specifically, we harness an MoE model's ability to efficiently manage complex data and complex tasks by dynamically selecting the most relevant experts running various sub-models to enhance the capabilities of GAI.","We then present a novel framework that improves video content generation quality and consistency, and demonstrate its application through case studies.","Our findings underscore the efficacy of MoE and GAI integration to redefine virtual experiences by offering a scalable, efficient pathway to harvest the Metaverse's full potential."],"url":"http://arxiv.org/abs/2404.03321v1","category":"cs.NI"}
{"created":"2024-04-04 09:33:52","title":"Analysis of a class of Lotka--Volterra systems","abstract":"A generalized two-dimensional cubic Lotka-Volterra model with infinitesimal parameters is studied. Three different cases have been considered, one non-degenerate and two degenerate. The local behavior of the model has been studied in the three cases. Six bifurcation diagrams with thirty different regions have been obtained in the non-degenerate case, respectively, sixteen diagrams with forty regions in the two degenerate cases.","sentences":["A generalized two-dimensional cubic Lotka-Volterra model with infinitesimal parameters is studied.","Three different cases have been considered, one non-degenerate and two degenerate.","The local behavior of the model has been studied in the three cases.","Six bifurcation diagrams with thirty different regions have been obtained in the non-degenerate case, respectively, sixteen diagrams with forty regions in the two degenerate cases."],"url":"http://arxiv.org/abs/2404.03316v1","category":"math.DS"}
{"created":"2024-04-04 09:23:02","title":"Learning to Bid in Forward Electricity Markets Using a No-Regret Algorithm","abstract":"It is a common practice in the current literature of electricity markets to use game-theoretic approaches for strategic price bidding. However, they generally rely on the assumption that the strategic bidders have prior knowledge of rival bids, either perfectly or with some uncertainty. This is not necessarily a realistic assumption. This paper takes a different approach by relaxing such an assumption and exploits a no-regret learning algorithm for repeated games. In particular, by using the \\emph{a posteriori} information about rivals' bids, a learner can implement a no-regret algorithm to optimize her/his decision making. Given this information, we utilize a multiplicative weight-update algorithm, adapting bidding strategies over multiple rounds of an auction to minimize her/his regret. Our numerical results show that when the proposed learning approach is used the social cost and the market-clearing prices can be higher than those corresponding to the classical game-theoretic approaches. The takeaway for market regulators is that electricity markets might be exposed to greater market power of suppliers than what classical analysis shows.","sentences":["It is a common practice in the current literature of electricity markets to use game-theoretic approaches for strategic price bidding.","However, they generally rely on the assumption that the strategic bidders have prior knowledge of rival bids, either perfectly or with some uncertainty.","This is not necessarily a realistic assumption.","This paper takes a different approach by relaxing such an assumption and exploits a no-regret learning algorithm for repeated games.","In particular, by using the \\emph{a posteriori} information about rivals' bids, a learner can implement a no-regret algorithm to optimize her/his decision making.","Given this information, we utilize a multiplicative weight-update algorithm, adapting bidding strategies over multiple rounds of an auction to minimize her/his regret.","Our numerical results show that when the proposed learning approach is used the social cost and the market-clearing prices can be higher than those corresponding to the classical game-theoretic approaches.","The takeaway for market regulators is that electricity markets might be exposed to greater market power of suppliers than what classical analysis shows."],"url":"http://arxiv.org/abs/2404.03314v1","category":"cs.GT"}
{"created":"2024-04-04 09:12:13","title":"Site-specific Deterministic Temperature and Humidity Forecasts with Explainable and Reliable Machine Learning","abstract":"Site-specific weather forecasts are essential to accurate prediction of power demand and are consequently of great interest to energy operators. However, weather forecasts from current numerical weather prediction (NWP) models lack the fine-scale detail to capture all important characteristics of localised real-world sites. Instead they provide weather information representing a rectangular gridbox (usually kilometres in size). Even after post-processing and bias correction, area-averaged information is usually not optimal for specific sites. Prior work on site optimised forecasts has focused on linear methods, weighted consensus averaging, time-series methods, and others. Recent developments in machine learning (ML) have prompted increasing interest in applying ML as a novel approach towards this problem. In this study, we investigate the feasibility of optimising forecasts at sites by adopting the popular machine learning model gradient boosting decision tree, supported by the Python version of the XGBoost package. Regression trees have been trained with historical NWP and site observations as training data, aimed at predicting temperature and dew point at multiple site locations across Australia. We developed a working ML framework, named 'Multi-SiteBoost' and initial testing results show a significant improvement compared with gridded values from bias-corrected NWP models. The improvement from XGBoost is found to be comparable with non-ML methods reported in literature. With the insights provided by SHapley Additive exPlanations (SHAP), this study also tests various approaches to understand the ML predictions and increase the reliability of the forecasts generated by ML.","sentences":["Site-specific weather forecasts are essential to accurate prediction of power demand and are consequently of great interest to energy operators.","However, weather forecasts from current numerical weather prediction (NWP) models lack the fine-scale detail to capture all important characteristics of localised real-world sites.","Instead they provide weather information representing a rectangular gridbox (usually kilometres in size).","Even after post-processing and bias correction, area-averaged information is usually not optimal for specific sites.","Prior work on site optimised forecasts has focused on linear methods, weighted consensus averaging, time-series methods, and others.","Recent developments in machine learning (ML) have prompted increasing interest in applying ML as a novel approach towards this problem.","In this study, we investigate the feasibility of optimising forecasts at sites by adopting the popular machine learning model gradient boosting decision tree, supported by the Python version of the XGBoost package.","Regression trees have been trained with historical NWP and site observations as training data, aimed at predicting temperature and dew point at multiple site locations across Australia.","We developed a working ML framework, named 'Multi-SiteBoost' and initial testing results show a significant improvement compared with gridded values from bias-corrected NWP models.","The improvement from XGBoost is found to be comparable with non-ML methods reported in literature.","With the insights provided by SHapley Additive exPlanations (SHAP), this study also tests various approaches to understand the ML predictions and increase the reliability of the forecasts generated by ML."],"url":"http://arxiv.org/abs/2404.03310v1","category":"physics.ao-ph"}
{"created":"2024-04-04 09:05:29","title":"Formal Verification of Linear Temporal Logic Specifications Using Hybrid Zonotope-Based Reachability Analysis","abstract":"In this paper, we introduce a hybrid zonotope-based approach for formally verifying the behavior of autonomous systems operating under Linear Temporal Logic (LTL) specifications. In particular, we formally verify the LTL formula by constructing temporal logic trees (TLT)s via backward reachability analysis (BRA). In previous works, TLTs are predominantly constructed with either highly general and computationally intensive level set-based BRA or simplistic and computationally efficient polytope-based BRA. In this work, we instead propose the construction of TLTs using hybrid zonotope-based BRA. By using hybrid zonotopes, we show that we are able to formally verify LTL specifications in a computationally efficient manner while still being able to represent complex geometries that are often present when deploying autonomous systems, such as non-convex, disjoint sets. Moreover, we evaluate our approach on a parking example, providing preliminary indications of how hybrid zonotopes facilitate computationally efficient formal verification of LTL specifications in environments that naturally lead to non-convex, disjoint geometries.","sentences":["In this paper, we introduce a hybrid zonotope-based approach for formally verifying the behavior of autonomous systems operating under Linear Temporal Logic (LTL) specifications.","In particular, we formally verify the LTL formula by constructing temporal logic trees (TLT)s via backward reachability analysis (BRA).","In previous works, TLTs are predominantly constructed with either highly general and computationally intensive level set-based BRA or simplistic and computationally efficient polytope-based BRA.","In this work, we instead propose the construction of TLTs using hybrid zonotope-based BRA.","By using hybrid zonotopes, we show that we are able to formally verify LTL specifications in a computationally efficient manner while still being able to represent complex geometries that are often present when deploying autonomous systems, such as non-convex, disjoint sets.","Moreover, we evaluate our approach on a parking example, providing preliminary indications of how hybrid zonotopes facilitate computationally efficient formal verification of LTL specifications in environments that naturally lead to non-convex, disjoint geometries."],"url":"http://arxiv.org/abs/2404.03308v1","category":"eess.SY"}
{"created":"2024-04-04 09:01:17","title":"Light's Impact on Fertility: Unveiling the Maybe Connection Between Nighttime Illumination and Global Societal Changes","abstract":"Chinese people are talking about a national birth rate of 6.4 per 1,000 people in 2023, the lowest in the world. Whether this is related to the wrong use of various artificial night light rays passing through our eyes is worth reflection. Non-visual effects of light are the effects of the blue component of light, which effectively inhibits the melatonin secretion in the pineal gland. As melatonin acts through high-affinity receptors located centrally and in all organs, blue light affects, in principle, the hormone secretion throughout the body, including the secretion of sex hormones and cortisol. These effects have been widely used in animal reproduction. Given that the night light environment has significantly changed over the past century, an effect of light on fertility and behavior could be detected in multiple cases. We showed that the ovulation phase of hens could be shifted by the light as low as 0.2 Lx, which corresponds to the illumination intensity at full moon,and the effect of 0.2 Lx on ovulation is normally far greater than all other factors combined. Two rounds of experiments and many other facts listed support that human intrinsic fertility may have not declined significantly, and food, chemical pollution and policy making are all not the main factors affecting fertility. The longer lighting time at night has brought modern humans into a radically different endocrine state compared with that of humans who lived 100 years ago, which has a huge impact on human reproduction, values and even the progress of civilization. With the development of the digital era, our eyes will be exposed to even longer light time at night. How to restore human beings to a normal hormonal state by combating the misuse of light may be the urgent issue. The light of the digital world (met averse, AI, and so on)may be should not continue to flood.","sentences":["Chinese people are talking about a national birth rate of 6.4 per 1,000 people in 2023, the lowest in the world.","Whether this is related to the wrong use of various artificial night light rays passing through our eyes is worth reflection.","Non-visual effects of light are the effects of the blue component of light, which effectively inhibits the melatonin secretion in the pineal gland.","As melatonin acts through high-affinity receptors located centrally and in all organs, blue light affects, in principle, the hormone secretion throughout the body, including the secretion of sex hormones and cortisol.","These effects have been widely used in animal reproduction.","Given that the night light environment has significantly changed over the past century, an effect of light on fertility and behavior could be detected in multiple cases.","We showed that the ovulation phase of hens could be shifted by the light as low as 0.2 Lx, which corresponds to the illumination intensity at full moon,and the effect of 0.2 Lx on ovulation is normally far greater than all other factors combined.","Two rounds of experiments and many other facts listed support that human intrinsic fertility may have not declined significantly, and food, chemical pollution and policy making are all not the main factors affecting fertility.","The longer lighting time at night has brought modern humans into a radically different endocrine state compared with that of humans who lived 100 years ago, which has a huge impact on human reproduction, values and even the progress of civilization.","With the development of the digital era, our eyes will be exposed to even longer light time at night.","How to restore human beings to a normal hormonal state by combating the misuse of light may be the urgent issue.","The light of the digital world (met averse, AI, and so on)may be should not continue to flood."],"url":"http://arxiv.org/abs/2404.03306v1","category":"physics.soc-ph"}
{"created":"2024-04-04 09:01:17","title":"Bi-level Trajectory Optimization on Uneven Terrains with Differentiable Wheel-Terrain Interaction Model","abstract":"Navigation of wheeled vehicles on uneven terrain necessitates going beyond the 2D approaches for trajectory planning. Specifically, it is essential to incorporate the full 6dof variation of vehicle pose and its associated stability cost in the planning process. To this end, most recent works aim to learn a neural network model to predict the vehicle evolution. However, such approaches are data-intensive and fraught with generalization issues. In this paper, we present a purely model-based approach that just requires the digital elevation information of the terrain. Specifically, we express the wheel-terrain interaction and 6dof pose prediction as a non-linear least squares (NLS) problem. As a result, trajectory planning can be viewed as a bi-level optimization. The inner optimization layer predicts the pose on the terrain along a given trajectory, while the outer layer deforms the trajectory itself to reduce the stability and kinematic costs of the pose. We improve the state-of-the-art in the following respects. First, we show that our NLS based pose prediction closely matches the output from a high-fidelity physics engine. This result coupled with the fact that we can query gradients of the NLS solver, makes our pose predictor, a differentiable wheel-terrain interaction model. We further leverage this differentiability to efficiently solve the proposed bi-level trajectory optimization problem. Finally, we perform extensive experiments, and comparison with a baseline to showcase the effectiveness of our approach in obtaining smooth, stable trajectories.","sentences":["Navigation of wheeled vehicles on uneven terrain necessitates going beyond the 2D approaches for trajectory planning.","Specifically, it is essential to incorporate the full 6dof variation of vehicle pose and its associated stability cost in the planning process.","To this end, most recent works aim to learn a neural network model to predict the vehicle evolution.","However, such approaches are data-intensive and fraught with generalization issues.","In this paper, we present a purely model-based approach that just requires the digital elevation information of the terrain.","Specifically, we express the wheel-terrain interaction and 6dof pose prediction as a non-linear least squares (NLS) problem.","As a result, trajectory planning can be viewed as a bi-level optimization.","The inner optimization layer predicts the pose on the terrain along a given trajectory, while the outer layer deforms the trajectory itself to reduce the stability and kinematic costs of the pose.","We improve the state-of-the-art in the following respects.","First, we show that our NLS based pose prediction closely matches the output from a high-fidelity physics engine.","This result coupled with the fact that we can query gradients of the NLS solver, makes our pose predictor, a differentiable wheel-terrain interaction model.","We further leverage this differentiability to efficiently solve the proposed bi-level trajectory optimization problem.","Finally, we perform extensive experiments, and comparison with a baseline to showcase the effectiveness of our approach in obtaining smooth, stable trajectories."],"url":"http://arxiv.org/abs/2404.03307v1","category":"cs.RO"}
{"created":"2024-04-04 08:56:48","title":"Concept -- An Evaluation Protocol on Conversation Recommender Systems with System- and User-centric Factors","abstract":"The conversational recommendation system (CRS) has been criticized regarding its user experience in real-world scenarios, despite recent significant progress achieved in academia. Existing evaluation protocols for CRS may prioritize system-centric factors such as effectiveness and fluency in conversation while neglecting user-centric aspects. Thus, we propose a new and inclusive evaluation protocol, Concept, which integrates both system- and user-centric factors. We conceptualise three key characteristics in representing such factors and further divide them into six primary abilities. To implement Concept, we adopt a LLM-based user simulator and evaluator with scoring rubrics that are tailored for each primary ability. Our protocol, Concept, serves a dual purpose. First, it provides an overview of the pros and cons in current CRS models. Second, it pinpoints the problem of low usability in the \"omnipotent\" ChatGPT and offers a comprehensive reference guide for evaluating CRS, thereby setting the foundation for CRS improvement.","sentences":["The conversational recommendation system (CRS) has been criticized regarding its user experience in real-world scenarios, despite recent significant progress achieved in academia.","Existing evaluation protocols for CRS may prioritize system-centric factors such as effectiveness and fluency in conversation while neglecting user-centric aspects.","Thus, we propose a new and inclusive evaluation protocol, Concept, which integrates both system- and user-centric factors.","We conceptualise three key characteristics in representing such factors and further divide them into six primary abilities.","To implement Concept, we adopt a LLM-based user simulator and evaluator with scoring rubrics that are tailored for each primary ability.","Our protocol, Concept, serves a dual purpose.","First, it provides an overview of the pros and cons in current CRS models.","Second, it pinpoints the problem of low usability in the \"omnipotent\" ChatGPT and offers a comprehensive reference guide for evaluating CRS, thereby setting the foundation for CRS improvement."],"url":"http://arxiv.org/abs/2404.03304v1","category":"cs.CL"}
{"created":"2024-04-04 08:54:15","title":"Benchmarking Parameter Control Methods in Differential Evolution for Mixed-Integer Black-Box Optimization","abstract":"Differential evolution (DE) generally requires parameter control methods (PCMs) for the scale factor and crossover rate. Although a better understanding of PCMs provides a useful clue to designing an efficient DE, their effectiveness is poorly understood in mixed-integer black-box optimization. In this context, this paper benchmarks PCMs in DE on the mixed-integer black-box optimization benchmarking function (bbob-mixint) suite in a component-wise manner. First, we demonstrate that the best PCM significantly depends on the combination of the mutation strategy and repair method. Although the PCM of SHADE is state-of-the-art for numerical black-box optimization, our results show its poor performance for mixed-integer black-box optimization. In contrast, our results show that some simple PCMs (e.g., the PCM of CoDE) perform the best in most cases. Then, we demonstrate that a DE with a suitable PCM performs significantly better than CMA-ES with integer handling for larger budgets of function evaluations. Finally, we show how the adaptation in the PCM of SHADE fails.","sentences":["Differential evolution (DE) generally requires parameter control methods (PCMs) for the scale factor and crossover rate.","Although a better understanding of PCMs provides a useful clue to designing an efficient DE, their effectiveness is poorly understood in mixed-integer black-box optimization.","In this context, this paper benchmarks PCMs in DE on the mixed-integer black-box optimization benchmarking function (bbob-mixint) suite in a component-wise manner.","First, we demonstrate that the best PCM significantly depends on the combination of the mutation strategy and repair method.","Although the PCM of SHADE is state-of-the-art for numerical black-box optimization, our results show its poor performance for mixed-integer black-box optimization.","In contrast, our results show that some simple PCMs (e.g., the PCM of CoDE) perform the best in most cases.","Then, we demonstrate that a DE with a suitable PCM performs significantly better than CMA-ES with integer handling for larger budgets of function evaluations.","Finally, we show how the adaptation in the PCM of SHADE fails."],"url":"http://arxiv.org/abs/2404.03303v1","category":"cs.NE"}
{"created":"2024-04-04 08:48:30","title":"SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular Diffusion Models","abstract":"Synthetic tabular data is crucial for sharing and augmenting data across silos, especially for enterprises with proprietary data. However, existing synthesizers are designed for centrally stored data. Hence, they struggle with real-world scenarios where features are distributed across multiple silos, necessitating on-premise data storage. We introduce SiloFuse, a novel generative framework for high-quality synthesis from cross-silo tabular data. To ensure privacy, SiloFuse utilizes a distributed latent tabular diffusion architecture. Through autoencoders, latent representations are learned for each client's features, masking their actual values. We employ stacked distributed training to improve communication efficiency, reducing the number of rounds to a single step. Under SiloFuse, we prove the impossibility of data reconstruction for vertically partitioned synthesis and quantify privacy risks through three attacks using our benchmark framework. Experimental results on nine datasets showcase SiloFuse's competence against centralized diffusion-based synthesizers. Notably, SiloFuse achieves 43.8 and 29.8 higher percentage points over GANs in resemblance and utility. Experiments on communication show stacked training's fixed cost compared to the growing costs of end-to-end training as the number of training iterations increases. Additionally, SiloFuse proves robust to feature permutations and varying numbers of clients.","sentences":["Synthetic tabular data is crucial for sharing and augmenting data across silos, especially for enterprises with proprietary data.","However, existing synthesizers are designed for centrally stored data.","Hence, they struggle with real-world scenarios where features are distributed across multiple silos, necessitating on-premise data storage.","We introduce SiloFuse, a novel generative framework for high-quality synthesis from cross-silo tabular data.","To ensure privacy, SiloFuse utilizes a distributed latent tabular diffusion architecture.","Through autoencoders, latent representations are learned for each client's features, masking their actual values.","We employ stacked distributed training to improve communication efficiency, reducing the number of rounds to a single step.","Under SiloFuse, we prove the impossibility of data reconstruction for vertically partitioned synthesis and quantify privacy risks through three attacks using our benchmark framework.","Experimental results on nine datasets showcase SiloFuse's competence against centralized diffusion-based synthesizers.","Notably, SiloFuse achieves 43.8 and 29.8 higher percentage points over GANs in resemblance and utility.","Experiments on communication show stacked training's fixed cost compared to the growing costs of end-to-end training as the number of training iterations increases.","Additionally, SiloFuse proves robust to feature permutations and varying numbers of clients."],"url":"http://arxiv.org/abs/2404.03299v1","category":"cs.LG"}
{"created":"2024-04-04 08:34:44","title":"On the solutions of linear systems over additively idempotent semirings","abstract":"The aim of this article is to solve the system $XA=Y$ where $A=(a_{ij})\\in M_{m\\times n}(S)$, $Y\\in S^{m}$ and $X$ is an unknown vector of size $n$, being $S$ an additively idempotent semiring. If the system has solutions then we completely characterize its maximal one, and in the particular case where $S$ is a generalized tropical semiring a complete characterization of its solutions is provided as well as an explicit bound of the computational cost associated to its computation. Finally, when $S$ is finite, we give a cryptographic application by presenting an attack to the key exchange protocol proposed by Maze, Monico and Rosenthal.","sentences":["The aim of this article is to solve the system $XA=Y$ where $A=(a_{ij})\\in M_{m\\times n}(S)$, $Y\\in S^{m}$ and $X$ is an unknown vector of size $n$, being $S$ an additively idempotent semiring.","If the system has solutions then we completely characterize its maximal one, and in the particular case where $S$ is a generalized tropical semiring a complete characterization of its solutions is provided as well as an explicit bound of the computational cost associated to its computation.","Finally, when $S$ is finite, we give a cryptographic application by presenting an attack to the key exchange protocol proposed by Maze, Monico and Rosenthal."],"url":"http://arxiv.org/abs/2404.03294v1","category":"cs.IT"}
{"created":"2024-04-04 08:24:57","title":"Learning-to-Optimize with PAC-Bayesian Guarantees: Theoretical Considerations and Practical Implementation","abstract":"We use the PAC-Bayesian theory for the setting of learning-to-optimize. To the best of our knowledge, we present the first framework to learn optimization algorithms with provable generalization guarantees (PAC-Bayesian bounds) and explicit trade-off between convergence guarantees and convergence speed, which contrasts with the typical worst-case analysis. Our learned optimization algorithms provably outperform related ones derived from a (deterministic) worst-case analysis. The results rely on PAC-Bayesian bounds for general, possibly unbounded loss-functions based on exponential families. Then, we reformulate the learning procedure into a one-dimensional minimization problem and study the possibility to find a global minimum. Furthermore, we provide a concrete algorithmic realization of the framework and new methodologies for learning-to-optimize, and we conduct four practically relevant experiments to support our theory. With this, we showcase that the provided learning framework yields optimization algorithms that provably outperform the state-of-the-art by orders of magnitude.","sentences":["We use the PAC-Bayesian theory for the setting of learning-to-optimize.","To the best of our knowledge, we present the first framework to learn optimization algorithms with provable generalization guarantees (PAC-Bayesian bounds) and explicit trade-off between convergence guarantees and convergence speed, which contrasts with the typical worst-case analysis.","Our learned optimization algorithms provably outperform related ones derived from a (deterministic) worst-case analysis.","The results rely on PAC-Bayesian bounds for general, possibly unbounded loss-functions based on exponential families.","Then, we reformulate the learning procedure into a one-dimensional minimization problem and study the possibility to find a global minimum.","Furthermore, we provide a concrete algorithmic realization of the framework and new methodologies for learning-to-optimize, and we conduct four practically relevant experiments to support our theory.","With this, we showcase that the provided learning framework yields optimization algorithms that provably outperform the state-of-the-art by orders of magnitude."],"url":"http://arxiv.org/abs/2404.03290v1","category":"cs.LG"}
{"created":"2024-04-04 08:23:21","title":"Small degree Salem numbers with trace -3","abstract":"The contribution of this work is to provide tables of Salem numbers with trace -3 and small degrees, namely degrees 2d = 34, 36, 38, and 40. The implemented method also generates a list of totally positive polynomials of degrees d = 17, 18, 19, and 20, with trace 2d - 3, a single root strictly greater than 4, and all other roots strictly between 0 and 4.","sentences":["The contribution of this work is to provide tables of Salem numbers with trace -3 and small degrees, namely degrees 2d = 34, 36, 38, and 40.","The implemented method also generates a list of totally positive polynomials of degrees d","= 17, 18, 19, and 20, with trace 2d - 3, a single root strictly greater than 4, and all other roots strictly between 0 and 4."],"url":"http://arxiv.org/abs/2404.03288v1","category":"math.NT"}
{"created":"2024-04-04 08:15:36","title":"Maximizing network capacity, control and management in designing a Telemedicine network: a review and recent challenges","abstract":"Telemedicine networks have seen significant changes in their capacity, monitoring, management, and control framework during the previous decades. The evolution of network capacity, control, and management for Unmanned Aerial Vehicle (UAV) & Software-Defined Networks (SDN) as support to telemedicine, artificial intelligence in telemedicine networks, and capabilities in designing a telemedicine network with respect to its performance and customization is presented in this study, with a historical history and a future view. The first section of the article goes over the history of traffic and capacity expansion, as well as future projections. By introducing a medical and image data communication protocol for telemedicine, the second section examines the technological constraints of expanding capacity in the era of UAV & software defined networking. The third section discusses ways to maximize network capacity by considering quality of service (QoS) capacity issues. Finally, the article explores how to construct a telemedicine network that can provide performance, customization, and capabilities to keep up with increased traffic in the coming decades. Research gaps and future directions were presented in the last section","sentences":["Telemedicine networks have seen significant changes in their capacity, monitoring, management, and control framework during the previous decades.","The evolution of network capacity, control, and management for Unmanned Aerial Vehicle (UAV) & Software-Defined Networks (SDN) as support to telemedicine, artificial intelligence in telemedicine networks, and capabilities in designing a telemedicine network with respect to its performance and customization is presented in this study, with a historical history and a future view.","The first section of the article goes over the history of traffic and capacity expansion, as well as future projections.","By introducing a medical and image data communication protocol for telemedicine, the second section examines the technological constraints of expanding capacity in the era of UAV & software defined networking.","The third section discusses ways to maximize network capacity by considering quality of service (QoS) capacity issues.","Finally, the article explores how to construct a telemedicine network that can provide performance, customization, and capabilities to keep up with increased traffic in the coming decades.","Research gaps and future directions were presented in the last section"],"url":"http://arxiv.org/abs/2404.03284v1","category":"cs.NI"}
{"created":"2024-04-04 08:12:38","title":"Involutions in Coxeter groups","abstract":"We combinatorially characterize the number $\\mathrm{cc}_2$ of conjugacy classes of involutions in any Coxeter group in terms of higher rank odd graphs. This notion naturally generalizes the concept of odd graphs, used previously to count the number of conjugacy classes of reflections. We provide uniform bounds and discuss some extremal cases, where the number $\\mathrm{cc}_2$ is smallest or largest possible. Moreover, we provide formulae for $\\mathrm{cc}_2$ in free and direct products as well as for some finite and affine types, besides computing $\\mathrm{cc}_2$ for all triangle groups, and all affine irreducible Coxeter groups of rank up to eleven.","sentences":["We combinatorially characterize the number $\\mathrm{cc}_2$ of conjugacy classes of involutions in any Coxeter group in terms of higher rank odd graphs.","This notion naturally generalizes the concept of odd graphs, used previously to count the number of conjugacy classes of reflections.","We provide uniform bounds and discuss some extremal cases, where the number $\\mathrm{cc}_2$ is smallest or largest possible.","Moreover, we provide formulae for $\\mathrm{cc}_2$ in free and direct products as well as for some finite and affine types, besides computing $\\mathrm{cc}_2$ for all triangle groups, and all affine irreducible Coxeter groups of rank up to eleven."],"url":"http://arxiv.org/abs/2404.03283v1","category":"math.GR"}
{"created":"2024-04-04 08:07:22","title":"Faster and shorter synthesis of Hamiltonian simulation circuits","abstract":"We devise greedy heuristics tailored for synthesizing quantum circuits that implement a specified set of Pauli rotations. Our heuristics are designed to minimize either the count of entangling gates or the depth of entangling gates, and they can be adjusted to either maintain or loosen the ordering of rotations. We present benchmark results demonstrating a depth reduction of up to a factor of 4 compared to the current state-of-the-art heuristics for synthesizing Hamiltonian simulation circuits. We also show that these heuristics can be used to optimize generic quantum circuits by decomposing and resynthesizing them.","sentences":["We devise greedy heuristics tailored for synthesizing quantum circuits that implement a specified set of Pauli rotations.","Our heuristics are designed to minimize either the count of entangling gates or the depth of entangling gates, and they can be adjusted to either maintain or loosen the ordering of rotations.","We present benchmark results demonstrating a depth reduction of up to a factor of 4 compared to the current state-of-the-art heuristics for synthesizing Hamiltonian simulation circuits.","We also show that these heuristics can be used to optimize generic quantum circuits by decomposing and resynthesizing them."],"url":"http://arxiv.org/abs/2404.03280v1","category":"quant-ph"}
{"created":"2024-04-04 08:04:00","title":"Design and Development of a Framework For Stroke-Based Handwritten Gujarati Font Generation","abstract":"Handwritten font generation is important for preserving cultural heritage and creating personalized designs. It adds an authentic and expressive touch to printed materials, making them visually appealing and establishing a stronger connection with the audience. This paper aims to design a framework for generating handwritten fonts in the Gujarati script, mimicking the variation of human handwriting. The proposed font generation model consists of a learning phase and a generation phase. In the learning phase, Gujarati scripts are analyzed, and rules for designing each character are formulated. This ruleset involves the concatenation of strokes in a stroke-based manner, ensuring visual consistency in the resulting glyphs. The generation phase involves the user providing a small subset of characters, and the system automatically generates the remaining character glyphs based on extracted strokes and learned rules, resulting in handwritten Gujarati fonts. The resulting character glyphs are converted into an open-type font using the FontForge tool, making them compatible with any Gujarati editor. Both subjective and objective evaluations are conducted to assess the synthesized images and fonts. Subjective evaluation through user studies provides feedback on quality and visual appeal, achieving an overall accuracy of 84.84%. Notably, eleven characters demonstrated a success ratio above 90%. Objective evaluation using an existing recognition system achieves an overall accuracy of 84.28% in OCR evaluation. Notably, fifteen characters had a success ratio of 80% or higher.","sentences":["Handwritten font generation is important for preserving cultural heritage and creating personalized designs.","It adds an authentic and expressive touch to printed materials, making them visually appealing and establishing a stronger connection with the audience.","This paper aims to design a framework for generating handwritten fonts in the Gujarati script, mimicking the variation of human handwriting.","The proposed font generation model consists of a learning phase and a generation phase.","In the learning phase, Gujarati scripts are analyzed, and rules for designing each character are formulated.","This ruleset involves the concatenation of strokes in a stroke-based manner, ensuring visual consistency in the resulting glyphs.","The generation phase involves the user providing a small subset of characters, and the system automatically generates the remaining character glyphs based on extracted strokes and learned rules, resulting in handwritten Gujarati fonts.","The resulting character glyphs are converted into an open-type font using the FontForge tool, making them compatible with any Gujarati editor.","Both subjective and objective evaluations are conducted to assess the synthesized images and fonts.","Subjective evaluation through user studies provides feedback on quality and visual appeal, achieving an overall accuracy of 84.84%.","Notably, eleven characters demonstrated a success ratio above 90%.","Objective evaluation using an existing recognition system achieves an overall accuracy of 84.28% in OCR evaluation.","Notably, fifteen characters had a success ratio of 80% or higher."],"url":"http://arxiv.org/abs/2404.03277v1","category":"cs.CV"}
{"created":"2024-04-04 08:00:12","title":"A Deep Reinforcement Learning Approach for Security-Aware Service Acquisition in IoT","abstract":"The novel Internet of Things (IoT) paradigm is composed of a growing number of heterogeneous smart objects and services that are transforming architectures and applications, increasing systems' complexity, and the need for reliability and autonomy. In this context, both smart objects and services are often provided by third parties which do not give full transparency regarding the security and privacy of the features offered. Although machine-based Service Level Agreements (SLA) have been recently leveraged to establish and share policies in Cloud-based scenarios, and also in the IoT context, the issue of making end users aware of the overall system security levels and the fulfillment of their privacy requirements through the provision of the requested service remains a challenging task. To tackle this problem, we propose a complete framework that defines suitable levels of privacy and security requirements in the acquisition of services in IoT, according to the user needs. Through the use of a Reinforcement Learning based solution, a user agent, inside the environment, is trained to choose the best smart objects granting access to the target services. Moreover, the solution is designed to guarantee deadline requirements and user security and privacy needs. Finally, to evaluate the correctness and the performance of the proposed approach we illustrate an extensive experimental analysis.","sentences":["The novel Internet of Things (IoT) paradigm is composed of a growing number of heterogeneous smart objects and services that are transforming architectures and applications, increasing systems' complexity, and the need for reliability and autonomy.","In this context, both smart objects and services are often provided by third parties which do not give full transparency regarding the security and privacy of the features offered.","Although machine-based Service Level Agreements (SLA) have been recently leveraged to establish and share policies in Cloud-based scenarios, and also in the IoT context, the issue of making end users aware of the overall system security levels and the fulfillment of their privacy requirements through the provision of the requested service remains a challenging task.","To tackle this problem, we propose a complete framework that defines suitable levels of privacy and security requirements in the acquisition of services in IoT, according to the user needs.","Through the use of a Reinforcement Learning based solution, a user agent, inside the environment, is trained to choose the best smart objects granting access to the target services.","Moreover, the solution is designed to guarantee deadline requirements and user security and privacy needs.","Finally, to evaluate the correctness and the performance of the proposed approach we illustrate an extensive experimental analysis."],"url":"http://arxiv.org/abs/2404.03276v1","category":"cs.CR"}
{"created":"2024-04-04 07:59:24","title":"DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models","abstract":"Recent advancements in Large Language Models (LLMs) have sparked a revolution across various research fields. In particular, the integration of common-sense knowledge from LLMs into robot task and motion planning has been proven to be a game-changer, elevating performance in terms of explainability and downstream task efficiency to unprecedented heights. However, managing the vast knowledge encapsulated within these large models has posed challenges, often resulting in infeasible plans generated by LLM-based planning systems due to hallucinations or missing domain information. To overcome these challenges and obtain even greater planning feasibility and computational efficiency, we propose a novel LLM-driven task planning approach called DELTA. For achieving better grounding from environmental topology into actionable knowledge, DELTA leverages the power of scene graphs as environment representations within LLMs, enabling the fast generation of precise planning problem descriptions. For obtaining higher planning performance, we use LLMs to decompose the long-term task goals into an autoregressive sequence of sub-goals for an automated task planner to solve. Our contribution enables a more efficient and fully automatic task planning pipeline, achieving higher planning success rates and significantly shorter planning times compared to the state of the art.","sentences":["Recent advancements in Large Language Models (LLMs) have sparked a revolution across various research fields.","In particular, the integration of common-sense knowledge from LLMs into robot task and motion planning has been proven to be a game-changer, elevating performance in terms of explainability and downstream task efficiency to unprecedented heights.","However, managing the vast knowledge encapsulated within these large models has posed challenges, often resulting in infeasible plans generated by LLM-based planning systems due to hallucinations or missing domain information.","To overcome these challenges and obtain even greater planning feasibility and computational efficiency, we propose a novel LLM-driven task planning approach called DELTA.","For achieving better grounding from environmental topology into actionable knowledge, DELTA leverages the power of scene graphs as environment representations within LLMs, enabling the fast generation of precise planning problem descriptions.","For obtaining higher planning performance, we use LLMs to decompose the long-term task goals into an autoregressive sequence of sub-goals for an automated task planner to solve.","Our contribution enables a more efficient and fully automatic task planning pipeline, achieving higher planning success rates and significantly shorter planning times compared to the state of the art."],"url":"http://arxiv.org/abs/2404.03275v1","category":"cs.RO"}
{"created":"2024-04-04 07:55:46","title":"Gaussian-Smoothed Sliced Probability Divergences","abstract":"Gaussian smoothed sliced Wasserstein distance has been recently introduced for comparing probability distributions, while preserving privacy on the data. It has been shown that it provides performances similar to its non-smoothed (non-private) counterpart. However, the computationaland statistical properties of such a metric have not yet been well-established. This work investigates the theoretical properties of this distance as well as those of generalized versions denoted as Gaussian-smoothed sliced divergences. We first show that smoothing and slicing preserve the metric property and the weak topology. To study the sample complexity of such divergences, we then introduce $\\hat{\\hat\\mu}_{n}$ the double empirical distribution for the smoothed-projected $\\mu$. The distribution $\\hat{\\hat\\mu}_{n}$ is a result of a double sampling process: one from sampling according to the origin distribution $\\mu$ and the second according to the convolution of the projection of $\\mu$ on the unit sphere and the Gaussian smoothing. We particularly focus on the Gaussian smoothed sliced Wasserstein distance and prove that it converges with a rate $O(n^{-1/2})$. We also derive other properties, including continuity, of different divergences with respect to the smoothing parameter. We support our theoretical findings with empirical studies in the context of privacy-preserving domain adaptation.","sentences":["Gaussian smoothed sliced Wasserstein distance has been recently introduced for comparing probability distributions, while preserving privacy on the data.","It has been shown that it provides performances similar to its non-smoothed (non-private) counterpart.","However, the computationaland statistical properties of such a metric have not yet been well-established.","This work investigates the theoretical properties of this distance as well as those of generalized versions denoted as Gaussian-smoothed sliced divergences.","We first show that smoothing and slicing preserve the metric property and the weak topology.","To study the sample complexity of such divergences, we then introduce $\\hat{\\hat\\mu}_{n}$ the double empirical distribution for the smoothed-projected $\\mu$. The distribution $\\hat{\\hat\\mu}_{n}$ is a result of a double sampling process: one from sampling according to the origin distribution $\\mu$ and the second according to the convolution of the projection of $\\mu$ on the unit sphere and the Gaussian smoothing.","We particularly focus on the Gaussian smoothed sliced Wasserstein distance and prove that it converges with a rate $O(n^{-1/2})$. We also derive other properties, including continuity, of different divergences with respect to the smoothing parameter.","We support our theoretical findings with empirical studies in the context of privacy-preserving domain adaptation."],"url":"http://arxiv.org/abs/2404.03273v1","category":"cs.LG"}
{"created":"2024-04-04 07:49:09","title":"Cryptographic Hardness of Score Estimation","abstract":"We show that $L^2$-accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard even when sample complexity is polynomial in the relevant problem parameters. Our reduction builds on the result of Chen et al. (ICLR 2023), who showed that the problem of generating samples from an unknown data distribution reduces to $L^2$-accurate score estimation. Our hard-to-estimate distributions are the \"Gaussian pancakes\" distributions, originally due to Diakonikolas et al. (FOCS 2017), which have been shown to be computationally indistinguishable from the standard Gaussian under widely believed hardness assumptions from lattice-based cryptography (Bruna et al., STOC 2021; Gupte et al., FOCS 2022).","sentences":["We show that $L^2$-accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard even when sample complexity is polynomial in the relevant problem parameters.","Our reduction builds on the result of Chen et al.","(ICLR 2023), who showed that the problem of generating samples from an unknown data distribution reduces to $L^2$-accurate score estimation.","Our hard-to-estimate distributions are the \"Gaussian pancakes\" distributions, originally due to Diakonikolas et al.","(FOCS 2017), which have been shown to be computationally indistinguishable from the standard Gaussian under widely believed hardness assumptions from lattice-based cryptography (Bruna et al., STOC 2021; Gupte et al., FOCS 2022)."],"url":"http://arxiv.org/abs/2404.03272v1","category":"cs.LG"}
{"created":"2024-04-04 07:45:15","title":"Two-Scale Geometric Modelling for Defective Media","abstract":"A new geometrically exact micro-structured model is constructed using a generalisation of the notion of Riemann-Cartan manifolds and fibre bundle theory of rank 3. This model is based around the concept of two different length scales: a macroscopic scale -- of dimensions 1, 2, or 3 -- and a microscopic one -- of dimension 3. As they interact with each other, they produce emergent behaviours such as dislocations (torsion) and disclinations (curvature). A first-order placement map F : TB --> TE between a micro-structured body B and the micro-structured ambient space E is constructed, allowing to pull the ambient Riemann-Cartan geometry back onto the body. I norder to allow for curvature to arise, F is, in general, not required to be a gradient. Central to this model is the new notion of pseudo-metric, providing, in addition to a macroscopic metric (the usual Cauchy-Green tensor) and a microscopic metric, a notion of coupling between the microscopic and macroscopic realms. A notion of frame indifference is formalised and invariants are computed. In the case of a micro-linear structure, it is shown that the data of these invariants is equivalent to the data of the pseudo-metric.","sentences":["A new geometrically exact micro-structured model is constructed using a generalisation of the notion of Riemann-Cartan manifolds and fibre bundle theory of rank 3.","This model is based around the concept of two different length scales: a macroscopic scale -- of dimensions 1, 2, or 3 -- and a microscopic one -- of dimension 3.","As they interact with each other, they produce emergent behaviours such as dislocations (torsion) and disclinations (curvature).","A first-order placement map F : TB --> TE between a micro-structured body B and the micro-structured ambient space E is constructed, allowing to pull the ambient Riemann-Cartan geometry back onto the body.","I norder to allow for curvature to arise, F is, in general, not required to be a gradient.","Central to this model is the new notion of pseudo-metric, providing, in addition to a macroscopic metric (the usual Cauchy-Green tensor) and a microscopic metric, a notion of coupling between the microscopic and macroscopic realms.","A notion of frame indifference is formalised and invariants are computed.","In the case of a micro-linear structure, it is shown that the data of these invariants is equivalent to the data of the pseudo-metric."],"url":"http://arxiv.org/abs/2404.03269v1","category":"math.DG"}
{"created":"2024-04-04 07:45:04","title":"Efficient Ground State Estimation Using Generalized Hund's Rule","abstract":"Quantum computers offer a promising approach to simulate the ground state of molecules, which is crucial for understanding molecular properties and chemical reactions. However, the limited number of available qubits on current devices poses a challenge for simulation. This paper investigates the feasibility of reducing the qubit usage of molecular simulation by examining specific fermionic states according to Hund's rule.   We introduced a new framework based on qubit efficiency encoding. Based on this framework, the Hamiltonian is restricted to the Hund subspace. Compared to only concerned particle conservation, the proposed method can reduce $N$ qubit usage for an $M$ orbitals and $N$ electrons molecule when $M\\gg N$. Additionally, when using the STO-3G basis sets, the simulations of the $15$ molecules with given molecular geometry by the proposed method are close to the full configuration interaction. The absolute difference is at most $0.121\\%$. Meanwhile, predictions from potential energy surfaces using the proposed method have an absolute difference at most $4.1\\%$.","sentences":["Quantum computers offer a promising approach to simulate the ground state of molecules, which is crucial for understanding molecular properties and chemical reactions.","However, the limited number of available qubits on current devices poses a challenge for simulation.","This paper investigates the feasibility of reducing the qubit usage of molecular simulation by examining specific fermionic states according to Hund's rule.   ","We introduced a new framework based on qubit efficiency encoding.","Based on this framework, the Hamiltonian is restricted to the Hund subspace.","Compared to only concerned particle conservation, the proposed method can reduce $N$ qubit usage for an $M$ orbitals and $N$ electrons molecule when $M\\gg N$. Additionally, when using the STO-3G basis sets, the simulations of the $15$ molecules with given molecular geometry by the proposed method are close to the full configuration interaction.","The absolute difference is at most $0.121\\%$. Meanwhile, predictions from potential energy surfaces using the proposed method have an absolute difference at most $4.1\\%$."],"url":"http://arxiv.org/abs/2404.03268v1","category":"quant-ph"}
{"created":"2024-04-04 07:41:11","title":"Matrix-Free Geometric Multigrid Preconditioning Of Combined Newton-GMRES For Solving Phase-Field Fracture With Local Mesh Refinement","abstract":"In this work, the matrix-free solution of quasi-static phase-field fracture problems is further investigated. More specifically, we consider a quasi-monolithic formulation in which the irreversibility constraint is imposed with a primal-dual active set method. The resulting nonlinear problem is solved with a line-search assisted Newton method. Therein, the arising linear equation systems are solved with a generalized minimal residual method (GMRES), which is preconditioned with a matrix-free geometric multigrid method including geometric local mesh refinement. Our solver is substantiated with a numerical test on locally refined meshes.","sentences":["In this work, the matrix-free solution of quasi-static phase-field fracture problems is further investigated.","More specifically, we consider a quasi-monolithic formulation in which the irreversibility constraint is imposed with a primal-dual active set method.","The resulting nonlinear problem is solved with a line-search assisted Newton method.","Therein, the arising linear equation systems are solved with a generalized minimal residual method (GMRES), which is preconditioned with a matrix-free geometric multigrid method including geometric local mesh refinement.","Our solver is substantiated with a numerical test on locally refined meshes."],"url":"http://arxiv.org/abs/2404.03265v1","category":"math.NA"}
{"created":"2024-04-04 07:39:55","title":"Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions","abstract":"Foundation model, which is pre-trained on broad data and is able to adapt to a wide range of tasks, is advancing healthcare. It promotes the development of healthcare artificial intelligence (AI) models, breaking the contradiction between limited AI models and diverse healthcare practices. Much more widespread healthcare scenarios will benefit from the development of a healthcare foundation model (HFM), improving their advanced intelligent healthcare services. Despite the impending widespread deployment of HFMs, there is currently a lack of clear understanding about how they work in the healthcare field, their current challenges, and where they are headed in the future. To answer these questions, a comprehensive and deep survey of the challenges, opportunities, and future directions of HFMs is presented in this survey. It first conducted a comprehensive overview of the HFM including the methods, data, and applications for a quick grasp of the current progress. Then, it made an in-depth exploration of the challenges present in data, algorithms, and computing infrastructures for constructing and widespread application of foundation models in healthcare. This survey also identifies emerging and promising directions in this field for future development. We believe that this survey will enhance the community's comprehension of the current progress of HFM and serve as a valuable source of guidance for future development in this field. The latest HFM papers and related resources are maintained on our website: https://github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare.","sentences":["Foundation model, which is pre-trained on broad data and is able to adapt to a wide range of tasks, is advancing healthcare.","It promotes the development of healthcare artificial intelligence (AI) models, breaking the contradiction between limited AI models and diverse healthcare practices.","Much more widespread healthcare scenarios will benefit from the development of a healthcare foundation model (HFM), improving their advanced intelligent healthcare services.","Despite the impending widespread deployment of HFMs, there is currently a lack of clear understanding about how they work in the healthcare field, their current challenges, and where they are headed in the future.","To answer these questions, a comprehensive and deep survey of the challenges, opportunities, and future directions of HFMs is presented in this survey.","It first conducted a comprehensive overview of the HFM including the methods, data, and applications for a quick grasp of the current progress.","Then, it made an in-depth exploration of the challenges present in data, algorithms, and computing infrastructures for constructing and widespread application of foundation models in healthcare.","This survey also identifies emerging and promising directions in this field for future development.","We believe that this survey will enhance the community's comprehension of the current progress of HFM and serve as a valuable source of guidance for future development in this field.","The latest HFM papers and related resources are maintained on our website: https://github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare."],"url":"http://arxiv.org/abs/2404.03264v1","category":"cs.CY"}
{"created":"2024-04-04 07:38:11","title":"On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models","abstract":"In this paper, we propose that small models may not need to absorb the cost of pre-training to reap its benefits. Instead, they can capitalize on the astonishing results achieved by modern, enormous models to a surprising degree. We observe that, when distilled on a task from a pre-trained teacher model, a small model can achieve or surpass the performance it would achieve if it was pre-trained then finetuned on that task. To allow this phenomenon to be easily leveraged, we establish a connection reducing knowledge distillation to modern contrastive learning, opening two doors: (1) vastly different model architecture pairings can work for the distillation, and (2) most contrastive learning algorithms rooted in the theory of Noise Contrastive Estimation can be easily applied and used. We demonstrate this paradigm using pre-trained teacher models from open-source model hubs, Transformer and convolution based model combinations, and a novel distillation algorithm that massages the Alignment/Uniformity perspective of contrastive learning by Wang & Isola (2020) into a distillation objective. We choose this flavor of contrastive learning due to its low computational cost, an overarching theme of this work. We also observe that this phenomenon tends not to occur if the task is data-limited. However, this can be alleviated by leveraging yet another scale-inspired development: large, pre-trained generative models for dataset augmentation. Again, we use an open-source model, and our rudimentary prompts are sufficient to boost the small model`s performance. Thus, we highlight a training method for small models that is up to 94% faster than the standard pre-training paradigm without sacrificing performance. For practitioners discouraged from fully utilizing modern foundation datasets for their small models due to the prohibitive scale, we believe our work keeps that door open.","sentences":["In this paper, we propose that small models may not need to absorb the cost of pre-training to reap its benefits.","Instead, they can capitalize on the astonishing results achieved by modern, enormous models to a surprising degree.","We observe that, when distilled on a task from a pre-trained teacher model, a small model can achieve or surpass the performance it would achieve if it was pre-trained then finetuned on that task.","To allow this phenomenon to be easily leveraged, we establish a connection reducing knowledge distillation to modern contrastive learning, opening two doors: (1) vastly different model architecture pairings can work for the distillation, and (2) most contrastive learning algorithms rooted in the theory of Noise Contrastive Estimation can be easily applied and used.","We demonstrate this paradigm using pre-trained teacher models from open-source model hubs, Transformer and convolution based model combinations, and a novel distillation algorithm that massages the Alignment/Uniformity perspective of contrastive learning by Wang & Isola (2020) into a distillation objective.","We choose this flavor of contrastive learning due to its low computational cost, an overarching theme of this work.","We also observe that this phenomenon tends not to occur if the task is data-limited.","However, this can be alleviated by leveraging yet another scale-inspired development: large, pre-trained generative models for dataset augmentation.","Again, we use an open-source model, and our rudimentary prompts are sufficient to boost the small model`s performance.","Thus, we highlight a training method for small models that is up to 94% faster than the standard pre-training paradigm without sacrificing performance.","For practitioners discouraged from fully utilizing modern foundation datasets for their small models due to the prohibitive scale, we believe our work keeps that door open."],"url":"http://arxiv.org/abs/2404.03263v1","category":"cs.LG"}
{"created":"2024-04-04 07:31:56","title":"Enhancing the Performance of Aspect-Based Sentiment Analysis Systems","abstract":"Aspect-based sentiment analysis aims to predict sentiment polarity with fine granularity. While Graph Convolutional Networks (GCNs) are widely utilized for sentimental feature extraction, their naive application for syntactic feature extraction can compromise information preservation. This study introduces an innovative edge-enhanced GCN, named SentiSys, to navigate the syntactic graph while preserving intact feature information, leading to enhanced performance. Specifically,we first integrate a bidirectional long short-term memory (Bi-LSTM) network and a self-attention-based transformer. This combination facilitates effective text encoding, preventing the loss of information and predicting long dependency text. A bidirectional GCN (Bi-GCN) with message passing is then employed to encode relationships between entities. Additionally, unnecessary information is filtered out using an aspect-specific masking technique. To validate the effectiveness of our proposed model, we conduct extensive evaluation experiments and ablation studies on four benchmark datasets. The results consistently demonstrate improved performance in aspect-based sentiment analysis when employing SentiSys. This approach successfully addresses the challenges associated with syntactic feature extraction, highlighting its potential for advancing sentiment analysis methodologies.","sentences":["Aspect-based sentiment analysis aims to predict sentiment polarity with fine granularity.","While Graph Convolutional Networks (GCNs) are widely utilized for sentimental feature extraction, their naive application for syntactic feature extraction can compromise information preservation.","This study introduces an innovative edge-enhanced GCN, named SentiSys, to navigate the syntactic graph while preserving intact feature information, leading to enhanced performance.","Specifically,we first integrate a bidirectional long short-term memory (Bi-LSTM) network and a self-attention-based transformer.","This combination facilitates effective text encoding, preventing the loss of information and predicting long dependency text.","A bidirectional GCN (Bi-GCN) with message passing is then employed to encode relationships between entities.","Additionally, unnecessary information is filtered out using an aspect-specific masking technique.","To validate the effectiveness of our proposed model, we conduct extensive evaluation experiments and ablation studies on four benchmark datasets.","The results consistently demonstrate improved performance in aspect-based sentiment analysis when employing SentiSys.","This approach successfully addresses the challenges associated with syntactic feature extraction, highlighting its potential for advancing sentiment analysis methodologies."],"url":"http://arxiv.org/abs/2404.03259v1","category":"cs.CL"}
{"created":"2024-04-04 07:26:26","title":"Multi Positive Contrastive Learning with Pose-Consistent Generated Images","abstract":"Model pre-training has become essential in various recognition tasks. Meanwhile, with the remarkable advancements in image generation models, pre-training methods utilizing generated images have also emerged given their ability to produce unlimited training data. However, while existing methods utilizing generated images excel in classification, they fall short in more practical tasks, such as human pose estimation. In this paper, we have experimentally demonstrated it and propose the generation of visually distinct images with identical human poses. We then propose a novel multi-positive contrastive learning, which optimally utilize the previously generated images to learn structural features of the human body. We term the entire learning pipeline as GenPoCCL. Despite using only less than 1% amount of data compared to current state-of-the-art method, GenPoCCL captures structural features of the human body more effectively, surpassing existing methods in a variety of human-centric perception tasks.","sentences":["Model pre-training has become essential in various recognition tasks.","Meanwhile, with the remarkable advancements in image generation models, pre-training methods utilizing generated images have also emerged given their ability to produce unlimited training data.","However, while existing methods utilizing generated images excel in classification, they fall short in more practical tasks, such as human pose estimation.","In this paper, we have experimentally demonstrated it and propose the generation of visually distinct images with identical human poses.","We then propose a novel multi-positive contrastive learning, which optimally utilize the previously generated images to learn structural features of the human body.","We term the entire learning pipeline as GenPoCCL.","Despite using only less than 1% amount of data compared to current state-of-the-art method, GenPoCCL captures structural features of the human body more effectively, surpassing existing methods in a variety of human-centric perception tasks."],"url":"http://arxiv.org/abs/2404.03256v1","category":"cs.CV"}
{"created":"2024-04-04 07:24:35","title":"Mining Area Skyline Objects from Map-based Big Data using Apache Spark Framework","abstract":"The computation of the skyline provides a mechanism for utilizing multiple location-based criteria to identify optimal data points. However, the efficiency of these computations diminishes and becomes more challenging as the input data expands. This study presents a novel algorithm aimed at mitigating this challenge by harnessing the capabilities of Apache Spark, a distributed processing platform, for conducting area skyline computations. The proposed algorithm enhances processing speed and scalability. In particular, our algorithm encompasses three key phases: the computation of distances between data points, the generation of distance tuples, and the execution of the skyline operators. Notably, the second phase employs a local partial skyline extraction technique to minimize the volume of data transmitted from each executor (a parallel processing procedure) to the driver (a central processing procedure). Afterwards, the driver processes the received data to determine the final skyline and creates filters to exclude irrelevant points. Extensive experimentation on eight datasets reveals that our algorithm significantly reduces both data size and computation time required for area skyline computation.","sentences":["The computation of the skyline provides a mechanism for utilizing multiple location-based criteria to identify optimal data points.","However, the efficiency of these computations diminishes and becomes more challenging as the input data expands.","This study presents a novel algorithm aimed at mitigating this challenge by harnessing the capabilities of Apache Spark, a distributed processing platform, for conducting area skyline computations.","The proposed algorithm enhances processing speed and scalability.","In particular, our algorithm encompasses three key phases: the computation of distances between data points, the generation of distance tuples, and the execution of the skyline operators.","Notably, the second phase employs a local partial skyline extraction technique to minimize the volume of data transmitted from each executor (a parallel processing procedure) to the driver (a central processing procedure).","Afterwards, the driver processes the received data to determine the final skyline and creates filters to exclude irrelevant points.","Extensive experimentation on eight datasets reveals that our algorithm significantly reduces both data size and computation time required for area skyline computation."],"url":"http://arxiv.org/abs/2404.03254v1","category":"cs.DC"}
{"created":"2024-04-04 07:19:31","title":"A dataset of primary nasopharyngeal carcinoma MRI with multi-modalities segmentation","abstract":"Multi-modality magnetic resonance imaging data with various sequences facilitate the early diagnosis, tumor segmentation, and disease staging in the management of nasopharyngeal carcinoma (NPC). The lack of publicly available, comprehensive datasets limits advancements in diagnosis, treatment planning, and the development of machine learning algorithms for NPC. Addressing this critical need, we introduce the first comprehensive NPC MRI dataset, encompassing MR axial imaging of 277 primary NPC patients. This dataset includes T1-weighted, T2-weighted, and contrast-enhanced T1-weighted sequences, totaling 831 scans. In addition to the corresponding clinical data, manually annotated and labeled segmentations by experienced radiologists offer high-quality data resources from untreated primary NPC.","sentences":["Multi-modality magnetic resonance imaging data with various sequences facilitate the early diagnosis, tumor segmentation, and disease staging in the management of nasopharyngeal carcinoma (NPC).","The lack of publicly available, comprehensive datasets limits advancements in diagnosis, treatment planning, and the development of machine learning algorithms for NPC.","Addressing this critical need, we introduce the first comprehensive NPC MRI dataset, encompassing MR axial imaging of 277 primary NPC patients.","This dataset includes T1-weighted, T2-weighted, and contrast-enhanced T1-weighted sequences, totaling 831 scans.","In addition to the corresponding clinical data, manually annotated and labeled segmentations by experienced radiologists offer high-quality data resources from untreated primary NPC."],"url":"http://arxiv.org/abs/2404.03253v1","category":"eess.IV"}
{"created":"2024-04-04 07:14:12","title":"Real-time Noise Source Estimation of a Camera System from an Image and Metadata","abstract":"Autonomous machines must self-maintain proper functionality to ensure the safety of humans and themselves. This pertains particularly to its cameras as predominant sensors to perceive the environment and support actions. A fundamental camera problem addressed in this study is noise. Solutions often focus on denoising images a posteriori, that is, fighting symptoms rather than root causes. However, tackling root causes requires identifying the noise sources, considering the limitations of mobile platforms. This work investigates a real-time, memory-efficient and reliable noise source estimator that combines data- and physically-based models. To this end, a DNN that examines an image with camera metadata for major camera noise sources is built and trained. In addition, it quantifies unexpected factors that impact image noise or metadata. This study investigates seven different estimators on six datasets that include synthetic noise, real-world noise from two camera systems, and real field campaigns. For these, only the model with most metadata is capable to accurately and robustly quantify all individual noise contributions. This method outperforms total image noise estimators and can be plug-and-play deployed. It also serves as a basis to include more advanced noise sources, or as part of an automatic countermeasure feedback-loop to approach fully reliable machines.","sentences":["Autonomous machines must self-maintain proper functionality to ensure the safety of humans and themselves.","This pertains particularly to its cameras as predominant sensors to perceive the environment and support actions.","A fundamental camera problem addressed in this study is noise.","Solutions often focus on denoising images a posteriori, that is, fighting symptoms rather than root causes.","However, tackling root causes requires identifying the noise sources, considering the limitations of mobile platforms.","This work investigates a real-time, memory-efficient and reliable noise source estimator that combines data- and physically-based models.","To this end, a DNN that examines an image with camera metadata for major camera noise sources is built and trained.","In addition, it quantifies unexpected factors that impact image noise or metadata.","This study investigates seven different estimators on six datasets that include synthetic noise, real-world noise from two camera systems, and real field campaigns.","For these, only the model with most metadata is capable to accurately and robustly quantify all individual noise contributions.","This method outperforms total image noise estimators and can be plug-and-play deployed.","It also serves as a basis to include more advanced noise sources, or as part of an automatic countermeasure feedback-loop to approach fully reliable machines."],"url":"http://arxiv.org/abs/2404.03251v1","category":"cs.CV"}
{"created":"2024-04-04 07:06:09","title":"Stronger Speed Limit for Observables: Tight bound for Capacity of Entanglement, Modular Hamiltonian and Charging of Quantum Battery","abstract":"How fast an observable can evolve in time is answered by so-called the observable speed limit. Here, we prove a stronger version of the observable speed limit and show that the previously obtained bound is a special case of the new bound. The stronger quantum speed limit for the state also follows from the stronger quantum speed limit for observables (SQSLO). We apply this to prove a stronger bound for the entanglement rate using the notion of capacity of entanglement (the quantum information theoretic counterpart of the heat capacity) and show that it outperforms previous bounds. Furthermore, we apply the SQSLO for the rate of modular Hamiltonian and in the context of interacting qubits in a quantum battery. These illustrative examples reveal that the speed limit for the modular energy and the time required to charge the battery can be exactly predicted using the new bound. This shows that for estimating the charging time of quantum battery SQSLO is actually tight, i.e., it saturates. Our findings can have important applications in quantum thermodynamics, the complexity of operator growth, predicting the time rate of quantum correlation growth and quantum technology, in general.","sentences":["How fast an observable can evolve in time is answered by so-called the observable speed limit.","Here, we prove a stronger version of the observable speed limit and show that the previously obtained bound is a special case of the new bound.","The stronger quantum speed limit for the state also follows from the stronger quantum speed limit for observables (SQSLO).","We apply this to prove a stronger bound for the entanglement rate using the notion of capacity of entanglement (the quantum information theoretic counterpart of the heat capacity) and show that it outperforms previous bounds.","Furthermore, we apply the SQSLO for the rate of modular Hamiltonian and in the context of interacting qubits in a quantum battery.","These illustrative examples reveal that the speed limit for the modular energy and the time required to charge the battery can be exactly predicted using the new bound.","This shows that for estimating the charging time of quantum battery SQSLO is actually tight, i.e., it saturates.","Our findings can have important applications in quantum thermodynamics, the complexity of operator growth, predicting the time rate of quantum correlation growth and quantum technology, in general."],"url":"http://arxiv.org/abs/2404.03247v1","category":"quant-ph"}
{"created":"2024-04-04 07:02:32","title":"Memory Sharing with CXL: Hardware and Software Design Approaches","abstract":"Compute Express Link (CXL) is a rapidly emerging coherent interconnect standard that provides opportunities for memory pooling and sharing. Memory sharing is a well-established software feature that improves memory utilization by avoiding unnecessary data movement. In this paper, we discuss multiple approaches to enable memory sharing with different generations of CXL protocol (i.e., CXL 2.0 and CXL 3.0) considering the challenges with each of the architectures from the device hardware and software viewpoint.","sentences":["Compute Express Link (CXL) is a rapidly emerging coherent interconnect standard that provides opportunities for memory pooling and sharing.","Memory sharing is a well-established software feature that improves memory utilization by avoiding unnecessary data movement.","In this paper, we discuss multiple approaches to enable memory sharing with different generations of CXL protocol (i.e., CXL 2.0 and CXL 3.0) considering the challenges with each of the architectures from the device hardware and software viewpoint."],"url":"http://arxiv.org/abs/2404.03245v1","category":"cs.ET"}
{"created":"2024-04-04 06:59:34","title":"About semilinear low dimension Bessel PDEs","abstract":"We prove existence and uniqueness of solutions of a semilinear PDE driven by a Bessel type generator$L^\\delta$ with low dimension $0 < \\delta < 1$. $L^\\delta$ is a local operator, whose drift is thederivative of $x \\mapsto \\log (\\vert x\\vert)$:in particular it is a Schwartz distribution, whichis not the derivative of a continuous function.The solutions are intended in a duality (''weak'') sensewith respect to state space$L^2(\\mathbb{R}_+, d\\mu),$ $\\mu$ being an invariant measure for the Bessel semigroup.","sentences":["We prove existence and uniqueness of solutions of a semilinear PDE driven by a Bessel type generator$L^\\delta$ with low dimension $0 < \\delta <","1$. $L^\\delta$ is a local operator, whose drift is thederivative of $x \\mapsto \\log (\\vert x\\vert)$:in particular it is a Schwartz distribution, whichis not the derivative of a continuous function.","The solutions are intended in a duality (''weak'') sensewith respect to state space$L^2(\\mathbb{R}_+, d\\mu),$","$\\mu$ being an invariant measure for the Bessel semigroup."],"url":"http://arxiv.org/abs/2404.03243v1","category":"math.PR"}
{"created":"2024-04-04 06:58:39","title":"Would Deep Generative Models Amplify Bias in Future Models?","abstract":"We investigate the impact of deep generative models on potential social biases in upcoming computer vision models. As the internet witnesses an increasing influx of AI-generated images, concerns arise regarding inherent biases that may accompany them, potentially leading to the dissemination of harmful content. This paper explores whether a detrimental feedback loop, resulting in bias amplification, would occur if generated images were used as the training data for future models. We conduct simulations by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion. The modified datasets are used to train OpenCLIP and image captioning models, which we evaluate in terms of quality and bias. Contrary to expectations, our findings indicate that introducing generated images during training does not uniformly amplify bias. Instead, instances of bias mitigation across specific tasks are observed. We further explore the factors that may influence these phenomena, such as artifacts in image generation (e.g., blurry faces) or pre-existing biases in the original datasets.","sentences":["We investigate the impact of deep generative models on potential social biases in upcoming computer vision models.","As the internet witnesses an increasing influx of AI-generated images, concerns arise regarding inherent biases that may accompany them, potentially leading to the dissemination of harmful content.","This paper explores whether a detrimental feedback loop, resulting in bias amplification, would occur if generated images were used as the training data for future models.","We conduct simulations by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion.","The modified datasets are used to train OpenCLIP and image captioning models, which we evaluate in terms of quality and bias.","Contrary to expectations, our findings indicate that introducing generated images during training does not uniformly amplify bias.","Instead, instances of bias mitigation across specific tasks are observed.","We further explore the factors that may influence these phenomena, such as artifacts in image generation (e.g., blurry faces) or pre-existing biases in the original datasets."],"url":"http://arxiv.org/abs/2404.03242v1","category":"cs.CV"}
{"created":"2024-04-04 06:56:38","title":"A logarithm law for nonautonomous systems fastly converging to equilibrium and mean field coupled systems","abstract":"We prove that if a nonautonomous system has in a certain sense a fast convergence to equilibrium (faster than any power law behavior) then the time $\\tau _{r}(x,y)$ needed for a typical point $x$ to enter for the first time in a ball $B(y,r)$ centered in $y$, with small radius \\ $r $ scales as the local dimension of the equilibrium measure \\ $\\mu $ at $y$, i.e. $$ \\underset{r\\rightarrow 0}{\\lim }\\frac{\\log \\tau _{r}(x,y)}{-\\log r}% =d_{\\mu }(y).$$   We then apply the general result to concrete systems of different kind, showing such a logarithm law for asymptotically authonomous solenoidal maps and mean field coupled expanding maps.","sentences":["We prove that if a nonautonomous system has in a certain sense a fast convergence to equilibrium (faster than any power law behavior) then the time $\\tau _{r}(x,y)$ needed for a typical point $x$ to enter for the first time in a ball $B(y,r)$ centered in $y$, with small radius \\ $r $ scales as the local dimension of the equilibrium measure \\ $\\mu $ at $y$, i.e. $$ \\underset{r\\rightarrow 0}{\\lim }\\frac{\\log \\tau _{r}(x,y)}{-\\log r}% =d_{\\mu }(","y).$$   We then apply the general result to concrete systems of different kind, showing such a logarithm law for asymptotically authonomous solenoidal maps and mean field coupled expanding maps."],"url":"http://arxiv.org/abs/2404.03241v1","category":"math.DS"}
{"created":"2024-04-04 06:54:44","title":"Exploring Emotions in Multi-componential Space using Interactive VR Games","abstract":"Emotion understanding is a complex process that involves multiple components. The ability to recognise emotions not only leads to new context awareness methods but also enhances system interaction's effectiveness by perceiving and expressing emotions. Despite the attention to discrete and dimensional models, neuroscientific evidence supports those emotions as being complex and multi-faceted. One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of emotions with five interconnected components: appraisal, expression, motivation, physiology and feeling. However, the relationship between CPM and discrete emotions has not yet been fully explored. Therefore, to better understand emotions underlying processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected multimodal measures (self-reports, physiological and facial signals) from 39 participants. We used Machine Learning (ML) methods to identify the unique contributions of each component to emotion differentiation. Our results showed the role of different components in emotion differentiation, with the model including all components demonstrating the most significant contribution. Moreover, we found that at least five dimensions are needed to represent the variation of emotions in our dataset. These findings also have implications for using VR environments in emotion research and highlight the role of physiological signals in emotion recognition within such environments.","sentences":["Emotion understanding is a complex process that involves multiple components.","The ability to recognise emotions not only leads to new context awareness methods but also enhances system interaction's effectiveness by perceiving and expressing emotions.","Despite the attention to discrete and dimensional models, neuroscientific evidence supports those emotions as being complex and multi-faceted.","One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of emotions with five interconnected components: appraisal, expression, motivation, physiology and feeling.","However, the relationship between CPM and discrete emotions has not yet been fully explored.","Therefore, to better understand emotions underlying processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected multimodal measures (self-reports, physiological and facial signals) from 39 participants.","We used Machine Learning (ML) methods to identify the unique contributions of each component to emotion differentiation.","Our results showed the role of different components in emotion differentiation, with the model including all components demonstrating the most significant contribution.","Moreover, we found that at least five dimensions are needed to represent the variation of emotions in our dataset.","These findings also have implications for using VR environments in emotion research and highlight the role of physiological signals in emotion recognition within such environments."],"url":"http://arxiv.org/abs/2404.03239v1","category":"cs.HC"}
{"created":"2024-04-04 06:52:01","title":"Dynamics and Emission Properties of Flux Ropes from Two-Temperature GRMHD Simulations with Multiple Magnetic Loops","abstract":"Flux ropes erupting from the vicinity of the black hole are thought to be a potential model for the flares observed in Sgr\\,A$^*$. In this study, we examine the radiative properties of flux ropes that emerged from the vicinity of the black hole. We have performed three-dimensional two-temperature General Relativistic Magnetohydrodynamic (GRMHD) simulations of magnetized accretion flows with alternating multiple magnetic loops, and General Relativistic Radiation Transfer (GRRT) calculations. In GRMHD simulations, two different sizes of initial magnetic loops are implemented. In the small loop case, magnetic dissipation leads to a weaker excitement of magneto-rotational instability inside the torus which generates a lower accretion rate compared to the large loop case. However, it makes more generation of flux ropes due to frequent reconnection by magnetic loops with different polarities. By calculating the thermal synchrotron emission, we found that the variability of light curves and emitting region are tightly related. At $230\\,\\rm GHz$ and higher frequency, the emission from the flux ropes is relatively stronger compared with the background, which is responsible for the filamentary structure in the images. At lower frequencies, e.g. $43\\,\\rm GHz$, emission comes from more extended regions, which have a less filamentary structure in the image. Our study shows self-consistent electron temperature models are essential for the calculation of thermal synchrotron radiation and the morphology of the GRRT images. Flux ropes contribute considerable emission at frequencies $\\gtrsim 230\\,\\rm GHz$.","sentences":["Flux ropes erupting from the vicinity of the black hole are thought to be a potential model for the flares observed in Sgr\\,A$^*$. In this study, we examine the radiative properties of flux ropes that emerged from the vicinity of the black hole.","We have performed three-dimensional two-temperature General Relativistic Magnetohydrodynamic (GRMHD) simulations of magnetized accretion flows with alternating multiple magnetic loops, and General Relativistic Radiation Transfer (GRRT) calculations.","In GRMHD simulations, two different sizes of initial magnetic loops are implemented.","In the small loop case, magnetic dissipation leads to a weaker excitement of magneto-rotational instability inside the torus which generates a lower accretion rate compared to the large loop case.","However, it makes more generation of flux ropes due to frequent reconnection by magnetic loops with different polarities.","By calculating the thermal synchrotron emission, we found that the variability of light curves and emitting region are tightly related.","At $230\\,\\rm GHz$ and higher frequency, the emission from the flux ropes is relatively stronger compared with the background, which is responsible for the filamentary structure in the images.","At lower frequencies, e.g. $43\\,\\rm GHz$, emission comes from more extended regions, which have a less filamentary structure in the image.","Our study shows self-consistent electron temperature models are essential for the calculation of thermal synchrotron radiation and the morphology of the GRRT images.","Flux ropes contribute considerable emission at frequencies $\\gtrsim 230\\,\\rm GHz$."],"url":"http://arxiv.org/abs/2404.03237v1","category":"astro-ph.HE"}
{"created":"2024-04-04 06:39:28","title":"Geometry of degenerate quantum states, configurations of $m$-planes and invariants on complex Grassmannians","abstract":"Understanding the geometric information contained in quantum states is valuable in various branches of physics, particularly in solid-state physics when Bloch states play a crucial role. While the Fubini-Study metric and Berry curvature form offer comprehensive descriptions of non-degenerate quantum states, a similar description for degenerate states did not exist. In this work, we fill this gap by showing how to reduce the geometry of degenerate states to the non-abelian (Wilczek-Zee) connection $A$ and a previously unexplored matrix-valued metric tensor $G$. Mathematically, this problem is equivalent to finding the $U(N)$ invariants of a configuration of subspaces in $\\mathbb{C}^n$. For two subspaces, the configuration was known to be described by a set of $m$ principal angles that generalize the notion of quantum distance. For more subspaces, we find $3 m^2 - 3 m + 1$ additional independent invariants associated with each triple of subspaces. Some of them generalize the Berry-Pancharatnam phase, and some do not have analogues for 1-dimensional subspaces. We also develop a procedure for calculating these invariants as integrals of $A$ and $G$ over geodesics on the Grassmannain manifold. Finally, we briefly discuss possible application of these results to quantum state preparation and $PT$-symmetric band structures.","sentences":["Understanding the geometric information contained in quantum states is valuable in various branches of physics, particularly in solid-state physics when Bloch states play a crucial role.","While the Fubini-Study metric and Berry curvature form offer comprehensive descriptions of non-degenerate quantum states, a similar description for degenerate states did not exist.","In this work, we fill this gap by showing how to reduce the geometry of degenerate states to the non-abelian (Wilczek-Zee) connection $A$ and a previously unexplored matrix-valued metric tensor $G$. Mathematically, this problem is equivalent to finding the $U(N)$ invariants of a configuration of subspaces in $\\mathbb{C}^n$. For two subspaces, the configuration was known to be described by a set of $m$ principal angles that generalize the notion of quantum distance.","For more subspaces, we find $3 m^2 - 3 m + 1$ additional independent invariants associated with each triple of subspaces.","Some of them generalize the Berry-Pancharatnam phase, and some do not have analogues for 1-dimensional subspaces.","We also develop a procedure for calculating these invariants as integrals of $A$ and $G$ over geodesics on the Grassmannain manifold.","Finally, we briefly discuss possible application of these results to quantum state preparation and $PT$-symmetric band structures."],"url":"http://arxiv.org/abs/2404.03234v1","category":"quant-ph"}
{"created":"2024-04-04 06:35:30","title":"On the primitive ideal space of radial representations of free groups","abstract":"Radial representations of finitely generated free groups are studied. The associated C*-algebra is located between the reduced and full group C*-algebras and its primitive ideal space is described concretely as a topological space.","sentences":["Radial representations of finitely generated free groups are studied.","The associated C*-algebra is located between the reduced and full group C*-algebras and its primitive ideal space is described concretely as a topological space."],"url":"http://arxiv.org/abs/2404.03231v1","category":"math.OA"}
{"created":"2024-04-04 06:29:42","title":"Relation between the keV-MeV and TeV emission of GRB 221009A and its implications","abstract":"Gamma-ray bursts (GRBs) are believed to launch relativistic jets, which generate prompt emission by their internal processes and drive external shocks into surrounding medium, accounting for the long-lasting afterglow emission. However, how the jet powers the external shock is an open question. The unprecedented observations of the keV-MeV emission with GECAM and the TeV emission with LHAASO of so far the brightest burst, GRB 221009A, offer a great opportunity to study the prompt-to-afterglow transition and the early dynamical evolution of the external shock. In this letter, we find that the cumulative light curve of keV-MeV emission could well fit the rising stage of the TeV light curve of GRB 221009A, with a time delay of $4.45^{+0.26}_{-0.26}$\\,s for TeV emission. Moreover, both the rapid increase in the initial stage and the excess from about \\T+260\\,s to 270\\,s in the TeV light curve could be interpreted by inverse Compton (IC) scatterings of the inner-coming photons by the energetic electrons in external shock. Our results not only reveal a close relation between the keV-MeV and TeV emission, but also indicate a continuous, rather than impulsive, energy injection to the external shock. Assuming an energy injection rate proportional to the keV-MeV flux, we build a continuous energy injection model which well fits the TeV light curve of GRB 221009A, and provides an estimate of the Lorentz factor of the jet.","sentences":["Gamma-ray bursts (GRBs) are believed to launch relativistic jets, which generate prompt emission by their internal processes and drive external shocks into surrounding medium, accounting for the long-lasting afterglow emission.","However, how the jet powers the external shock is an open question.","The unprecedented observations of the keV-MeV emission with GECAM and the TeV emission with LHAASO of so far the brightest burst, GRB 221009A, offer a great opportunity to study the prompt-to-afterglow transition and the early dynamical evolution of the external shock.","In this letter, we find that the cumulative light curve of keV-MeV emission could well fit the rising stage of the TeV light curve of GRB 221009A, with a time delay of $4.45^{+0.26}_{-0.26}$\\,s for TeV emission.","Moreover, both the rapid increase in the initial stage and the excess from about \\T+260\\,s to 270\\,s in the TeV light curve could be interpreted by inverse Compton (IC) scatterings of the inner-coming photons by the energetic electrons in external shock.","Our results not only reveal a close relation between the keV-MeV and TeV emission, but also indicate a continuous, rather than impulsive, energy injection to the external shock.","Assuming an energy injection rate proportional to the keV-MeV flux, we build a continuous energy injection model which well fits the TeV light curve of GRB 221009A, and provides an estimate of the Lorentz factor of the jet."],"url":"http://arxiv.org/abs/2404.03229v1","category":"astro-ph.HE"}
{"created":"2024-04-04 06:27:46","title":"Steering nonlocality in high-speed telecommunication system without detection loophole","abstract":"Nonlocal correlation represents the key feature of quantum mechanics, which is exploited as a resource in quantum information processing. However, the loophole issues hamper the practical applications. We report the first demonstration of steering nonlocality with detection loophole closed at telecommunication wavelengths. In this endeavour, we design and fabricate a low-loss silicon chip for efficient entanglement generation, and further apply the direct modulation technique to its optical pump to eliminate phase-encoding loss at the steering side. The newly proposed phase-encoding measurement setting adapts to an ultra-fast modulation rate (GHz). Consequently, we build a fiber-optic setup that can overcome the detection efficiency that is required by quantum steering with multiple measurement settings. Our setup provides an immediate platform for exploring applications based on steering nonlocality, especially for quantum communication.","sentences":["Nonlocal correlation represents the key feature of quantum mechanics, which is exploited as a resource in quantum information processing.","However, the loophole issues hamper the practical applications.","We report the first demonstration of steering nonlocality with detection loophole closed at telecommunication wavelengths.","In this endeavour, we design and fabricate a low-loss silicon chip for efficient entanglement generation, and further apply the direct modulation technique to its optical pump to eliminate phase-encoding loss at the steering side.","The newly proposed phase-encoding measurement setting adapts to an ultra-fast modulation rate (GHz).","Consequently, we build a fiber-optic setup that can overcome the detection efficiency that is required by quantum steering with multiple measurement settings.","Our setup provides an immediate platform for exploring applications based on steering nonlocality, especially for quantum communication."],"url":"http://arxiv.org/abs/2404.03228v1","category":"quant-ph"}
{"created":"2024-04-04 06:08:05","title":"Double bracket vector fields on Poisson manifolds","abstract":"We generalize the double bracket vector fields defined on compact semi-simple Lie algebras to the case of general Poisson manifolds endowed with a pseudo-Riemannian metric. We construct a generalization of the normal metric such that the above vector fields, when restricted to a symplectic leaf, become gradient vector fields. We illustrate the discussion at a variety of examples and carefully discuss complications that arise when the pseudo-Riemannian metric does not induce a non-degenerate metric on parts of the symplectic leaves.","sentences":["We generalize the double bracket vector fields defined on compact semi-simple Lie algebras to the case of general Poisson manifolds endowed with a pseudo-Riemannian metric.","We construct a generalization of the normal metric such that the above vector fields, when restricted to a symplectic leaf, become gradient vector fields.","We illustrate the discussion at a variety of examples and carefully discuss complications that arise when the pseudo-Riemannian metric does not induce a non-degenerate metric on parts of the symplectic leaves."],"url":"http://arxiv.org/abs/2404.03221v1","category":"math.DG"}
{"created":"2024-04-04 06:06:38","title":"Commitments are equivalent to one-way state generators","abstract":"One-way state generators (OWSG) are natural quantum analogs to classical one-way functions. We show that $O\\left(\\frac{n}{\\log(n)}\\right)$-copy OWSGs ($n$ represents the input length) are equivalent to $poly(n)$-copy OWSG and to quantum commitments. Since known results show that $o\\left(\\frac{n}{\\log(n)}\\right)$-copy OWSG cannot imply commitments, this shows that $O\\left(\\frac{n}{\\log(n)}\\right)$-copy OWSGs are the weakest OWSGs from which we can get commitments (and hence much of quantum cryptography).   Our construction follows along the lines of H\\r{a}stad, Impagliazzo, Levin and Luby [HILL], who obtained classical pseudorandom generators (PRG) from classical one-way functions (OWF), however with crucial modifications. Our construction, when applied to the classical case, provides an alternative to the construction provided by [HILL]. Since we do not argue conditioned on the output of the one-way function, our construction and analysis are arguably simpler and may be of independent interest.","sentences":["One-way state generators (OWSG) are natural quantum analogs to classical one-way functions.","We show that $O\\left(\\frac{n}{\\log(n)}\\right)$-copy OWSGs ($n$ represents the input length) are equivalent to $poly(n)$-copy OWSG and to quantum commitments.","Since known results show that $o\\left(\\frac{n}{\\log(n)}\\right)$-copy OWSG cannot imply commitments, this shows that $O\\left(\\frac{n}{\\log(n)}\\right)$-copy OWSGs are the weakest OWSGs from which we can get commitments (and hence much of quantum cryptography).   ","Our construction follows along the lines of H\\r{a}stad, Impagliazzo, Levin and Luby [HILL], who obtained classical pseudorandom generators (PRG) from classical one-way functions (OWF), however with crucial modifications.","Our construction, when applied to the classical case, provides an alternative to the construction provided by [HILL].","Since we do not argue conditioned on the output of the one-way function, our construction and analysis are arguably simpler and may be of independent interest."],"url":"http://arxiv.org/abs/2404.03220v1","category":"quant-ph"}
{"created":"2024-04-04 05:33:06","title":"HDR Imaging for Dynamic Scenes with Events","abstract":"High dynamic range imaging (HDRI) for real-world dynamic scenes is challenging because moving objects may lead to hybrid degradation of low dynamic range and motion blur. Existing event-based approaches only focus on a separate task, while cascading HDRI and motion deblurring would lead to sub-optimal solutions, and unavailable ground-truth sharp HDR images aggravate the predicament. To address these challenges, we propose an Event-based HDRI framework within a Self-supervised learning paradigm, i.e., Self-EHDRI, which generalizes HDRI performance in real-world dynamic scenarios. Specifically, a self-supervised learning strategy is carried out by learning cross-domain conversions from blurry LDR images to sharp LDR images, which enables sharp HDR images to be accessible in the intermediate process even though ground-truth sharp HDR images are missing. Then, we formulate the event-based HDRI and motion deblurring model and conduct a unified network to recover the intermediate sharp HDR results, where both the high dynamic range and high temporal resolution of events are leveraged simultaneously for compensation. We construct large-scale synthetic and real-world datasets to evaluate the effectiveness of our method. Comprehensive experiments demonstrate that the proposed Self-EHDRI outperforms state-of-the-art approaches by a large margin. The codes, datasets, and results are available at https://lxp-whu.github.io/Self-EHDRI.","sentences":["High dynamic range imaging (HDRI) for real-world dynamic scenes is challenging because moving objects may lead to hybrid degradation of low dynamic range and motion blur.","Existing event-based approaches only focus on a separate task, while cascading HDRI and motion deblurring would lead to sub-optimal solutions, and unavailable ground-truth sharp HDR images aggravate the predicament.","To address these challenges, we propose an Event-based HDRI framework within a Self-supervised learning paradigm, i.e., Self-EHDRI, which generalizes HDRI performance in real-world dynamic scenarios.","Specifically, a self-supervised learning strategy is carried out by learning cross-domain conversions from blurry LDR images to sharp LDR images, which enables sharp HDR images to be accessible in the intermediate process even though ground-truth sharp HDR images are missing.","Then, we formulate the event-based HDRI and motion deblurring model and conduct a unified network to recover the intermediate sharp HDR results, where both the high dynamic range and high temporal resolution of events are leveraged simultaneously for compensation.","We construct large-scale synthetic and real-world datasets to evaluate the effectiveness of our method.","Comprehensive experiments demonstrate that the proposed Self-EHDRI outperforms state-of-the-art approaches by a large margin.","The codes, datasets, and results are available at https://lxp-whu.github.io/Self-EHDRI."],"url":"http://arxiv.org/abs/2404.03210v1","category":"cs.CV"}
{"created":"2024-04-04 05:30:19","title":"CSR-dMRI: Continuous Super-Resolution of Diffusion MRI with Anatomical Structure-assisted Implicit Neural Representation Learning","abstract":"Deep learning-based dMRI super-resolution methods can effectively enhance image resolution by leveraging the learning capabilities of neural networks on large datasets. However, these methods tend to learn a fixed scale mapping between low-resolution (LR) and high-resolution (HR) images, overlooking the need for radiologists to scale the images at arbitrary resolutions. Moreover, the pixel-wise loss in the image domain tends to generate over-smoothed results, losing fine textures and edge information. To address these issues, we propose a novel continuous super-resolution of dMRI with anatomical structure-assisted implicit neural representation learning method, called CSR-dMRI. Specifically, the CSR-dMRI model consists of two components. The first is the latent feature extractor, which primarily extracts latent space feature maps from LR dMRI and anatomical images while learning structural prior information from the anatomical images. The second is the implicit function network, which utilizes voxel coordinates and latent feature vectors to generate voxel intensities at corresponding positions. Additionally, a frequency-domain-based loss is introduced to preserve the structural and texture information, further enhancing the image quality. Extensive experiments on the publicly available HCP dataset validate the effectiveness of our approach. Furthermore, our method demonstrates superior generalization capability and can be applied to arbitrary-scale super-resolution, including non-integer scale factors, expanding its applicability beyond conventional approaches.","sentences":["Deep learning-based dMRI super-resolution methods can effectively enhance image resolution by leveraging the learning capabilities of neural networks on large datasets.","However, these methods tend to learn a fixed scale mapping between low-resolution (LR) and high-resolution (HR) images, overlooking the need for radiologists to scale the images at arbitrary resolutions.","Moreover, the pixel-wise loss in the image domain tends to generate over-smoothed results, losing fine textures and edge information.","To address these issues, we propose a novel continuous super-resolution of dMRI with anatomical structure-assisted implicit neural representation learning method, called CSR-dMRI.","Specifically, the CSR-dMRI model consists of two components.","The first is the latent feature extractor, which primarily extracts latent space feature maps from LR dMRI and anatomical images while learning structural prior information from the anatomical images.","The second is the implicit function network, which utilizes voxel coordinates and latent feature vectors to generate voxel intensities at corresponding positions.","Additionally, a frequency-domain-based loss is introduced to preserve the structural and texture information, further enhancing the image quality.","Extensive experiments on the publicly available HCP dataset validate the effectiveness of our approach.","Furthermore, our method demonstrates superior generalization capability and can be applied to arbitrary-scale super-resolution, including non-integer scale factors, expanding its applicability beyond conventional approaches."],"url":"http://arxiv.org/abs/2404.03209v1","category":"eess.IV"}
{"created":"2024-04-04 05:30:03","title":"Multimodal hierarchical multi-task deep learning framework for jointly predicting and explaining Alzheimer disease progression","abstract":"Early identification of Mild Cognitive Impairment (MCI) subjects who will eventually progress to Alzheimer Disease (AD) is challenging. Existing deep learning models are mostly single-modality single-task models predicting risk of disease progression at a fixed timepoint. We proposed a multimodal hierarchical multi-task learning approach which can monitor the risk of disease progression at each timepoint of the visit trajectory. Longitudinal visit data from multiple modalities (MRI, cognition, and clinical data) were collected from MCI individuals of the Alzheimer Disease Neuroimaging Initiative (ADNI) dataset. Our hierarchical model predicted at every timepoint a set of neuropsychological composite cognitive function scores as auxiliary tasks and used the forecasted scores at every timepoint to predict the future risk of disease. Relevance weights for each composite function provided explanations about potential factors for disease progression. Our proposed model performed better than state-of-the-art baselines in predicting AD progression risk and the composite scores. Ablation study on the number of modalities demonstrated that imaging and cognition data contributed most towards the outcome. Model explanations at each timepoint can inform clinicians 6 months in advance the potential cognitive function decline that can lead to progression to AD in future. Our model monitored their risk of AD progression every 6 months throughout the visit trajectory of individuals. The hierarchical learning of auxiliary tasks allowed better optimization and allowed longitudinal explanations for the outcome. Our framework is flexible with the number of input modalities and the selection of auxiliary tasks and hence can be generalized to other clinical problems too.","sentences":["Early identification of Mild Cognitive Impairment (MCI) subjects who will eventually progress to Alzheimer Disease (AD) is challenging.","Existing deep learning models are mostly single-modality single-task models predicting risk of disease progression at a fixed timepoint.","We proposed a multimodal hierarchical multi-task learning approach which can monitor the risk of disease progression at each timepoint of the visit trajectory.","Longitudinal visit data from multiple modalities (MRI, cognition, and clinical data) were collected from MCI individuals of the Alzheimer Disease Neuroimaging Initiative (ADNI) dataset.","Our hierarchical model predicted at every timepoint a set of neuropsychological composite cognitive function scores as auxiliary tasks and used the forecasted scores at every timepoint to predict the future risk of disease.","Relevance weights for each composite function provided explanations about potential factors for disease progression.","Our proposed model performed better than state-of-the-art baselines in predicting AD progression risk and the composite scores.","Ablation study on the number of modalities demonstrated that imaging and cognition data contributed most towards the outcome.","Model explanations at each timepoint can inform clinicians 6 months in advance the potential cognitive function decline that can lead to progression to AD in future.","Our model monitored their risk of AD progression every 6 months throughout the visit trajectory of individuals.","The hierarchical learning of auxiliary tasks allowed better optimization and allowed longitudinal explanations for the outcome.","Our framework is flexible with the number of input modalities and the selection of auxiliary tasks and hence can be generalized to other clinical problems too."],"url":"http://arxiv.org/abs/2404.03208v1","category":"cs.LG"}
{"created":"2024-04-04 05:24:39","title":"NLP4Gov: A Comprehensive Library for Computational Policy Analysis","abstract":"Formal rules and policies are fundamental in formally specifying a social system: its operation, boundaries, processes, and even ontology. Recent scholarship has highlighted the role of formal policy in collective knowledge creation, game communities, the production of digital public goods, and national social media governance. Researchers have shown interest in how online communities convene tenable self-governance mechanisms to regulate member activities and distribute rights and privileges by designating responsibilities, roles, and hierarchies. We present NLP4Gov, an interactive kit to train and aid scholars and practitioners alike in computational policy analysis. The library explores and integrates methods and capabilities from computational linguistics and NLP to generate semantic and symbolic representations of community policies from text records. Versatile, documented, and accessible, NLP4Gov provides granular and comparative views into institutional structures and interactions, along with other information extraction capabilities for downstream analysis.","sentences":["Formal rules and policies are fundamental in formally specifying a social system: its operation, boundaries, processes, and even ontology.","Recent scholarship has highlighted the role of formal policy in collective knowledge creation, game communities, the production of digital public goods, and national social media governance.","Researchers have shown interest in how online communities convene tenable self-governance mechanisms to regulate member activities and distribute rights and privileges by designating responsibilities, roles, and hierarchies.","We present NLP4Gov, an interactive kit to train and aid scholars and practitioners alike in computational policy analysis.","The library explores and integrates methods and capabilities from computational linguistics and NLP to generate semantic and symbolic representations of community policies from text records.","Versatile, documented, and accessible, NLP4Gov provides granular and comparative views into institutional structures and interactions, along with other information extraction capabilities for downstream analysis."],"url":"http://arxiv.org/abs/2404.03206v1","category":"cs.HC"}
{"created":"2024-04-04 05:15:07","title":"RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis","abstract":"We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $6.3\\%$ (without reranking) and $2.1\\%$ (with reranking) to $2.8\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.","sentences":["We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis.","While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models.","The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS.","To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style.","Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens.","Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $6.3\\%$ (without reranking) and $2.1\\%$ (with reranking) to $2.8\\%$ and $1.0\\%$, respectively.","Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$."],"url":"http://arxiv.org/abs/2404.03204v1","category":"eess.AS"}
{"created":"2024-04-04 05:12:12","title":"Giant and controllable nonlinear magneto-optical effects in two-dimensional magnets","abstract":"The interplay of polarization and magnetism in materials with light can create rich nonlinear magneto-optical (NLMO) effects, and the recent discovery of two-dimensional (2D) van der Waals magnets provides remarkable control over NLMO effects due to their superb tunability. Here, based on first-principles calculations, we reported giant NLMO effects in CrI3-based 2D magnets, including a dramatic change of second-harmonics generation (SHG) polarization direction (90 degrees) and intensity (on/off switch) under magnetization reversal, and a 100% SHG circular dichroism effect. We further revealed that these effects could not only be used to design ultra-thin multifunctional optical devices, but also to detect subtle magnetic orderings. Remarkably, we analytically derived conditions to achieve giant NLMO effects and propose general strategies to realize them in 2D magnets. Our work not only uncovers a series of intriguing NLMO phenomena, but also paves the way for both fundamental research and device applications of ultra-thin NLMO materials.","sentences":["The interplay of polarization and magnetism in materials with light can create rich nonlinear magneto-optical (NLMO) effects, and the recent discovery of two-dimensional (2D) van der Waals magnets provides remarkable control over NLMO effects due to their superb tunability.","Here, based on first-principles calculations, we reported giant NLMO effects in CrI3-based 2D magnets, including a dramatic change of second-harmonics generation (SHG) polarization direction (90 degrees) and intensity (on/off switch) under magnetization reversal, and a 100% SHG circular dichroism effect.","We further revealed that these effects could not only be used to design ultra-thin multifunctional optical devices, but also to detect subtle magnetic orderings.","Remarkably, we analytically derived conditions to achieve giant NLMO effects and propose general strategies to realize them in 2D magnets.","Our work not only uncovers a series of intriguing NLMO phenomena, but also paves the way for both fundamental research and device applications of ultra-thin NLMO materials."],"url":"http://arxiv.org/abs/2404.03203v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-04 05:08:51","title":"Future-Proofing Class Incremental Learning","abstract":"Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning.","sentences":["Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable.","Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs.","However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step.","To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor.","Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes.","Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning."],"url":"http://arxiv.org/abs/2404.03200v1","category":"cs.LG"}
{"created":"2024-04-04 04:49:46","title":"Okay, Let's Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation","abstract":"In NLP, Event Coreference Resolution (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems. In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive LLMs as distant supervision of smaller student models for cross-document coreference (CDCR) of events. We implement novel rationale-oriented event clustering and knowledge distillation methods for event coreference scoring that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document clustering. Our model using coreference specific knowledge distillation achieves SOTA B3 F1 on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found at https://github.com/csu-signal/llama_cdcr","sentences":["In NLP, Event Coreference Resolution (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems.","In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive LLMs as distant supervision of smaller student models for cross-document coreference (CDCR) of events.","We implement novel rationale-oriented event clustering and knowledge distillation methods for event coreference scoring that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document clustering.","Our model using coreference specific knowledge distillation achieves SOTA B3 F1 on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus.","Our code can be found at https://github.com/csu-signal/llama_cdcr"],"url":"http://arxiv.org/abs/2404.03196v1","category":"cs.CL"}
{"created":"2024-04-04 17:36:24","title":"Collective coupling of driven multilevel atoms and its effect on four-wave mixing","abstract":"Microscopic models based on multilevel atoms are central to controlling non-linear optical responses and coherent control of light. These models are traditionally based on single-atom responses that are then parametrically extrapolated to include collective effects, such as an enhanced response or propagation within the medium. In this work we present a systematic analysis of the cooperative effects arising in driven systems composed of multi-level atoms coupled by a common electromagnetic environment. The analysis is based on an interplay between dressed states induced by the driving field, photon exchanges, and the collective decay channels. This theory is applied to the case of four-wave mixing mediated by a pair of atoms in the diamond configuration, a nonlinear process commonly used to create correlated photon pairs. The effects of inter-atomic correlations and collective decay channels over outgoing photons are then explored. We find that the resulting correlation function changes from a Lorentzian distribution for independent atoms to a two-peaked distribution when dipole-dipole interactions are included. The two-peak structure is related to a joint action between the dressed states and the collective decay channels. Signatures of these processes are identified for existing experimental realizations. The intuition obtained from this connection helps to uncover relevant parameters that could be exploited for quantum control protocols based on dispersive and dissipative cooperative effects in multi-level systems.","sentences":["Microscopic models based on multilevel atoms are central to controlling non-linear optical responses and coherent control of light.","These models are traditionally based on single-atom responses that are then parametrically extrapolated to include collective effects, such as an enhanced response or propagation within the medium.","In this work we present a systematic analysis of the cooperative effects arising in driven systems composed of multi-level atoms coupled by a common electromagnetic environment.","The analysis is based on an interplay between dressed states induced by the driving field, photon exchanges, and the collective decay channels.","This theory is applied to the case of four-wave mixing mediated by a pair of atoms in the diamond configuration, a nonlinear process commonly used to create correlated photon pairs.","The effects of inter-atomic correlations and collective decay channels over outgoing photons are then explored.","We find that the resulting correlation function changes from a Lorentzian distribution for independent atoms to a two-peaked distribution when dipole-dipole interactions are included.","The two-peak structure is related to a joint action between the dressed states and the collective decay channels.","Signatures of these processes are identified for existing experimental realizations.","The intuition obtained from this connection helps to uncover relevant parameters that could be exploited for quantum control protocols based on dispersive and dissipative cooperative effects in multi-level systems."],"url":"http://arxiv.org/abs/2404.03615v1","category":"quant-ph"}
{"created":"2024-04-04 17:19:47","title":"Evaluating LLMs at Detecting Errors in LLM Responses","abstract":"With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake.","sentences":["With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial.","However, little research has been conducted on error detection of LLM responses.","Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization).","This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs.","ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts.","We use ReaLMistake to evaluate error detectors based on 12 LLMs.","Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans.","2) Explanations by LLM-based error detectors lack reliability.","3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve.","4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance.","Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake."],"url":"http://arxiv.org/abs/2404.03602v1","category":"cs.CL"}
{"created":"2024-04-04 16:03:02","title":"Streamlining CXL Adoption for Hyperscale Efficiency","abstract":"In our exploration of Composable Memory systems utilizing CXL, we focus on overcoming adoption barriers at Hyperscale, underscored by economic models demonstrating Total Cost of Ownership (TCO). While CXL addresses the pressing memory capacity needs of emerging Hyperscale applications, the escalating demands from evolving use cases such as AI outpace the capabilities of current CXL solutions. Hyperscalers resort to software-based memory (de)compression technology, alleviating memory capacity, storage, and network constraints but incurring a notable \"Tax\" on Compute CPU cycles. As a pivotal guide to the CXL community, Hyperscalers have formulated the groundbreaking Open Compute Project (OCP) Hyperscale CXL Tiered Memory Expander specification. If implemented, this specification lowers TCO adoption barriers, enabling diverse CXL deployments at both Hyperscaler and Enterprise levels. We present a CXL integrated solution, aligning with the aforementioned specification, introducing an energy-efficient, scalable, hardware-accelerated, Lossless Compressed Memory CXL Tier. This solution, slated for mid-2024 production and open for integration with Memory Expander controller manufacturers, offers 2-3X CXL memory compression in nanoseconds, delivering a 20-25% reduction in TCO for end customers without requiring additional physical slots. In our discussion, we pinpoint areas for collaborative innovation within the CXL Community to expedite software/hardware advancements for CXL Tiered Memory Expansion. Furthermore, we delve into unresolved challenges in Pooled deployment and explore potential solutions, collectively aiming to make CXL adoption a \"No Brainer\" at Hyperscale.","sentences":["In our exploration of Composable Memory systems utilizing CXL, we focus on overcoming adoption barriers at Hyperscale, underscored by economic models demonstrating Total Cost of Ownership (TCO).","While CXL addresses the pressing memory capacity needs of emerging Hyperscale applications, the escalating demands from evolving use cases such as AI outpace the capabilities of current CXL solutions.","Hyperscalers resort to software-based memory (de)compression technology, alleviating memory capacity, storage, and network constraints but incurring a notable \"Tax\" on Compute CPU cycles.","As a pivotal guide to the CXL community, Hyperscalers have formulated the groundbreaking Open Compute Project (OCP)","Hyperscale CXL Tiered Memory Expander specification.","If implemented, this specification lowers TCO adoption barriers, enabling diverse CXL deployments at both Hyperscaler and Enterprise levels.","We present a CXL integrated solution, aligning with the aforementioned specification, introducing an energy-efficient, scalable, hardware-accelerated, Lossless Compressed Memory CXL Tier.","This solution, slated for mid-2024 production and open for integration with Memory Expander controller manufacturers, offers 2-3X CXL memory compression in nanoseconds, delivering a 20-25% reduction in TCO for end customers without requiring additional physical slots.","In our discussion, we pinpoint areas for collaborative innovation within the CXL Community to expedite software/hardware advancements for CXL Tiered Memory Expansion.","Furthermore, we delve into unresolved challenges in Pooled deployment and explore potential solutions, collectively aiming to make CXL adoption a \"No Brainer\" at Hyperscale."],"url":"http://arxiv.org/abs/2404.03551v1","category":"cs.ET"}
{"created":"2024-04-04 15:45:25","title":"If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces","abstract":"Recent advances in deep face recognition have spurred a growing demand for large, diverse, and manually annotated face datasets. Acquiring authentic, high-quality data for face recognition has proven to be a challenge, primarily due to privacy concerns. Large face datasets are primarily sourced from web-based images, lacking explicit user consent. In this paper, we examine whether and how synthetic face data can be used to train effective face recognition models with reduced reliance on authentic images, thereby mitigating data collection concerns. First, we explored the performance gap among recent state-of-the-art face recognition models, trained with synthetic data only and authentic (scarce) data only. Then, we deepened our analysis by training a state-of-the-art backbone with various combinations of synthetic and authentic data, gaining insights into optimizing the limited use of the latter for verification accuracy. Finally, we assessed the effectiveness of data augmentation approaches on synthetic and authentic data, with the same goal in mind. Our results highlighted the effectiveness of FR trained on combined datasets, particularly when combined with appropriate augmentation techniques.","sentences":["Recent advances in deep face recognition have spurred a growing demand for large, diverse, and manually annotated face datasets.","Acquiring authentic, high-quality data for face recognition has proven to be a challenge, primarily due to privacy concerns.","Large face datasets are primarily sourced from web-based images, lacking explicit user consent.","In this paper, we examine whether and how synthetic face data can be used to train effective face recognition models with reduced reliance on authentic images, thereby mitigating data collection concerns.","First, we explored the performance gap among recent state-of-the-art face recognition models, trained with synthetic data only and authentic (scarce) data only.","Then, we deepened our analysis by training a state-of-the-art backbone with various combinations of synthetic and authentic data, gaining insights into optimizing the limited use of the latter for verification accuracy.","Finally, we assessed the effectiveness of data augmentation approaches on synthetic and authentic data, with the same goal in mind.","Our results highlighted the effectiveness of FR trained on combined datasets, particularly when combined with appropriate augmentation techniques."],"url":"http://arxiv.org/abs/2404.03537v1","category":"cs.CV"}
{"created":"2024-04-04 13:59:04","title":"Synergy as the failure of distributivity","abstract":"A physical system is synergistic if it cannot be reduced to its constituents. Intuitively this is paraphrased into the common statement that 'the whole is greater than the sum of its parts'. In this manner, many basic parts in combination may give rise to some unexpected collective behavior. A paradigmatic example of such phenomenon is information. Several sources, which are already known individually, may provide some new knowledge when joined together. Here we take the trivial case of discrete random variables and explore whether and how it is possible to get more information out of lesser parts. Our approach is inspired by set theory as the fundamental description of part-whole relations. If taken unaltered, synergistic behavior is forbidden by the set theoretical axioms. Indeed, the union of sets cannot contain extra elements not found in any particular set. However, random variables are not a perfect analogy of sets. We formalise the distinction, finding a single broken axiom - union/intersection distributivity. Nevertheless, it remains possible to describe information using Venn-type diagrams. We directly connect the existence of synergy to the failure of distributivity for random variables. When compared to the partial information decomposition framework (PID), our technique fully reproduces previous results while resolving the self-contradictions that plagued them and providing additional constraints on the solutions. This opens the way towards quantifying emergence in large systems.","sentences":["A physical system is synergistic if it cannot be reduced to its constituents.","Intuitively this is paraphrased into the common statement that 'the whole is greater than the sum of its parts'.","In this manner, many basic parts in combination may give rise to some unexpected collective behavior.","A paradigmatic example of such phenomenon is information.","Several sources, which are already known individually, may provide some new knowledge when joined together.","Here we take the trivial case of discrete random variables and explore whether and how it is possible to get more information out of lesser parts.","Our approach is inspired by set theory as the fundamental description of part-whole relations.","If taken unaltered, synergistic behavior is forbidden by the set theoretical axioms.","Indeed, the union of sets cannot contain extra elements not found in any particular set.","However, random variables are not a perfect analogy of sets.","We formalise the distinction, finding a single broken axiom - union/intersection distributivity.","Nevertheless, it remains possible to describe information using Venn-type diagrams.","We directly connect the existence of synergy to the failure of distributivity for random variables.","When compared to the partial information decomposition framework (PID), our technique fully reproduces previous results while resolving the self-contradictions that plagued them and providing additional constraints on the solutions.","This opens the way towards quantifying emergence in large systems."],"url":"http://arxiv.org/abs/2404.03455v1","category":"cs.IT"}
{"created":"2024-04-04 13:55:06","title":"How Much Data are Enough? Investigating Dataset Requirements for Patch-Based Brain MRI Segmentation Tasks","abstract":"Training deep neural networks reliably requires access to large-scale datasets. However, obtaining such datasets can be challenging, especially in the context of neuroimaging analysis tasks, where the cost associated with image acquisition and annotation can be prohibitive. To mitigate both the time and financial costs associated with model development, a clear understanding of the amount of data required to train a satisfactory model is crucial. This paper focuses on an early stage phase of deep learning research, prior to model development, and proposes a strategic framework for estimating the amount of annotated data required to train patch-based segmentation networks. This framework includes the establishment of performance expectations using a novel Minor Boundary Adjustment for Threshold (MinBAT) method, and standardizing patch selection through the ROI-based Expanded Patch Selection (REPS) method. Our experiments demonstrate that tasks involving regions of interest (ROIs) with different sizes or shapes may yield variably acceptable Dice Similarity Coefficient (DSC) scores. By setting an acceptable DSC as the target, the required amount of training data can be estimated and even predicted as data accumulates. This approach could assist researchers and engineers in estimating the cost associated with data collection and annotation when defining a new segmentation task based on deep neural networks, ultimately contributing to their efficient translation to real-world applications.","sentences":["Training deep neural networks reliably requires access to large-scale datasets.","However, obtaining such datasets can be challenging, especially in the context of neuroimaging analysis tasks, where the cost associated with image acquisition and annotation can be prohibitive.","To mitigate both the time and financial costs associated with model development, a clear understanding of the amount of data required to train a satisfactory model is crucial.","This paper focuses on an early stage phase of deep learning research, prior to model development, and proposes a strategic framework for estimating the amount of annotated data required to train patch-based segmentation networks.","This framework includes the establishment of performance expectations using a novel Minor Boundary Adjustment for Threshold (MinBAT) method, and standardizing patch selection through the ROI-based Expanded Patch Selection (REPS) method.","Our experiments demonstrate that tasks involving regions of interest (ROIs) with different sizes or shapes may yield variably acceptable Dice Similarity Coefficient (DSC) scores.","By setting an acceptable DSC as the target, the required amount of training data can be estimated and even predicted as data accumulates.","This approach could assist researchers and engineers in estimating the cost associated with data collection and annotation when defining a new segmentation task based on deep neural networks, ultimately contributing to their efficient translation to real-world applications."],"url":"http://arxiv.org/abs/2404.03451v1","category":"cs.CV"}
{"created":"2024-04-04 13:36:01","title":"Knowledge Graph Representation for Political Information Sources","abstract":"With the rise of computational social science, many scholars utilize data analysis and natural language processing tools to analyze social media, news articles, and other accessible data sources for examining political and social discourse. Particularly, the study of the emergence of echo-chambers due to the dissemination of specific information has become a topic of interest in mixed methods research areas. In this paper, we analyze data collected from two news portals, Breitbart News (BN) and New York Times (NYT) to prove the hypothesis that the formation of echo-chambers can be partially explained on the level of an individual information consumption rather than a collective topology of individuals' social networks. Our research findings are presented through knowledge graphs, utilizing a dataset spanning 11.5 years gathered from BN and NYT media portals. We demonstrate that the application of knowledge representation techniques to the aforementioned news streams highlights, contrary to common assumptions, shows relative \"internal\" neutrality of both sources and polarizing attitude towards a small fraction of entities. Additionally, we argue that such characteristics in information sources lead to fundamental disparities in audience worldviews, potentially acting as a catalyst for the formation of echo-chambers.","sentences":["With the rise of computational social science, many scholars utilize data analysis and natural language processing tools to analyze social media, news articles, and other accessible data sources for examining political and social discourse.","Particularly, the study of the emergence of echo-chambers due to the dissemination of specific information has become a topic of interest in mixed methods research areas.","In this paper, we analyze data collected from two news portals, Breitbart News (BN) and New York Times (NYT) to prove the hypothesis that the formation of echo-chambers can be partially explained on the level of an individual information consumption rather than a collective topology of individuals' social networks.","Our research findings are presented through knowledge graphs, utilizing a dataset spanning 11.5 years gathered from BN and NYT media portals.","We demonstrate that the application of knowledge representation techniques to the aforementioned news streams highlights, contrary to common assumptions, shows relative \"internal\" neutrality of both sources and polarizing attitude towards a small fraction of entities.","Additionally, we argue that such characteristics in information sources lead to fundamental disparities in audience worldviews, potentially acting as a catalyst for the formation of echo-chambers."],"url":"http://arxiv.org/abs/2404.03437v1","category":"cs.CL"}
{"created":"2024-04-04 13:22:29","title":"Equivalence and Similarity Refutation for Probabilistic Programs","abstract":"We consider the problems of statically refuting equivalence and similarity of output distributions defined by a pair of probabilistic programs. Equivalence and similarity are two fundamental relational properties of probabilistic programs that are essential for their correctness both in implementation and in compilation. In this work, we present a new method for static equivalence and similarity refutation. Our method refutes equivalence and similarity by computing a function over program outputs whose expected value with respect to the output distributions of two programs is different. The function is computed simultaneously with an upper expectation supermartingale and a lower expectation submartingale for the two programs, which we show to together provide a formal certificate for refuting equivalence and similarity. To the best of our knowledge, our method is the first approach to relational program analysis to offer the combination of the following desirable features: (1) it is fully automated, (2) it is applicable to infinite-state probabilistic programs, and (3) it provides formal guarantees on the correctness of its results. We implement a prototype of our method and our experiments demonstrate the effectiveness of our method to refute equivalence and similarity for a number of examples collected from the literature.","sentences":["We consider the problems of statically refuting equivalence and similarity of output distributions defined by a pair of probabilistic programs.","Equivalence and similarity are two fundamental relational properties of probabilistic programs that are essential for their correctness both in implementation and in compilation.","In this work, we present a new method for static equivalence and similarity refutation.","Our method refutes equivalence and similarity by computing a function over program outputs whose expected value with respect to the output distributions of two programs is different.","The function is computed simultaneously with an upper expectation supermartingale and a lower expectation submartingale for the two programs, which we show to together provide a formal certificate for refuting equivalence and similarity.","To the best of our knowledge, our method is the first approach to relational program analysis to offer the combination of the following desirable features: (1) it is fully automated, (2) it is applicable to infinite-state probabilistic programs, and (3) it provides formal guarantees on the correctness of its results.","We implement a prototype of our method and our experiments demonstrate the effectiveness of our method to refute equivalence and similarity for a number of examples collected from the literature."],"url":"http://arxiv.org/abs/2404.03430v1","category":"cs.PL"}
{"created":"2024-04-04 12:13:06","title":"Comparative Efficacy of Commercial Wearables for Circadian Rhythm Home Monitoring from Activity, Heart Rate, and Core Body Temperature","abstract":"Circadian rhythms govern biological patterns that follow a 24-hour cycle. Dysfunctions in circadian rhythms can contribute to various health problems, such as sleep disorders. Current circadian rhythm assessment methods, often invasive or subjective, limit circadian rhythm monitoring to laboratories. Hence, this study aims to investigate scalable consumer-centric wearables for circadian rhythm monitoring outside traditional laboratories. In a two-week longitudinal study conducted in real-world settings, 36 participants wore an Actigraph, a smartwatch, and a core body temperature sensor to collect activity, temperature, and heart rate data. We evaluated circadian rhythms calculated from commercial wearables by comparing them with circadian rhythm reference measures, i.e., Actigraph activities and chronotype questionnaire scores. The circadian rhythm metric acrophases, determined from commercial wearables using activity, heart rate, and temperature data, significantly correlated with the acrophase derived from Actigraph activities (r=0.96, r=0.87, r=0.79; all p<0.001) and chronotype questionnaire (r=-0.66, r=-0.73, r=-0.61; all p<0.001). The acrophases obtained concurrently from consumer sensors significantly predicted the chronotype (R2=0.64; p<0.001). Our study validates commercial sensors for circadian rhythm assessment, highlighting their potential to support maintaining healthy rhythms and provide scalable and timely health monitoring in real-life scenarios.","sentences":["Circadian rhythms govern biological patterns that follow a 24-hour cycle.","Dysfunctions in circadian rhythms can contribute to various health problems, such as sleep disorders.","Current circadian rhythm assessment methods, often invasive or subjective, limit circadian rhythm monitoring to laboratories.","Hence, this study aims to investigate scalable consumer-centric wearables for circadian rhythm monitoring outside traditional laboratories.","In a two-week longitudinal study conducted in real-world settings, 36 participants wore an Actigraph, a smartwatch, and a core body temperature sensor to collect activity, temperature, and heart rate data.","We evaluated circadian rhythms calculated from commercial wearables by comparing them with circadian rhythm reference measures, i.e., Actigraph activities and chronotype questionnaire scores.","The circadian rhythm metric acrophases, determined from commercial wearables using activity, heart rate, and temperature data, significantly correlated with the acrophase derived from Actigraph activities (r=0.96, r=0.87, r=0.79; all p<0.001) and chronotype questionnaire (r=-0.66, r=-0.73, r=-0.61; all p<0.001).","The acrophases obtained concurrently from consumer sensors significantly predicted the chronotype (R2=0.64; p<0.001).","Our study validates commercial sensors for circadian rhythm assessment, highlighting their potential to support maintaining healthy rhythms and provide scalable and timely health monitoring in real-life scenarios."],"url":"http://arxiv.org/abs/2404.03408v1","category":"eess.SP"}
{"created":"2024-04-04 09:55:11","title":"MPOFI: Multichannel Partially Observed Functional Modeling for Defect Classification with Imbalanced Dataset via Deep Metric Learning","abstract":"In modern manufacturing, most of the product lines are conforming. Few products are nonconforming but with different defect types. The identification of defect types can help further root cause diagnosis of production lines. With the sensing development, continuous signals of process variables can be collected in high resolution, which can be regarded as multichannel functional data. They have abundant information to characterize the process and help identify the defect types. Motivated by a real example from the pipe tightening process, we target at detect classification when each sample is a multichannel functional data. However, the available samples for each defect type are limited and imbalanced. Moreover, the functions are partially observed since the pre-tightening process before the pipe tightening process is unobserved. To classify the defect samples based on imbalanced, multichannel, and partially observed functional data is very important but challenging. Thus, we propose an innovative framework known as \"Multichannel Partially Observed Functional Modeling for Defect Classification with an Imbalanced Dataset\" (MPOFI). The framework leverages the power of deep metric learning in conjunction with a neural network specially crafted for processing functional data. This paper introduces a neural network explicitly tailored for handling multichannel and partially observed functional data, complemented by developing a corresponding loss function for training on imbalanced datasets. The results from a real-world case study demonstrate the superior accuracy of our framework when compared to existing benchmarks.","sentences":["In modern manufacturing, most of the product lines are conforming.","Few products are nonconforming but with different defect types.","The identification of defect types can help further root cause diagnosis of production lines.","With the sensing development, continuous signals of process variables can be collected in high resolution, which can be regarded as multichannel functional data.","They have abundant information to characterize the process and help identify the defect types.","Motivated by a real example from the pipe tightening process, we target at detect classification when each sample is a multichannel functional data.","However, the available samples for each defect type are limited and imbalanced.","Moreover, the functions are partially observed since the pre-tightening process before the pipe tightening process is unobserved.","To classify the defect samples based on imbalanced, multichannel, and partially observed functional data is very important but challenging.","Thus, we propose an innovative framework known as \"Multichannel Partially Observed Functional Modeling for Defect Classification with an Imbalanced Dataset\" (MPOFI).","The framework leverages the power of deep metric learning in conjunction with a neural network specially crafted for processing functional data.","This paper introduces a neural network explicitly tailored for handling multichannel and partially observed functional data, complemented by developing a corresponding loss function for training on imbalanced datasets.","The results from a real-world case study demonstrate the superior accuracy of our framework when compared to existing benchmarks."],"url":"http://arxiv.org/abs/2404.03329v1","category":"cs.LG"}
{"created":"2024-04-04 09:52:45","title":"A Directional Diffusion Graph Transformer for Recommendation","abstract":"In real-world recommender systems, implicitly collected user feedback, while abundant, often includes noisy false-positive and false-negative interactions. The possible misinterpretations of the user-item interactions pose a significant challenge for traditional graph neural recommenders. These approaches aggregate the users' or items' neighbours based on implicit user-item interactions in order to accurately capture the users' profiles. To account for and model possible noise in the users' interactions in graph neural recommenders, we propose a novel Diffusion Graph Transformer (DiffGT) model for top-k recommendation. Our DiffGT model employs a diffusion process, which includes a forward phase for gradually introducing noise to implicit interactions, followed by a reverse process to iteratively refine the representations of the users' hidden preferences (i.e., a denoising process). In our proposed approach, given the inherent anisotropic structure observed in the user-item interaction graph, we specifically use anisotropic and directional Gaussian noises in the forward diffusion process. Our approach differs from the sole use of isotropic Gaussian noises in existing diffusion models. In the reverse diffusion process, to reverse the effect of noise added earlier and recover the true users' preferences, we integrate a graph transformer architecture with a linear attention module to denoise the noisy user/item embeddings in an effective and efficient manner. In addition, such a reverse diffusion process is further guided by personalised information (e.g., interacted items) to enable the accurate estimation of the users' preferences on items. Our extensive experiments conclusively demonstrate the superiority of our proposed graph diffusion model over ten existing state-of-the-art approaches across three benchmark datasets.","sentences":["In real-world recommender systems, implicitly collected user feedback, while abundant, often includes noisy false-positive and false-negative interactions.","The possible misinterpretations of the user-item interactions pose a significant challenge for traditional graph neural recommenders.","These approaches aggregate the users' or items' neighbours based on implicit user-item interactions in order to accurately capture the users' profiles.","To account for and model possible noise in the users' interactions in graph neural recommenders, we propose a novel Diffusion Graph Transformer (DiffGT) model for top-k recommendation.","Our DiffGT model employs a diffusion process, which includes a forward phase for gradually introducing noise to implicit interactions, followed by a reverse process to iteratively refine the representations of the users' hidden preferences (i.e., a denoising process).","In our proposed approach, given the inherent anisotropic structure observed in the user-item interaction graph, we specifically use anisotropic and directional Gaussian noises in the forward diffusion process.","Our approach differs from the sole use of isotropic Gaussian noises in existing diffusion models.","In the reverse diffusion process, to reverse the effect of noise added earlier and recover the true users' preferences, we integrate a graph transformer architecture with a linear attention module to denoise the noisy user/item embeddings in an effective and efficient manner.","In addition, such a reverse diffusion process is further guided by personalised information (e.g., interacted items) to enable the accurate estimation of the users' preferences on items.","Our extensive experiments conclusively demonstrate the superiority of our proposed graph diffusion model over ten existing state-of-the-art approaches across three benchmark datasets."],"url":"http://arxiv.org/abs/2404.03326v1","category":"cs.IR"}
{"created":"2024-04-04 09:34:04","title":"The GAPS Programme at TNG. XXX: Characterization of the low-density gas giant HAT-P-67 b with GIARPS","abstract":"HAT-P-67 b is one of the lowest-density gas giants known to date, making it an excellent target for atmospheric characterization through the transmission spectroscopy technique. In the framework of the GAPS large programme, we collected four transit events, with the aim of studying the exoplanet atmosphere and deriving the orbital projected obliquity. We exploited the high-precision GIARPS observing mode of the TNG, along with additional archival TESS photometry, to explore the activity level of the host star. We performed transmission spectroscopy, both in the VIS and in the nIR wavelength range, and analysed the RML effect both fitting the RVs and the Doppler shadow. Based on the TESS photometry, we redetermined the transit parameters of HAT-P-67 b. By modelling the RML effect, we derived a sky-projected obliquity of ($2.2\\pm0.4$){\\deg} indicating an aligned planetary orbit. The chromospheric activity index $\\log\\,R^{\\prime}_{\\rm HK}$, the CCF profile, and the variability in the transmission spectrum of the H$\\alpha$ line suggest that the host star shows signatures of stellar activity and/or pulsations. We found no evidence of atomic or molecular species in the VIS transmission spectra, with the exception of pseudo-signals corresponding to Cr I, Fe I, H$\\alpha$, Na I, and Ti I. In the nIR range, we found an absorption signal of the He I triplet of 5.56$^{+0.29}_{-0.30}$%(19.0$\\sigma$), corresponding to an effective planetary radius of $\\sim$3$R_p$ (where $R_p\\sim$2$R_J$) which extends beyond the planet's Roche Lobe radius. Owing to the stellar variability, together with the high uncertainty of the model, we could not confirm the planetary origin of the signals found in the optical transmission spectrum. On the other hand, we confirmed previous detections of the infrared He I triplet, providing a 19.0$\\sigma$ detection. Our finding indicates that the planet's atmosphere is evaporating.","sentences":["HAT-P-67 b is one of the lowest-density gas giants known to date, making it an excellent target for atmospheric characterization through the transmission spectroscopy technique.","In the framework of the GAPS large programme, we collected four transit events, with the aim of studying the exoplanet atmosphere and deriving the orbital projected obliquity.","We exploited the high-precision GIARPS observing mode of the TNG, along with additional archival TESS photometry, to explore the activity level of the host star.","We performed transmission spectroscopy, both in the VIS and in the nIR wavelength range, and analysed the RML effect both fitting the RVs and the Doppler shadow.","Based on the TESS photometry, we redetermined the transit parameters of HAT-P-67 b.","By modelling the RML effect, we derived a sky-projected obliquity of ($2.2\\pm0.4$){\\deg} indicating an aligned planetary orbit.","The chromospheric activity index $\\log\\,R^{\\prime}_{\\rm HK}$, the CCF profile, and the variability in the transmission spectrum of the H$\\alpha$ line suggest that the host star shows signatures of stellar activity and/or pulsations.","We found no evidence of atomic or molecular species in the VIS transmission spectra, with the exception of pseudo-signals corresponding to Cr","I, Fe I, H$\\alpha$, Na I, and Ti I.","In the nIR range, we found an absorption signal of the He I triplet of 5.56$^{+0.29}_{-0.30}$%(19.0$\\sigma$), corresponding to an effective planetary radius of $\\sim$3$R_p$ (where $R_p\\sim$2$R_J$) which extends beyond the planet's Roche Lobe radius.","Owing to the stellar variability, together with the high uncertainty of the model, we could not confirm the planetary origin of the signals found in the optical transmission spectrum.","On the other hand, we confirmed previous detections of the infrared He I triplet, providing a 19.0$\\sigma$ detection.","Our finding indicates that the planet's atmosphere is evaporating."],"url":"http://arxiv.org/abs/2404.03317v1","category":"astro-ph.EP"}
{"created":"2024-04-04 05:49:17","title":"Evidence of the $h_c\\to K_S^0 K^+\u03c0^-+c.c.$ decay","abstract":"Based on $(2.712\\pm0.014)\\times10^9$ $\\psi(3686)$ events collected by the BESIII collaboration, evidence of the hadronic decay $h_c\\to K_S^0K^+\\pi^-+c.c.$ is found with a significance of $4.3\\sigma$ in the $\\psi(3686)\\to\\pi^0 h_c$ process. The branching fraction of $h_c\\to K_S^0 K^+\\pi^- +c.c.$ is measured to be $(7.3\\pm0.8\\pm1.8)\\times10^{-4}$, where the first and second uncertainties are statistical and systematic, respectively. Combining with the exclusive decay width of $\\eta_c\\to K\\bar{K}\\pi$, our result indicates inconsistencies with both pQCD and NRQCD predictions.","sentences":["Based on $(2.712\\pm0.014)\\times10^9$ $\\psi(3686)$ events collected by the BESIII collaboration, evidence of the hadronic decay $h_c\\to K_S^0K^+\\pi^-+c.c.$ is found with a significance of $4.3\\sigma$ in the $\\psi(3686)\\to\\pi^0 h_c$ process.","The branching fraction of $h_c\\to K_S^0 K^+\\pi^-","+c.c.$ is measured to be $(7.3\\pm0.8\\pm1.8)\\times10^{-4}$, where the first and second uncertainties are statistical and systematic, respectively.","Combining with the exclusive decay width of $\\eta_c\\to K\\bar{K}\\pi$, our result indicates inconsistencies with both pQCD and NRQCD predictions."],"url":"http://arxiv.org/abs/2404.03217v1","category":"hep-ex"}
{"created":"2024-04-04 04:22:50","title":"CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception Tasks","abstract":"Numerous roadside perception datasets have been introduced to propel advancements in autonomous driving and intelligent transportation systems research and development. However, it has been observed that the majority of their concentrates is on urban arterial roads, inadvertently overlooking residential areas such as parks and campuses that exhibit entirely distinct characteristics. In light of this gap, we propose CORP, which stands as the first public benchmark dataset tailored for multi-modal roadside perception tasks under campus scenarios. Collected in a university campus, CORP consists of over 205k images plus 102k point clouds captured from 18 cameras and 9 LiDAR sensors. These sensors with different configurations are mounted on roadside utility poles to provide diverse viewpoints within the campus region. The annotations of CORP encompass multi-dimensional information beyond 2D and 3D bounding boxes, providing extra support for 3D seamless tracking and instance segmentation with unique IDs and pixel masks for identifying targets, to enhance the understanding of objects and their behaviors distributed across the campus premises. Unlike other roadside datasets about urban traffic, CORP extends the spectrum to highlight the challenges for multi-modal perception in campuses and other residential areas.","sentences":["Numerous roadside perception datasets have been introduced to propel advancements in autonomous driving and intelligent transportation systems research and development.","However, it has been observed that the majority of their concentrates is on urban arterial roads, inadvertently overlooking residential areas such as parks and campuses that exhibit entirely distinct characteristics.","In light of this gap, we propose CORP, which stands as the first public benchmark dataset tailored for multi-modal roadside perception tasks under campus scenarios.","Collected in a university campus, CORP consists of over 205k images plus 102k point clouds captured from 18 cameras and 9 LiDAR sensors.","These sensors with different configurations are mounted on roadside utility poles to provide diverse viewpoints within the campus region.","The annotations of CORP encompass multi-dimensional information beyond 2D and 3D bounding boxes, providing extra support for 3D seamless tracking and instance segmentation with unique IDs and pixel masks for identifying targets, to enhance the understanding of objects and their behaviors distributed across the campus premises.","Unlike other roadside datasets about urban traffic, CORP extends the spectrum to highlight the challenges for multi-modal perception in campuses and other residential areas."],"url":"http://arxiv.org/abs/2404.03191v1","category":"cs.CV"}
{"created":"2024-04-04 04:22:25","title":"Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation","abstract":"In self-supervised monocular depth estimation tasks, discrete disparity prediction has been proven to attain higher quality depth maps than common continuous methods. However, current discretization strategies often divide depth ranges of scenes into bins in a handcrafted and rigid manner, limiting model performance. In this paper, we propose a learnable module, Adaptive Discrete Disparity Volume (ADDV), which is capable of dynamically sensing depth distributions in different RGB images and generating adaptive bins for them. Without any extra supervision, this module can be integrated into existing CNN architectures, allowing networks to produce representative values for bins and a probability volume over them. Furthermore, we introduce novel training strategies - uniformizing and sharpening - through a loss term and temperature parameter, respectively, to provide regularizations under self-supervised conditions, preventing model degradation or collapse. Empirical results demonstrate that ADDV effectively processes global information, generating appropriate bins for various scenes and producing higher quality depth maps compared to handcrafted methods.","sentences":["In self-supervised monocular depth estimation tasks, discrete disparity prediction has been proven to attain higher quality depth maps than common continuous methods.","However, current discretization strategies often divide depth ranges of scenes into bins in a handcrafted and rigid manner, limiting model performance.","In this paper, we propose a learnable module, Adaptive Discrete Disparity Volume (ADDV), which is capable of dynamically sensing depth distributions in different RGB images and generating adaptive bins for them.","Without any extra supervision, this module can be integrated into existing CNN architectures, allowing networks to produce representative values for bins and a probability volume over them.","Furthermore, we introduce novel training strategies - uniformizing and sharpening - through a loss term and temperature parameter, respectively, to provide regularizations under self-supervised conditions, preventing model degradation or collapse.","Empirical results demonstrate that ADDV effectively processes global information, generating appropriate bins for various scenes and producing higher quality depth maps compared to handcrafted methods."],"url":"http://arxiv.org/abs/2404.03190v1","category":"cs.CV"}
{"created":"2024-04-04 04:20:04","title":"The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models","abstract":"In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.","sentences":["In order to oversee advanced AI systems, it is important to understand their underlying decision-making process.","When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators.","However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions.","In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions.","Previous metrics used in such tests take into account only binary changes in the predictions.","Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness.","We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023).","We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks.","We find that our metric measures aspects of faithfulness which the CT misses."],"url":"http://arxiv.org/abs/2404.03189v1","category":"cs.CL"}
{"created":"2024-04-04 03:50:34","title":"The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0","abstract":"Machine reading comprehension is an essential natural language processing task, which takes into a pair of context and query and predicts the corresponding answer to query. In this project, we developed an end-to-end question answering model incorporating BERT and additional linguistic features. We conclude that the BERT base model will be improved by incorporating the features. The EM score and F1 score are improved 2.17 and 2.14 compared with BERT(base). Our best single model reaches EM score 76.55 and F1 score 79.97 in the hidden test set. Our error analysis also shows that the linguistic architecture can help model understand the context better in that it can locate answers that BERT only model predicted \"No Answer\" wrongly.","sentences":["Machine reading comprehension is an essential natural language processing task, which takes into a pair of context and query and predicts the corresponding answer to query.","In this project, we developed an end-to-end question answering model incorporating BERT and additional linguistic features.","We conclude that the BERT base model will be improved by incorporating the features.","The EM score and F1 score are improved 2.17 and 2.14 compared with BERT(base).","Our best single model reaches EM score 76.55 and F1 score 79.97 in the hidden test set.","Our error analysis also shows that the linguistic architecture can help model understand the context better in that it can locate answers that BERT only model predicted \"No Answer\" wrongly."],"url":"http://arxiv.org/abs/2404.03184v1","category":"cs.CL"}
{"created":"2024-04-04 02:32:58","title":"Does Knowledge Graph Really Matter for Recommender Systems?","abstract":"Recommender systems (RSs) are designed to provide personalized recommendations to users. Recently, knowledge graphs (KGs) have been widely introduced in RSs to improve recommendation accuracy. In this study, however, we demonstrate that RSs do not necessarily perform worse even if the KG is downgraded to the user-item interaction graph only (or removed). We propose an evaluation framework KG4RecEval to systematically evaluate how much a KG contributes to the recommendation accuracy of a KG-based RS, using our defined metric KGER (KG utilization efficiency in recommendation). We consider the scenarios where knowledge in a KG gets completely removed, randomly distorted and decreased, and also where recommendations are for cold-start users. Our extensive experiments on four commonly used datasets and a number of state-of-the-art KG-based RSs reveal that: to remove, randomly distort or decrease knowledge does not necessarily decrease recommendation accuracy, even for cold-start users. These findings inspire us to rethink how to better utilize knowledge from existing KGs, whereby we discuss and provide insights into what characteristics of datasets and KG-based RSs may help improve KG utilization efficiency.","sentences":["Recommender systems (RSs) are designed to provide personalized recommendations to users.","Recently, knowledge graphs (KGs) have been widely introduced in RSs to improve recommendation accuracy.","In this study, however, we demonstrate that RSs do not necessarily perform worse even if the KG is downgraded to the user-item interaction graph only (or removed).","We propose an evaluation framework KG4RecEval to systematically evaluate how much a KG contributes to the recommendation accuracy of a KG-based RS, using our defined metric KGER (KG utilization efficiency in recommendation).","We consider the scenarios where knowledge in a KG gets completely removed, randomly distorted and decreased, and also where recommendations are for cold-start users.","Our extensive experiments on four commonly used datasets and a number of state-of-the-art KG-based RSs reveal that: to remove, randomly distort or decrease knowledge does not necessarily decrease recommendation accuracy, even for cold-start users.","These findings inspire us to rethink how to better utilize knowledge from existing KGs, whereby we discuss and provide insights into what characteristics of datasets and KG-based RSs may help improve KG utilization efficiency."],"url":"http://arxiv.org/abs/2404.03164v1","category":"cs.IR"}
{"created":"2024-04-04 02:31:05","title":"Uncertainty in Language Models: Assessment through Rank-Calibration","abstract":"Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.","sentences":["Language Models (LMs) have shown promising performance in natural language generation.","However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs.","In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed.","However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$).","In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs.","Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average.","Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR).","The broad applicability and the granular interpretability of our methods are demonstrated empirically."],"url":"http://arxiv.org/abs/2404.03163v1","category":"cs.CL"}
{"created":"2024-04-04 02:16:07","title":"Simultaneous clustering and estimation of additive shape invariant models for recurrent event data","abstract":"Technological advancements have enabled the recording of spiking activities from large neuron ensembles, presenting an exciting yet challenging opportunity for statistical analysis. This project considers the challenges from a common type of neuroscience experiments, where randomized interventions are applied over the course of each trial. The objective is to identify groups of neurons with unique stimulation responses and estimate these responses. The observed data, however, comprise superpositions of neural responses to all stimuli, which is further complicated by varying response latencies across neurons. We introduce a novel additive shape invariant model that is capable of simultaneously accommodating multiple clusters, additive components, and unknown time-shifts. We establish conditions for the identifiability of model parameters, offering guidance for the design of future experiments. We examine the properties of the proposed algorithm through simulation studies, and apply the proposed method on neural data collected in mice.","sentences":["Technological advancements have enabled the recording of spiking activities from large neuron ensembles, presenting an exciting yet challenging opportunity for statistical analysis.","This project considers the challenges from a common type of neuroscience experiments, where randomized interventions are applied over the course of each trial.","The objective is to identify groups of neurons with unique stimulation responses and estimate these responses.","The observed data, however, comprise superpositions of neural responses to all stimuli, which is further complicated by varying response latencies across neurons.","We introduce a novel additive shape invariant model that is capable of simultaneously accommodating multiple clusters, additive components, and unknown time-shifts.","We establish conditions for the identifiability of model parameters, offering guidance for the design of future experiments.","We examine the properties of the proposed algorithm through simulation studies, and apply the proposed method on neural data collected in mice."],"url":"http://arxiv.org/abs/2404.03160v1","category":"stat.AP"}
{"created":"2024-04-04 02:05:38","title":"Age-of-Information-Aware Distributed Task Offloading and Resource Allocation in Mobile Edge Computing Networks","abstract":"The growth in artificial intelligence (AI) technology has attracted substantial interests in age-of-information (AoI)-aware task offloading of mobile edge computing (MEC)-namely, minimizing service latency. Additionally, the use of MEC systems poses an additional problem arising from limited battery resources of MDs. This paper tackles the pressing challenge of AoI-aware distributed task offloading optimization, where user association (UA), resource allocation (RA), full-task offloading, and battery of mobile devices (MDs) are jointly considered. In existing studies, joint optimization of overall task offloading and UA is seldom considered due to the complexity of combinatorial optimization problems, and in cases where it is considered, linear objective functions such as power consumption are adopted. Revolutionizing the realm of MEC, our objective includes all major components contributing to users' quality of experience, including AoI and energy consumption. To achieve this, we first formulate an NP-hard combinatorial problem, where the objective function comprises three elements: communication latency, computation latency, and battery usage. We derive a closed-form RA solution of the problem; next, we provide a distributed pricing-based UA solution. We simulate the proposed algorithm for various vision and language AI tasks. Our numerical results show that the proposed method Pareto-dominates baseline methods. More specifically, the results demonstrate that the proposed method can outperform baseline methods by 1.62 times smaller AoI with 41.2% less energy consumption.","sentences":["The growth in artificial intelligence (AI) technology has attracted substantial interests in age-of-information (AoI)-aware task offloading of mobile edge computing (MEC)-namely, minimizing service latency.","Additionally, the use of MEC systems poses an additional problem arising from limited battery resources of MDs.","This paper tackles the pressing challenge of AoI-aware distributed task offloading optimization, where user association (UA), resource allocation (RA), full-task offloading, and battery of mobile devices (MDs) are jointly considered.","In existing studies, joint optimization of overall task offloading and UA is seldom considered due to the complexity of combinatorial optimization problems, and in cases where it is considered, linear objective functions such as power consumption are adopted.","Revolutionizing the realm of MEC, our objective includes all major components contributing to users' quality of experience, including AoI and energy consumption.","To achieve this, we first formulate an NP-hard combinatorial problem, where the objective function comprises three elements: communication latency, computation latency, and battery usage.","We derive a closed-form RA solution of the problem; next, we provide a distributed pricing-based UA solution.","We simulate the proposed algorithm for various vision and language AI tasks.","Our numerical results show that the proposed method Pareto-dominates baseline methods.","More specifically, the results demonstrate that the proposed method can outperform baseline methods by 1.62 times smaller AoI with 41.2% less energy consumption."],"url":"http://arxiv.org/abs/2404.03154v1","category":"eess.SP"}
{"created":"2024-04-04 01:50:20","title":"NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA","abstract":"This paper presents our submission to the SemEval 2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure. We present two approaches to solving the task of legal answer validation, given an introduction to the case, a question and an answer candidate. Firstly, we fine-tuned pre-trained BERT-based models and found that models trained on domain knowledge perform better. Secondly, we performed few-shot prompting on GPT models and found that reformulating the answer validation task to be a multiple-choice QA task remarkably improves the performance of the model. Our best submission is a BERT-based model that achieved the 7th place out of 20.","sentences":["This paper presents our submission to the SemEval 2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure.","We present two approaches to solving the task of legal answer validation, given an introduction to the case, a question and an answer candidate.","Firstly, we fine-tuned pre-trained BERT-based models and found that models trained on domain knowledge perform better.","Secondly, we performed few-shot prompting on GPT models and found that reformulating the answer validation task to be a multiple-choice QA task remarkably improves the performance of the model.","Our best submission is a BERT-based model that achieved the 7th place out of 20."],"url":"http://arxiv.org/abs/2404.03150v1","category":"cs.CL"}
{"created":"2024-04-04 01:42:28","title":"Eigenpruning","abstract":"We introduce eigenpruning, a method that removes singular values from weight matrices in an LLM to improve its performance in a particular task. This method is inspired by interpretability methods designed to automatically find subnetworks of a model which solve a specific task. In our tests, the pruned model outperforms the original model by a large margin, while only requiring minimal computation to prune the weight matrices. In the case of a small synthetic task in integer multiplication, the Phi-2 model can improve its accuracy in the test set from 13.75% to 97.50%. Interestingly, these results seem to indicate the existence of a computation path that can solve the task very effectively, but it was not being used by the original model. Finally, we plan to open-source our implementation in the camera-ready version of our work.","sentences":["We introduce eigenpruning, a method that removes singular values from weight matrices in an LLM to improve its performance in a particular task.","This method is inspired by interpretability methods designed to automatically find subnetworks of a model which solve a specific task.","In our tests, the pruned model outperforms the original model by a large margin, while only requiring minimal computation to prune the weight matrices.","In the case of a small synthetic task in integer multiplication, the Phi-2 model can improve its accuracy in the test set from 13.75% to 97.50%.","Interestingly, these results seem to indicate the existence of a computation path that can solve the task very effectively, but it was not being used by the original model.","Finally, we plan to open-source our implementation in the camera-ready version of our work."],"url":"http://arxiv.org/abs/2404.03147v1","category":"cs.LG"}
{"created":"2024-04-04 00:58:19","title":"A Framework for Guided Motion Planning","abstract":"Randomized sampling based algorithms are widely used in robot motion planning due to the problem's intractability, and are experimentally effective on a wide range of problem instances. Most variants bias their sampling using various heuristics related to the known underlying structure of the search space. In this work, we formalize the intuitive notion of guided search by defining the concept of a guiding space. This new language encapsulates many seemingly distinct prior methods under the same framework, and allows us to reason about guidance, a previously obscured core contribution of different algorithms. We suggest an information theoretic method to evaluate guidance, which experimentally matches intuition when tested on known algorithms in a variety of environments. The language and evaluation of guidance suggests improvements to existing methods, and allows for simple hybrid algorithms that combine guidance from multiple sources.","sentences":["Randomized sampling based algorithms are widely used in robot motion planning due to the problem's intractability, and are experimentally effective on a wide range of problem instances.","Most variants bias their sampling using various heuristics related to the known underlying structure of the search space.","In this work, we formalize the intuitive notion of guided search by defining the concept of a guiding space.","This new language encapsulates many seemingly distinct prior methods under the same framework, and allows us to reason about guidance, a previously obscured core contribution of different algorithms.","We suggest an information theoretic method to evaluate guidance, which experimentally matches intuition when tested on known algorithms in a variety of environments.","The language and evaluation of guidance suggests improvements to existing methods, and allows for simple hybrid algorithms that combine guidance from multiple sources."],"url":"http://arxiv.org/abs/2404.03133v1","category":"cs.RO"}
{"created":"2024-04-04 00:50:05","title":"Semantic Compression with Information Lattice Learning","abstract":"Data-driven artificial intelligence (AI) techniques are becoming prominent for learning in support of data compression, but are focused on standard problems such as text compression. To instead address the emerging problem of semantic compression, we argue that the lattice theory of information is particularly expressive and mathematically precise in capturing notions of abstraction as a form of lossy semantic compression. As such, we demonstrate that a novel AI technique called information lattice learning, originally developed for knowledge discovery and creativity, is powerful for learning to compress in a semantically-meaningful way. The lattice structure further implies the optimality of group codes and the successive refinement property for progressive transmission.","sentences":["Data-driven artificial intelligence (AI) techniques are becoming prominent for learning in support of data compression, but are focused on standard problems such as text compression.","To instead address the emerging problem of semantic compression, we argue that the lattice theory of information is particularly expressive and mathematically precise in capturing notions of abstraction as a form of lossy semantic compression.","As such, we demonstrate that a novel AI technique called information lattice learning, originally developed for knowledge discovery and creativity, is powerful for learning to compress in a semantically-meaningful way.","The lattice structure further implies the optimality of group codes and the successive refinement property for progressive transmission."],"url":"http://arxiv.org/abs/2404.03131v1","category":"cs.IT"}
{"created":"2024-04-03 23:57:34","title":"LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models","abstract":"In the rapidly evolving landscape of artificial intelligence, multi-modal large language models are emerging as a significant area of interest. These models, which combine various forms of data input, are becoming increasingly popular. However, understanding their internal mechanisms remains a complex task. Numerous advancements have been made in the field of explainability tools and mechanisms, yet there is still much to explore. In this work, we present a novel interactive application aimed towards understanding the internal mechanisms of large vision-language models. Our interface is designed to enhance the interpretability of the image patches, which are instrumental in generating an answer, and assess the efficacy of the language model in grounding its output in the image. With our application, a user can systematically investigate the model and uncover system limitations, paving the way for enhancements in system capabilities. Finally, we present a case study of how our application can aid in understanding failure mechanisms in a popular large multi-modal model: LLaVA.","sentences":["In the rapidly evolving landscape of artificial intelligence, multi-modal large language models are emerging as a significant area of interest.","These models, which combine various forms of data input, are becoming increasingly popular.","However, understanding their internal mechanisms remains a complex task.","Numerous advancements have been made in the field of explainability tools and mechanisms, yet there is still much to explore.","In this work, we present a novel interactive application aimed towards understanding the internal mechanisms of large vision-language models.","Our interface is designed to enhance the interpretability of the image patches, which are instrumental in generating an answer, and assess the efficacy of the language model in grounding its output in the image.","With our application, a user can systematically investigate the model and uncover system limitations, paving the way for enhancements in system capabilities.","Finally, we present a case study of how our application can aid in understanding failure mechanisms in a popular large multi-modal model: LLaVA."],"url":"http://arxiv.org/abs/2404.03118v1","category":"cs.CV"}
{"created":"2024-04-03 23:33:56","title":"Testing the Effect of Code Documentation on Large Language Model Code Understanding","abstract":"Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding. However, little work has investigated how documentation and other code properties affect an LLM's ability to understand and generate code or documentation. We present an empirical analysis of how underlying properties of code or documentation can affect an LLM's capabilities. We show that providing an LLM with \"incorrect\" documentation can greatly hinder code understanding, while incomplete or missing documentation does not seem to significantly affect an LLM's ability to understand code.","sentences":["Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding.","However, little work has investigated how documentation and other code properties affect an LLM's ability to understand and generate code or documentation.","We present an empirical analysis of how underlying properties of code or documentation can affect an LLM's capabilities.","We show that providing an LLM with \"incorrect\" documentation can greatly hinder code understanding, while incomplete or missing documentation does not seem to significantly affect an LLM's ability to understand code."],"url":"http://arxiv.org/abs/2404.03114v1","category":"cs.SE"}
{"created":"2024-04-03 22:42:37","title":"Composite Bayesian Optimization In Function Spaces Using NEON -- Neural Epistemic Operator Networks","abstract":"Operator learning is a rising field of scientific computing where inputs or outputs of a machine learning model are functions defined in infinite-dimensional spaces. In this paper, we introduce NEON (Neural Epistemic Operator Networks), an architecture for generating predictions with uncertainty using a single operator network backbone, which presents orders of magnitude less trainable parameters than deep ensembles of comparable performance. We showcase the utility of this method for sequential decision-making by examining the problem of composite Bayesian Optimization (BO), where we aim to optimize a function $f=g\\circ h$, where $h:X\\to C(\\mathcal{Y},\\mathbb{R}^{d_s})$ is an unknown map which outputs elements of a function space, and $g: C(\\mathcal{Y},\\mathbb{R}^{d_s})\\to \\mathbb{R}$ is a known and cheap-to-compute functional. By comparing our approach to other state-of-the-art methods on toy and real world scenarios, we demonstrate that NEON achieves state-of-the-art performance while requiring orders of magnitude less trainable parameters.","sentences":["Operator learning is a rising field of scientific computing where inputs or outputs of a machine learning model are functions defined in infinite-dimensional spaces.","In this paper, we introduce NEON (Neural Epistemic Operator Networks), an architecture for generating predictions with uncertainty using a single operator network backbone, which presents orders of magnitude less trainable parameters than deep ensembles of comparable performance.","We showcase the utility of this method for sequential decision-making by examining the problem of composite Bayesian Optimization (BO), where we aim to optimize a function $f=g\\circ h$, where $h:X\\to C(\\mathcal{Y},\\mathbb{R}^{d_s})$ is an unknown map which outputs elements of a function space, and $g: C(\\mathcal{Y},\\mathbb{R}^{d_s})\\to \\mathbb{R}$ is a known and cheap-to-compute functional.","By comparing our approach to other state-of-the-art methods on toy and real world scenarios, we demonstrate that NEON achieves state-of-the-art performance while requiring orders of magnitude less trainable parameters."],"url":"http://arxiv.org/abs/2404.03099v1","category":"cs.LG"}
{"created":"2024-04-03 22:39:33","title":"Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales","abstract":"Saliency post-hoc explainability methods are important tools for understanding increasingly complex NLP models. While these methods can reflect the model's reasoning, they may not align with human intuition, making the explanations not plausible. In this work, we present a methodology for incorporating rationales, which are text annotations explaining human decisions, into text classification models. This incorporation enhances the plausibility of post-hoc explanations while preserving their faithfulness. Our approach is agnostic to model architectures and explainability methods. We introduce the rationales during model training by augmenting the standard cross-entropy loss with a novel loss function inspired by contrastive learning. By leveraging a multi-objective optimization algorithm, we explore the trade-off between the two loss functions and generate a Pareto-optimal frontier of models that balance performance and plausibility. Through extensive experiments involving diverse models, datasets, and explainability methods, we demonstrate that our approach significantly enhances the quality of model explanations without causing substantial (sometimes negligible) degradation in the original model's performance.","sentences":["Saliency post-hoc explainability methods are important tools for understanding increasingly complex NLP models.","While these methods can reflect the model's reasoning, they may not align with human intuition, making the explanations not plausible.","In this work, we present a methodology for incorporating rationales, which are text annotations explaining human decisions, into text classification models.","This incorporation enhances the plausibility of post-hoc explanations while preserving their faithfulness.","Our approach is agnostic to model architectures and explainability methods.","We introduce the rationales during model training by augmenting the standard cross-entropy loss with a novel loss function inspired by contrastive learning.","By leveraging a multi-objective optimization algorithm, we explore the trade-off between the two loss functions and generate a Pareto-optimal frontier of models that balance performance and plausibility.","Through extensive experiments involving diverse models, datasets, and explainability methods, we demonstrate that our approach significantly enhances the quality of model explanations without causing substantial (sometimes negligible) degradation in the original model's performance."],"url":"http://arxiv.org/abs/2404.03098v1","category":"cs.CL"}
{"created":"2024-04-03 22:03:28","title":"Robust Federated Learning for Wireless Networks: A Demonstration with Channel Estimation","abstract":"Federated learning (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application. Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention. In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various adversarial attacks or deployment tactics. In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through simulation.","sentences":["Federated learning (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application.","Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention.","In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various adversarial attacks or deployment tactics.","In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through simulation."],"url":"http://arxiv.org/abs/2404.03088v1","category":"cs.LG"}
{"created":"2024-04-03 21:55:44","title":"Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference","abstract":"On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria: a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.","sentences":["On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences.","However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power.","To help practitioners create efficient ML models, we designed and developed Talaria: a model visualization and optimization system.","Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics.","Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria."],"url":"http://arxiv.org/abs/2404.03085v1","category":"cs.HC"}
{"created":"2024-04-03 21:55:17","title":"Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience","abstract":"Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspiration from human cultural transmission and learning. It involves a teacher algorithm shaping the learning process of a learner algorithm by exposing it to controlled experiences. Despite its success, understanding the conditions under which TSCL is effective remains challenging. In this paper, we propose a data-centric perspective to analyze the underlying mechanics of the teacher-student interactions in TSCL. We leverage cooperative game theory to describe how the composition of the set of experiences presented by the teacher to the learner, as well as their order, influences the performance of the curriculum that is found by TSCL approaches. To do so, we demonstrate that for every TSCL problem, there exists an equivalent cooperative game, and several key components of the TSCL framework can be reinterpreted using game-theoretic principles. Through experiments covering supervised learning, reinforcement learning, and classical games, we estimate the cooperative values of experiences and use value-proportional curriculum mechanisms to construct curricula, even in cases where TSCL struggles. The framework and experimental setup we present in this work represent a novel foundation for a deeper exploration of TSCL, shedding light on its underlying mechanisms and providing insights into its broader applicability in machine learning.","sentences":["Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspiration from human cultural transmission and learning.","It involves a teacher algorithm shaping the learning process of a learner algorithm by exposing it to controlled experiences.","Despite its success, understanding the conditions under which TSCL is effective remains challenging.","In this paper, we propose a data-centric perspective to analyze the underlying mechanics of the teacher-student interactions in TSCL.","We leverage cooperative game theory to describe how the composition of the set of experiences presented by the teacher to the learner, as well as their order, influences the performance of the curriculum that is found by TSCL approaches.","To do so, we demonstrate that for every TSCL problem, there exists an equivalent cooperative game, and several key components of the TSCL framework can be reinterpreted using game-theoretic principles.","Through experiments covering supervised learning, reinforcement learning, and classical games, we estimate the cooperative values of experiences and use value-proportional curriculum mechanisms to construct curricula, even in cases where TSCL struggles.","The framework and experimental setup we present in this work represent a novel foundation for a deeper exploration of TSCL, shedding light on its underlying mechanisms and providing insights into its broader applicability in machine learning."],"url":"http://arxiv.org/abs/2404.03084v1","category":"cs.LG"}
{"created":"2024-04-03 21:46:14","title":"Construction of Functional Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model","abstract":"The convergence of materials science and artificial intelligence has unlocked new opportunities for gathering, analyzing, and generating novel materials sourced from extensive scientific literature. Despite the potential benefits, persistent challenges such as manual annotation, precise extraction, and traceability issues remain. Large language models have emerged as promising solutions to address these obstacles. This paper introduces Functional Materials Knowledge Graph (FMKG), a multidisciplinary materials science knowledge graph. Through the utilization of advanced natural language processing techniques, extracting millions of entities to form triples from a corpus comprising all high-quality research papers published in the last decade. It organizes unstructured information into nine distinct labels, covering Name, Formula, Acronym, Structure/Phase, Properties, Descriptor, Synthesis, Characterization Method, Application, and Domain, seamlessly integrating papers' Digital Object Identifiers. As the latest structured database for functional materials, FMKG acts as a powerful catalyst for expediting the development of functional materials and a fundation for building a more comprehensive material knowledge graph using full paper text. Furthermore, our research lays the groundwork for practical text-mining-based knowledge management systems, not only in intricate materials systems but also applicable to other specialized domains.","sentences":["The convergence of materials science and artificial intelligence has unlocked new opportunities for gathering, analyzing, and generating novel materials sourced from extensive scientific literature.","Despite the potential benefits, persistent challenges such as manual annotation, precise extraction, and traceability issues remain.","Large language models have emerged as promising solutions to address these obstacles.","This paper introduces Functional Materials Knowledge Graph (FMKG), a multidisciplinary materials science knowledge graph.","Through the utilization of advanced natural language processing techniques, extracting millions of entities to form triples from a corpus comprising all high-quality research papers published in the last decade.","It organizes unstructured information into nine distinct labels, covering Name, Formula, Acronym, Structure/Phase, Properties, Descriptor, Synthesis, Characterization Method, Application, and Domain, seamlessly integrating papers' Digital Object Identifiers.","As the latest structured database for functional materials, FMKG acts as a powerful catalyst for expediting the development of functional materials and a fundation for building a more comprehensive material knowledge graph using full paper text.","Furthermore, our research lays the groundwork for practical text-mining-based knowledge management systems, not only in intricate materials systems but also applicable to other specialized domains."],"url":"http://arxiv.org/abs/2404.03080v1","category":"cs.CL"}
{"created":"2024-04-03 21:26:40","title":"Human Mobility in the Metaverse","abstract":"The metaverse promises a shift in the way humans interact with each other, and with their digital and physical environments. The lack of geographical boundaries and travel costs in the metaverse prompts us to ask if the fundamental laws that govern human mobility in the physical world apply. We collected data on avatar movements, along with their network mobility extracted from NFT purchases. We find that despite the absence of commuting costs, an individuals inclination to explore new locations diminishes over time, limiting movement to a small fraction of the metaverse. We also find a lack of correlation between land prices and visitation, a deviation from the patterns characterizing the physical world. Finally, we identify the scaling laws that characterize meta mobility and show that we need to add preferential selection to the existing models to explain quantitative patterns of metaverse mobility. Our ability to predict the characteristics of the emerging meta mobility network implies that the laws governing human mobility are rooted in fundamental patterns of human dynamics, rather than the nature of space and cost of movement.","sentences":["The metaverse promises a shift in the way humans interact with each other, and with their digital and physical environments.","The lack of geographical boundaries and travel costs in the metaverse prompts us to ask if the fundamental laws that govern human mobility in the physical world apply.","We collected data on avatar movements, along with their network mobility extracted from NFT purchases.","We find that despite the absence of commuting costs, an individuals inclination to explore new locations diminishes over time, limiting movement to a small fraction of the metaverse.","We also find a lack of correlation between land prices and visitation, a deviation from the patterns characterizing the physical world.","Finally, we identify the scaling laws that characterize meta mobility and show that we need to add preferential selection to the existing models to explain quantitative patterns of metaverse mobility.","Our ability to predict the characteristics of the emerging meta mobility network implies that the laws governing human mobility are rooted in fundamental patterns of human dynamics, rather than the nature of space and cost of movement."],"url":"http://arxiv.org/abs/2404.03071v1","category":"cs.SI"}
{"created":"2024-04-03 21:16:19","title":"Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System","abstract":"Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.","sentences":["Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area.","To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations.","Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration.","For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations."],"url":"http://arxiv.org/abs/2404.03067v1","category":"cs.RO"}
{"created":"2024-04-03 21:04:54","title":"WebSPL: A Software Product Line for Web Applications","abstract":"Companies developing Web applications have faced an increasing demand for high-quality products with low cost and production time ever smaller. However, developing such applications is still considered a time-consuming and error-prone task, mainly due to the difficulty of promoting the reuse of features (or functionalities) and modules, and the heterogeneity of Web frameworks. Nowadays, companies must face ever-changing requirements. Software product lines emerged as an alternative to face this challenge by creating a collection of applications from a core of software assets. Despite the potential, the current literature lacks works that propose a product line for Web applications. This paper, therefore, presents WebSPL, a product line for Web applications that supports the main features found in Wed applications in real-world settings. The proposed WebSPL was evaluated by comparing it with a Web application developed based on a traditional approach. A case study that involves the development of two Web applications enabled data collection. Two Web applications were developed -- one with and another without the support of the proposed WebSPL. We compared these two applications using software design metrics, including complexity, size, duplicate lines, and technical debt. The initial results were encouraging and showed the potential for using WebSPL to support the development of Web applications.","sentences":["Companies developing Web applications have faced an increasing demand for high-quality products with low cost and production time ever smaller.","However, developing such applications is still considered a time-consuming and error-prone task, mainly due to the difficulty of promoting the reuse of features (or functionalities) and modules, and the heterogeneity of Web frameworks.","Nowadays, companies must face ever-changing requirements.","Software product lines emerged as an alternative to face this challenge by creating a collection of applications from a core of software assets.","Despite the potential, the current literature lacks works that propose a product line for Web applications.","This paper, therefore, presents WebSPL, a product line for Web applications that supports the main features found in Wed applications in real-world settings.","The proposed WebSPL was evaluated by comparing it with a Web application developed based on a traditional approach.","A case study that involves the development of two Web applications enabled data collection.","Two Web applications were developed -- one with and another without the support of the proposed WebSPL.","We compared these two applications using software design metrics, including complexity, size, duplicate lines, and technical debt.","The initial results were encouraging and showed the potential for using WebSPL to support the development of Web applications."],"url":"http://arxiv.org/abs/2404.03061v1","category":"cs.SE"}
{"created":"2024-04-03 20:50:48","title":"Automatic Extraction of Linguistic Description from Fuzzy Rule Base","abstract":"Neuro-fuzzy systems are a technique of explainable artificial intelligence (XAI). They elaborate knowledge models as a set of fuzzy rules. Fuzzy sets are crucial components of fuzzy rules. They are used to model linguistic terms. In this paper, we present an automatic extraction of fuzzy rules in the natural English language. Full implementation is available free from a public repository.","sentences":["Neuro-fuzzy systems are a technique of explainable artificial intelligence (XAI).","They elaborate knowledge models as a set of fuzzy rules.","Fuzzy sets are crucial components of fuzzy rules.","They are used to model linguistic terms.","In this paper, we present an automatic extraction of fuzzy rules in the natural English language.","Full implementation is available free from a public repository."],"url":"http://arxiv.org/abs/2404.03058v1","category":"cs.LG"}
{"created":"2024-04-03 20:38:22","title":"Data-Driven Goal Recognition Design for General Behavioral Agents","abstract":"Goal recognition design aims to make limited modifications to decision-making environments with the goal of making it easier to infer the goals of agents acting within those environments. Although various research efforts have been made in goal recognition design, existing approaches are computationally demanding and often assume that agents are (near-)optimal in their decision-making. To address these limitations, we introduce a data-driven approach to goal recognition design that can account for agents with general behavioral models. Following existing literature, we use worst-case distinctiveness ($\\textit{wcd}$) as a measure of the difficulty in inferring the goal of an agent in a decision-making environment. Our approach begins by training a machine learning model to predict the $\\textit{wcd}$ for a given environment and the agent behavior model. We then propose a gradient-based optimization framework that accommodates various constraints to optimize decision-making environments for enhanced goal recognition. Through extensive simulations, we demonstrate that our approach outperforms existing methods in reducing $\\textit{wcd}$ and enhancing runtime efficiency in conventional setups, and it also adapts to scenarios not previously covered in the literature, such as those involving flexible budget constraints, more complex environments, and suboptimal agent behavior. Moreover, we have conducted human-subject experiments which confirm that our method can create environments that facilitate efficient goal recognition from real-world human decision-makers.","sentences":["Goal recognition design aims to make limited modifications to decision-making environments with the goal of making it easier to infer the goals of agents acting within those environments.","Although various research efforts have been made in goal recognition design, existing approaches are computationally demanding and often assume that agents are (near-)optimal in their decision-making.","To address these limitations, we introduce a data-driven approach to goal recognition design that can account for agents with general behavioral models.","Following existing literature, we use worst-case distinctiveness ($\\textit{wcd}$) as a measure of the difficulty in inferring the goal of an agent in a decision-making environment.","Our approach begins by training a machine learning model to predict the $\\textit{wcd}$ for a given environment and the agent behavior model.","We then propose a gradient-based optimization framework that accommodates various constraints to optimize decision-making environments for enhanced goal recognition.","Through extensive simulations, we demonstrate that our approach outperforms existing methods in reducing $\\textit{wcd}$ and enhancing runtime efficiency in conventional setups, and it also adapts to scenarios not previously covered in the literature, such as those involving flexible budget constraints, more complex environments, and suboptimal agent behavior.","Moreover, we have conducted human-subject experiments which confirm that our method can create environments that facilitate efficient goal recognition from real-world human decision-makers."],"url":"http://arxiv.org/abs/2404.03054v1","category":"cs.AI"}
{"created":"2024-04-03 20:08:15","title":"The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies","abstract":"The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations. Developed via manual curation, with the additional assistance of large language models (LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies. The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain. The ontology is structured around six top-level branches: Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI.   AIO's development utilized the Ontology Development Kit (ODK) for its creation and maintenance, with its content being dynamically updated through AI-driven curation support. This approach not only ensures the ontology's relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies.   The ontology's utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research. The AIO ontology is open source and is available on GitHub (https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal (https://bioportal.bioontology.org/ontologies/AIO).","sentences":["The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations.","Developed via manual curation, with the additional assistance of large language models (LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies.","The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain.","The ontology is structured around six top-level branches: Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI.   ","AIO's development utilized the Ontology Development Kit (ODK) for its creation and maintenance, with its content being dynamically updated through AI-driven curation support.","This approach not only ensures the ontology's relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies.   ","The ontology's utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research.","The AIO ontology is open source and is available on GitHub (https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal (https://bioportal.bioontology.org/ontologies/AIO)."],"url":"http://arxiv.org/abs/2404.03044v1","category":"cs.LG"}
{"created":"2024-04-03 19:48:13","title":"Model-based Reinforcement Learning for Parameterized Action Spaces","abstract":"We propose a novel model-based reinforcement learning algorithm -- Dynamics Learning and predictive control with Parameterized Actions (DLPA) -- for Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control. We theoretically quantify the difference between the generated trajectory and the optimal trajectory during planning in terms of the value they achieved through the lens of Lipschitz Continuity. Our empirical results on several standard benchmarks show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods.","sentences":["We propose a novel model-based reinforcement learning algorithm -- Dynamics Learning and predictive control with Parameterized Actions (DLPA) -- for Parameterized Action Markov Decision Processes (PAMDPs).","The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control.","We theoretically quantify the difference between the generated trajectory and the optimal trajectory during planning in terms of the value they achieved through the lens of Lipschitz Continuity.","Our empirical results on several standard benchmarks show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods."],"url":"http://arxiv.org/abs/2404.03037v1","category":"cs.LG"}
{"created":"2024-04-03 19:23:18","title":"JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks","abstract":"With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.","sentences":["With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge.","In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs.","To explore this issue, we introduce JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks.","Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios.","Our evaluation of 10 open-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities.","Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs."],"url":"http://arxiv.org/abs/2404.03027v1","category":"cs.CR"}
{"created":"2024-04-03 19:21:24","title":"When Digital Twin Meets Generative AI: Intelligent Closed-Loop Network Management","abstract":"Generative artificial intelligence (GAI) and digital twin (DT) are advanced data processing and virtualization technologies to revolutionize communication networks. Thanks to the powerful data processing capabilities of GAI, integrating it into DT is a potential approach to construct an intelligent holistic virtualized network for better network management performance. To this end, we propose a GAI-driven DT (GDT) network architecture to enable intelligent closed-loop network management. In the architecture, various GAI models can empower DT status emulation, feature abstraction, and network decision-making. The interaction between GAI-based and model-based data processing can facilitate intelligent external and internal closed-loop network management. To further enhance network management performance, three potential approaches are proposed, i.e., model light-weighting, adaptive model selection, and data-model-driven network management. We present a case study pertaining to data-model-driven network management for the GDT network, followed by some open research issues.","sentences":["Generative artificial intelligence (GAI) and digital twin (DT) are advanced data processing and virtualization technologies to revolutionize communication networks.","Thanks to the powerful data processing capabilities of GAI, integrating it into DT is a potential approach to construct an intelligent holistic virtualized network for better network management performance.","To this end, we propose a GAI-driven DT (GDT) network architecture to enable intelligent closed-loop network management.","In the architecture, various GAI models can empower DT status emulation, feature abstraction, and network decision-making.","The interaction between GAI-based and model-based data processing can facilitate intelligent external and internal closed-loop network management.","To further enhance network management performance, three potential approaches are proposed, i.e., model light-weighting, adaptive model selection, and data-model-driven network management.","We present a case study pertaining to data-model-driven network management for the GDT network, followed by some open research issues."],"url":"http://arxiv.org/abs/2404.03025v1","category":"cs.NI"}
{"created":"2024-04-03 19:18:25","title":"Toward Safe Evolution of Artificial Intelligence (AI) based Conversational Agents to Support Adolescent Mental and Sexual Health Knowledge Discovery","abstract":"Following the recent release of various Artificial Intelligence (AI) based Conversation Agents (CAs), adolescents are increasingly using CAs for interactive knowledge discovery on sensitive topics, including mental and sexual health topics. Exploring such sensitive topics through online search has been an essential part of adolescent development, and CAs can support their knowledge discovery on such topics through human-like dialogues. Yet, unintended risks have been documented with adolescents' interactions with AI-based CAs, such as being exposed to inappropriate content, false information, and/or being given advice that is detrimental to their mental and physical well-being (e.g., to self-harm). In this position paper, we discuss the current landscape and opportunities for CAs to support adolescents' mental and sexual health knowledge discovery. We also discuss some of the challenges related to ensuring the safety of adolescents when interacting with CAs regarding sexual and mental health topics. We call for a discourse on how to set guardrails for the safe evolution of AI-based CAs for adolescents.","sentences":["Following the recent release of various Artificial Intelligence (AI) based Conversation Agents (CAs), adolescents are increasingly using CAs for interactive knowledge discovery on sensitive topics, including mental and sexual health topics.","Exploring such sensitive topics through online search has been an essential part of adolescent development, and CAs can support their knowledge discovery on such topics through human-like dialogues.","Yet, unintended risks have been documented with adolescents' interactions with AI-based CAs, such as being exposed to inappropriate content, false information, and/or being given advice that is detrimental to their mental and physical well-being (e.g., to self-harm).","In this position paper, we discuss the current landscape and opportunities for CAs to support adolescents' mental and sexual health knowledge discovery.","We also discuss some of the challenges related to ensuring the safety of adolescents when interacting with CAs regarding sexual and mental health topics.","We call for a discourse on how to set guardrails for the safe evolution of AI-based CAs for adolescents."],"url":"http://arxiv.org/abs/2404.03023v1","category":"cs.HC"}
{"created":"2024-04-03 19:14:45","title":"Blessing or curse? A survey on the Impact of Generative AI on Fake News","abstract":"Fake news significantly influence our society. They impact consumers, voters, and many other societal groups. While Fake News exist for a centuries, Generative AI brings fake news on a new level. It is now possible to automate the creation of masses of high-quality individually targeted Fake News. On the other end, Generative AI can also help detecting Fake News. Both fields are young but developing fast.   This survey provides a comprehensive examination of the research and practical use of Generative AI for Fake News detection and creation in 2024. Following the Structured Literature Survey approach, the paper synthesizes current results in the following topic clusters 1) enabling technologies, 2) creation of Fake News, 3) case study social media as most relevant distribution channel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.   The article also identifies current challenges and open issues.","sentences":["Fake news significantly influence our society.","They impact consumers, voters, and many other societal groups.","While Fake News exist for a centuries, Generative AI brings fake news on a new level.","It is now possible to automate the creation of masses of high-quality individually targeted Fake News.","On the other end, Generative AI can also help detecting Fake News.","Both fields are young but developing fast.   ","This survey provides a comprehensive examination of the research and practical use of Generative AI for Fake News detection and creation in 2024.","Following the Structured Literature Survey approach, the paper synthesizes current results in the following topic clusters 1) enabling technologies, 2) creation of Fake News, 3) case study social media as most relevant distribution channel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.   ","The article also identifies current challenges and open issues."],"url":"http://arxiv.org/abs/2404.03021v1","category":"cs.CL"}
{"created":"2024-04-03 18:48:45","title":"Transfer learning applications for anomaly detection in wind turbines","abstract":"Anomaly detection in wind turbines typically involves using normal behaviour models to detect faults early. However, training autoencoder models for each turbine is time-consuming and resource intensive. Thus, transfer learning becomes essential for wind turbines with limited data or applications with limited computational resources. This study examines how cross-turbine transfer learning can be applied to autoencoder-based anomaly detection. Here, autoencoders are combined with constant thresholds for the reconstruction error to determine if input data contains an anomaly. The models are initially trained on one year's worth of data from one or more source wind turbines. They are then fine-tuned using smaller amounts of data from another turbine. Three methods for fine-tuning are investigated: adjusting the entire autoencoder, only the decoder, or only the threshold of the model. The performance of the transfer learning models is compared to baseline models that were trained on one year's worth of data from the target wind turbine. The results of the tests conducted in this study indicate that models trained on data of multiple wind turbines do not improve the anomaly detection capability compared to models trained on data of one source wind turbine. In addition, modifying the model's threshold can lead to comparable or even superior performance compared to the baseline, whereas fine-tuning the decoder or autoencoder further enhances the models' performance.","sentences":["Anomaly detection in wind turbines typically involves using normal behaviour models to detect faults early.","However, training autoencoder models for each turbine is time-consuming and resource intensive.","Thus, transfer learning becomes essential for wind turbines with limited data or applications with limited computational resources.","This study examines how cross-turbine transfer learning can be applied to autoencoder-based anomaly detection.","Here, autoencoders are combined with constant thresholds for the reconstruction error to determine if input data contains an anomaly.","The models are initially trained on one year's worth of data from one or more source wind turbines.","They are then fine-tuned using smaller amounts of data from another turbine.","Three methods for fine-tuning are investigated: adjusting the entire autoencoder, only the decoder, or only the threshold of the model.","The performance of the transfer learning models is compared to baseline models that were trained on one year's worth of data from the target wind turbine.","The results of the tests conducted in this study indicate that models trained on data of multiple wind turbines do not improve the anomaly detection capability compared to models trained on data of one source wind turbine.","In addition, modifying the model's threshold can lead to comparable or even superior performance compared to the baseline, whereas fine-tuning the decoder or autoencoder further enhances the models' performance."],"url":"http://arxiv.org/abs/2404.03011v1","category":"cs.LG"}
{"created":"2024-04-03 18:41:53","title":"Validation of the DESI 2024 Ly$\u03b1$ forest BAO analysis using synthetic datasets","abstract":"The first year of data from the Dark Energy Spectroscopic Instrument (DESI) contains the largest set of Lyman-$\\alpha$ (Ly$\\alpha$) forest spectra ever observed. This data, collected in the DESI Data Release 1 (DR1) sample, has been used to measure the Baryon Acoustic Oscillation (BAO) feature at redshift $z=2.33$. In this work, we use a set of 150 synthetic realizations of DESI DR1 to validate the DESI 2024 Ly$\\alpha$ forest BAO measurement. The synthetic data sets are based on Gaussian random fields using the log-normal approximation. We produce realistic synthetic DESI spectra that include all major contaminants affecting the Ly$\\alpha$ forest. The synthetic data sets span a redshift range $1.8<z<3.8$, and are analysed using the same framework and pipeline used for the DESI 2024 Ly$\\alpha$ forest BAO measurement. To measure BAO, we use both the Ly$\\alpha$ auto-correlation and its cross-correlation with quasar positions. We use the mean of correlation functions from the set of DESI DR1 realizations to show that our model is able to recover unbiased measurements of the BAO position. We also fit each mock individually and study the population of BAO fits in order to validate BAO uncertainties and test our method for estimating the covariance matrix of the Ly$\\alpha$ forest correlation functions. Finally, we discuss the implications of our results and identify the needs for the next generation of Ly$\\alpha$ forest synthetic data sets, with the top priority being to simulate the effect of BAO broadening due to non-linear evolution.","sentences":["The first year of data from the Dark Energy Spectroscopic Instrument (DESI) contains the largest set of Lyman-$\\alpha$ (Ly$\\alpha$) forest spectra ever observed.","This data, collected in the DESI Data Release 1 (DR1) sample, has been used to measure the Baryon Acoustic Oscillation (BAO) feature at redshift $z=2.33$. In this work, we use a set of 150 synthetic realizations of DESI DR1 to validate the DESI 2024 Ly$\\alpha$ forest BAO measurement.","The synthetic data sets are based on Gaussian random fields using the log-normal approximation.","We produce realistic synthetic DESI spectra that include all major contaminants affecting the Ly$\\alpha$ forest.","The synthetic data sets span a redshift range $1.8<z<3.8$, and are analysed using the same framework and pipeline used for the DESI 2024 Ly$\\alpha$ forest BAO measurement.","To measure BAO, we use both the Ly$\\alpha$ auto-correlation and its cross-correlation with quasar positions.","We use the mean of correlation functions from the set of DESI DR1 realizations to show that our model is able to recover unbiased measurements of the BAO position.","We also fit each mock individually and study the population of BAO fits in order to validate BAO uncertainties and test our method for estimating the covariance matrix of the Ly$\\alpha$ forest correlation functions.","Finally, we discuss the implications of our results and identify the needs for the next generation of Ly$\\alpha$ forest synthetic data sets, with the top priority being to simulate the effect of BAO broadening due to non-linear evolution."],"url":"http://arxiv.org/abs/2404.03004v1","category":"astro-ph.CO"}
{"created":"2024-04-03 18:20:41","title":"ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale","abstract":"Generative image models have emerged as a promising technology to produce realistic images. Despite potential benefits, concerns grow about its misuse, particularly in generating deceptive images that could raise significant ethical, legal, and societal issues. Consequently, there is growing demand to empower users to effectively discern and comprehend patterns of AI-generated images. To this end, we developed ASAP, an interactive visualization system that automatically extracts distinct patterns of AI-generated images and allows users to interactively explore them via various views. To uncover fake patterns, ASAP introduces a novel image encoder, adapted from CLIP, which transforms images into compact \"distilled\" representations, enriched with information for differentiating authentic and fake images. These representations generate gradients that propagate back to the attention maps of CLIP's transformer block. This process quantifies the relative importance of each pixel to image authenticity or fakeness, exposing key deceptive patterns. ASAP enables the at scale interactive analysis of these patterns through multiple, coordinated visualizations. This includes a representation overview with innovative cell glyphs to aid in the exploration and qualitative evaluation of fake patterns across a vast array of images, as well as a pattern view that displays authenticity-indicating patterns in images and quantifies their impact. ASAP supports the analysis of cutting-edge generative models with the latest architectures, including GAN-based models like proGAN and diffusion models like the latent diffusion model. We demonstrate ASAP's usefulness through two usage scenarios using multiple fake image detection benchmark datasets, revealing its ability to identify and understand hidden patterns in AI-generated images, especially in detecting fake human faces produced by diffusion-based techniques.","sentences":["Generative image models have emerged as a promising technology to produce realistic images.","Despite potential benefits, concerns grow about its misuse, particularly in generating deceptive images that could raise significant ethical, legal, and societal issues.","Consequently, there is growing demand to empower users to effectively discern and comprehend patterns of AI-generated images.","To this end, we developed ASAP, an interactive visualization system that automatically extracts distinct patterns of AI-generated images and allows users to interactively explore them via various views.","To uncover fake patterns, ASAP introduces a novel image encoder, adapted from CLIP, which transforms images into compact \"distilled\" representations, enriched with information for differentiating authentic and fake images.","These representations generate gradients that propagate back to the attention maps of CLIP's transformer block.","This process quantifies the relative importance of each pixel to image authenticity or fakeness, exposing key deceptive patterns.","ASAP enables the at scale interactive analysis of these patterns through multiple, coordinated visualizations.","This includes a representation overview with innovative cell glyphs to aid in the exploration and qualitative evaluation of fake patterns across a vast array of images, as well as a pattern view that displays authenticity-indicating patterns in images and quantifies their impact.","ASAP supports the analysis of cutting-edge generative models with the latest architectures, including GAN-based models like proGAN and diffusion models like the latent diffusion model.","We demonstrate ASAP's usefulness through two usage scenarios using multiple fake image detection benchmark datasets, revealing its ability to identify and understand hidden patterns in AI-generated images, especially in detecting fake human faces produced by diffusion-based techniques."],"url":"http://arxiv.org/abs/2404.02990v1","category":"cs.CV"}
{"created":"2024-04-03 18:00:09","title":"Orbital obliquity of the young planet TOI-5398 b and the evolutionary history of the system","abstract":"Multi-planet systems exhibit remarkable architectural diversity. However, short-period giant planets are typically isolated. Compact systems like TOI-5398, with an outer close-orbit giant and an inner small-size planet, are rare among systems containing short-period giants. TOI-5398's unusual architecture coupled with its young age (650 $\\pm$ 150 Myr) make it a promising system for measuring the original obliquity between the orbital axis of the giant and the stellar spin axis in order to gain insight into its formation and orbital migration. We collected in-transit (plus suitable off-transit) observations of TOI-5398 b with HARPS-N at TNG on March 25, 2023, obtaining high-precision radial velocity time series that allowed us to measure the Rossiter-McLaughlin (RM) effect. By modelling the RM effect, we obtained a sky-projected obliquity of $\\lambda = 3.0^{+6.8}_{-4.2}$ deg for TOI-5398 b, consistent with the planet being aligned. With knowledge of the stellar rotation period, we estimated the true 3D obliquity, finding $\\psi = (13.2\\pm8.2)$ deg. Based on theoretical considerations, the orientation we measure is unaffected by tidal effects, offering a direct diagnostic for understanding the formation path of this planetary system. The orbital characteristics of TOI-5398, with its compact architecture, eccentricity consistent with circular orbits, and hints of orbital alignment, appear more compatible with the disc-driven migration scenario. TOI-5398, with its relative youth (compared with similar compact systems) and exceptional suitability for transmission spectroscopy studies, presents an outstanding opportunity to establish a benchmark for exploring the disc-driven migration model.","sentences":["Multi-planet systems exhibit remarkable architectural diversity.","However, short-period giant planets are typically isolated.","Compact systems like TOI-5398, with an outer close-orbit giant and an inner small-size planet, are rare among systems containing short-period giants.","TOI-5398's unusual architecture coupled with its young age (650 $\\pm$ 150 Myr) make it a promising system for measuring the original obliquity between the orbital axis of the giant and the stellar spin axis in order to gain insight into its formation and orbital migration.","We collected in-transit (plus suitable off-transit) observations of TOI-5398 b with HARPS-N at TNG on March 25, 2023, obtaining high-precision radial velocity time series that allowed us to measure the Rossiter-McLaughlin (RM) effect.","By modelling the RM effect, we obtained a sky-projected obliquity of $\\lambda = 3.0^{+6.8}_{-4.2}$ deg for TOI-5398 b, consistent with the planet being aligned.","With knowledge of the stellar rotation period, we estimated the true 3D obliquity, finding $\\psi = (13.2\\pm8.2)$ deg.","Based on theoretical considerations, the orientation we measure is unaffected by tidal effects, offering a direct diagnostic for understanding the formation path of this planetary system.","The orbital characteristics of TOI-5398, with its compact architecture, eccentricity consistent with circular orbits, and hints of orbital alignment, appear more compatible with the disc-driven migration scenario.","TOI-5398, with its relative youth (compared with similar compact systems) and exceptional suitability for transmission spectroscopy studies, presents an outstanding opportunity to establish a benchmark for exploring the disc-driven migration model."],"url":"http://arxiv.org/abs/2404.02969v1","category":"astro-ph.EP"}
{"created":"2024-04-03 18:00:00","title":"Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections","abstract":"In recent years there has been increased interest in understanding the interplay between deep generative models (DGMs) and the manifold hypothesis. Research in this area focuses on understanding the reasons why commonly-used DGMs succeed or fail at learning distributions supported on unknown low-dimensional manifolds, as well as developing new models explicitly designed to account for manifold-supported data. This manifold lens provides both clarity as to why some DGMs (e.g. diffusion models and some generative adversarial networks) empirically surpass others (e.g. likelihood-based models such as variational autoencoders, normalizing flows, or energy-based models) at sample generation, and guidance for devising more performant DGMs. We carry out the first survey of DGMs viewed through this lens, making two novel contributions along the way. First, we formally establish that numerical instability of high-dimensional likelihoods is unavoidable when modelling low-dimensional data. We then show that DGMs on learned representations of autoencoders can be interpreted as approximately minimizing Wasserstein distance: this result, which applies to latent diffusion models, helps justify their outstanding empirical results. The manifold lens provides a rich perspective from which to understand DGMs, which we aim to make more accessible and widespread.","sentences":["In recent years there has been increased interest in understanding the interplay between deep generative models (DGMs) and the manifold hypothesis.","Research in this area focuses on understanding the reasons why commonly-used DGMs succeed or fail at learning distributions supported on unknown low-dimensional manifolds, as well as developing new models explicitly designed to account for manifold-supported data.","This manifold lens provides both clarity as to why some DGMs (e.g. diffusion models and some generative adversarial networks) empirically surpass others (e.g. likelihood-based models such as variational autoencoders, normalizing flows, or energy-based models) at sample generation, and guidance for devising more performant DGMs.","We carry out the first survey of DGMs viewed through this lens, making two novel contributions along the way.","First, we formally establish that numerical instability of high-dimensional likelihoods is unavoidable when modelling low-dimensional data.","We then show that DGMs on learned representations of autoencoders can be interpreted as approximately minimizing Wasserstein distance: this result, which applies to latent diffusion models, helps justify their outstanding empirical results.","The manifold lens provides a rich perspective from which to understand DGMs, which we aim to make more accessible and widespread."],"url":"http://arxiv.org/abs/2404.02954v1","category":"cs.LG"}
{"created":"2024-04-03 17:56:28","title":"The SaTML '24 CNN Interpretability Competition: New Innovations for Concept-Level Interpretability","abstract":"Interpretability techniques are valuable for helping humans understand and oversee AI systems. The SaTML 2024 CNN Interpretability Competition solicited novel methods for studying convolutional neural networks (CNNs) at the ImageNet scale. The objective of the competition was to help human crowd-workers identify trojans in CNNs. This report showcases the methods and results of four featured competition entries. It remains challenging to help humans reliably diagnose trojans via interpretability tools. However, the competition's entries have contributed new techniques and set a new record on the benchmark from Casper et al., 2023.","sentences":["Interpretability techniques are valuable for helping humans understand and oversee AI systems.","The SaTML 2024 CNN Interpretability Competition solicited novel methods for studying convolutional neural networks (CNNs) at the ImageNet scale.","The objective of the competition was to help human crowd-workers identify trojans in CNNs.","This report showcases the methods and results of four featured competition entries.","It remains challenging to help humans reliably diagnose trojans via interpretability tools.","However, the competition's entries have contributed new techniques and set a new record on the benchmark from Casper et al., 2023."],"url":"http://arxiv.org/abs/2404.02949v1","category":"cs.LG"}
{"created":"2024-04-03 15:06:43","title":"PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models","abstract":"As the parameters of LLMs expand, the computational cost of fine-tuning the entire model becomes prohibitive. To address this challenge, we introduce a PEFT method, Principal Singular values and Singular vectors Adaptation (PiSSA), which optimizes a significantly reduced parameter space while achieving or surpassing the performance of full-parameter fine-tuning. PiSSA is inspired by Intrinsic SAID, which suggests that pre-trained, over-parametrized models inhabit a space of low intrinsic dimension. Consequently, PiSSA represents a matrix W within the model by the product of two trainable matrices A and B, plus a residual matrix $W^{res}$ for error correction. SVD is employed to factorize W, and the principal singular values and vectors of W are utilized to initialize A and B. The residual singular values and vectors initialize the residual matrix $W^{res}$, which keeps frozen during fine-tuning. Notably, PiSSA shares the same architecture with LoRA. However, LoRA approximates Delta W through the product of two matrices, A, initialized with Gaussian noise, and B, initialized with zeros, while PiSSA initializes A and B with principal singular values and vectors of the original matrix W. PiSSA can better approximate the outcomes of full-parameter fine-tuning at the beginning by changing the essential parts while freezing the \"noisy\" parts. In comparison, LoRA freezes the original matrix and updates the \"noise\". This distinction enables PiSSA to convergence much faster than LoRA and also achieve better performance in the end. Due to the same architecture, PiSSA inherits many of LoRA's advantages, such as parameter efficiency and compatibility with quantization. Leveraging a fast SVD method, the initialization of PiSSA takes only a few seconds, inducing negligible cost of switching LoRA to PiSSA.","sentences":["As the parameters of LLMs expand, the computational cost of fine-tuning the entire model becomes prohibitive.","To address this challenge, we introduce a PEFT method, Principal Singular values and Singular vectors Adaptation (PiSSA), which optimizes a significantly reduced parameter space while achieving or surpassing the performance of full-parameter fine-tuning.","PiSSA is inspired by Intrinsic SAID, which suggests that pre-trained, over-parametrized models inhabit a space of low intrinsic dimension.","Consequently, PiSSA represents a matrix W within the model by the product of two trainable matrices A and B, plus a residual matrix $W^{res}$ for error correction.","SVD is employed to factorize W, and the principal singular values and vectors of W are utilized to initialize A and","B. The residual singular values and vectors initialize the residual matrix $W^{res}$, which keeps frozen during fine-tuning.","Notably, PiSSA shares the same architecture with LoRA.","However, LoRA approximates Delta W through the product of two matrices, A, initialized with Gaussian noise, and B, initialized with zeros, while PiSSA initializes A and B with principal singular values and vectors of the original matrix W. PiSSA can better approximate the outcomes of full-parameter fine-tuning at the beginning by changing the essential parts while freezing the \"noisy\" parts.","In comparison, LoRA freezes the original matrix and updates the \"noise\".","This distinction enables PiSSA to convergence much faster than LoRA and also achieve better performance in the end.","Due to the same architecture, PiSSA inherits many of LoRA's advantages, such as parameter efficiency and compatibility with quantization.","Leveraging a fast SVD method, the initialization of PiSSA takes only a few seconds, inducing negligible cost of switching LoRA to PiSSA."],"url":"http://arxiv.org/abs/2404.02948v1","category":"cs.LG"}
{"created":"2024-04-03 15:06:09","title":"DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization","abstract":"The imperative to deploy Deep Neural Network (DNN) models on resource-constrained edge devices, spurred by privacy concerns, has become increasingly apparent. To facilitate the transition from cloud to edge computing, this paper introduces a technique that effectively reduces the memory footprint of DNNs, accommodating the limitations of resource-constrained edge devices while preserving model accuracy. Our proposed technique, named Post-Training Intra-Layer Multi-Precision Quantization (PTILMPQ), employs a post-training quantization approach, eliminating the need for extensive training data. By estimating the importance of layers and channels within the network, the proposed method enables precise bit allocation throughout the quantization process. Experimental results demonstrate that PTILMPQ offers a promising solution for deploying DNNs on edge devices with restricted memory resources. For instance, in the case of ResNet50, it achieves an accuracy of 74.57\\% with a memory footprint of 9.5 MB, representing a 25.49\\% reduction compared to previous similar methods, with only a minor 1.08\\% decrease in accuracy.","sentences":["The imperative to deploy Deep Neural Network (DNN) models on resource-constrained edge devices, spurred by privacy concerns, has become increasingly apparent.","To facilitate the transition from cloud to edge computing, this paper introduces a technique that effectively reduces the memory footprint of DNNs, accommodating the limitations of resource-constrained edge devices while preserving model accuracy.","Our proposed technique, named Post-Training Intra-Layer Multi-Precision Quantization (PTILMPQ), employs a post-training quantization approach, eliminating the need for extensive training data.","By estimating the importance of layers and channels within the network, the proposed method enables precise bit allocation throughout the quantization process.","Experimental results demonstrate that PTILMPQ offers a promising solution for deploying DNNs on edge devices with restricted memory resources.","For instance, in the case of ResNet50, it achieves an accuracy of 74.57\\% with a memory footprint of 9.5 MB, representing a 25.49\\% reduction compared to previous similar methods, with only a minor 1.08\\% decrease in accuracy."],"url":"http://arxiv.org/abs/2404.02947v1","category":"cs.LG"}
{"created":"2024-04-03 14:14:08","title":"Optimizing the Deployment of Tiny Transformers on Low-Power MCUs","abstract":"Transformer networks are rapidly becoming SotA in many fields, such as NLP and CV. Similarly to CNN, there is a strong push for deploying Transformer models at the extreme edge, ultimately fitting the tiny power budget and memory footprint of MCUs. However, the early approaches in this direction are mostly ad-hoc, platform, and model-specific. This work aims to enable and optimize the flexible, multi-platform deployment of encoder Tiny Transformers on commercial MCUs. We propose a complete framework to perform end-to-end deployment of Transformer models onto single and multi-core MCUs. Our framework provides an optimized library of kernels to maximize data reuse and avoid unnecessary data marshaling operations into the crucial attention block. A novel MHSA inference schedule, named Fused-Weight Self-Attention, is introduced, fusing the linear projection weights offline to further reduce the number of operations and parameters. Furthermore, to mitigate the memory peak reached by the computation of the attention map, we present a Depth-First Tiling scheme for MHSA. We evaluate our framework on three different MCU classes exploiting ARM and RISC-V ISA, namely the STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2). We reach an average of 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN (ARM) and PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA depth-first tiling scheme reduces the memory peak by up to 6.19x, while the fused-weight attention can reduce the runtime by 1.53x, and number of parameters by 25%. We report significant improvements across several Tiny Transformers: for instance, when executing a transformer block for the task of radar-based hand-gesture recognition on GAP9, we achieve a latency of 0.14ms and energy consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN library on the same platform.","sentences":["Transformer networks are rapidly becoming SotA in many fields, such as NLP and CV.","Similarly to CNN, there is a strong push for deploying Transformer models at the extreme edge, ultimately fitting the tiny power budget and memory footprint of MCUs.","However, the early approaches in this direction are mostly ad-hoc, platform, and model-specific.","This work aims to enable and optimize the flexible, multi-platform deployment of encoder Tiny Transformers on commercial MCUs.","We propose a complete framework to perform end-to-end deployment of Transformer models onto single and multi-core MCUs.","Our framework provides an optimized library of kernels to maximize data reuse and avoid unnecessary data marshaling operations into the crucial attention block.","A novel MHSA inference schedule, named Fused-Weight Self-Attention, is introduced, fusing the linear projection weights offline to further reduce the number of operations and parameters.","Furthermore, to mitigate the memory peak reached by the computation of the attention map, we present a Depth-First Tiling scheme for MHSA.","We evaluate our framework on three different MCU classes exploiting ARM and RISC-V ISA, namely the STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2).","We reach an average of 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN (ARM) and PULP-NN (RISC-V), respectively.","Moreover, we show that our MHSA depth-first tiling scheme reduces the memory peak by up to 6.19x, while the fused-weight attention can reduce the runtime by 1.53x, and number of parameters by 25%.","We report significant improvements across several Tiny Transformers: for instance, when executing a transformer block for the task of radar-based hand-gesture recognition on GAP9, we achieve a latency of 0.14ms and energy consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN library on the same platform."],"url":"http://arxiv.org/abs/2404.02945v1","category":"cs.LG"}
{"created":"2024-04-03 14:07:02","title":"AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs","abstract":"Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training. The analysis of experts' vs. non-experts' annotations confirms theoretical findings in the social science literature.","sentences":["Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science.","Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible.","While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred.","In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post.","Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency.","We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score.","We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training.","The analysis of experts' vs. non-experts' annotations confirms theoretical findings in the social science literature."],"url":"http://arxiv.org/abs/2404.02761v2","category":"cs.CL"}
{"created":"2024-04-03 13:32:44","title":"Foundation Models for Structural Health Monitoring","abstract":"Structural Health Monitoring (SHM) is a critical task for ensuring the safety and reliability of civil infrastructures, typically realized on bridges and viaducts by means of vibration monitoring. In this paper, we propose for the first time the use of Transformer neural networks, with a Masked Auto-Encoder architecture, as Foundation Models for SHM. We demonstrate the ability of these models to learn generalizable representations from multiple large datasets through self-supervised pre-training, which, coupled with task-specific fine-tuning, allows them to outperform state-of-the-art traditional methods on diverse tasks, including Anomaly Detection (AD) and Traffic Load Estimation (TLE). We then extensively explore model size versus accuracy trade-offs and experiment with Knowledge Distillation (KD) to improve the performance of smaller Transformers, enabling their embedding directly into the SHM edge nodes. We showcase the effectiveness of our foundation models using data from three operational viaducts. For AD, we achieve a near-perfect 99.9% accuracy with a monitoring time span of just 15 windows. In contrast, a state-of-the-art method based on Principal Component Analysis (PCA) obtains its first good result (95.03% accuracy) only considering 120 windows. On two different TLE tasks, our models obtain state-of-the-art performance on multiple evaluation metrics (R$^2$ score, MAE% and MSE%). On the first benchmark, we achieve an R$^2$ score of 0.97 and 0.85 for light and heavy vehicle traffic, respectively, while the best previous approach stops at 0.91 and 0.84. On the second one, we achieve an R$^2$ score of 0.54 versus the 0.10 of the best existing method.","sentences":["Structural Health Monitoring (SHM) is a critical task for ensuring the safety and reliability of civil infrastructures, typically realized on bridges and viaducts by means of vibration monitoring.","In this paper, we propose for the first time the use of Transformer neural networks, with a Masked Auto-Encoder architecture, as Foundation Models for SHM.","We demonstrate the ability of these models to learn generalizable representations from multiple large datasets through self-supervised pre-training, which, coupled with task-specific fine-tuning, allows them to outperform state-of-the-art traditional methods on diverse tasks, including Anomaly Detection (AD) and Traffic Load Estimation (TLE).","We then extensively explore model size versus accuracy trade-offs and experiment with Knowledge Distillation (KD) to improve the performance of smaller Transformers, enabling their embedding directly into the SHM edge nodes.","We showcase the effectiveness of our foundation models using data from three operational viaducts.","For AD, we achieve a near-perfect 99.9% accuracy with a monitoring time span of just 15 windows.","In contrast, a state-of-the-art method based on Principal Component Analysis (PCA) obtains its first good result (95.03% accuracy) only considering 120 windows.","On two different TLE tasks, our models obtain state-of-the-art performance on multiple evaluation metrics (R$^2$ score, MAE% and MSE%).","On the first benchmark, we achieve an R$^2$ score of 0.97 and 0.85 for light and heavy vehicle traffic, respectively, while the best previous approach stops at 0.91 and 0.84.","On the second one, we achieve an R$^2$ score of 0.54 versus the 0.10 of the best existing method."],"url":"http://arxiv.org/abs/2404.02944v1","category":"cs.LG"}
{"created":"2024-04-03 13:31:49","title":"Learning in Convolutional Neural Networks Accelerated by Transfer Entropy","abstract":"Recently, there is a growing interest in applying Transfer Entropy (TE) in quantifying the effective connectivity between artificial neurons. In a feedforward network, the TE can be used to quantify the relationships between neuron output pairs located in different layers. Our focus is on how to include the TE in the learning mechanisms of a Convolutional Neural Network (CNN) architecture. We introduce a novel training mechanism for CNN architectures which integrates the TE feedback connections. Adding the TE feedback parameter accelerates the training process, as fewer epochs are needed. On the flip side, it adds computational overhead to each epoch. According to our experiments on CNN classifiers, to achieve a reasonable computational overhead--accuracy trade-off, it is efficient to consider only the inter-neural information transfer of a random subset of the neuron pairs from the last two fully connected layers. The TE acts as a smoothing factor, generating stability and becoming active only periodically, not after processing each input sample. Therefore, we can consider the TE is in our model a slowly changing meta-parameter.","sentences":["Recently, there is a growing interest in applying Transfer Entropy (TE) in quantifying the effective connectivity between artificial neurons.","In a feedforward network, the TE can be used to quantify the relationships between neuron output pairs located in different layers.","Our focus is on how to include the TE in the learning mechanisms of a Convolutional Neural Network (CNN) architecture.","We introduce a novel training mechanism for CNN architectures which integrates the TE feedback connections.","Adding the TE feedback parameter accelerates the training process, as fewer epochs are needed.","On the flip side, it adds computational overhead to each epoch.","According to our experiments on CNN classifiers, to achieve a reasonable computational overhead--accuracy trade-off, it is efficient to consider only the inter-neural information transfer of a random subset of the neuron pairs from the last two fully connected layers.","The TE acts as a smoothing factor, generating stability and becoming active only periodically, not after processing each input sample.","Therefore, we can consider the TE is in our model a slowly changing meta-parameter."],"url":"http://arxiv.org/abs/2404.02943v1","category":"cs.LG"}
{"created":"2024-04-03 13:09:31","title":"Measurement of differential ZZ+jets production cross sections in pp collisions at $\\sqrt{s}$ = 13 TeV","abstract":"Diboson production in association with jets is studied in the fully leptonic final states, pp $\\to$ (Z$\\gamma^*$)(Z/$\\gamma^*$)+jets $\\to$ 2$\\ell$2$\\ell'$+jets, ($\\ell,\\ell'$ = e or $\\mu$) in proton-proton collisions at a center-of-mass energy of 13 TeV. The data sample corresponds to an integrated luminosity of 138 fb$^{-1}$ collected with the CMS detector at the LHC. Differential distributions and normalized differential cross sections are measured as a function of jet multiplicity, transverse momentum $p_\\mathrm{T}$, pseudorapidity $\\eta$, invariant mass and $\\Delta\\eta$ of the highest-$p_\\mathrm{T}$ and second-highest-$p_\\mathrm{T}$ jets, and as a function of invariant mass of the four-lepton system for events with various jet multiplicities. These differential cross sections are compared with theoretical predictions that mostly agree with the experimental data. However, in a few regions we observe discrepancies between the predicted and measured values. Further improvement of the predictions is required to describe the ZZ+jets production in the whole phase space.","sentences":["Diboson production in association with jets is studied in the fully leptonic final states, pp $\\to$ (Z$\\gamma^*$)(Z/$\\gamma^*$)+jets $\\to$ 2$\\ell$2$\\ell'$+jets, ($\\ell,\\ell'$ = e or $\\mu$) in proton-proton collisions at a center-of-mass energy of 13 TeV.","The data sample corresponds to an integrated luminosity of 138 fb$^{-1}$ collected with the CMS detector at the LHC.","Differential distributions and normalized differential cross sections are measured as a function of jet multiplicity, transverse momentum $p_\\mathrm{T}$, pseudorapidity $\\eta$, invariant mass and $\\Delta\\eta$ of the highest-$p_\\mathrm{T}$ and second-highest-$p_\\mathrm{T}$ jets, and as a function of invariant mass of the four-lepton system for events with various jet multiplicities.","These differential cross sections are compared with theoretical predictions that mostly agree with the experimental data.","However, in a few regions we observe discrepancies between the predicted and measured values.","Further improvement of the predictions is required to describe the ZZ+jets production in the whole phase space."],"url":"http://arxiv.org/abs/2404.02711v1","category":"hep-ex"}
{"created":"2024-04-03 12:38:12","title":"Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles","abstract":"Understanding the decisions of tree-based ensembles and their relationships is pivotal for machine learning model interpretation. Recent attempts to mitigate the human-in-the-loop interpretation challenge have explored the extraction of the decision structure underlying the model taking advantage of graph simplification and path emphasis. However, while these efforts enhance the visualisation experience, they may either result in a visually complex representation or compromise the interpretability of the original ensemble model. In addressing this challenge, especially in complex scenarios, we introduce the Decision Predicate Graph (DPG) as a model-agnostic tool to provide a global interpretation of the model. DPG is a graph structure that captures the tree-based ensemble model and learned dataset details, preserving the relations among features, logical decisions, and predictions towards emphasising insightful points. Leveraging well-known graph theory concepts, such as the notions of centrality and community, DPG offers additional quantitative insights into the model, complementing visualisation techniques, expanding the problem space descriptions, and offering diverse possibilities for extensions. Empirical experiments demonstrate the potential of DPG in addressing traditional benchmarks and complex classification scenarios.","sentences":["Understanding the decisions of tree-based ensembles and their relationships is pivotal for machine learning model interpretation.","Recent attempts to mitigate the human-in-the-loop interpretation challenge have explored the extraction of the decision structure underlying the model taking advantage of graph simplification and path emphasis.","However, while these efforts enhance the visualisation experience, they may either result in a visually complex representation or compromise the interpretability of the original ensemble model.","In addressing this challenge, especially in complex scenarios, we introduce the Decision Predicate Graph (DPG) as a model-agnostic tool to provide a global interpretation of the model.","DPG is a graph structure that captures the tree-based ensemble model and learned dataset details, preserving the relations among features, logical decisions, and predictions towards emphasising insightful points.","Leveraging well-known graph theory concepts, such as the notions of centrality and community, DPG offers additional quantitative insights into the model, complementing visualisation techniques, expanding the problem space descriptions, and offering diverse possibilities for extensions.","Empirical experiments demonstrate the potential of DPG in addressing traditional benchmarks and complex classification scenarios."],"url":"http://arxiv.org/abs/2404.02942v1","category":"cs.LG"}
