{"created":"2024-05-30 17:59:54","title":"Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image","abstract":"In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability. Previous methods based on Score Distillation Sampling (SDS) can produce diversified 3D results by distilling 3D knowledge from large 2D diffusion models, but they usually suffer from long per-case optimization time with inconsistent issues. Recent works address the problem and generate better 3D results either by finetuning a multi-view diffusion model or training a fast feed-forward model. However, they still lack intricate textures and complex geometries due to inconsistency and limited generated resolution. To simultaneously achieve high fidelity, consistency, and efficiency in single image-to-3D, we propose a novel framework Unique3D that includes a multi-view diffusion model with a corresponding normal diffusion model to generate multi-view images with their normal maps, a multi-level upscale process to progressively improve the resolution of generated orthographic multi-views, as well as an instant and consistent mesh reconstruction algorithm called ISOMER, which fully integrates the color and geometric priors into mesh results. Extensive experiments demonstrate that our Unique3D significantly outperforms other image-to-3D baselines in terms of geometric and textural details.","sentences":["In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.","Previous methods based on Score Distillation Sampling (SDS) can produce diversified 3D results by distilling 3D knowledge from large 2D diffusion models, but they usually suffer from long per-case optimization time with inconsistent issues.","Recent works address the problem and generate better 3D results either by finetuning a multi-view diffusion model or training a fast feed-forward model.","However, they still lack intricate textures and complex geometries due to inconsistency and limited generated resolution.","To simultaneously achieve high fidelity, consistency, and efficiency in single image-to-3D, we propose a novel framework Unique3D that includes a multi-view diffusion model with a corresponding normal diffusion model to generate multi-view images with their normal maps, a multi-level upscale process to progressively improve the resolution of generated orthographic multi-views, as well as an instant and consistent mesh reconstruction algorithm called ISOMER, which fully integrates the color and geometric priors into mesh results.","Extensive experiments demonstrate that our Unique3D significantly outperforms other image-to-3D baselines in terms of geometric and textural details."],"url":"http://arxiv.org/abs/2405.20343v1","category":"cs.CV"}
{"created":"2024-05-30 17:59:47","title":"Visual Perception by Large Language Model's Weights","abstract":"Existing Multimodal Large Language Models (MLLMs) follow the paradigm that perceives visual information by aligning visual features with the input space of Large Language Models (LLMs), and concatenating visual tokens with text tokens to form a unified sequence input for LLMs. These methods demonstrate promising results on various vision-language tasks but are limited by the high computational effort due to the extended input sequence resulting from the involvement of visual tokens. In this paper, instead of input space alignment, we propose a novel parameter space alignment paradigm that represents visual information as model weights. For each input image, we use a vision encoder to extract visual features, convert features into perceptual weights, and merge the perceptual weights with LLM's weights. In this way, the input of LLM does not require visual tokens, which reduces the length of the input sequence and greatly improves efficiency. Following this paradigm, we propose VLoRA with the perceptual weights generator. The perceptual weights generator is designed to convert visual features to perceptual weights with low-rank property, exhibiting a form similar to LoRA. The experimental results show that our VLoRA achieves comparable performance on various benchmarks for MLLMs, while significantly reducing the computational costs for both training and inference. The code and models will be made open-source.","sentences":["Existing Multimodal Large Language Models (MLLMs) follow the paradigm that perceives visual information by aligning visual features with the input space of Large Language Models (LLMs), and concatenating visual tokens with text tokens to form a unified sequence input for LLMs.","These methods demonstrate promising results on various vision-language tasks but are limited by the high computational effort due to the extended input sequence resulting from the involvement of visual tokens.","In this paper, instead of input space alignment, we propose a novel parameter space alignment paradigm that represents visual information as model weights.","For each input image, we use a vision encoder to extract visual features, convert features into perceptual weights, and merge the perceptual weights with LLM's weights.","In this way, the input of LLM does not require visual tokens, which reduces the length of the input sequence and greatly improves efficiency.","Following this paradigm, we propose VLoRA with the perceptual weights generator.","The perceptual weights generator is designed to convert visual features to perceptual weights with low-rank property, exhibiting a form similar to LoRA.","The experimental results show that our VLoRA achieves comparable performance on various benchmarks for MLLMs, while significantly reducing the computational costs for both training and inference.","The code and models will be made open-source."],"url":"http://arxiv.org/abs/2405.20339v1","category":"cs.CV"}
{"created":"2024-05-30 17:59:42","title":"OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving","abstract":"Understanding the evolution of 3D scenes is important for effective autonomous driving. While conventional methods mode scene development with the motion of individual instances, world models emerge as a generative framework to describe the general scene dynamics. However, most existing methods adopt an autoregressive framework to perform next-token prediction, which suffer from inefficiency in modeling long-term temporal evolutions. To address this, we propose a diffusion-based 4D occupancy generation model, OccSora, to simulate the development of the 3D world for autonomous driving. We employ a 4D scene tokenizer to obtain compact discrete spatial-temporal representations for 4D occupancy input and achieve high-quality reconstruction for long-sequence occupancy videos. We then learn a diffusion transformer on the spatial-temporal representations and generate 4D occupancy conditioned on a trajectory prompt. We conduct extensive experiments on the widely used nuScenes dataset with Occ3D occupancy annotations. OccSora can generate 16s-videos with authentic 3D layout and temporal consistency, demonstrating its ability to understand the spatial and temporal distributions of driving scenes. With trajectory-aware 4D generation, OccSora has the potential to serve as a world simulator for the decision-making of autonomous driving. Code is available at: https://github.com/wzzheng/OccSora.","sentences":["Understanding the evolution of 3D scenes is important for effective autonomous driving.","While conventional methods mode scene development with the motion of individual instances, world models emerge as a generative framework to describe the general scene dynamics.","However, most existing methods adopt an autoregressive framework to perform next-token prediction, which suffer from inefficiency in modeling long-term temporal evolutions.","To address this, we propose a diffusion-based 4D occupancy generation model, OccSora, to simulate the development of the 3D world for autonomous driving.","We employ a 4D scene tokenizer to obtain compact discrete spatial-temporal representations for 4D occupancy input and achieve high-quality reconstruction for long-sequence occupancy videos.","We then learn a diffusion transformer on the spatial-temporal representations and generate 4D occupancy conditioned on a trajectory prompt.","We conduct extensive experiments on the widely used nuScenes dataset with Occ3D occupancy annotations.","OccSora can generate 16s-videos with authentic 3D layout and temporal consistency, demonstrating its ability to understand the spatial and temporal distributions of driving scenes.","With trajectory-aware 4D generation, OccSora has the potential to serve as a world simulator for the decision-making of autonomous driving.","Code is available at: https://github.com/wzzheng/OccSora."],"url":"http://arxiv.org/abs/2405.20337v1","category":"cs.CV"}
{"created":"2024-05-30 17:59:39","title":"RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text","abstract":"In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information, and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation. The project page is available for research purposes at https://vis-www.cs.umass.edu/RapVerse.","sentences":["In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation.","To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes.","With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions.","For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information, and singer identity.","By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions.","Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation.","The project page is available for research purposes at https://vis-www.cs.umass.edu/RapVerse."],"url":"http://arxiv.org/abs/2405.20336v1","category":"cs.CV"}
{"created":"2024-05-30 17:59:31","title":"Xwin-LM: Strong and Scalable Alignment Practice for LLMs","abstract":"In this work, we present Xwin-LM, a comprehensive suite of alignment methodologies for large language models (LLMs). This suite encompasses several key techniques, including supervised finetuning (SFT), reward modeling (RM), rejection sampling finetuning (RS), and direct preference optimization (DPO). The key components are as follows: (1) Xwin-LM-SFT, models initially finetuned with high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turn preference dataset meticulously annotated using GPT-4; (3) Xwin-RM, reward models trained on Xwin-Pair, developed at scales of 7B, 13B, and 70B parameters; (4) Xwin-Set, a multiwise preference dataset in which each prompt is linked to 64 unique responses generated by Xwin-LM-SFT and scored by Xwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responses from Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using the DPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrate consistent and significant improvements across the pipeline, demonstrating the strength and scalability of Xwin-LM. The repository https://github.com/Xwin-LM/Xwin-LM will be continually updated to foster community research.","sentences":["In this work, we present Xwin-LM, a comprehensive suite of alignment methodologies for large language models (LLMs).","This suite encompasses several key techniques, including supervised finetuning (SFT), reward modeling (RM), rejection sampling finetuning (RS), and direct preference optimization (DPO).","The key components are as follows: (1) Xwin-LM-SFT, models initially finetuned with high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turn preference dataset meticulously annotated using GPT-4; (3) Xwin-RM, reward models trained on Xwin-Pair, developed at scales of 7B, 13B, and 70B parameters; (4) Xwin-Set, a multiwise preference dataset in which each prompt is linked to 64 unique responses generated by Xwin-LM-SFT and scored by Xwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responses from Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using the DPO algorithm.","Our evaluations on AlpacaEval and MT-bench demonstrate consistent and significant improvements across the pipeline, demonstrating the strength and scalability of Xwin-LM.","The repository https://github.com/Xwin-LM/Xwin-LM will be continually updated to foster community research."],"url":"http://arxiv.org/abs/2405.20335v1","category":"cs.CL"}
{"created":"2024-05-30 17:59:24","title":"VividDream: Generating 3D Scene with Ambient Dynamics","abstract":"We introduce VividDream, a method for generating explorable 4D scenes with ambient dynamics from a single input image or text prompt. VividDream first expands an input image into a static 3D point cloud through iterative inpainting and geometry merging. An ensemble of animated videos is then generated using video diffusion models with quality refinement techniques and conditioned on renderings of the static 3D scene from the sampled camera trajectories. We then optimize a canonical 4D scene representation using an animated video ensemble, with per-video motion embeddings and visibility masks to mitigate inconsistencies. The resulting 4D scene enables free-view exploration of a 3D scene with plausible ambient scene dynamics. Experiments demonstrate that VividDream can provide human viewers with compelling 4D experiences generated based on diverse real images and text prompts.","sentences":["We introduce VividDream, a method for generating explorable 4D scenes with ambient dynamics from a single input image or text prompt.","VividDream first expands an input image into a static 3D point cloud through iterative inpainting and geometry merging.","An ensemble of animated videos is then generated using video diffusion models with quality refinement techniques and conditioned on renderings of the static 3D scene from the sampled camera trajectories.","We then optimize a canonical 4D scene representation using an animated video ensemble, with per-video motion embeddings and visibility masks to mitigate inconsistencies.","The resulting 4D scene enables free-view exploration of a 3D scene with plausible ambient scene dynamics.","Experiments demonstrate that VividDream can provide human viewers with compelling 4D experiences generated based on diverse real images and text prompts."],"url":"http://arxiv.org/abs/2405.20334v1","category":"cs.CV"}
{"created":"2024-05-30 17:59:04","title":"CoSy: Evaluating Textual Explanations of Neurons","abstract":"A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations. While various methods exist to connect neurons to textual descriptions of human-understandable concepts, evaluating the quality of these explanation methods presents a major challenge in the field due to a lack of unified, general-purpose quantitative evaluation. In this work, we introduce CoSy (Concept Synthesis) -- a novel, architecture-agnostic framework to evaluate the quality of textual explanations for latent neurons. Given textual explanations, our proposed framework leverages a generative model conditioned on textual input to create data points representing the textual explanation. Then, the neuron's response to these explanation data points is compared with the response to control data points, providing a quality estimate of the given explanation. We ensure the reliability of our proposed framework in a series of meta-evaluation experiments and demonstrate practical value through insights from benchmarking various concept-based textual explanation methods for Computer Vision tasks, showing that tested explanation methods significantly differ in quality.","sentences":["A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations.","While various methods exist to connect neurons to textual descriptions of human-understandable concepts, evaluating the quality of these explanation methods presents a major challenge in the field due to a lack of unified, general-purpose quantitative evaluation.","In this work, we introduce CoSy (Concept Synthesis) -- a novel, architecture-agnostic framework to evaluate the quality of textual explanations for latent neurons.","Given textual explanations, our proposed framework leverages a generative model conditioned on textual input to create data points representing the textual explanation.","Then, the neuron's response to these explanation data points is compared with the response to control data points, providing a quality estimate of the given explanation.","We ensure the reliability of our proposed framework in a series of meta-evaluation experiments and demonstrate practical value through insights from benchmarking various concept-based textual explanation methods for Computer Vision tasks, showing that tested explanation methods significantly differ in quality."],"url":"http://arxiv.org/abs/2405.20331v1","category":"cs.LG"}
{"created":"2024-05-30 17:59:02","title":"4DHands: Reconstructing Interactive Hands in 4D with Transformers","abstract":"In this paper, we introduce 4DHands, a robust approach to recovering interactive hand meshes and their relative movement from monocular inputs. Our approach addresses two major limitations of previous methods: lacking a unified solution for handling various hand image inputs and neglecting the positional relationship of two hands within images. To overcome these challenges, we develop a transformer-based architecture with novel tokenization and feature fusion strategies. Specifically, we propose a Relation-aware Two-Hand Tokenization (RAT) method to embed positional relation information into the hand tokens. In this way, our network can handle both single-hand and two-hand inputs and explicitly leverage relative hand positions, facilitating the reconstruction of intricate hand interactions in real-world scenarios. As such tokenization indicates the relative relationship of two hands, it also supports more effective feature fusion. To this end, we further develop a Spatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4D with attention and decode them into 3D hand meshes and relative temporal movements. The efficacy of our approach is validated on several benchmark datasets. The results on in-the-wild videos and real-world scenarios demonstrate the superior performances of our approach for interactive hand reconstruction. More video results can be found on the project page: https://4dhands.github.io.","sentences":["In this paper, we introduce 4DHands, a robust approach to recovering interactive hand meshes and their relative movement from monocular inputs.","Our approach addresses two major limitations of previous methods: lacking a unified solution for handling various hand image inputs and neglecting the positional relationship of two hands within images.","To overcome these challenges, we develop a transformer-based architecture with novel tokenization and feature fusion strategies.","Specifically, we propose a Relation-aware Two-Hand Tokenization (RAT) method to embed positional relation information into the hand tokens.","In this way, our network can handle both single-hand and two-hand inputs and explicitly leverage relative hand positions, facilitating the reconstruction of intricate hand interactions in real-world scenarios.","As such tokenization indicates the relative relationship of two hands, it also supports more effective feature fusion.","To this end, we further develop a Spatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4D with attention and decode them into 3D hand meshes and relative temporal movements.","The efficacy of our approach is validated on several benchmark datasets.","The results on in-the-wild videos and real-world scenarios demonstrate the superior performances of our approach for interactive hand reconstruction.","More video results can be found on the project page: https://4dhands.github.io."],"url":"http://arxiv.org/abs/2405.20330v1","category":"cs.CV"}
{"created":"2024-05-30 17:58:17","title":"mRNA secondary structure prediction using utility-scale quantum computers","abstract":"Recent advancements in quantum computing have opened new avenues for tackling long-standing complex combinatorial optimization problems that are intractable for classical computers. Predicting secondary structure of mRNA is one such notoriously difficult problem that can benefit from the ever-increasing maturity of quantum computing technology. Accurate prediction of mRNA secondary structure is critical in designing RNA-based therapeutics as it dictates various steps of an mRNA life cycle, including transcription, translation, and decay. The current generation of quantum computers have reached utility-scale, allowing us to explore relatively large problem sizes. In this paper, we examine the feasibility of solving mRNA secondary structures on a quantum computer with sequence length up to 60 nucleotides representing problems in the qubit range of 10 to 80. We use Conditional Value at Risk (CVaR)-based VQE algorithm to solve the optimization problems, originating from the mRNA structure prediction problem, on the IBM Eagle and Heron quantum processors. To our encouragement, even with ``minimal'' error mitigation and fixed-depth circuits, our hardware runs yield accurate predictions of minimum free energy (MFE) structures that match the results of the classical solver CPLEX. Our results provide sufficient evidence for the viability of solving mRNA structure prediction problems on a quantum computer and motivate continued research in this direction.","sentences":["Recent advancements in quantum computing have opened new avenues for tackling long-standing complex combinatorial optimization problems that are intractable for classical computers.","Predicting secondary structure of mRNA is one such notoriously difficult problem that can benefit from the ever-increasing maturity of quantum computing technology.","Accurate prediction of mRNA secondary structure is critical in designing RNA-based therapeutics as it dictates various steps of an mRNA life cycle, including transcription, translation, and decay.","The current generation of quantum computers have reached utility-scale, allowing us to explore relatively large problem sizes.","In this paper, we examine the feasibility of solving mRNA secondary structures on a quantum computer with sequence length up to 60 nucleotides representing problems in the qubit range of 10 to 80.","We use Conditional Value at Risk (CVaR)-based VQE algorithm to solve the optimization problems, originating from the mRNA structure prediction problem, on the IBM Eagle and Heron quantum processors.","To our encouragement, even with ``minimal'' error mitigation and fixed-depth circuits, our hardware runs yield accurate predictions of minimum free energy (MFE) structures that match the results of the classical solver CPLEX.","Our results provide sufficient evidence for the viability of solving mRNA structure prediction problems on a quantum computer and motivate continued research in this direction."],"url":"http://arxiv.org/abs/2405.20328v1","category":"quant-ph"}
{"created":"2024-05-30 17:58:00","title":"GECO: Generative Image-to-3D within a SECOnd","abstract":"3D generation has seen remarkable progress in recent years. Existing techniques, such as score distillation methods, produce notable results but require extensive per-scene optimization, impacting time efficiency. Alternatively, reconstruction-based approaches prioritize efficiency but compromise quality due to their limited handling of uncertainty. We introduce GECO, a novel method for high-quality 3D generative modeling that operates within a second. Our approach addresses the prevalent issues of uncertainty and inefficiency in current methods through a two-stage approach. In the initial stage, we train a single-step multi-view generative model with score distillation. Then, a second-stage distillation is applied to address the challenge of view inconsistency from the multi-view prediction. This two-stage process ensures a balanced approach to 3D generation, optimizing both quality and efficiency. Our comprehensive experiments demonstrate that GECO achieves high-quality image-to-3D generation with an unprecedented level of efficiency.","sentences":["3D generation has seen remarkable progress in recent years.","Existing techniques, such as score distillation methods, produce notable results but require extensive per-scene optimization, impacting time efficiency.","Alternatively, reconstruction-based approaches prioritize efficiency but compromise quality due to their limited handling of uncertainty.","We introduce GECO, a novel method for high-quality 3D generative modeling that operates within a second.","Our approach addresses the prevalent issues of uncertainty and inefficiency in current methods through a two-stage approach.","In the initial stage, we train a single-step multi-view generative model with score distillation.","Then, a second-stage distillation is applied to address the challenge of view inconsistency from the multi-view prediction.","This two-stage process ensures a balanced approach to 3D generation, optimizing both quality and efficiency.","Our comprehensive experiments demonstrate that GECO achieves high-quality image-to-3D generation with an unprecedented level of efficiency."],"url":"http://arxiv.org/abs/2405.20327v1","category":"cs.CV"}
{"created":"2024-05-30 17:57:53","title":"Interacting Fields at Spatial Infinity","abstract":"We study the properties of massive fields extrapolated to the blowup of spatial infinity ($\\hat{i}^0$), extending the program initiated in arXiv:2207.06406. In the free theory, we find an explicit representation of boundary two-point functions and boundary to bulk two-point functions, and also present an HKLL-type reconstruction formula for local bulk operators in terms of smeared boundary operators. We study interacting Wightman correlators and find that, generically, interacting massive fields decay slower than free fields as one approaches $\\hat{i}^0$. We propose that meaningful correlators at $\\hat{i}^0$ can be obtained through an LSZ-like prescription that isolates the on-shell part of bulk Wightman correlators before extrapolating them to $\\hat{i}^0$. We show that a natural basis for operators at $\\hat{i}^0$, defined via this prescription, is given by the average of \"in\" and \"out\" operators defined at $i^-$ and $i^+$ respectively. Therefore, correlators at $\\hat{i}^0$ and cross correlators between $\\hat{i}^0$, $i^-$ and $i^+$ can be represented within the class of asymptotic observables studied by Caron-Huot et al. in arXiv:2308.02125. We present several sample calculations.","sentences":["We study the properties of massive fields extrapolated to the blowup of spatial infinity ($\\hat{i}^0$), extending the program initiated in arXiv:2207.06406.","In the free theory, we find an explicit representation of boundary two-point functions and boundary to bulk two-point functions, and also present an HKLL-type reconstruction formula for local bulk operators in terms of smeared boundary operators.","We study interacting Wightman correlators and find that, generically, interacting massive fields decay slower than free fields as one approaches $\\hat{i}^0$. We propose that meaningful correlators at $\\hat{i}^0$ can be obtained through an LSZ-like prescription that isolates the on-shell part of bulk Wightman correlators before extrapolating them to $\\hat{i}^0$. We show that a natural basis for operators at $\\hat{i}^0$, defined via this prescription, is given by the average of \"in\" and \"out\" operators defined at $i^-$ and $i^+$ respectively.","Therefore, correlators at $\\hat{i}^0$ and cross correlators between $\\hat{i}^0$, $i^-$ and $i^+$ can be represented within the class of asymptotic observables studied by Caron-Huot et al. in arXiv:2308.02125.","We present several sample calculations."],"url":"http://arxiv.org/abs/2405.20326v1","category":"hep-th"}
{"created":"2024-05-30 17:57:26","title":"Don't drop your samples! Coherence-aware training benefits Conditional diffusion","abstract":"Conditional diffusion models are powerful generative models that can leverage various types of conditional information, such as class labels, segmentation masks, or text captions. However, in many real-world scenarios, conditional information may be noisy or unreliable due to human annotation errors or weak alignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a novel method that integrates coherence in conditional information into diffusion models, allowing them to learn from noisy annotations without discarding data. We assume that each data point has an associated coherence score that reflects the quality of the conditional information. We then condition the diffusion model on both the conditional information and the coherence score. In this way, the model learns to ignore or discount the conditioning when the coherence is low. We show that CAD is theoretically sound and empirically effective on various conditional generation tasks. Moreover, we show that leveraging coherence generates realistic and diverse samples that respect conditional information better than models trained on cleaned datasets where samples with low coherence have been discarded.","sentences":["Conditional diffusion models are powerful generative models that can leverage various types of conditional information, such as class labels, segmentation masks, or text captions.","However, in many real-world scenarios, conditional information may be noisy or unreliable due to human annotation errors or weak alignment.","In this paper, we propose the Coherence-Aware Diffusion (CAD), a novel method that integrates coherence in conditional information into diffusion models, allowing them to learn from noisy annotations without discarding data.","We assume that each data point has an associated coherence score that reflects the quality of the conditional information.","We then condition the diffusion model on both the conditional information and the coherence score.","In this way, the model learns to ignore or discount the conditioning when the coherence is low.","We show that CAD is theoretically sound and empirically effective on various conditional generation tasks.","Moreover, we show that leveraging coherence generates realistic and diverse samples that respect conditional information better than models trained on cleaned datasets where samples with low coherence have been discarded."],"url":"http://arxiv.org/abs/2405.20324v1","category":"cs.CV"}
{"created":"2024-05-30 17:57:08","title":"$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving","abstract":"Photorealistic 3D reconstruction of street scenes is a critical technique for developing real-world simulators for autonomous driving. Despite the efficacy of Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting (3DGS) emerges as a promising direction due to its faster speed and more explicit representation. However, most existing street 3DGS methods require tracked 3D vehicle bounding boxes to decompose the static and dynamic elements for effective reconstruction, limiting their applications for in-the-wild scenarios. To facilitate efficient 3D scene reconstruction without costly annotations, we propose a self-supervised street Gaussian ($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from 4D consistency. We represent each scene with 3D Gaussians to preserve the explicitness and further accompany them with a spatial-temporal field network to compactly model the 4D dynamics. We conduct extensive experiments on the challenging Waymo-Open dataset to evaluate the effectiveness of our method. Our $\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic scenes and achieves the best performance without using 3D annotations. Code is available at: https://github.com/nnanhuang/S3Gaussian/.","sentences":["Photorealistic 3D reconstruction of street scenes is a critical technique for developing real-world simulators for autonomous driving.","Despite the efficacy of Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting (3DGS) emerges as a promising direction due to its faster speed and more explicit representation.","However, most existing street 3DGS methods require tracked 3D vehicle bounding boxes to decompose the static and dynamic elements for effective reconstruction, limiting their applications for in-the-wild scenarios.","To facilitate efficient 3D scene reconstruction without costly annotations, we propose a self-supervised street Gaussian ($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from 4D consistency.","We represent each scene with 3D Gaussians to preserve the explicitness and further accompany them with a spatial-temporal field network to compactly model the 4D dynamics.","We conduct extensive experiments on the challenging Waymo-Open dataset to evaluate the effectiveness of our method.","Our $\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic scenes and achieves the best performance without using 3D annotations.","Code is available at: https://github.com/nnanhuang/S3Gaussian/."],"url":"http://arxiv.org/abs/2405.20323v1","category":"cs.CV"}
{"created":"2024-05-30 17:57:04","title":"Quantum generalizations of Glauber and Metropolis dynamics","abstract":"Classical Markov Chain Monte Carlo methods have been essential for simulating statistical physical systems and have proven well applicable to other systems with complex degrees of freedom. Motivated by the statistical physics origins, Chen, Kastoryano, and Gily\\'en [CKG23] proposed a continuous-time quantum thermodynamic analog to Glauber dynamic that is (i) exactly detailed balanced, (ii) efficiently implementable, and (iii) quasi-local for geometrically local systems. Physically, their construction gives a smooth variant of the Davies' generator derived from weak system-bath interaction. In this work, we give an efficiently implementable discrete-time quantum counterpart to Metropolis sampling that also enjoys the desirable features (i)-(iii). Also, we give an alternative highly coherent quantum generalization of detailed balanced dynamics that resembles another physically derived master equation, and propose a smooth interpolation between this and earlier constructions. We study generic properties of all constructions, including the uniqueness of the fixed-point and the locality of the resulting operators. We hope our results provide a systematic approach to the possible quantum generalizations of classical Glauber and Metropolis dynamics.","sentences":["Classical Markov Chain Monte Carlo methods have been essential for simulating statistical physical systems and have proven well applicable to other systems with complex degrees of freedom.","Motivated by the statistical physics origins, Chen, Kastoryano, and Gily\\'en","[CKG23] proposed a continuous-time quantum thermodynamic analog to Glauber dynamic that is (i) exactly detailed balanced, (ii) efficiently implementable, and (iii) quasi-local for geometrically local systems.","Physically, their construction gives a smooth variant of the Davies' generator derived from weak system-bath interaction.","In this work, we give an efficiently implementable discrete-time quantum counterpart to Metropolis sampling that also enjoys the desirable features (i)-(iii).","Also, we give an alternative highly coherent quantum generalization of detailed balanced dynamics that resembles another physically derived master equation, and propose a smooth interpolation between this and earlier constructions.","We study generic properties of all constructions, including the uniqueness of the fixed-point and the locality of the resulting operators.","We hope our results provide a systematic approach to the possible quantum generalizations of classical Glauber and Metropolis dynamics."],"url":"http://arxiv.org/abs/2405.20322v1","category":"quant-ph"}
{"created":"2024-05-30 17:56:54","title":"Vision-based Manipulation from Single Human Video with Open-World Object Graphs","abstract":"We present an object-centric approach to empower robots to learn vision-based manipulation skills from human videos. We investigate the problem of imitating robot manipulation from a single human video in the open-world setting, where a robot must learn to manipulate novel objects from one video demonstration. We introduce ORION, an algorithm that tackles the problem by extracting an object-centric manipulation plan from a single RGB-D video and deriving a policy that conditions on the extracted plan. Our method enables the robot to learn from videos captured by daily mobile devices such as an iPad and generalize the policies to deployment environments with varying visual backgrounds, camera angles, spatial layouts, and novel object instances. We systematically evaluate our method on both short-horizon and long-horizon tasks, demonstrating the efficacy of ORION in learning from a single human video in the open world. Videos can be found in the project website https://ut-austin-rpl.github.io/ORION-release.","sentences":["We present an object-centric approach to empower robots to learn vision-based manipulation skills from human videos.","We investigate the problem of imitating robot manipulation from a single human video in the open-world setting, where a robot must learn to manipulate novel objects from one video demonstration.","We introduce ORION, an algorithm that tackles the problem by extracting an object-centric manipulation plan from a single RGB-D video and deriving a policy that conditions on the extracted plan.","Our method enables the robot to learn from videos captured by daily mobile devices such as an iPad and generalize the policies to deployment environments with varying visual backgrounds, camera angles, spatial layouts, and novel object instances.","We systematically evaluate our method on both short-horizon and long-horizon tasks, demonstrating the efficacy of ORION in learning from a single human video in the open world.","Videos can be found in the project website https://ut-austin-rpl.github.io/ORION-release."],"url":"http://arxiv.org/abs/2405.20321v1","category":"cs.RO"}
{"created":"2024-05-30 17:56:04","title":"Improving the Training of Rectified Flows","abstract":"Diffusion models have shown great promise for image and video generation, but sampling from state-of-the-art models requires expensive numerical integration of a generative ODE. One approach for tackling this problem is rectified flows, which iteratively learn smooth ODE paths that are less susceptible to truncation error. However, rectified flows still require a relatively large number of function evaluations (NFEs). In this work, we propose improved techniques for training rectified flows, allowing them to compete with knowledge distillation methods even in the low NFE setting. Our main insight is that under realistic settings, a single iteration of the Reflow algorithm for training rectified flows is sufficient to learn nearly straight trajectories; hence, the current practice of using multiple Reflow iterations is unnecessary. We thus propose techniques to improve one-round training of rectified flows, including a U-shaped timestep distribution and LPIPS-Huber premetric. With these techniques, we improve the FID of the previous 2-rectified flow by up to 72% in the 1 NFE setting on CIFAR-10. On ImageNet 64$\\times$64, our improved rectified flow outperforms the state-of-the-art distillation methods such as consistency distillation and progressive distillation in both one-step and two-step settings and rivals the performance of improved consistency training (iCT) in FID. Code is available at https://github.com/sangyun884/rfpp.","sentences":["Diffusion models have shown great promise for image and video generation, but sampling from state-of-the-art models requires expensive numerical integration of a generative ODE.","One approach for tackling this problem is rectified flows, which iteratively learn smooth ODE paths that are less susceptible to truncation error.","However, rectified flows still require a relatively large number of function evaluations (NFEs).","In this work, we propose improved techniques for training rectified flows, allowing them to compete with knowledge distillation methods even in the low NFE setting.","Our main insight is that under realistic settings, a single iteration of the Reflow algorithm for training rectified flows is sufficient to learn nearly straight trajectories; hence, the current practice of using multiple Reflow iterations is unnecessary.","We thus propose techniques to improve one-round training of rectified flows, including a U-shaped timestep distribution and LPIPS-Huber premetric.","With these techniques, we improve the FID of the previous 2-rectified flow by up to 72% in the 1 NFE setting on CIFAR-10.","On ImageNet 64$\\times$64, our improved rectified flow outperforms the state-of-the-art distillation methods such as consistency distillation and progressive distillation in both one-step and two-step settings and rivals the performance of improved consistency training (iCT) in FID.","Code is available at https://github.com/sangyun884/rfpp."],"url":"http://arxiv.org/abs/2405.20320v1","category":"cs.CV"}
{"created":"2024-05-30 17:55:46","title":"ParSEL: Parameterized Shape Editing with Language","abstract":"The ability to edit 3D assets from natural language presents a compelling paradigm to aid in the democratization of 3D content creation. However, while natural language is often effective at communicating general intent, it is poorly suited for specifying precise manipulation. To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets from natural language. Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program. Adjusting the program parameters allows users to explore shape variations with a precise control over the magnitudes of edits. To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs). However, while we find that LLMs excel at identifying initial edit operations, they often fail to infer complete editing programs, and produce outputs that violate shape semantics. To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed. Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis. Experimentally we demonstrate ParSEL's effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs.","sentences":["The ability to edit 3D assets from natural language presents a compelling paradigm to aid in the democratization of 3D content creation.","However, while natural language is often effective at communicating general intent, it is poorly suited for specifying precise manipulation.","To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets from natural language.","Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program.","Adjusting the program parameters allows users to explore shape variations with a precise control over the magnitudes of edits.","To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs).","However, while we find that LLMs excel at identifying initial edit operations, they often fail to infer complete editing programs, and produce outputs that violate shape semantics.","To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed.","Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis.","Experimentally we demonstrate ParSEL's effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs."],"url":"http://arxiv.org/abs/2405.20319v1","category":"cs.CV"}
{"created":"2024-05-30 17:55:28","title":"CausalQuest: Collecting Natural Causal Questions for AI Agents","abstract":"Humans have an innate drive to seek out causality. Whether fuelled by curiosity or specific goals, we constantly question why things happen, how they are interconnected, and many other related phenomena. To develop AI agents capable of addressing this natural human quest for causality, we urgently need a comprehensive dataset of natural causal questions. Unfortunately, existing datasets either contain only artificially-crafted questions that do not reflect real AI usage scenarios or have limited coverage of questions from specific sources. To address this gap, we present CausalQuest, a dataset of 13,500 naturally occurring questions sourced from social networks, search engines, and AI assistants. We formalize the definition of causal questions and establish a taxonomy for finer-grained classification. Through a combined effort of human annotators and large language models (LLMs), we carefully label the dataset. We find that 42% of the questions humans ask are indeed causal, with the majority seeking to understand the causes behind given effects. Using this dataset, we train efficient classifiers (up to 2.85B parameters) for the binary task of identifying causal questions, achieving high performance with F1 scores of up to 0.877. We conclude with a rich set of future research directions that can build upon our data and models.","sentences":["Humans have an innate drive to seek out causality.","Whether fuelled by curiosity or specific goals, we constantly question why things happen, how they are interconnected, and many other related phenomena.","To develop AI agents capable of addressing this natural human quest for causality, we urgently need a comprehensive dataset of natural causal questions.","Unfortunately, existing datasets either contain only artificially-crafted questions that do not reflect real AI usage scenarios or have limited coverage of questions from specific sources.","To address this gap, we present CausalQuest, a dataset of 13,500 naturally occurring questions sourced from social networks, search engines, and AI assistants.","We formalize the definition of causal questions and establish a taxonomy for finer-grained classification.","Through a combined effort of human annotators and large language models (LLMs), we carefully label the dataset.","We find that 42% of the questions humans ask are indeed causal, with the majority seeking to understand the causes behind given effects.","Using this dataset, we train efficient classifiers (up to 2.85B parameters) for the binary task of identifying causal questions, achieving high performance with F1 scores of up to 0.877.","We conclude with a rich set of future research directions that can build upon our data and models."],"url":"http://arxiv.org/abs/2405.20318v1","category":"cs.CL"}
{"created":"2024-05-30 17:55:27","title":"Analytic Kramer sampling and quasi Lagrange-type interpolation in vector valued RKHS","abstract":"This paper discusses an abstract Kramer sampling theorem for functions within a reproducing kernel Hilbert space (RKHS) of vector valued holomorphic functions. Additionally, we extend the concept of quasi Lagrange-type interpolation for functions within a RKHS of vector valued entire functions. The dependence of having quasi Lagrange-type interpolation on an invariance condition under the generalized backward shift operator has also been discussed. Furthermore, the paper establishes the connection between quasi Lagrange-type interpolation, operator of multiplication by the independent variable, and de Branges spaces of vector valued entire functions.","sentences":["This paper discusses an abstract Kramer sampling theorem for functions within a reproducing kernel Hilbert space (RKHS) of vector valued holomorphic functions.","Additionally, we extend the concept of quasi Lagrange-type interpolation for functions within a RKHS of vector valued entire functions.","The dependence of having quasi Lagrange-type interpolation on an invariance condition under the generalized backward shift operator has also been discussed.","Furthermore, the paper establishes the connection between quasi Lagrange-type interpolation, operator of multiplication by the independent variable, and de Branges spaces of vector valued entire functions."],"url":"http://arxiv.org/abs/2405.20317v1","category":"math.FA"}
{"created":"2024-05-30 17:54:40","title":"ANAH: Analytical Annotation of Hallucinations in Large Language Models","abstract":"Reducing the `$\\textit{hallucination}$' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present $\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical $\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.","sentences":["Reducing the `$\\textit{hallucination}$' problem of Large Language Models (LLMs) is crucial for their wide applications.","A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community.","Thus, we present $\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical $\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative Question Answering.","Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content.","ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline.","Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators.","We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions."],"url":"http://arxiv.org/abs/2405.20315v1","category":"cs.CL"}
{"created":"2024-05-30 17:53:50","title":"Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation","abstract":"Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow-2, a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation. FoldFlow-2 presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples -- crucial for de-novo drug design -- we train FoldFlow-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.","sentences":["Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences.","In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow-2, a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation.","FoldFlow-2 presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder.","To increase diversity and novelty of generated samples -- crucial for de-novo drug design -- we train FoldFlow-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering.","We further demonstrate the ability to align FoldFlow-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective.","We empirically observe that FoldFlow-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling.","Finally, we demonstrate that a fine-tuned FoldFlow-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody."],"url":"http://arxiv.org/abs/2405.20313v1","category":"cs.LG"}
{"created":"2024-05-30 17:53:40","title":"Active Dwarf Galaxy Database I: Overlap between active galactic nuclei selected by different techniques","abstract":"We assemble a sample of 733 dwarf galaxies ($M_{\\ast} \\le 10^{9.5} \\text{M}_\\odot$) with signatures of active galactic nuclei (AGN) and explore the intersection between different AGN selection techniques. Objects in our database are compiled from previous studies that identify AGN in dwarf galaxies through spectroscopy, X-ray emission, infrared colors, and optical photometric variability. We apply a uniform set of AGN diagnostic tools to the database using archival data. We find that any single selection method captures no more than half of the overall AGN population, and there is a general disagreement amongst the AGN selection methods in this stellar mass regime. The largest overlap between methods is found when both methods use optical spectroscopic data. In contrast, the populations of AGN intersect the least when comparing those methods that use photometric data at different wavelengths. These results can be used to better constrain the active fraction in dwarf galaxies, which is in turn an important constraint for black hole seed formation models. In a follow-up paper, we will explore links between the effectiveness of each selection technique and host galaxy properties.","sentences":["We assemble a sample of 733 dwarf galaxies ($M_{\\ast} \\le 10^{9.5} \\text{M}_\\odot$) with signatures of active galactic nuclei (AGN) and explore the intersection between different AGN selection techniques.","Objects in our database are compiled from previous studies that identify AGN in dwarf galaxies through spectroscopy, X-ray emission, infrared colors, and optical photometric variability.","We apply a uniform set of AGN diagnostic tools to the database using archival data.","We find that any single selection method captures no more than half of the overall AGN population, and there is a general disagreement amongst the AGN selection methods in this stellar mass regime.","The largest overlap between methods is found when both methods use optical spectroscopic data.","In contrast, the populations of AGN intersect the least when comparing those methods that use photometric data at different wavelengths.","These results can be used to better constrain the active fraction in dwarf galaxies, which is in turn an important constraint for black hole seed formation models.","In a follow-up paper, we will explore links between the effectiveness of each selection technique and host galaxy properties."],"url":"http://arxiv.org/abs/2405.20312v1","category":"astro-ph.GA"}
{"created":"2024-05-30 17:52:36","title":"Large Language Models Can Self-Improve At Web Agent Tasks","abstract":"Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.","sentences":["Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data.","Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts.","Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself.","In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark.","In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective.","We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure.","We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement."],"url":"http://arxiv.org/abs/2405.20309v1","category":"cs.LG"}
{"created":"2024-05-30 17:51:10","title":"Variational Mapping of Chern Bands to Landau Levels: Application to Fractional Chern Insulators in Twisted MoTe$_2$","abstract":"We present a theoretical study of mapping between Chern bands and generalized Landau levels in twisted bilayer MoTe$_2$, where fractional Chern insulators have been observed. We construct an exact Landau-level representation of moir\\'e bands, where the bases are derived from Landau-level wavefunctions dressed by spinors aligned or antialigned with the layer pseudospin skyrmion field and maintain uniform quantum geometry. We further generalize the dressed zeroth Landau level to a variational wavefunction with an ideal yet nonuniform quantum geometry and variationally maximize its weight in the first moir\\'e band. The variational wavefunction quantitatively reproduces the exact diagonalization spectra of fractional Chern insulators at hole-filling factors $\\nu_h=2/3$ and $3/5$ across a large twist-angle range. Our work introduces a new approach to studying fractional states by bridging the gap between Chern bands and Landau levels.","sentences":["We present a theoretical study of mapping between Chern bands and generalized Landau levels in twisted bilayer MoTe$_2$, where fractional Chern insulators have been observed.","We construct an exact Landau-level representation of moir\\'e bands, where the bases are derived from Landau-level wavefunctions dressed by spinors aligned or antialigned with the layer pseudospin skyrmion field and maintain uniform quantum geometry.","We further generalize the dressed zeroth Landau level to a variational wavefunction with an ideal yet nonuniform quantum geometry and variationally maximize its weight in the first moir\\'e band.","The variational wavefunction quantitatively reproduces the exact diagonalization spectra of fractional Chern insulators at hole-filling factors $\\nu_h=2/3$ and $3/5$ across a large twist-angle range.","Our work introduces a new approach to studying fractional states by bridging the gap between Chern bands and Landau levels."],"url":"http://arxiv.org/abs/2405.20307v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 17:50:08","title":"Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models","abstract":"We introduce PlausiVL, a large video-language model for anticipating action sequences that are plausible in the real-world. While significant efforts have been made towards anticipating future actions, prior approaches do not take into account the aspect of plausibility in an action sequence. To address this limitation, we explore the generative capability of a large video-language model in our work and further, develop the understanding of plausibility in an action sequence by introducing two objective functions, a counterfactual-based plausible action sequence learning loss and a long-horizon action repetition loss. We utilize temporal logical constraints as well as verb-noun action pair logical constraints to create implausible/counterfactual action sequences and use them to train the model with plausible action sequence learning loss. This loss helps the model to differentiate between plausible and not plausible action sequences and also helps the model to learn implicit temporal cues crucial for the task of action anticipation. The long-horizon action repetition loss puts a higher penalty on the actions that are more prone to repetition over a longer temporal window. With this penalization, the model is able to generate diverse, plausible action sequences. We evaluate our approach on two large-scale datasets, Ego4D and EPIC-Kitchens-100, and show improvements on the task of action anticipation.","sentences":["We introduce PlausiVL, a large video-language model for anticipating action sequences that are plausible in the real-world.","While significant efforts have been made towards anticipating future actions, prior approaches do not take into account the aspect of plausibility in an action sequence.","To address this limitation, we explore the generative capability of a large video-language model in our work and further, develop the understanding of plausibility in an action sequence by introducing two objective functions, a counterfactual-based plausible action sequence learning loss and a long-horizon action repetition loss.","We utilize temporal logical constraints as well as verb-noun action pair logical constraints to create implausible/counterfactual action sequences and use them to train the model with plausible action sequence learning loss.","This loss helps the model to differentiate between plausible and not plausible action sequences and also helps the model to learn implicit temporal cues crucial for the task of action anticipation.","The long-horizon action repetition loss puts a higher penalty on the actions that are more prone to repetition over a longer temporal window.","With this penalization, the model is able to generate diverse, plausible action sequences.","We evaluate our approach on two large-scale datasets, Ego4D and EPIC-Kitchens-100, and show improvements on the task of action anticipation."],"url":"http://arxiv.org/abs/2405.20305v1","category":"cs.CV"}
{"created":"2024-05-30 17:45:57","title":"Massless Quasiparticles in Bogoliubov-de Gennes Systems","abstract":"Gapless quasiparticles can exist in the Bogoliubov-de Gennes (BdG) Hamiltonians in the mean field description of superconductors (SCs), fermionic superfluids (SFs) and quantum spin liquids (QSLs). The mechanism of gapless quasiparticles in superconductors was studied in literature based on the homotopy theory or symmetry-indicators. However, important properties of the gapless quasiparticles including the degeneracy, the energy-momentum dispersion and the responses to external probe fields need to be determined. In the present work, we investigate gapless quasiparticles in general BdG systems by using projective representation theory for the full `symmetry' groups formed by combinations of lattice, spin and charge operations. We found that (I) charge conjugation (or effective charge conjugation) symmetry can yield gapless quasiparticles with linear, quadratic or higher order dispersions at high symmetry points of the Brillouin zone; (II) different quantum numbers protected level crossing can give rise to zero modes along high symmetry lines; (III) combined spatial inversion and time reversal symmetry can protect zero modes appearing at generic $k$ point. To obtain the low energy properties of gapless quasiparticles, the $k\\cdot p$ theory is provided using a high efficient method--the Hamiltonian approach. Based on generalized band representation theory for BdG systems, several lattice models are constructed to illustrate the above results. Our theory provides a method to classify nodal SCs/SFs/QSLs with given symmetries, and enlightens the realization of Majorana type massless quasiparticles in condensed matter physics.","sentences":["Gapless quasiparticles can exist in the Bogoliubov-de Gennes (BdG) Hamiltonians in the mean field description of superconductors (SCs), fermionic superfluids (SFs) and quantum spin liquids (QSLs).","The mechanism of gapless quasiparticles in superconductors was studied in literature based on the homotopy theory or symmetry-indicators.","However, important properties of the gapless quasiparticles including the degeneracy, the energy-momentum dispersion and the responses to external probe fields need to be determined.","In the present work, we investigate gapless quasiparticles in general BdG systems by using projective representation theory for the full `symmetry' groups formed by combinations of lattice, spin and charge operations.","We found that (I) charge conjugation (or effective charge conjugation) symmetry can yield gapless quasiparticles with linear, quadratic or higher order dispersions at high symmetry points of the Brillouin zone; (II) different quantum numbers protected level crossing can give rise to zero modes along high symmetry lines; (III) combined spatial inversion and time reversal symmetry can protect zero modes appearing at generic $k$ point.","To obtain the low energy properties of gapless quasiparticles, the $k\\cdot p$ theory is provided using a high efficient method--the Hamiltonian approach.","Based on generalized band representation theory for BdG systems, several lattice models are constructed to illustrate the above results.","Our theory provides a method to classify nodal SCs/SFs/QSLs with given symmetries, and enlightens the realization of Majorana type massless quasiparticles in condensed matter physics."],"url":"http://arxiv.org/abs/2405.20298v1","category":"cond-mat.str-el"}
{"created":"2024-05-30 17:44:03","title":"How (not) to Build Quantum PKE in Minicrypt","abstract":"The seminal work by Impagliazzo and Rudich (STOC'89) demonstrated the impossibility of constructing classical public key encryption (PKE) from one-way functions (OWF) in a black-box manner. However, the question remains: can quantum PKE (QPKE) be constructed from quantumly secure OWF? A recent line of work has shown that it is indeed possible to build QPKE from OWF, but with one caveat -- they rely on quantum public keys, which cannot be authenticated and reused. In this work, we re-examine the possibility of perfect complete QPKE in the quantum random oracle model (QROM), where OWF exists. Our first main result: QPKE with classical public keys, secret keys and ciphertext, does not exist in the QROM, if the key generation only makes classical queries. Therefore, a necessary condition for constructing such QPKE from OWF is to have the key generation classically ``un-simulatable''. Previous discussions (Austrin et al. CRYPTO'22) on the impossibility of QPKE from OWF rely on a seemingly strong conjecture. Our work makes a significant step towards a complete and unconditional quantization of Impagliazzo and Rudich's results. Our second main result extends to QPKE with quantum public keys. The second main result: QPKE with quantum public keys, classical secret keys and ciphertext, does not exist in the QROM, if the key generation only makes classical queries and the quantum public key is either pure or ``efficiently clonable''. The result is tight due to all existing QPKEs constructions. Our result further gives evidence on why existing QPKEs lose reusability. To achieve these results, we use a novel argument based on conditional mutual information and quantum Markov chain by Fawzi and Renner (Communications in Mathematical Physics). We believe the techniques used in the work will find other usefulness in separations in quantum cryptography/complexity.","sentences":["The seminal work by Impagliazzo and Rudich (STOC'89) demonstrated the impossibility of constructing classical public key encryption (PKE) from one-way functions (OWF) in a black-box manner.","However, the question remains: can quantum PKE (QPKE) be constructed from quantumly secure OWF?","A recent line of work has shown that it is indeed possible to build QPKE from OWF, but with one caveat -- they rely on quantum public keys, which cannot be authenticated and reused.","In this work, we re-examine the possibility of perfect complete QPKE in the quantum random oracle model (QROM), where OWF exists.","Our first main result: QPKE with classical public keys, secret keys and ciphertext, does not exist in the QROM, if the key generation only makes classical queries.","Therefore, a necessary condition for constructing such QPKE from OWF is to have the key generation classically ``un-simulatable''.","Previous discussions (Austrin et al. CRYPTO'22) on the impossibility of QPKE from OWF rely on a seemingly strong conjecture.","Our work makes a significant step towards a complete and unconditional quantization of Impagliazzo and Rudich's results.","Our second main result extends to QPKE with quantum public keys.","The second main result: QPKE with quantum public keys, classical secret keys and ciphertext, does not exist in the QROM, if the key generation only makes classical queries and the quantum public key is either pure or ``efficiently clonable''.","The result is tight due to all existing QPKEs constructions.","Our result further gives evidence on why existing QPKEs lose reusability.","To achieve these results, we use a novel argument based on conditional mutual information and quantum Markov chain by Fawzi and Renner (Communications in Mathematical Physics).","We believe the techniques used in the work will find other usefulness in separations in quantum cryptography/complexity."],"url":"http://arxiv.org/abs/2405.20295v1","category":"quant-ph"}
{"created":"2024-05-30 17:43:18","title":"Points of bounded height on quintic del Pezzo surfaces over number fields","abstract":"We prove Manin's conjecture for split smooth quintic del Pezzo surfaces over arbitrary number fields with respect to fairly general anticanonical height functions. After passing to universal torsors, we first show that we may restrict the torsor variables to their typical sizes, and then we can solve the counting problem in the framework of o-minimal structures.","sentences":["We prove Manin's conjecture for split smooth quintic del Pezzo surfaces over arbitrary number fields with respect to fairly general anticanonical height functions.","After passing to universal torsors, we first show that we may restrict the torsor variables to their typical sizes, and then we can solve the counting problem in the framework of o-minimal structures."],"url":"http://arxiv.org/abs/2405.20293v1","category":"math.NT"}
{"created":"2024-05-30 17:40:11","title":"DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation","abstract":"Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs. Diffusion Inference-Time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use. We propose Distilled Diffusion Inference-Time T -Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control. Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation. Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once. Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control. Sound examples can be found at https://ditto-music.github.io/ditto2/.","sentences":["Controllable music generation methods are critical for human-centered AI-based music creation, but are currently limited by speed, quality, and control design trade-offs.","Diffusion Inference-Time T-optimization (DITTO), in particular, offers state-of-the-art results, but is over 10x slower than real-time, limiting practical use.","We propose Distilled Diffusion Inference-Time T -Optimization (or DITTO-2), a new method to speed up inference-time optimization-based control and unlock faster-than-real-time generation for a wide-variety of applications such as music inpainting, outpainting, intensity, melody, and musical structure control.","Our method works by (1) distilling a pre-trained diffusion model for fast sampling via an efficient, modified consistency or consistency trajectory distillation process (2) performing inference-time optimization using our distilled model with one-step sampling as an efficient surrogate optimization task and (3) running a final multi-step sampling generation (decoding) using our estimated noise latents for best-quality, fast, controllable generation.","Through thorough evaluation, we find our method not only speeds up generation over 10-20x, but simultaneously improves control adherence and generation quality all at once.","Furthermore, we apply our approach to a new application of maximizing text adherence (CLAP score) and show we can convert an unconditional diffusion model without text inputs into a model that yields state-of-the-art text control.","Sound examples can be found at https://ditto-music.github.io/ditto2/."],"url":"http://arxiv.org/abs/2405.20289v1","category":"cs.SD"}
{"created":"2024-05-30 17:39:51","title":"A family of cyclic quartic monogenic polynomials","abstract":"We produce an explicit family of totally real cyclic quartic polynomials that are monogenic in many cases, and conjecturally are monogenic and generating distinct quartic fields infinitely often.","sentences":["We produce an explicit family of totally real cyclic quartic polynomials that are monogenic in many cases, and conjecturally are monogenic and generating distinct quartic fields infinitely often."],"url":"http://arxiv.org/abs/2405.20288v1","category":"math.NT"}
{"created":"2024-05-30 17:39:15","title":"Flexible SE(2) graph neural networks with applications to PDE surrogates","abstract":"This paper presents a novel approach for constructing graph neural networks equivariant to 2D rotations and translations and leveraging them as PDE surrogates on non-gridded domains. We show that aligning the representations with the principal axis allows us to sidestep many constraints while preserving SE(2) equivariance. By applying our model as a surrogate for fluid flow simulations and conducting thorough benchmarks against non-equivariant models, we demonstrate significant gains in terms of both data efficiency and accuracy.","sentences":["This paper presents a novel approach for constructing graph neural networks equivariant to 2D rotations and translations and leveraging them as PDE surrogates on non-gridded domains.","We show that aligning the representations with the principal axis allows us to sidestep many constraints while preserving SE(2) equivariance.","By applying our model as a surrogate for fluid flow simulations and conducting thorough benchmarks against non-equivariant models, we demonstrate significant gains in terms of both data efficiency and accuracy."],"url":"http://arxiv.org/abs/2405.20287v1","category":"cs.LG"}
{"created":"2024-05-30 17:38:44","title":"Who Writes the Review, Human or AI?","abstract":"With the increasing use of Artificial Intelligence in Natural Language Processing, concerns have been raised regarding the detection of AI-generated text in various domains. This study aims to investigate this issue by proposing a methodology to accurately distinguish AI-generated and human-written book reviews. Our approach utilizes transfer learning, enabling the model to identify generated text across different topics while improving its ability to detect variations in writing style and vocabulary. To evaluate the effectiveness of the proposed methodology, we developed a dataset consisting of real book reviews and AI-generated reviews using the recently proposed Vicuna open-source language model. The experimental results demonstrate that it is feasible to detect the original source of text, achieving an accuracy rate of 96.86%. Our efforts are oriented toward the exploration of the capabilities and limitations of Large Language Models in the context of text identification. Expanding our knowledge in these aspects will be valuable for effectively navigating similar models in the future and ensuring the integrity and authenticity of human-generated content.","sentences":["With the increasing use of Artificial Intelligence in Natural Language Processing, concerns have been raised regarding the detection of AI-generated text in various domains.","This study aims to investigate this issue by proposing a methodology to accurately distinguish AI-generated and human-written book reviews.","Our approach utilizes transfer learning, enabling the model to identify generated text across different topics while improving its ability to detect variations in writing style and vocabulary.","To evaluate the effectiveness of the proposed methodology, we developed a dataset consisting of real book reviews and AI-generated reviews using the recently proposed Vicuna open-source language model.","The experimental results demonstrate that it is feasible to detect the original source of text, achieving an accuracy rate of 96.86%.","Our efforts are oriented toward the exploration of the capabilities and limitations of Large Language Models in the context of text identification.","Expanding our knowledge in these aspects will be valuable for effectively navigating similar models in the future and ensuring the integrity and authenticity of human-generated content."],"url":"http://arxiv.org/abs/2405.20285v1","category":"cs.CL"}
{"created":"2024-05-30 17:37:44","title":"Fock's dimer model on the Aztec diamond","abstract":"We consider the dimer model on the Aztec diamond with Fock's weights, which is gauge equivalent to the model with any choice of positive weight function. We prove an explicit, compact formula for the inverse Kasteleyn matrix, thus extending numerous results in the case of periodic graphs. We also show an explicit product formula for the partition function; as a specific instance of the genus 0 case, we recover Stanley's formula. We then use our explicit formula for the inverse Kasteleyn matrix to recover, in a simple way, limit shape results; we also obtain new ones. In doing so, we extend the correspondence between the limit shape and the amoeba of the corresponding spectral curve of arXiv:2306.07482 to the case of non-generic weights.","sentences":["We consider the dimer model on the Aztec diamond with Fock's weights, which is gauge equivalent to the model with any choice of positive weight function.","We prove an explicit, compact formula for the inverse Kasteleyn matrix, thus extending numerous results in the case of periodic graphs.","We also show an explicit product formula for the partition function; as a specific instance of the genus 0 case, we recover Stanley's formula.","We then use our explicit formula for the inverse Kasteleyn matrix to recover, in a simple way, limit shape results; we also obtain new ones.","In doing so, we extend the correspondence between the limit shape and the amoeba of the corresponding spectral curve of arXiv:2306.07482 to the case of non-generic weights."],"url":"http://arxiv.org/abs/2405.20284v1","category":"math.PR"}
{"created":"2024-05-30 17:35:49","title":"TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes","abstract":"We present TetSphere splatting, an explicit, Lagrangian representation for reconstructing 3D shapes with high-quality geometry. In contrast to conventional object reconstruction methods which predominantly use Eulerian representations, including both neural implicit (e.g., NeRF, NeuS) and explicit representations (e.g., DMTet), and often struggle with high computational demands and suboptimal mesh quality, TetSphere splatting utilizes an underused but highly effective geometric primitive -- tetrahedral meshes. This approach directly yields superior mesh quality without relying on neural networks or post-processing. It deforms multiple initial tetrahedral spheres to accurately reconstruct the 3D shape through a combination of differentiable rendering and geometric energy optimization, resulting in significant computational efficiency. Serving as a robust and versatile geometry representation, Tet-Sphere splatting seamlessly integrates into diverse applications, including single-view 3D reconstruction, image-/text-to-3D content generation. Experimental results demonstrate that TetSphere splatting outperforms existing representations, delivering faster optimization speed, enhanced mesh quality, and reliable preservation of thin structures.","sentences":["We present TetSphere splatting, an explicit, Lagrangian representation for reconstructing 3D shapes with high-quality geometry.","In contrast to conventional object reconstruction methods which predominantly use Eulerian representations, including both neural implicit (e.g., NeRF, NeuS) and explicit representations (e.g., DMTet), and often struggle with high computational demands and suboptimal mesh quality, TetSphere splatting utilizes an underused but highly effective geometric primitive -- tetrahedral meshes.","This approach directly yields superior mesh quality without relying on neural networks or post-processing.","It deforms multiple initial tetrahedral spheres to accurately reconstruct the 3D shape through a combination of differentiable rendering and geometric energy optimization, resulting in significant computational efficiency.","Serving as a robust and versatile geometry representation, Tet-Sphere splatting seamlessly integrates into diverse applications, including single-view 3D reconstruction, image-/text-to-3D content generation.","Experimental results demonstrate that TetSphere splatting outperforms existing representations, delivering faster optimization speed, enhanced mesh quality, and reliable preservation of thin structures."],"url":"http://arxiv.org/abs/2405.20283v1","category":"cs.CV"}
{"created":"2024-05-30 17:34:40","title":"SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified Flow","abstract":"Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation. While existing methods consider them as two distinct tasks, we propose a unified diffusion-based framework (SemFlow) and model them as a pair of reverse problems. Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks. As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly. For semantic segmentation, our approach solves the contradiction between the randomness of diffusion outputs and the uniqueness of segmentation results. For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories. Experiments show that our SemFlow achieves competitive results on semantic segmentation and semantic image synthesis tasks. We hope this simple framework will motivate people to rethink the unification of low-level and high-level vision. Project page: https://github.com/wang-chaoyang/SemFlow.","sentences":["Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation.","While existing methods consider them as two distinct tasks, we propose a unified diffusion-based framework (SemFlow) and model them as a pair of reverse problems.","Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks.","As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly.","For semantic segmentation, our approach solves the contradiction between the randomness of diffusion outputs and the uniqueness of segmentation results.","For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories.","Experiments show that our SemFlow achieves competitive results on semantic segmentation and semantic image synthesis tasks.","We hope this simple framework will motivate people to rethink the unification of low-level and high-level vision.","Project page: https://github.com/wang-chaoyang/SemFlow."],"url":"http://arxiv.org/abs/2405.20282v1","category":"cs.CV"}
{"created":"2024-05-30 17:34:25","title":"Tight Characterizations for Preprocessing against Cryptographic Salting","abstract":"Cryptography often considers the strongest yet plausible attacks in the real world. Preprocessing (a.k.a. non-uniform attack) plays an important role in both theory and practice: an efficient online attacker can take advantage of advice prepared by a time-consuming preprocessing stage.   Salting is a heuristic strategy to counter preprocessing attacks by feeding a small amount of randomness to the cryptographic primitive. We present general and tight characterizations of preprocessing against cryptographic salting, with upper bounds matching the advantages of the most intuitive attack. Our result quantitatively strengthens the previous work by Coretti, Dodis, Guo, and Steinberger (EUROCRYPT'18). Our proof exploits a novel connection between the non-uniform security of salted games and direct product theorems for memoryless algorithms.   For quantum adversaries, we give similar characterizations for property finding games, resolving an open problem of the quantum non-uniform security of salted collision resistant hash by Chung, Guo, Liu, and Qian (FOCS'20). Our proof extends the compressed oracle framework of Zhandry (CRYPTO'19) to prove quantum strong direct product theorems for property finding games in the average-case hardness.","sentences":["Cryptography often considers the strongest yet plausible attacks in the real world.","Preprocessing (a.k.a. non-uniform attack) plays an important role in both theory and practice: an efficient online attacker can take advantage of advice prepared by a time-consuming preprocessing stage.   ","Salting is a heuristic strategy to counter preprocessing attacks by feeding a small amount of randomness to the cryptographic primitive.","We present general and tight characterizations of preprocessing against cryptographic salting, with upper bounds matching the advantages of the most intuitive attack.","Our result quantitatively strengthens the previous work by Coretti, Dodis, Guo, and Steinberger (EUROCRYPT'18).","Our proof exploits a novel connection between the non-uniform security of salted games and direct product theorems for memoryless algorithms.   ","For quantum adversaries, we give similar characterizations for property finding games, resolving an open problem of the quantum non-uniform security of salted collision resistant hash by Chung, Guo, Liu, and Qian (FOCS'20).","Our proof extends the compressed oracle framework of Zhandry (CRYPTO'19) to prove quantum strong direct product theorems for property finding games in the average-case hardness."],"url":"http://arxiv.org/abs/2405.20281v1","category":"cs.CR"}
{"created":"2024-05-30 17:33:38","title":"Quarkonium Polarization in Medium from Open Quantum Systems and Chromomagnetic Correlators","abstract":"We study the spin-dependent in-medium dynamics of quarkonia by using the potential nonrelativistic QCD (pNRQCD) and the open quantum system framework. We consider the pNRQCD Lagrangian valid up to the order $\\frac{r}{M^0}=r$ and $\\frac{r^0}{M}=\\frac{1}{M}$ in the double power counting. By considering the Markovian condition and applying the Wigner transformation upon the diagonal spin components of the quarkonium density matrix with the semiclassical expansion, we systematically derive the Boltzmann transport equation for quarkonia with polarization dependence in the quantum optical limit. Unlike the spin-independent collision terms governed by certain chromoelectric field correlators, new gauge invariant correlators of chromomagnetic fields determine the recombination and dissociation terms with polarization dependence at the order we are working. We also derive a Lindblad equation describing the in-medium transitions between spin-singlet and spin-triplet heavy quark-antiquark pairs in the quantum Brownian motion limit. The Lindblad equation is governed by new transport coefficients defined in terms of the chromomagnetic field correlators. Our formalism is generic and valid for both weakly-coupled and strongly-coupled quark gluon plasmas. It can be further applied to study spin alignment of vector quarkonia in heavy ion collisions.","sentences":["We study the spin-dependent in-medium dynamics of quarkonia by using the potential nonrelativistic QCD (pNRQCD) and the open quantum system framework.","We consider the pNRQCD Lagrangian valid up to the order $\\frac{r}{M^0}=r$ and $\\frac{r^0}{M}=\\frac{1}{M}$ in the double power counting.","By considering the Markovian condition and applying the Wigner transformation upon the diagonal spin components of the quarkonium density matrix with the semiclassical expansion, we systematically derive the Boltzmann transport equation for quarkonia with polarization dependence in the quantum optical limit.","Unlike the spin-independent collision terms governed by certain chromoelectric field correlators, new gauge invariant correlators of chromomagnetic fields determine the recombination and dissociation terms with polarization dependence at the order we are working.","We also derive a Lindblad equation describing the in-medium transitions between spin-singlet and spin-triplet heavy quark-antiquark pairs in the quantum Brownian motion limit.","The Lindblad equation is governed by new transport coefficients defined in terms of the chromomagnetic field correlators.","Our formalism is generic and valid for both weakly-coupled and strongly-coupled quark gluon plasmas.","It can be further applied to study spin alignment of vector quarkonia in heavy ion collisions."],"url":"http://arxiv.org/abs/2405.20280v1","category":"hep-ph"}
{"created":"2024-05-30 17:33:10","title":"CV-VAE: A Compatible Video VAE for Latent Generative Video Models","abstract":"Spatio-temporal compression of videos, utilizing networks such as Variational Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other video generative models. For instance, many LLM-like video models learn the distribution of discrete tokens derived from 3D VAEs within the VQVAE framework, while most diffusion-based video models capture the distribution of continuous latent extracted by 2D VAEs without quantization. The temporal compression is simply realized by uniform frame sampling which results in unsmooth motion between consecutive frames. Currently, there lacks of a commonly used continuous video (3D) VAE for latent diffusion-based video models in the research community. Moreover, since current diffusion-based approaches are often implemented using pre-trained text-to-image (T2I) models, directly training a video VAE without considering the compatibility with existing T2I models will result in a latent space gap between them, which will take huge computational resources for training to bridge the gap even with the T2I models as initialization. To address this issue, we propose a method for training a video VAE of latent video models, namely CV-VAE, whose latent space is compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion (SD). The compatibility is achieved by the proposed novel latent space regularization, which involves formulating a regularization loss using the image VAE. Benefiting from the latent space compatibility, video models can be trained seamlessly from pre-trained T2I or video models in a truly spatio-temporally compressed latent space, rather than simply sampling video frames at equal intervals. With our CV-VAE, existing video models can generate four times more frames with minimal finetuning. Extensive experiments are conducted to demonstrate the effectiveness of the proposed video VAE.","sentences":["Spatio-temporal compression of videos, utilizing networks such as Variational Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other video generative models.","For instance, many LLM-like video models learn the distribution of discrete tokens derived from 3D VAEs within the VQVAE framework, while most diffusion-based video models capture the distribution of continuous latent extracted by 2D VAEs without quantization.","The temporal compression is simply realized by uniform frame sampling which results in unsmooth motion between consecutive frames.","Currently, there lacks of a commonly used continuous video (3D) VAE for latent diffusion-based video models in the research community.","Moreover, since current diffusion-based approaches are often implemented using pre-trained text-to-image (T2I) models, directly training a video VAE without considering the compatibility with existing T2I models will result in a latent space gap between them, which will take huge computational resources for training to bridge the gap even with the T2I models as initialization.","To address this issue, we propose a method for training a video VAE of latent video models, namely CV-VAE, whose latent space is compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion (SD).","The compatibility is achieved by the proposed novel latent space regularization, which involves formulating a regularization loss using the image VAE.","Benefiting from the latent space compatibility, video models can be trained seamlessly from pre-trained T2I or video models in a truly spatio-temporally compressed latent space, rather than simply sampling video frames at equal intervals.","With our CV-VAE, existing video models can generate four times more frames with minimal finetuning.","Extensive experiments are conducted to demonstrate the effectiveness of the proposed video VAE."],"url":"http://arxiv.org/abs/2405.20279v1","category":"cs.CV"}
{"created":"2024-05-30 17:32:46","title":"Length independent generalization bounds for deep SSM architectures with stability constraints","abstract":"Many state-of-the-art models trained on long-range sequences, for example S4, S5 or LRU, are made of sequential blocks combining State-Space Models (SSMs) with neural networks. In this paper we provide a PAC bound that holds for these kind of architectures with stable SSM blocks and does not depend on the length of the input sequence. Imposing stability of the SSM blocks is a standard practice in the literature, and it is known to help performance. Our results provide a theoretical justification for the use of stable SSM blocks as the proposed PAC bound decreases as the degree of stability of the SSM blocks increases.","sentences":["Many state-of-the-art models trained on long-range sequences, for example S4, S5 or LRU, are made of sequential blocks combining State-Space Models (SSMs) with neural networks.","In this paper we provide a PAC bound that holds for these kind of architectures with stable SSM blocks and does not depend on the length of the input sequence.","Imposing stability of the SSM blocks is a standard practice in the literature, and it is known to help performance.","Our results provide a theoretical justification for the use of stable SSM blocks as the proposed PAC bound decreases as the degree of stability of the SSM blocks increases."],"url":"http://arxiv.org/abs/2405.20278v1","category":"cs.LG"}
{"created":"2024-05-30 17:30:04","title":"Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation","abstract":"Community detection (CD) is a classic graph inference task that partitions nodes of a graph into densely connected groups. While many CD methods have been proposed with either impressive quality or efficiency, balancing the two aspects remains a challenge. This study explores the potential of deep graph learning to achieve a better trade-off between the quality and efficiency of K-agnostic CD, where the number of communities K is unknown. We propose PRoCD (Pre-training & Refinement fOr Community Detection), a simple yet effective method that reformulates K-agnostic CD as the binary node pair classification. PRoCD follows a pre-training & refinement paradigm inspired by recent advances in pre-training techniques. We first conduct the offline pre-training of PRoCD on small synthetic graphs covering various topology properties. Based on the inductive inference across graphs, we then generalize the pre-trained model (with frozen parameters) to large real graphs and use the derived CD results as the initialization of an existing efficient CD method (e.g., InfoMap) to further refine the quality of CD results. In addition to benefiting from the transfer ability regarding quality, the online generalization and refinement can also help achieve high inference efficiency, since there is no time-consuming model optimization. Experiments on public datasets with various scales demonstrate that PRoCD can ensure higher efficiency in K-agnostic CD without significant quality degradation.","sentences":["Community detection (CD) is a classic graph inference task that partitions nodes of a graph into densely connected groups.","While many CD methods have been proposed with either impressive quality or efficiency, balancing the two aspects remains a challenge.","This study explores the potential of deep graph learning to achieve a better trade-off between the quality and efficiency of K-agnostic CD, where the number of communities K is unknown.","We propose PRoCD (Pre-training & Refinement fOr Community Detection), a simple yet effective method that reformulates K-agnostic CD as the binary node pair classification.","PRoCD follows a pre-training & refinement paradigm inspired by recent advances in pre-training techniques.","We first conduct the offline pre-training of PRoCD on small synthetic graphs covering various topology properties.","Based on the inductive inference across graphs, we then generalize the pre-trained model (with frozen parameters) to large real graphs and use the derived CD results as the initialization of an existing efficient CD method (e.g., InfoMap) to further refine the quality of CD results.","In addition to benefiting from the transfer ability regarding quality, the online generalization and refinement can also help achieve high inference efficiency, since there is no time-consuming model optimization.","Experiments on public datasets with various scales demonstrate that PRoCD can ensure higher efficiency in K-agnostic CD without significant quality degradation."],"url":"http://arxiv.org/abs/2405.20277v1","category":"cs.SI"}
{"created":"2024-05-30 17:29:59","title":"Proof of the Diaconis--Freedman Conjecture on partially-exchangeable processes","abstract":"We prove a conjecture of Diaconis and Freedman (Ann. Probab. 1980) characterising the extreme points of the set of partially-exchangeable processes on a countable set. More concretely, we prove that the partially exchangeable sigma-algebra of any transient partially exchangeable process $X=(X_i)_{i\\geq 0}$ (and hence any transient Markov chain) coincides up to null sets with the sigma-algebra generated by the initial state $X_0$ and the transition counts $( \\#\\{i\\geq 0: X_i=x, X_{i+1}=y\\} : x,y\\in S)$. Our proof is based on an analysis of Gibbs measures for Eulerian paths on rooted digraphs, relying in particular on the connection to uniform spanning trees and Wilson's algorithm via the de Bruijn--Ehrenfest--Smith--Tutte (BEST) bijection, and yields an explicit method to sample from the conditional distribution of a transient Markov chain given its transition counts.","sentences":["We prove a conjecture of Diaconis and Freedman (Ann. Probab.","1980)","characterising the extreme points of the set of partially-exchangeable processes on a countable set.","More concretely, we prove that the partially exchangeable sigma-algebra of any transient partially exchangeable process $X=(X_i)_{i\\geq 0}$ (and hence any transient Markov chain) coincides up to null sets with the sigma-algebra generated by the initial state $X_0$ and the transition counts $( \\#\\{i\\geq 0: X_i=x, X_{i+1}=y\\} : x,y\\in S)$. Our proof is based on an analysis of Gibbs measures for Eulerian paths on rooted digraphs, relying in particular on the connection to uniform spanning trees and Wilson's algorithm via the de Bruijn--Ehrenfest--Smith--Tutte (BEST) bijection, and yields an explicit method to sample from the conditional distribution of a transient Markov chain given its transition counts."],"url":"http://arxiv.org/abs/2405.20276v1","category":"math.PR"}
{"created":"2024-05-30 17:29:15","title":"ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection","abstract":"Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansion and diversity due to various shared tasks spanning several languages and fields and organized via SemEval workshops and Germeval. Nonetheless, a few shortcomings still need to be addressed, such as the lack of low-resource language evaluations and the emphasis on sentence-level analysis. To thoroughly assess ABSA techniques in the context of complete reviews, this research presents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST). ROAST seeks to close the gap between sentence-level and text-level ABSA by identifying every ABSA constituent at the review level. We extend the available datasets to enable ROAST, addressing the drawbacks noted in previous research by incorporating low-resource languages, numerous languages, and a variety of topics. Through this effort, ABSA research will be able to cover more ground and get a deeper comprehension of the task and its practical application in a variety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA).","sentences":["Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansion and diversity due to various shared tasks spanning several languages and fields and organized via SemEval workshops and Germeval.","Nonetheless, a few shortcomings still need to be addressed, such as the lack of low-resource language evaluations and the emphasis on sentence-level analysis.","To thoroughly assess ABSA techniques in the context of complete reviews, this research presents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST).","ROAST seeks to close the gap between sentence-level and text-level ABSA by identifying every ABSA constituent at the review level.","We extend the available datasets to enable ROAST, addressing the drawbacks noted in previous research by incorporating low-resource languages, numerous languages, and a variety of topics.","Through this effort, ABSA research will be able to cover more ground and get a deeper comprehension of the task and its practical application in a variety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA)."],"url":"http://arxiv.org/abs/2405.20274v1","category":"cs.CL"}
{"created":"2024-05-30 17:27:44","title":"Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable","abstract":"Machine unlearning is motivated by desire for data autonomy: a person can request to have their data's influence removed from deployed models, and those models should be updated as if they were retrained without the person's data. We show that, counter-intuitively, these updates expose individuals to high-accuracy reconstruction attacks which allow the attacker to recover their data in its entirety, even when the original models are so simple that privacy risk might not otherwise have been a concern. We show how to mount a near-perfect attack on the deleted data point from linear regression models. We then generalize our attack to other loss functions and architectures, and empirically demonstrate the effectiveness of our attacks across a wide range of datasets (capturing both tabular and image data). Our work highlights that privacy risk is significant even for extremely simple model classes when individuals can request deletion of their data from the model.","sentences":["Machine unlearning is motivated by desire for data autonomy: a person can request to have their data's influence removed from deployed models, and those models should be updated as if they were retrained without the person's data.","We show that, counter-intuitively, these updates expose individuals to high-accuracy reconstruction attacks which allow the attacker to recover their data in its entirety, even when the original models are so simple that privacy risk might not otherwise have been a concern.","We show how to mount a near-perfect attack on the deleted data point from linear regression models.","We then generalize our attack to other loss functions and architectures, and empirically demonstrate the effectiveness of our attacks across a wide range of datasets (capturing both tabular and image data).","Our work highlights that privacy risk is significant even for extremely simple model classes when individuals can request deletion of their data from the model."],"url":"http://arxiv.org/abs/2405.20272v1","category":"cs.LG"}
{"created":"2024-05-30 17:26:02","title":"ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections","abstract":"Parameter-efficient finetuning (PEFT) has become ubiquitous to adapt foundation models to downstream task requirements while retaining their generalization ability. However, the amount of additionally introduced parameters and compute for successful adaptation and hyperparameter searches can explode quickly, especially when deployed at scale to serve numerous individual requests. To ensure effective, parameter-efficient, and hyperparameter-robust adaptation, we propose the ETHER transformation family, which performs Efficient fineTuning via HypErplane Reflections. By design, ETHER transformations require a minimal number of parameters, are less likely to deteriorate model performance, and exhibit robustness to hyperparameter and learning rate choices. In particular, we introduce ETHER and its relaxation ETHER+, which match or outperform existing PEFT methods with significantly fewer parameters ($\\sim$$10$-$100$ times lower than LoRA or OFT) across multiple image synthesis and natural language tasks without exhaustive hyperparameter tuning. Finally, we investigate the recent emphasis on Hyperspherical Energy retention for adaptation and raise questions on its practical utility. The code is available at https://github.com/mwbini/ether.","sentences":["Parameter-efficient finetuning (PEFT) has become ubiquitous to adapt foundation models to downstream task requirements while retaining their generalization ability.","However, the amount of additionally introduced parameters and compute for successful adaptation and hyperparameter searches can explode quickly, especially when deployed at scale to serve numerous individual requests.","To ensure effective, parameter-efficient, and hyperparameter-robust adaptation, we propose the ETHER transformation family, which performs Efficient fineTuning via HypErplane Reflections.","By design, ETHER transformations require a minimal number of parameters, are less likely to deteriorate model performance, and exhibit robustness to hyperparameter and learning rate choices.","In particular, we introduce ETHER and its relaxation ETHER+, which match or outperform existing PEFT methods with significantly fewer parameters ($\\sim$$10$-$100$ times lower than LoRA or OFT) across multiple image synthesis and natural language tasks without exhaustive hyperparameter tuning.","Finally, we investigate the recent emphasis on Hyperspherical Energy retention for adaptation and raise questions on its practical utility.","The code is available at https://github.com/mwbini/ether."],"url":"http://arxiv.org/abs/2405.20271v1","category":"cs.LG"}
{"created":"2024-05-30 17:24:52","title":"Bridging electronic and classical density-functional theory using universal machine-learned functional approximations","abstract":"The accuracy of density-functional theory (DFT) is determined by the quality of the approximate functionals, such as exchange-correlation in electronic DFT and the excess functional in the classical DFT formalism of fluids. The exact functional is highly nonlocal for both electrons and fluids, yet most approximate functionals are semi-local or nonlocal in a limited weighted-density form. Machine-learned (ML) nonlocal density-functional approximations are promising in both electronic and classical DFT, but have so far employed disparate approaches with limited generality. Here, we formulate a universal approximation framework and training protocol for nonlocal ML functionals, combining features of equivariant convolutional neural networks and the weighted-density approximation. We prototype this approach for several 1D and quasi-1D problems and demonstrate that a functional with exactly the same hyperparameters achieves excellent accuracy for the hard-rod fluid, the inhomogeneous Ising model, the exact exchange functional for electrons, the electron kinetic energy functional for orbital-free DFT, as well as for liquid water with 1D inhomogeneities. These results lay the foundation for a universal ML approach to exact 3D functionals spanning electronic and classical DFT.","sentences":["The accuracy of density-functional theory (DFT) is determined by the quality of the approximate functionals, such as exchange-correlation in electronic DFT and the excess functional in the classical DFT formalism of fluids.","The exact functional is highly nonlocal for both electrons and fluids, yet most approximate functionals are semi-local or nonlocal in a limited weighted-density form.","Machine-learned (ML) nonlocal density-functional approximations are promising in both electronic and classical DFT, but have so far employed disparate approaches with limited generality.","Here, we formulate a universal approximation framework and training protocol for nonlocal ML functionals, combining features of equivariant convolutional neural networks and the weighted-density approximation.","We prototype this approach for several 1D and quasi-1D problems and demonstrate that a functional with exactly the same hyperparameters achieves excellent accuracy for the hard-rod fluid, the inhomogeneous Ising model, the exact exchange functional for electrons, the electron kinetic energy functional for orbital-free DFT, as well as for liquid water with 1D inhomogeneities.","These results lay the foundation for a universal ML approach to exact 3D functionals spanning electronic and classical DFT."],"url":"http://arxiv.org/abs/2405.20270v1","category":"physics.chem-ph"}
{"created":"2024-05-30 17:16:01","title":"Geometric categorifications of Verma modules: Grassmannian Quiver Hecke algebras","abstract":"Naisse and Vaz defined an extension of KLR algebras to categorify Verma modules. We realise these algebras geometrically as convolution algebras in Borel--Moore homology. For this we introduce \\emph{Grassmannian--Steinberg quiver flag varieties}. They generalize Steinberg quiver flag varieties in a non-obvious way, reflecting the diagrammatics from the Naisse--Vaz construction. Using different kind of stratifications we provide geometric explanations of the rather mysterious algebraic and diagrammatic basis theorems.   A geometric categorification of Verma modules was recently found in the special case of $\\mathfrak{sl}_2$ by Rouquier. Rouquier's construction uses coherent sheaves on certain quasi-map spaces to flag varieties (zastavas), whereas our construction is implicitly based on perverse sheaves. Both should be seen as parts (on dual sides) of a general geometric framework for the Naisse--Vaz approach.   We first treat the (substantially easier) $\\mathfrak{sl}_2$ case in detail and construct as a byproduct a geometric dg-model of the nil-Hecke algebras. The extra difficulties we encounter in general require the use of more complicated Grassmannian--Steinberg quiver flag varieties. Their definition arises from combinatorially defined \\emph{diagram varieties} which we assign to each Naisse--Vaz basis diagram. Our explicit analysis here might shed some light on categories of coherent sheaves on more general zastava spaces studied by Feigin--Finkelberg--Kuznetsov--Mirkovi{\\'c} and Braverman, which we expect to occur in a generalization of Rouquier's construction away from $\\mathfrak{sl}_2$.","sentences":["Naisse and Vaz defined an extension of KLR algebras to categorify Verma modules.","We realise these algebras geometrically as convolution algebras in Borel--Moore homology.","For this we introduce \\emph{Grassmannian--Steinberg quiver flag varieties}.","They generalize Steinberg quiver flag varieties in a non-obvious way, reflecting the diagrammatics from the Naisse--Vaz construction.","Using different kind of stratifications we provide geometric explanations of the rather mysterious algebraic and diagrammatic basis theorems.   ","A geometric categorification of Verma modules was recently found in the special case of $\\mathfrak{sl}_2$ by Rouquier.","Rouquier's construction uses coherent sheaves on certain quasi-map spaces to flag varieties (zastavas), whereas our construction is implicitly based on perverse sheaves.","Both should be seen as parts (on dual sides) of a general geometric framework for the Naisse--Vaz approach.   ","We first treat the (substantially easier) $\\mathfrak{sl}_2$ case in detail and construct as a byproduct a geometric dg-model of the nil-Hecke algebras.","The extra difficulties we encounter in general require the use of more complicated Grassmannian--Steinberg quiver flag varieties.","Their definition arises from combinatorially defined \\emph{diagram varieties} which we assign to each Naisse--Vaz basis diagram.","Our explicit analysis here might shed some light on categories of coherent sheaves on more general zastava spaces studied by Feigin--Finkelberg--Kuznetsov--Mirkovi{\\'c} and Braverman, which we expect to occur in a generalization of Rouquier's construction away from $\\mathfrak{sl}_2$."],"url":"http://arxiv.org/abs/2405.20262v1","category":"math.RT"}
{"created":"2024-05-30 17:15:59","title":"Speed Profile Definition for GLOSA Implementation on Buses Based on Statistical Analysis of Experimental Data","abstract":"Intelligent Transportation Systems (ITS) are pushing an increasing interest and development when dealing with eco-driving systems. In this framework, this paper presents a method to define speed profiles specifically designed for Green Light Optimal Speed Advisory (GLOSA) systems on buses. GLOSA aims to optimize traffic flow by providing vehicles with real-time speed recommendations synchronized with traffic signal timings. Leveraging statistical analysis of experimental data collected from an urban bus, the study develops a methodology to extract meaningful insights into bus behaviour and traffic dynamics. The proposed approach considers road topology, scheduled bus stops, and signal timings to define simple although suitable speed profiles considering the peculiarities of the motion of a bus in an urban scenario. Through extensive data collection robust statistical data are defined, allowing the definition of vehicle motion profile for effectively develop and implement GLOSA systems. This research contributes to the advancement of Intelligent Transportation Systems by providing realistic data and practical insights for optimizing bus operations in urban environments.","sentences":["Intelligent Transportation Systems (ITS) are pushing an increasing interest and development when dealing with eco-driving systems.","In this framework, this paper presents a method to define speed profiles specifically designed for Green Light Optimal Speed Advisory (GLOSA) systems on buses.","GLOSA aims to optimize traffic flow by providing vehicles with real-time speed recommendations synchronized with traffic signal timings.","Leveraging statistical analysis of experimental data collected from an urban bus, the study develops a methodology to extract meaningful insights into bus behaviour and traffic dynamics.","The proposed approach considers road topology, scheduled bus stops, and signal timings to define simple although suitable speed profiles considering the peculiarities of the motion of a bus in an urban scenario.","Through extensive data collection robust statistical data are defined, allowing the definition of vehicle motion profile for effectively develop and implement GLOSA systems.","This research contributes to the advancement of Intelligent Transportation Systems by providing realistic data and practical insights for optimizing bus operations in urban environments."],"url":"http://arxiv.org/abs/2405.20261v1","category":"eess.SY"}
{"created":"2024-05-30 17:09:05","title":"FaceMixup: Enhancing Facial Expression Recognition through Mixed Face Regularization","abstract":"The proliferation of deep learning solutions and the scarcity of large annotated datasets pose significant challenges in real-world applications. Various strategies have been explored to overcome this challenge, with data augmentation (DA) approaches emerging as prominent solutions. DA approaches involve generating additional examples by transforming existing labeled data, thereby enriching the dataset and helping deep learning models achieve improved generalization without succumbing to overfitting. In real applications, where solutions based on deep learning are widely used, there is facial expression recognition (FER), which plays an essential role in human communication, improving a range of knowledge areas (e.g., medicine, security, and marketing). In this paper, we propose a simple and comprehensive face data augmentation approach based on mixed face component regularization that outperforms the classical DA approaches from the literature, including the MixAugment which is a specific approach for the target task in two well-known FER datasets existing in the literature.","sentences":["The proliferation of deep learning solutions and the scarcity of large annotated datasets pose significant challenges in real-world applications.","Various strategies have been explored to overcome this challenge, with data augmentation (DA) approaches emerging as prominent solutions.","DA approaches involve generating additional examples by transforming existing labeled data, thereby enriching the dataset and helping deep learning models achieve improved generalization without succumbing to overfitting.","In real applications, where solutions based on deep learning are widely used, there is facial expression recognition (FER), which plays an essential role in human communication, improving a range of knowledge areas (e.g., medicine, security, and marketing).","In this paper, we propose a simple and comprehensive face data augmentation approach based on mixed face component regularization that outperforms the classical DA approaches from the literature, including the MixAugment which is a specific approach for the target task in two well-known FER datasets existing in the literature."],"url":"http://arxiv.org/abs/2405.20259v1","category":"cs.CV"}
{"created":"2024-05-30 17:07:32","title":"Gravitational Lensing in More Realistic Dark Matter Halo Models","abstract":"In this study, we investigate gravitational lensing within the framework of more realistic dark matter halo models, transcending the limitations of spherical-collapse approximations. Through analytical computations utilizing diverse mass functions, we address critical factors typically overlooked in the standard Press-Schechter formalism, including ellipsoidal-collapse conditions, angular momentum dynamics, dynamical friction, and the cosmological constant. Our analysis incorporates two widely recognized halo density profiles, the Navarro-Frenk-White and Einasto profiles, considering both spherical and ellipsoidal-collapse scenarios. We present relevant calculations of pivotal gravitational lensing observables, such as Einstein radii, lensing optical depths, and time delays, spanning a wide range of redshifts and masses across two distinct lensing models: the point mass and singular isothermal sphere (SIS) lens models. Our findings illuminate that adopting more realistic dark matter halo models leads to heightened lensing effects compared to their spherical-collapse counterparts. Furthermore, our analyses of lensing optical depths and time delays reveal distinct characteristics between point mass and SIS lens models. These outcomes highlight the need for more realistic halo descriptions instead of simple approximations when modeling gravitational lensing, as this approach can potentially better reveal the complex structures of dark matter.","sentences":["In this study, we investigate gravitational lensing within the framework of more realistic dark matter halo models, transcending the limitations of spherical-collapse approximations.","Through analytical computations utilizing diverse mass functions, we address critical factors typically overlooked in the standard Press-Schechter formalism, including ellipsoidal-collapse conditions, angular momentum dynamics, dynamical friction, and the cosmological constant.","Our analysis incorporates two widely recognized halo density profiles, the Navarro-Frenk-White and Einasto profiles, considering both spherical and ellipsoidal-collapse scenarios.","We present relevant calculations of pivotal gravitational lensing observables, such as Einstein radii, lensing optical depths, and time delays, spanning a wide range of redshifts and masses across two distinct lensing models: the point mass and singular isothermal sphere (SIS) lens models.","Our findings illuminate that adopting more realistic dark matter halo models leads to heightened lensing effects compared to their spherical-collapse counterparts.","Furthermore, our analyses of lensing optical depths and time delays reveal distinct characteristics between point mass and SIS lens models.","These outcomes highlight the need for more realistic halo descriptions instead of simple approximations when modeling gravitational lensing, as this approach can potentially better reveal the complex structures of dark matter."],"url":"http://arxiv.org/abs/2405.20256v1","category":"astro-ph.CO"}
{"created":"2024-05-30 17:06:03","title":"Evaluating Large Language Model Biases in Persona-Steered Generation","abstract":"The task of persona-steered text generation requires large language models (LLMs) to generate text that reflects the distribution of views that an individual fitting a persona could have. People have multifaceted personas, but prior work on bias in LLM-generated opinions has only explored multiple-choice settings or one-dimensional personas. We define an incongruous persona as a persona with multiple traits where one trait makes its other traits less likely in human survey data, e.g. political liberals who support increased military spending. We find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance. Models that we evaluate that are fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas. We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation. Our results show the importance of evaluating models in open-ended text generation, as it can surface new LLM opinion biases. Moreover, such a setup can shed light on our ability to steer models toward a richer and more diverse range of viewpoints.","sentences":["The task of persona-steered text generation requires large language models (LLMs) to generate text that reflects the distribution of views that an individual fitting a persona could have.","People have multifaceted personas, but prior work on bias in LLM-generated opinions has only explored multiple-choice settings or one-dimensional personas.","We define an incongruous persona as a persona with multiple traits where one trait makes its other traits less likely in human survey data, e.g. political liberals who support increased military spending.","We find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance.","Models that we evaluate that are fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas.","We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation.","Our results show the importance of evaluating models in open-ended text generation, as it can surface new LLM opinion biases.","Moreover, such a setup can shed light on our ability to steer models toward a richer and more diverse range of viewpoints."],"url":"http://arxiv.org/abs/2405.20253v1","category":"cs.CL"}
{"created":"2024-05-30 17:05:45","title":"Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization","abstract":"Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications. Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly. Therefore, recent works have developed many strategies for improving the prompt, including both manual crafting and in-domain optimization. However, their efficacy in unrestricted scenarios remains questionable, as the former depends on human design for specific questions and the latter usually generalizes poorly to unseen scenarios. To address these problems, we give LLMs the freedom to design the best prompts according to themselves. Specifically, we include a hierarchy of LLMs, first constructing a prompt with precise instructions and accurate wording in a hierarchical manner, and then using this prompt to generate the final answer to the user query. We term this pipeline Hierarchical Multi-Agent Workflow, or HMAW. In contrast with prior works, HMAW imposes no human restriction and requires no training, and is completely task-agnostic while capable of adjusting to the nuances of the underlying task. Through both quantitative and qualitative experiments across multiple benchmarks, we verify that despite its simplicity, the proposed approach can create detailed and suitable prompts, further boosting the performance of current LLMs.","sentences":["Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications.","Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly.","Therefore, recent works have developed many strategies for improving the prompt, including both manual crafting and in-domain optimization.","However, their efficacy in unrestricted scenarios remains questionable, as the former depends on human design for specific questions and the latter usually generalizes poorly to unseen scenarios.","To address these problems, we give LLMs the freedom to design the best prompts according to themselves.","Specifically, we include a hierarchy of LLMs, first constructing a prompt with precise instructions and accurate wording in a hierarchical manner, and then using this prompt to generate the final answer to the user query.","We term this pipeline Hierarchical Multi-Agent Workflow, or HMAW.","In contrast with prior works, HMAW imposes no human restriction and requires no training, and is completely task-agnostic while capable of adjusting to the nuances of the underlying task.","Through both quantitative and qualitative experiments across multiple benchmarks, we verify that despite its simplicity, the proposed approach can create detailed and suitable prompts, further boosting the performance of current LLMs."],"url":"http://arxiv.org/abs/2405.20252v1","category":"cs.CL"}
{"created":"2024-05-30 17:02:18","title":"Entropy annealing for policy mirror descent in continuous time and space","abstract":"Entropy regularization has been extensively used in policy optimization algorithms to regularize the optimization landscape and accelerate convergence; however, it comes at the cost of introducing an additional regularization bias. This work quantifies the impact of entropy regularization on the convergence of policy gradient methods for stochastic exit time control problems. We analyze a continuous-time policy mirror descent dynamics, which updates the policy based on the gradient of an entropy-regularized value function and adjusts the strength of entropy regularization as the algorithm progresses. We prove that with a fixed entropy level, the dynamics converges exponentially to the optimal solution of the regularized problem. We further show that when the entropy level decays at suitable polynomial rates, the annealed flow converges to the solution of the unregularized problem at a rate of $\\mathcal O(1/S)$ for discrete action spaces and, under suitable conditions, at a rate of $\\mathcal O(1/\\sqrt{S})$ for general action spaces, with $S$ being the gradient flow time. This paper explains how entropy regularization improves policy optimization, even with the true gradient, from the perspective of convergence rate.","sentences":["Entropy regularization has been extensively used in policy optimization algorithms to regularize the optimization landscape and accelerate convergence; however, it comes at the cost of introducing an additional regularization bias.","This work quantifies the impact of entropy regularization on the convergence of policy gradient methods for stochastic exit time control problems.","We analyze a continuous-time policy mirror descent dynamics, which updates the policy based on the gradient of an entropy-regularized value function and adjusts the strength of entropy regularization as the algorithm progresses.","We prove that with a fixed entropy level, the dynamics converges exponentially to the optimal solution of the regularized problem.","We further show that when the entropy level decays at suitable polynomial rates, the annealed flow converges to the solution of the unregularized problem at a rate of $\\mathcal O(1/S)$ for discrete action spaces and, under suitable conditions, at a rate of $\\mathcal O(1/\\sqrt{S})$ for general action spaces, with $S$ being the gradient flow time.","This paper explains how entropy regularization improves policy optimization, even with the true gradient, from the perspective of convergence rate."],"url":"http://arxiv.org/abs/2405.20250v1","category":"math.OC"}
{"created":"2024-05-30 16:58:34","title":"KerasCV and KerasNLP: Vision and Language Power-Ups","abstract":"We present the Keras domain packages KerasCV and KerasNLP, extensions of the Keras API for Computer Vision and Natural Language Processing workflows, capable of running on either JAX, TensorFlow, or PyTorch. These domain packages are designed to enable fast experimentation, with a focus on ease-of-use and performance. We adopt a modular, layered design: at the library's lowest level of abstraction, we provide building blocks for creating models and data preprocessing pipelines, and at the library's highest level of abstraction, we provide pretrained ``task\" models for popular architectures such as Stable Diffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, T5, etc. Task models have built-in preprocessing, pretrained weights, and can be fine-tuned on raw inputs. To enable efficient training, we support XLA compilation for all models, and run all preprocessing via a compiled graph of TensorFlow operations using the tf.data API. The libraries are fully open-source (Apache 2.0 license) and available on GitHub.","sentences":["We present the Keras domain packages KerasCV and KerasNLP, extensions of the Keras API for Computer Vision and Natural Language Processing workflows, capable of running on either JAX, TensorFlow, or PyTorch.","These domain packages are designed to enable fast experimentation, with a focus on ease-of-use and performance.","We adopt a modular, layered design: at the library's lowest level of abstraction, we provide building blocks for creating models and data preprocessing pipelines, and at the library's highest level of abstraction, we provide pretrained ``task\" models for popular architectures such as Stable Diffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, T5, etc.","Task models have built-in preprocessing, pretrained weights, and can be fine-tuned on raw inputs.","To enable efficient training, we support XLA compilation for all models, and run all preprocessing via a compiled graph of TensorFlow operations using the tf.data API.","The libraries are fully open-source (Apache 2.0 license) and available on GitHub."],"url":"http://arxiv.org/abs/2405.20247v1","category":"cs.AI"}
{"created":"2024-05-30 16:55:05","title":"Detecting horizons of symmetric black holes using relative differential invariants","abstract":"Let $\\mathfrak{k}$ be a nontrivial finite-dimensional Lie algebra of vector fields on a manifold M, and consider the family of Lorentzian metrics on M whose Killing algebra contains $\\mathfrak{k}$. We show that scalar relative differential invariants, with respect to a Lie algebra of vector fields on M preserving $\\mathfrak{k}$, can be used to detect the horizons of several well-known black holes. In particular, using the Lie algebra structure of $\\mathfrak{k}$, we construct a general relative differential invariant of order 0 that always vanishes on $\\mathfrak{k}$-invariant Killing horizons.","sentences":["Let $\\mathfrak{k}$ be a nontrivial finite-dimensional Lie algebra of vector fields on a manifold M, and consider the family of Lorentzian metrics on M whose Killing algebra contains $\\mathfrak{k}$. We show that scalar relative differential invariants, with respect to a Lie algebra of vector fields on M preserving $\\mathfrak{k}$, can be used to detect the horizons of several well-known black holes.","In particular, using the Lie algebra structure of $\\mathfrak{k}$, we construct a general relative differential invariant of order 0 that always vanishes on $\\mathfrak{k}$-invariant Killing horizons."],"url":"http://arxiv.org/abs/2405.20246v1","category":"gr-qc"}
{"created":"2024-05-30 16:54:42","title":"Retrieval Augmented Structured Generation: Business Document Information Extraction As Tool Use","abstract":"Business Document Information Extraction (BDIE) is the problem of transforming a blob of unstructured information (raw text, scanned documents, etc.) into a structured format that downstream systems can parse and use. It has two main tasks: Key-Information Extraction (KIE) and Line Items Recognition (LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem, where the tools are these downstream systems. We then present Retrieval Augmented Structured Generation (RASG), a novel general framework for BDIE that achieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE benchmarks.   The contributions of this paper are threefold: (1) We show, with ablation benchmarks, that Large Language Models (LLMs) with RASG are already competitive with or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on BDIE benchmarks. (2) We propose a new metric class for Line Items Recognition, General Line Items Recognition Metric (GLIRM), that is more aligned with practical BDIE use cases compared to existing metrics, such as ANLS*, DocILE, and GriTS. (3) We provide a heuristic algorithm for backcalculating bounding boxes of predicted line items and tables without the need for vision encoders. Finally, we claim that, while LMMs might sometimes offer marginal performance benefits, LLMs + RASG is oftentimes superior given real-world applications and constraints of BDIE.","sentences":["Business Document Information Extraction (BDIE) is the problem of transforming a blob of unstructured information (raw text, scanned documents, etc.) into a structured format that downstream systems can parse and use.","It has two main tasks: Key-Information Extraction (KIE) and Line Items Recognition (LIR).","In this paper, we argue that BDIE is best modeled as a Tool Use problem, where the tools are these downstream systems.","We then present Retrieval Augmented Structured Generation (RASG), a novel general framework for BDIE that achieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE benchmarks.   ","The contributions of this paper are threefold: (1) We show, with ablation benchmarks, that Large Language Models (LLMs) with RASG are already competitive with or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on BDIE benchmarks.","(2) We propose a new metric class for Line Items Recognition, General Line Items Recognition Metric (GLIRM), that is more aligned with practical BDIE use cases compared to existing metrics, such as ANLS*, DocILE, and GriTS.","(3) We provide a heuristic algorithm for backcalculating bounding boxes of predicted line items and tables without the need for vision encoders.","Finally, we claim that, while LMMs might sometimes offer marginal performance benefits, LLMs + RASG is oftentimes superior given real-world applications and constraints of BDIE."],"url":"http://arxiv.org/abs/2405.20245v1","category":"cs.CL"}
{"created":"2024-05-30 16:50:37","title":"Chiral $\u039b$-$\\mathfrak{bms}_4$ symmetry of 3d conformal gravity","abstract":"We propose mixed boundary conditions for 3d conformal gravity consistent with variational principle in its second-order formalism that admit the chiral $\\Lambda$-$\\mathfrak{bms}_4$ algebra as their asymptotic symmetry algebra. This algebra is one of the four chiral $\\mathcal W$-algebra extensions of $\\mathfrak{so}(2,3)$ and is a generalisation of the chiral $\\mathfrak{bms}_4$ algebra responsible for soft theorems of graviton MHV amplitudes in ${\\mathbb R}^{1,3}$ gravity to the case of non-zero negative cosmological constant. The corresponding charges calculated using the modified covariant phase space formalism are shown to be finite and integrable, and realise this non-linear ${\\cal W}$-algebra.","sentences":["We propose mixed boundary conditions for 3d conformal gravity consistent with variational principle in its second-order formalism that admit the chiral $\\Lambda$-$\\mathfrak{bms}_4$ algebra as their asymptotic symmetry algebra.","This algebra is one of the four chiral $\\mathcal W$-algebra extensions of $\\mathfrak{so}(2,3)$ and is a generalisation of the chiral $\\mathfrak{bms}_4$ algebra responsible for soft theorems of graviton MHV amplitudes in ${\\mathbb R}^{1,3}$ gravity to the case of non-zero negative cosmological constant.","The corresponding charges calculated using the modified covariant phase space formalism are shown to be finite and integrable, and realise this non-linear ${\\cal W}$-algebra."],"url":"http://arxiv.org/abs/2405.20244v1","category":"hep-th"}
{"created":"2024-05-30 16:50:14","title":"MANTA: A Negative-Triangularity NASEM-Compliant Fusion Pilot Plant","abstract":"The MANTA (Modular Adjustable Negative Triangularity ARC-class) design study investigated how negative-triangularity (NT) may be leveraged in a compact, fusion pilot plant (FPP) to take a ``power-handling first\" approach. The result is a pulsed, radiative, ELM-free tokamak that satisfies and exceeds the FPP requirements described in the 2021 National Academies of Sciences, Engineering, and Medicine report ``Bringing Fusion to the U.S. Grid\". A self-consistent integrated modeling workflow predicts a fusion power of 450 MW and a plasma gain of 11.5 with only 23.5 MW of power to the scrape-off layer (SOL). This low $P_\\text{SOL}$ together with impurity seeding and high density at the separatrix results in a peak heat flux of just 2.8 MW/m$^{2}$. MANTA's high aspect ratio provides space for a large central solenoid (CS), resulting in ${\\sim}$15 minute inductive pulses. In spite of the high B fields on the CS and the other REBCO-based magnets, the electromagnetic stresses remain below structural and critical current density limits. Iterative optimization of neutron shielding and tritium breeding blanket yield tritium self-sufficiency with a breeding ratio of 1.15, a blanket power multiplication factor of 1.11, toroidal field coil lifetimes of $3100 \\pm 400$ MW-yr, and poloidal field coil lifetimes of at least $890 \\pm 40$ MW-yr. Following balance of plant modeling, MANTA is projected to generate 90 MW of net electricity at an electricity gain factor of ${\\sim}2.4$. Systems-level economic analysis estimates an overnight cost of US\\$3.4 billion, meeting the NASEM FPP requirement that this first-of-a-kind be less than US\\$5 billion. The toroidal field coil cost and replacement time are the most critical upfront and lifetime cost drivers, respectively.","sentences":["The MANTA (Modular Adjustable Negative Triangularity ARC-class) design study investigated how negative-triangularity (NT) may be leveraged in a compact, fusion pilot plant (FPP) to take a ``power-handling first\" approach.","The result is a pulsed, radiative, ELM-free tokamak that satisfies and exceeds the FPP requirements described in the 2021 National Academies of Sciences, Engineering, and Medicine report ``Bringing Fusion to the U.S. Grid\".","A self-consistent integrated modeling workflow predicts a fusion power of 450 MW and a plasma gain of 11.5 with only 23.5 MW of power to the scrape-off layer (SOL).","This low $P_\\text{SOL}$ together with impurity seeding and high density at the separatrix results in a peak heat flux of just 2.8 MW/m$^{2}$. MANTA's high aspect ratio provides space for a large central solenoid (CS), resulting in ${\\sim}$15 minute inductive pulses.","In spite of the high B fields on the CS and the other REBCO-based magnets, the electromagnetic stresses remain below structural and critical current density limits.","Iterative optimization of neutron shielding and tritium breeding blanket yield tritium self-sufficiency with a breeding ratio of 1.15, a blanket power multiplication factor of 1.11, toroidal field coil lifetimes of $3100 \\pm 400$ MW-yr, and poloidal field coil lifetimes of at least $890 \\pm 40$ MW-yr.","Following balance of plant modeling, MANTA is projected to generate 90 MW of net electricity at an electricity gain factor of ${\\sim}2.4$. Systems-level economic analysis estimates an overnight cost of US\\$3.4 billion, meeting the NASEM FPP requirement that this first-of-a-kind be less than US\\$5 billion.","The toroidal field coil cost and replacement time are the most critical upfront and lifetime cost drivers, respectively."],"url":"http://arxiv.org/abs/2405.20243v1","category":"physics.plasm-ph"}
{"created":"2024-05-30 16:45:32","title":"Decoherence-free many-body Hamiltonians in nonlinear waveguide quantum electrodynamics","abstract":"Enhancing interactions in many-body quantum systems, while protecting them from environmental decoherence, is at the heart of many quantum technologies. Waveguide quantum electrodynamics is a promising platform for achieving this, as it hosts infinite-range interactions and decoherence-free subspaces of quantum emitters. However, as coherent interactions between emitters are typically washed out in the wavelength-spacing regime hosting decoherence-free states, coherent control over the latter becomes limited, and many-body Hamiltonians in this important regime remain out of reach. Here we show that by incorporating emitter arrays with nonlinear waveguides hosting parametric gain, we obtain a unique class of many-body interaction Hamiltonians with coupling strengths that increase with emitter spacing, and persist even for wavelength-spaced arrays. We then propose to use these Hamiltonians to coherently generate decoherence-free states directly from the ground state, using only global squeezing drives, without the need for local addressing of individual emitters. Interestingly, we find that the dynamics approaches a unitary evolution in the limit of weak intra-waveguide squeezing, and discuss potential experimental realizations of this effect. Our results pave the way towards coherent control protocols in waveguide quantum electrodynamics, with applications including quantum computing, simulation, memory and nonclassical light generation.","sentences":["Enhancing interactions in many-body quantum systems, while protecting them from environmental decoherence, is at the heart of many quantum technologies.","Waveguide quantum electrodynamics is a promising platform for achieving this, as it hosts infinite-range interactions and decoherence-free subspaces of quantum emitters.","However, as coherent interactions between emitters are typically washed out in the wavelength-spacing regime hosting decoherence-free states, coherent control over the latter becomes limited, and many-body Hamiltonians in this important regime remain out of reach.","Here we show that by incorporating emitter arrays with nonlinear waveguides hosting parametric gain, we obtain a unique class of many-body interaction Hamiltonians with coupling strengths that increase with emitter spacing, and persist even for wavelength-spaced arrays.","We then propose to use these Hamiltonians to coherently generate decoherence-free states directly from the ground state, using only global squeezing drives, without the need for local addressing of individual emitters.","Interestingly, we find that the dynamics approaches a unitary evolution in the limit of weak intra-waveguide squeezing, and discuss potential experimental realizations of this effect.","Our results pave the way towards coherent control protocols in waveguide quantum electrodynamics, with applications including quantum computing, simulation, memory and nonclassical light generation."],"url":"http://arxiv.org/abs/2405.20241v1","category":"quant-ph"}
{"created":"2024-05-30 16:44:03","title":"Alleviating the $H_0$ and $\u03c3_8$ tensions in the interacting cubic covariant Galileon model","abstract":"The interaction between dark matter and dark energy has become a focal point in contemporary cosmological research, particularly in addressing current cosmological tensions. This study explores the cubic Galileon model's interaction with dark matter, where the interaction potential in the dark sector is proportional to the dark energy density of the Galileon field. By employing dimensionless variables, we transform the field equations into an autonomous dynamical system. We calculate the critical points of the corresponding autonomous systems and demonstrate the existence of a stable de Sitter epoch. Our investigation proceeds in two phases. First, we conduct a detailed analysis of the exact interacting cubic Galileon (ICG) model, derived from the precise solution of the equations of motion. Second, we explore an approximate tracker solution, labeled TICG, assuming a small coupling parameter between dark matter and dark energy. We evaluate the evolution of these models using data from two experiments, aiming to resolve the tensions surrounding $H_0$ and $S_8$. The analysis of the TICG model indicates a preference for a phantom regime and provides a negative coupling parameter in the dark sector at a $68\\%$ confidence level. This model also shows that the current tensions regarding $H_0$ and $S_8$ are alleviated. Conversely, the ICG model, despite its preference for the phantom regime, is plagued by an excess in today's matter density and a higher expansion rate, easing only the $H_0$ tension.","sentences":["The interaction between dark matter and dark energy has become a focal point in contemporary cosmological research, particularly in addressing current cosmological tensions.","This study explores the cubic Galileon model's interaction with dark matter, where the interaction potential in the dark sector is proportional to the dark energy density of the Galileon field.","By employing dimensionless variables, we transform the field equations into an autonomous dynamical system.","We calculate the critical points of the corresponding autonomous systems and demonstrate the existence of a stable de Sitter epoch.","Our investigation proceeds in two phases.","First, we conduct a detailed analysis of the exact interacting cubic Galileon (ICG) model, derived from the precise solution of the equations of motion.","Second, we explore an approximate tracker solution, labeled TICG, assuming a small coupling parameter between dark matter and dark energy.","We evaluate the evolution of these models using data from two experiments, aiming to resolve the tensions surrounding $H_0$ and $S_8$. The analysis of the TICG model indicates a preference for a phantom regime and provides a negative coupling parameter in the dark sector at a $68\\%$ confidence level.","This model also shows that the current tensions regarding $H_0$ and $S_8$ are alleviated.","Conversely, the ICG model, despite its preference for the phantom regime, is plagued by an excess in today's matter density and a higher expansion rate, easing only the $H_0$ tension."],"url":"http://arxiv.org/abs/2405.20240v1","category":"gr-qc"}
{"created":"2024-05-30 16:41:25","title":"A relativistic statistical field theory on a discretized Minkowski lattice with quantum-like observables","abstract":"A relativistic statistical field theory is constructed for a fluctuating complex-valued scalar field on a discretized Minkowski lattice. A Hilbert space of observables is then constructed from functionals of the fluctuating complex-valued scalar field with an inner product defined in terms of expectation values of the functionals. A bosonic Fock space is then constructed from the Hilbert space and creation and annihilation operators that act on the Fock space are defined. The creation and annihilation operators are used to define field operators. These field operators have some interesting quantum-like properties. For example, the field operators do not commute in general and, in the particular case of the free field theory, can be shown to satisfy the microcausality condition.","sentences":["A relativistic statistical field theory is constructed for a fluctuating complex-valued scalar field on a discretized Minkowski lattice.","A Hilbert space of observables is then constructed from functionals of the fluctuating complex-valued scalar field with an inner product defined in terms of expectation values of the functionals.","A bosonic Fock space is then constructed from the Hilbert space and creation and annihilation operators that act on the Fock space are defined.","The creation and annihilation operators are used to define field operators.","These field operators have some interesting quantum-like properties.","For example, the field operators do not commute in general and, in the particular case of the free field theory, can be shown to satisfy the microcausality condition."],"url":"http://arxiv.org/abs/2405.20238v1","category":"quant-ph"}
{"created":"2024-05-30 16:40:28","title":"Training-efficient density quantum machine learning","abstract":"Quantum machine learning requires powerful, flexible and efficiently trainable models to be successful in solving challenging problems. In this work, we present density quantum neural networks, a learning model incorporating randomisation over a set of trainable unitaries. These models generalise quantum neural networks using parameterised quantum circuits, and allow a trade-off between expressibility and efficient trainability, particularly on quantum hardware. We demonstrate the flexibility of the formalism by applying it to two recently proposed model families. The first are commuting-block quantum neural networks (QNNs) which are efficiently trainable but may be limited in expressibility. The second are orthogonal (Hamming-weight preserving) quantum neural networks which provide well-defined and interpretable transformations on data but are challenging to train at scale on quantum devices. Density commuting QNNs improve capacity with minimal gradient complexity overhead, and density orthogonal neural networks admit a quadratic-to-constant gradient query advantage with minimal to no performance loss. We conduct numerical experiments on synthetic translationally invariant data and MNIST image data with hyperparameter optimisation to support our findings. Finally, we discuss the connection to post-variational quantum neural networks, measurement-based quantum machine learning and the dropout mechanism.","sentences":["Quantum machine learning requires powerful, flexible and efficiently trainable models to be successful in solving challenging problems.","In this work, we present density quantum neural networks, a learning model incorporating randomisation over a set of trainable unitaries.","These models generalise quantum neural networks using parameterised quantum circuits, and allow a trade-off between expressibility and efficient trainability, particularly on quantum hardware.","We demonstrate the flexibility of the formalism by applying it to two recently proposed model families.","The first are commuting-block quantum neural networks (QNNs) which are efficiently trainable but may be limited in expressibility.","The second are orthogonal (Hamming-weight preserving) quantum neural networks which provide well-defined and interpretable transformations on data but are challenging to train at scale on quantum devices.","Density commuting QNNs improve capacity with minimal gradient complexity overhead, and density orthogonal neural networks admit a quadratic-to-constant gradient query advantage with minimal to no performance loss.","We conduct numerical experiments on synthetic translationally invariant data and MNIST image data with hyperparameter optimisation to support our findings.","Finally, we discuss the connection to post-variational quantum neural networks, measurement-based quantum machine learning and the dropout mechanism."],"url":"http://arxiv.org/abs/2405.20237v1","category":"quant-ph"}
{"created":"2024-05-30 16:40:07","title":"Disentangling and Mitigating the Impact of Task Similarity for Continual Learning","abstract":"Continual learning of partially similar tasks poses a challenge for artificial neural networks, as task similarity presents both an opportunity for knowledge transfer and a risk of interference and catastrophic forgetting. However, it remains unclear how task similarity in input features and readout patterns influences knowledge transfer and forgetting, as well as how they interact with common algorithms for continual learning. Here, we develop a linear teacher-student model with latent structure and show analytically that high input feature similarity coupled with low readout similarity is catastrophic for both knowledge transfer and retention. Conversely, the opposite scenario is relatively benign. Our analysis further reveals that task-dependent activity gating improves knowledge retention at the expense of transfer, while task-dependent plasticity gating does not affect either retention or transfer performance at the over-parameterized limit. In contrast, weight regularization based on the Fisher information metric significantly improves retention, regardless of task similarity, without compromising transfer performance. Nevertheless, its diagonal approximation and regularization in the Euclidean space are much less robust against task similarity. We demonstrate consistent results in a permuted MNIST task with latent variables. Overall, this work provides insights into when continual learning is difficult and how to mitigate it.","sentences":["Continual learning of partially similar tasks poses a challenge for artificial neural networks, as task similarity presents both an opportunity for knowledge transfer and a risk of interference and catastrophic forgetting.","However, it remains unclear how task similarity in input features and readout patterns influences knowledge transfer and forgetting, as well as how they interact with common algorithms for continual learning.","Here, we develop a linear teacher-student model with latent structure and show analytically that high input feature similarity coupled with low readout similarity is catastrophic for both knowledge transfer and retention.","Conversely, the opposite scenario is relatively benign.","Our analysis further reveals that task-dependent activity gating improves knowledge retention at the expense of transfer, while task-dependent plasticity gating does not affect either retention or transfer performance at the over-parameterized limit.","In contrast, weight regularization based on the Fisher information metric significantly improves retention, regardless of task similarity, without compromising transfer performance.","Nevertheless, its diagonal approximation and regularization in the Euclidean space are much less robust against task similarity.","We demonstrate consistent results in a permuted MNIST task with latent variables.","Overall, this work provides insights into when continual learning is difficult and how to mitigate it."],"url":"http://arxiv.org/abs/2405.20236v1","category":"stat.ML"}
{"created":"2024-05-30 16:39:02","title":"Generalized Cluster Correlation Expansion theory for STIRAP processes in the presence of a spin bath","abstract":"The Stimulated Raman Adiabatic Passage (STIRAP) is applied to a system coupled to a bath made of fully-interacting two-level systems, whose dynamics is studied exploiting the generalized Cluster Correlation Expansion (gCCE) theory. We specialize our analysis to a negatively charged silicon vacancy (SiV-1) in non-purified 4H-SiC to assess the possibility of transferring population between two states of the ground manifold, also taking into account the interaction with a spherical nuclear spin bath formed by nuclei of 29Si and 13C. For this system, it is demonstrated that the presence of a small/medium sized bath has no effect on the protocol, finding in particular a set of parameter values for an efficient STIRAP process.","sentences":["The Stimulated Raman Adiabatic Passage (STIRAP) is applied to a system coupled to a bath made of fully-interacting two-level systems, whose dynamics is studied exploiting the generalized Cluster Correlation Expansion (gCCE) theory.","We specialize our analysis to a negatively charged silicon vacancy (SiV-1) in non-purified 4H-SiC to assess the possibility of transferring population between two states of the ground manifold, also taking into account the interaction with a spherical nuclear spin bath formed by nuclei of 29Si and 13C. For this system, it is demonstrated that the presence of a small/medium sized bath has no effect on the protocol, finding in particular a set of parameter values for an efficient STIRAP process."],"url":"http://arxiv.org/abs/2405.20235v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 16:36:47","title":"Context Injection Attacks on Large Language Models","abstract":"Large Language Models (LLMs) such as ChatGPT and Llama-2 have become prevalent in real-world applications, exhibiting impressive text generation performance. LLMs are fundamentally developed from a scenario where the input data remains static and lacks a clear structure. To behave interactively over time, LLM-based chat systems must integrate additional contextual information (i.e., chat history) into their inputs, following a pre-defined structure. This paper identifies how such integration can expose LLMs to misleading context from untrusted sources and fail to differentiate between system and user inputs, allowing users to inject context. We present a systematic methodology for conducting context injection attacks aimed at eliciting disallowed responses by introducing fabricated context. This could lead to illegal actions, inappropriate content, or technology misuse. Our context fabrication strategies, acceptance elicitation and word anonymization, effectively create misleading contexts that can be structured with attacker-customized prompt templates, achieving injection through malicious user messages. Comprehensive evaluations on real-world LLMs such as ChatGPT and Llama-2 confirm the efficacy of the proposed attack with success rates reaching 97%. We also discuss potential countermeasures that can be adopted for attack detection and developing more secure models. Our findings provide insights into the challenges associated with the real-world deployment of LLMs for interactive and structured data scenarios.","sentences":["Large Language Models (LLMs) such as ChatGPT and Llama-2 have become prevalent in real-world applications, exhibiting impressive text generation performance.","LLMs are fundamentally developed from a scenario where the input data remains static and lacks a clear structure.","To behave interactively over time, LLM-based chat systems must integrate additional contextual information (i.e., chat history) into their inputs, following a pre-defined structure.","This paper identifies how such integration can expose LLMs to misleading context from untrusted sources and fail to differentiate between system and user inputs, allowing users to inject context.","We present a systematic methodology for conducting context injection attacks aimed at eliciting disallowed responses by introducing fabricated context.","This could lead to illegal actions, inappropriate content, or technology misuse.","Our context fabrication strategies, acceptance elicitation and word anonymization, effectively create misleading contexts that can be structured with attacker-customized prompt templates, achieving injection through malicious user messages.","Comprehensive evaluations on real-world LLMs such as ChatGPT and Llama-2 confirm the efficacy of the proposed attack with success rates reaching 97%.","We also discuss potential countermeasures that can be adopted for attack detection and developing more secure models.","Our findings provide insights into the challenges associated with the real-world deployment of LLMs for interactive and structured data scenarios."],"url":"http://arxiv.org/abs/2405.20234v1","category":"cs.AI"}
{"created":"2024-05-30 16:35:30","title":"Grokfast: Accelerated Grokking by Amplifying Slow Gradients","abstract":"One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than $\\times 50$ with only a few lines of code that amplifies the slow-varying components of gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization. Our code is available at \\url{https://github.com/ironjr/grokfast}.","sentences":["One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data.","Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon.","By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component.","This analysis allows us to accelerate the grokking phenomenon more than $\\times 50$ with only a few lines of code that amplifies the slow-varying components of gradients.","The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization.","Our code is available at \\url{https://github.com/ironjr/grokfast}."],"url":"http://arxiv.org/abs/2405.20233v1","category":"cs.LG"}
{"created":"2024-05-30 16:32:31","title":"The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof","abstract":"Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries -- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenomena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries. With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training.","sentences":["Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries -- transformations of neural network parameters that do not change the underlying neural network function.","These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes.","However, theoretical analysis of the relationship between parameter space symmetries and these phenomena is difficult.","In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries.","We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries.","With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries.","Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training."],"url":"http://arxiv.org/abs/2405.20231v1","category":"cs.LG"}
{"created":"2024-05-30 16:32:08","title":"Love-C relations for elastic hybrid stars","abstract":"Neutron stars (NSs) provide a unique laboratory to study matter under extreme densities. Recent observations from gravitational and electromagnetic waves have enabled constraints on NS properties, such as tidal deformability (related to the tidal Love number) and stellar compactness. Although each of these two NS observables depends strongly on the stellar internal structure, the relation between them (called the Love-C relation) is known to be equation-of-state insensitive. In this study, we investigate the effects of a possible crystalline phase in the core of hybrid stars (HSs) on the mass-radius and Love-C relations, where HSs are a subclass of NS models with a quark matter core and a nuclear matter envelope with a sharp phase transition in between. We find that both the maximum mass and the corresponding radius increase as one increases the stiffness of the quark matter core controlled by the speed of sound, while the density discontinuity at the nuclear-quark matter transition effectively softens the equations of state. Deviations of the Love-C relation for elastic HSs from that of fluid NSs become more pronounced with a larger shear modulus, lower transition pressure, and larger density gap and can be as large as 60%. These findings suggest a potential method for testing the existence of distinct phases within HSs, though deviations are not large enough to be detected with current measurements of the tidal deformability and compactness.","sentences":["Neutron stars (NSs) provide a unique laboratory to study matter under extreme densities.","Recent observations from gravitational and electromagnetic waves have enabled constraints on NS properties, such as tidal deformability (related to the tidal Love number) and stellar compactness.","Although each of these two NS observables depends strongly on the stellar internal structure, the relation between them (called the Love-C relation) is known to be equation-of-state insensitive.","In this study, we investigate the effects of a possible crystalline phase in the core of hybrid stars (HSs) on the mass-radius and Love-C relations, where HSs are a subclass of NS models with a quark matter core and a nuclear matter envelope with a sharp phase transition in between.","We find that both the maximum mass and the corresponding radius increase as one increases the stiffness of the quark matter core controlled by the speed of sound, while the density discontinuity at the nuclear-quark matter transition effectively softens the equations of state.","Deviations of the Love-C relation for elastic HSs from that of fluid NSs become more pronounced with a larger shear modulus, lower transition pressure, and larger density gap and can be as large as 60%.","These findings suggest a potential method for testing the existence of distinct phases within HSs, though deviations are not large enough to be detected with current measurements of the tidal deformability and compactness."],"url":"http://arxiv.org/abs/2405.20228v1","category":"gr-qc"}
{"created":"2024-05-30 16:30:16","title":"Novel oracle constructions for quantum random access memory","abstract":"We present novel ways of designing quantum (random access) memory, also known as quantum dictionary encoders or data-access oracles. More precisely, given a function, $f : \\mathbb{F}_2^n \\rightarrow \\mathbb{F}_2^d$, we construct oracles, $\\mathcal{O}_f$, with the property $\\mathcal{O}_f |x\\rangle_n |0\\rangle_d = |x\\rangle_n |f(x)\\rangle_d$. Our constructions are based on the Walsh--Hadamard transform of $f$, viewed as an integer valued function. In general, the complexity of our method scales with the sparsity of the Walsh--Hadamard transform and not the sparsity of $f$, yielding more favorable constructions in cases such as binary optimization problems and function with low-degree Walsh--Hadamard Transforms. Our design comes with a tuneable amount of ancillas that can trade depth for size. In the ancillas-free design, these oracles can be $\\epsilon$-approximated so that the Clifford $+$ $T$ depth is $O \\left( \\left( n + \\log_2\\left( \\tfrac{d}{\\epsilon} \\right) \\right) \\mathcal{W}_f \\right)$, where $\\mathcal{W}_f$ is the number of nonzero components in the Walsh--Hadamard Transform. The depth of the shallowest design is $O \\left( \\log_2 \\left( \\mathcal{W}_f \\right) + \\log_2 \\left( \\tfrac{d}{\\epsilon} \\right) \\right)$, using $n + d \\mathcal{W}_f$ qubits.","sentences":["We present novel ways of designing quantum (random access) memory, also known as quantum dictionary encoders or data-access oracles.","More precisely, given a function, $f : \\mathbb{F}_2^n \\rightarrow \\mathbb{F}_2^d$, we construct oracles, $\\mathcal{O}_f$, with the property $\\mathcal{O}_f |x\\rangle_n |0\\rangle_d = |x\\rangle_n |f(x)\\rangle_d$.","Our constructions are based on the Walsh--Hadamard transform of $f$, viewed as an integer valued function.","In general, the complexity of our method scales with the sparsity of the Walsh--Hadamard transform and not the sparsity of $f$, yielding more favorable constructions in cases such as binary optimization problems and function with low-degree Walsh--Hadamard Transforms.","Our design comes with a tuneable amount of ancillas that can trade depth for size.","In the ancillas-free design, these oracles can be $\\epsilon$-approximated so that the Clifford $+$ $T$ depth is $O \\left( \\left( n + \\log_2\\left( \\tfrac{d}{\\epsilon} \\right) \\right) \\mathcal{W}_f","\\right)$, where $\\mathcal{W}_f$ is the number of nonzero components in the Walsh--Hadamard Transform.","The depth of the shallowest design is $O \\left( \\log_2 \\left( \\mathcal{W}_f \\right) + \\log_2 \\left( \\tfrac{d}{\\epsilon} \\right) \\right)$, using $n + d \\mathcal{W}_f$ qubits."],"url":"http://arxiv.org/abs/2405.20225v1","category":"quant-ph"}
{"created":"2024-05-30 16:22:22","title":"MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model","abstract":"We present MOFA-Video, an advanced controllable image animation method that generates video from the given image using various additional controllable signals (such as human landmarks reference, manual trajectories, and another even provided video) or their combinations. This is different from previous methods which only can work on a specific motion domain or show weak control abilities with diffusion prior. To achieve our goal, we design several domain-aware motion field adapters (\\ie, MOFA-Adapters) to control the generated motions in the video generation pipeline. For MOFA-Adapters, we consider the temporal motion consistency of the video and generate the dense motion flow from the given sparse control conditions first, and then, the multi-scale features of the given image are wrapped as a guided feature for stable video diffusion generation. We naively train two motion adapters for the manual trajectories and the human landmarks individually since they both contain sparse information about the control. After training, the MOFA-Adapters in different domains can also work together for more controllable video generation.","sentences":["We present MOFA-Video, an advanced controllable image animation method that generates video from the given image using various additional controllable signals (such as human landmarks reference, manual trajectories, and another even provided video) or their combinations.","This is different from previous methods which only can work on a specific motion domain or show weak control abilities with diffusion prior.","To achieve our goal, we design several domain-aware motion field adapters (\\ie, MOFA-Adapters) to control the generated motions in the video generation pipeline.","For MOFA-Adapters, we consider the temporal motion consistency of the video and generate the dense motion flow from the given sparse control conditions first, and then, the multi-scale features of the given image are wrapped as a guided feature for stable video diffusion generation.","We naively train two motion adapters for the manual trajectories and the human landmarks individually since they both contain sparse information about the control.","After training, the MOFA-Adapters in different domains can also work together for more controllable video generation."],"url":"http://arxiv.org/abs/2405.20222v1","category":"cs.CV"}
{"created":"2024-05-30 16:22:08","title":"On the generalization of the study of a letter power substitution on modulo-recurrent words","abstract":"Let us consider an infinite word and $k\\geq 1$ an integer. By steps of $k$, we substitute a letter ofthis infinite word by the power of an external letter. The new word obtaining by this process is called $k$ to $k$ substitution of a power letter. After the application of this new notion on modulo-reccurent words and in particular on Sturmian words. We establish the complexity function of those words.","sentences":["Let us consider an infinite word and $k\\geq 1$ an integer.","By steps of $k$, we substitute a letter ofthis infinite word by the power of an external letter.","The new word obtaining by this process is called $k$ to $k$ substitution of a power letter.","After the application of this new notion on modulo-reccurent words and in particular on Sturmian words.","We establish the complexity function of those words."],"url":"http://arxiv.org/abs/2405.20221v1","category":"math.CO"}
{"created":"2024-05-30 16:19:02","title":"ESG-FTSE: A corpus of news articles with ESG relevance labels and use cases","abstract":"We present ESG-FTSE, the first corpus comprised of news articles with Environmental, Social and Governance (ESG) relevance annotations. In recent years, investors and regulators have pushed ESG investing to the mainstream due to the urgency of climate change. This has led to the rise of ESG scores to evaluate an investment's credentials as socially responsible. While demand for ESG scores is high, their quality varies wildly. Quantitative techniques can be applied to improve ESG scores, thus, responsible investing. To contribute to resource building for ESG and financial text mining, we pioneer the ESG-FTSE corpus. We further present the first of its kind ESG annotation schema. It has three levels: a binary classification (relevant versus irrelevant news articles), ESG classification (ESG-related news articles), and target company. Both supervised and unsupervised learning experiments for ESG relevance detection were conducted to demonstrate that the corpus can be used in different settings to derive accurate ESG predictions. Keywords: corpus annotation, ESG labels, annotation schema, news article, natural language processing","sentences":["We present ESG-FTSE, the first corpus comprised of news articles with Environmental, Social and Governance (ESG) relevance annotations.","In recent years, investors and regulators have pushed ESG investing to the mainstream due to the urgency of climate change.","This has led to the rise of ESG scores to evaluate an investment's credentials as socially responsible.","While demand for ESG scores is high, their quality varies wildly.","Quantitative techniques can be applied to improve ESG scores, thus, responsible investing.","To contribute to resource building for ESG and financial text mining, we pioneer the ESG-FTSE corpus.","We further present the first of its kind ESG annotation schema.","It has three levels: a binary classification (relevant versus irrelevant news articles), ESG classification (ESG-related news articles), and target company.","Both supervised and unsupervised learning experiments for ESG relevance detection were conducted to demonstrate that the corpus can be used in different settings to derive accurate ESG predictions.","Keywords: corpus annotation, ESG labels, annotation schema, news article, natural language processing"],"url":"http://arxiv.org/abs/2405.20218v1","category":"cs.AI"}
{"created":"2024-05-30 16:18:05","title":"Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback","abstract":"The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.","sentences":["The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task.","Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results.","Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences.","In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO).","Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback.","We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity.","Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation.","Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment."],"url":"http://arxiv.org/abs/2405.20216v1","category":"cs.CV"}
{"created":"2024-05-30 16:17:40","title":"TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models","abstract":"Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the \"TS-Align\" framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model. The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment.","sentences":["Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates.","The standard process for iterative alignment of LLMs involves collecting new human feedback for each update.","However, the data collection process is costly and challenging to scale.","To address this issue, we introduce the \"TS-Align\" framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs.","This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model.","The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework.","Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets.","Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment."],"url":"http://arxiv.org/abs/2405.20215v1","category":"cs.CL"}
{"created":"2024-05-30 16:16:25","title":"PostDoc: Generating Poster from a Long Multimodal Document Using Deep Submodular Optimization","abstract":"A poster from a long input document can be considered as a one-page easy-to-read multimodal (text and images) summary presented on a nice template with good design elements. Automatic transformation of a long document into a poster is a very less studied but challenging task. It involves content summarization of the input document followed by template generation and harmonization. In this work, we propose a novel deep submodular function which can be trained on ground truth summaries to extract multimodal content from the document and explicitly ensures good coverage, diversity and alignment of text and images. Then, we use an LLM based paraphraser and propose to generate a template with various design aspects conditioned on the input content. We show the merits of our approach through extensive automated and human evaluations.","sentences":["A poster from a long input document can be considered as a one-page easy-to-read multimodal (text and images) summary presented on a nice template with good design elements.","Automatic transformation of a long document into a poster is a very less studied but challenging task.","It involves content summarization of the input document followed by template generation and harmonization.","In this work, we propose a novel deep submodular function which can be trained on ground truth summaries to extract multimodal content from the document and explicitly ensures good coverage, diversity and alignment of text and images.","Then, we use an LLM based paraphraser and propose to generate a template with various design aspects conditioned on the input content.","We show the merits of our approach through extensive automated and human evaluations."],"url":"http://arxiv.org/abs/2405.20213v1","category":"cs.AI"}
{"created":"2024-05-30 16:15:41","title":"Low-rank and sparse approximations for contact mechanics","abstract":"(Rephrased) Non-conformance decision-making processes in high-precision manufacturing of engineering structures are often delayed due to numerical simulations that are needed for analyzing the defective parts and assemblies. Interfaces between parts of assemblies can only be simulated using the modeling of contact. Thus, efficient parametric ROMs are necessary for performing contact mechanics simulations in near real-time scenarios. Typical strategies for reducing the cost of contact models use low-rank approximations. Assumptions include the existence of a low-dimensional subspace for displacement and a low-dimensional non-negative subcone for contact pressure. However, the contact pressure exhibits a local nature, as the position of contact can vary with parameters like loading or geometry. The adequacy of low-rank approximations for contact mechanics is investigated and alternative routes based on sparse regression techniques are explored. It is shown that the local nature leads to loss of linear separability of contact pressure, thereby limiting the accuracy of low-rank methods. The applicability of the low-rank assumption to contact pressure is analyzed using 3 different criteria: compactness, generalization and specificity. Subsequently, over-complete dictionaries with a large number of snapshots to mitigate the inseparability issues is investigated. Two strategies are devised: a greedy active-set method where the dictionary elements are selected greedily and a convex hull approximation method that eliminates the necessity of explicitly enforcing non-penetration constraints in convex problems. Lastly, Dynamic Time Warping is studied as a possible non-linear interpolation method that permits the exploration of the non-linear manifoldm synthesising snapshots not computed in the training set with low complexity; reducing the offline costs.","sentences":["(Rephrased) Non-conformance decision-making processes in high-precision manufacturing of engineering structures are often delayed due to numerical simulations that are needed for analyzing the defective parts and assemblies.","Interfaces between parts of assemblies can only be simulated using the modeling of contact.","Thus, efficient parametric ROMs are necessary for performing contact mechanics simulations in near real-time scenarios.","Typical strategies for reducing the cost of contact models use low-rank approximations.","Assumptions include the existence of a low-dimensional subspace for displacement and a low-dimensional non-negative subcone for contact pressure.","However, the contact pressure exhibits a local nature, as the position of contact can vary with parameters like loading or geometry.","The adequacy of low-rank approximations for contact mechanics is investigated and alternative routes based on sparse regression techniques are explored.","It is shown that the local nature leads to loss of linear separability of contact pressure, thereby limiting the accuracy of low-rank methods.","The applicability of the low-rank assumption to contact pressure is analyzed using 3 different criteria: compactness, generalization and specificity.","Subsequently, over-complete dictionaries with a large number of snapshots to mitigate the inseparability issues is investigated.","Two strategies are devised: a greedy active-set method where the dictionary elements are selected greedily and a convex hull approximation method that eliminates the necessity of explicitly enforcing non-penetration constraints in convex problems.","Lastly, Dynamic Time Warping is studied as a possible non-linear interpolation method that permits the exploration of the non-linear manifoldm synthesising snapshots not computed in the training set with low complexity; reducing the offline costs."],"url":"http://arxiv.org/abs/2405.20211v1","category":"math.NA"}
{"created":"2024-05-30 16:14:05","title":"Proper time path integrals for gravitational waves: an improved wave optics framework","abstract":"When gravitational waves travel from their source to an observer, they interact with matter structures along their path, causing distinct deformations in their waveforms. In this study we introduce a novel theoretical framework for wave optics effects in gravitational lensing, addressing the limitations of existing approaches. We achieve this by incorporating the proper time technique, typically used in field theory studies, into gravitational lensing. This approach allows us to extend the standard formalism beyond the eikonal and paraxial approximations, which are traditionally assumed, and to account for polarization effects, which are typically neglected in the literature. We demonstrate that our method provides a robust generalization of conventional approaches, including them as special cases. Our findings enhance our understanding of gravitational wave propagation, which is crucial for accurately interpreting gravitational wave observations and extracting unbiased information about the lenses from the gravitational wave waveforms.","sentences":["When gravitational waves travel from their source to an observer, they interact with matter structures along their path, causing distinct deformations in their waveforms.","In this study we introduce a novel theoretical framework for wave optics effects in gravitational lensing, addressing the limitations of existing approaches.","We achieve this by incorporating the proper time technique, typically used in field theory studies, into gravitational lensing.","This approach allows us to extend the standard formalism beyond the eikonal and paraxial approximations, which are traditionally assumed, and to account for polarization effects, which are typically neglected in the literature.","We demonstrate that our method provides a robust generalization of conventional approaches, including them as special cases.","Our findings enhance our understanding of gravitational wave propagation, which is crucial for accurately interpreting gravitational wave observations and extracting unbiased information about the lenses from the gravitational wave waveforms."],"url":"http://arxiv.org/abs/2405.20208v1","category":"astro-ph.CO"}
{"created":"2024-05-30 16:07:54","title":"Jina CLIP: Your CLIP Model Is Also Your Text Retriever","abstract":"Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.","sentences":["Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors.","These models are key to multimodal information retrieval and related tasks.","However, CLIP models generally underperform in text-only tasks compared to specialized text models.","This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks.","We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks."],"url":"http://arxiv.org/abs/2405.20204v1","category":"cs.CL"}
{"created":"2024-05-30 16:05:15","title":"One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments","abstract":"Large Language Models (LLMs) have advanced rapidly but face significant memory demands. While quantization has shown promise for LLMs, current methods typically require lengthy training to alleviate the performance degradation from quantization loss. However, deploying LLMs across diverse scenarios with different resource constraints, e.g., servers and personal computers, requires repeated training per application, which amplifies the lengthy training problem. Given that, it is advantageous to train a once-for-all (OFA) supernet capable of yielding diverse optimal subnets for downstream applications through one-shot training. Nonetheless, the scale of current language models impedes efficiency and amplifies interference from weight sharing between subnets. We make an initial attempt to extend the once-for-all framework to large language models. Specifically, we decouple shared weights to eliminate the interference and incorporate Low-Rank adapters for training efficiency. Furthermore, we observe the imbalance allocation of training resources from the traditional uniform sampling. A non-parametric scheduler is introduced to adjust the sampling rate for each quantization configuration, achieving a more balanced allocation among subnets with varying demands. We validate the approach on LLaMA2 families, and downstream evaluation confirms our ability to maintain high performance while significantly reducing deployment time faced with multiple scenarios.","sentences":["Large Language Models (LLMs) have advanced rapidly but face significant memory demands.","While quantization has shown promise for LLMs, current methods typically require lengthy training to alleviate the performance degradation from quantization loss.","However, deploying LLMs across diverse scenarios with different resource constraints, e.g., servers and personal computers, requires repeated training per application, which amplifies the lengthy training problem.","Given that, it is advantageous to train a once-for-all (OFA) supernet capable of yielding diverse optimal subnets for downstream applications through one-shot training.","Nonetheless, the scale of current language models impedes efficiency and amplifies interference from weight sharing between subnets.","We make an initial attempt to extend the once-for-all framework to large language models.","Specifically, we decouple shared weights to eliminate the interference and incorporate Low-Rank adapters for training efficiency.","Furthermore, we observe the imbalance allocation of training resources from the traditional uniform sampling.","A non-parametric scheduler is introduced to adjust the sampling rate for each quantization configuration, achieving a more balanced allocation among subnets with varying demands.","We validate the approach on LLaMA2 families, and downstream evaluation confirms our ability to maintain high performance while significantly reducing deployment time faced with multiple scenarios."],"url":"http://arxiv.org/abs/2405.20202v1","category":"cs.AI"}
{"created":"2024-05-30 16:04:35","title":"Unified Explanations in Machine Learning Models: A Perturbation Approach","abstract":"A high-velocity paradigm shift towards Explainable Artificial Intelligence (XAI) has emerged in recent years. Highly complex Machine Learning (ML) models have flourished in many tasks of intelligence, and the questions have started to shift away from traditional metrics of validity towards something deeper: What is this model telling me about my data, and how is it arriving at these conclusions? Inconsistencies between XAI and modeling techniques can have the undesirable effect of casting doubt upon the efficacy of these explainability approaches. To address these problems, we propose a systematic, perturbation-based analysis against a popular, model-agnostic method in XAI, SHapley Additive exPlanations (Shap). We devise algorithms to generate relative feature importance in settings of dynamic inference amongst a suite of popular machine learning and deep learning methods, and metrics that allow us to quantify how well explanations generated under the static case hold. We propose a taxonomy for feature importance methodology, measure alignment, and observe quantifiable similarity amongst explanation models across several datasets.","sentences":["A high-velocity paradigm shift towards Explainable Artificial Intelligence (XAI) has emerged in recent years.","Highly complex Machine Learning (ML) models have flourished in many tasks of intelligence, and the questions have started to shift away from traditional metrics of validity towards something deeper: What is this model telling me about my data, and how is it arriving at these conclusions?","Inconsistencies between XAI and modeling techniques can have the undesirable effect of casting doubt upon the efficacy of these explainability approaches.","To address these problems, we propose a systematic, perturbation-based analysis against a popular, model-agnostic method in XAI, SHapley Additive exPlanations (Shap).","We devise algorithms to generate relative feature importance in settings of dynamic inference amongst a suite of popular machine learning and deep learning methods, and metrics that allow us to quantify how well explanations generated under the static case hold.","We propose a taxonomy for feature importance methodology, measure alignment, and observe quantifiable similarity amongst explanation models across several datasets."],"url":"http://arxiv.org/abs/2405.20200v1","category":"cs.LG"}
{"created":"2024-05-30 16:01:56","title":"Generation planning and operation under power stability constraints: A Hydro-Quebec use case","abstract":"Hydro-Quebec (HQ) is a vertically integrated utility that produces, transmits, and distributes most of the electricity in the province of Quebec. The power grid it operates has a particular architecture created by large hydroelectric dams located far north and the extensive 735kV transmission grid that allows the generated power to reach the majority of the load located thousands of kilometers away in the southern region of Quebec. The specificity of the grid has led HQ to develop monitoring tools responsible for generating so-called stability limits. Those stability limits take into account several nonlinear phenomena such as angular stability, frequency stability, or voltage stability. Since generation planning and operation tools rely mostly on mixed integer linear programming formulation, HQ had to adapt its tools to integrate stability limits into them. This paper presents the challenges it faced, especially considering its reserve monitoring tool and unit commitment tool.","sentences":["Hydro-Quebec (HQ) is a vertically integrated utility that produces, transmits, and distributes most of the electricity in the province of Quebec.","The power grid it operates has a particular architecture created by large hydroelectric dams located far north and the extensive 735kV transmission grid that allows the generated power to reach the majority of the load located thousands of kilometers away in the southern region of Quebec.","The specificity of the grid has led HQ to develop monitoring tools responsible for generating so-called stability limits.","Those stability limits take into account several nonlinear phenomena such as angular stability, frequency stability, or voltage stability.","Since generation planning and operation tools rely mostly on mixed integer linear programming formulation, HQ had to adapt its tools to integrate stability limits into them.","This paper presents the challenges it faced, especially considering its reserve monitoring tool and unit commitment tool."],"url":"http://arxiv.org/abs/2405.20199v1","category":"eess.SY"}
{"created":"2024-05-30 16:00:01","title":"Time-delay interferometry with onboard optical delays","abstract":"Time-delay interferometry (TDI) is a data processing technique for space-based gravitational-wave detectors to create laser-noise-free equal-optical-path-length interferometers virtually on the ground. It relies on the interspacecraft signal propagation delays, which are delivered by intersatellite ranging monitors. Also delays due to onboard signal propagation and processing have a nonnegligible impact on the TDI combinations. However, these onboard delays were only partially considered in previous TDI-related research; onboard optical path lengths have been neglected so far. In this paper, we study onboard optical path lengths in TDI. We derive analytical models for their coupling to the second-generation TDI Michelson combinations and verify these models numerically. Furthermore, we derive a compensation scheme for onboard optical path lengths in TDI and validate its performance via numerical simulations.","sentences":["Time-delay interferometry (TDI) is a data processing technique for space-based gravitational-wave detectors to create laser-noise-free equal-optical-path-length interferometers virtually on the ground.","It relies on the interspacecraft signal propagation delays, which are delivered by intersatellite ranging monitors.","Also delays due to onboard signal propagation and processing have a nonnegligible impact on the TDI combinations.","However, these onboard delays were only partially considered in previous TDI-related research; onboard optical path lengths have been neglected so far.","In this paper, we study onboard optical path lengths in TDI.","We derive analytical models for their coupling to the second-generation TDI Michelson combinations and verify these models numerically.","Furthermore, we derive a compensation scheme for onboard optical path lengths in TDI and validate its performance via numerical simulations."],"url":"http://arxiv.org/abs/2405.20196v1","category":"gr-qc"}
{"created":"2024-05-30 15:58:22","title":"Occam Gradient Descent","abstract":"Deep learning neural network models must be large enough to adapt to their problem domain, while small enough to avoid overfitting training data during gradient descent. To balance these competing demands, overprovisioned deep learning models such as transformers are trained for a single epoch on large data sets, and hence inefficient with both computing resources and training data. In response to these inefficiencies, we exploit learning theory to derive Occam Gradient Descent, an algorithm that interleaves adaptive reduction of model size to minimize generalization error, with gradient descent on model weights to minimize fitting error. In contrast, traditional gradient descent greedily minimizes fitting error without regard to generalization error. Our algorithm simultaneously descends the space of weights and topological size of any neural network without modification, and is effective in our experiments in outperforming traditional gradient descent with or without post-train pruning in accuracy, compute and model compression.","sentences":["Deep learning neural network models must be large enough to adapt to their problem domain, while small enough to avoid overfitting training data during gradient descent.","To balance these competing demands, overprovisioned deep learning models such as transformers are trained for a single epoch on large data sets, and hence inefficient with both computing resources and training data.","In response to these inefficiencies, we exploit learning theory to derive Occam Gradient Descent, an algorithm that interleaves adaptive reduction of model size to minimize generalization error, with gradient descent on model weights to minimize fitting error.","In contrast, traditional gradient descent greedily minimizes fitting error without regard to generalization error.","Our algorithm simultaneously descends the space of weights and topological size of any neural network without modification, and is effective in our experiments in outperforming traditional gradient descent with or without post-train pruning in accuracy, compute and model compression."],"url":"http://arxiv.org/abs/2405.20194v1","category":"cs.LG"}
{"created":"2024-05-30 15:58:00","title":"Space-time superpositions as fluctuating geometries","abstract":"Superpositions of black holes can be described geometrically using a combined canonical formulation for space-time and quantum states. A previously introduced black-hole model that includes quantum fluctuations of metric components is shown here to give full access to the corresponding space-time geometry of weak-field gravity in terms of suitable line elements with quantum corrections. These results can be interpreted as providing covariant formulations of the gravitational force implied by a distribution of black holes in superposition. They can also be understood as a distribution of quantum matter constituents in superposition for a single black hole. A detailed analysis in the weak-field limit reveals quantum corrections to Newton's potential in generic semiclassical states, as well as new bounds on quantum fluctuations, implied by the covariance condition, rather than the usual uncertainty principle. These results provide additional control on quantum effects in Newton's potential that can be used in a broad range of predictions to be compared with observations.","sentences":["Superpositions of black holes can be described geometrically using a combined canonical formulation for space-time and quantum states.","A previously introduced black-hole model that includes quantum fluctuations of metric components is shown here to give full access to the corresponding space-time geometry of weak-field gravity in terms of suitable line elements with quantum corrections.","These results can be interpreted as providing covariant formulations of the gravitational force implied by a distribution of black holes in superposition.","They can also be understood as a distribution of quantum matter constituents in superposition for a single black hole.","A detailed analysis in the weak-field limit reveals quantum corrections to Newton's potential in generic semiclassical states, as well as new bounds on quantum fluctuations, implied by the covariance condition, rather than the usual uncertainty principle.","These results provide additional control on quantum effects in Newton's potential that can be used in a broad range of predictions to be compared with observations."],"url":"http://arxiv.org/abs/2405.20193v1","category":"gr-qc"}
{"created":"2024-05-30 15:57:19","title":"TAIA: Large Language Models are Out-of-Distribution Data Learners","abstract":"Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or harmless content generation, it is nearly impossible to obtain a large volume of high-quality data that matches the downstream distribution. To improve the performance of LLMs in data-scarce domains with domain-mismatched data, we re-evaluated the Transformer architecture and discovered that not all parameter updates during fine-tuning contribute positively to downstream performance. Our analysis reveals that within the self-attention and feed-forward networks, only the fine-tuned attention parameters are particularly beneficial when the training set's distribution does not fully align with the test set. Based on this insight, we propose an effective inference-time intervention method: \\uline{T}raining \\uline{A}ll parameters but \\uline{I}nferring with only \\uline{A}ttention (\\trainallInfAttn). We empirically validate \\trainallInfAttn using two general instruction-tuning datasets and evaluate it on seven downstream tasks involving math, reasoning, and knowledge understanding across LLMs of different parameter sizes and fine-tuning techniques. Our comprehensive experiments demonstrate that \\trainallInfAttn achieves superior improvements compared to both the fully fine-tuned model and the base model in most scenarios, with significant performance gains. The high tolerance of \\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning and enhances specialized tasks using general data.","sentences":["Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks.","However, in certain specialized domains, such as healthcare or harmless content generation, it is nearly impossible to obtain a large volume of high-quality data that matches the downstream distribution.","To improve the performance of LLMs in data-scarce domains with domain-mismatched data, we re-evaluated the Transformer architecture and discovered that not all parameter updates during fine-tuning contribute positively to downstream performance.","Our analysis reveals that within the self-attention and feed-forward networks, only the fine-tuned attention parameters are particularly beneficial when the training set's distribution does not fully align with the test set.","Based on this insight, we propose an effective inference-time intervention method: \\uline{T}raining \\uline{A}ll parameters but \\uline{I}nferring with only \\uline{A}ttention (\\trainallInfAttn).","We empirically validate \\trainallInfAttn using two general instruction-tuning datasets and evaluate it on seven downstream tasks involving math, reasoning, and knowledge understanding across LLMs of different parameter sizes and fine-tuning techniques.","Our comprehensive experiments demonstrate that \\trainallInfAttn achieves superior improvements compared to both the fully fine-tuned model and the base model in most scenarios, with significant performance gains.","The high tolerance of \\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning and enhances specialized tasks using general data."],"url":"http://arxiv.org/abs/2405.20192v1","category":"cs.CL"}
{"created":"2024-05-30 15:55:41","title":"Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory","abstract":"In this work, we describe our approach to developing an intelligent and robust social robotic system for the Nadine social robot platform. We achieve this by integrating Large Language Models (LLMs) and skilfully leveraging the powerful reasoning and instruction-following capabilities of these types of models to achieve advanced human-like affective and cognitive capabilities. This approach is novel compared to the current state-of-the-art LLM-based agents which do not implement human-like long-term memory or sophisticated emotional appraisal. The naturalness of social robots, consisting of multiple modules, highly depends on the performance and capabilities of each component of the system and the seamless integration of the components. We built a social robot system that enables generating appropriate behaviours through multimodal input processing, bringing episodic memories accordingly to the recognised user, and simulating the emotional states of the robot induced by the interaction with the human partner. In particular, we introduce an LLM-agent frame for social robots, SoR-ReAct, serving as a core component for the interaction module in our system. This design has brought forth the advancement of social robots and aims to increase the quality of human-robot interaction.","sentences":["In this work, we describe our approach to developing an intelligent and robust social robotic system for the Nadine social robot platform.","We achieve this by integrating Large Language Models (LLMs) and skilfully leveraging the powerful reasoning and instruction-following capabilities of these types of models to achieve advanced human-like affective and cognitive capabilities.","This approach is novel compared to the current state-of-the-art LLM-based agents which do not implement human-like long-term memory or sophisticated emotional appraisal.","The naturalness of social robots, consisting of multiple modules, highly depends on the performance and capabilities of each component of the system and the seamless integration of the components.","We built a social robot system that enables generating appropriate behaviours through multimodal input processing, bringing episodic memories accordingly to the recognised user, and simulating the emotional states of the robot induced by the interaction with the human partner.","In particular, we introduce an LLM-agent frame for social robots, SoR-ReAct, serving as a core component for the interaction module in our system.","This design has brought forth the advancement of social robots and aims to increase the quality of human-robot interaction."],"url":"http://arxiv.org/abs/2405.20189v1","category":"cs.RO"}
{"created":"2024-05-30 15:49:34","title":"A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models","abstract":"This paper analyzes Large Language Models (LLMs) with regard to their programming exercise generation capabilities. Through a survey study, we defined the state of the art, extracted their strengths and weaknesses and finally proposed an evaluation matrix, helping researchers and educators to decide which LLM is the best fitting for the programming exercise generation use case. We also found that multiple LLMs are capable of producing useful programming exercises. Nevertheless, there exist challenges like the ease with which LLMs might solve exercises generated by LLMs. This paper contributes to the ongoing discourse on the integration of LLMs in education.","sentences":["This paper analyzes Large Language Models (LLMs) with regard to their programming exercise generation capabilities.","Through a survey study, we defined the state of the art, extracted their strengths and weaknesses and finally proposed an evaluation matrix, helping researchers and educators to decide which LLM is the best fitting for the programming exercise generation use case.","We also found that multiple LLMs are capable of producing useful programming exercises.","Nevertheless, there exist challenges like the ease with which LLMs might solve exercises generated by LLMs.","This paper contributes to the ongoing discourse on the integration of LLMs in education."],"url":"http://arxiv.org/abs/2405.20183v1","category":"cs.AI"}
{"created":"2024-05-30 15:49:26","title":"Convergence Analysis for A Stochastic Maximum Principle Based Data Driven Feedback Control Algorithm","abstract":"This paper presents convergence analysis of a novel data-driven feedback control algorithm designed for generating online controls based on partial noisy observational data. The algorithm comprises a particle filter-enabled state estimation component, estimating the controlled system's state via indirect observations, alongside an efficient stochastic maximum principle type optimal control solver. By integrating weak convergence techniques for the particle filter with convergence analysis for the stochastic maximum principle control solver, we derive a weak convergence result for the optimization procedure in search of optimal data-driven feedback control. Numerical experiments are performed to validate the theoretical findings.","sentences":["This paper presents convergence analysis of a novel data-driven feedback control algorithm designed for generating online controls based on partial noisy observational data.","The algorithm comprises a particle filter-enabled state estimation component, estimating the controlled system's state via indirect observations, alongside an efficient stochastic maximum principle type optimal control solver.","By integrating weak convergence techniques for the particle filter with convergence analysis for the stochastic maximum principle control solver, we derive a weak convergence result for the optimization procedure in search of optimal data-driven feedback control.","Numerical experiments are performed to validate the theoretical findings."],"url":"http://arxiv.org/abs/2405.20182v1","category":"math.OC"}
{"created":"2024-05-30 15:48:48","title":"Conic Section on the Sky: Shadows of Linearly Superrotated Black Holes","abstract":"Soft hairs are intrinsically infrared features of a black hole, which may also affect near-horizon physics. In this work, we study one of the primary observables through which we can study the effects of a particular type of soft hair in Einstein's gravity: the shadow of a non-rotating black hole implanted with linear {\\it regular} superrotational hair. We show that such soft hairs deform the shadow from the characteristic circular shape of bald Schwarzschild black holes into ellipses. Such a unique effect can be observed in future black hole imaging projects. This starkly contrasts with the supertranslated counterpart, for which the only changes are the static shift of the circular shadow's position and thus cannot be detectable. Our results demonstrate the richness of observable effects due to the infrared structures of Einstein's gravity.","sentences":["Soft hairs are intrinsically infrared features of a black hole, which may also affect near-horizon physics.","In this work, we study one of the primary observables through which we can study the effects of a particular type of soft hair in Einstein's gravity: the shadow of a non-rotating black hole implanted with linear {\\it regular} superrotational hair.","We show that such soft hairs deform the shadow from the characteristic circular shape of bald Schwarzschild black holes into ellipses.","Such a unique effect can be observed in future black hole imaging projects.","This starkly contrasts with the supertranslated counterpart, for which the only changes are the static shift of the circular shadow's position and thus cannot be detectable.","Our results demonstrate the richness of observable effects due to the infrared structures of Einstein's gravity."],"url":"http://arxiv.org/abs/2405.20181v1","category":"hep-th"}
{"created":"2024-05-30 15:48:04","title":"Transformers and Slot Encoding for Sample Efficient Physical World Modelling","abstract":"World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world. Recent applications of the Transformer architecture to the problem of world modelling from video input show notable improvements in sample efficiency. However, existing approaches tend to work only at the image level thus disregarding that the environment is composed of objects interacting with each other. In this paper, we propose an architecture combining Transformers for world modelling with the slot-attention paradigm, an approach for learning representations of objects appearing in a scene. We describe the resulting neural architecture and report experimental results showing an improvement over the existing solutions in terms of sample efficiency and a reduction of the variation of the performance over the training examples. The code for our architecture and experiments is available at https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm","sentences":["World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world.","Recent applications of the Transformer architecture to the problem of world modelling from video input show notable improvements in sample efficiency.","However, existing approaches tend to work only at the image level thus disregarding that the environment is composed of objects interacting with each other.","In this paper, we propose an architecture combining Transformers for world modelling with the slot-attention paradigm, an approach for learning representations of objects appearing in a scene.","We describe the resulting neural architecture and report experimental results showing an improvement over the existing solutions in terms of sample efficiency and a reduction of the variation of the performance over the training examples.","The code for our architecture and experiments is available at https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm"],"url":"http://arxiv.org/abs/2405.20180v1","category":"cs.LG"}
{"created":"2024-05-30 15:47:54","title":"Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs","abstract":"Large language models (LLMs) have shown great promise at generating robot programs from natural language given domain-specific robot application programming interfaces (APIs). However, the performance gap between proprietary LLMs and smaller open-weight LLMs remains wide. This raises a question: Can we fine-tune smaller open-weight LLMs for generating domain-specific robot programs to close the performance gap with proprietary LLMs? While Self-Instruct is a promising solution by generating a diverse set of training data, it cannot verify the correctness of these programs. In contrast, a robot simulator with a well-defined world can identify execution errors but limits the diversity of programs that it can verify. In this work, we introduce Robo-Instruct, which brings the best of both worlds -- it promotes the diversity of Self-Instruct while providing the correctness of simulator-based checking. Robo-Instruct introduces RoboSim to synthesize a consistent world state on the fly by inferring properties relevant to the program being checked, and simulating actions accordingly. Furthermore, the instructions and programs generated by Self-Instruct may be subtly inconsistent -- such as the program missing a step implied by the instruction. Robo-Instruct further addresses this with InstAlign, an instruction-program alignment procedure that revises the task instruction to reflect the actual results of the generated program. Given a few seed task descriptions and the robot APIs, Robo-Instruct is capable of generating a training dataset using only a small open-weight model. This dataset can then be used to fine-tune small open-weight language models, enabling them to match or even exceed the performance of several proprietary LLMs, such as GPT-3.5-Turbo and Gemini-Pro.","sentences":["Large language models (LLMs) have shown great promise at generating robot programs from natural language given domain-specific robot application programming interfaces (APIs).","However, the performance gap between proprietary LLMs and smaller open-weight LLMs remains wide.","This raises a question: Can we fine-tune smaller open-weight LLMs for generating domain-specific robot programs to close the performance gap with proprietary LLMs?","While Self-Instruct is a promising solution by generating a diverse set of training data, it cannot verify the correctness of these programs.","In contrast, a robot simulator with a well-defined world can identify execution errors but limits the diversity of programs that it can verify.","In this work, we introduce Robo-Instruct, which brings the best of both worlds -- it promotes the diversity of Self-Instruct while providing the correctness of simulator-based checking.","Robo-Instruct introduces RoboSim to synthesize a consistent world state on the fly by inferring properties relevant to the program being checked, and simulating actions accordingly.","Furthermore, the instructions and programs generated by Self-Instruct may be subtly inconsistent -- such as the program missing a step implied by the instruction.","Robo-Instruct further addresses this with InstAlign, an instruction-program alignment procedure that revises the task instruction to reflect the actual results of the generated program.","Given a few seed task descriptions and the robot APIs, Robo-Instruct is capable of generating a training dataset using only a small open-weight model.","This dataset can then be used to fine-tune small open-weight language models, enabling them to match or even exceed the performance of several proprietary LLMs, such as GPT-3.5-Turbo and Gemini-Pro."],"url":"http://arxiv.org/abs/2405.20179v1","category":"cs.CL"}
{"created":"2024-05-30 15:45:13","title":"InstructionCP: A fast approach to transfer Large Language Models into target language","abstract":"The rapid development of large language models (LLMs) in recent years has largely focused on English, resulting in models that respond exclusively in English. To adapt these models to other languages, continual pre-training (CP) is often employed, followed by supervised fine-tuning (SFT) to maintain conversational abilities. However, CP and SFT can reduce a model's ability to filter harmful content. We propose Instruction Continual Pre-training (InsCP), which integrates instruction tags into the CP process to prevent loss of conversational proficiency while acquiring new languages. Our experiments demonstrate that InsCP retains conversational and Reinforcement Learning from Human Feedback (RLHF) abilities. Empirical evaluations on language alignment, reliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably, this approach requires only 0.1 billion tokens of high-quality instruction-following data, thereby reducing resource consumption.","sentences":["The rapid development of large language models (LLMs) in recent years has largely focused on English, resulting in models that respond exclusively in English.","To adapt these models to other languages, continual pre-training (CP) is often employed, followed by supervised fine-tuning (SFT) to maintain conversational abilities.","However, CP and SFT can reduce a model's ability to filter harmful content.","We propose Instruction Continual Pre-training (InsCP), which integrates instruction tags into the CP process to prevent loss of conversational proficiency while acquiring new languages.","Our experiments demonstrate that InsCP retains conversational and Reinforcement Learning from Human Feedback (RLHF) abilities.","Empirical evaluations on language alignment, reliability, and knowledge benchmarks confirm the efficacy of InsCP.","Notably, this approach requires only 0.1 billion tokens of high-quality instruction-following data, thereby reducing resource consumption."],"url":"http://arxiv.org/abs/2405.20175v1","category":"cs.CL"}
{"created":"2024-05-30 15:44:27","title":"Iterative Feature Boosting for Explainable Speech Emotion Recognition","abstract":"In speech emotion recognition (SER), using predefined features without considering their practical importance may lead to high dimensional datasets, including redundant and irrelevant information. Consequently, high-dimensional learning often results in decreasing model accuracy while increasing computational complexity. Our work underlines the importance of carefully considering and analyzing features in order to build efficient SER systems. We present a new supervised SER method based on an efficient feature engineering approach. We pay particular attention to the explainability of results to evaluate feature relevance and refine feature sets. This is performed iteratively through feature evaluation loop, using Shapley values to boost feature selection and improve overall framework performance. Our approach allows thus to balance the benefits between model performance and transparency. The proposed method outperforms human-level performance (HLP) and state-of-the-art machine learning methods in emotion recognition on the TESS dataset.","sentences":["In speech emotion recognition (SER), using predefined features without considering their practical importance may lead to high dimensional datasets, including redundant and irrelevant information.","Consequently, high-dimensional learning often results in decreasing model accuracy while increasing computational complexity.","Our work underlines the importance of carefully considering and analyzing features in order to build efficient SER systems.","We present a new supervised SER method based on an efficient feature engineering approach.","We pay particular attention to the explainability of results to evaluate feature relevance and refine feature sets.","This is performed iteratively through feature evaluation loop, using Shapley values to boost feature selection and improve overall framework performance.","Our approach allows thus to balance the benefits between model performance and transparency.","The proposed method outperforms human-level performance (HLP) and state-of-the-art machine learning methods in emotion recognition on the TESS dataset."],"url":"http://arxiv.org/abs/2405.20172v1","category":"cs.SD"}
{"created":"2024-05-30 15:43:49","title":"Projected Augmented Wave (PAW) Method with multiple pseudo statesmapped onto the same all electron wavefunction","abstract":"The Projected Augmented Wave (PAW) Method is based on generating a transform between the pseudo wavefunction and all electron wavefunction. For the accuracy of the method it is important that the local part of the transform (in each atomic sphere $\\mathbf{R}$) be over a complete basis set (with deviations from completeness leading to corrections to the total energy not computed within current implementations of PAW). Here we show how to make this basis much closer to complete without significant additional computational work by allowing for multiple smooth pseudo wavefunction states to be mapped onto the same all electron wavefunction.","sentences":["The Projected Augmented Wave (PAW) Method is based on generating a transform between the pseudo wavefunction and all electron wavefunction.","For the accuracy of the method it is important that the local part of the transform (in each atomic sphere $\\mathbf{R}$) be over a complete basis set (with deviations from completeness leading to corrections to the total energy not computed within current implementations of PAW).","Here we show how to make this basis much closer to complete without significant additional computational work by allowing for multiple smooth pseudo wavefunction states to be mapped onto the same all electron wavefunction."],"url":"http://arxiv.org/abs/2405.20171v1","category":"cond-mat.other"}
{"created":"2024-05-30 15:43:26","title":"Role of Spatial Curvature in a Dark Energy Interacting Model","abstract":"This paper investigates the effects of spatial curvature in a model where dark matter and dark energy interact. The analysis employs a range of datasets, including CMB, BAO, Type Ia Supernova, $H(z)$ from cosmic chronometers, $H_0$ measurements from Megamasers and SH0ES, growth rate data and strong lensing time delay measurements, to assess the model's fit and explore the late-time dynamics of the interacting dark sector in a non-flat cosmological framework. The study indicates that introducing curvature can significantly impact the Hubble constant ($H_0$) and the structure growth parameter ($S_8$), potentially easing tensions between early and late universe observations. The observational data shows an indication for a closed universe. This implies that the presence of curvature and its influence cannot be neglected entirely.","sentences":["This paper investigates the effects of spatial curvature in a model where dark matter and dark energy interact.","The analysis employs a range of datasets, including CMB, BAO, Type Ia Supernova, $H(z)$ from cosmic chronometers, $H_0$ measurements from Megamasers and SH0ES, growth rate data and strong lensing time delay measurements, to assess the model's fit and explore the late-time dynamics of the interacting dark sector in a non-flat cosmological framework.","The study indicates that introducing curvature can significantly impact the Hubble constant ($H_0$) and the structure growth parameter ($S_8$), potentially easing tensions between early and late universe observations.","The observational data shows an indication for a closed universe.","This implies that the presence of curvature and its influence cannot be neglected entirely."],"url":"http://arxiv.org/abs/2405.20170v1","category":"astro-ph.CO"}
{"created":"2024-05-30 15:43:19","title":"Multi-watt 1 GHz single-cycle frequency combs","abstract":"Single-cycle optical pulses offer a strong carrier-envelope-offset (CEO) dependent electric field and the highest peak intensity for a given pulse energy. Absence of demonstrated GHz single-cycle lasers constrains exploration of single/sub-cycle dynamics at this repetition rate. By leveraging fiber soliton effects and suppressing higher-order dispersion, we achieve single-cycle pulse generation at a 1 GHz repetition rate in an all-fiber format. The laser produces 7.1 fs (1.1-cycle) pulses with 1.8 W average power, centered around 1970 nm. Temporal characterization shows 60% of the pulse energy is concentrated in the pulse center, yielding a peak power of 110 kW. The seed laser demonstrates a 43 dB signal-to-noise ratio for the CEO frequency, facilitating comb stabilization and CEO control. Our model, which matches experimental observations, identifies conditions for achieving single-cycle duration and predicts scalability to a 2 GHz repetition rate. This work presents the first GHz single-cycle source. We envision these advances will drive studies in single/sub-cycle light-matter interaction, spectroscopy, microscopy, and CEO-sensitive nonlinear optics.","sentences":["Single-cycle optical pulses offer a strong carrier-envelope-offset (CEO) dependent electric field and the highest peak intensity for a given pulse energy.","Absence of demonstrated GHz single-cycle lasers constrains exploration of single/sub-cycle dynamics at this repetition rate.","By leveraging fiber soliton effects and suppressing higher-order dispersion, we achieve single-cycle pulse generation at a 1 GHz repetition rate in an all-fiber format.","The laser produces 7.1 fs (1.1-cycle) pulses with 1.8 W average power, centered around 1970 nm.","Temporal characterization shows 60% of the pulse energy is concentrated in the pulse center, yielding a peak power of 110 kW. The seed laser demonstrates a 43 dB signal-to-noise ratio for the CEO frequency, facilitating comb stabilization and CEO control.","Our model, which matches experimental observations, identifies conditions for achieving single-cycle duration and predicts scalability to a 2 GHz repetition rate.","This work presents the first GHz single-cycle source.","We envision these advances will drive studies in single/sub-cycle light-matter interaction, spectroscopy, microscopy, and CEO-sensitive nonlinear optics."],"url":"http://arxiv.org/abs/2405.20169v1","category":"physics.optics"}
{"created":"2024-05-30 15:41:39","title":"Enhancing Battlefield Awareness: An Aerial RIS-assisted ISAC System with Deep Reinforcement Learning","abstract":"This paper considers a joint communication and sensing technique for enhancing situational awareness in practical battlefield scenarios. In particular, we propose an aerial reconfigurable intelligent surface (ARIS)-assisted integrated sensing and communication (ISAC) system consisting of a single access point (AP), an ARIS, multiple users, and a sensing target. With deep reinforcement learning (DRL), we jointly optimize the transmit beamforming of the AP, the RIS phase shifts, and the trajectory of the ARIS under signal-to-interference-noise ratio (SINR) constraints. Numerical results demonstrate that the proposed technique outperforms the conventional benchmark schemes by suppressing the self-interference and clutter echo signals or optimizing the RIS phase shifts.","sentences":["This paper considers a joint communication and sensing technique for enhancing situational awareness in practical battlefield scenarios.","In particular, we propose an aerial reconfigurable intelligent surface (ARIS)-assisted integrated sensing and communication (ISAC) system consisting of a single access point (AP), an ARIS, multiple users, and a sensing target.","With deep reinforcement learning (DRL), we jointly optimize the transmit beamforming of the AP, the RIS phase shifts, and the trajectory of the ARIS under signal-to-interference-noise ratio (SINR) constraints.","Numerical results demonstrate that the proposed technique outperforms the conventional benchmark schemes by suppressing the self-interference and clutter echo signals or optimizing the RIS phase shifts."],"url":"http://arxiv.org/abs/2405.20168v1","category":"eess.SY"}
{"created":"2024-05-30 15:38:54","title":"Reasoning about concepts with LLMs: Inconsistencies abound","abstract":"The ability to summarize and organize knowledge into abstract concepts is key to learning and reasoning. Many industrial applications rely on the consistent and systematic use of concepts, especially when dealing with decision-critical knowledge. However, we demonstrate that, when methodically questioned, large language models (LLMs) often display and demonstrate significant inconsistencies in their knowledge. Computationally, the basic aspects of the conceptualization of a given domain can be represented as Is-A hierarchies in a knowledge graph (KG) or ontology, together with a few properties or axioms that enable straightforward reasoning. We show that even simple ontologies can be used to reveal conceptual inconsistencies across several LLMs. We also propose strategies that domain experts can use to evaluate and improve the coverage of key domain concepts in LLMs of various sizes. In particular, we have been able to significantly enhance the performance of LLMs of various sizes with openly available weights using simple knowledge-graph (KG) based prompting strategies.","sentences":["The ability to summarize and organize knowledge into abstract concepts is key to learning and reasoning.","Many industrial applications rely on the consistent and systematic use of concepts, especially when dealing with decision-critical knowledge.","However, we demonstrate that, when methodically questioned, large language models (LLMs) often display and demonstrate significant inconsistencies in their knowledge.","Computationally, the basic aspects of the conceptualization of a given domain can be represented as Is-A hierarchies in a knowledge graph (KG) or ontology, together with a few properties or axioms that enable straightforward reasoning.","We show that even simple ontologies can be used to reveal conceptual inconsistencies across several LLMs.","We also propose strategies that domain experts can use to evaluate and improve the coverage of key domain concepts in LLMs of various sizes.","In particular, we have been able to significantly enhance the performance of LLMs of various sizes with openly available weights using simple knowledge-graph (KG) based prompting strategies."],"url":"http://arxiv.org/abs/2405.20163v1","category":"cs.CL"}
{"created":"2024-05-30 15:31:39","title":"Fast Algorithm for Multiplication on the Skein Algebra of One-hole Torus","abstract":"The Kauffman bracket skein algebra of a surface is a generalization of the Jones polynomial invariant for links and plays a principal role in the Witten-Reshetikhin- Turaev topological quantum field theory. However, the multiplicative structure of the skein algebra is not well understood, with a priori exponential complexity. We consider the case of one-hole torus, and provide a polynomial algorithm for computing multiplication of any two skein elements. Some closed form formulas for multiplication of curves with low crossing number are also given.","sentences":["The Kauffman bracket skein algebra of a surface is a generalization of the Jones polynomial invariant for links and plays a principal role in the Witten-Reshetikhin- Turaev topological quantum field theory.","However, the multiplicative structure of the skein algebra is not well understood, with a priori exponential complexity.","We consider the case of one-hole torus, and provide a polynomial algorithm for computing multiplication of any two skein elements.","Some closed form formulas for multiplication of curves with low crossing number are also given."],"url":"http://arxiv.org/abs/2405.20159v1","category":"math.GT"}
{"created":"2024-05-30 15:31:36","title":"Parity-violating scalar-tensor theory and the Qi-Xiu","abstract":"We investigate the parity-violating scalar-tensor theory and pay special attention to terms that are free of the Ostrogradsky ghost in the unitary gauge, i.e., when the scalar field possesses a timelike gradient. We exhaustively identify the generally covariant scalar-tensor theory (GST) monomials with parity violation up to $d=4$, where $d$ is the total number of derivatives in the unitary gauge. According to the correspondence between GST terms and the spatially covariant gravity (SCG) terms in the unitary gauge, we also exhaustively identify the SCG monomials with parity violation up to $d=4$, where the Lie derivatives of the extrinsic curvature and the lapse function are necessarily introduced. We find a total of 9 independent parity-violating SCG monomials, of which 7 contain no higher-order Lie derivatives and are thus automatically free of ghosts, while 2 involve Lie derivatives of the extrinsic curvature and the lapse function and are thus potentially dangerous. By explicitly deriving their generally covariant correspondence, we obtain 7 independent scalar-tensor terms dubbed the ``Qi-Xiu'' Lagrangians, which are the most general parity-violating scalar-tensor theories that are ghost-free in the unitary gauge up to $d=4$. Our results include the existing theories in the literature, such as the Chern-Simons term and the chiral scalar-tensor theories, as special cases.","sentences":["We investigate the parity-violating scalar-tensor theory and pay special attention to terms that are free of the Ostrogradsky ghost in the unitary gauge, i.e., when the scalar field possesses a timelike gradient.","We exhaustively identify the generally covariant scalar-tensor theory (GST) monomials with parity violation up to $d=4$, where $d$ is the total number of derivatives in the unitary gauge.","According to the correspondence between GST terms and the spatially covariant gravity (SCG) terms in the unitary gauge, we also exhaustively identify the SCG monomials with parity violation up to $d=4$, where the Lie derivatives of the extrinsic curvature and the lapse function are necessarily introduced.","We find a total of 9 independent parity-violating SCG monomials, of which 7 contain no higher-order Lie derivatives and are thus automatically free of ghosts, while 2 involve Lie derivatives of the extrinsic curvature and the lapse function and are thus potentially dangerous.","By explicitly deriving their generally covariant correspondence, we obtain 7 independent scalar-tensor terms dubbed the ``Qi-Xiu'' Lagrangians, which are the most general parity-violating scalar-tensor theories that are ghost-free in the unitary gauge up to $d=4$. Our results include the existing theories in the literature, such as the Chern-Simons term and the chiral scalar-tensor theories, as special cases."],"url":"http://arxiv.org/abs/2405.20158v1","category":"hep-th"}
{"created":"2024-05-30 15:31:32","title":"A Multiband T-Shaped Antenna Array for 6G Mobile Communication","abstract":"The paradigm shift in the use cases of wireless communication necessitates the need to move toward higher data rates, large bandwidths, and intelligent reconfiguration in 6G. This paper presents a novel double T-shaped antenna array that operates between 4GHz to 16GHz for 6G mobile communication. The antenna consists of a rectangular microstrip with a fractal Tshaped slot, cut at the rear of the microstrip to provide an air gap for an improved radiation pattern.","sentences":["The paradigm shift in the use cases of wireless communication necessitates the need to move toward higher data rates, large bandwidths, and intelligent reconfiguration in 6G.","This paper presents a novel double T-shaped antenna array that operates between 4GHz to 16GHz for 6G mobile communication.","The antenna consists of a rectangular microstrip with a fractal Tshaped slot, cut at the rear of the microstrip to provide an air gap for an improved radiation pattern."],"url":"http://arxiv.org/abs/2405.20157v1","category":"eess.SP"}
{"created":"2024-05-30 15:31:12","title":"Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)","abstract":"This paper seeks to bridge the gap between archival text analysis and network analysis by applying network clustering methods to analyze the coverage of Bulgaria in 123 issues of the newspaper Osservatore Romano published between January and May 1877. Utilizing optical character recognition and generalized homogeneity blockmodeling, the study constructs networks of relevant keywords. Those including the sets Bulgaria and Russia are rather isomorphic and they largely overlap with those for Germany, Britain, and War. In structural terms, the blockmodel of the two networks exhibits a clear core-semiperiphery-periphery structure that reflects relations between concepts in the newpaper's coverage. The newspaper's lexical choices effectively delegitimised the Bulgarian national revival, highlighting the influence of the Holy See on the newspaper's editorial line.","sentences":["This paper seeks to bridge the gap between archival text analysis and network analysis by applying network clustering methods to analyze the coverage of Bulgaria in 123 issues of the newspaper Osservatore Romano published between January and May 1877.","Utilizing optical character recognition and generalized homogeneity blockmodeling, the study constructs networks of relevant keywords.","Those including the sets Bulgaria and Russia are rather isomorphic and they largely overlap with those for Germany, Britain, and War.","In structural terms, the blockmodel of the two networks exhibits a clear core-semiperiphery-periphery structure that reflects relations between concepts in the newpaper's coverage.","The newspaper's lexical choices effectively delegitimised the Bulgarian national revival, highlighting the influence of the Holy See on the newspaper's editorial line."],"url":"http://arxiv.org/abs/2405.20156v1","category":"cs.DL"}
{"created":"2024-05-30 15:30:38","title":"MotionDreamer: Zero-Shot 3D Mesh Animation from Video Diffusion Models","abstract":"Animation techniques bring digital 3D worlds and characters to life. However, manual animation is tedious and automated techniques are often specialized to narrow shape classes. In our work, we propose a technique for automatic re-animation of arbitrary 3D shapes based on a motion prior extracted from a video diffusion model. Unlike existing 4D generation methods, we focus solely on the motion, and we leverage an explicit mesh-based representation compatible with existing computer-graphics pipelines. Furthermore, our utilization of diffusion features enhances accuracy of our motion fitting. We analyze efficacy of these features for animation fitting and we experimentally validate our approach for two different diffusion models and four animation models. Finally, we demonstrate that our time-efficient zero-shot method achieves a superior performance re-animating a diverse set of 3D shapes when compared to existing techniques in a user study. The project website is located at https://lukas.uzolas.com/MotionDreamer.","sentences":["Animation techniques bring digital 3D worlds and characters to life.","However, manual animation is tedious and automated techniques are often specialized to narrow shape classes.","In our work, we propose a technique for automatic re-animation of arbitrary 3D shapes based on a motion prior extracted from a video diffusion model.","Unlike existing 4D generation methods, we focus solely on the motion, and we leverage an explicit mesh-based representation compatible with existing computer-graphics pipelines.","Furthermore, our utilization of diffusion features enhances accuracy of our motion fitting.","We analyze efficacy of these features for animation fitting and we experimentally validate our approach for two different diffusion models and four animation models.","Finally, we demonstrate that our time-efficient zero-shot method achieves a superior performance re-animating a diverse set of 3D shapes when compared to existing techniques in a user study.","The project website is located at https://lukas.uzolas.com/MotionDreamer."],"url":"http://arxiv.org/abs/2405.20155v1","category":"cs.CV"}
{"created":"2024-05-30 15:27:56","title":"Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals","abstract":"With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from different counterfactual sets, where each set contains images which are largely identical in their depiction of a common subject (e.g., a doctor), but vary only in terms of intersectional social attributes (e.g., race and gender). We comprehensively evaluate the text produced by different models under this counterfactual generation setting at scale, producing over 57 million responses from popular LVLMs. Our multi-dimensional analysis reveals that social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of depicted individuals. We additionally explore the relationship between social bias in LVLMs and their corresponding LLMs, as well as inference-time strategies to mitigate bias.","sentences":["With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs.","Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat.","While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs.","Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities.","To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images.","Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from different counterfactual sets, where each set contains images which are largely identical in their depiction of a common subject (e.g., a doctor), but vary only in terms of intersectional social attributes (e.g., race and gender).","We comprehensively evaluate the text produced by different models under this counterfactual generation setting at scale, producing over 57 million responses from popular LVLMs.","Our multi-dimensional analysis reveals that social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of depicted individuals.","We additionally explore the relationship between social bias in LVLMs and their corresponding LLMs, as well as inference-time strategies to mitigate bias."],"url":"http://arxiv.org/abs/2405.20152v1","category":"cs.CV"}
{"created":"2024-05-30 15:24:26","title":"SLE and its partition function in multiply connected domains via the Gaussian Free Field and restriction measures","abstract":"One way to uniquely define Schramm-Loewner Evolution (SLE) in multiply connected domains is to use the restriction property. This gives an implicit definition of a $\\sigma$-finite measure on curves; yet it is in general not clear how to construct such measures nor whether the mass of these measures, called the partition function, is finite.   We provide an explicit construction of the such conformal restriction SLEs in multiply connected domains when $\\kappa = 4$ using the Gaussian Free Field (GFF). In particular, both when the target points of the curve are on the same or on distinct boundary components, we show that there is a mixture of laws of level lines of GFFs that satisfies the restriction property. This allows us to give an expression for the partition function of $\\mathrm{sle}_4$ on multiply connected domains and shows that the partition function is finite, answering the question raised in [Lawler, J. Stat. Phys. 2009].   In a second part, we provide a second construction of $\\mathrm{sle}_\\kappa$ in multiply-connected domains for the whole range $\\kappa \\in (8/3,4]$; specific, however, to the case of the two target points belonging to the same boundary components. This is inspired by [Werner, Wu, Electron. J. Probab. 2013] and consists of a mixture of laws on curves obtained by following $\\mathrm{cle}_\\kappa$ loops and restriction hulls attached to parts of the boundary of the domain. In this case as well, we obtain as a corollary the finiteness of the partition function for this type of $\\mathrm{sle}_\\kappa$.","sentences":["One way to uniquely define Schramm-Loewner Evolution (SLE) in multiply connected domains is to use the restriction property.","This gives an implicit definition of a $\\sigma$-finite measure on curves; yet it is in general not clear how to construct such measures nor whether the mass of these measures, called the partition function, is finite.   ","We provide an explicit construction of the such conformal restriction SLEs in multiply connected domains when $\\kappa = 4$ using the Gaussian Free Field (GFF).","In particular, both when the target points of the curve are on the same or on distinct boundary components, we show that there is a mixture of laws of level lines of GFFs that satisfies the restriction property.","This allows us to give an expression for the partition function of $\\mathrm{sle}_4$ on multiply connected domains and shows that the partition function is finite, answering the question raised in [Lawler, J. Stat.","Phys. 2009].   ","In a second part, we provide a second construction of $\\mathrm{sle}_\\kappa$ in multiply-connected domains for the whole range $\\kappa \\in (8/3,4]$; specific, however, to the case of the two target points belonging to the same boundary components.","This is inspired by [Werner, Wu, Electron.","J. Probab.","2013] and consists of a mixture of laws on curves obtained by following $\\mathrm{cle}_\\kappa$ loops and restriction hulls attached to parts of the boundary of the domain.","In this case as well, we obtain as a corollary the finiteness of the partition function for this type of $\\mathrm{sle}_\\kappa$."],"url":"http://arxiv.org/abs/2405.20148v1","category":"math.PR"}
{"created":"2024-05-30 15:21:47","title":"Flowy: High performance probabilistic lava emplacement prediction","abstract":"Lava emplacement is a complex physical phenomenon, affected by several factors. These include, but are not limited to features of the terrain, the lava settling process, the effusion rate or total erupted volume, and the probability of effusion from different locations. One method, which has been successfully employed to predict lava flow emplacement and forecast the inundated area and final lava thickness, is the MrLavaLoba method from Vitturi et al. The MrLavaLoba method is implemented in their code of the same name. Here, we introduce Flowy, a new computational tool that implements the MrLavaLoba method of Vitturi et al. in a more efficient manner. New fast algorithms have been incorporated for all performance critical code paths, resulting in a complete overhaul of the implementation. When compared to the MrLavaLoba code, Flowy exhibits a significant reduction in runtime - between 100 to 400 times faster - depending on the specific input parameters. The accuracy and the probabilistic convergence of the model outputs are not compromised, maintaining high fidelity in generating possible lava flow paths and deposition characteristics. We have validated Flowy's performance and reliability through comprehensive unit-testing and a real-world eruption scenario. The source code is freely available on GitHub, facilitating transparency, reproducibility and collaboration within the geoscientific community.","sentences":["Lava emplacement is a complex physical phenomenon, affected by several factors.","These include, but are not limited to features of the terrain, the lava settling process, the effusion rate or total erupted volume, and the probability of effusion from different locations.","One method, which has been successfully employed to predict lava flow emplacement and forecast the inundated area and final lava thickness, is the MrLavaLoba method from Vitturi et al.","The MrLavaLoba method is implemented in their code of the same name.","Here, we introduce Flowy, a new computational tool that implements the MrLavaLoba method of Vitturi et al.","in a more efficient manner.","New fast algorithms have been incorporated for all performance critical code paths, resulting in a complete overhaul of the implementation.","When compared to the MrLavaLoba code, Flowy exhibits a significant reduction in runtime - between 100 to 400 times faster - depending on the specific input parameters.","The accuracy and the probabilistic convergence of the model outputs are not compromised, maintaining high fidelity in generating possible lava flow paths and deposition characteristics.","We have validated Flowy's performance and reliability through comprehensive unit-testing and a real-world eruption scenario.","The source code is freely available on GitHub, facilitating transparency, reproducibility and collaboration within the geoscientific community."],"url":"http://arxiv.org/abs/2405.20144v1","category":"physics.geo-ph"}
{"created":"2024-05-30 15:19:27","title":"On the interpretation of quantum theory as games between physicists and nature played in Minkowski spacetime","abstract":"In 2019, we introduced games in Minkowski spacetime as a generalization of game theory to special relativity that subsumes games in normal form (spacelike separation) and games in extensive form (timelike separation). Many concepts including Nash equilibria naturally extend to spacetime games. We also emphasized the importance of these games to model quantum experiments such as Bell experiments and more generally any adaptive measurements. Subsequent work suggested to formalize a special case of such games in terms of strategy presheaves. In the case that measurements have a unique causal bridge and if a natural cover is taken, we show that the two frameworks are isomorphic to each other and provide complementary perspectives. Spacetime games provide a visual and intuitive framework that also captures the distinction between joint experiments and either-or experiments, so that they are rich enough in their causal structure to imply a natural cover for the corresponding causal contextuality scenario. Based on this observation, we suggest to define the strategy presheaf directly based on the pure strategies (and restrictions thereof) of the spacetime game, and we show that the sheaf property obtains for the games at hand. The argument is rather simple and similar to event sheaves for the flat case. Finally, we explain how, in the other direction, the failure of the sheaf property on strategy distribution presheaves is consistent with our previous argument that Nash game theory is not compatible with quantum physics. This shows that the insights of the two frameworks, taken together, can contribute positively to the advancement of the field of quantum foundations.","sentences":["In 2019, we introduced games in Minkowski spacetime as a generalization of game theory to special relativity that subsumes games in normal form (spacelike separation) and games in extensive form (timelike separation).","Many concepts including Nash equilibria naturally extend to spacetime games.","We also emphasized the importance of these games to model quantum experiments such as Bell experiments and more generally any adaptive measurements.","Subsequent work suggested to formalize a special case of such games in terms of strategy presheaves.","In the case that measurements have a unique causal bridge and if a natural cover is taken, we show that the two frameworks are isomorphic to each other and provide complementary perspectives.","Spacetime games provide a visual and intuitive framework that also captures the distinction between joint experiments and either-or experiments, so that they are rich enough in their causal structure to imply a natural cover for the corresponding causal contextuality scenario.","Based on this observation, we suggest to define the strategy presheaf directly based on the pure strategies (and restrictions thereof) of the spacetime game, and we show that the sheaf property obtains for the games at hand.","The argument is rather simple and similar to event sheaves for the flat case.","Finally, we explain how, in the other direction, the failure of the sheaf property on strategy distribution presheaves is consistent with our previous argument that Nash game theory is not compatible with quantum physics.","This shows that the insights of the two frameworks, taken together, can contribute positively to the advancement of the field of quantum foundations."],"url":"http://arxiv.org/abs/2405.20143v1","category":"quant-ph"}
{"created":"2024-05-30 15:16:53","title":"MSSC-BiMamba: Multimodal Sleep Stage Classification and Early Diagnosis of Sleep Disorders with Bidirectional Mamba","abstract":"Background and Objectives: Monitoring sleep states is crucial for assessing sleep quality and diagnosing sleep disorders. Traditional manual staging methods are not only time-consuming but also subject to subjective judgment, leading to inconsistent results. This study developed an automated sleep staging and sleep disorder classification model through deep learning technology, aimed at improving diagnostic accuracy and efficiency.   Methods: Considering the characteristics of polysomnography (PSG) multi-lead sleep monitoring, we designed a sleep state classification model, MSSC-BiMamba, that combines an Efficient Channel Attention (ECA) mechanism with a Bidirectional State Space Model (BSSM). The ECA module allows for weighting data from different sensor channels, thereby amplifying the influence of diverse sensor inputs. Additionally, the implementation of mamba enables the model to effectively capture the multidimensional features and long-range dependencies of PSG data.   Results: The developed model demonstrated impressive performance on sleep stage classification tasks. Furthermore, the model exhibited an accuracy of 0.952 for sleep health prediction when evaluated on a combined dataset consisting of ISRUC and Sleep-EDF.   Conclusion: Our model is the first to apply the bidirectional Mamba to sleep staging with complex PSG data, showing substantial gains in computational and memory efficiency over traditional Transformer-style models. This method not only makes health monitoring more accessible but also broadens the reach of advanced healthcare, thereby enhancing sleep health management with innovative technology.","sentences":["Background and Objectives: Monitoring sleep states is crucial for assessing sleep quality and diagnosing sleep disorders.","Traditional manual staging methods are not only time-consuming but also subject to subjective judgment, leading to inconsistent results.","This study developed an automated sleep staging and sleep disorder classification model through deep learning technology, aimed at improving diagnostic accuracy and efficiency.   ","Methods: Considering the characteristics of polysomnography (PSG) multi-lead sleep monitoring, we designed a sleep state classification model, MSSC-BiMamba, that combines an Efficient Channel Attention (ECA) mechanism with a Bidirectional State Space Model (BSSM).","The ECA module allows for weighting data from different sensor channels, thereby amplifying the influence of diverse sensor inputs.","Additionally, the implementation of mamba enables the model to effectively capture the multidimensional features and long-range dependencies of PSG data.   ","Results:","The developed model demonstrated impressive performance on sleep stage classification tasks.","Furthermore, the model exhibited an accuracy of 0.952 for sleep health prediction when evaluated on a combined dataset consisting of ISRUC and Sleep-EDF.   Conclusion: Our model is the first to apply the bidirectional Mamba to sleep staging with complex PSG data, showing substantial gains in computational and memory efficiency over traditional Transformer-style models.","This method not only makes health monitoring more accessible but also broadens the reach of advanced healthcare, thereby enhancing sleep health management with innovative technology."],"url":"http://arxiv.org/abs/2405.20142v1","category":"cs.AI"}
{"created":"2024-05-30 15:16:06","title":"OpenDAS: Domain Adaptation for Open-Vocabulary Segmentation","abstract":"The advent of Vision Language Models (VLMs) transformed image understanding from closed-set classifications to dynamic image-language interactions, enabling open-vocabulary segmentation. Despite this flexibility, VLMs often fall behind closed-set classifiers in accuracy due to their reliance on ambiguous image captions and lack of domain-specific knowledge. We, therefore, introduce a new task domain adaptation for open-vocabulary segmentation, enhancing VLMs with domain-specific priors while preserving their open-vocabulary nature. Existing adaptation methods, when applied to segmentation tasks, improve performance on training queries but can reduce VLM performance on zero-shot text inputs. To address this shortcoming, we propose an approach that combines parameter-efficient prompt tuning with a triplet-loss-based training strategy. This strategy is designed to enhance open-vocabulary generalization while adapting to the visual domain. Our results outperform other parameter-efficient adaptation strategies in open-vocabulary segment classification tasks across indoor and outdoor datasets. Notably, our approach is the only one that consistently surpasses the original VLM on zero-shot queries. Our adapted VLMs can be plug-and-play integrated into existing open-vocabulary segmentation pipelines, improving OV-Seg by +6.0% mIoU on ADE20K, and OpenMask3D by +4.1% AP on ScanNet++ Offices without any changes to the methods.","sentences":["The advent of Vision Language Models (VLMs) transformed image understanding from closed-set classifications to dynamic image-language interactions, enabling open-vocabulary segmentation.","Despite this flexibility, VLMs often fall behind closed-set classifiers in accuracy due to their reliance on ambiguous image captions and lack of domain-specific knowledge.","We, therefore, introduce a new task domain adaptation for open-vocabulary segmentation, enhancing VLMs with domain-specific priors while preserving their open-vocabulary nature.","Existing adaptation methods, when applied to segmentation tasks, improve performance on training queries but can reduce VLM performance on zero-shot text inputs.","To address this shortcoming, we propose an approach that combines parameter-efficient prompt tuning with a triplet-loss-based training strategy.","This strategy is designed to enhance open-vocabulary generalization while adapting to the visual domain.","Our results outperform other parameter-efficient adaptation strategies in open-vocabulary segment classification tasks across indoor and outdoor datasets.","Notably, our approach is the only one that consistently surpasses the original VLM on zero-shot queries.","Our adapted VLMs can be plug-and-play integrated into existing open-vocabulary segmentation pipelines, improving OV-Seg by +6.0% mIoU on ADE20K, and OpenMask3D by +4.1% AP on ScanNet++ Offices without any changes to the methods."],"url":"http://arxiv.org/abs/2405.20141v1","category":"cs.CV"}
{"created":"2024-05-30 15:14:24","title":"GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning","abstract":"Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG. Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.","sentences":["Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph.","Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG.","Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language.","On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG.","In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.","First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question.","Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths.","The extracted paths are verbalized and given as input for LLM reasoning with RAG.","In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA.","Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG.","Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM.","In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1."],"url":"http://arxiv.org/abs/2405.20139v1","category":"cs.CL"}
{"created":"2024-05-30 15:13:46","title":"Separation and Collapse of Equilibria Inequalities on AND-OR Trees without Shape Constraints","abstract":"Herein, we investigate the randomized complexity, which is the least cost against the worst input, of AND-OR tree computation by imposing various restrictions on the algorithm to find the Boolean value of the root of that tree and no restrictions on the tree shape. When a tree satisfies a certain condition regarding its symmetry, directional algorithms proposed by Saks and Wigderson (1986), special randomized algorithms, are known to achieve the randomized complexity. Furthermore, there is a known example of a tree that is so unbalanced that no directional algorithm achieves the randomized complexity (Vereshchagin 1998). In this study, we aim to identify where deviations arise between the general randomized Boolean decision tree and its special case, directional algorithms. In this paper, we show that for any AND-OR tree, randomized depth-first algorithms, which form a broader class compared with directional algorithms, have the same equilibrium as that of the directional algorithms. Thus, we get the collapse result on equilibria inequalities that holds for an arbitrary AND-OR tree. This implies that there exists a case where even depth-first algorithms cannot be the fastest, leading to the separation result on equilibria inequality. Additionally, a new algorithm is introduced as a key concept for proof of the separation result.","sentences":["Herein, we investigate the randomized complexity, which is the least cost against the worst input, of AND-OR tree computation by imposing various restrictions on the algorithm to find the Boolean value of the root of that tree and no restrictions on the tree shape.","When a tree satisfies a certain condition regarding its symmetry, directional algorithms proposed by Saks and Wigderson (1986), special randomized algorithms, are known to achieve the randomized complexity.","Furthermore, there is a known example of a tree that is so unbalanced that no directional algorithm achieves the randomized complexity (Vereshchagin 1998).","In this study, we aim to identify where deviations arise between the general randomized Boolean decision tree and its special case, directional algorithms.","In this paper, we show that for any AND-OR tree, randomized depth-first algorithms, which form a broader class compared with directional algorithms, have the same equilibrium as that of the directional algorithms.","Thus, we get the collapse result on equilibria inequalities that holds for an arbitrary AND-OR tree.","This implies that there exists a case where even depth-first algorithms cannot be the fastest, leading to the separation result on equilibria inequality.","Additionally, a new algorithm is introduced as a key concept for proof of the separation result."],"url":"http://arxiv.org/abs/2405.20138v1","category":"cs.AI"}
{"created":"2024-05-30 15:13:40","title":"A unified framework of principal component analysis and factor analysis","abstract":"Principal component analysis and factor analysis are fundamental multivariate analysis methods. In this paper a unified framework to connect them is introduced. Under a general latent variable model, we present matrix optimization problems from the viewpoint of loss function minimization, and show that the two methods can be viewed as solutions to the optimization problems with specific loss functions. Specifically, principal component analysis can be derived from a broad class of loss functions including the L2 norm, while factor analysis corresponds to a modified L0 norm problem. Related problems are discussed, including algorithms, penalized maximum likelihood estimation under the latent variable model, and a principal component factor model. These results can lead to new tools of data analysis and research topics.","sentences":["Principal component analysis and factor analysis are fundamental multivariate analysis methods.","In this paper a unified framework to connect them is introduced.","Under a general latent variable model, we present matrix optimization problems from the viewpoint of loss function minimization, and show that the two methods can be viewed as solutions to the optimization problems with specific loss functions.","Specifically, principal component analysis can be derived from a broad class of loss functions including the L2 norm, while factor analysis corresponds to a modified L0 norm problem.","Related problems are discussed, including algorithms, penalized maximum likelihood estimation under the latent variable model, and a principal component factor model.","These results can lead to new tools of data analysis and research topics."],"url":"http://arxiv.org/abs/2405.20137v1","category":"stat.ME"}
{"created":"2024-05-30 15:12:18","title":"A Multimodal Dangerous State Recognition and Early Warning System for Elderly with Intermittent Dementia","abstract":"In response to the social issue of the increasing number of elderly vulnerable groups going missing due to the aggravating aging population in China, our team has developed a wearable anti-loss device and intelligent early warning system for elderly individuals with intermittent dementia using artificial intelligence and IoT technology. This system comprises an anti-loss smart helmet, a cloud computing module, and an intelligent early warning application on the caregiver's mobile device. The smart helmet integrates a miniature camera module, a GPS module, and a 5G communication module to collect first-person images and location information of the elderly. Data is transmitted remotely via 5G, FTP, and TCP protocols. In the cloud computing module, our team has proposed for the first time a multimodal dangerous state recognition network based on scene and location information to accurately assess the risk of elderly individuals going missing. Finally, the application software interface designed for the caregiver's mobile device implements multi-level early warnings. The system developed by our team requires no operation or response from the elderly, achieving fully automatic environmental perception, risk assessment, and proactive alarming. This overcomes the limitations of traditional monitoring devices, which require active operation and response, thus avoiding the issue of the digital divide for the elderly. It effectively prevents accidental loss and potential dangers for elderly individuals with dementia.","sentences":["In response to the social issue of the increasing number of elderly vulnerable groups going missing due to the aggravating aging population in China, our team has developed a wearable anti-loss device and intelligent early warning system for elderly individuals with intermittent dementia using artificial intelligence and IoT technology.","This system comprises an anti-loss smart helmet, a cloud computing module, and an intelligent early warning application on the caregiver's mobile device.","The smart helmet integrates a miniature camera module, a GPS module, and a 5G communication module to collect first-person images and location information of the elderly.","Data is transmitted remotely via 5G, FTP, and TCP protocols.","In the cloud computing module, our team has proposed for the first time a multimodal dangerous state recognition network based on scene and location information to accurately assess the risk of elderly individuals going missing.","Finally, the application software interface designed for the caregiver's mobile device implements multi-level early warnings.","The system developed by our team requires no operation or response from the elderly, achieving fully automatic environmental perception, risk assessment, and proactive alarming.","This overcomes the limitations of traditional monitoring devices, which require active operation and response, thus avoiding the issue of the digital divide for the elderly.","It effectively prevents accidental loss and potential dangers for elderly individuals with dementia."],"url":"http://arxiv.org/abs/2405.20136v1","category":"cs.CV"}
{"created":"2024-05-30 15:12:02","title":"Classifying the Polish semigroup topologies on the symmetric inverse monoid","abstract":"We classify all Polish semigroup topologies on the symmetric inverse monoid on the natural numbers. This result answers a question of Elliott et al. There are countably infinitely many such topologies. Under containment, these Polish semigroup topologies form a join-semilattice with infinite descending chains, no infinite ascending chains, and arbitrarily large finite anti-chains. Also, we show that the monoid endowed with any second countable T_1 semigroup topology is homeomorphic to the Baire space.","sentences":["We classify all Polish semigroup topologies on the symmetric inverse monoid on the natural numbers.","This result answers a question of Elliott et al.","There are countably infinitely many such topologies.","Under containment, these Polish semigroup topologies form a join-semilattice with infinite descending chains, no infinite ascending chains, and arbitrarily large finite anti-chains.","Also, we show that the monoid endowed with any second countable T_1 semigroup topology is homeomorphic to the Baire space."],"url":"http://arxiv.org/abs/2405.20134v1","category":"math.RA"}
{"created":"2024-05-30 15:10:59","title":"LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating Metaheuristics","abstract":"Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets. This paper introduces a novel Large Language Model Evolutionary Algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms. Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates and selects algorithms based on performance metrics and feedback from runtime evaluations. This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise. We show how this framework can be used to generate novel black-box metaheuristic optimization algorithms automatically. LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (Covariance Matrix Adaptation Evolution Strategy and Differential Evolution) on the five dimensional black box optimization benchmark (BBOB). The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs.","sentences":["Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets.","This paper introduces a novel Large Language Model Evolutionary Algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms.","Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates and selects algorithms based on performance metrics and feedback from runtime evaluations.","This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise.","We show how this framework can be used to generate novel black-box metaheuristic optimization algorithms automatically.","LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (Covariance Matrix Adaptation Evolution Strategy and Differential Evolution) on the five dimensional black box optimization benchmark (BBOB).","The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs."],"url":"http://arxiv.org/abs/2405.20132v1","category":"cs.NE"}
{"created":"2024-05-30 15:10:37","title":"Language Models Need Inductive Biases to Count Inductively","abstract":"Counting is a fundamental example of generalization, whether viewed through the mathematical lens of Peano's axioms defining the natural numbers or the cognitive science literature for children learning to count. The argument holds for both cases that learning to count means learning to count infinitely. While few papers have tried to distill transformer \"reasoning\" to the simplest case of counting, investigating length generalization does occur throughout the literature. In the \"train short, test long\" paradigm of NLP, length refers to the training sentence length. In formal language recognition, length refers to the input sequence length, or the maximum stack size induced by a pushdown automata. In general problem solving, length refers to the number of hops in a deductive reasoning chain or the recursion depth. For all cases, counting is central to task success. And crucially, generalizing counting inductively is central to success on OOD instances. This work provides extensive empirical results on training language models to count. We experiment with architectures ranging from RNNs, Transformers, State-Space Models and RWKV. We present carefully-designed task formats, auxiliary tasks and positional embeddings to avoid limitations in generalization with OOD-position and OOD-vocabulary. We find that while traditional RNNs trivially achieve inductive counting, Transformers have to rely on positional embeddings to count out-of-domain. As counting is the basis for many arguments concerning the expressivity of Transformers, our finding calls for the community to reexamine the application scope of primitive functions defined in formal characterizations. Finally, modern RNNs also largely underperform traditional RNNs in generalizing counting inductively. We discuss how design choices that enable parallelized training of modern RNNs cause them to lose merits of a recurrent nature.","sentences":["Counting is a fundamental example of generalization, whether viewed through the mathematical lens of Peano's axioms defining the natural numbers or the cognitive science literature for children learning to count.","The argument holds for both cases that learning to count means learning to count infinitely.","While few papers have tried to distill transformer \"reasoning\" to the simplest case of counting, investigating length generalization does occur throughout the literature.","In the \"train short, test long\" paradigm of NLP, length refers to the training sentence length.","In formal language recognition, length refers to the input sequence length, or the maximum stack size induced by a pushdown automata.","In general problem solving, length refers to the number of hops in a deductive reasoning chain or the recursion depth.","For all cases, counting is central to task success.","And crucially, generalizing counting inductively is central to success on OOD instances.","This work provides extensive empirical results on training language models to count.","We experiment with architectures ranging from RNNs, Transformers, State-Space Models and RWKV.","We present carefully-designed task formats, auxiliary tasks and positional embeddings to avoid limitations in generalization with OOD-position and OOD-vocabulary.","We find that while traditional RNNs trivially achieve inductive counting, Transformers have to rely on positional embeddings to count out-of-domain.","As counting is the basis for many arguments concerning the expressivity of Transformers, our finding calls for the community to reexamine the application scope of primitive functions defined in formal characterizations.","Finally, modern RNNs also largely underperform traditional RNNs in generalizing counting inductively.","We discuss how design choices that enable parallelized training of modern RNNs cause them to lose merits of a recurrent nature."],"url":"http://arxiv.org/abs/2405.20131v1","category":"cs.CL"}
{"created":"2024-05-30 15:01:18","title":"A Geometric Unification of Distributionally Robust Covariance Estimators: Shrinking the Spectrum by Inflating the Ambiguity Set","abstract":"The state-of-the-art methods for estimating high-dimensional covariance matrices all shrink the eigenvalues of the sample covariance matrix towards a data-insensitive shrinkage target. The underlying shrinkage transformation is either chosen heuristically - without compelling theoretical justification - or optimally in view of restrictive distributional assumptions. In this paper, we propose a principled approach to construct covariance estimators without imposing restrictive assumptions. That is, we study distributionally robust covariance estimation problems that minimize the worst-case Frobenius error with respect to all data distributions close to a nominal distribution, where the proximity of distributions is measured via a divergence on the space of covariance matrices. We identify mild conditions on this divergence under which the resulting minimizers represent shrinkage estimators. We show that the corresponding shrinkage transformations are intimately related to the geometrical properties of the underlying divergence. We also prove that our robust estimators are efficiently computable and asymptotically consistent and that they enjoy finite-sample performance guarantees. We exemplify our general methodology by synthesizing explicit estimators induced by the Kullback-Leibler, Fisher-Rao, and Wasserstein divergences. Numerical experiments based on synthetic and real data show that our robust estimators are competitive with state-of-the-art estimators.","sentences":["The state-of-the-art methods for estimating high-dimensional covariance matrices all shrink the eigenvalues of the sample covariance matrix towards a data-insensitive shrinkage target.","The underlying shrinkage transformation is either chosen heuristically - without compelling theoretical justification - or optimally in view of restrictive distributional assumptions.","In this paper, we propose a principled approach to construct covariance estimators without imposing restrictive assumptions.","That is, we study distributionally robust covariance estimation problems that minimize the worst-case Frobenius error with respect to all data distributions close to a nominal distribution, where the proximity of distributions is measured via a divergence on the space of covariance matrices.","We identify mild conditions on this divergence under which the resulting minimizers represent shrinkage estimators.","We show that the corresponding shrinkage transformations are intimately related to the geometrical properties of the underlying divergence.","We also prove that our robust estimators are efficiently computable and asymptotically consistent and that they enjoy finite-sample performance guarantees.","We exemplify our general methodology by synthesizing explicit estimators induced by the Kullback-Leibler, Fisher-Rao, and Wasserstein divergences.","Numerical experiments based on synthetic and real data show that our robust estimators are competitive with state-of-the-art estimators."],"url":"http://arxiv.org/abs/2405.20124v1","category":"stat.ML"}
{"created":"2024-05-30 15:00:31","title":"Exact resolution of a simultaneous vehicle routing and crew scheduling problem in long-haul transport","abstract":"This work focuses on exact methods for a Simultaneous Vehicle Routing and Crew Scheduling Problem in long-haul transport. Pickup-and-delivery requests with time windows must be fullfiled over a multi-day planning horizon. Unlike some classic approaches, the correspondence between trucks and drivers is not fixed and they can be exchanged in some locations and at any time. Drivers can also travel for free as truck passengers or take external taxis for an additional cost. The objective is to minimise the truck and taxi travel costs and the penalties for late deliveries. Routes for trucks and drivers are represented separately as directed paths in certain digraphs and then synchronised in time and space. Three compact Integer Linear Programming formulations are proposed and many families of valid inequalities are described. Extensive computational experiments are conducted on randomly generated instances. The formulations are experimentally compared and the effectiveness of the proposed valid inequalities as cutting planes in a branch-and-cut algorithm is evaluated.","sentences":["This work focuses on exact methods for a Simultaneous Vehicle Routing and Crew Scheduling Problem in long-haul transport.","Pickup-and-delivery requests with time windows must be fullfiled over a multi-day planning horizon.","Unlike some classic approaches, the correspondence between trucks and drivers is not fixed and they can be exchanged in some locations and at any time.","Drivers can also travel for free as truck passengers or take external taxis for an additional cost.","The objective is to minimise the truck and taxi travel costs and the penalties for late deliveries.","Routes for trucks and drivers are represented separately as directed paths in certain digraphs and then synchronised in time and space.","Three compact Integer Linear Programming formulations are proposed and many families of valid inequalities are described.","Extensive computational experiments are conducted on randomly generated instances.","The formulations are experimentally compared and the effectiveness of the proposed valid inequalities as cutting planes in a branch-and-cut algorithm is evaluated."],"url":"http://arxiv.org/abs/2405.20123v1","category":"math.OC"}
{"created":"2024-05-30 14:57:16","title":"A Structure-Aware Lane Graph Transformer Model for Vehicle Trajectory Prediction","abstract":"Accurate prediction of future trajectories for surrounding vehicles is vital for the safe operation of autonomous vehicles. This study proposes a Lane Graph Transformer (LGT) model with structure-aware capabilities. Its key contribution lies in encoding the map topology structure into the attention mechanism. To address variations in lane information from different directions, four Relative Positional Encoding (RPE) matrices are introduced to capture the local details of the map topology structure. Additionally, two Shortest Path Distance (SPD) matrices are employed to capture distance information between two accessible lanes. Numerical results indicate that the proposed LGT model achieves a significantly higher prediction performance on the Argoverse 2 dataset. Specifically, the minFDE$_6$ metric was decreased by 60.73% compared to the Argoverse 2 baseline model (Nearest Neighbor) and the b-minFDE$_6$ metric was reduced by 2.65% compared to the baseline LaneGCN model. Furthermore, ablation experiments demonstrated that the consideration of map topology structure led to a 4.24% drop in the b-minFDE$_6$ metric, validating the effectiveness of this model.","sentences":["Accurate prediction of future trajectories for surrounding vehicles is vital for the safe operation of autonomous vehicles.","This study proposes a Lane Graph Transformer (LGT) model with structure-aware capabilities.","Its key contribution lies in encoding the map topology structure into the attention mechanism.","To address variations in lane information from different directions, four Relative Positional Encoding (RPE) matrices are introduced to capture the local details of the map topology structure.","Additionally, two Shortest Path Distance (SPD) matrices are employed to capture distance information between two accessible lanes.","Numerical results indicate that the proposed LGT model achieves a significantly higher prediction performance on the Argoverse 2 dataset.","Specifically, the minFDE$_6$ metric was decreased by 60.73% compared to the Argoverse 2 baseline model (Nearest Neighbor) and the b-minFDE$_6$ metric was reduced by 2.65% compared to the baseline LaneGCN model.","Furthermore, ablation experiments demonstrated that the consideration of map topology structure led to a 4.24% drop in the b-minFDE$_6$ metric, validating the effectiveness of this model."],"url":"http://arxiv.org/abs/2405.20121v1","category":"cs.AI"}
{"created":"2024-05-30 14:52:14","title":"Complexity of Zeroth- and First-order Stochastic Trust-Region Algorithms","abstract":"Model update (MU) and candidate evaluation (CE) are classical steps incorporated inside many stochastic trust-region (TR) algorithms. The sampling effort exerted within these steps, often decided with the aim of controlling model error, largely determines a stochastic TR algorithm's sample complexity. Given that MU and CE are amenable to variance reduction, we investigate the effect of incorporating common random numbers (CRN) within MU and CE on complexity. Using ASTRO and ASTRO-DF as prototype first-order and zeroth-order families of algorithms, we demonstrate that CRN's effectiveness leads to a range of complexities depending on sample-path regularity and the oracle order. For instance, we find that in first-order oracle settings with smooth sample paths, CRN's effect is pronounced -- ASTRO with CRN achieves $\\tilde{O}(\\epsilon^{-2})$ a.s. sample complexity compared to $\\tilde{O}(\\epsilon^{-6})$ a.s. in the generic no-CRN setting. By contrast, CRN's effect is muted when the sample paths are not Lipschitz, with the sample complexity improving from $\\tilde{O}(\\epsilon^{-6})$ a.s. to $\\tilde{O}(\\epsilon^{-5})$ and $\\tilde{O}(\\epsilon^{-4})$ a.s. in the zeroth- and first-order settings, respectively. Since our results imply that improvements in complexity are largely inherited from generic aspects of variance reduction, e.g., finite-differencing for zeroth-order settings and sample-path smoothness for first-order settings within MU, we anticipate similar trends in other contexts.","sentences":["Model update (MU) and candidate evaluation (CE) are classical steps incorporated inside many stochastic trust-region (TR) algorithms.","The sampling effort exerted within these steps, often decided with the aim of controlling model error, largely determines a stochastic TR algorithm's sample complexity.","Given that MU and CE are amenable to variance reduction, we investigate the effect of incorporating common random numbers (CRN) within MU and CE on complexity.","Using ASTRO and ASTRO-DF as prototype first-order and zeroth-order families of algorithms, we demonstrate that CRN's effectiveness leads to a range of complexities depending on sample-path regularity and the oracle order.","For instance, we find that in first-order oracle settings with smooth sample paths, CRN's effect is pronounced -- ASTRO with CRN achieves $\\tilde{O}(\\epsilon^{-2})$ a.s. sample complexity compared to $\\tilde{O}(\\epsilon^{-6})$ a.s.","in the generic no-CRN setting.","By contrast, CRN's effect is muted when the sample paths are not Lipschitz, with the sample complexity improving from $\\tilde{O}(\\epsilon^{-6})$ a.s. to $\\tilde{O}(\\epsilon^{-5})$ and $\\tilde{O}(\\epsilon^{-4})$ a.s.","in the zeroth- and first-order settings, respectively.","Since our results imply that improvements in complexity are largely inherited from generic aspects of variance reduction, e.g., finite-differencing for zeroth-order settings and sample-path smoothness for first-order settings within MU, we anticipate similar trends in other contexts."],"url":"http://arxiv.org/abs/2405.20116v1","category":"math.OC"}
{"created":"2024-05-30 14:51:57","title":"Near Optimal Decentralized Optimization with Compression and Momentum Tracking","abstract":"Communication efficiency has garnered significant attention as it is considered the main bottleneck for large-scale decentralized Machine Learning applications in distributed and federated settings. In this regime, clients are restricted to transmitting small amounts of quantized information to their neighbors over a communication graph. Numerous endeavors have been made to address this challenging problem by developing algorithms with compressed communication for decentralized non-convex optimization problems. Despite considerable efforts, the current results suffer from various issues such as non-scalability with the number of clients, requirements for large batches, or bounded gradient assumption. In this paper, we introduce MoTEF, a novel approach that integrates communication compression with Momentum Tracking and Error Feedback. Our analysis demonstrates that MoTEF achieves most of the desired properties, and significantly outperforms existing methods under arbitrary data heterogeneity. We provide numerical experiments to validate our theoretical findings and confirm the practical superiority of MoTEF.","sentences":["Communication efficiency has garnered significant attention as it is considered the main bottleneck for large-scale decentralized Machine Learning applications in distributed and federated settings.","In this regime, clients are restricted to transmitting small amounts of quantized information to their neighbors over a communication graph.","Numerous endeavors have been made to address this challenging problem by developing algorithms with compressed communication for decentralized non-convex optimization problems.","Despite considerable efforts, the current results suffer from various issues such as non-scalability with the number of clients, requirements for large batches, or bounded gradient assumption.","In this paper, we introduce MoTEF, a novel approach that integrates communication compression with Momentum Tracking and Error Feedback.","Our analysis demonstrates that MoTEF achieves most of the desired properties, and significantly outperforms existing methods under arbitrary data heterogeneity.","We provide numerical experiments to validate our theoretical findings and confirm the practical superiority of MoTEF."],"url":"http://arxiv.org/abs/2405.20114v1","category":"cs.LG"}
{"created":"2024-05-30 14:49:54","title":"RIGID: A Training-free and Model-Agnostic Framework for Robust AI-Generated Image Detection","abstract":"The rapid advances in generative AI models have empowered the creation of highly realistic images with arbitrary content, raising concerns about potential misuse and harm, such as Deepfakes. Current research focuses on training detectors using large datasets of generated images. However, these training-based solutions are often computationally expensive and show limited generalization to unseen generated images. In this paper, we propose a training-free method to distinguish between real and AI-generated images. We first observe that real images are more robust to tiny noise perturbations than AI-generated images in the representation space of vision foundation models. Based on this observation, we propose RIGID, a training-free and model-agnostic method for robust AI-generated image detection. RIGID is a simple yet effective approach that identifies whether an image is AI-generated by comparing the representation similarity between the original and the noise-perturbed counterpart. Our evaluation on a diverse set of AI-generated images and benchmarks shows that RIGID significantly outperforms existing trainingbased and training-free detectors. In particular, the average performance of RIGID exceeds the current best training-free method by more than 25%. Importantly, RIGID exhibits strong generalization across different image generation methods and robustness to image corruptions.","sentences":["The rapid advances in generative AI models have empowered the creation of highly realistic images with arbitrary content, raising concerns about potential misuse and harm, such as Deepfakes.","Current research focuses on training detectors using large datasets of generated images.","However, these training-based solutions are often computationally expensive and show limited generalization to unseen generated images.","In this paper, we propose a training-free method to distinguish between real and AI-generated images.","We first observe that real images are more robust to tiny noise perturbations than AI-generated images in the representation space of vision foundation models.","Based on this observation, we propose RIGID, a training-free and model-agnostic method for robust AI-generated image detection.","RIGID is a simple yet effective approach that identifies whether an image is AI-generated by comparing the representation similarity between the original and the noise-perturbed counterpart.","Our evaluation on a diverse set of AI-generated images and benchmarks shows that RIGID significantly outperforms existing trainingbased and training-free detectors.","In particular, the average performance of RIGID exceeds the current best training-free method by more than 25%.","Importantly, RIGID exhibits strong generalization across different image generation methods and robustness to image corruptions."],"url":"http://arxiv.org/abs/2405.20112v1","category":"cs.CV"}
{"created":"2024-05-30 14:45:02","title":"FMARS: Annotating Remote Sensing Images for Disaster Management using Foundation Models","abstract":"Very-High Resolution (VHR) remote sensing imagery is increasingly accessible, but often lacks annotations for effective machine learning applications. Recent foundation models like GroundingDINO and Segment Anything (SAM) provide opportunities to automatically generate annotations. This study introduces FMARS (Foundation Model Annotations in Remote Sensing), a methodology leveraging VHR imagery and foundation models for fast and robust annotation. We focus on disaster management and provide a large-scale dataset with labels obtained from pre-event imagery over 19 disaster events, derived from the Maxar Open Data initiative. We train segmentation models on the generated labels, using Unsupervised Domain Adaptation (UDA) techniques to increase transferability to real-world scenarios. Our results demonstrate the effectiveness of leveraging foundation models to automatically annotate remote sensing data at scale, enabling robust downstream models for critical applications. Code and dataset are available at \\url{https://github.com/links-ads/igarss-fmars}.","sentences":["Very-High Resolution (VHR) remote sensing imagery is increasingly accessible, but often lacks annotations for effective machine learning applications.","Recent foundation models like GroundingDINO and Segment Anything (SAM) provide opportunities to automatically generate annotations.","This study introduces FMARS (Foundation Model Annotations in Remote Sensing), a methodology leveraging VHR imagery and foundation models for fast and robust annotation.","We focus on disaster management and provide a large-scale dataset with labels obtained from pre-event imagery over 19 disaster events, derived from the Maxar Open Data initiative.","We train segmentation models on the generated labels, using Unsupervised Domain Adaptation (UDA) techniques to increase transferability to real-world scenarios.","Our results demonstrate the effectiveness of leveraging foundation models to automatically annotate remote sensing data at scale, enabling robust downstream models for critical applications.","Code and dataset are available at \\url{https://github.com/links-ads/igarss-fmars}."],"url":"http://arxiv.org/abs/2405.20109v1","category":"cs.CV"}
{"created":"2024-05-30 14:43:12","title":"A Perspective on the Impact of Group Delay Dispersion in Future Terahertz Wireless Systems","abstract":"This article discusses the challenges and opportunities of managing group delay dispersion (GDD) and its relation to the performance standards of future sixth-generation (6G) wireless communication systems utilizing terahertz frequency waves. The unique susceptibilities of 6G systems to GDD are described, along with a quantitative description of the sources of GDD, including multipath, rough surface scattering, intelligent reflecting surfaces, and propagation through the atmosphere. An experimental case-study is presented that confirms previous models quantifying the impact of atmospheric GDD. Several GDD manipulation strategies are presented illustrating their hindered effectiveness in the 6G context. Conversely, some benefits of leveraging GDD to enhance 6G systems, such as improved security and simplified hardware, are also discussed. Finally, a perspective on using photonic GDD control devices is provided, revealing quantitative benefits that may unburden existing equalization schemes. The article argues that GDD will uniquely and significantly impact some 6G systems, but that its careful consideration along with new mitigation strategies, including photonic devices, will help optimize system performance. The conclusion provides a perspective to guide future research in this area.","sentences":["This article discusses the challenges and opportunities of managing group delay dispersion (GDD) and its relation to the performance standards of future sixth-generation (6G) wireless communication systems utilizing terahertz frequency waves.","The unique susceptibilities of 6G systems to GDD are described, along with a quantitative description of the sources of GDD, including multipath, rough surface scattering, intelligent reflecting surfaces, and propagation through the atmosphere.","An experimental case-study is presented that confirms previous models quantifying the impact of atmospheric GDD.","Several GDD manipulation strategies are presented illustrating their hindered effectiveness in the 6G context.","Conversely, some benefits of leveraging GDD to enhance 6G systems, such as improved security and simplified hardware, are also discussed.","Finally, a perspective on using photonic GDD control devices is provided, revealing quantitative benefits that may unburden existing equalization schemes.","The article argues that GDD will uniquely and significantly impact some 6G systems, but that its careful consideration along with new mitigation strategies, including photonic devices, will help optimize system performance.","The conclusion provides a perspective to guide future research in this area."],"url":"http://arxiv.org/abs/2405.20107v1","category":"eess.SP"}
{"created":"2024-05-30 14:41:05","title":"Dynamic Slack Bus","abstract":"This letter proposes a general dynamic formulation of slack bus. With this aim, the angle constraint imposed by the slack bus is redefined as a set of differential equations and an energy source. The existence and role of the transient component of this source is also discussed in the letter. Based on this framework, the letter shows that the swing equations of synchronous machines can be interpreted as distributed, dynamic, multi-variable, local slack buses. Other relevant cases, including primary and secondary frequency regulation, passive loads as well as grid following and grid forming converters are discussed.","sentences":["This letter proposes a general dynamic formulation of slack bus.","With this aim, the angle constraint imposed by the slack bus is redefined as a set of differential equations and an energy source.","The existence and role of the transient component of this source is also discussed in the letter.","Based on this framework, the letter shows that the swing equations of synchronous machines can be interpreted as distributed, dynamic, multi-variable, local slack buses.","Other relevant cases, including primary and secondary frequency regulation, passive loads as well as grid following and grid forming converters are discussed."],"url":"http://arxiv.org/abs/2405.20100v1","category":"eess.SY"}
{"created":"2024-05-30 14:35:23","title":"Investigating pump harmonics generation in a SNAIL-based Traveling Wave Parametric Amplifier","abstract":"Traveling Wave Parametric Amplifiers (TWPAs) are extensively employed in experiments involving weak microwave signals for their highly desirable quantum-limited and broadband characteristics. However, TWPAs' broadband nature comes with the disadvantage of admitting the activation of spurious nonlinear processes, such as harmonics generation, that can potentially degrade amplification performance. Here we experimentally investigate a Josephson TWPA device with SNAIL (Superconducting Nonlinear Asymmetric Inductive Element)-based unit cells focusing on the amplification behaviour along with the generation of second and third harmonics of the pump. By comparing experimental results with transient numerical simulations, we demonstrate the influence of Josephson junctions' fabrication imperfections on the occurrence of harmonics and on the gain behaviour.","sentences":["Traveling Wave Parametric Amplifiers (TWPAs) are extensively employed in experiments involving weak microwave signals for their highly desirable quantum-limited and broadband characteristics.","However, TWPAs' broadband nature comes with the disadvantage of admitting the activation of spurious nonlinear processes, such as harmonics generation, that can potentially degrade amplification performance.","Here we experimentally investigate a Josephson TWPA device with SNAIL (Superconducting Nonlinear Asymmetric Inductive Element)-based unit cells focusing on the amplification behaviour along with the generation of second and third harmonics of the pump.","By comparing experimental results with transient numerical simulations, we demonstrate the influence of Josephson junctions' fabrication imperfections on the occurrence of harmonics and on the gain behaviour."],"url":"http://arxiv.org/abs/2405.20096v1","category":"quant-ph"}
{"created":"2024-05-30 14:31:33","title":"Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation","abstract":"Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements. Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage self-tests to refine the generated program. Yet, planning deep-inside requirements in advance can be challenging, and the tests need to be accurate to accomplish self-improvement. To this end, we propose FunCoder, a code generation framework incorporating the divide-and-conquer strategy with functional consensus. Specifically, FunCoder recursively branches off sub-functions as smaller goals during code generation, represented by a tree hierarchy. These sub-functions are then composited to attain more complex objectives. Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation. FunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval, MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method demonstrates superiority on smaller models: With FunCoder, StableCode-3b surpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on HumanEval. Further analysis reveals that our proposed dynamic function decomposition is capable of handling complex requirements, and the functional consensus prevails over self-testing in correctness evaluation.","sentences":["Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements.","Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage self-tests to refine the generated program.","Yet, planning deep-inside requirements in advance can be challenging, and the tests need to be accurate to accomplish self-improvement.","To this end, we propose FunCoder, a code generation framework incorporating the divide-and-conquer strategy with functional consensus.","Specifically, FunCoder recursively branches off sub-functions as smaller goals during code generation, represented by a tree hierarchy.","These sub-functions are then composited to attain more complex objectives.","Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation.","FunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval, MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4.","Moreover, our method demonstrates superiority on smaller models: With FunCoder, StableCode-3b surpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on HumanEval.","Further analysis reveals that our proposed dynamic function decomposition is capable of handling complex requirements, and the functional consensus prevails over self-testing in correctness evaluation."],"url":"http://arxiv.org/abs/2405.20092v1","category":"cs.CL"}
{"created":"2024-05-30 14:27:20","title":"Typography Leads Semantic Diversifying: Amplifying Adversarial Transferability across Multimodal Large Language Models","abstract":"Following the advent of the Artificial Intelligence (AI) era of large models, Multimodal Large Language Models (MLLMs) with the ability to understand cross-modal interactions between vision and text have attracted wide attention. Adversarial examples with human-imperceptible perturbation are shown to possess a characteristic known as transferability, which means that a perturbation generated by one model could also mislead another different model. Augmenting the diversity in input data is one of the most significant methods for enhancing adversarial transferability. This method has been certified as a way to significantly enlarge the threat impact under black-box conditions. Research works also demonstrate that MLLMs can be exploited to generate adversarial examples in the white-box scenario. However, the adversarial transferability of such perturbations is quite limited, failing to achieve effective black-box attacks across different models. In this paper, we propose the Typographic-based Semantic Transfer Attack (TSTA), which is inspired by: (1) MLLMs tend to process semantic-level information; (2) Typographic Attack could effectively distract the visual information captured by MLLMs. In the scenarios of Harmful Word Insertion and Important Information Protection, our TSTA demonstrates superior performance.","sentences":["Following the advent of the Artificial Intelligence (AI) era of large models, Multimodal Large Language Models (MLLMs) with the ability to understand cross-modal interactions between vision and text have attracted wide attention.","Adversarial examples with human-imperceptible perturbation are shown to possess a characteristic known as transferability, which means that a perturbation generated by one model could also mislead another different model.","Augmenting the diversity in input data is one of the most significant methods for enhancing adversarial transferability.","This method has been certified as a way to significantly enlarge the threat impact under black-box conditions.","Research works also demonstrate that MLLMs can be exploited to generate adversarial examples in the white-box scenario.","However, the adversarial transferability of such perturbations is quite limited, failing to achieve effective black-box attacks across different models.","In this paper, we propose the Typographic-based Semantic Transfer Attack (TSTA), which is inspired by: (1) MLLMs tend to process semantic-level information; (2) Typographic Attack could effectively distract the visual information captured by MLLMs.","In the scenarios of Harmful Word Insertion and Important Information Protection, our TSTA demonstrates superior performance."],"url":"http://arxiv.org/abs/2405.20090v1","category":"cs.CV"}
{"created":"2024-05-30 14:25:56","title":"The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities","abstract":"Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters. Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.","sentences":["Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality.","However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations.","We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters.","Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade.","In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation.","On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data.","We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality.","Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation."],"url":"http://arxiv.org/abs/2405.20089v1","category":"cs.CL"}
{"created":"2024-05-30 14:23:20","title":"Personalized Predictions from Population Level Experiments: A Study on Alzheimer's Disease","abstract":"The purpose of this article is to infer patient level outcomes from population level randomized control trials (RCTs). In this pursuit, we utilize the recently proposed synthetic nearest neighbors (SNN) estimator. At its core, SNN leverages information across patients to impute missing data associated with each patient of interest. We focus on two types of missing data: (i) unrecorded outcomes from discontinuing the assigned treatments and (ii) unobserved outcomes associated with unassigned treatments. Data imputation in the former powers and de-biases RCTs, while data imputation in the latter simulates \"synthetic RCTs\" to predict the outcomes for each patient under every treatment. The SNN estimator is interpretable, transparent, and causally justified under a broad class of missing data scenarios. Relative to several standard methods, we empirically find that SNN performs well for the above two applications using Phase 3 clinical trial data on patients with Alzheimer's Disease. Our findings directly suggest that SNN can tackle a current pain point within the clinical trial workflow on patient dropouts and serve as a new tool towards the development of precision medicine. Building on our insights, we discuss how SNN can further generalize to real-world applications.","sentences":["The purpose of this article is to infer patient level outcomes from population level randomized control trials (RCTs).","In this pursuit, we utilize the recently proposed synthetic nearest neighbors (SNN) estimator.","At its core, SNN leverages information across patients to impute missing data associated with each patient of interest.","We focus on two types of missing data: (i) unrecorded outcomes from discontinuing the assigned treatments and (ii) unobserved outcomes associated with unassigned treatments.","Data imputation in the former powers and de-biases RCTs, while data imputation in the latter simulates \"synthetic RCTs\" to predict the outcomes for each patient under every treatment.","The SNN estimator is interpretable, transparent, and causally justified under a broad class of missing data scenarios.","Relative to several standard methods, we empirically find that SNN performs well for the above two applications using Phase 3 clinical trial data on patients with Alzheimer's Disease.","Our findings directly suggest that SNN can tackle a current pain point within the clinical trial workflow on patient dropouts and serve as a new tool towards the development of precision medicine.","Building on our insights, we discuss how SNN can further generalize to real-world applications."],"url":"http://arxiv.org/abs/2405.20088v1","category":"stat.AP"}
{"created":"2024-05-30 14:20:53","title":"Characterization of probability distributions on some locally compact Abelian groups containing an element of order 2","abstract":"The well-known Heyde theorem characterizes the Gaussian distributions on the real line by the symmetry of the conditional distribution of one linear form of independent random variables given another. We generalize this theorem to groups of the form $\\mathbb{R}\\times F$, where $F$ is a finite Abelian group such that its 2-component is isomorphic to the additive group of the integers modulo $2$. In so doing, coefficients of the linear forms are arbitrary topological automorphisms of the group. Previously, a similar result was proved in the case when the group $F$ contains no elements of order 2. The presence of an element of order 2 in $F$ leads to the fact that a new class of probability distributions is characterized","sentences":["The well-known Heyde theorem characterizes the Gaussian distributions on the real line by the symmetry of the conditional distribution of one linear form of independent random variables given another.","We generalize this theorem to groups of the form $\\mathbb{R}\\times F$, where $F$ is a finite Abelian group such that its 2-component is isomorphic to the additive group of the integers modulo $2$.","In so doing, coefficients of the linear forms are arbitrary topological automorphisms of the group.","Previously, a similar result was proved in the case when the group $F$ contains no elements of order 2.","The presence of an element of order 2 in $F$ leads to the fact that a new class of probability distributions is characterized"],"url":"http://arxiv.org/abs/2405.20087v1","category":"math.PR"}
{"created":"2024-05-30 14:14:39","title":"Estimating Human Poses Across Datasets: A Unified Skeleton and Multi-Teacher Distillation Approach","abstract":"Human pose estimation is a key task in computer vision with various applications such as activity recognition and interactive systems. However, the lack of consistency in the annotated skeletons across different datasets poses challenges in developing universally applicable models. To address this challenge, we propose a novel approach integrating multi-teacher knowledge distillation with a unified skeleton representation. Our networks are jointly trained on the COCO and MPII datasets, containing 17 and 16 keypoints, respectively. We demonstrate enhanced adaptability by predicting an extended set of 21 keypoints, 4 (COCO) and 5 (MPII) more than original annotations, improving cross-dataset generalization. Our joint models achieved an average accuracy of 70.89 and 76.40, compared to 53.79 and 55.78 when trained on a single dataset and evaluated on both. Moreover, we also evaluate all 21 predicted points by our two models by reporting an AP of 66.84 and 72.75 on the Halpe dataset. This highlights the potential of our technique to address one of the most pressing challenges in pose estimation research and application - the inconsistency in skeletal annotations.","sentences":["Human pose estimation is a key task in computer vision with various applications such as activity recognition and interactive systems.","However, the lack of consistency in the annotated skeletons across different datasets poses challenges in developing universally applicable models.","To address this challenge, we propose a novel approach integrating multi-teacher knowledge distillation with a unified skeleton representation.","Our networks are jointly trained on the COCO and MPII datasets, containing 17 and 16 keypoints, respectively.","We demonstrate enhanced adaptability by predicting an extended set of 21 keypoints, 4 (COCO) and 5 (MPII) more than original annotations, improving cross-dataset generalization.","Our joint models achieved an average accuracy of 70.89 and 76.40, compared to 53.79 and 55.78 when trained on a single dataset and evaluated on both.","Moreover, we also evaluate all 21 predicted points by our two models by reporting an AP of 66.84 and 72.75 on the Halpe dataset.","This highlights the potential of our technique to address one of the most pressing challenges in pose estimation research and application - the inconsistency in skeletal annotations."],"url":"http://arxiv.org/abs/2405.20084v1","category":"cs.CV"}
{"created":"2024-05-30 14:11:29","title":"Segment, Shuffle, and Stitch: A Simple Mechanism for Improving Time-Series Representations","abstract":"Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning. However, non-adjacent sections of real-world time-series may have strong dependencies. Accordingly we raise the question: Is there an alternative arrangement for time-series which could enable more effective representation learning? To address this, we propose a simple plug-and-play mechanism called Segment, Shuffle, and Stitch (S3) designed to improve time-series representation learning of existing models. S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is the most optimal for the task at hand. It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence. S3 is modular and can be stacked to create various degrees of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead. Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification and forecasting, improving performance on certain datasets by up to 68\\%. We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline. The code is available at https://github.com/shivam-grover/S3-TimeSeries .","sentences":["Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning.","However, non-adjacent sections of real-world time-series may have strong dependencies.","Accordingly we raise the question: Is there an alternative arrangement for time-series which could enable more effective representation learning?","To address this, we propose a simple plug-and-play mechanism called Segment, Shuffle, and Stitch (S3) designed to improve time-series representation learning of existing models.","S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is the most optimal for the task at hand.","It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence.","S3 is modular and can be stacked to create various degrees of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead.","Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification and forecasting, improving performance on certain datasets by up to 68\\%.","We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline.","The code is available at https://github.com/shivam-grover/S3-TimeSeries ."],"url":"http://arxiv.org/abs/2405.20082v1","category":"cs.LG"}
{"created":"2024-05-30 14:11:27","title":"NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models","abstract":"Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models. However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images. Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information. In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations. Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning. Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data. Code and models are available at https://kaiwu5.github.io/noiseboost.","sentences":["Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models.","However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images.","Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information.","In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations.","Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens.","Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning.","Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data.","Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data.","Code and models are available at https://kaiwu5.github.io/noiseboost."],"url":"http://arxiv.org/abs/2405.20081v1","category":"cs.CV"}
{"created":"2024-05-30 14:09:43","title":"Student Answer Forecasting: Transformer-Driven Answer Choice Prediction for Language Learning","abstract":"Intelligent Tutoring Systems (ITS) enhance personalized learning by predicting student answers to provide immediate and customized instruction. However, recent research has primarily focused on the correctness of the answer rather than the student's performance on specific answer choices, limiting insights into students' thought processes and potential misconceptions. To address this gap, we present MCQStudentBert, an answer forecasting model that leverages the capabilities of Large Language Models (LLMs) to integrate contextual understanding of students' answering history along with the text of the questions and answers. By predicting the specific answer choices students are likely to make, practitioners can easily extend the model to new answer choices or remove answer choices for the same multiple-choice question (MCQ) without retraining the model. In particular, we compare MLP, LSTM, BERT, and Mistral 7B architectures to generate embeddings from students' past interactions, which are then incorporated into a finetuned BERT's answer-forecasting mechanism. We apply our pipeline to a dataset of language learning MCQ, gathered from an ITS with over 10,000 students to explore the predictive accuracy of MCQStudentBert, which incorporates student interaction patterns, in comparison to correct answer prediction and traditional mastery-learning feature-based approaches. This work opens the door to more personalized content, modularization, and granular support.","sentences":["Intelligent Tutoring Systems (ITS) enhance personalized learning by predicting student answers to provide immediate and customized instruction.","However, recent research has primarily focused on the correctness of the answer rather than the student's performance on specific answer choices, limiting insights into students' thought processes and potential misconceptions.","To address this gap, we present MCQStudentBert, an answer forecasting model that leverages the capabilities of Large Language Models (LLMs) to integrate contextual understanding of students' answering history along with the text of the questions and answers.","By predicting the specific answer choices students are likely to make, practitioners can easily extend the model to new answer choices or remove answer choices for the same multiple-choice question (MCQ) without retraining the model.","In particular, we compare MLP, LSTM, BERT, and Mistral 7B architectures to generate embeddings from students' past interactions, which are then incorporated into a finetuned BERT's answer-forecasting mechanism.","We apply our pipeline to a dataset of language learning MCQ, gathered from an ITS with over 10,000 students to explore the predictive accuracy of MCQStudentBert, which incorporates student interaction patterns, in comparison to correct answer prediction and traditional mastery-learning feature-based approaches.","This work opens the door to more personalized content, modularization, and granular support."],"url":"http://arxiv.org/abs/2405.20079v1","category":"cs.CL"}
{"created":"2024-05-30 14:08:09","title":"NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics Evaluation","abstract":"Neural radiance fields (NeRF) are a groundbreaking computer vision technology that enables the generation of high-quality, immersive visual content from multiple viewpoints. This capability holds significant advantages for applications such as virtual/augmented reality, 3D modelling and content creation for the film and entertainment industry. However, the evaluation of NeRF methods poses several challenges, including a lack of comprehensive datasets, reliable assessment methodologies, and objective quality metrics. This paper addresses the problem of NeRF quality assessment thoroughly, by conducting a rigorous subjective quality assessment test that considers several scene classes and recently proposed NeRF view synthesis methods. Additionally, the performance of a wide range of state-of-the-art conventional and learning-based full-reference 2D image and video quality assessment metrics is evaluated against the subjective scores of the subjective study. The experimental results are analyzed in depth, providing a comparative evaluation of several NeRF methods and objective quality metrics, across different classes of visual scenes, including real and synthetic content for front-face and 360-degree camera trajectories.","sentences":["Neural radiance fields (NeRF) are a groundbreaking computer vision technology that enables the generation of high-quality, immersive visual content from multiple viewpoints.","This capability holds significant advantages for applications such as virtual/augmented reality, 3D modelling and content creation for the film and entertainment industry.","However, the evaluation of NeRF methods poses several challenges, including a lack of comprehensive datasets, reliable assessment methodologies, and objective quality metrics.","This paper addresses the problem of NeRF quality assessment thoroughly, by conducting a rigorous subjective quality assessment test that considers several scene classes and recently proposed NeRF view synthesis methods.","Additionally, the performance of a wide range of state-of-the-art conventional and learning-based full-reference 2D image and video quality assessment metrics is evaluated against the subjective scores of the subjective study.","The experimental results are analyzed in depth, providing a comparative evaluation of several NeRF methods and objective quality metrics, across different classes of visual scenes, including real and synthetic content for front-face and 360-degree camera trajectories."],"url":"http://arxiv.org/abs/2405.20078v1","category":"cs.MM"}
{"created":"2024-05-30 14:05:20","title":"On Low-Rank Multiplicity-Free Fusion Categories","abstract":"This thesis explains the methods and algorithms we used to obtain explicit F symbols, R symbols, and pivotal coefficients of all multiplicity-free pivotal fusion categories up to rank 7. The thesis starts by introducing the concept of a unitary modular fusion system via two applications: modeling anyons for topological quantum computation and calculating braid group representations. Next, the notions of a pivotal, spherical, braided, ribbon, and modular fusion system are introduced. Unitarity and its implications on the pivotal structure are discussed as well. The next part of the thesis is devoted to algorithms for finding fusion systems and compatible structures. First, an algorithm to find low-rank fusion rings is explained, and its results are given. Special attention is given to the structure of non-commutative fusion rings and the construction of songs, which are generalizations of the Tambara-Yamagami and Haagerup Izumi fusion rings, is given. Then, the algorithms used to find fusion systems are discussed. Particular attention is given to how the individual steps for solving the consistency equations are done with Anyonica, a software package we developed for working with fusion systems. Gauge and automorphism equivalence are reviewed, and algorithms that put solutions in a unitary gauge and remove redundant solutions are given. Some results on the categorification of all multiplicity-free pivotal fusion rings up to rank 7 are presented. The final part of the thesis is devoted to building models of anyons on graphs and how their behavior differs from those in the plane. The appendices contain a minimal mathematical exposition on fusion categories with their relation to fusion systems, a list of all multiplicity-free fusion rings up to rank 9 with information on categorifiability, a list of all multiplicity-free fusion categories up to rank 7, and data on some graph-braid models.","sentences":["This thesis explains the methods and algorithms we used to obtain explicit F symbols, R symbols, and pivotal coefficients of all multiplicity-free pivotal fusion categories up to rank 7.","The thesis starts by introducing the concept of a unitary modular fusion system via two applications: modeling anyons for topological quantum computation and calculating braid group representations.","Next, the notions of a pivotal, spherical, braided, ribbon, and modular fusion system are introduced.","Unitarity and its implications on the pivotal structure are discussed as well.","The next part of the thesis is devoted to algorithms for finding fusion systems and compatible structures.","First, an algorithm to find low-rank fusion rings is explained, and its results are given.","Special attention is given to the structure of non-commutative fusion rings and the construction of songs, which are generalizations of the Tambara-Yamagami and Haagerup Izumi fusion rings, is given.","Then, the algorithms used to find fusion systems are discussed.","Particular attention is given to how the individual steps for solving the consistency equations are done with Anyonica, a software package we developed for working with fusion systems.","Gauge and automorphism equivalence are reviewed, and algorithms that put solutions in a unitary gauge and remove redundant solutions are given.","Some results on the categorification of all multiplicity-free pivotal fusion rings up to rank 7 are presented.","The final part of the thesis is devoted to building models of anyons on graphs and how their behavior differs from those in the plane.","The appendices contain a minimal mathematical exposition on fusion categories with their relation to fusion systems, a list of all multiplicity-free fusion rings up to rank 9 with information on categorifiability, a list of all multiplicity-free fusion categories up to rank 7, and data on some graph-braid models."],"url":"http://arxiv.org/abs/2405.20075v1","category":"math-ph"}
{"created":"2024-05-30 13:58:17","title":"Pick-up and assembling of chemically sensitive van der Waals heterostructures using dry cryogenic exfoliation","abstract":"Assembling atomic layers of van der Waals materials (vdW) combines the physics of two materials, offering opportunities for novel functional devices. Realization of this has been possible because of advancements in nanofabrication processes which often involve chemical processing of the materials under study; this can be detrimental to device performance. To address this issue, we have developed a modified micro-manipulator setup for cryogenic exfoliation, pick up, and transfer of vdW materials to assemble heterostructures. We use the glass transition of a polymer PDMS to cleave a flake into two, followed by its pick-up and drop to form pristine twisted junctions. To demonstrate the potential of the technique, we fabricated twisted heterostructure of Bi$_2$Sr$_2$CaCu$_2$O$_{8+x}$ (BSCCO), a van der Waals high-temperature cuprate superconductor. We also employed this method to re-exfoliate NbSe$_2$ and make twisted heterostructure. Transport measurements of the fabricated devices indicate the high quality of the artificial twisted interface. In addition, we extend this cryogenic exfoliation method for other vdW materials, offering an effective way of assembling heterostructures and twisted junctions with pristine interfaces.","sentences":["Assembling atomic layers of van der Waals materials (vdW) combines the physics of two materials, offering opportunities for novel functional devices.","Realization of this has been possible because of advancements in nanofabrication processes which often involve chemical processing of the materials under study; this can be detrimental to device performance.","To address this issue, we have developed a modified micro-manipulator setup for cryogenic exfoliation, pick up, and transfer of vdW materials to assemble heterostructures.","We use the glass transition of a polymer PDMS to cleave a flake into two, followed by its pick-up and drop to form pristine twisted junctions.","To demonstrate the potential of the technique, we fabricated twisted heterostructure of Bi$_2$Sr$_2$CaCu$_2$O$_{8+x}$ (BSCCO), a van der Waals high-temperature cuprate superconductor.","We also employed this method to re-exfoliate NbSe$_2$ and make twisted heterostructure.","Transport measurements of the fabricated devices indicate the high quality of the artificial twisted interface.","In addition, we extend this cryogenic exfoliation method for other vdW materials, offering an effective way of assembling heterostructures and twisted junctions with pristine interfaces."],"url":"http://arxiv.org/abs/2405.20070v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 13:58:12","title":"Stacking-engineered ferroelectricty and multiferroic order in van der Waals magnets","abstract":"Two-dimensional (2D) materials that exhibit spontaneous magnetization, polarization or strain (referred to as ferroics) have the potential to revolutionize nanotechnology by enhancing the multifunctionality of nanoscale devices. However, multiferroic order is difficult to achieve, requiring complicated coupling between electron and spin degrees of freedom. We propose a universal method to engineer multiferroics from van der Waals magnets by taking advantage of the fact that changing the stacking between 2D layers can break inversion symmetry, resulting in ferroelectricity and possibly magnetoelectric coupling. We illustrate this concept using first-principles calculations in bilayer NiI$_2$, which can be made ferroelectric upon rotating two adjacent layers by $180^{\\circ}$ with respect to the bulk stacking. Furthermore, we discover a novel multiferroic order induced by interlayer charge transfer which couples the interlayer spin order and electronic polarization. Our approach is not only general but also systematic, and can enable the discovery of a wide variety of 2D multiferroics.","sentences":["Two-dimensional (2D) materials that exhibit spontaneous magnetization, polarization or strain (referred to as ferroics) have the potential to revolutionize nanotechnology by enhancing the multifunctionality of nanoscale devices.","However, multiferroic order is difficult to achieve, requiring complicated coupling between electron and spin degrees of freedom.","We propose a universal method to engineer multiferroics from van der Waals magnets by taking advantage of the fact that changing the stacking between 2D layers can break inversion symmetry, resulting in ferroelectricity and possibly magnetoelectric coupling.","We illustrate this concept using first-principles calculations in bilayer NiI$_2$, which can be made ferroelectric upon rotating two adjacent layers by $180^{\\circ}$ with respect to the bulk stacking.","Furthermore, we discover a novel multiferroic order induced by interlayer charge transfer which couples the interlayer spin order and electronic polarization.","Our approach is not only general but also systematic, and can enable the discovery of a wide variety of 2D multiferroics."],"url":"http://arxiv.org/abs/2405.20069v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 13:47:53","title":"Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation","abstract":"Separating vocal elements from musical tracks is a longstanding challenge in audio signal processing. This study tackles the distinct separation of vocal components from musical spectrograms. We employ the Short Time Fourier Transform (STFT) to extract audio waves into detailed frequency-time spectrograms, utilizing the benchmark MUSDB18 dataset for music separation. Subsequently, we implement a UNet neural network to segment the spectrogram image, aiming to delineate and extract singing voice components accurately. We achieved noteworthy results in audio source separation using of our U-Net-based models. The combination of frequency-axis normalization with Min/Max scaling and the Mean Absolute Error (MAE) loss function achieved the highest Source-to-Distortion Ratio (SDR) of 7.1 dB, indicating a high level of accuracy in preserving the quality of the original signal during separation. This setup also recorded impressive Source-to-Interference Ratio (SIR) and Source-to-Artifact Ratio (SAR) scores of 25.2 dB and 7.2 dB, respectively. These values significantly outperformed other configurations, particularly those using Quantile-based normalization or a Mean Squared Error (MSE) loss function. Our source code, model weights, and demo material can be found at the project's GitHub repository: https://github.com/mbrotos/SoundSeg","sentences":["Separating vocal elements from musical tracks is a longstanding challenge in audio signal processing.","This study tackles the distinct separation of vocal components from musical spectrograms.","We employ the Short Time Fourier Transform (STFT) to extract audio waves into detailed frequency-time spectrograms, utilizing the benchmark MUSDB18 dataset for music separation.","Subsequently, we implement a UNet neural network to segment the spectrogram image, aiming to delineate and extract singing voice components accurately.","We achieved noteworthy results in audio source separation using of our U-Net-based models.","The combination of frequency-axis normalization with Min/Max scaling and the Mean Absolute Error (MAE) loss function achieved the highest Source-to-Distortion Ratio (SDR) of 7.1 dB, indicating a high level of accuracy in preserving the quality of the original signal during separation.","This setup also recorded impressive Source-to-Interference Ratio (SIR) and Source-to-Artifact Ratio (SAR) scores of 25.2 dB and 7.2 dB, respectively.","These values significantly outperformed other configurations, particularly those using Quantile-based normalization or a Mean Squared Error (MSE) loss function.","Our source code, model weights, and demo material can be found at the project's GitHub repository:","https://github.com/mbrotos/SoundSeg"],"url":"http://arxiv.org/abs/2405.20059v1","category":"cs.SD"}
{"created":"2024-05-30 13:45:23","title":"A General Automata Model for First-Order Temporal Logics (Extended Version)","abstract":"First-order linear temporal logic (FOLTL) is a flexible and expressive formalism capable of naturally describing complex behaviors and properties. Although the logic is in general highly undecidable, the idea of using it as a specification language for the verification of complex infinite-state systems is appealing. However, a missing piece, which has proved to be an invaluable tool in dealing with other temporal logics, is an automaton model capable of capturing the logic. In this paper we address this issue, by defining and studying such a model, which we call first-order automaton. We define this very general class of automata, and the corresponding notion of regular first-order language, showing their closure under most common language-theoretic operations. We show how they can capture any FOLTL formula over any signature and theory, and provide sufficient conditions for the semi-decidability of their non-emptiness problem. Then, to show the usefulness of the formalism, we prove the decidability of monodic FOLTL, a classic result known in the literature, with a simpler and direct proof.","sentences":["First-order linear temporal logic (FOLTL) is a flexible and expressive formalism capable of naturally describing complex behaviors and properties.","Although the logic is in general highly undecidable, the idea of using it as a specification language for the verification of complex infinite-state systems is appealing.","However, a missing piece, which has proved to be an invaluable tool in dealing with other temporal logics, is an automaton model capable of capturing the logic.","In this paper we address this issue, by defining and studying such a model, which we call first-order automaton.","We define this very general class of automata, and the corresponding notion of regular first-order language, showing their closure under most common language-theoretic operations.","We show how they can capture any FOLTL formula over any signature and theory, and provide sufficient conditions for the semi-decidability of their non-emptiness problem.","Then, to show the usefulness of the formalism, we prove the decidability of monodic FOLTL, a classic result known in the literature, with a simpler and direct proof."],"url":"http://arxiv.org/abs/2405.20057v1","category":"cs.LO"}
{"created":"2024-05-30 13:42:09","title":"A brief conversation about subtraction games","abstract":"In this survey we revisit {\\sc finite subtraction}, one-heap subtraction games on finite rulesets. The main purpose is to give a general overview of the development, and specifically to draw attention to Flammenkamp's thesis (1997), where he, contrary to other studies, experimentally observes exponential eventual period length of the outcomes, for a carefully selected subclass of games. In addition, we contribute an appendix on {\\sc finite excluded subtraction} by Suetsugu.","sentences":["In this survey we revisit {\\sc finite subtraction}, one-heap subtraction games on finite rulesets.","The main purpose is to give a general overview of the development, and specifically to draw attention to Flammenkamp's thesis (1997), where he, contrary to other studies, experimentally observes exponential eventual period length of the outcomes, for a carefully selected subclass of games.","In addition, we contribute an appendix on {\\sc finite excluded subtraction} by Suetsugu."],"url":"http://arxiv.org/abs/2405.20054v1","category":"math.CO"}
{"created":"2024-05-30 13:38:28","title":"A Hardware-Efficient EMG Decoder with an Attractor-based Neural Network for Next-Generation Hand Prostheses","abstract":"Advancements in neural engineering have enabled the development of Robotic Prosthetic Hands (RPHs) aimed at restoring hand functionality. Current commercial RPHs offer limited control through basic on/off commands. Recent progresses in machine learning enable finger movement decoding with higher degrees of freedom, yet the high computational complexity of such models limits their application in portable devices. Future RPH designs must balance portability, low power consumption, and high decoding accuracy to be practical for individuals with disabilities. To this end, we introduce a novel attractor-based neural network to realize on-chip movement decoding for next-generation portable RPHs. The proposed architecture comprises an encoder, an attention layer, an attractor network, and a refinement regressor. We tested our model on four healthy subjects and achieved a decoding accuracy of 80.6\\pm3.3\\%. Our proposed model is over 120 and 50 times more compact compared to state-of-the-art LSTM and CNN models, respectively, with comparable (or superior) decoding accuracy. Therefore, it exhibits minimal hardware complexity and can be effectively integrated as a System-on-Chip.","sentences":["Advancements in neural engineering have enabled the development of Robotic Prosthetic Hands (RPHs) aimed at restoring hand functionality.","Current commercial RPHs offer limited control through basic on/off commands.","Recent progresses in machine learning enable finger movement decoding with higher degrees of freedom, yet the high computational complexity of such models limits their application in portable devices.","Future RPH designs must balance portability, low power consumption, and high decoding accuracy to be practical for individuals with disabilities.","To this end, we introduce a novel attractor-based neural network to realize on-chip movement decoding for next-generation portable RPHs.","The proposed architecture comprises an encoder, an attention layer, an attractor network, and a refinement regressor.","We tested our model on four healthy subjects and achieved a decoding accuracy of 80.6\\pm3.3\\%.","Our proposed model is over 120 and 50 times more compact compared to state-of-the-art LSTM and CNN models, respectively, with comparable (or superior) decoding accuracy.","Therefore, it exhibits minimal hardware complexity and can be effectively integrated as a System-on-Chip."],"url":"http://arxiv.org/abs/2405.20052v1","category":"eess.SP"}
{"created":"2024-05-30 13:37:53","title":"Threshold-Independent Fair Matching through Score Calibration","abstract":"Entity Matching (EM) is a critical task in numerous fields, such as healthcare, finance, and public administration, as it identifies records that refer to the same entity within or across different databases. EM faces considerable challenges, particularly with false positives and negatives. These are typically addressed by generating matching scores and apply thresholds to balance false positives and negatives in various contexts. However, adjusting these thresholds can affect the fairness of the outcomes, a critical factor that remains largely overlooked in current fair EM research. The existing body of research on fair EM tends to concentrate on static thresholds, neglecting their critical impact on fairness. To address this, we introduce a new approach in EM using recent metrics for evaluating biases in score based binary classification, particularly through the lens of distributional parity. This approach enables the application of various bias metrics like equalized odds, equal opportunity, and demographic parity without depending on threshold settings. Our experiments with leading matching methods reveal potential biases, and by applying a calibration technique for EM scores using Wasserstein barycenters, we not only mitigate these biases but also preserve accuracy across real world datasets. This paper contributes to the field of fairness in data cleaning, especially within EM, which is a central task in data cleaning, by promoting a method for generating matching scores that reduce biases across different thresholds.","sentences":["Entity Matching (EM) is a critical task in numerous fields, such as healthcare, finance, and public administration, as it identifies records that refer to the same entity within or across different databases.","EM faces considerable challenges, particularly with false positives and negatives.","These are typically addressed by generating matching scores and apply thresholds to balance false positives and negatives in various contexts.","However, adjusting these thresholds can affect the fairness of the outcomes, a critical factor that remains largely overlooked in current fair EM research.","The existing body of research on fair EM tends to concentrate on static thresholds, neglecting their critical impact on fairness.","To address this, we introduce a new approach in EM using recent metrics for evaluating biases in score based binary classification, particularly through the lens of distributional parity.","This approach enables the application of various bias metrics like equalized odds, equal opportunity, and demographic parity without depending on threshold settings.","Our experiments with leading matching methods reveal potential biases, and by applying a calibration technique for EM scores using Wasserstein barycenters, we not only mitigate these biases but also preserve accuracy across real world datasets.","This paper contributes to the field of fairness in data cleaning, especially within EM, which is a central task in data cleaning, by promoting a method for generating matching scores that reduce biases across different thresholds."],"url":"http://arxiv.org/abs/2405.20051v1","category":"cs.LG"}
{"created":"2024-05-30 13:29:32","title":"Schubert Subspace Codes","abstract":"In this paper, we initiate the study of constant dimension subspace codes restricted to Schubert varieties, which we call Schubert subspace codes. These codes have a very natural geometric description, as objects that we call intersecting sets with respect to a fixed subspace. We provide a geometric construction of maximum size constant dimension subspace codes in some Schubert varieties with the largest possible value for the minimum subspace distance. Finally, we generalize the problem to different values of the minimum distance.","sentences":["In this paper, we initiate the study of constant dimension subspace codes restricted to Schubert varieties, which we call Schubert subspace codes.","These codes have a very natural geometric description, as objects that we call intersecting sets with respect to a fixed subspace.","We provide a geometric construction of maximum size constant dimension subspace codes in some Schubert varieties with the largest possible value for the minimum subspace distance.","Finally, we generalize the problem to different values of the minimum distance."],"url":"http://arxiv.org/abs/2405.20047v1","category":"cs.IT"}
{"created":"2024-05-30 13:27:30","title":"Cross-Training with Multi-View Knowledge Fusion for Heterogenous Federated Learning","abstract":"Federated learning benefits from cross-training strategies, which enables models to train on data from distinct sources to improve the generalization capability. However, the data heterogeneity between sources may lead models to gradually forget previously acquired knowledge when undergoing cross-training to adapt to new tasks or data sources. We argue that integrating personalized and global knowledge to gather information from multiple perspectives could potentially improve performance. To achieve this goal, this paper presents a novel approach that enhances federated learning through a cross-training scheme incorporating multi-view information. Specifically, the proposed method, termed FedCT, includes three main modules, where the consistency-aware knowledge broadcasting module aims to optimize model assignment strategies, which enhances collaborative advantages between clients and achieves an efficient federated learning process. The multi-view knowledge-guided representation learning module leverages fused prototypical knowledge from both global and local views to enhance the preservation of local knowledge before and after model exchange, as well as to ensure consistency between local and global knowledge. The mixup-based feature augmentation module aggregates rich information to further increase the diversity of feature spaces, which enables the model to better discriminate complex samples. Extensive experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study. The results demonstrated that FedCT alleviates knowledge forgetting from both local and global views, which enables it outperform state-of-the-art methods.","sentences":["Federated learning benefits from cross-training strategies, which enables models to train on data from distinct sources to improve the generalization capability.","However, the data heterogeneity between sources may lead models to gradually forget previously acquired knowledge when undergoing cross-training to adapt to new tasks or data sources.","We argue that integrating personalized and global knowledge to gather information from multiple perspectives could potentially improve performance.","To achieve this goal, this paper presents a novel approach that enhances federated learning through a cross-training scheme incorporating multi-view information.","Specifically, the proposed method, termed FedCT, includes three main modules, where the consistency-aware knowledge broadcasting module aims to optimize model assignment strategies, which enhances collaborative advantages between clients and achieves an efficient federated learning process.","The multi-view knowledge-guided representation learning module leverages fused prototypical knowledge from both global and local views to enhance the preservation of local knowledge before and after model exchange, as well as to ensure consistency between local and global knowledge.","The mixup-based feature augmentation module aggregates rich information to further increase the diversity of feature spaces, which enables the model to better discriminate complex samples.","Extensive experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study.","The results demonstrated that FedCT alleviates knowledge forgetting from both local and global views, which enables it outperform state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.20046v1","category":"cs.AI"}
{"created":"2024-05-30 13:22:11","title":"A simultaneous unbinned differential cross section measurement of twenty-four $Z$+jets kinematic observables with the ATLAS detector","abstract":"$Z$ boson events at the Large Hadron Collider can be selected with high purity and are sensitive to a diverse range of QCD phenomena. As a result, these events are often used to probe the nature of the strong force, improve Monte Carlo event generators, and search for deviations from Standard Model predictions. All previous measurements of $Z$ boson production characterize the event properties using a small number of observables and present the results as differential cross sections in predetermined bins. In this analysis, a machine learning method called OmniFold is used to produce a simultaneous measurement of twenty-four $Z$+jets observables using $139$ fb$^{-1}$ of proton-proton collisions at $\\sqrt{s}=13$ TeV collected with the ATLAS detector. Unlike any previous fiducial differential cross-section measurement, this result is presented unbinned as a dataset of particle-level events, allowing for flexible re-use in a variety of contexts and for new observables to be constructed from the twenty-four measured observables.","sentences":["$Z$ boson events at the Large Hadron Collider can be selected with high purity and are sensitive to a diverse range of QCD phenomena.","As a result, these events are often used to probe the nature of the strong force, improve Monte Carlo event generators, and search for deviations from Standard Model predictions.","All previous measurements of $Z$ boson production characterize the event properties using a small number of observables and present the results as differential cross sections in predetermined bins.","In this analysis, a machine learning method called OmniFold is used to produce a simultaneous measurement of twenty-four $Z$+jets observables using $139$ fb$^{-1}$ of proton-proton collisions at $\\sqrt{s}=13$ TeV collected with the ATLAS detector.","Unlike any previous fiducial differential cross-section measurement, this result is presented unbinned as a dataset of particle-level events, allowing for flexible re-use in a variety of contexts and for new observables to be constructed from the twenty-four measured observables."],"url":"http://arxiv.org/abs/2405.20041v1","category":"hep-ex"}
{"created":"2024-05-30 13:19:23","title":"Deep Reinforcement Learning for Intrusion Detection in IoT: A Survey","abstract":"The rise of new complex attacks scenarios in Internet of things (IoT) environments necessitate more advanced and intelligent cyber defense techniques such as various Intrusion Detection Systems (IDSs) which are responsible for detecting and mitigating malicious activities in IoT networks without human intervention. To address this issue, deep reinforcement learning (DRL) has been proposed in recent years, to automatically tackle intrusions/attacks. In this paper, a comprehensive survey of DRL-based IDS on IoT is presented. Furthermore, in this survey, the state-of-the-art DRL-based IDS methods have been classified into five categories including wireless sensor network (WSN), deep Q-network (DQN), healthcare, hybrid, and other techniques. In addition, the most crucial performance metrics, namely accuracy, recall, precision, false negative rate (FNR), false positive rate (FPR), and F-measure, are detailed, in order to evaluate the performance of each proposed method. The paper provides a summary of datasets utilized in the studies as well.","sentences":["The rise of new complex attacks scenarios in Internet of things (IoT) environments necessitate more advanced and intelligent cyber defense techniques such as various Intrusion Detection Systems (IDSs) which are responsible for detecting and mitigating malicious activities in IoT networks without human intervention.","To address this issue, deep reinforcement learning (DRL) has been proposed in recent years, to automatically tackle intrusions/attacks.","In this paper, a comprehensive survey of DRL-based IDS on IoT is presented.","Furthermore, in this survey, the state-of-the-art DRL-based IDS methods have been classified into five categories including wireless sensor network (WSN), deep Q-network (DQN), healthcare, hybrid, and other techniques.","In addition, the most crucial performance metrics, namely accuracy, recall, precision, false negative rate (FNR), false positive rate (FPR), and F-measure, are detailed, in order to evaluate the performance of each proposed method.","The paper provides a summary of datasets utilized in the studies as well."],"url":"http://arxiv.org/abs/2405.20038v1","category":"cs.CR"}
{"created":"2024-05-30 13:19:16","title":"Linguistic Landscape of Generative AI Perception: A Global Twitter Analysis Across 14 Languages","abstract":"The advent of generative AI tools has had a profound impact on societies globally, transcending geographical boundaries. Understanding these tools' global reception and utilization is crucial for service providers and policymakers in shaping future policies. Therefore, to unravel the perceptions and engagements of individuals within diverse linguistic communities with regard to generative AI tools, we extensively analyzed over 6.8 million tweets in 14 different languages. Our findings reveal a global trend in the perception of generative AI, accompanied by language-specific nuances. While sentiments toward these tools vary significantly across languages, there is a prevalent positive inclination toward Image tools and a negative one toward Chat tools. Notably, the ban of ChatGPT in Italy led to a sentiment decline and initiated discussions across languages. Furthermore, we established a taxonomy for interactions with chatbots, creating a framework for social analysis underscoring variations in generative AI usage among linguistic communities. We find that the Chinese community predominantly employs chatbots as substitutes for search, while the Italian community tends to present more intricate prompts. Our research provides a robust foundation for further explorations of the social dynamics surrounding generative AI tools and offers invaluable insights for decision-makers in policy, technology, and education.","sentences":["The advent of generative AI tools has had a profound impact on societies globally, transcending geographical boundaries.","Understanding these tools' global reception and utilization is crucial for service providers and policymakers in shaping future policies.","Therefore, to unravel the perceptions and engagements of individuals within diverse linguistic communities with regard to generative AI tools, we extensively analyzed over 6.8 million tweets in 14 different languages.","Our findings reveal a global trend in the perception of generative AI, accompanied by language-specific nuances.","While sentiments toward these tools vary significantly across languages, there is a prevalent positive inclination toward Image tools and a negative one toward Chat tools.","Notably, the ban of ChatGPT in Italy led to a sentiment decline and initiated discussions across languages.","Furthermore, we established a taxonomy for interactions with chatbots, creating a framework for social analysis underscoring variations in generative AI usage among linguistic communities.","We find that the Chinese community predominantly employs chatbots as substitutes for search, while the Italian community tends to present more intricate prompts.","Our research provides a robust foundation for further explorations of the social dynamics surrounding generative AI tools and offers invaluable insights for decision-makers in policy, technology, and education."],"url":"http://arxiv.org/abs/2405.20037v1","category":"cs.CY"}
{"created":"2024-05-30 13:18:17","title":"Optimal Control of Bipartite Quantum Systems","abstract":"Closed bipartite quantum systems subject to fast local unitary control are studied using quantum optimal control theory and a method of reduced control systems based on the Schmidt decomposition. Particular focus is given to the time-optimal generation of maximally entangled states and product states, as well as to the problem of stabilizing quantum states with a certain amount of entanglement. Explicit analytical solutions are given for general systems consisting of two qubits (as well as for bosonic and fermionic analogues) and also for a class of systems consisting of two coupled qutrits which is studied using the Pontryagin Maximum Principle.","sentences":["Closed bipartite quantum systems subject to fast local unitary control are studied using quantum optimal control theory and a method of reduced control systems based on the Schmidt decomposition.","Particular focus is given to the time-optimal generation of maximally entangled states and product states, as well as to the problem of stabilizing quantum states with a certain amount of entanglement.","Explicit analytical solutions are given for general systems consisting of two qubits (as well as for bosonic and fermionic analogues) and also for a class of systems consisting of two coupled qutrits which is studied using the Pontryagin Maximum Principle."],"url":"http://arxiv.org/abs/2405.20034v1","category":"quant-ph"}
{"created":"2024-05-30 13:16:57","title":"Chemical Space-Informed Machine Learning Models for Rapid Predictions of X-ray Photoelectron Spectra of Organic Molecules","abstract":"We present machine learning models based on kernel-ridge regression for predicting X-ray photoelectron spectra of organic molecules originating from the $K$-shell ionization energies of carbon (C), nitrogen (N), oxygen (O), and fluorine (F) atoms. We constructed the training dataset through high-throughput calculations of $K$-shell core-electron binding energies (CEBEs) for 12,880 small organic molecules in the bigQM7$\\omega$ dataset, employing the $\\Delta$-SCF formalism coupled with meta-GGA-DFT and a variationally converged basis set. The models are cost-effective, as they require the atomic coordinates of a molecule generated using universal force fields while estimating the target-level CEBEs corresponding to DFT-level equilibrium geometry. We explore transfer learning by utilizing the atomic environment feature vectors learned using a graph neural network framework in kernel-ridge regression. Additionally, we enhance accuracy within the $\\Delta$-machine learning framework by leveraging inexpensive baseline spectra derived from Kohn--Sham eigenvalues. When applied to 208 combinatorially substituted uracil molecules larger than those in the training set, our analyses suggest that the models may not provide quantitatively accurate predictions of CEBEs but offer a strong linear correlation relevant for virtual high-throughput screening. We present the dataset and models as the Python module, ${\\tt cebeconf}$, to facilitate further explorations.","sentences":["We present machine learning models based on kernel-ridge regression for predicting X-ray photoelectron spectra of organic molecules originating from the $K$-shell ionization energies of carbon (C), nitrogen (N), oxygen (O), and fluorine (F) atoms.","We constructed the training dataset through high-throughput calculations of $K$-shell core-electron binding energies (CEBEs) for 12,880 small organic molecules in the bigQM7$\\omega$ dataset, employing the $\\Delta$-SCF formalism coupled with meta-GGA-DFT and a variationally converged basis set.","The models are cost-effective, as they require the atomic coordinates of a molecule generated using universal force fields while estimating the target-level CEBEs corresponding to DFT-level equilibrium geometry.","We explore transfer learning by utilizing the atomic environment feature vectors learned using a graph neural network framework in kernel-ridge regression.","Additionally, we enhance accuracy within the $\\Delta$-machine learning framework by leveraging inexpensive baseline spectra derived from Kohn--Sham eigenvalues.","When applied to 208 combinatorially substituted uracil molecules larger than those in the training set, our analyses suggest that the models may not provide quantitatively accurate predictions of CEBEs but offer a strong linear correlation relevant for virtual high-throughput screening.","We present the dataset and models as the Python module, ${\\tt cebeconf}$, to facilitate further explorations."],"url":"http://arxiv.org/abs/2405.20033v1","category":"physics.chem-ph"}
{"created":"2024-05-30 13:16:48","title":"Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion","abstract":"With the exponential growth of video traffic, traditional video streaming systems are approaching their limits in compression efficiency and communication capacity. To further reduce bitrate while maintaining quality, we propose Promptus, a disruptive novel system that streaming prompts instead of video content with Stable Diffusion, which converts video frames into a series of \"prompts\" for delivery. To ensure pixel alignment, a gradient descent-based prompt fitting framework is proposed. To achieve adaptive bitrate for prompts, a low-rank decomposition-based bitrate control algorithm is introduced. For inter-frame compression of prompts, a temporal smoothing-based prompt interpolation algorithm is proposed. Evaluations across various video domains and real network traces demonstrate Promptus can enhance the perceptual quality by 0.111 and 0.092 (in LPIPS) compared to VAE and H.265, respectively, and decreases the ratio of severely distorted frames by 89.3% and 91.7%. Moreover, Promptus achieves real-time video generation from prompts at over 150 FPS. To the best of our knowledge, Promptus is the first attempt to replace video codecs with prompt inversion and the first to use prompt streaming instead of video streaming. Our work opens up a new paradigm for efficient video communication beyond the Shannon limit.","sentences":["With the exponential growth of video traffic, traditional video streaming systems are approaching their limits in compression efficiency and communication capacity.","To further reduce bitrate while maintaining quality, we propose Promptus, a disruptive novel system that streaming prompts instead of video content with Stable Diffusion, which converts video frames into a series of \"prompts\" for delivery.","To ensure pixel alignment, a gradient descent-based prompt fitting framework is proposed.","To achieve adaptive bitrate for prompts, a low-rank decomposition-based bitrate control algorithm is introduced.","For inter-frame compression of prompts, a temporal smoothing-based prompt interpolation algorithm is proposed.","Evaluations across various video domains and real network traces demonstrate Promptus can enhance the perceptual quality by 0.111 and 0.092 (in LPIPS) compared to VAE and H.265, respectively, and decreases the ratio of severely distorted frames by 89.3% and 91.7%.","Moreover, Promptus achieves real-time video generation from prompts at over 150 FPS.","To the best of our knowledge, Promptus is the first attempt to replace video codecs with prompt inversion and the first to use prompt streaming instead of video streaming.","Our work opens up a new paradigm for efficient video communication beyond the Shannon limit."],"url":"http://arxiv.org/abs/2405.20032v1","category":"cs.NI"}
{"created":"2024-05-30 13:15:18","title":"EMAG: Ego-motion Aware and Generalizable 2D Hand Forecasting from Egocentric Videos","abstract":"Predicting future human behavior from egocentric videos is a challenging but critical task for human intention understanding. Existing methods for forecasting 2D hand positions rely on visual representations and mainly focus on hand-object interactions. In this paper, we investigate the hand forecasting task and tackle two significant issues that persist in the existing methods: (1) 2D hand positions in future frames are severely affected by ego-motions in egocentric videos; (2) prediction based on visual information tends to overfit to background or scene textures, posing a challenge for generalization on novel scenes or human behaviors. To solve the aforementioned problems, we propose EMAG, an ego-motion-aware and generalizable 2D hand forecasting method. In response to the first problem, we propose a method that considers ego-motion, represented by a sequence of homography matrices of two consecutive frames. We further leverage modalities such as optical flow, trajectories of hands and interacting objects, and ego-motions, thereby alleviating the second issue. Extensive experiments on two large-scale egocentric video datasets, Ego4D and EPIC-Kitchens 55, verify the effectiveness of the proposed method. In particular, our model outperforms prior methods by $7.0$\\% on cross-dataset evaluations. Project page: https://masashi-hatano.github.io/EMAG/","sentences":["Predicting future human behavior from egocentric videos is a challenging but critical task for human intention understanding.","Existing methods for forecasting 2D hand positions rely on visual representations and mainly focus on hand-object interactions.","In this paper, we investigate the hand forecasting task and tackle two significant issues that persist in the existing methods: (1) 2D hand positions in future frames are severely affected by ego-motions in egocentric videos; (2) prediction based on visual information tends to overfit to background or scene textures, posing a challenge for generalization on novel scenes or human behaviors.","To solve the aforementioned problems, we propose EMAG, an ego-motion-aware and generalizable 2D hand forecasting method.","In response to the first problem, we propose a method that considers ego-motion, represented by a sequence of homography matrices of two consecutive frames.","We further leverage modalities such as optical flow, trajectories of hands and interacting objects, and ego-motions, thereby alleviating the second issue.","Extensive experiments on two large-scale egocentric video datasets, Ego4D and EPIC-Kitchens 55, verify the effectiveness of the proposed method.","In particular, our model outperforms prior methods by $7.0$\\% on cross-dataset evaluations.","Project page: https://masashi-hatano.github.io/EMAG/"],"url":"http://arxiv.org/abs/2405.20030v1","category":"cs.CV"}
{"created":"2024-05-30 13:11:08","title":"From Forest to Zoo: Great Ape Behavior Recognition with ChimpBehave","abstract":"This paper addresses the significant challenge of recognizing behaviors in non-human primates, specifically focusing on chimpanzees. Automated behavior recognition is crucial for both conservation efforts and the advancement of behavioral research. However, it is significantly hindered by the labor-intensive process of manual video annotation. Despite the availability of large-scale animal behavior datasets, the effective application of machine learning models across varied environmental settings poses a critical challenge, primarily due to the variability in data collection contexts and the specificity of annotations.   In this paper, we introduce ChimpBehave, a novel dataset featuring over 2 hours of video (approximately 193,000 video frames) of zoo-housed chimpanzees, meticulously annotated with bounding boxes and behavior labels for action recognition. ChimpBehave uniquely aligns its behavior classes with existing datasets, allowing for the study of domain adaptation and cross-dataset generalization methods between different visual settings. Furthermore, we benchmark our dataset using a state-of-the-art CNN-based action recognition model, providing the first baseline results for both within and cross-dataset settings. The dataset, models, and code can be accessed at: https://github.com/MitchFuchs/ChimpBehave","sentences":["This paper addresses the significant challenge of recognizing behaviors in non-human primates, specifically focusing on chimpanzees.","Automated behavior recognition is crucial for both conservation efforts and the advancement of behavioral research.","However, it is significantly hindered by the labor-intensive process of manual video annotation.","Despite the availability of large-scale animal behavior datasets, the effective application of machine learning models across varied environmental settings poses a critical challenge, primarily due to the variability in data collection contexts and the specificity of annotations.   ","In this paper, we introduce ChimpBehave, a novel dataset featuring over 2 hours of video (approximately 193,000 video frames) of zoo-housed chimpanzees, meticulously annotated with bounding boxes and behavior labels for action recognition.","ChimpBehave uniquely aligns its behavior classes with existing datasets, allowing for the study of domain adaptation and cross-dataset generalization methods between different visual settings.","Furthermore, we benchmark our dataset using a state-of-the-art CNN-based action recognition model, providing the first baseline results for both within and cross-dataset settings.","The dataset, models, and code can be accessed at: https://github.com/MitchFuchs/ChimpBehave"],"url":"http://arxiv.org/abs/2405.20025v1","category":"cs.CV"}
{"created":"2024-05-30 13:06:40","title":"Applications of Generative AI (GAI) for Mobile and Wireless Networking: A Survey","abstract":"The success of Artificial Intelligence (AI) in multiple disciplines and vertical domains in recent years has promoted the evolution of mobile networking and the future Internet toward an AI-integrated Internet-of-Things (IoT) era. Nevertheless, most AI techniques rely on data generated by physical devices (e.g., mobile devices and network nodes) or specific applications (e.g., fitness trackers and mobile gaming). To bypass this circumvent, Generative AI (GAI), a.k.a. AI-generated content (AIGC), has emerged as a powerful AI paradigm; thanks to its ability to efficiently learn complex data distributions and generate synthetic data to represent the original data in various forms. This impressive feature is projected to transform the management of mobile networking and diversify the current services and applications provided. On this basis, this work presents a concise tutorial on the role of GAIs in mobile and wireless networking. In particular, this survey first provides the fundamentals of GAI and representative GAI models, serving as an essential preliminary to the understanding of the applications of GAI in mobile and wireless networking. Then, this work provides a comprehensive review of state-of-the-art studies and GAI applications in network management, wireless security, semantic communication, and lessons learned from the open literature. Finally, this work summarizes the current research on GAI for mobile and wireless networking by outlining important challenges that need to be resolved to facilitate the development and applicability of GAI in this edge-cutting area.","sentences":["The success of Artificial Intelligence (AI) in multiple disciplines and vertical domains in recent years has promoted the evolution of mobile networking and the future Internet toward an AI-integrated Internet-of-Things (IoT) era.","Nevertheless, most AI techniques rely on data generated by physical devices (e.g., mobile devices and network nodes) or specific applications (e.g., fitness trackers and mobile gaming).","To bypass this circumvent, Generative AI (GAI), a.k.a. AI-generated content (AIGC), has emerged as a powerful AI paradigm; thanks to its ability to efficiently learn complex data distributions and generate synthetic data to represent the original data in various forms.","This impressive feature is projected to transform the management of mobile networking and diversify the current services and applications provided.","On this basis, this work presents a concise tutorial on the role of GAIs in mobile and wireless networking.","In particular, this survey first provides the fundamentals of GAI and representative GAI models, serving as an essential preliminary to the understanding of the applications of GAI in mobile and wireless networking.","Then, this work provides a comprehensive review of state-of-the-art studies and GAI applications in network management, wireless security, semantic communication, and lessons learned from the open literature.","Finally, this work summarizes the current research on GAI for mobile and wireless networking by outlining important challenges that need to be resolved to facilitate the development and applicability of GAI in this edge-cutting area."],"url":"http://arxiv.org/abs/2405.20024v1","category":"cs.NI"}
{"created":"2024-05-30 13:05:32","title":"Equality between two general ridge estimators and equivalence of their residual sums of squares","abstract":"General ridge estimators are typical linear estimators in a general linear model. The class of them include some shrinkage estimators in addition to classical linear unbiased estimators such as the ordinary least squares estimator and the weighted least squares estimator. We derive necessary and sufficient conditions under which two typical general ridge estimators coincide. In particular, two noteworthy conditions are added to those from previous studies. The first condition is given as a seemingly column space relationship to the covariance matrix of the error term, and the second one is based on the biases of general ridge estimators. Another problem studied in this paper is to derive an equivalence condition such that equality between two residual sums of squares holds when general ridge estimators are considered.","sentences":["General ridge estimators are typical linear estimators in a general linear model.","The class of them include some shrinkage estimators in addition to classical linear unbiased estimators such as the ordinary least squares estimator and the weighted least squares estimator.","We derive necessary and sufficient conditions under which two typical general ridge estimators coincide.","In particular, two noteworthy conditions are added to those from previous studies.","The first condition is given as a seemingly column space relationship to the covariance matrix of the error term, and the second one is based on the biases of general ridge estimators.","Another problem studied in this paper is to derive an equivalence condition such that equality between two residual sums of squares holds when general ridge estimators are considered."],"url":"http://arxiv.org/abs/2405.20023v1","category":"math.ST"}
{"created":"2024-05-30 13:01:45","title":"Chaotic advection in a steady three-dimensional MHD flow","abstract":"We investigate a real 3D stationary flow characterized by chaotic advection generated by a magnetic field created by permanent magnets acting on a weakly conductive fluid subjected to a weak constant current. The model under consideration involves the Stokes equations for viscous incompressible fluid at low Reynolds number in which the density forces correspond to the Lorentz force generated by the magnetic field of the magnets and the electric current through the fluid. An innovative numerical approach based on a mixed finite element method has been developed and implemented for computing the flow velocity fields with the electromagnetic force. This ensures highly accurate numerical results, allowing a detailed analysis of the chaotic behavior of fluid trajectories through the computations of associated Poincar\\'e sections and Lyapunov exponents. Subsequently, an examination of mixing efficiency is conducted, employing computations of contamination and homogeneity rates, as well as mixing time. The obtained results underscore the relevance of the modeling and computational tools employed, as well as the design of the magnetohydrodynamic device used.","sentences":["We investigate a real 3D stationary flow characterized by chaotic advection generated by a magnetic field created by permanent magnets acting on a weakly conductive fluid subjected to a weak constant current.","The model under consideration involves the Stokes equations for viscous incompressible fluid at low Reynolds number in which the density forces correspond to the Lorentz force generated by the magnetic field of the magnets and the electric current through the fluid.","An innovative numerical approach based on a mixed finite element method has been developed and implemented for computing the flow velocity fields with the electromagnetic force.","This ensures highly accurate numerical results, allowing a detailed analysis of the chaotic behavior of fluid trajectories through the computations of associated Poincar\\'e sections and Lyapunov exponents.","Subsequently, an examination of mixing efficiency is conducted, employing computations of contamination and homogeneity rates, as well as mixing time.","The obtained results underscore the relevance of the modeling and computational tools employed, as well as the design of the magnetohydrodynamic device used."],"url":"http://arxiv.org/abs/2405.20021v1","category":"physics.flu-dyn"}
{"created":"2024-05-30 12:51:22","title":"Direct, indirect, and self-trapped excitons in Cs$_2$AgBiBr$_6$","abstract":"Cs$_2$AgBiBr$_6$ is a representative halide double perovskite which exhibits promising photovoltaic and light-emitting properties, making it a candidate for next-generation solar cells and LED technologies. Here, we study various possible excited states of this material to understand its absorption and emission properties. We use Time-Dependent Density Functional Theory (TD-DFT) coupled with non-empirical hybrid functionals, specifically PBE0($\\alpha$) and dielectric-dependent hybrids (DDH) to explore direct, indirect, and self-trapped excitons in this material. Based on comparison with experiment, we show that these methods can give excellent prediction of the absorption spectrum and that the fundamental band gap has been underestimated in previous computational studies. We connect the experimental photoluminescence signals at 1.9-2.0 eV to the emission from self-trapped excitons and electron polarons. Finally, we reveal a complex landscape with energetically competing direct, indirect, and self-trapped excitons in the material.","sentences":["Cs$_2$AgBiBr$_6$ is a representative halide double perovskite which exhibits promising photovoltaic and light-emitting properties, making it a candidate for next-generation solar cells and LED technologies.","Here, we study various possible excited states of this material to understand its absorption and emission properties.","We use Time-Dependent Density Functional Theory (TD-DFT) coupled with non-empirical hybrid functionals, specifically PBE0($\\alpha$) and dielectric-dependent hybrids (DDH) to explore direct, indirect, and self-trapped excitons in this material.","Based on comparison with experiment, we show that these methods can give excellent prediction of the absorption spectrum and that the fundamental band gap has been underestimated in previous computational studies.","We connect the experimental photoluminescence signals at 1.9-2.0 eV to the emission from self-trapped excitons and electron polarons.","Finally, we reveal a complex landscape with energetically competing direct, indirect, and self-trapped excitons in the material."],"url":"http://arxiv.org/abs/2405.20017v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 12:50:32","title":"Efficient LLM-Jailbreaking by Introducing Visual Modality","abstract":"This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) through the incorporation of a visual module into the target LLM. Subsequently, we conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings embJS. Finally, we convert the embJS into text space to facilitate the jailbreaking of the target LLM. Compared to direct LLM-jailbreaking, our approach is more efficient, as MLLMs are more vulnerable to jailbreaking than pure LLM. Additionally, to improve the attack success rate (ASR) of jailbreaking, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class jailbreaking capabilities.","sentences":["This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries.","Unlike previous LLM-jailbreaks that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) through the incorporation of a visual module into the target LLM.","Subsequently, we conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings embJS.","Finally, we convert the embJS into text space to facilitate the jailbreaking of the target LLM.","Compared to direct LLM-jailbreaking, our approach is more efficient, as MLLMs are more vulnerable to jailbreaking than pure LLM.","Additionally, to improve the attack success rate (ASR) of jailbreaking, we propose an image-text semantic matching scheme to identify a suitable initial input.","Extensive experiments demonstrate that our approach surpasses current state-of-the-art methods in terms of both efficiency and effectiveness.","Moreover, our approach exhibits superior cross-class jailbreaking capabilities."],"url":"http://arxiv.org/abs/2405.20015v1","category":"cs.AI"}
{"created":"2024-05-30 12:49:34","title":"subMFL: Compatiple subModel Generation for Federated Learning in Device Heterogenous Environment","abstract":"Federated Learning (FL) is commonly used in systems with distributed and heterogeneous devices with access to varying amounts of data and diverse computing and storage capacities. FL training process enables such devices to update the weights of a shared model locally using their local data and then a trusted central server combines all of those models to generate a global model. In this way, a global model is generated while the data remains local to devices to preserve privacy. However, training large models such as Deep Neural Networks (DNNs) on resource-constrained devices can take a prohibitively long time and consume a large amount of energy. In the current process, the low-capacity devices are excluded from the training process, although they might have access to unseen data. To overcome this challenge, we propose a model compression approach that enables heterogeneous devices with varying computing capacities to participate in the FL process. In our approach, the server shares a dense model with all devices to train it: Afterwards, the trained model is gradually compressed to obtain submodels with varying levels of sparsity to be used as suitable initial global models for resource-constrained devices that were not capable of train the first dense model. This results in an increased participation rate of resource-constrained devices while the transferred weights from the previous round of training are preserved. Our validation experiments show that despite reaching about 50 per cent global sparsity, generated submodels maintain their accuracy while can be shared to increase participation by around 50 per cent.","sentences":["Federated Learning (FL) is commonly used in systems with distributed and heterogeneous devices with access to varying amounts of data and diverse computing and storage capacities.","FL training process enables such devices to update the weights of a shared model locally using their local data and then a trusted central server combines all of those models to generate a global model.","In this way, a global model is generated while the data remains local to devices to preserve privacy.","However, training large models such as Deep Neural Networks (DNNs) on resource-constrained devices can take a prohibitively long time and consume a large amount of energy.","In the current process, the low-capacity devices are excluded from the training process, although they might have access to unseen data.","To overcome this challenge, we propose a model compression approach that enables heterogeneous devices with varying computing capacities to participate in the FL process.","In our approach, the server shares a dense model with all devices to train it: Afterwards, the trained model is gradually compressed to obtain submodels with varying levels of sparsity to be used as suitable initial global models for resource-constrained devices that were not capable of train the first dense model.","This results in an increased participation rate of resource-constrained devices while the transferred weights from the previous round of training are preserved.","Our validation experiments show that despite reaching about 50 per cent global sparsity, generated submodels maintain their accuracy while can be shared to increase participation by around 50 per cent."],"url":"http://arxiv.org/abs/2405.20014v1","category":"cs.LG"}
{"created":"2024-05-30 12:48:44","title":"FlexiDrop: Theoretical Insights and Practical Advances in Random Dropout Method on GNNs","abstract":"Graph Neural Networks (GNNs) are powerful tools for handling graph-type data. Recently, GNNs have been widely applied in various domains, but they also face some issues, such as overfitting, over-smoothing and non-robustness. The existing research indicates that random dropout methods are an effective way to address these issues. However, random dropout methods in GNNs still face unresolved problems. Currently, the choice of dropout rate, often determined by heuristic or grid search methods, can increase the generalization error, contradicting the principal aims of dropout. In this paper, we propose a novel random dropout method for GNNs called FlexiDrop. First, we conduct a theoretical analysis of dropout in GNNs using rademacher complexity and demonstrate that the generalization error of traditional random dropout methods is constrained by a function related to the dropout rate. Subsequently, we use this function as a regularizer to unify the dropout rate and empirical loss within a single loss function, optimizing them simultaneously. Therefore, our method enables adaptive adjustment of the dropout rate and theoretically balances the trade-off between model complexity and generalization ability. Furthermore, extensive experimental results on benchmark datasets show that FlexiDrop outperforms traditional random dropout methods in GNNs.","sentences":["Graph Neural Networks (GNNs) are powerful tools for handling graph-type data.","Recently, GNNs have been widely applied in various domains, but they also face some issues, such as overfitting, over-smoothing and non-robustness.","The existing research indicates that random dropout methods are an effective way to address these issues.","However, random dropout methods in GNNs still face unresolved problems.","Currently, the choice of dropout rate, often determined by heuristic or grid search methods, can increase the generalization error, contradicting the principal aims of dropout.","In this paper, we propose a novel random dropout method for GNNs called FlexiDrop.","First, we conduct a theoretical analysis of dropout in GNNs using rademacher complexity and demonstrate that the generalization error of traditional random dropout methods is constrained by a function related to the dropout rate.","Subsequently, we use this function as a regularizer to unify the dropout rate and empirical loss within a single loss function, optimizing them simultaneously.","Therefore, our method enables adaptive adjustment of the dropout rate and theoretically balances the trade-off between model complexity and generalization ability.","Furthermore, extensive experimental results on benchmark datasets show that FlexiDrop outperforms traditional random dropout methods in GNNs."],"url":"http://arxiv.org/abs/2405.20012v1","category":"cs.LG"}
{"created":"2024-05-30 12:43:04","title":"Galois subcovers of the Hermitian curve in characteristic $p$ with respect to subgroups of order $dp$ with $d\\not=p$ prime","abstract":"A problem of current interest, also motivated by applications to Coding theory, is to find explicit equations for \\textit{maximal} curves, that are projective, geometrically irreducible, non-singular curves defined over a finite field $\\mathbb{F}_{q^2}$ whose number of $\\mathbb{F}_{q^2}$-rational points attains the Hasse-Weil upper bound of $q^2+2\\mathfrak{g}q+1$ where $\\mathfrak{g}$ is the genus of the curve $\\mathcal{X}$. For curves which are Galois covered of the Hermitian curve, this has been done so far ad hoc, in particular in the cases where the Galois group has prime order and also when has order the square of the characteristic. In this paper we obtain explicit equations of all Galois covers of the Hermitian curve with Galois group of order $dp$ where $p$ is the characteristic of $\\mathbb{F}_{q^2}$ and $d$ is prime other than $p$. We also compute the generators of the Weierstrass semigroup at a special $\\mathbb{F}_{q^2}$-rational point of some of the curves, and discuss some possible positive impacts on the minimum distance problems of AG-codes.","sentences":["A problem of current interest, also motivated by applications to Coding theory, is to find explicit equations for \\textit{maximal} curves, that are projective, geometrically irreducible, non-singular curves defined over a finite field $\\mathbb{F}_{q^2}$ whose number of $\\mathbb{F}_{q^2}$-rational points attains the Hasse-Weil upper bound of $q^2+2\\mathfrak{g}q+1$ where $\\mathfrak{g}$ is the genus of the curve $\\mathcal{X}$. For curves which are Galois covered of the Hermitian curve, this has been done so far ad hoc, in particular in the cases where the Galois group has prime order and also when has order the square of the characteristic.","In this paper we obtain explicit equations of all Galois covers of the Hermitian curve with Galois group of order $dp$ where $p$ is the characteristic of $\\mathbb{F}_{q^2}$ and $d$ is prime other than $p$. We also compute the generators of the Weierstrass semigroup at a special $\\mathbb{F}_{q^2}$-rational point of some of the curves, and discuss some possible positive impacts on the minimum distance problems of AG-codes."],"url":"http://arxiv.org/abs/2405.20005v1","category":"math.AG"}
{"created":"2024-05-30 12:42:24","title":"A low cosmic-ray ionisation rate in the prestellar core Ophiuchus/H-MM1. Mapping of the molecular ions ortho-H2D+, N2H+, and DCO+","abstract":"(abridged) We have mapped the prestellar core H-MM1 in Ophiuchus in rotational lines of ortho-H2D+ (oH2D+), N2H+, and DCO+ at the wavelength 0.8 mm with the Large APEX sub-Millimeter Array (LAsMA) multibeam receiver of the Atacama Pathfinder EXperiment (APEX) telescope. We also ran a series of chemistry models to predict the abundance distributions of the observed molecules, and to estimate the effect of the cosmic-ray ionisation rate on their abundances. The three line maps show different distributions. The oH2D+ map is extended and outlines the general structure of the core, while N2H+ mainly shows the density maxima, and the DCO+ emission peaks are shifted towards one edge of the core where a region of enhanced desorption has been found previously. According to the chemical simulation, the fractional oH2D+ abundance remains relatively high in the centre of the core, and its column density correlates strongly with the cosmic-ray ionisation rate. Simulated line maps constrain the cosmic-ray ionisation rate per hydrogen molecule to be low, between 5e-18/s and 1e-17/s in the H-MM1 core. This estimate agrees with the gas temperature measured in the core. Modelling line emission of oH2D+ provides a straightforward method of determining the cosmic-ray ionisation rate in dense clouds, where the primary ion, H3+, is not observable.","sentences":["(abridged)","We have mapped the prestellar core H-MM1 in Ophiuchus in rotational lines of ortho-H2D+ (oH2D+), N2H+, and DCO+ at the wavelength 0.8 mm with the Large APEX sub-Millimeter Array (LAsMA) multibeam receiver of the Atacama Pathfinder EXperiment (APEX) telescope.","We also ran a series of chemistry models to predict the abundance distributions of the observed molecules, and to estimate the effect of the cosmic-ray ionisation rate on their abundances.","The three line maps show different distributions.","The oH2D+ map is extended and outlines the general structure of the core, while N2H+ mainly shows the density maxima, and the DCO+ emission peaks are shifted towards one edge of the core where a region of enhanced desorption has been found previously.","According to the chemical simulation, the fractional oH2D+ abundance remains relatively high in the centre of the core, and its column density correlates strongly with the cosmic-ray ionisation rate.","Simulated line maps constrain the cosmic-ray ionisation rate per hydrogen molecule to be low, between 5e-18/s and 1e-17/s in the H-MM1 core.","This estimate agrees with the gas temperature measured in the core.","Modelling line emission of oH2D+ provides a straightforward method of determining the cosmic-ray ionisation rate in dense clouds, where the primary ion, H3+, is not observable."],"url":"http://arxiv.org/abs/2405.20004v1","category":"astro-ph.GA"}
{"created":"2024-05-30 12:42:05","title":"Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities","abstract":"Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations. Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness. To address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.","sentences":["Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important.","In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations.","Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness.","To address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs.","KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy.","It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers.","We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures."],"url":"http://arxiv.org/abs/2405.20003v1","category":"cs.LG"}
{"created":"2024-05-30 12:41:23","title":"Regular bipartite multigraphs have many (but not too many) symmetries","abstract":"Let $k$ and $l$ be integers, both at least 2. A $(k,l)$-bipartite graph is an $l$-regular bipartite multigraph with coloured bipartite sets of size $k$. Define $\\chi(k,l)$ and $\\mu(k,l)$ to be the minimum and maximum order of automorphism groups of $(k,l)$-bipartite graphs, respectively. We determine $\\chi(k,l)$ and $\\mu(k,l)$ for $k\\geq 8$, and analyse the generic situation when $k$ is fixed and $l$ is large. In particular, we show that almost all such graphs have automorphism groups which fix the vertices pointwise and have order far less than $\\mu(k,l)$. These graphs are intimately connected with both integer doubly-stochastic matrices and uniform set partitions; we examine the uniform distribution on the set of $k\\times k$ integer doubly-stochastic matrices with all line sums $l$, showing that with high probability all entries stray far from the mean. We also show that the symmetric group acting on uniform set partitions is non-synchronizing.","sentences":["Let $k$ and $l$ be integers, both at least 2.","A $(k,l)$-bipartite graph is an $l$-regular bipartite multigraph with coloured bipartite sets of size $k$. Define $\\chi(k,l)$ and $\\mu(k,l)$ to be the minimum and maximum order of automorphism groups of $(k,l)$-bipartite graphs, respectively.","We determine $\\chi(k,l)$ and $\\mu(k,l)$ for $k\\geq 8$, and analyse the generic situation when $k$ is fixed and $l$ is large.","In particular, we show that almost all such graphs have automorphism groups which fix the vertices pointwise and have order far less than $\\mu(k,l)$.","These graphs are intimately connected with both integer doubly-stochastic matrices and uniform set partitions; we examine the uniform distribution on the set of $k\\times k$ integer doubly-stochastic matrices with all line sums $l$, showing that with high probability all entries stray far from the mean.","We also show that the symmetric group acting on uniform set partitions is non-synchronizing."],"url":"http://arxiv.org/abs/2405.20002v1","category":"math.CO"}
{"created":"2024-05-30 12:34:58","title":"LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning","abstract":"In cooperative multi-agent reinforcement learning (MARL), agents collaborate to achieve common goals, such as defeating enemies and scoring a goal. However, learning goal-reaching paths toward such a semantic goal takes a considerable amount of time in complex tasks and the trained model often fails to find such paths. To address this, we present LAtent Goal-guided Multi-Agent reinforcement learning (LAGMA), which generates a goal-reaching trajectory in latent space and provides a latent goal-guided incentive to transitions toward this reference trajectory. LAGMA consists of three major components: (a) quantized latent space constructed via a modified VQ-VAE for efficient sample utilization, (b) goal-reaching trajectory generation via extended VQ codebook, and (c) latent goal-guided intrinsic reward generation to encourage transitions towards the sampled goal-reaching path. The proposed method is evaluated by StarCraft II with both dense and sparse reward settings and Google Research Football. Empirical results show further performance improvement over state-of-the-art baselines.","sentences":["In cooperative multi-agent reinforcement learning (MARL), agents collaborate to achieve common goals, such as defeating enemies and scoring a goal.","However, learning goal-reaching paths toward such a semantic goal takes a considerable amount of time in complex tasks and the trained model often fails to find such paths.","To address this, we present LAtent Goal-guided Multi-Agent reinforcement learning (LAGMA), which generates a goal-reaching trajectory in latent space and provides a latent goal-guided incentive to transitions toward this reference trajectory.","LAGMA consists of three major components: (a) quantized latent space constructed via a modified VQ-VAE for efficient sample utilization, (b) goal-reaching trajectory generation via extended VQ codebook, and (c) latent goal-guided intrinsic reward generation to encourage transitions towards the sampled goal-reaching path.","The proposed method is evaluated by StarCraft II with both dense and sparse reward settings and Google Research Football.","Empirical results show further performance improvement over state-of-the-art baselines."],"url":"http://arxiv.org/abs/2405.19998v1","category":"cs.MA"}
{"created":"2024-05-30 12:34:39","title":"Analyzing the impact of forecast errors in the planning of wine grape harvesting operations using a multi-stage stochastic model approach","abstract":"Forecasts and future beliefs play a critical role in the harvest labor hiring planning, especially when errors in them entails fixing previous made decisions, which can carry extra costs or losses. In this article, we study the effect that errors in the forecast/belief can have in the wine grape harvest planning process and the losses of the product. Errors are reflected in the prediction of yields and in the estimation of rain transition probabilities have on the value and losses of product. Also, using a multi-stage stochastic optimization model we can study the effect that second stage decisions have on the ability fix the planning decisions, reduce product losses and generate value. In a first step, we develop a multi-stage stochastic model which considers grape growth uncertainty given a belief in future events. The model decisions variables are: hiring, firing and maintaining harvest labor through periods, and also the harvested quantities in each period and block. Once the model defines the plan for the coming epoch, some decisions are implemented and a deviation in the forecast is revealed and the decision maker can adjust future decisions and beliefs. Results indicate that the effect of the errors in yield determination is not symmetrical; underestimations of the yields have a more significant negative effect on the objective function, while overestimation does not. Flexibility to revise hiring decisions does not make a significant difference if the yields are overestimated. The model significantly reduces losses of the better-quality grapes, since they correspond to a significant proportion of the income and account for the largest portion of income loss. Last, grapes that have an early improvement of their quality give the decision-maker an extra level of flexibility to adjust the harvesting plan.","sentences":["Forecasts and future beliefs play a critical role in the harvest labor hiring planning, especially when errors in them entails fixing previous made decisions, which can carry extra costs or losses.","In this article, we study the effect that errors in the forecast/belief can have in the wine grape harvest planning process and the losses of the product.","Errors are reflected in the prediction of yields and in the estimation of rain transition probabilities have on the value and losses of product.","Also, using a multi-stage stochastic optimization model we can study the effect that second stage decisions have on the ability fix the planning decisions, reduce product losses and generate value.","In a first step, we develop a multi-stage stochastic model which considers grape growth uncertainty given a belief in future events.","The model decisions variables are: hiring, firing and maintaining harvest labor through periods, and also the harvested quantities in each period and block.","Once the model defines the plan for the coming epoch, some decisions are implemented and a deviation in the forecast is revealed and the decision maker can adjust future decisions and beliefs.","Results indicate that the effect of the errors in yield determination is not symmetrical; underestimations of the yields have a more significant negative effect on the objective function, while overestimation does not.","Flexibility to revise hiring decisions does not make a significant difference if the yields are overestimated.","The model significantly reduces losses of the better-quality grapes, since they correspond to a significant proportion of the income and account for the largest portion of income loss.","Last, grapes that have an early improvement of their quality give the decision-maker an extra level of flexibility to adjust the harvesting plan."],"url":"http://arxiv.org/abs/2405.19997v1","category":"math.OC"}
{"created":"2024-05-30 12:32:35","title":"DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in the Wild","abstract":"Image quality assessment (IQA) plays a critical role in selecting high-quality images and guiding compression and enhancement methods in a series of applications. The blind IQA, which assesses the quality of in-the-wild images containing complex authentic distortions without reference images, poses greater challenges. Existing methods are limited to modeling a uniform distribution with local patches and are bothered by the gap between low and high-level visions (caused by widely adopted pre-trained classification networks). In this paper, we propose a novel IQA method called diffusion priors-based IQA (DP-IQA), which leverages the prior knowledge from the pre-trained diffusion model with its excellent powers to bridge semantic gaps in the perception of the visual quality of images. Specifically, we use pre-trained stable diffusion as the backbone, extract multi-level features from the denoising U-Net during the upsampling process at a specified timestep, and decode them to estimate the image quality score. The text and image adapters are adopted to mitigate the domain gap for downstream tasks and correct the information loss caused by the variational autoencoder bottleneck. Finally, we distill the knowledge in the above model into a CNN-based student model, significantly reducing the parameter to enhance applicability, with the student model performing similarly or even better than the teacher model surprisingly. Experimental results demonstrate that our DP-IQA achieves state-of-the-art results on various in-the-wild datasets with better generalization capability, which shows the superiority of our method in global modeling and utilizing the hierarchical feature clues of diffusion for evaluating image quality.","sentences":["Image quality assessment (IQA) plays a critical role in selecting high-quality images and guiding compression and enhancement methods in a series of applications.","The blind IQA, which assesses the quality of in-the-wild images containing complex authentic distortions without reference images, poses greater challenges.","Existing methods are limited to modeling a uniform distribution with local patches and are bothered by the gap between low and high-level visions (caused by widely adopted pre-trained classification networks).","In this paper, we propose a novel IQA method called diffusion priors-based IQA (DP-IQA), which leverages the prior knowledge from the pre-trained diffusion model with its excellent powers to bridge semantic gaps in the perception of the visual quality of images.","Specifically, we use pre-trained stable diffusion as the backbone, extract multi-level features from the denoising U-Net during the upsampling process at a specified timestep, and decode them to estimate the image quality score.","The text and image adapters are adopted to mitigate the domain gap for downstream tasks and correct the information loss caused by the variational autoencoder bottleneck.","Finally, we distill the knowledge in the above model into a CNN-based student model, significantly reducing the parameter to enhance applicability, with the student model performing similarly or even better than the teacher model surprisingly.","Experimental results demonstrate that our DP-IQA achieves state-of-the-art results on various in-the-wild datasets with better generalization capability, which shows the superiority of our method in global modeling and utilizing the hierarchical feature clues of diffusion for evaluating image quality."],"url":"http://arxiv.org/abs/2405.19996v1","category":"cs.CV"}
{"created":"2024-05-30 12:32:18","title":"Symmetries in Overparametrized Neural Networks: A Mean-Field View","abstract":"We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under data symmetric in law wrt the action of a general compact group $G$. We consider for this a class of generalized shallow NNs given by an ensemble of $N$ multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking $N\\to\\infty$ and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI laws and minimizes therein the population risk. We also give a counterexample to the general attainability of an optimum over SI laws. Despite this, quite remarkably, we show that the set of SI laws is also preserved by the MF dynamics even when freely trained. This sharply contrasts the finite-$N$ setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as $N$ gets larger in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We last deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.","sentences":["We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under data symmetric in","law wrt","the action of a general compact group $G$. We consider for this a class of generalized shallow NNs given by an ensemble of $N$ multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA).","We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA).","This allows us to define symmetric models compatible with taking $N\\to\\infty$ and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits.","When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI laws and minimizes therein the population risk.","We also give a counterexample to the general attainability of an optimum over SI laws.","Despite this, quite remarkably, we show that the set of SI laws is also preserved by the MF dynamics even when freely trained.","This sharply contrasts the finite-$N$ setting, in which EAs are generally not preserved by unconstrained SGD.","We illustrate the validity of our findings as $N$ gets larger in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes.","We last deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error."],"url":"http://arxiv.org/abs/2405.19995v1","category":"stat.ML"}
{"created":"2024-05-30 12:25:04","title":"Asymptotic v-numbers of graded (co)homology modules involving powers of an ideal","abstract":"Let $R$ be a Noetherian $\\mathbb{N}$-graded ring. Let $L$, $M$ and $N$ be finitely generated graded $R$-modules with $N \\subseteq M$. For a homogeneous ideal $I$, and for each fixed $k \\in \\mathbb{N}$, we show the asymptotic linearity of v-numbers of the graded modules $ {\\rm Ext}_R^{k}(L,{I^{n}M}/{I^{n}N})$ and ${\\rm Tor}_k^{R}(L,{I^{n}M}/{I^{n}N})$ as functions of $n$. Moreover, under some conditions on ${\\rm Ext}_R^k(L,M)$ and ${\\rm Tor}_k^R(L,M)$ respectively, we prove similar behaviour for v-numbers of ${\\rm Ext}_R^{k}(L,{M}/{I^{n}N})$ and $ {\\rm Tor}_k^{R}(L,{M}/{I^{n}N})$. The last result is obtained by proving the asymptotic linearity of v-number of $(U+I^{n}V)/I^{n}W$, where $U$, $V$ and $W$ are graded submodules of a finitely generated graded $R$-module such that $W \\subseteq V$ and $(0:_{U}I) = 0$.","sentences":["Let $R$ be a Noetherian $\\mathbb{N}$-graded ring.","Let $L$, $M$ and $N$ be finitely generated graded $R$-modules with $N \\subseteq M$. For a homogeneous ideal $I$, and for each fixed $k \\in \\mathbb{N}$, we show the asymptotic linearity of v-numbers of the graded modules $ {\\rm Ext}_R^{k}(L,{I^{n}M}/{I^{n}N})$ and ${\\rm Tor}_k^{R}(L,{I^{n}M}/{I^{n}N})$ as functions of $n$. Moreover, under some conditions on ${\\rm Ext}_R^k(L,M)$ and ${\\rm Tor}_k^R(L,M)$ respectively, we prove similar behaviour for v-numbers of ${\\rm Ext}_R^{k}(L,{M}/{I^{n}N})$ and","$ {\\rm Tor}_k^{R}(L,{M}/{I^{n}N})$.","The last result is obtained by proving the asymptotic linearity of v-number of $(U+I^{n}V)/I^{n}W$, where $U$, $V$ and $W$ are graded submodules of a finitely generated graded $R$-module such that $W \\subseteq V$ and $(0:_{U}I) = 0$."],"url":"http://arxiv.org/abs/2405.19992v1","category":"math.AC"}
{"created":"2024-05-30 12:24:03","title":"OpenTM: An Open-source, Single-GPU, Large-scale Thermal Microstructure Design Framework","abstract":"Thermal microstructures are artificially engineered materials designed to manipulate and control heat flow in unconventional ways. This paper presents an educational framework, called \\emph{OpenTM}, to use a single GPU for designing periodic 3D high-resolution thermal microstructures to match the predefined thermal conductivity matrices with volume fraction constraints. Specifically, we use adaptive volume fraction to make the Optimality Criteria (OC) method run stably to obtain the thermal microstructures without a large memory overhead.Practical examples with a high resolution $128 \\times 128 \\times 128$ run under 90 seconds per structure on an NVIDIA GeForce GTX 4070Ti GPU with a peak GPU memory of 355 MB. Our open-source, high-performance implementation is publicly accessible at \\url{https://github.com/quanyuchen2000/OPENTM}, and it is easy to install using Anaconda. Moreover, we provide a Python interface to make OpenTM well-suited for novices in C/C++.","sentences":["Thermal microstructures are artificially engineered materials designed to manipulate and control heat flow in unconventional ways.","This paper presents an educational framework, called \\emph{OpenTM}, to use a single GPU for designing periodic 3D high-resolution thermal microstructures to match the predefined thermal conductivity matrices with volume fraction constraints.","Specifically, we use adaptive volume fraction to make the Optimality Criteria (OC) method run stably to obtain the thermal microstructures without a large memory overhead.","Practical examples with a high resolution $128 \\times 128 \\times 128$ run under 90 seconds per structure on an NVIDIA GeForce GTX 4070Ti GPU with a peak GPU memory of 355 MB.","Our open-source, high-performance implementation is publicly accessible at \\url{https://github.com/quanyuchen2000/OPENTM}, and it is easy to install using Anaconda.","Moreover, we provide a Python interface to make OpenTM well-suited for novices in C/C++."],"url":"http://arxiv.org/abs/2405.19991v1","category":"cs.CE"}
{"created":"2024-05-30 12:22:06","title":"DiffPhysBA: Diffusion-based Physical Backdoor Attack against Person Re-Identification in Real-World","abstract":"Person Re-Identification (ReID) systems pose a significant security risk from backdoor attacks, allowing adversaries to evade tracking or impersonate others. Beyond recognizing this issue, we investigate how backdoor attacks can be deployed in real-world scenarios, where a ReID model is typically trained on data collected in the digital domain and then deployed in a physical environment. This attack scenario requires an attack flow that embeds backdoor triggers in the digital domain realistically enough to also activate the buried backdoor in person ReID models in the physical domain. This paper realizes this attack flow by leveraging a diffusion model to generate realistic accessories on pedestrian images (e.g., bags, hats, etc.) as backdoor triggers. However, the noticeable domain gap between the triggers generated by the off-the-shelf diffusion model and their physical counterparts results in a low attack success rate. Therefore, we introduce a novel diffusion-based physical backdoor attack (DiffPhysBA) method that adopts a training-free similarity-guided sampling process to enhance the resemblance between generated and physical triggers. Consequently, DiffPhysBA can generate realistic attributes as semantic-level triggers in the digital domain and provides higher physical ASR compared to the direct paste method by 25.6% on the real-world test set. Through evaluations on newly proposed real-world and synthetic ReID test sets, DiffPhysBA demonstrates an impressive success rate exceeding 90% in both the digital and physical domains. Notably, it excels in digital stealth metrics and can effectively evade state-of-the-art defense methods.","sentences":["Person Re-Identification (ReID) systems pose a significant security risk from backdoor attacks, allowing adversaries to evade tracking or impersonate others.","Beyond recognizing this issue, we investigate how backdoor attacks can be deployed in real-world scenarios, where a ReID model is typically trained on data collected in the digital domain and then deployed in a physical environment.","This attack scenario requires an attack flow that embeds backdoor triggers in the digital domain realistically enough to also activate the buried backdoor in person ReID models in the physical domain.","This paper realizes this attack flow by leveraging a diffusion model to generate realistic accessories on pedestrian images (e.g., bags, hats, etc.)","as backdoor triggers.","However, the noticeable domain gap between the triggers generated by the off-the-shelf diffusion model and their physical counterparts results in a low attack success rate.","Therefore, we introduce a novel diffusion-based physical backdoor attack (DiffPhysBA) method that adopts a training-free similarity-guided sampling process to enhance the resemblance between generated and physical triggers.","Consequently, DiffPhysBA can generate realistic attributes as semantic-level triggers in the digital domain and provides higher physical ASR compared to the direct paste method by 25.6% on the real-world test set.","Through evaluations on newly proposed real-world and synthetic ReID test sets, DiffPhysBA demonstrates an impressive success rate exceeding 90% in both the digital and physical domains.","Notably, it excels in digital stealth metrics and can effectively evade state-of-the-art defense methods."],"url":"http://arxiv.org/abs/2405.19990v1","category":"cs.CV"}
{"created":"2024-05-30 12:21:43","title":"Self-locked broadband Raman-electro-optic microcomb","abstract":"Optical frequency combs (OFCs), composed of equally spaced frequency tones, have spurred advancements in communications, spectroscopy, precision measurement and fundamental physics research. A prevalent method for generating OFCs involves the electro-optic (EO) effect, i.e., EO comb, renowned for its rapid tunability via precise microwave field control. Recent advances in integrated lithium niobate (LN) photonics have greatly enhanced the efficiency of EO effect, enabling the generation of broadband combs with reduced microwave power. However, parasitic nonlinear effects, such as Raman scattering and four-wave mixing, often emerge in high quality nonlinear devices, impeding the expansion of comb bandwidth and the minimization of frequency noise. Here, we tame these nonlinear effects and present a novel type of OFC, i.e., the self-locked Raman-electro-optic (REO) microcomb by leveraging the collaboration of EO, Kerr and Raman scattering processes. The spectral width of the REO microcomb benefits from the Raman gain and Kerr effect, encompassing nearly 1400 comb lines spanning over 300 nm with a fine repetition rate of 26.03 GHz, much larger than the pure EO combs. Remarkably, the system can maintain a self-locked low-noise state in the presence of multiple nonlinearities without the need for external active feedback. Our approach points to a direction for improving the performance of microcombs and paves the way for exploring new nonlinear physics, such as new laser locking techniques, through the collaboration of inevitable multiple nonlinear effects in integrated photonics.","sentences":["Optical frequency combs (OFCs), composed of equally spaced frequency tones, have spurred advancements in communications, spectroscopy, precision measurement and fundamental physics research.","A prevalent method for generating OFCs involves the electro-optic (EO) effect, i.e., EO comb, renowned for its rapid tunability via precise microwave field control.","Recent advances in integrated lithium niobate (LN) photonics have greatly enhanced the efficiency of EO effect, enabling the generation of broadband combs with reduced microwave power.","However, parasitic nonlinear effects, such as Raman scattering and four-wave mixing, often emerge in high quality nonlinear devices, impeding the expansion of comb bandwidth and the minimization of frequency noise.","Here, we tame these nonlinear effects and present a novel type of OFC, i.e., the self-locked Raman-electro-optic (REO) microcomb by leveraging the collaboration of EO, Kerr and Raman scattering processes.","The spectral width of the REO microcomb benefits from the Raman gain and Kerr effect, encompassing nearly 1400 comb lines spanning over 300 nm with a fine repetition rate of 26.03 GHz, much larger than the pure EO combs.","Remarkably, the system can maintain a self-locked low-noise state in the presence of multiple nonlinearities without the need for external active feedback.","Our approach points to a direction for improving the performance of microcombs and paves the way for exploring new nonlinear physics, such as new laser locking techniques, through the collaboration of inevitable multiple nonlinear effects in integrated photonics."],"url":"http://arxiv.org/abs/2405.19989v1","category":"physics.optics"}
{"created":"2024-05-30 12:18:06","title":"Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics","abstract":"Natural language is often the easiest and most convenient modality for humans to specify tasks for robots. However, learning to ground language to behavior typically requires impractical amounts of diverse, language-annotated demonstrations collected on each target robot. In this work, we aim to separate the problem of what to accomplish from how to accomplish it, as the former can benefit from substantial amounts of external observation-only data, and only the latter depends on a specific robot embodiment. To this end, we propose Video-Language Critic, a reward model that can be trained on readily available cross-embodiment data using contrastive learning and a temporal ranking objective, and use it to score behavior traces from a separate reinforcement learning actor. When trained on Open X-Embodiment data, our reward model enables 2x more sample-efficient policy training on Meta-World tasks than a sparse reward only, despite a significant domain gap. Using in-domain data but in a challenging task generalization setting on Meta-World, we further demonstrate more sample-efficient training than is possible with prior language-conditioned reward models that are either trained with binary classification, use static images, or do not leverage the temporal information present in video data.","sentences":["Natural language is often the easiest and most convenient modality for humans to specify tasks for robots.","However, learning to ground language to behavior typically requires impractical amounts of diverse, language-annotated demonstrations collected on each target robot.","In this work, we aim to separate the problem of what to accomplish from how to accomplish it, as the former can benefit from substantial amounts of external observation-only data, and only the latter depends on a specific robot embodiment.","To this end, we propose Video-Language Critic, a reward model that can be trained on readily available cross-embodiment data using contrastive learning and a temporal ranking objective, and use it to score behavior traces from a separate reinforcement learning actor.","When trained on Open X-Embodiment data, our reward model enables 2x more sample-efficient policy training on Meta-World tasks than a sparse reward only, despite a significant domain gap.","Using in-domain data but in a challenging task generalization setting on Meta-World, we further demonstrate more sample-efficient training than is possible with prior language-conditioned reward models that are either trained with binary classification, use static images, or do not leverage the temporal information present in video data."],"url":"http://arxiv.org/abs/2405.19988v1","category":"cs.RO"}
{"created":"2024-05-30 12:14:25","title":"Targeted Sequential Indirect Experiment Design","abstract":"Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health. Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect. Therefore, they perturb the target variable, but do not remove potential confounding factors. If, additionally, the resulting experimental measurements are multi-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified. We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query. While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.","sentences":["Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health.","Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect.","Therefore, they perturb the target variable, but do not remove potential confounding factors.","If, additionally, the resulting experimental measurements are multi-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified.","We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query.","While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings."],"url":"http://arxiv.org/abs/2405.19985v1","category":"stat.ME"}
{"created":"2024-05-30 12:07:08","title":"A Deep Reinforcement Learning Approach for Trading Optimization in the Forex Market with Multi-Agent Asynchronous Distribution","abstract":"In today's forex market traders increasingly turn to algorithmic trading, leveraging computers to seek more profits. Deep learning techniques as cutting-edge advancements in machine learning, capable of identifying patterns in financial data. Traders utilize these patterns to execute more effective trades, adhering to algorithmic trading rules. Deep reinforcement learning methods (DRL), by directly executing trades based on identified patterns and assessing their profitability, offer advantages over traditional DL approaches. This research pioneers the application of a multi-agent (MA) RL framework with the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) algorithm. The proposed method employs parallel learning across multiple asynchronous workers, each specialized in trading across multiple currency pairs to explore the potential for nuanced strategies tailored to different market conditions and currency pairs. Two different A3C with lock and without lock MA model was proposed and trained on single currency and multi-currency. The results indicate that both model outperform on Proximal Policy Optimization model. A3C with lock outperforms other in single currency training scenario and A3C without Lock outperforms other in multi-currency scenario. The findings demonstrate that this approach facilitates broader and faster exploration of different currency pairs, significantly enhancing trading returns. Additionally, the agent can learn a more profitable trading strategy in a shorter time.","sentences":["In today's forex market traders increasingly turn to algorithmic trading, leveraging computers to seek more profits.","Deep learning techniques as cutting-edge advancements in machine learning, capable of identifying patterns in financial data.","Traders utilize these patterns to execute more effective trades, adhering to algorithmic trading rules.","Deep reinforcement learning methods (DRL), by directly executing trades based on identified patterns and assessing their profitability, offer advantages over traditional DL approaches.","This research pioneers the application of a multi-agent (MA) RL framework with the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) algorithm.","The proposed method employs parallel learning across multiple asynchronous workers, each specialized in trading across multiple currency pairs to explore the potential for nuanced strategies tailored to different market conditions and currency pairs.","Two different A3C with lock and without lock MA model was proposed and trained on single currency and multi-currency.","The results indicate that both model outperform on Proximal Policy Optimization model.","A3C with lock outperforms other in single currency training scenario and A3C without Lock outperforms other in multi-currency scenario.","The findings demonstrate that this approach facilitates broader and faster exploration of different currency pairs, significantly enhancing trading returns.","Additionally, the agent can learn a more profitable trading strategy in a shorter time."],"url":"http://arxiv.org/abs/2405.19982v1","category":"cs.CE"}
{"created":"2024-05-30 12:01:37","title":"Cosmological Infrared Subtractions & Infrared-Safe Computables","abstract":"Cosmological observables in perturbation theory turn out to be plagued with infrared divergences, which represents both a conceptual and computational challenge. In this paper we present a proof of concept for a systematic procedure to remove these divergences in a large class of scalar cosmological integrals and consistently define an infrared safe computable in perturbation theory. We provide diagrammatic rules which are based on the nestohedra underlying the asymptotic structure of such integrals.","sentences":["Cosmological observables in perturbation theory turn out to be plagued with infrared divergences, which represents both a conceptual and computational challenge.","In this paper we present a proof of concept for a systematic procedure to remove these divergences in a large class of scalar cosmological integrals and consistently define an infrared safe computable in perturbation theory.","We provide diagrammatic rules which are based on the nestohedra underlying the asymptotic structure of such integrals."],"url":"http://arxiv.org/abs/2405.19979v1","category":"hep-th"}
{"created":"2024-05-30 12:01:12","title":"Domain Adaptation with Cauchy-Schwarz Divergence","abstract":"Domain adaptation aims to use training data from one or multiple source domains to learn a hypothesis that can be generalized to a different, but related, target domain. As such, having a reliable measure for evaluating the discrepancy of both marginal and conditional distributions is crucial. We introduce Cauchy-Schwarz (CS) divergence to the problem of unsupervised domain adaptation (UDA). The CS divergence offers a theoretically tighter generalization error bound than the popular Kullback-Leibler divergence. This holds for the general case of supervised learning, including multi-class classification and regression. Furthermore, we illustrate that the CS divergence enables a simple estimator on the discrepancy of both marginal and conditional distributions between source and target domains in the representation space, without requiring any distributional assumptions. We provide multiple examples to illustrate how the CS divergence can be conveniently used in both distance metric- or adversarial training-based UDA frameworks, resulting in compelling performance.","sentences":["Domain adaptation aims to use training data from one or multiple source domains to learn a hypothesis that can be generalized to a different, but related, target domain.","As such, having a reliable measure for evaluating the discrepancy of both marginal and conditional distributions is crucial.","We introduce Cauchy-Schwarz (CS) divergence to the problem of unsupervised domain adaptation (UDA).","The CS divergence offers a theoretically tighter generalization error bound than the popular Kullback-Leibler divergence.","This holds for the general case of supervised learning, including multi-class classification and regression.","Furthermore, we illustrate that the CS divergence enables a simple estimator on the discrepancy of both marginal and conditional distributions between source and target domains in the representation space, without requiring any distributional assumptions.","We provide multiple examples to illustrate how the CS divergence can be conveniently used in both distance metric- or adversarial training-based UDA frameworks, resulting in compelling performance."],"url":"http://arxiv.org/abs/2405.19978v1","category":"cs.LG"}
{"created":"2024-05-30 11:55:21","title":"GasTrace: Detecting Sandwich Attack Malicious Accounts in Ethereum","abstract":"The openness and transparency of Ethereum transaction data make it easy to be exploited by any entities, executing malicious attacks. The sandwich attack manipulates the Automated Market Maker (AMM) mechanism, profiting from manipulating the market price through front or after-running transactions. To identify and prevent sandwich attacks, we propose a cascade classification framework GasTrace. GasTrace analyzes various transaction features to detect malicious accounts, notably through the analysis and modeling of Gas features. In the initial classification, we utilize the Support Vector Machine (SVM) with the Radial Basis Function (RBF) kernel to generate the predicted probabilities of accounts, further constructing a detailed transaction network. Subsequently, the behavior features are captured by the Graph Attention Network (GAT) technique in the second classification. Through cascade classification, GasTrace can analyze and classify the sandwich attacks. Our experimental results demonstrate that GasTrace achieves a remarkable detection and generation capability, performing an accuracy of 96.73\\% and an F1 score of 95.71\\% for identifying sandwich attack accounts.","sentences":["The openness and transparency of Ethereum transaction data make it easy to be exploited by any entities, executing malicious attacks.","The sandwich attack manipulates the Automated Market Maker (AMM) mechanism, profiting from manipulating the market price through front or after-running transactions.","To identify and prevent sandwich attacks, we propose a cascade classification framework GasTrace.","GasTrace analyzes various transaction features to detect malicious accounts, notably through the analysis and modeling of Gas features.","In the initial classification, we utilize the Support Vector Machine (SVM) with the Radial Basis Function (RBF) kernel to generate the predicted probabilities of accounts, further constructing a detailed transaction network.","Subsequently, the behavior features are captured by the Graph Attention Network (GAT) technique in the second classification.","Through cascade classification, GasTrace can analyze and classify the sandwich attacks.","Our experimental results demonstrate that GasTrace achieves a remarkable detection and generation capability, performing an accuracy of 96.73\\% and an F1 score of 95.71\\% for identifying sandwich attack accounts."],"url":"http://arxiv.org/abs/2405.19971v1","category":"cs.CR"}
{"created":"2024-05-30 11:55:10","title":"Strategies to Counter Artificial Intelligence in Law Enforcement: Cross-Country Comparison of Citizens in Greece, Italy and Spain","abstract":"This paper investigates citizens' counter-strategies to the use of Artificial Intelligence (AI) by law enforcement agencies (LEAs). Based on information from three countries (Greece, Italy and Spain) we demonstrate disparities in the likelihood of ten specific counter-strategies. We further identified factors that increase the propensity for counter-strategies. Our study provides an important new perspective to societal impacts of security-focused AI applications by illustrating the conscious, strategic choices by citizens when confronted with AI capabilities for LEAs.","sentences":["This paper investigates citizens' counter-strategies to the use of Artificial Intelligence (AI) by law enforcement agencies (LEAs).","Based on information from three countries (Greece, Italy and Spain) we demonstrate disparities in the likelihood of ten specific counter-strategies.","We further identified factors that increase the propensity for counter-strategies.","Our study provides an important new perspective to societal impacts of security-focused AI applications by illustrating the conscious, strategic choices by citizens when confronted with AI capabilities for LEAs."],"url":"http://arxiv.org/abs/2405.19970v1","category":"cs.AI"}
{"created":"2024-05-30 11:48:32","title":"A Dynamic Logic for Information Evaluation in Intelligence","abstract":"In the field of human intelligence, officers use an alphanumeric scale, known as the Admiralty System, to rate the credibility of messages and the reliability of their sources (NATO AJP-2.1, 2016). During this evaluation, they are expected to estimate the credibility and reliability dimensions independently of each other (NATO STANAG, 2003). However, empirical results show that officers perceive these dimensions as strongly correlated (Baker et al., 1968). More precisely, they consider credibility as playing the leading role over reliability, the importance of which is only secondary (Samet, 1975). In this paper, we present a formal evaluative procedure, called L(intel), in line with these findings. We adapt dynamic belief revision to make credibility the main dimension of evaluation and introduce dynamic operators to update credibility ratings with the source's reliability. In addition to being empirically sound, we show that L(intel) provides an effective procedure to classify intelligence messages along the descriptive taxonomy presented in Icard (2023).","sentences":["In the field of human intelligence, officers use an alphanumeric scale, known as the Admiralty System, to rate the credibility of messages and the reliability of their sources (NATO AJP-2.1, 2016).","During this evaluation, they are expected to estimate the credibility and reliability dimensions independently of each other (NATO STANAG, 2003).","However, empirical results show that officers perceive these dimensions as strongly correlated (Baker et al., 1968).","More precisely, they consider credibility as playing the leading role over reliability, the importance of which is only secondary (Samet, 1975).","In this paper, we present a formal evaluative procedure, called L(intel), in line with these findings.","We adapt dynamic belief revision to make credibility the main dimension of evaluation and introduce dynamic operators to update credibility ratings with the source's reliability.","In addition to being empirically sound, we show that L(intel) provides an effective procedure to classify intelligence messages along the descriptive taxonomy presented in Icard (2023)."],"url":"http://arxiv.org/abs/2405.19968v1","category":"cs.LO"}
{"created":"2024-05-30 11:46:42","title":"Improved Out-of-Scope Intent Classification with Dual Encoding and Threshold-based Re-Classification","abstract":"Detecting out-of-scope user utterances is essential for task-oriented dialogues and intent classification. Current methodologies face difficulties with the unpredictable distribution of outliers and often rely on assumptions about data distributions. We present the Dual Encoder for Threshold-Based Re-Classification (DETER) to address these challenges. This end-to-end framework efficiently detects out-of-scope intents without requiring assumptions on data distributions or additional post-processing steps. The core of DETER utilizes dual text encoders, the Universal Sentence Encoder (USE) and the Transformer-based Denoising AutoEncoder (TSDAE), to generate user utterance embeddings, which are classified through a branched neural architecture. Further, DETER generates synthetic outliers using self-supervision and incorporates out-of-scope phrases from open-domain datasets. This approach ensures a comprehensive training set for out-of-scope detection. Additionally, a threshold-based re-classification mechanism refines the model's initial predictions. Evaluations on the CLINC-150, Stackoverflow, and Banking77 datasets demonstrate DETER's efficacy. Our model outperforms previous benchmarks, increasing up to 13% and 5% in F1 score for known and unknown intents on CLINC-150 and Stackoverflow, and 16% for known and 24% % for unknown intents on Banking77. The source code has been released at https://github.com/Hossam-Mohammed-tech/Intent\\_Classification\\_OOS.","sentences":["Detecting out-of-scope user utterances is essential for task-oriented dialogues and intent classification.","Current methodologies face difficulties with the unpredictable distribution of outliers and often rely on assumptions about data distributions.","We present the Dual Encoder for Threshold-Based Re-Classification (DETER) to address these challenges.","This end-to-end framework efficiently detects out-of-scope intents without requiring assumptions on data distributions or additional post-processing steps.","The core of DETER utilizes dual text encoders, the Universal Sentence Encoder (USE) and the Transformer-based Denoising AutoEncoder (TSDAE), to generate user utterance embeddings, which are classified through a branched neural architecture.","Further, DETER generates synthetic outliers using self-supervision and incorporates out-of-scope phrases from open-domain datasets.","This approach ensures a comprehensive training set for out-of-scope detection.","Additionally, a threshold-based re-classification mechanism refines the model's initial predictions.","Evaluations on the CLINC-150, Stackoverflow, and Banking77 datasets demonstrate DETER's efficacy.","Our model outperforms previous benchmarks, increasing up to 13% and 5% in F1 score for known and unknown intents on CLINC-150 and Stackoverflow, and 16% for known and 24% % for unknown intents on Banking77.","The source code has been released at https://github.com/Hossam-Mohammed-tech/Intent\\_Classification\\_OOS."],"url":"http://arxiv.org/abs/2405.19967v1","category":"cs.CL"}
{"created":"2024-05-30 11:39:38","title":"Comparison of non-decoy single-photon source and decoy weak coherent pulse in quantum key distribution","abstract":"Advancements in practical single-photon sources (SPS) exhibiting high brightness and low $g^{(2)}(0)$ have garnered significant interest for their application in quantum key distribution (QKD). To assess their QKD performance, it is essential to compare them with the widely employed weak coherent pulses (WCPs) in the decoy state method. In this work, we analyze the non-decoy efficient BB84 protocol for an SPS, partially characterising its photon statistics by its $g^{(2)}(0)$ and mean photon number. We compare it to the 2-decoy efficient BB84 with WCPs within the finite-key analysis framework while optimizing the parameters of both protocols. Our findings indicate that the non-decoy SPS with a mean photon number of $\\langle n \\rangle = 0.5$ and $g^{(2)}(0) = 3.6\\%$ can enhance the secure key generation over the 2-decoy WCP for block sizes under $4.66 \\cdot 10^9$ sent signals ($29$ seconds of acquisition time) at a channel loss of $10$ dB ($52.5$ km of optical fibre). Additionally, we demonstrate an increase in the maximum tolerable channel loss for SPSs with mean photon number $\\langle n \\rangle \\geq 0.0142$ at block sizes below $10^8$ sent signals ($0.62$ seconds of acquisition time). These results suggest that SPSs hold potential for key rate enhancement in short-range QKD networks, though further research is required to evaluate their key generation capabilities when integrated into the decoy method.","sentences":["Advancements in practical single-photon sources (SPS) exhibiting high brightness and low $g^{(2)}(0)$ have garnered significant interest for their application in quantum key distribution (QKD).","To assess their QKD performance, it is essential to compare them with the widely employed weak coherent pulses (WCPs) in the decoy state method.","In this work, we analyze the non-decoy efficient BB84 protocol for an SPS, partially characterising its photon statistics by its $g^{(2)}(0)$ and mean photon number.","We compare it to the 2-decoy efficient BB84 with WCPs within the finite-key analysis framework while optimizing the parameters of both protocols.","Our findings indicate that the non-decoy SPS with a mean photon number of $\\langle n \\rangle = 0.5$ and $g^{(2)}(0) = 3.6\\%$ can enhance the secure key generation over the 2-decoy WCP for block sizes under $4.66 \\cdot 10^9$ sent signals ($29$ seconds of acquisition time) at a channel loss of $10$ dB ($52.5$ km of optical fibre).","Additionally, we demonstrate an increase in the maximum tolerable channel loss for SPSs with mean photon number $\\langle n \\rangle \\geq 0.0142$ at block sizes below $10^8$ sent signals ($0.62$ seconds of acquisition time).","These results suggest that SPSs hold potential for key rate enhancement in short-range QKD networks, though further research is required to evaluate their key generation capabilities when integrated into the decoy method."],"url":"http://arxiv.org/abs/2405.19963v1","category":"quant-ph"}
{"created":"2024-05-30 11:32:42","title":"Collective Variable Free Transition Path Sampling with Generative Flow Network","abstract":"Understanding transition paths between meta-stable states in molecular systems is fundamental for material design and drug discovery. However, sampling these paths via molecular dynamics simulations is computationally prohibitive due to the high-energy barriers between the meta-stable states. Recent machine learning approaches are often restricted to simple systems or rely on collective variables (CVs) extracted from expensive domain knowledge. In this work, we propose to leverage generative flow networks (GFlowNets) to sample transition paths without relying on CVs. We reformulate the problem as amortized energy-based sampling over molecular trajectories and train a bias potential by minimizing the squared log-ratio between the target distribution and the generator, derived from the flow matching objective of GFlowNets. Our evaluation on three proteins (Alanine Dipeptide, Polyproline, and Chignolin) demonstrates that our approach, called TPS-GFN, generates more realistic and diverse transition paths than the previous CV-free machine learning approach.","sentences":["Understanding transition paths between meta-stable states in molecular systems is fundamental for material design and drug discovery.","However, sampling these paths via molecular dynamics simulations is computationally prohibitive due to the high-energy barriers between the meta-stable states.","Recent machine learning approaches are often restricted to simple systems or rely on collective variables (CVs) extracted from expensive domain knowledge.","In this work, we propose to leverage generative flow networks (GFlowNets) to sample transition paths without relying on CVs.","We reformulate the problem as amortized energy-based sampling over molecular trajectories and train a bias potential by minimizing the squared log-ratio between the target distribution and the generator, derived from the flow matching objective of GFlowNets.","Our evaluation on three proteins (Alanine Dipeptide, Polyproline, and Chignolin) demonstrates that our approach, called TPS-GFN, generates more realistic and diverse transition paths than the previous CV-free machine learning approach."],"url":"http://arxiv.org/abs/2405.19961v1","category":"cs.LG"}
{"created":"2024-05-30 11:25:42","title":"Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation","abstract":"Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., \"positive\" from sentiment and \"sport\" from topic). For ease of obtaining training samples, existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that MAGIC outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios. Our source code and data are available at https://github.com/nju-websoft/MAGIC.","sentences":["Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., \"positive\" from sentiment and \"sport\" from topic).","For ease of obtaining training samples, existing works neglect attribute correlations formed by the intertwining of different attributes.","Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control.","In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation.","We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement.","During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control.","Experiments show that MAGIC outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios.","Our source code and data are available at https://github.com/nju-websoft/MAGIC."],"url":"http://arxiv.org/abs/2405.19958v1","category":"cs.CL"}
{"created":"2024-05-30 11:23:01","title":"PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting","abstract":"As text-conditioned diffusion models (DMs) achieve breakthroughs in image, video, and 3D generation, the research community's focus has shifted to the more challenging task of text-to-4D synthesis, which introduces a temporal dimension to generate dynamic 3D objects. In this context, we identify Score Distillation Sampling (SDS), a widely used technique for text-to-3D synthesis, as a significant hindrance to text-to-4D performance due to its Janus-faced and texture-unrealistic problems coupled with high computational costs. In this paper, we propose \\textbf{P}ixel-\\textbf{L}evel \\textbf{A}lignments for Text-to-\\textbf{4D} Gaussian Splatting (\\textbf{PLA4D}), a novel method that utilizes text-to-video frames as explicit pixel alignment targets to generate static 3D objects and inject motion into them. Specifically, we introduce Focal Alignment to calibrate camera poses for rendering and GS-Mesh Contrastive Learning to distill geometry priors from rendered image contrasts at the pixel level. Additionally, we develop Motion Alignment using a deformation network to drive changes in Gaussians and implement Reference Refinement for smooth 4D object surfaces. These techniques enable 4D Gaussian Splatting to align geometry, texture, and motion with generated videos at the pixel level. Compared to previous methods, PLA4D produces synthesized outputs with better texture details in less time and effectively mitigates the Janus-faced problem. PLA4D is fully implemented using open-source models, offering an accessible, user-friendly, and promising direction for 4D digital content creation. Our project page: \\href{https://github.com/MiaoQiaowei/PLA4D.github.io}{https://github.com/MiaoQiaowei/PLA4D.github.io}.","sentences":["As text-conditioned diffusion models (DMs) achieve breakthroughs in image, video, and 3D generation, the research community's focus has shifted to the more challenging task of text-to-4D synthesis, which introduces a temporal dimension to generate dynamic 3D objects.","In this context, we identify Score Distillation Sampling (SDS), a widely used technique for text-to-3D synthesis, as a significant hindrance to text-to-4D performance due to its Janus-faced and texture-unrealistic problems coupled with high computational costs.","In this paper, we propose \\textbf{P}ixel-\\textbf{L}evel \\textbf{A}lignments for Text-to-\\textbf{4D} Gaussian Splatting (\\textbf{PLA4D}), a novel method that utilizes text-to-video frames as explicit pixel alignment targets to generate static 3D objects and inject motion into them.","Specifically, we introduce Focal Alignment to calibrate camera poses for rendering and GS-Mesh Contrastive Learning to distill geometry priors from rendered image contrasts at the pixel level.","Additionally, we develop Motion Alignment using a deformation network to drive changes in Gaussians and implement Reference Refinement for smooth 4D object surfaces.","These techniques enable 4D Gaussian Splatting to align geometry, texture, and motion with generated videos at the pixel level.","Compared to previous methods, PLA4D produces synthesized outputs with better texture details in less time and effectively mitigates the Janus-faced problem.","PLA4D is fully implemented using open-source models, offering an accessible, user-friendly, and promising direction for 4D digital content creation.","Our project page: \\href{https://github.com/MiaoQiaowei/PLA4D.github.io}{https://github.com/MiaoQiaowei/PLA4D.github.io}."],"url":"http://arxiv.org/abs/2405.19957v1","category":"cs.CV"}
{"created":"2024-05-30 11:22:55","title":"HOLMES: to Detect Adversarial Examples with Multiple Detectors","abstract":"Deep neural networks (DNNs) can easily be cheated by some imperceptible but purposeful noise added to images, and erroneously classify them. Previous defensive work mostly focused on retraining the models or detecting the noise, but has either shown limited success rates or been attacked by new adversarial examples. Instead of focusing on adversarial images or the interior of DNN models, we observed that adversarial examples generated by different algorithms can be identified based on the output of DNNs (logits). Logit can serve as an exterior feature to train detectors. Then, we propose HOLMES (Hierarchically Organized Light-weight Multiple dEtector System) to reinforce DNNs by detecting potential adversarial examples to minimize the threats they may bring in practical. HOLMES is able to distinguish \\textit{unseen} adversarial examples from multiple attacks with high accuracy and low false positive rates than single detector systems even in an adaptive model. To ensure the diversity and randomness of detectors in HOLMES, we use two methods: training dedicated detectors for each label and training detectors with top-k logits. Our effective and inexpensive strategies neither modify original DNN models nor require its internal parameters. HOLMES is not only compatible with all kinds of learning models (even only with external APIs), but also complementary to other defenses to achieve higher detection rates (may also fully protect the system against various adversarial examples).","sentences":["Deep neural networks (DNNs) can easily be cheated by some imperceptible but purposeful noise added to images, and erroneously classify them.","Previous defensive work mostly focused on retraining the models or detecting the noise, but has either shown limited success rates or been attacked by new adversarial examples.","Instead of focusing on adversarial images or the interior of DNN models, we observed that adversarial examples generated by different algorithms can be identified based on the output of DNNs (logits).","Logit can serve as an exterior feature to train detectors.","Then, we propose HOLMES (Hierarchically Organized Light-weight Multiple dEtector System) to reinforce DNNs by detecting potential adversarial examples to minimize the threats they may bring in practical.","HOLMES is able to distinguish \\textit{unseen} adversarial examples from multiple attacks with high accuracy and low false positive rates than single detector systems even in an adaptive model.","To ensure the diversity and randomness of detectors in HOLMES, we use two methods: training dedicated detectors for each label and training detectors with top-k logits.","Our effective and inexpensive strategies neither modify original DNN models nor require its internal parameters.","HOLMES is not only compatible with all kinds of learning models (even only with external APIs), but also complementary to other defenses to achieve higher detection rates (may also fully protect the system against various adversarial examples)."],"url":"http://arxiv.org/abs/2405.19956v1","category":"cs.AI"}
{"created":"2024-05-30 11:18:52","title":"GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection, Localization, Reasoning, and Remediation","abstract":"A key challenge associated with Kubernetes configuration files (KCFs) is that they are often highly complex and error-prone, leading to security vulnerabilities and operational setbacks. Rule-based (RB) tools for KCF misconfiguration detection rely on static rule sets, making them inherently limited and unable to detect newly-discovered misconfigurations. RB tools also suffer from misdetection, since mistakes are likely when coding the detection rules. Recent methods for detecting and remediating KCF misconfigurations are limited in terms of their scalability and detection coverage, or due to the fact that they have high expertise requirements and do not offer automated remediation along with misconfiguration detection. Novel approaches that employ LLMs in their pipeline rely on API-based, general-purpose, and mainly commercial models. Thus, they pose security challenges, have inconsistent classification performance, and can be costly. In this paper, we propose GenKubeSec, a comprehensive and adaptive, LLM-based method, which, in addition to detecting a wide variety of KCF misconfigurations, also identifies the exact location of the misconfigurations and provides detailed reasoning about them, along with suggested remediation. When empirically compared with three industry-standard RB tools, GenKubeSec achieved equivalent precision (0.990) and superior recall (0.999). When a random sample of KCFs was examined by a Kubernetes security expert, GenKubeSec's explanations as to misconfiguration localization, reasoning and remediation were 100% correct, informative and useful. To facilitate further advancements in this domain, we share the unique dataset we collected, a unified misconfiguration index we developed for label standardization, our experimentation code, and GenKubeSec itself as an open-source tool.","sentences":["A key challenge associated with Kubernetes configuration files (KCFs) is that they are often highly complex and error-prone, leading to security vulnerabilities and operational setbacks.","Rule-based (RB) tools for KCF misconfiguration detection rely on static rule sets, making them inherently limited and unable to detect newly-discovered misconfigurations.","RB tools also suffer from misdetection, since mistakes are likely when coding the detection rules.","Recent methods for detecting and remediating KCF misconfigurations are limited in terms of their scalability and detection coverage, or due to the fact that they have high expertise requirements and do not offer automated remediation along with misconfiguration detection.","Novel approaches that employ LLMs in their pipeline rely on API-based, general-purpose, and mainly commercial models.","Thus, they pose security challenges, have inconsistent classification performance, and can be costly.","In this paper, we propose GenKubeSec, a comprehensive and adaptive, LLM-based method, which, in addition to detecting a wide variety of KCF misconfigurations, also identifies the exact location of the misconfigurations and provides detailed reasoning about them, along with suggested remediation.","When empirically compared with three industry-standard RB tools, GenKubeSec achieved equivalent precision (0.990) and superior recall (0.999).","When a random sample of KCFs was examined by a Kubernetes security expert, GenKubeSec's explanations as to misconfiguration localization, reasoning and remediation were 100% correct, informative and useful.","To facilitate further advancements in this domain, we share the unique dataset we collected, a unified misconfiguration index we developed for label standardization, our experimentation code, and GenKubeSec itself as an open-source tool."],"url":"http://arxiv.org/abs/2405.19954v1","category":"cs.CR"}
{"created":"2024-05-30 11:18:17","title":"Do high redshift QSOs and GRBs corroborate JWST?","abstract":"The James Webb Space Telescope (JWST) is reporting unexpectedly massive high redshift galaxies that appear challenging from the $\\Lambda$CDM perspective. Interpreted as a problem of cosmological origin, this necessitates Planck underestimating either matter density $\\Omega_m$ or physical matter density $\\Omega_m h^2$ at higher redshifts. Through standard frequentist profile likelihoods, we identify corroborating quasar (QSO) and gamma-ray burst (GRB) data sets where $\\Omega_m$ increases with effective redshift $z_{\\textrm{eff}}$, with $\\Omega_m$ remaining anomalously large at higher redshifts. While the variation of $\\Omega_m$ with $z_{\\textrm{eff}}$ is at odds with the $\\Lambda$CDM model, demarcating frequentist confidence intervals through differences in $\\chi^2$ in profile likelihoods, the prevailing technique in the literature, points to $3.9 \\sigma$ and $7.9 \\sigma$ tensions between GRBs and QSOs, respectively, and Planck-$\\Lambda$CDM. We explain the approximations inherent in the existing profile likelihood literature, and highlight fresh methodology that generalises the prescription. We show that alternative methods, including Bayesian approaches, lead to similar tensions.","sentences":["The James Webb Space Telescope (JWST) is reporting unexpectedly massive high redshift galaxies that appear challenging from the $\\Lambda$CDM perspective.","Interpreted as a problem of cosmological origin, this necessitates Planck underestimating either matter density $\\Omega_m$ or physical matter density $\\Omega_m h^2$ at higher redshifts.","Through standard frequentist profile likelihoods, we identify corroborating quasar (QSO) and gamma-ray burst (GRB) data sets where $\\Omega_m$ increases with effective redshift $z_{\\textrm{eff}}$, with $\\Omega_m$ remaining anomalously large at higher redshifts.","While the variation of $\\Omega_m$ with $z_{\\textrm{eff}}$ is at odds with the $\\Lambda$CDM model, demarcating frequentist confidence intervals through differences in $\\chi^2$ in profile likelihoods, the prevailing technique in the literature, points to $3.9 \\sigma$ and $7.9 \\sigma$ tensions between GRBs and QSOs, respectively, and Planck-$\\Lambda$CDM.","We explain the approximations inherent in the existing profile likelihood literature, and highlight fresh methodology that generalises the prescription.","We show that alternative methods, including Bayesian approaches, lead to similar tensions."],"url":"http://arxiv.org/abs/2405.19953v1","category":"astro-ph.CO"}
{"created":"2024-05-30 11:15:28","title":"Generalized Bigraded Toda Hierarchy","abstract":"Bigraded Toda hierarchy $L_1^M(n)=L_2^N(n)$ is generalized to $L_1^M(n)=L_2^{N}(n)+\\sum_{j\\in \\mathbb Z}\\sum_{i=1}^{m}q^{(i)}_n\\Lambda^jr^{(i)}_{n+1}$, which is the analogue of the famous constrained KP hierarchy $L^{k}= (L^{k})_{\\geq0}+\\sum_{i=1}^{m}q_{i}\\partial^{-1}r_i$. It is known that different bosonizations of fermionic KP hierarchy will give rise to different kinds of integrable hierarchies. Starting from the fermionic form of constrained KP hierarchy, bilinear equation of this generalized bigraded Toda hierarchy (GBTH) are derived by using 2--component boson--fermion correspondence. Next based upon this, the Lax structure of GBTH is obtained. Conversely, we also derive bilinear equation of GBTH from the corresponding Lax structure.","sentences":["Bigraded Toda hierarchy $L_1^M(n)=L_2^N(n)$ is generalized to $L_1^M(n)=L_2^{N}(n)+\\sum_{j\\in \\mathbb Z}\\sum_{i=1}^{m}q^{(i)}_n\\Lambda^jr^{(i)}_{n+1}$, which is the analogue of the famous constrained KP hierarchy $L^{k}= (L^{k})_{\\geq0}+\\sum_{i=1}^{m}q_{i}\\partial^{-1}r_i$.","It is known that different bosonizations of fermionic KP hierarchy will give rise to different kinds of integrable hierarchies.","Starting from the fermionic form of constrained KP hierarchy, bilinear equation of this generalized bigraded Toda hierarchy (GBTH) are derived by using 2--component boson--fermion correspondence.","Next based upon this, the Lax structure of GBTH is obtained.","Conversely, we also derive bilinear equation of GBTH from the corresponding Lax structure."],"url":"http://arxiv.org/abs/2405.19952v1","category":"nlin.SI"}
{"created":"2024-05-30 11:14:16","title":"A Simple Linear Convergence Analysis of the Point-SAGA Algorithm","abstract":"Point-SAGA is a randomized algorithm for minimizing a sum of convex functions using their proximity operators (proxs), proposed by Defazio (2016). At every iteration, the prox of only one randomly chosen function is called. We generalize the algorithm to any number of prox calls per iteration, not only one, and propose a simple proof of linear convergence when the functions are smooth and strongly convex.","sentences":["Point-SAGA is a randomized algorithm for minimizing a sum of convex functions using their proximity operators (proxs), proposed by Defazio (2016).","At every iteration, the prox of only one randomly chosen function is called.","We generalize the algorithm to any number of prox calls per iteration, not only one, and propose a simple proof of linear convergence when the functions are smooth and strongly convex."],"url":"http://arxiv.org/abs/2405.19951v1","category":"math.OC"}
{"created":"2024-05-30 11:14:01","title":"MM-Lego: Modular Biomedical Multimodal Models with Minimal Fine-Tuning","abstract":"Learning holistic computational representations in physical, chemical or biological systems requires the ability to process information from different distributions and modalities within the same model. Thus, the demand for multimodal machine learning models has sharply risen for modalities that go beyond vision and language, such as sequences, graphs, time series, or tabular data. While there are many available multimodal fusion and alignment approaches, most of them require end-to-end training, scale quadratically with the number of modalities, cannot handle cases of high modality imbalance in the training set, or are highly topology-specific, making them too restrictive for many biomedical learning tasks. This paper presents Multimodal Lego (MM-Lego), a modular and general-purpose fusion and model merging framework to turn any set of encoders into a competitive multimodal model with no or minimal fine-tuning. We achieve this by introducing a wrapper for unimodal encoders that enforces lightweight dimensionality assumptions between modalities and harmonises their representations by learning features in the frequency domain to enable model merging with little signal interference. We show that MM-Lego 1) can be used as a model merging method which achieves competitive performance with end-to-end fusion models without any fine-tuning, 2) can operate on any unimodal encoder, and 3) is a model fusion method that, with minimal fine-tuning, achieves state-of-the-art results on six benchmarked multimodal biomedical tasks.","sentences":["Learning holistic computational representations in physical, chemical or biological systems requires the ability to process information from different distributions and modalities within the same model.","Thus, the demand for multimodal machine learning models has sharply risen for modalities that go beyond vision and language, such as sequences, graphs, time series, or tabular data.","While there are many available multimodal fusion and alignment approaches, most of them require end-to-end training, scale quadratically with the number of modalities, cannot handle cases of high modality imbalance in the training set, or are highly topology-specific, making them too restrictive for many biomedical learning tasks.","This paper presents Multimodal Lego (MM-Lego), a modular and general-purpose fusion and model merging framework to turn any set of encoders into a competitive multimodal model with no or minimal fine-tuning.","We achieve this by introducing a wrapper for unimodal encoders that enforces lightweight dimensionality assumptions between modalities and harmonises their representations by learning features in the frequency domain to enable model merging with little signal interference.","We show that MM-Lego 1) can be used as a model merging method which achieves competitive performance with end-to-end fusion models without any fine-tuning, 2) can operate on any unimodal encoder, and 3) is a model fusion method that, with minimal fine-tuning, achieves state-of-the-art results on six benchmarked multimodal biomedical tasks."],"url":"http://arxiv.org/abs/2405.19950v1","category":"cs.LG"}
{"created":"2024-05-30 11:11:54","title":"Hyper-Transformer for Amodal Completion","abstract":"Amodal object completion is a complex task that involves predicting the invisible parts of an object based on visible segments and background information. Learning shape priors is crucial for effective amodal completion, but traditional methods often rely on two-stage processes or additional information, leading to inefficiencies and potential error accumulation. To address these shortcomings, we introduce a novel framework named the Hyper-Transformer Amodal Network (H-TAN). This framework utilizes a hyper transformer equipped with a dynamic convolution head to directly learn shape priors and accurately predict amodal masks. Specifically, H-TAN uses a dual-branch structure to extract multi-scale features from both images and masks. The multi-scale features from the image branch guide the hyper transformer in learning shape priors and in generating the weights for dynamic convolution tailored to each instance. The dynamic convolution head then uses the features from the mask branch to predict precise amodal masks. We extensively evaluate our model on three benchmark datasets: KINS, COCOA-cls, and D2SA, where H-TAN demonstrated superior performance compared to existing methods. Additional experiments validate the effectiveness and stability of the novel hyper transformer in our framework.","sentences":["Amodal object completion is a complex task that involves predicting the invisible parts of an object based on visible segments and background information.","Learning shape priors is crucial for effective amodal completion, but traditional methods often rely on two-stage processes or additional information, leading to inefficiencies and potential error accumulation.","To address these shortcomings, we introduce a novel framework named the Hyper-Transformer Amodal Network (H-TAN).","This framework utilizes a hyper transformer equipped with a dynamic convolution head to directly learn shape priors and accurately predict amodal masks.","Specifically, H-TAN uses a dual-branch structure to extract multi-scale features from both images and masks.","The multi-scale features from the image branch guide the hyper transformer in learning shape priors and in generating the weights for dynamic convolution tailored to each instance.","The dynamic convolution head then uses the features from the mask branch to predict precise amodal masks.","We extensively evaluate our model on three benchmark datasets: KINS, COCOA-cls, and D2SA, where H-TAN demonstrated superior performance compared to existing methods.","Additional experiments validate the effectiveness and stability of the novel hyper transformer in our framework."],"url":"http://arxiv.org/abs/2405.19949v1","category":"cs.CV"}
{"created":"2024-05-30 11:10:11","title":"Scalable Test Generation to Trigger Rare Targets in High-Level Synthesizable IPs for Cloud FPGAs","abstract":"High-Level Synthesis (HLS) has transformed the development of complex Hardware IPs (HWIP) by offering abstraction and configurability through languages like SystemC/C++, particularly for Field Programmable Gate Array (FPGA) accelerators in high-performance and cloud computing contexts. These IPs can be synthesized for different FPGA boards in cloud, offering compact area requirements and enhanced flexibility. HLS enables designs to execute directly on ARM processors within modern FPGAs without the need for Register Transfer Level (RTL) synthesis, thereby conserving FPGA resources. While HLS offers flexibility and efficiency, it also introduces potential vulnerabilities such as the presence of hidden circuitry, including the possibility of hosting hardware trojans within designs. In cloud environments, these vulnerabilities pose significant security concerns such as leakage of sensitive data, IP functionality disruption and hardware damage, necessitating the development of robust testing frameworks. This research presents an advanced testing approach for HLS-developed cloud IPs, specifically targeting hidden malicious functionalities that may exist in rare conditions within the design. The proposed method leverages selective instrumentation, combining greybox fuzzing and concolic execution techniques to enhance test generation capabilities. Evaluation conducted on various HLS benchmarks, possessing characteristics of FPGA-based cloud IPs with embedded cloud related threats, demonstrates the effectiveness of our framework in detecting trojans and rare scenarios, showcasing improvements in coverage, time efficiency, memory usage, and testing costs compared to existing methods.","sentences":["High-Level Synthesis (HLS) has transformed the development of complex Hardware IPs (HWIP) by offering abstraction and configurability through languages like SystemC/C++, particularly for Field Programmable Gate Array (FPGA) accelerators in high-performance and cloud computing contexts.","These IPs can be synthesized for different FPGA boards in cloud, offering compact area requirements and enhanced flexibility.","HLS enables designs to execute directly on ARM processors within modern FPGAs without the need for Register Transfer Level (RTL) synthesis, thereby conserving FPGA resources.","While HLS offers flexibility and efficiency, it also introduces potential vulnerabilities such as the presence of hidden circuitry, including the possibility of hosting hardware trojans within designs.","In cloud environments, these vulnerabilities pose significant security concerns such as leakage of sensitive data, IP functionality disruption and hardware damage, necessitating the development of robust testing frameworks.","This research presents an advanced testing approach for HLS-developed cloud IPs, specifically targeting hidden malicious functionalities that may exist in rare conditions within the design.","The proposed method leverages selective instrumentation, combining greybox fuzzing and concolic execution techniques to enhance test generation capabilities.","Evaluation conducted on various HLS benchmarks, possessing characteristics of FPGA-based cloud IPs with embedded cloud related threats, demonstrates the effectiveness of our framework in detecting trojans and rare scenarios, showcasing improvements in coverage, time efficiency, memory usage, and testing costs compared to existing methods."],"url":"http://arxiv.org/abs/2405.19948v1","category":"cs.CR"}
{"created":"2024-05-30 11:07:06","title":"Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf","abstract":"Communication is a fundamental aspect of human society, facilitating the exchange of information and beliefs among people. Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games. As a variant of the famous communication game Werewolf, One Night Ultimate Werewolf (ONUW) requires players to develop strategic discussion policies due to the potential role changes that increase the uncertainty and complexity of the game. In this work, we first present the existence of the Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with discussion and one without. The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics. Based on the insights obtained from the analyses, we propose an RL-instructed language agent framework, where a discussion policy trained by reinforcement learning (RL) is employed to determine appropriate discussion tactics to adopt. Our experimental results on several ONUW game settings demonstrate the effectiveness and generalizability of our proposed framework.","sentences":["Communication is a fundamental aspect of human society, facilitating the exchange of information and beliefs among people.","Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games.","As a variant of the famous communication game Werewolf, One Night Ultimate Werewolf (ONUW) requires players to develop strategic discussion policies due to the potential role changes that increase the uncertainty and complexity of the game.","In this work, we first present the existence of the Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with discussion and one without.","The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics.","Based on the insights obtained from the analyses, we propose an RL-instructed language agent framework, where a discussion policy trained by reinforcement learning (RL) is employed to determine appropriate discussion tactics to adopt.","Our experimental results on several ONUW game settings demonstrate the effectiveness and generalizability of our proposed framework."],"url":"http://arxiv.org/abs/2405.19946v1","category":"cs.AI"}
{"created":"2024-05-30 11:03:27","title":"Multi-View People Detection in Large Scenes via Supervised View-Wise Contribution Weighting","abstract":"Recent deep learning-based multi-view people detection (MVD) methods have shown promising results on existing datasets. However, current methods are mainly trained and evaluated on small, single scenes with a limited number of multi-view frames and fixed camera views. As a result, these methods may not be practical for detecting people in larger, more complex scenes with severe occlusions and camera calibration errors. This paper focuses on improving multi-view people detection by developing a supervised view-wise contribution weighting approach that better fuses multi-camera information under large scenes. Besides, a large synthetic dataset is adopted to enhance the model's generalization ability and enable more practical evaluation and comparison. The model's performance on new testing scenes is further improved with a simple domain adaptation technique. Experimental results demonstrate the effectiveness of our approach in achieving promising cross-scene multi-view people detection performance. See code here: https://vcc.tech/research/2024/MVD.","sentences":["Recent deep learning-based multi-view people detection (MVD) methods have shown promising results on existing datasets.","However, current methods are mainly trained and evaluated on small, single scenes with a limited number of multi-view frames and fixed camera views.","As a result, these methods may not be practical for detecting people in larger, more complex scenes with severe occlusions and camera calibration errors.","This paper focuses on improving multi-view people detection by developing a supervised view-wise contribution weighting approach that better fuses multi-camera information under large scenes.","Besides, a large synthetic dataset is adopted to enhance the model's generalization ability and enable more practical evaluation and comparison.","The model's performance on new testing scenes is further improved with a simple domain adaptation technique.","Experimental results demonstrate the effectiveness of our approach in achieving promising cross-scene multi-view people detection performance.","See code here: https://vcc.tech/research/2024/MVD."],"url":"http://arxiv.org/abs/2405.19943v1","category":"cs.CV"}
{"created":"2024-05-30 11:02:08","title":"Synthetic Patients: Simulating Difficult Conversations with Multimodal Generative AI for Medical Education","abstract":"Problem: Effective patient-centered communication is a core competency for physicians. However, both seasoned providers and medical trainees report decreased confidence in leading conversations on sensitive topics such as goals of care or end-of-life discussions. The significant administrative burden and the resources required to provide dedicated training in leading difficult conversations has been a long-standing problem in medical education.   Approach: In this work, we present a novel educational tool designed to facilitate interactive, real-time simulations of difficult conversations in a video-based format through the use of multimodal generative artificial intelligence (AI). Leveraging recent advances in language modeling, computer vision, and generative audio, this tool creates realistic, interactive scenarios with avatars, or \"synthetic patients.\" These synthetic patients interact with users throughout various stages of medical care using a custom-built video chat application, offering learners the chance to practice conversations with patients from diverse belief systems, personalities, and ethnic backgrounds.   Outcomes: While the development of this platform demanded substantial upfront investment in labor, it offers a highly-realistic simulation experience with minimal financial investment. For medical trainees, this educational tool can be implemented within programs to simulate patient-provider conversations and can be incorporated into existing palliative care curriculum to provide a scalable, high-fidelity simulation environment for mastering difficult conversations.   Next Steps: Future developments will explore enhancing the authenticity of these encounters by working with patients to incorporate their histories and personalities, as well as employing the use of AI-generated evaluations to offer immediate, constructive feedback to learners post-simulation.","sentences":["Problem: Effective patient-centered communication is a core competency for physicians.","However, both seasoned providers and medical trainees report decreased confidence in leading conversations on sensitive topics such as goals of care or end-of-life discussions.","The significant administrative burden and the resources required to provide dedicated training in leading difficult conversations has been a long-standing problem in medical education.   ","Approach:","In this work, we present a novel educational tool designed to facilitate interactive, real-time simulations of difficult conversations in a video-based format through the use of multimodal generative artificial intelligence (AI).","Leveraging recent advances in language modeling, computer vision, and generative audio, this tool creates realistic, interactive scenarios with avatars, or \"synthetic patients.\"","These synthetic patients interact with users throughout various stages of medical care using a custom-built video chat application, offering learners the chance to practice conversations with patients from diverse belief systems, personalities, and ethnic backgrounds.   ","Outcomes: While the development of this platform demanded substantial upfront investment in labor, it offers a highly-realistic simulation experience with minimal financial investment.","For medical trainees, this educational tool can be implemented within programs to simulate patient-provider conversations and can be incorporated into existing palliative care curriculum to provide a scalable, high-fidelity simulation environment for mastering difficult conversations.   ","Next Steps:","Future developments will explore enhancing the authenticity of these encounters by working with patients to incorporate their histories and personalities, as well as employing the use of AI-generated evaluations to offer immediate, constructive feedback to learners post-simulation."],"url":"http://arxiv.org/abs/2405.19941v1","category":"cs.HC"}
{"created":"2024-05-30 10:58:04","title":"High-accuracy Nuclear Spin Dependent Parity Violating Amplitudes in $^{133}$Cs","abstract":"Relativistic coupled-cluster (RCC) theory at the singles and doubles approximation has been implemented to estimate nuclear spin dependent (NSD) parity violating (PV) electric dipole (E1) transition amplitudes ($E1_{PV}^{NSD}$) among hyperfine levels of the $6s ~^2S_{1/2} \\rightarrow 7s ~^2S_{1/2}$ transition in $^{133}$Cs. To validate our calculations, we reproduce the Dirac-Hartree-Fock values and results from the combined coupled-Dirac-Hartree-Fock and random phase approximation (CPDF-RPA) method reported earlier. Contributions from the double-core-polarization (DCP) effects at the CPDF-RPA method were found to be between 3-12\\% among different hyperfine levels. We derived a generalized expression for $E1_{PV}^{NSD}$, which helped incorporate both the NSD PV Hamiltonian and E1 operator simultaneously in the perturbative approach to account for the DCP contributions. The RCC method subsumes the CPDF-RPA and DCP effects in addition to contributions from the Br\\\"uckner pair-correlations and normalization of the wave functions, and correlations among them. To improve accuracy of the $E1_{PV}^{NSD}$ amplitudes further, we replace the {\\it ab initio} values of the E1 matrix elements and energies by their experimental values via a sum-over-states approach.","sentences":["Relativistic coupled-cluster (RCC) theory at the singles and doubles approximation has been implemented to estimate nuclear spin dependent (NSD) parity violating (PV) electric dipole (E1) transition amplitudes ($E1_{PV}^{NSD}$) among hyperfine levels of the $6s ~^2S_{1/2} \\rightarrow 7s ~^2S_{1/2}$ transition in $^{133}$Cs.","To validate our calculations, we reproduce the Dirac-Hartree-Fock values and results from the combined coupled-Dirac-Hartree-Fock and random phase approximation (CPDF-RPA) method reported earlier.","Contributions from the double-core-polarization (DCP) effects at the CPDF-RPA method were found to be between 3-12\\% among different hyperfine levels.","We derived a generalized expression for $E1_{PV}^{NSD}$, which helped incorporate both the NSD PV Hamiltonian and E1 operator simultaneously in the perturbative approach to account for the DCP contributions.","The RCC method subsumes the CPDF-RPA and DCP effects in addition to contributions from the Br\\\"uckner pair-correlations and normalization of the wave functions, and correlations among them.","To improve accuracy of the $E1_{PV}^{NSD}$ amplitudes further, we replace the {\\it ab initio} values of the E1 matrix elements and energies by their experimental values via a sum-over-states approach."],"url":"http://arxiv.org/abs/2405.19937v1","category":"physics.atom-ph"}
{"created":"2024-05-30 10:49:22","title":"Learning Latent Graph Structures and their Uncertainty","abstract":"Within a prediction task, Graph Neural Networks (GNNs) use relational information as an inductive bias to enhance the model's accuracy. As task-relevant relations might be unknown, graph structure learning approaches have been proposed to learn them while solving the downstream prediction task. In this paper, we demonstrate that minimization of a point-prediction loss function, e.g., the mean absolute error, does not guarantee proper learning of the latent relational information and its associated uncertainty. Conversely, we prove that a suitable loss function on the stochastic model outputs simultaneously grants (i) the unknown adjacency matrix latent distribution and (ii) optimal performance on the prediction task. Finally, we propose a sampling-based method that solves this joint learning task. Empirical results validate our theoretical claims and demonstrate the effectiveness of the proposed approach.","sentences":["Within a prediction task, Graph Neural Networks (GNNs) use relational information as an inductive bias to enhance the model's accuracy.","As task-relevant relations might be unknown, graph structure learning approaches have been proposed to learn them while solving the downstream prediction task.","In this paper, we demonstrate that minimization of a point-prediction loss function, e.g., the mean absolute error, does not guarantee proper learning of the latent relational information and its associated uncertainty.","Conversely, we prove that a suitable loss function on the stochastic model outputs simultaneously grants (i) the unknown adjacency matrix latent distribution and (ii) optimal performance on the prediction task.","Finally, we propose a sampling-based method that solves this joint learning task.","Empirical results validate our theoretical claims and demonstrate the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2405.19933v1","category":"cs.LG"}
{"created":"2024-05-30 10:48:35","title":"A weighted Murnaghan-Nakayama rule for $(P, w)$-partitions","abstract":"The $(P, w)$-partition generating function $K_{(P,w)}(x)$ is a quasisymmetric function obtained from a labeled poset. Recently, Liu and Weselcouch gave a formula for the coefficients of $K_{(P,w)}(x)$ when expanded in the quasisymmetric power sum function basis. This formula generalizes the classical Murnaghan--Nakayama rule for Schur functions.   We extend this result to weighted $(P, w)$-partitions and provide a short combinatorial proof, avoiding the Hopf algebra machinery used by Liu-Weselcouch.","sentences":["The $(P, w)$-partition generating function $K_{(P,w)}(x)$ is a quasisymmetric function obtained from a labeled poset.","Recently, Liu and Weselcouch gave a formula for the coefficients of $K_{(P,w)}(x)$ when expanded in the quasisymmetric power sum function basis.","This formula generalizes the classical Murnaghan--Nakayama rule for Schur functions.   ","We extend this result to weighted $(P, w)$-partitions and provide a short combinatorial proof, avoiding the Hopf algebra machinery used by Liu-Weselcouch."],"url":"http://arxiv.org/abs/2405.19932v1","category":"math.CO"}
{"created":"2024-05-30 10:47:48","title":"Exploring Diffusion Models' Corruption Stage in Few-Shot Fine-tuning and Mitigating with Bayesian Neural Networks","abstract":"Few-shot fine-tuning of Diffusion Models (DMs) is a key advancement, significantly reducing training costs and enabling personalized AI applications. However, we explore the training dynamics of DMs and observe an unanticipated phenomenon: during the training process, image fidelity initially improves, then unexpectedly deteriorates with the emergence of noisy patterns, only to recover later with severe overfitting. We term the stage with generated noisy patterns as corruption stage. To understand this corruption stage, we begin by theoretically modeling the one-shot fine-tuning scenario, and then extend this modeling to more general cases. Through this modeling, we identify the primary cause of this corruption stage: a narrowed learning distribution inherent in the nature of few-shot fine-tuning. To tackle this, we apply Bayesian Neural Networks (BNNs) on DMs with variational inference to implicitly broaden the learned distribution, and present that the learning target of the BNNs can be naturally regarded as an expectation of the diffusion loss and a further regularization with the pretrained DMs. This approach is highly compatible with current few-shot fine-tuning methods in DMs and does not introduce any extra inference costs. Experimental results demonstrate that our method significantly mitigates corruption, and improves the fidelity, quality and diversity of the generated images in both object-driven and subject-driven generation tasks.","sentences":["Few-shot fine-tuning of Diffusion Models (DMs) is a key advancement, significantly reducing training costs and enabling personalized AI applications.","However, we explore the training dynamics of DMs and observe an unanticipated phenomenon: during the training process, image fidelity initially improves, then unexpectedly deteriorates with the emergence of noisy patterns, only to recover later with severe overfitting.","We term the stage with generated noisy patterns as corruption stage.","To understand this corruption stage, we begin by theoretically modeling the one-shot fine-tuning scenario, and then extend this modeling to more general cases.","Through this modeling, we identify the primary cause of this corruption stage: a narrowed learning distribution inherent in the nature of few-shot fine-tuning.","To tackle this, we apply Bayesian Neural Networks (BNNs) on DMs with variational inference to implicitly broaden the learned distribution, and present that the learning target of the BNNs can be naturally regarded as an expectation of the diffusion loss and a further regularization with the pretrained DMs.","This approach is highly compatible with current few-shot fine-tuning methods in DMs and does not introduce any extra inference costs.","Experimental results demonstrate that our method significantly mitigates corruption, and improves the fidelity, quality and diversity of the generated images in both object-driven and subject-driven generation tasks."],"url":"http://arxiv.org/abs/2405.19931v1","category":"cs.CV"}
{"created":"2024-05-30 17:59:50","title":"MotionLLM: Understanding Human Behaviors from Human Motions and Videos","abstract":"This study delves into the realm of multi-modality (i.e., video and motion modalities) human behavior understanding by leveraging the powerful capabilities of Large Language Models (LLMs). Diverging from recent LLMs designed for video-only or motion-only understanding, we argue that understanding human behavior necessitates joint modeling from both videos and motion sequences (e.g., SMPL sequences) to capture nuanced body part dynamics and semantics effectively. In light of this, we present MotionLLM, a straightforward yet effective framework for human motion understanding, captioning, and reasoning. Specifically, MotionLLM adopts a unified video-motion training strategy that leverages the complementary advantages of existing coarse video-text data and fine-grained motion-text data to glean rich spatial-temporal insights. Furthermore, we collect a substantial dataset, MoVid, comprising diverse videos, motions, captions, and instructions. Additionally, we propose the MoVid-Bench, with carefully manual annotations, for better evaluation of human behavior understanding on video and motion. Extensive experiments show the superiority of MotionLLM in the caption, spatial-temporal comprehension, and reasoning ability.","sentences":["This study delves into the realm of multi-modality (i.e., video and motion modalities) human behavior understanding by leveraging the powerful capabilities of Large Language Models (LLMs).","Diverging from recent LLMs designed for video-only or motion-only understanding, we argue that understanding human behavior necessitates joint modeling from both videos and motion sequences (e.g., SMPL sequences) to capture nuanced body part dynamics and semantics effectively.","In light of this, we present MotionLLM, a straightforward yet effective framework for human motion understanding, captioning, and reasoning.","Specifically, MotionLLM adopts a unified video-motion training strategy that leverages the complementary advantages of existing coarse video-text data and fine-grained motion-text data to glean rich spatial-temporal insights.","Furthermore, we collect a substantial dataset, MoVid, comprising diverse videos, motions, captions, and instructions.","Additionally, we propose the MoVid-Bench, with carefully manual annotations, for better evaluation of human behavior understanding on video and motion.","Extensive experiments show the superiority of MotionLLM in the caption, spatial-temporal comprehension, and reasoning ability."],"url":"http://arxiv.org/abs/2405.20340v1","category":"cs.CV"}
{"created":"2024-05-30 17:19:19","title":"Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions","abstract":"As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation method that can provide robust evaluation results in a timely fashion. Currently, as static benchmarks are prone to contamination concerns, users tend to trust human voting platforms, such as Chatbot Arena. However, human annotations require extensive manual efforts. To provide an automatic, robust, and trustworthy evaluation framework, we innovatively propose the Auto-Arena of LLMs, which automates the entire evaluation process with LLM agents. Firstly, an examiner LLM devises queries. Then, a pair of candidate LLMs engage in a multi-round peer-battle around the query, during which the LLM's true performance gaps become visible. Finally, a committee of LLM judges collectively discuss and determine the winner, which alleviates bias and promotes fairness. In our extensive experiment on the 17 newest LLMs, Auto-Arena shows the highest correlation with human preferences, providing a promising alternative to human evaluation platforms.","sentences":["As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation method that can provide robust evaluation results in a timely fashion.","Currently, as static benchmarks are prone to contamination concerns, users tend to trust human voting platforms, such as Chatbot Arena.","However, human annotations require extensive manual efforts.","To provide an automatic, robust, and trustworthy evaluation framework, we innovatively propose the Auto-Arena of LLMs, which automates the entire evaluation process with LLM agents.","Firstly, an examiner LLM devises queries.","Then, a pair of candidate LLMs engage in a multi-round peer-battle around the query, during which the LLM's true performance gaps become visible.","Finally, a committee of LLM judges collectively discuss and determine the winner, which alleviates bias and promotes fairness.","In our extensive experiment on the 17 newest LLMs, Auto-Arena shows the highest correlation with human preferences, providing a promising alternative to human evaluation platforms."],"url":"http://arxiv.org/abs/2405.20267v1","category":"cs.CL"}
{"created":"2024-05-30 17:07:26","title":"Beyond spin-charge separation: Helical modes and topological quantum phase transitions in one-dimensional Fermi gases with spin-orbit and Rabi couplings","abstract":"Motivated by the experimental observation of spin-charge separation in one-dimensional interacting Fermi gases, we investigate these systems in the presence of spin-orbit coupling and Rabi fields. We demonstrate that spin-charge-separated modes evolve into helical collective modes due to the special mixing of spin and charge induced by spin-orbit coupling and Rabi fields. We obtain the phase diagram of chemical potential versus Rabi fields for given spin-orbit coupling and interactions, and find several topological quantum phase transitions of the Lifshitz type. We show that the velocities of the collective modes are nonanalytic at the boundaries between different phases. Lastly, we analyze the charge-charge, spin-charge and spin-spin dynamical structure factors to show that the dispersions, spectral weights and helicities of the collective modes can be experimentally extracted in systems such as $^{6}{\\rm Li}$, $^{40}{\\rm K}$ and $^{173}{\\rm Yb}$.","sentences":["Motivated by the experimental observation of spin-charge separation in one-dimensional interacting Fermi gases, we investigate these systems in the presence of spin-orbit coupling and Rabi fields.","We demonstrate that spin-charge-separated modes evolve into helical collective modes due to the special mixing of spin and charge induced by spin-orbit coupling and Rabi fields.","We obtain the phase diagram of chemical potential versus Rabi fields for given spin-orbit coupling and interactions, and find several topological quantum phase transitions of the Lifshitz type.","We show that the velocities of the collective modes are nonanalytic at the boundaries between different phases.","Lastly, we analyze the charge-charge, spin-charge and spin-spin dynamical structure factors to show that the dispersions, spectral weights and helicities of the collective modes can be experimentally extracted in systems such as $^{6}{\\rm Li}$, $^{40}{\\rm K}$ and $^{173}{\\rm Yb}$."],"url":"http://arxiv.org/abs/2405.20255v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-30 17:01:12","title":"Collective Coordinate Models for 2-Vortex Shape Mode Dynamics","abstract":"Models are developed for the motion of charge-2 Abelian Higgs vortices through the 2-vortex moduli space $M$, with the vortices excited by their shape mode oscillations. The models simplify to the well-known geodesic flow on $M$, modified by a potential, when the mode oscillations are fast relative to the moduli space motion and their amplitudes are small. When the lowest-frequency mode is excited with a large amplitude, the geodesic flow is not a correct description. Instead, a chaotic, or even fractal, multi-bounce structure in vortex-vortex collisions is predicted.","sentences":["Models are developed for the motion of charge-2 Abelian Higgs vortices through the 2-vortex moduli space $M$, with the vortices excited by their shape mode oscillations.","The models simplify to the well-known geodesic flow on $M$, modified by a potential, when the mode oscillations are fast relative to the moduli space motion and their amplitudes are small.","When the lowest-frequency mode is excited with a large amplitude, the geodesic flow is not a correct description.","Instead, a chaotic, or even fractal, multi-bounce structure in vortex-vortex collisions is predicted."],"url":"http://arxiv.org/abs/2405.20249v1","category":"hep-th"}
{"created":"2024-05-30 16:00:59","title":"A collection of cancellative, right LCM, not group-embeddable monoids","abstract":"By classical results of Malcev, cancellative monoids need not be group-embeddable. In this paper, we describe and give presentations for and study an infinite family $\\mathcal{M}_n$ of cancellative monoids which are not group-embeddable, originating from Malcev's original work. We show that $\\mathcal{M}_n$ is right LCM for $n \\geq 2$, owing to applications in the study of $\\mathrm{C}^*$-algebras by Brix, Bruce and Dor-On. We finish by showing that $\\mathcal{M}_1$ is not right LCM, but is $2$-aligned.","sentences":["By classical results of Malcev, cancellative monoids need not be group-embeddable.","In this paper, we describe and give presentations for and study an infinite family $\\mathcal{M}_n$ of cancellative monoids which are not group-embeddable, originating from Malcev's original work.","We show that $\\mathcal{M}_n$ is right LCM for $n \\geq 2$, owing to applications in the study of $\\mathrm{C}^*$-algebras by Brix, Bruce and Dor-On.","We finish by showing that $\\mathcal{M}_1$ is not right LCM, but is $2$-aligned."],"url":"http://arxiv.org/abs/2405.20197v1","category":"math.RA"}
{"created":"2024-05-30 15:24:56","title":"Accounting for Mismatch Error in Small Area Estimation with Linked Data","abstract":"In small area estimation different data sources are integrated in order to produce reliable estimates of target parameters (e.g., a mean or a proportion) for a collection of small subsets (areas) of a finite population. Regression models such as the linear mixed effects model or M-quantile regression are often used to improve the precision of survey sample estimates by leveraging auxiliary information for which means or totals are known at the area level. In many applications, the unit-level linkage of records from different sources is probabilistic and potentially error-prone. In this paper, we present adjustments of the small area predictors that are based on either the linear mixed effects model or M-quantile regression to account for the presence of linkage error. These adjustments are developed from a two-component mixture model that hinges on the assumption of independence of the target and auxiliary variable given incorrect linkage. Estimation and inference is based on composite likelihoods and machinery revolving around the Expectation-Maximization Algorithm. For each of the two regression methods, we propose modified small area predictors and approximations for their mean squared errors. The empirical performance of the proposed approaches is studied in both design-based and model-based simulations that include comparisons to a variety of baselines.","sentences":["In small area estimation different data sources are integrated in order to produce reliable estimates of target parameters (e.g., a mean or a proportion) for a collection of small subsets (areas) of a finite population.","Regression models such as the linear mixed effects model or M-quantile regression are often used to improve the precision of survey sample estimates by leveraging auxiliary information for which means or totals are known at the area level.","In many applications, the unit-level linkage of records from different sources is probabilistic and potentially error-prone.","In this paper, we present adjustments of the small area predictors that are based on either the linear mixed effects model or M-quantile regression to account for the presence of linkage error.","These adjustments are developed from a two-component mixture model that hinges on the assumption of independence of the target and auxiliary variable given incorrect linkage.","Estimation and inference is based on composite likelihoods and machinery revolving around the Expectation-Maximization Algorithm.","For each of the two regression methods, we propose modified small area predictors and approximations for their mean squared errors.","The empirical performance of the proposed approaches is studied in both design-based and model-based simulations that include comparisons to a variety of baselines."],"url":"http://arxiv.org/abs/2405.20149v1","category":"stat.ME"}
{"created":"2024-05-30 14:55:08","title":"Assistance-Seeking in Human-Supervised Autonomy: Role of Trust and Secondary Task Engagement (Extended Version)","abstract":"Using a dual-task paradigm, we explore how robot actions, performance, and the introduction of a secondary task influence human trust and engagement. In our study, a human supervisor simultaneously engages in a target-tracking task while supervising a mobile manipulator performing an object collection task. The robot can either autonomously collect the object or ask for human assistance. The human supervisor also has the choice to rely upon or interrupt the robot. Using data from initial experiments, we model the dynamics of human trust and engagement using a linear dynamical system (LDS). Furthermore, we develop a human action model to define the probability of human reliance on the robot. Our model suggests that participants are more likely to interrupt the robot when their trust and engagement are low during high-complexity collection tasks. Using Model Predictive Control (MPC), we design an optimal assistance-seeking policy. Evaluation experiments demonstrate the superior performance of the MPC policy over the baseline policy for most participants.","sentences":["Using a dual-task paradigm, we explore how robot actions, performance, and the introduction of a secondary task influence human trust and engagement.","In our study, a human supervisor simultaneously engages in a target-tracking task while supervising a mobile manipulator performing an object collection task.","The robot can either autonomously collect the object or ask for human assistance.","The human supervisor also has the choice to rely upon or interrupt the robot.","Using data from initial experiments, we model the dynamics of human trust and engagement using a linear dynamical system (LDS).","Furthermore, we develop a human action model to define the probability of human reliance on the robot.","Our model suggests that participants are more likely to interrupt the robot when their trust and engagement are low during high-complexity collection tasks.","Using Model Predictive Control (MPC), we design an optimal assistance-seeking policy.","Evaluation experiments demonstrate the superior performance of the MPC policy over the baseline policy for most participants."],"url":"http://arxiv.org/abs/2405.20118v1","category":"cs.RO"}
{"created":"2024-05-30 14:27:40","title":"Visual Attention Analysis in Online Learning","abstract":"In this paper, we present an approach in the Multimodal Learning Analytics field. Within this approach, we have developed a tool to visualize and analyze eye movement data collected during learning sessions in online courses. The tool is named VAAD (an acronym for Visual Attention Analysis Dashboard). These eye movement data have been gathered using an eye-tracker and subsequently processed and visualized for interpretation. The purpose of the tool is to conduct a descriptive analysis of the data by facilitating its visualization, enabling the identification of differences and learning patterns among various learner populations. Additionally, it integrates a predictive module capable of anticipating learner activities during a learning session. Consequently, VAAD holds the potential to offer valuable insights into online learning behaviors from both descriptive and predictive perspectives.","sentences":["In this paper, we present an approach in the Multimodal Learning Analytics field.","Within this approach, we have developed a tool to visualize and analyze eye movement data collected during learning sessions in online courses.","The tool is named VAAD (an acronym for Visual Attention Analysis Dashboard).","These eye movement data have been gathered using an eye-tracker and subsequently processed and visualized for interpretation.","The purpose of the tool is to conduct a descriptive analysis of the data by facilitating its visualization, enabling the identification of differences and learning patterns among various learner populations.","Additionally, it integrates a predictive module capable of anticipating learner activities during a learning session.","Consequently, VAAD holds the potential to offer valuable insights into online learning behaviors from both descriptive and predictive perspectives."],"url":"http://arxiv.org/abs/2405.20091v1","category":"cs.CV"}
{"created":"2024-05-30 14:02:40","title":"Faces of the Mind: Unveiling Mental Health States Through Facial Expressions in 11,427 Adolescents","abstract":"Mood disorders, including depression and anxiety, often manifest through facial expressions. While previous research has explored the connection between facial features and emotions, machine learning algorithms for estimating mood disorder severity have been hindered by small datasets and limited real-world application. To address this gap, we analyzed facial videos of 11,427 participants, a dataset two orders of magnitude larger than previous studies. This comprehensive collection includes standardized facial expression videos from reading tasks, along with a detailed psychological scale that measures depression, anxiety, and stress. By examining the relationships among these emotional states and employing clustering analysis, we identified distinct subgroups embodying different emotional profiles. We then trained tree-based classifiers and deep learning models to estimate emotional states from facial features. Results indicate that models previously effective on small datasets experienced decreased performance when applied to our large dataset, highlighting the importance of data scale and mitigating overfitting in practical settings. Notably, our study identified subtle shifts in pupil dynamics and gaze orientation as potential markers of mood disorders, providing valuable information on the interaction between facial expressions and mental health. This research marks the first large-scale and comprehensive investigation of facial expressions in the context of mental health, laying the groundwork for future data-driven advancements in this field.","sentences":["Mood disorders, including depression and anxiety, often manifest through facial expressions.","While previous research has explored the connection between facial features and emotions, machine learning algorithms for estimating mood disorder severity have been hindered by small datasets and limited real-world application.","To address this gap, we analyzed facial videos of 11,427 participants, a dataset two orders of magnitude larger than previous studies.","This comprehensive collection includes standardized facial expression videos from reading tasks, along with a detailed psychological scale that measures depression, anxiety, and stress.","By examining the relationships among these emotional states and employing clustering analysis, we identified distinct subgroups embodying different emotional profiles.","We then trained tree-based classifiers and deep learning models to estimate emotional states from facial features.","Results indicate that models previously effective on small datasets experienced decreased performance when applied to our large dataset, highlighting the importance of data scale and mitigating overfitting in practical settings.","Notably, our study identified subtle shifts in pupil dynamics and gaze orientation as potential markers of mood disorders, providing valuable information on the interaction between facial expressions and mental health.","This research marks the first large-scale and comprehensive investigation of facial expressions in the context of mental health, laying the groundwork for future data-driven advancements in this field."],"url":"http://arxiv.org/abs/2405.20072v1","category":"cs.CV"}
{"created":"2024-05-30 13:48:13","title":"Search for dark mesons decaying to top and bottom quarks in proton--proton collisions at $\\sqrt{s}=13$ TeV with the ATLAS detector","abstract":"A search for dark mesons originating from strongly-coupled, SU(2) dark flavor symmetry conserving models and decaying gaugephobically to pure Standard Model final states containing top and bottom quarks is presented. The search targets fully hadronic final states and final states with exactly one electron or muon. The analyzed dataset corresponds to an integrated luminosity of 140 $fb^{-1}$ of proton-proton collisions collected at $\\sqrt{s}=13$ TeV with the ATLAS detector at the Large Hadron Collider. No significant excess over the Standard Model background expectation is observed and the results are used to set the first direct constraints on this type of model. The two-dimensional signal space of dark pion masses $m_{\\pi_D}$ and dark rho-meson masses $m_{\\rho_D}$ is scanned. For $m_{\\pi_D}/m_{\\rho_D}=0.45$, dark pions with masses $m_{\\pi_D}<943$ GeV are excluded, while for $m_{\\pi_D}/m_{\\rho_D}=0.45$ masses of $m_{\\pi_D}<738$ GeV are excluded.","sentences":["A search for dark mesons originating from strongly-coupled, SU(2) dark flavor symmetry conserving models and decaying gaugephobically to pure Standard Model final states containing top and bottom quarks is presented.","The search targets fully hadronic final states and final states with exactly one electron or muon.","The analyzed dataset corresponds to an integrated luminosity of 140 $fb^{-1}$ of proton-proton collisions collected at $\\sqrt{s}=13$ TeV with the ATLAS detector at the Large Hadron Collider.","No significant excess over the Standard Model background expectation is observed and the results are used to set the first direct constraints on this type of model.","The two-dimensional signal space of dark pion masses $m_{\\pi_D}$ and dark rho-meson masses $m_{\\rho_D}$ is scanned.","For $m_{\\pi_D}/m_{\\rho_D}=0.45$, dark pions with masses $m_{\\pi_D}<943$ GeV are excluded, while for $m_{\\pi_D}/m_{\\rho_D}=0.45$ masses of $m_{\\pi_D}<738$ GeV are excluded."],"url":"http://arxiv.org/abs/2405.20061v1","category":"hep-ex"}
{"created":"2024-05-30 13:30:32","title":"Mass Cycle and Dynamics of a Virtual Quiescent Prominence","abstract":"The mass cycle of solar prominences or filaments is still not completely understood. Researchers agree that these dense structures form by coronal in-situ condensations and plasma siphoning from the underlying chromosphere. In the evaporation-condensation model siphoning arises due to evaporation of chromospheric plasma from localised footpoint heating but this is challenging to justify observationally. Here, we simulate the reconnection-condensation model at extreme-resolutions down to 20.8 km within a three-dimensional magnetohydrodynamic coronal volume. We form a draining, quiescent prominence and associated coronal rain simultaneously. We show that thermal instability --acting as a trigger for local condensation formation-- by itself drives siphoning flows from the low-corona without the need of any localised heating. In addition, for the first time we demonstrate through a statistical analysis along more than 1000 magnetic field lines that cold condensations give rise to siphoning flows within magnetic threads. This siphoning arises from the strong pressure gradient along field lines induced by thermal instability. No correlation is found between siphoning flows and the prominence mass, making thermal instability the main in-situ mass collection mechanism. Our simulated prominence drains by gliding along strongly sheared, asymmetric, dipped magnetic arcades, and develops natural vertical fine-structure in an otherwise horizontal magnetic field due to the magnetic Rayleigh-Taylor instability. By synthesising our data, our model shows remarkable agreement with observations of quiescent prominences such as its dark coronal cavity in extreme-ultraviolet emission channels, fine-scale vertical structure and reconnection outflows which, for the first time, have been self-consistently obtained as the prominence evolves.","sentences":["The mass cycle of solar prominences or filaments is still not completely understood.","Researchers agree that these dense structures form by coronal in-situ condensations and plasma siphoning from the underlying chromosphere.","In the evaporation-condensation model siphoning arises due to evaporation of chromospheric plasma from localised footpoint heating but this is challenging to justify observationally.","Here, we simulate the reconnection-condensation model at extreme-resolutions down to 20.8 km within a three-dimensional magnetohydrodynamic coronal volume.","We form a draining, quiescent prominence and associated coronal rain simultaneously.","We show that thermal instability --acting as a trigger for local condensation formation-- by itself drives siphoning flows from the low-corona without the need of any localised heating.","In addition, for the first time we demonstrate through a statistical analysis along more than 1000 magnetic field lines that cold condensations give rise to siphoning flows within magnetic threads.","This siphoning arises from the strong pressure gradient along field lines induced by thermal instability.","No correlation is found between siphoning flows and the prominence mass, making thermal instability the main in-situ mass collection mechanism.","Our simulated prominence drains by gliding along strongly sheared, asymmetric, dipped magnetic arcades, and develops natural vertical fine-structure in an otherwise horizontal magnetic field due to the magnetic Rayleigh-Taylor instability.","By synthesising our data, our model shows remarkable agreement with observations of quiescent prominences such as its dark coronal cavity in extreme-ultraviolet emission channels, fine-scale vertical structure and reconnection outflows which, for the first time, have been self-consistently obtained as the prominence evolves."],"url":"http://arxiv.org/abs/2405.20048v1","category":"astro-ph.SR"}
{"created":"2024-05-30 10:26:36","title":"P$^2$-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer","abstract":"Vision Transformers (ViTs) have excelled in computer vision tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices. To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield non-negligible re-quantization overhead, limiting ViTs' hardware efficiency and motivating more hardware-friendly solutions. To this end, we propose \\emph{P$^2$-ViT}, the first \\underline{P}ower-of-Two (PoT) \\underline{p}ost-training quantization and acceleration framework to accelerate fully quantized ViTs. Specifically, {as for quantization,} we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the re-quantization overhead. Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency trade-offs. {In terms of hardware,} we develop {a dedicated chunk-based accelerator} featuring multiple tailored sub-processors to individually handle ViTs' different types of operations, alleviating reconfigurable overhead. Additionally, we design {a tailored row-stationary dataflow} to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput. Extensive experiments consistently validate P$^2$-ViT's effectiveness. {Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared to the counterpart with floating-point scaling factors. Besides, we achieve up to $\\mathbf{10.1\\times}$ speedup and $\\mathbf{36.8\\times}$ energy saving over GPU's Turing Tensor Cores, and up to $\\mathbf{1.84\\times}$ higher computation utilization efficiency against SOTA quantization-based ViT accelerators. Codes are available at \\url{https://github.com/shihuihong214/P2-ViT}.","sentences":["Vision Transformers (ViTs) have excelled in computer vision tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices.","To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield non-negligible re-quantization overhead, limiting ViTs' hardware efficiency and motivating more hardware-friendly solutions.","To this end, we propose \\emph{P$^2$-ViT}, the first \\underline{P}ower-of-Two (PoT) \\underline{p}ost-training quantization and acceleration framework to accelerate fully quantized ViTs.","Specifically, {as for quantization,} we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the re-quantization overhead.","Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency trade-offs.","{In terms of hardware,} we develop {a dedicated chunk-based accelerator} featuring multiple tailored sub-processors to individually handle ViTs' different types of operations, alleviating reconfigurable overhead.","Additionally, we design {a tailored row-stationary dataflow} to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput.","Extensive experiments consistently validate P$^2$-ViT's effectiveness.","{Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared to the counterpart with floating-point scaling factors.","Besides, we achieve up to $\\mathbf{10.1\\times}$ speedup and $\\mathbf{36.8\\times}$ energy saving over GPU's Turing Tensor Cores, and up to $\\mathbf{1.84\\times}$ higher computation utilization efficiency against SOTA quantization-based ViT accelerators.","Codes are available at \\url{https://github.com/shihuihong214/P2-ViT}."],"url":"http://arxiv.org/abs/2405.19915v1","category":"cs.AI"}
{"created":"2024-05-30 10:20:55","title":"Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning","abstract":"In offline reinforcement learning, the challenge of out-of-distribution (OOD) is pronounced. To address this, existing methods often constrain the learned policy through policy regularization. However, these methods often suffer from the issue of unnecessary conservativeness, hampering policy improvement. This occurs due to the indiscriminate use of all actions from the behavior policy that generates the offline dataset as constraints. The problem becomes particularly noticeable when the quality of the dataset is suboptimal. Thus, we propose Adaptive Advantage-guided Policy Regularization (A2PR), obtaining high-advantage actions from an augmented behavior policy combined with VAE to guide the learned policy. A2PR can select high-advantage actions that differ from those present in the dataset, while still effectively maintaining conservatism from OOD actions. This is achieved by harnessing the VAE capacity to generate samples matching the distribution of the data points. We theoretically prove that the improvement of the behavior policy is guaranteed. Besides, it effectively mitigates value overestimation with a bounded performance gap. Empirically, we conduct a series of experiments on the D4RL benchmark, where A2PR demonstrates state-of-the-art performance. Furthermore, experimental results on additional suboptimal mixed datasets reveal that A2PR exhibits superior performance. Code is available at https://github.com/ltlhuuu/A2PR.","sentences":["In offline reinforcement learning, the challenge of out-of-distribution (OOD) is pronounced.","To address this, existing methods often constrain the learned policy through policy regularization.","However, these methods often suffer from the issue of unnecessary conservativeness, hampering policy improvement.","This occurs due to the indiscriminate use of all actions from the behavior policy that generates the offline dataset as constraints.","The problem becomes particularly noticeable when the quality of the dataset is suboptimal.","Thus, we propose Adaptive Advantage-guided Policy Regularization (A2PR), obtaining high-advantage actions from an augmented behavior policy combined with VAE to guide the learned policy.","A2PR can select high-advantage actions that differ from those present in the dataset, while still effectively maintaining conservatism from OOD actions.","This is achieved by harnessing the VAE capacity to generate samples matching the distribution of the data points.","We theoretically prove that the improvement of the behavior policy is guaranteed.","Besides, it effectively mitigates value overestimation with a bounded performance gap.","Empirically, we conduct a series of experiments on the D4RL benchmark, where A2PR demonstrates state-of-the-art performance.","Furthermore, experimental results on additional suboptimal mixed datasets reveal that A2PR exhibits superior performance.","Code is available at https://github.com/ltlhuuu/A2PR."],"url":"http://arxiv.org/abs/2405.19909v1","category":"cs.LG"}
{"created":"2024-05-30 10:02:53","title":"Urban Air Pollution Forecasting: a Machine Learning Approach leveraging Satellite Observations and Meteorological Forecasts","abstract":"Air pollution poses a significant threat to public health and well-being, particularly in urban areas. This study introduces a series of machine-learning models that integrate data from the Sentinel-5P satellite, meteorological conditions, and topological characteristics to forecast future levels of five major pollutants. The investigation delineates the process of data collection, detailing the combination of diverse data sources utilized in the study. Through experiments conducted in the Milan metropolitan area, the models demonstrate their efficacy in predicting pollutant levels for the forthcoming day, achieving a percentage error of around 30%. The proposed models are advantageous as they are independent of monitoring stations, facilitating their use in areas without existing infrastructure. Additionally, we have released the collected dataset to the public, aiming to stimulate further research in this field. This research contributes to advancing our understanding of urban air quality dynamics and emphasizes the importance of amalgamating satellite, meteorological, and topographical data to develop robust pollution forecasting models.","sentences":["Air pollution poses a significant threat to public health and well-being, particularly in urban areas.","This study introduces a series of machine-learning models that integrate data from the Sentinel-5P satellite, meteorological conditions, and topological characteristics to forecast future levels of five major pollutants.","The investigation delineates the process of data collection, detailing the combination of diverse data sources utilized in the study.","Through experiments conducted in the Milan metropolitan area, the models demonstrate their efficacy in predicting pollutant levels for the forthcoming day, achieving a percentage error of around 30%.","The proposed models are advantageous as they are independent of monitoring stations, facilitating their use in areas without existing infrastructure.","Additionally, we have released the collected dataset to the public, aiming to stimulate further research in this field.","This research contributes to advancing our understanding of urban air quality dynamics and emphasizes the importance of amalgamating satellite, meteorological, and topographical data to develop robust pollution forecasting models."],"url":"http://arxiv.org/abs/2405.19901v1","category":"cs.LG"}
{"created":"2024-05-30 09:55:19","title":"Open-Set Domain Adaptation for Semantic Segmentation","abstract":"Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer the pixel-wise knowledge from the labeled source domain to the unlabeled target domain. However, current UDA methods typically assume a shared label space between source and target, limiting their applicability in real-world scenarios where novel categories may emerge in the target domain. In this paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) for the first time, where the target domain includes unknown classes. We identify two major problems in the OSDA-SS scenario as follows: 1) the existing UDA methods struggle to predict the exact boundary of the unknown classes, and 2) they fail to accurately predict the shape of the unknown classes. To address these issues, we propose Boundary and Unknown Shape-Aware open-set domain adaptation, coined BUS. Our BUS can accurately discern the boundaries between known and unknown classes in a contrastive manner using a novel dilation-erosion-based contrastive loss. In addition, we propose OpenReMix, a new domain mixing augmentation method that guides our model to effectively learn domain and size-invariant features for improving the shape detection of the known and unknown classes. Through extensive experiments, we demonstrate that our proposed BUS effectively detects unknown classes in the challenging OSDA-SS scenario compared to the previous methods by a large margin. The code is available at https://github.com/KHU-AGI/BUS.","sentences":["Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer the pixel-wise knowledge from the labeled source domain to the unlabeled target domain.","However, current UDA methods typically assume a shared label space between source and target, limiting their applicability in real-world scenarios where novel categories may emerge in the target domain.","In this paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) for the first time, where the target domain includes unknown classes.","We identify two major problems in the OSDA-SS scenario as follows: 1) the existing UDA methods struggle to predict the exact boundary of the unknown classes, and 2) they fail to accurately predict the shape of the unknown classes.","To address these issues, we propose Boundary and Unknown Shape-Aware open-set domain adaptation, coined BUS.","Our BUS can accurately discern the boundaries between known and unknown classes in a contrastive manner using a novel dilation-erosion-based contrastive loss.","In addition, we propose OpenReMix, a new domain mixing augmentation method that guides our model to effectively learn domain and size-invariant features for improving the shape detection of the known and unknown classes.","Through extensive experiments, we demonstrate that our proposed BUS effectively detects unknown classes in the challenging OSDA-SS scenario compared to the previous methods by a large margin.","The code is available at https://github.com/KHU-AGI/BUS."],"url":"http://arxiv.org/abs/2405.19899v1","category":"cs.CV"}
{"created":"2024-05-30 09:55:07","title":"Random attractors on countable state spaces","abstract":"We study the synchronization behavior of discrete-time Markov chains on countable state spaces. Representing a Markov chain in terms of a random dynamical system, which describes the collective dynamics of trajectories driven by the same noise, allows for the characterization of synchronization via random attractors.   We establish the existence and uniqueness of a random attractor under mild conditions and show that forward and pullback attraction are equivalent in our setting. Additionally, we provide necessary and sufficient conditions for reaching the random attractor, or synchronization respectively, in a time of finite mean.   By introducing insulated and synchronizing sets, we structure the state space with respect to the synchronization behavior and characterize the shape of the random attractor.","sentences":["We study the synchronization behavior of discrete-time Markov chains on countable state spaces.","Representing a Markov chain in terms of a random dynamical system, which describes the collective dynamics of trajectories driven by the same noise, allows for the characterization of synchronization via random attractors.   ","We establish the existence and uniqueness of a random attractor under mild conditions and show that forward and pullback attraction are equivalent in our setting.","Additionally, we provide necessary and sufficient conditions for reaching the random attractor, or synchronization respectively, in a time of finite mean.   ","By introducing insulated and synchronizing sets, we structure the state space with respect to the synchronization behavior and characterize the shape of the random attractor."],"url":"http://arxiv.org/abs/2405.19898v1","category":"math.DS"}
{"created":"2024-05-30 09:52:27","title":"Dispersion of personal spaces","abstract":"There are many entities that disseminate in the physical space - information, gossip, mood, innovation etc. Personal spaces are also entities that disperse and interplay. In this work we study the emergence of configurations formed by participants when choosing a place to sit in a rectangular auditorium. Based on experimental questionnaire data we design several models and assess their relevancy to a real time-lapse footage of lecture hall being filled up. The main focus is to compare the evolution of entropy of occupied seat configurations in time. Even though the process of choosing a seat is complex and could depend on various properties of participants or environment, some of the developed models can capture at least basic essence of the real processes. After introducing the problem of seat selection and related results in close research areas, we introduce preliminary collected data and build models of seat selection based on them. We compare the resulting models to the real observational data and discuss areas of future research directions.","sentences":["There are many entities that disseminate in the physical space - information, gossip, mood, innovation etc.","Personal spaces are also entities that disperse and interplay.","In this work we study the emergence of configurations formed by participants when choosing a place to sit in a rectangular auditorium.","Based on experimental questionnaire data we design several models and assess their relevancy to a real time-lapse footage of lecture hall being filled up.","The main focus is to compare the evolution of entropy of occupied seat configurations in time.","Even though the process of choosing a seat is complex and could depend on various properties of participants or environment, some of the developed models can capture at least basic essence of the real processes.","After introducing the problem of seat selection and related results in close research areas, we introduce preliminary collected data and build models of seat selection based on them.","We compare the resulting models to the real observational data and discuss areas of future research directions."],"url":"http://arxiv.org/abs/2405.19895v1","category":"cs.MA"}
{"created":"2024-05-30 09:48:50","title":"Hyperon-Nucleus/Nucleon Scattering at BESIII","abstract":"Utilizing the large quantity of hyperons and antihyperons produced by the decay of 10 billion $J/\\psi$ and 2.7 billion $\\psi(3686)$ collected at BESIII, the cross sections of several specific elastic or inelastic (anti-)hyperon-nucleus/nucleon rections have been measured via the scattering between the (anti-)hyperons and the nucleus in the dense objects of BESIII detector. The novel method developed in these works extends the research field and opens a new era for the experiments at $e^+ e^-$ colliders. The results of such measurements will definitely benefit a lot the precise probe of the (anti-)hyperon-nucleus/nucleon interactions and provide constraints for the studies of the potential of strong interaction, the origin of color confinement, the unified model for baryon-baryon interactions, and the internal structure of neutron stars. The desirable prospects of corresponding studies in the future Super Tau-Charm Factory (STCF) are also discussed in this report.","sentences":["Utilizing the large quantity of hyperons and antihyperons produced by the decay of 10 billion $J/\\psi$ and 2.7 billion $\\psi(3686)$ collected at BESIII, the cross sections of several specific elastic or inelastic (anti-)hyperon-nucleus/nucleon rections have been measured via the scattering between the (anti-)hyperons and the nucleus in the dense objects of BESIII detector.","The novel method developed in these works extends the research field and opens a new era for the experiments at $e^+ e^-$ colliders.","The results of such measurements will definitely benefit a lot the precise probe of the (anti-)hyperon-nucleus/nucleon interactions and provide constraints for the studies of the potential of strong interaction, the origin of color confinement, the unified model for baryon-baryon interactions, and the internal structure of neutron stars.","The desirable prospects of corresponding studies in the future Super Tau-Charm Factory (STCF) are also discussed in this report."],"url":"http://arxiv.org/abs/2405.19892v1","category":"nucl-ex"}
{"created":"2024-05-30 09:47:06","title":"Probing strangeness hadronization with event-by-event production of multistrange hadrons","abstract":"This Letter presents the first measurement of event-by-event fluctuations of the net number (difference between the particle and antiparticle multiplicities) of multistrange hadrons $\\Xi^-$ and $\\overline{\\Xi}^+$ and its correlation with the net-kaon number using the data collected by the ALICE Collaboration in pp, p$-$Pb, and Pb$-$Pb collisions at a center-of-mass energy per nucleon pair $\\sqrt{s_{\\mathrm{NN}}}=5.02\\ \\mathrm{TeV}$. The statistical hadronization model with a correlation over three units of rapidity between hadrons having the same and opposite strangeness content successfully describes the results. On the other hand, string-fragmentation models that mainly correlate strange hadrons with opposite strange quark content over a small rapidity range fail to describe the data.","sentences":["This Letter presents the first measurement of event-by-event fluctuations of the net number (difference between the particle and antiparticle multiplicities) of multistrange hadrons $\\Xi^-$ and $\\overline{\\Xi}^+$ and its correlation with the net-kaon number using the data collected by the ALICE Collaboration in pp, p$-$Pb, and Pb$-$Pb collisions at a center-of-mass energy per nucleon pair $\\sqrt{s_{\\mathrm{NN}}}=5.02\\ \\mathrm{TeV}$.","The statistical hadronization model with a correlation over three units of rapidity between hadrons having the same and opposite strangeness content successfully describes the results.","On the other hand, string-fragmentation models that mainly correlate strange hadrons with opposite strange quark content over a small rapidity range fail to describe the data."],"url":"http://arxiv.org/abs/2405.19890v1","category":"nucl-ex"}
{"created":"2024-05-30 09:32:14","title":"KNOW: A Real-World Ontology for Knowledge Capture with Large Language Models","abstract":"We present KNOW--the Knowledge Navigator Ontology for the World--the first ontology designed to capture everyday knowledge to augment large language models (LLMs) in real-world generative AI use cases such as personal AI assistants. Our domain is human life, both its everyday concerns and its major milestones. We have limited the initial scope of the modeled concepts to only established human universals: spacetime (places, events) plus social (people, groups, organizations). The inclusion criteria for modeled concepts are pragmatic, beginning with universality and utility. We compare and contrast previous work such as Schema.org and Cyc--as well as attempts at a synthesis of knowledge graphs and language models--noting how LLMs already encode internally much of the commonsense tacit knowledge that took decades to capture in the Cyc project. We also make available code-generated software libraries for the 12 most popular programming languages, enabling the direct use of ontology concepts in software engineering. We emphasize simplicity and developer experience in promoting AI interoperability.","sentences":["We present KNOW--the Knowledge Navigator Ontology for the World--the first ontology designed to capture everyday knowledge to augment large language models (LLMs) in real-world generative AI use cases such as personal AI assistants.","Our domain is human life, both its everyday concerns and its major milestones.","We have limited the initial scope of the modeled concepts to only established human universals: spacetime (places, events) plus social (people, groups, organizations).","The inclusion criteria for modeled concepts are pragmatic, beginning with universality and utility.","We compare and contrast previous work such as Schema.org and Cyc--as well as attempts at a synthesis of knowledge graphs and language models--noting how LLMs already encode internally much of the commonsense tacit knowledge that took decades to capture in the Cyc project.","We also make available code-generated software libraries for the 12 most popular programming languages, enabling the direct use of ontology concepts in software engineering.","We emphasize simplicity and developer experience in promoting AI interoperability."],"url":"http://arxiv.org/abs/2405.19877v1","category":"cs.AI"}
{"created":"2024-05-30 09:28:56","title":"Is In-Context Learning Sufficient for Instruction Following in LLMs?","abstract":"In-context learning (ICL) allows LLMs to learn from examples without changing their weights, which is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on established benchmarks such as MT-Bench and AlpacaEval 2.0 (LC), especially with more capable base LMs. Unlike for tasks such as classification, translation, or summarization, adding more ICL demonstrations for long-context LLMs does not systematically improve instruction following performance. To address this limitation, we derive a greedy selection approach for ICL examples that noticeably improves performance, yet without bridging the gap to instruction fine-tuning. Finally, we provide a series of ablation studies to better understand the reasons behind the remaining gap, and we show how some aspects of ICL depart from the existing knowledge and are specific to the instruction tuning setting. Overall, our work advances the understanding of ICL as an alignment technique. We provide our code at https://github.com/tml-epfl/icl-alignment.","sentences":["In-context learning (ICL) allows LLMs to learn from examples without changing their weights, which is a particularly promising capability for long-context LLMs that can potentially learn from many examples.","Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance.","In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on established benchmarks such as MT-Bench and AlpacaEval 2.0 (LC), especially with more capable base LMs.","Unlike for tasks such as classification, translation, or summarization, adding more ICL demonstrations for long-context LLMs does not systematically improve instruction following performance.","To address this limitation, we derive a greedy selection approach for ICL examples that noticeably improves performance, yet without bridging the gap to instruction fine-tuning.","Finally, we provide a series of ablation studies to better understand the reasons behind the remaining gap, and we show how some aspects of ICL depart from the existing knowledge and are specific to the instruction tuning setting.","Overall, our work advances the understanding of ICL as an alignment technique.","We provide our code at https://github.com/tml-epfl/icl-alignment."],"url":"http://arxiv.org/abs/2405.19874v1","category":"cs.CL"}
{"created":"2024-05-30 09:14:01","title":"Out-of-distribution Reject Option Method for Dataset Shift Problem in Early Disease Onset Prediction","abstract":"Machine learning is increasingly used to predict lifestyle-related disease onset using health and medical data. However, the prediction effectiveness is hindered by dataset shift, which involves discrepancies in data distribution between the training and testing datasets, misclassifying out-of-distribution (OOD) data. To diminish dataset shift effects, this paper proposes the out-of-distribution reject option for prediction (ODROP), which integrates OOD detection models to preclude OOD data from the prediction phase. We investigated the efficacy of five OOD detection methods (variational autoencoder, neural network ensemble std, neural network ensemble epistemic, neural network energy, and neural network gaussian mixture based energy measurement) across two datasets, the Hirosaki and Wakayama health checkup data, in the context of three disease onset prediction tasks: diabetes, dyslipidemia, and hypertension. To evaluate the ODROP method, we trained disease onset prediction models and OOD detection models on Hirosaki data and used AUROC-rejection curve plots from Wakayama data. The variational autoencoder method showed superior stability and magnitude of improvement in Area Under the Receiver Operating Curve (AUROC) in five cases: AUROC in the Wakayama data was improved from 0.80 to 0.90 at a 31.1% rejection rate for diabetes onset and from 0.70 to 0.76 at a 34% rejection rate for dyslipidemia. We categorized dataset shifts into two types using SHAP clustering - those that considerably affect predictions and those that do not. We expect that this classification will help standardize measuring instruments. This study is the first to apply OOD detection to actual health and medical data, demonstrating its potential to substantially improve the accuracy and reliability of disease prediction models amidst dataset shift.","sentences":["Machine learning is increasingly used to predict lifestyle-related disease onset using health and medical data.","However, the prediction effectiveness is hindered by dataset shift, which involves discrepancies in data distribution between the training and testing datasets, misclassifying out-of-distribution (OOD) data.","To diminish dataset shift effects, this paper proposes the out-of-distribution reject option for prediction (ODROP), which integrates OOD detection models to preclude OOD data from the prediction phase.","We investigated the efficacy of five OOD detection methods (variational autoencoder, neural network ensemble std, neural network ensemble epistemic, neural network energy, and neural network gaussian mixture based energy measurement) across two datasets, the Hirosaki and Wakayama health checkup data, in the context of three disease onset prediction tasks: diabetes, dyslipidemia, and hypertension.","To evaluate the ODROP method, we trained disease onset prediction models and OOD detection models on Hirosaki data and used AUROC-rejection curve plots from Wakayama data.","The variational autoencoder method showed superior stability and magnitude of improvement in Area Under the Receiver Operating Curve (AUROC) in five cases: AUROC in the Wakayama data was improved from 0.80 to 0.90 at a 31.1% rejection rate for diabetes onset and from 0.70 to 0.76 at a 34% rejection rate for dyslipidemia.","We categorized dataset shifts into two types using SHAP clustering - those that considerably affect predictions and those that do not.","We expect that this classification will help standardize measuring instruments.","This study is the first to apply OOD detection to actual health and medical data, demonstrating its potential to substantially improve the accuracy and reliability of disease prediction models amidst dataset shift."],"url":"http://arxiv.org/abs/2405.19864v1","category":"cs.LG"}
{"created":"2024-05-30 09:10:33","title":"Hierarchical Object-Centric Learning with Capsule Networks","abstract":"Capsule networks (CapsNets) were introduced to address convolutional neural networks limitations, learning object-centric representations that are more robust, pose-aware, and interpretable. They organize neurons into groups called capsules, where each capsule encodes the instantiation parameters of an object or one of its parts. Moreover, a routing algorithm connects capsules in different layers, thereby capturing hierarchical part-whole relationships in the data.   This thesis investigates the intriguing aspects of CapsNets and focuses on three key questions to unlock their full potential. First, we explore the effectiveness of the routing algorithm, particularly in small-sized networks. We propose a novel method that anneals the number of routing iterations during training, enhancing performance in architectures with fewer parameters.   Secondly, we investigate methods to extract more effective first-layer capsules, also known as primary capsules. By exploiting pruned backbones, we aim to improve computational efficiency by reducing the number of capsules while achieving high generalization. This approach reduces CapsNets memory requirements and computational effort.   Third, we explore part-relationship learning in CapsNets. Through extensive research, we demonstrate that capsules with low entropy can extract more concise and discriminative part-whole relationships compared to traditional capsule networks, even with reasonable network sizes.   Lastly, we showcase how CapsNets can be utilized in real-world applications, including autonomous localization of unmanned aerial vehicles, quaternion-based rotations prediction in synthetic datasets, and lung nodule segmentation in biomedical imaging.   The findings presented in this thesis contribute to a deeper understanding of CapsNets and highlight their potential to address complex computer vision challenges.","sentences":["Capsule networks (CapsNets) were introduced to address convolutional neural networks limitations, learning object-centric representations that are more robust, pose-aware, and interpretable.","They organize neurons into groups called capsules, where each capsule encodes the instantiation parameters of an object or one of its parts.","Moreover, a routing algorithm connects capsules in different layers, thereby capturing hierarchical part-whole relationships in the data.   ","This thesis investigates the intriguing aspects of CapsNets and focuses on three key questions to unlock their full potential.","First, we explore the effectiveness of the routing algorithm, particularly in small-sized networks.","We propose a novel method that anneals the number of routing iterations during training, enhancing performance in architectures with fewer parameters.   ","Secondly, we investigate methods to extract more effective first-layer capsules, also known as primary capsules.","By exploiting pruned backbones, we aim to improve computational efficiency by reducing the number of capsules while achieving high generalization.","This approach reduces CapsNets memory requirements and computational effort.   ","Third, we explore part-relationship learning in CapsNets.","Through extensive research, we demonstrate that capsules with low entropy can extract more concise and discriminative part-whole relationships compared to traditional capsule networks, even with reasonable network sizes.   ","Lastly, we showcase how CapsNets can be utilized in real-world applications, including autonomous localization of unmanned aerial vehicles, quaternion-based rotations prediction in synthetic datasets, and lung nodule segmentation in biomedical imaging.   ","The findings presented in this thesis contribute to a deeper understanding of CapsNets and highlight their potential to address complex computer vision challenges."],"url":"http://arxiv.org/abs/2405.19861v1","category":"cs.CV"}
{"created":"2024-05-30 08:55:48","title":"Deciphering Human Mobility: Inferring Semantics of Trajectories with Large Language Models","abstract":"Understanding human mobility patterns is essential for various applications, from urban planning to public safety. The individual trajectory such as mobile phone location data, while rich in spatio-temporal information, often lacks semantic detail, limiting its utility for in-depth mobility analysis. Existing methods can infer basic routine activity sequences from this data, lacking depth in understanding complex human behaviors and users' characteristics. Additionally, they struggle with the dependency on hard-to-obtain auxiliary datasets like travel surveys. To address these limitations, this paper defines trajectory semantic inference through three key dimensions: user occupation category, activity sequence, and trajectory description, and proposes the Trajectory Semantic Inference with Large Language Models (TSI-LLM) framework to leverage LLMs infer trajectory semantics comprehensively and deeply. We adopt spatio-temporal attributes enhanced data formatting (STFormat) and design a context-inclusive prompt, enabling LLMs to more effectively interpret and infer the semantics of trajectory data. Experimental validation on real-world trajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex human mobility patterns. This study explores the potential of LLMs in enhancing the semantic analysis of trajectory data, paving the way for more sophisticated and accessible human mobility research.","sentences":["Understanding human mobility patterns is essential for various applications, from urban planning to public safety.","The individual trajectory such as mobile phone location data, while rich in spatio-temporal information, often lacks semantic detail, limiting its utility for in-depth mobility analysis.","Existing methods can infer basic routine activity sequences from this data, lacking depth in understanding complex human behaviors and users' characteristics.","Additionally, they struggle with the dependency on hard-to-obtain auxiliary datasets like travel surveys.","To address these limitations, this paper defines trajectory semantic inference through three key dimensions: user occupation category, activity sequence, and trajectory description, and proposes the Trajectory Semantic Inference with Large Language Models (TSI-LLM) framework to leverage LLMs infer trajectory semantics comprehensively and deeply.","We adopt spatio-temporal attributes enhanced data formatting (STFormat) and design a context-inclusive prompt, enabling LLMs to more effectively interpret and infer the semantics of trajectory data.","Experimental validation on real-world trajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex human mobility patterns.","This study explores the potential of LLMs in enhancing the semantic analysis of trajectory data, paving the way for more sophisticated and accessible human mobility research."],"url":"http://arxiv.org/abs/2405.19850v1","category":"cs.AI"}
{"created":"2024-05-30 08:50:55","title":"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model","abstract":"Large language models, initially pre-trained with a limited context length, can better handle longer texts by continuing training on a corpus with extended contexts. However, obtaining effective long-context data is challenging due to the scarcity and uneven distribution of long documents across different domains. To address this issue, we propose a Query-centric data synthesis method, abbreviated as Quest. Quest is an interpretable method based on the observation that documents retrieved by similar queries are relevant but low-redundant, thus well-suited for synthesizing long-context data. The method is also scalable and capable of constructing large amounts of long-context data. Using Quest, we synthesize a long-context dataset up to 128k context length, significantly outperforming other data synthesis methods on multiple long-context benchmark datasets. In addition, we further verify that the Quest method is predictable through scaling law experiments, making it a reliable solution for advancing long-context models.","sentences":["Large language models, initially pre-trained with a limited context length, can better handle longer texts by continuing training on a corpus with extended contexts.","However, obtaining effective long-context data is challenging due to the scarcity and uneven distribution of long documents across different domains.","To address this issue, we propose a Query-centric data synthesis method, abbreviated as Quest.","Quest is an interpretable method based on the observation that documents retrieved by similar queries are relevant but low-redundant, thus well-suited for synthesizing long-context data.","The method is also scalable and capable of constructing large amounts of long-context data.","Using Quest, we synthesize a long-context dataset up to 128k context length, significantly outperforming other data synthesis methods on multiple long-context benchmark datasets.","In addition, we further verify that the Quest method is predictable through scaling law experiments, making it a reliable solution for advancing long-context models."],"url":"http://arxiv.org/abs/2405.19846v1","category":"cs.CL"}
{"created":"2024-05-30 08:49:34","title":"Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation","abstract":"Large language models (LLMs) exhibit enhanced reasoning at larger scales, driving efforts to distill these capabilities into smaller models via teacher-student learning. Previous works simply fine-tune student models on teachers' generated Chain-of-Thoughts (CoTs) data. Although these methods enhance in-domain (IND) reasoning performance, they struggle to generalize to out-of-domain (OOD) tasks. We believe that the widespread spurious correlations between questions and answers may lead the model to preset a specific answer which restricts the diversity and generalizability of its reasoning process. In this paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to address these issues by decomposing the traditional single-step learning process into two cascaded learning steps. Specifically, by restructuring the training objectives -- removing the answer from outputs and concatenating the question with the rationale as input -- CasCoD's two-step learning process ensures that students focus on learning rationales without interference from the preset answers, thus improving reasoning generalizability. Extensive experiments demonstrate the effectiveness of CasCoD on both IND and OOD benchmark reasoning datasets. Code can be found at https://github.com/C-W-D/CasCoD.","sentences":["Large language models (LLMs) exhibit enhanced reasoning at larger scales, driving efforts to distill these capabilities into smaller models via teacher-student learning.","Previous works simply fine-tune student models on teachers' generated Chain-of-Thoughts (CoTs) data.","Although these methods enhance in-domain (IND) reasoning performance, they struggle to generalize to out-of-domain (OOD) tasks.","We believe that the widespread spurious correlations between questions and answers may lead the model to preset a specific answer which restricts the diversity and generalizability of its reasoning process.","In this paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to address these issues by decomposing the traditional single-step learning process into two cascaded learning steps.","Specifically, by restructuring the training objectives -- removing the answer from outputs and concatenating the question with the rationale as input -- CasCoD's two-step learning process ensures that students focus on learning rationales without interference from the preset answers, thus improving reasoning generalizability.","Extensive experiments demonstrate the effectiveness of CasCoD on both IND and OOD benchmark reasoning datasets.","Code can be found at https://github.com/C-W-D/CasCoD."],"url":"http://arxiv.org/abs/2405.19842v1","category":"cs.CL"}
{"created":"2024-05-30 08:47:32","title":"Machine learning to explore high-entropy alloys with desired enthalpy for room-temperature hydrogen storage: Prediction of density functional theory and experimental data","abstract":"Safe and high-density storage of hydrogen, for a clean-fuel economy, can be realized by hydride-forming materials, but these materials should be able to store hydrogen at room temperature. Some high-entropy alloys (HEAs) have recently been shown to reversibly store hydrogen at room temperature, but the design of HEAs with appropriate thermodynamics is still challenging. To explore HEAs with appropriate hydride formation enthalpy, this study employs machine learning (ML), in particular, Gaussian process regression (GPR) using four different kernels by training with 420 datum points collected from literature and curated here. The developed ML models are used to predict the formation enthalpy of hydrides for the TixZr2-xCrMnFeNi (x = 0.5, 1.0 and 1.5) system, which is not in the training set. The predicted values by ML are consistent with data from experiments and density functional theory (DFT). The present study thus introduces ML as a rapid and reliable approach for the design of HEAs with hydride formation enthalpies of -25 to -39 kJ/mol for hydrogen storage at room temperature.","sentences":["Safe and high-density storage of hydrogen, for a clean-fuel economy, can be realized by hydride-forming materials, but these materials should be able to store hydrogen at room temperature.","Some high-entropy alloys (HEAs) have recently been shown to reversibly store hydrogen at room temperature, but the design of HEAs with appropriate thermodynamics is still challenging.","To explore HEAs with appropriate hydride formation enthalpy, this study employs machine learning (ML), in particular, Gaussian process regression (GPR) using four different kernels by training with 420 datum points collected from literature and curated here.","The developed ML models are used to predict the formation enthalpy of hydrides for the TixZr2-xCrMnFeNi (x = 0.5, 1.0 and 1.5) system, which is not in the training set.","The predicted values by ML are consistent with data from experiments and density functional theory (DFT).","The present study thus introduces ML as a rapid and reliable approach for the design of HEAs with hydride formation enthalpies of -25 to -39 kJ/mol for hydrogen storage at room temperature."],"url":"http://arxiv.org/abs/2405.19838v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 08:46:11","title":"Lifelong learning challenges in the era of artificial intelligence: a computational thinking perspective","abstract":"The rapid advancement of artificial intelligence (AI) has brought significant challenges to the education and workforce skills required to take advantage of AI for human-AI collaboration in the workplace. As AI continues to reshape industries and job markets, the need to define how AI literacy can be considered in lifelong learning has become increasingly critical (Cetindamar et al., 2022; Laupichler et al., 2022; Romero et al., 2023). Like any new technology, AI is the subject of both hopes and fears, and what it entails today presents major challenges (Cugurullo \\& Acheampong, 2023; Villani et al., 2018). It also raises profound questions about our own humanity. Will the machine surpass the intelligence of the humans who designed it? What will be the relationship between so-called AI and our human intelligences? How could human-AI collaboration be regulated in a way that serves the Sustainable Development Goals (SDGs)? This paper provides a review of the challenges of lifelong learning in the era of AI from a computational thinking, critical thinking, and creative competencies perspective, highlighting the implications for management and leadership in organizations.","sentences":["The rapid advancement of artificial intelligence (AI) has brought significant challenges to the education and workforce skills required to take advantage of AI for human-AI collaboration in the workplace.","As AI continues to reshape industries and job markets, the need to define how AI literacy can be considered in lifelong learning has become increasingly critical (Cetindamar et al., 2022; Laupichler et al., 2022; Romero et al., 2023).","Like any new technology, AI is the subject of both hopes and fears, and what it entails today presents major challenges (Cugurullo \\& Acheampong, 2023; Villani et al., 2018).","It also raises profound questions about our own humanity.","Will the machine surpass the intelligence of the humans who designed it?","What will be the relationship between so-called AI and our human intelligences?","How could human-AI collaboration be regulated in a way that serves the Sustainable Development Goals (SDGs)?","This paper provides a review of the challenges of lifelong learning in the era of AI from a computational thinking, critical thinking, and creative competencies perspective, highlighting the implications for management and leadership in organizations."],"url":"http://arxiv.org/abs/2405.19837v1","category":"cs.AI"}
{"created":"2024-05-30 08:41:54","title":"AI Safety: A Climb To Armageddon?","abstract":"This paper presents an argument that certain AI safety measures, rather than mitigating existential risk, may instead exacerbate it. Under certain key assumptions - the inevitability of AI failure, the expected correlation between an AI system's power at the point of failure and the severity of the resulting harm, and the tendency of safety measures to enable AI systems to become more powerful before failing - safety efforts have negative expected utility. The paper examines three response strategies: Optimism, Mitigation, and Holism. Each faces challenges stemming from intrinsic features of the AI safety landscape that we term Bottlenecking, the Perfection Barrier, and Equilibrium Fluctuation. The surprising robustness of the argument forces a re-examination of core assumptions around AI safety and points to several avenues for further research.","sentences":["This paper presents an argument that certain AI safety measures, rather than mitigating existential risk, may instead exacerbate it.","Under certain key assumptions - the inevitability of AI failure, the expected correlation between an AI system's power at the point of failure and the severity of the resulting harm, and the tendency of safety measures to enable AI systems to become more powerful before failing - safety efforts have negative expected utility.","The paper examines three response strategies: Optimism, Mitigation, and Holism.","Each faces challenges stemming from intrinsic features of the AI safety landscape that we term Bottlenecking, the Perfection Barrier, and Equilibrium Fluctuation.","The surprising robustness of the argument forces a re-examination of core assumptions around AI safety and points to several avenues for further research."],"url":"http://arxiv.org/abs/2405.19832v1","category":"cs.AI"}
{"created":"2024-05-30 08:31:18","title":"Joint Selective State Space Model and Detrending for Robust Time Series Anomaly Detection","abstract":"Deep learning-based sequence models are extensively employed in Time Series Anomaly Detection (TSAD) tasks due to their effective sequential modeling capabilities. However, the ability of TSAD is limited by two key challenges: (i) the ability to model long-range dependency and (ii) the generalization issue in the presence of non-stationary data. To tackle these challenges, an anomaly detector that leverages the selective state space model known for its proficiency in capturing long-term dependencies across various domains is proposed. Additionally, a multi-stage detrending mechanism is introduced to mitigate the prominent trend component in non-stationary data to address the generalization issue. Extensive experiments conducted on realworld public datasets demonstrate that the proposed methods surpass all 12 compared baseline methods.","sentences":["Deep learning-based sequence models are extensively employed in Time Series Anomaly Detection (TSAD) tasks due to their effective sequential modeling capabilities.","However, the ability of TSAD is limited by two key challenges: (i) the ability to model long-range dependency and (ii) the generalization issue in the presence of non-stationary data.","To tackle these challenges, an anomaly detector that leverages the selective state space model known for its proficiency in capturing long-term dependencies across various domains is proposed.","Additionally, a multi-stage detrending mechanism is introduced to mitigate the prominent trend component in non-stationary data to address the generalization issue.","Extensive experiments conducted on realworld public datasets demonstrate that the proposed methods surpass all 12 compared baseline methods."],"url":"http://arxiv.org/abs/2405.19823v1","category":"cs.LG"}
{"created":"2024-05-30 08:31:01","title":"Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology","abstract":"Collecting and annotating real-world data for the development of object detection models is a time-consuming and expensive process. In the military domain in particular, data collection can also be dangerous or infeasible. Training models on synthetic data may provide a solution for cases where access to real-world training data is restricted. However, bridging the reality gap between synthetic and real data remains a challenge. Existing methods usually build on top of baseline Convolutional Neural Network (CNN) models that have been shown to perform well when trained on real data, but have limited ability to perform well when trained on synthetic data. For example, some architectures allow for fine-tuning with the expectation of large quantities of training data and are prone to overfitting on synthetic data. Related work usually ignores various best practices from object detection on real data, e.g. by training on synthetic data from a single environment with relatively little variation. In this paper we propose a methodology for improving the performance of a pre-trained object detector when training on synthetic data. Our approach focuses on extracting the salient information from synthetic data without forgetting useful features learned from pre-training on real images. Based on the state of the art, we incorporate data augmentation methods and a Transformer backbone. Besides reaching relatively strong performance without any specialized synthetic data transfer methods, we show that our methods improve the state of the art on synthetic data trained object detection for the RarePlanes and DGTA-VisDrone datasets, and reach near-perfect performance on an in-house vehicle detection dataset.","sentences":["Collecting and annotating real-world data for the development of object detection models is a time-consuming and expensive process.","In the military domain in particular, data collection can also be dangerous or infeasible.","Training models on synthetic data may provide a solution for cases where access to real-world training data is restricted.","However, bridging the reality gap between synthetic and real data remains a challenge.","Existing methods usually build on top of baseline Convolutional Neural Network (CNN) models that have been shown to perform well when trained on real data, but have limited ability to perform well when trained on synthetic data.","For example, some architectures allow for fine-tuning with the expectation of large quantities of training data and are prone to overfitting on synthetic data.","Related work usually ignores various best practices from object detection on real data, e.g. by training on synthetic data from a single environment with relatively little variation.","In this paper we propose a methodology for improving the performance of a pre-trained object detector when training on synthetic data.","Our approach focuses on extracting the salient information from synthetic data without forgetting useful features learned from pre-training on real images.","Based on the state of the art, we incorporate data augmentation methods and a Transformer backbone.","Besides reaching relatively strong performance without any specialized synthetic data transfer methods, we show that our methods improve the state of the art on synthetic data trained object detection for the RarePlanes and DGTA-VisDrone datasets, and reach near-perfect performance on an in-house vehicle detection dataset."],"url":"http://arxiv.org/abs/2405.19822v1","category":"cs.CV"}
{"created":"2024-05-30 08:25:21","title":"WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark","abstract":"Underwater object tracking (UOT) is a foundational task for identifying and tracing submerged entities in underwater video sequences. However, current UOT datasets suffer from limitations in scale, diversity of target categories and scenarios covered, hindering the training and evaluation of modern tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \\ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \\eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \\eg, underwater vision-language tracking. Most existing trackers are tailored for open-air environments, leading to performance degradation when applied to UOT due to domain gaps. Retraining and fine-tuning these trackers are challenging due to sample imbalances and limited real-world underwater datasets. To tackle these challenges, we propose a novel omni-knowledge distillation framework based on WebUOT-1M, incorporating various strategies to guide the learning of the student Transformer. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. Furthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers, showcasing its value as a benchmark for UOT research by presenting new challenges and opportunities for future studies. The complete dataset, codes and tracking results, will be made publicly available.","sentences":["Underwater object tracking (UOT) is a foundational task for identifying and tracing submerged entities in underwater video sequences.","However, current UOT datasets suffer from limitations in scale, diversity of target categories and scenarios covered, hindering the training and evaluation of modern tracking algorithms.","To bridge this gap, we take the first step and introduce WebUOT-1M, \\ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments.","It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \\eg, UVOT400.","Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets.","Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \\eg, underwater vision-language tracking.","Most existing trackers are tailored for open-air environments, leading to performance degradation when applied to UOT due to domain gaps.","Retraining and fine-tuning these trackers are challenging due to sample imbalances and limited real-world underwater datasets.","To tackle these challenges, we propose a novel omni-knowledge distillation framework based on WebUOT-1M, incorporating various strategies to guide the learning of the student Transformer.","To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M.","Furthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers, showcasing its value as a benchmark for UOT research by presenting new challenges and opportunities for future studies.","The complete dataset, codes and tracking results, will be made publicly available."],"url":"http://arxiv.org/abs/2405.19818v1","category":"cs.CV"}
{"created":"2024-05-30 08:23:56","title":"Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally","abstract":"Machine learning tasks are generally formulated as optimization problems, where one searches for an optimal function within a certain functional space. In practice, parameterized functional spaces are considered, in order to be able to perform gradient descent. Typically, a neural network architecture is chosen and fixed, and its parameters (connection weights) are optimized, yielding an architecture-dependent result. This way of proceeding however forces the evolution of the function during training to lie within the realm of what is expressible with the chosen architecture, and prevents any optimization across architectures. Costly architectural hyper-parameter optimization is often performed to compensate for this. Instead, we propose to adapt the architecture on the fly during training. We show that the information about desirable architectural changes, due to expressivity bottlenecks when attempting to follow the functional gradient, can be extracted from %the backpropagation. To do this, we propose a mathematical definition of expressivity bottlenecks, which enables us to detect, quantify and solve them while training, by adding suitable neurons when and where needed. Thus, while the standard approach requires large networks, in terms of number of neurons per layer, for expressivity and optimization reasons, we are able to start with very small neural networks and let them grow appropriately. As a proof of concept, we show results~on the CIFAR dataset, matching large neural network accuracy, with competitive training time, while removing the need for standard architectural hyper-parameter search.","sentences":["Machine learning tasks are generally formulated as optimization problems, where one searches for an optimal function within a certain functional space.","In practice, parameterized functional spaces are considered, in order to be able to perform gradient descent.","Typically, a neural network architecture is chosen and fixed, and its parameters (connection weights) are optimized, yielding an architecture-dependent result.","This way of proceeding however forces the evolution of the function during training to lie within the realm of what is expressible with the chosen architecture, and prevents any optimization across architectures.","Costly architectural hyper-parameter optimization is often performed to compensate for this.","Instead, we propose to adapt the architecture on the fly during training.","We show that the information about desirable architectural changes, due to expressivity bottlenecks when attempting to follow the functional gradient, can be extracted from %the backpropagation.","To do this, we propose a mathematical definition of expressivity bottlenecks, which enables us to detect, quantify and solve them while training, by adding suitable neurons when and where needed.","Thus, while the standard approach requires large networks, in terms of number of neurons per layer, for expressivity and optimization reasons, we are able to start with very small neural networks and let them grow appropriately.","As a proof of concept, we show results~on the CIFAR dataset, matching large neural network accuracy, with competitive training time, while removing the need for standard architectural hyper-parameter search."],"url":"http://arxiv.org/abs/2405.19816v1","category":"cs.AI"}
{"created":"2024-05-30 08:23:04","title":"Efficient Stimuli Generation using Reinforcement Learning in Design Verification","abstract":"The increasing design complexity of System-on-Chips (SoCs) has led to significant verification challenges, particularly in meeting coverage targets within a timely manner. At present, coverage closure is heavily dependent on constrained random and coverage driven verification methodologies where the randomized stimuli are bounded to verify certain scenarios and to reach coverage goals. This process is said to be exhaustive and to consume a lot of project time. In this paper, a novel methodology is proposed to generate efficient stimuli with the help of Reinforcement Learning (RL) to reach the maximum code coverage of the Design Under Verification (DUV). Additionally, an automated framework is created using metamodeling to generate a SystemVerilog testbench and an RL environment for any given design. The proposed approach is applied to various designs and the produced results proves that the RL agent provides effective stimuli to achieve code coverage faster in comparison with baseline random simulations. Furthermore, various RL agents and reward schemes are analyzed in our work.","sentences":["The increasing design complexity of System-on-Chips (SoCs) has led to significant verification challenges, particularly in meeting coverage targets within a timely manner.","At present, coverage closure is heavily dependent on constrained random and coverage driven verification methodologies where the randomized stimuli are bounded to verify certain scenarios and to reach coverage goals.","This process is said to be exhaustive and to consume a lot of project time.","In this paper, a novel methodology is proposed to generate efficient stimuli with the help of Reinforcement Learning (RL) to reach the maximum code coverage of the Design Under Verification (DUV).","Additionally, an automated framework is created using metamodeling to generate a SystemVerilog testbench and an RL environment for any given design.","The proposed approach is applied to various designs and the produced results proves that the RL agent provides effective stimuli to achieve code coverage faster in comparison with baseline random simulations.","Furthermore, various RL agents and reward schemes are analyzed in our work."],"url":"http://arxiv.org/abs/2405.19815v1","category":"cs.AI"}
{"created":"2024-05-30 08:17:15","title":"AI with Alien Content and Alien Metasemantics","abstract":"AlphaGo plays chess and Go in a creative and novel way. It is natural for us to attribute contents to it, such as that it doesn't view being several pawns behind, if it has more board space, as bad. The framework introduced in Cappelen and Dever (2021) provides a way of thinking about the semantics and the metasemantics of AI content: does AlphaGo entertain contents like this, and if so, in virtue of what does a given state of the program mean that particular content? One salient question Cappelen and Dever didn't consider was the possibility of alien content. Alien content is content that is not or cannot be expressed by human beings. It's highly plausible that AlphaGo, or any other sophisticated AI system, expresses alien contents. That this is so, moreover, is plausibly a metasemantic fact: a fact that has to do with how AI comes to entertain content in the first place, one that will heed the vastly different etiology of AI and human content. This chapter explores the question of alien content in AI from a semantic and metasemantic perspective. It lays out the logical space of possible responses to the semantic and metasemantic questions alien content poses, considers whether and how we humans could communicate with entities who express alien content, and points out that getting clear about such questions might be important for more 'applied' issues in the philosophy of AI, such as existential risk and XAI.","sentences":["AlphaGo plays chess and Go in a creative and novel way.","It is natural for us to attribute contents to it, such as that it doesn't view being several pawns behind, if it has more board space, as bad.","The framework introduced in Cappelen and Dever (2021) provides a way of thinking about the semantics and the metasemantics of AI content: does AlphaGo entertain contents like this, and if so, in virtue of what does a given state of the program mean that particular content?","One salient question Cappelen and Dever didn't consider was the possibility of alien content.","Alien content is content that is not or cannot be expressed by human beings.","It's highly plausible that AlphaGo, or any other sophisticated AI system, expresses alien contents.","That this is so, moreover, is plausibly a metasemantic fact: a fact that has to do with how AI comes to entertain content in the first place, one that will heed the vastly different etiology of AI and human content.","This chapter explores the question of alien content in AI from a semantic and metasemantic perspective.","It lays out the logical space of possible responses to the semantic and metasemantic questions alien content poses, considers whether and how we humans could communicate with entities who express alien content, and points out that getting clear about such questions might be important for more 'applied' issues in the philosophy of AI, such as existential risk and XAI."],"url":"http://arxiv.org/abs/2405.19808v1","category":"cs.AI"}
{"created":"2024-05-30 08:12:51","title":"Exploring Key Factors for Long-Term Vessel Incident Risk Prediction","abstract":"Factor analysis acts a pivotal role in enhancing maritime safety. Most previous studies conduct factor analysis within the framework of incident-related label prediction, where the developed models can be categorized into short-term and long-term prediction models. The long-term models offer a more strategic approach, enabling more proactive risk management, compared to the short-term ones. Nevertheless, few studies have devoted to rigorously identifying the key factors for the long-term prediction and undertaking comprehensive factor analysis. Hence, this study aims to delve into the key factors for predicting the incident risk levels in the subsequent year given a specific datestamp. The majority of candidate factors potentially contributing to the incident risk are collected from vessels' historical safety performance data spanning up to five years. An improved embedded feature selection, which integrates Random Forest classifier with a feature filtering process is proposed to identify key risk-contributing factors from the candidate pool. The results demonstrate superior performance of the proposed method in incident prediction and factor interpretability. Comprehensive analysis is conducted upon the key factors, which could help maritime stakeholders formulate management strategies for incident prevenion.","sentences":["Factor analysis acts a pivotal role in enhancing maritime safety.","Most previous studies conduct factor analysis within the framework of incident-related label prediction, where the developed models can be categorized into short-term and long-term prediction models.","The long-term models offer a more strategic approach, enabling more proactive risk management, compared to the short-term ones.","Nevertheless, few studies have devoted to rigorously identifying the key factors for the long-term prediction and undertaking comprehensive factor analysis.","Hence, this study aims to delve into the key factors for predicting the incident risk levels in the subsequent year given a specific datestamp.","The majority of candidate factors potentially contributing to the incident risk are collected from vessels' historical safety performance data spanning up to five years.","An improved embedded feature selection, which integrates Random Forest classifier with a feature filtering process is proposed to identify key risk-contributing factors from the candidate pool.","The results demonstrate superior performance of the proposed method in incident prediction and factor interpretability.","Comprehensive analysis is conducted upon the key factors, which could help maritime stakeholders formulate management strategies for incident prevenion."],"url":"http://arxiv.org/abs/2405.19804v1","category":"cs.LG"}
{"created":"2024-05-30 08:12:08","title":"Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models","abstract":"Embodied intelligence empowers agents with a profound sense of perception, enabling them to respond in a manner closely aligned with real-world situations. Large Language Models (LLMs) delve into language instructions with depth, serving a crucial role in generating plans for intricate tasks. Thus, LLM-based embodied models further enhance the agent's capacity to comprehend and process information. However, this amalgamation also ushers in new challenges in the pursuit of heightened intelligence. Specifically, attackers can manipulate LLMs to produce irrelevant or even malicious outputs by altering their prompts. Confronted with this challenge, we observe a notable absence of multi-modal datasets essential for comprehensively evaluating the robustness of LLM-based embodied models. Consequently, we construct the Embodied Intelligent Robot Attack Dataset (EIRAD), tailored specifically for robustness evaluation. Additionally, two attack strategies are devised, including untargeted attacks and targeted attacks, to effectively simulate a range of diverse attack scenarios. At the same time, during the attack process, to more accurately ascertain whether our method is successful in attacking the LLM-based embodied model, we devise a new attack success evaluation method utilizing the BLIP2 model. Recognizing the time and cost-intensive nature of the GCG algorithm in attacks, we devise a scheme for prompt suffix initialization based on various target tasks, thus expediting the convergence process. Experimental results demonstrate that our method exhibits a superior attack success rate when targeting LLM-based embodied models, indicating a lower level of decision-level robustness in these models.","sentences":["Embodied intelligence empowers agents with a profound sense of perception, enabling them to respond in a manner closely aligned with real-world situations.","Large Language Models (LLMs) delve into language instructions with depth, serving a crucial role in generating plans for intricate tasks.","Thus, LLM-based embodied models further enhance the agent's capacity to comprehend and process information.","However, this amalgamation also ushers in new challenges in the pursuit of heightened intelligence.","Specifically, attackers can manipulate LLMs to produce irrelevant or even malicious outputs by altering their prompts.","Confronted with this challenge, we observe a notable absence of multi-modal datasets essential for comprehensively evaluating the robustness of LLM-based embodied models.","Consequently, we construct the Embodied Intelligent Robot Attack Dataset (EIRAD), tailored specifically for robustness evaluation.","Additionally, two attack strategies are devised, including untargeted attacks and targeted attacks, to effectively simulate a range of diverse attack scenarios.","At the same time, during the attack process, to more accurately ascertain whether our method is successful in attacking the LLM-based embodied model, we devise a new attack success evaluation method utilizing the BLIP2 model.","Recognizing the time and cost-intensive nature of the GCG algorithm in attacks, we devise a scheme for prompt suffix initialization based on various target tasks, thus expediting the convergence process.","Experimental results demonstrate that our method exhibits a superior attack success rate when targeting LLM-based embodied models, indicating a lower level of decision-level robustness in these models."],"url":"http://arxiv.org/abs/2405.19802v1","category":"cs.MM"}
{"created":"2024-05-30 08:04:28","title":"Explainable Attribute-Based Speaker Verification","abstract":"This paper proposes a fully explainable approach to speaker verification (SV), a task that fundamentally relies on individual speaker characteristics. The opaque use of speaker attributes in current SV systems raises concerns of trust. Addressing this, we propose an attribute-based explainable SV system that identifies speakers by comparing personal attributes such as gender, nationality, and age extracted automatically from voice recordings. We believe this approach better aligns with human reasoning, making it more understandable than traditional methods. Evaluated on the Voxceleb1 test set, the best performance of our system is comparable with the ground truth established when using all correct attributes, proving its efficacy. Whilst our approach sacrifices some performance compared to non-explainable methods, we believe that it moves us closer to the goal of transparent, interpretable AI and lays the groundwork for future enhancements through attribute expansion.","sentences":["This paper proposes a fully explainable approach to speaker verification (SV), a task that fundamentally relies on individual speaker characteristics.","The opaque use of speaker attributes in current SV systems raises concerns of trust.","Addressing this, we propose an attribute-based explainable SV system that identifies speakers by comparing personal attributes such as gender, nationality, and age extracted automatically from voice recordings.","We believe this approach better aligns with human reasoning, making it more understandable than traditional methods.","Evaluated on the Voxceleb1 test set, the best performance of our system is comparable with the ground truth established when using all correct attributes, proving its efficacy.","Whilst our approach sacrifices some performance compared to non-explainable methods, we believe that it moves us closer to the goal of transparent, interpretable AI and lays the groundwork for future enhancements through attribute expansion."],"url":"http://arxiv.org/abs/2405.19796v1","category":"cs.SD"}
{"created":"2024-05-30 08:03:15","title":"SLM as Guardian: Pioneering AI Safety with Small Language Models","abstract":"Most prior safety research of large language models (LLMs) has focused on enhancing the alignment of LLMs to better suit the safety requirements of humans. However, internalizing such safeguard features into larger models brought challenges of higher training cost and unintended degradation of helpfulness. To overcome such challenges, a modular approach employing a smaller LLM to detect harmful user queries is regarded as a convenient solution in designing LLM-based system with safety requirements.   In this paper, we leverage a smaller LLM for both harmful query detection and safeguard response generation. We introduce our safety requirements and the taxonomy of harmfulness categories, and then propose a multi-task learning mechanism fusing the two tasks into a single model. We demonstrate the effectiveness of our approach, providing on par or surpassing harmful query detection and safeguard response performance compared to the publicly available LLMs.","sentences":["Most prior safety research of large language models (LLMs) has focused on enhancing the alignment of LLMs to better suit the safety requirements of humans.","However, internalizing such safeguard features into larger models brought challenges of higher training cost and unintended degradation of helpfulness.","To overcome such challenges, a modular approach employing a smaller LLM to detect harmful user queries is regarded as a convenient solution in designing LLM-based system with safety requirements.   ","In this paper, we leverage a smaller LLM for both harmful query detection and safeguard response generation.","We introduce our safety requirements and the taxonomy of harmfulness categories, and then propose a multi-task learning mechanism fusing the two tasks into a single model.","We demonstrate the effectiveness of our approach, providing on par or surpassing harmful query detection and safeguard response performance compared to the publicly available LLMs."],"url":"http://arxiv.org/abs/2405.19795v1","category":"cs.CL"}
{"created":"2024-05-30 07:54:07","title":"From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers","abstract":"Instruction tuning -- tuning large language models on instruction-output pairs -- is a promising technique for making models better adapted to the real world. Yet, the key factors driving the model's capability to understand and follow instructions not seen during training remain under-explored. Our investigation begins with a series of synthetic experiments within the theoretical framework of a Turing-complete algorithm called Markov algorithm, which allows fine-grained control over the instruction-tuning data. Generalization and robustness with respect to the training distribution emerge once a diverse enough set of tasks is provided, even though very few examples are provided for each task. We extend these initial results to a real-world application scenario of code generation and find that a more diverse instruction set, extending beyond code-related tasks, improves the performance of code generation. Our observations suggest that a more diverse semantic space for instruction-tuning sets greatly improves the model's ability to follow instructions and perform tasks.","sentences":["Instruction tuning -- tuning large language models on instruction-output pairs -- is a promising technique for making models better adapted to the real world.","Yet, the key factors driving the model's capability to understand and follow instructions not seen during training remain under-explored.","Our investigation begins with a series of synthetic experiments within the theoretical framework of a Turing-complete algorithm called Markov algorithm, which allows fine-grained control over the instruction-tuning data.","Generalization and robustness with respect to the training distribution emerge once a diverse enough set of tasks is provided, even though very few examples are provided for each task.","We extend these initial results to a real-world application scenario of code generation and find that a more diverse instruction set, extending beyond code-related tasks, improves the performance of code generation.","Our observations suggest that a more diverse semantic space for instruction-tuning sets greatly improves the model's ability to follow instructions and perform tasks."],"url":"http://arxiv.org/abs/2405.19787v1","category":"cs.CL"}
{"created":"2024-05-30 07:48:43","title":"PixelsDB: Serverless and Natural-Language-Aided Data Analytics with Flexible Service Levels and Prices","abstract":"Serverless query processing has become increasingly popular due to its advantages, including automated hardware and software management, high elasticity, and pay-as-you-go pricing. For users who are not system experts, serverless query processing greatly reduces the cost of owning a data analytic system. However, it is still a significant challenge for non-expert users to transform their complex and evolving data analytic needs into proper SQL queries and select a serverless query engine that delivers satisfactory performance and price for each type of query.   This paper presents PixelsDB, an open-source data analytic system that allows users who lack system or SQL expertise to explore data efficiently. It allows users to generate and debug SQL queries using a natural language interface powered by fine-tuned language models. The queries are then executed by a serverless query engine that offers varying prices for different service levels on query urgency. The service levels are natively supported by dedicated architecture design and heterogeneous resource scheduling that can apply cost-efficient resources to process non-urgent queries. We envision that the combination of a serverless paradigm, a natural-language-aided interface, and flexible service levels and prices will substantially improve the user experience in data analysis.","sentences":["Serverless query processing has become increasingly popular due to its advantages, including automated hardware and software management, high elasticity, and pay-as-you-go pricing.","For users who are not system experts, serverless query processing greatly reduces the cost of owning a data analytic system.","However, it is still a significant challenge for non-expert users to transform their complex and evolving data analytic needs into proper SQL queries and select a serverless query engine that delivers satisfactory performance and price for each type of query.   ","This paper presents PixelsDB, an open-source data analytic system that allows users who lack system or SQL expertise to explore data efficiently.","It allows users to generate and debug SQL queries using a natural language interface powered by fine-tuned language models.","The queries are then executed by a serverless query engine that offers varying prices for different service levels on query urgency.","The service levels are natively supported by dedicated architecture design and heterogeneous resource scheduling that can apply cost-efficient resources to process non-urgent queries.","We envision that the combination of a serverless paradigm, a natural-language-aided interface, and flexible service levels and prices will substantially improve the user experience in data analysis."],"url":"http://arxiv.org/abs/2405.19784v1","category":"cs.DB"}
{"created":"2024-05-30 07:48:32","title":"Instruction-Guided Visual Masking","abstract":"Instruction following is crucial in contemporary LLM. However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image. To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with diverse multimodal models, such as LMM and robot model. By constructing visual masks for instruction-irrelevant regions, IVM-enhanced multimodal models can effectively focus on task-relevant image regions to better align with complex instructions. Specifically, we design a visual masking data generation pipeline and create an IVM-Mix-1M dataset with 1 million image-instruction pairs. We further introduce a new learning technique, Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training that prioritizes high-quality data samples. Experimental results on generic multimodal tasks such as VQA and embodied robotic control demonstrate the versatility of IVM, which as a plug-and-play tool, significantly boosts the performance of diverse multimodal models, yielding new state-of-the-art results across challenging multimodal benchmarks. Code is available at https://github.com/2toinf/IVM.","sentences":["Instruction following is crucial in contemporary LLM.","However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image.","To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with diverse multimodal models, such as LMM and robot model.","By constructing visual masks for instruction-irrelevant regions, IVM-enhanced multimodal models can effectively focus on task-relevant image regions to better align with complex instructions.","Specifically, we design a visual masking data generation pipeline and create an IVM-Mix-1M dataset with 1 million image-instruction pairs.","We further introduce a new learning technique, Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training that prioritizes high-quality data samples.","Experimental results on generic multimodal tasks such as VQA and embodied robotic control demonstrate the versatility of IVM, which as a plug-and-play tool, significantly boosts the performance of diverse multimodal models, yielding new state-of-the-art results across challenging multimodal benchmarks.","Code is available at https://github.com/2toinf/IVM."],"url":"http://arxiv.org/abs/2405.19783v1","category":"cs.CV"}
{"created":"2024-05-30 07:48:00","title":"Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion","abstract":"Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion. Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories. Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets. In this paper, we propose a dataflow-guided retrieval augmentation approach, called DraCo, for repository-level code completion. DraCo parses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph. Whenever triggering code completion, DraCo precisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs. Furthermore, we construct a large Python dataset, ReccEval, with more diverse completion targets. Our experiments demonstrate the superior accuracy and applicable efficiency of DraCo, improving code exact match by 3.43% and identifier F1-score by 3.27% on average compared to the state-of-the-art approach.","sentences":["Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion.","Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories.","Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets.","In this paper, we propose a dataflow-guided retrieval augmentation approach, called DraCo, for repository-level code completion.","DraCo parses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph.","Whenever triggering code completion, DraCo precisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs.","Furthermore, we construct a large Python dataset, ReccEval, with more diverse completion targets.","Our experiments demonstrate the superior accuracy and applicable efficiency of DraCo, improving code exact match by 3.43% and identifier F1-score by 3.27% on average compared to the state-of-the-art approach."],"url":"http://arxiv.org/abs/2405.19782v1","category":"cs.SE"}
{"created":"2024-05-30 07:44:16","title":"Enhancing Consistency and Role-Specific Knowledge Capturing by Rebuilding Fictional Character's Persona","abstract":"With the recent introduction of Assistants API, it is expected that document-based language models will be actively used in various domains, especially Role-playing. However, a key challenge lies in utilizing protagonist's persona: Assistants API often fails to achieve with its search because the information extraction part is different each time and it often omits important information such as protagonist's backstory or relationships. It is hard to maintain a consistent persona simply by using the persona document as input to the Assistants API. To address the challenge of achieving stable persona consistency, we propose CharacterGPT, a novel persona reconstruction framework to alleviate the shortcomings of the Assistants API. Our method involves Character Persona Training (CPT), an effective persona rebuilding process that updates the character persona by extracting the character's traits from given summary of the novel for each character as if the story in a novel progresses. In our experiments, we ask each character to take the Big Five Inventory personality test in various settings and analyze the results. To assess whether it can think outside the box, we let each character generate short novels. Extensive experiments and human evaluation demonstrate that CharacterGPT presents new possibilities for role-playing agent research.","sentences":["With the recent introduction of Assistants API, it is expected that document-based language models will be actively used in various domains, especially Role-playing.","However, a key challenge lies in utilizing protagonist's persona: Assistants API often fails to achieve with its search because the information extraction part is different each time and it often omits important information such as protagonist's backstory or relationships.","It is hard to maintain a consistent persona simply by using the persona document as input to the Assistants API.","To address the challenge of achieving stable persona consistency, we propose CharacterGPT, a novel persona reconstruction framework to alleviate the shortcomings of the Assistants API.","Our method involves Character Persona Training (CPT), an effective persona rebuilding process that updates the character persona by extracting the character's traits from given summary of the novel for each character as if the story in a novel progresses.","In our experiments, we ask each character to take the Big Five Inventory personality test in various settings and analyze the results.","To assess whether it can think outside the box, we let each character generate short novels.","Extensive experiments and human evaluation demonstrate that CharacterGPT presents new possibilities for role-playing agent research."],"url":"http://arxiv.org/abs/2405.19778v1","category":"cs.CL"}
{"created":"2024-05-30 07:44:04","title":"Magnetic nonreciprocity in a hybrid device of asymmetric artificial spin-ice-superconductors","abstract":"Controlling the size and distribution of potential barriers within a medium of interacting particles can unveil unique collective behaviors and innovative functionalities. In this study, we introduce a unique superconducting hybrid device using a novel artificial spin ice structure composed of asymmetric nanomagnets. This structure forms a distinctive superconducting pinning potential that steers unconventional motion of superconducting vortices, thereby inducing a magnetic nonreciprocal effect, in contrast to the electric nonreciprocal effect commonly observed in superconducting diodes. Furthermore, the polarity of the magnetic nonreciprocity is in-situ reversible through the tunable magnetic patterns of artificial spin ice. Our findings demonstrate that artificial spin ice not only precisely modulates superconducting characteristics but also opens the door to novel functionalities, offering a groundbreaking paradigm for superconducting electronics.","sentences":["Controlling the size and distribution of potential barriers within a medium of interacting particles can unveil unique collective behaviors and innovative functionalities.","In this study, we introduce a unique superconducting hybrid device using a novel artificial spin ice structure composed of asymmetric nanomagnets.","This structure forms a distinctive superconducting pinning potential that steers unconventional motion of superconducting vortices, thereby inducing a magnetic nonreciprocal effect, in contrast to the electric nonreciprocal effect commonly observed in superconducting diodes.","Furthermore, the polarity of the magnetic nonreciprocity is in-situ reversible through the tunable magnetic patterns of artificial spin ice.","Our findings demonstrate that artificial spin ice not only precisely modulates superconducting characteristics but also opens the door to novel functionalities, offering a groundbreaking paradigm for superconducting electronics."],"url":"http://arxiv.org/abs/2405.19777v1","category":"cond-mat.supr-con"}
{"created":"2024-05-30 07:31:24","title":"MAE-GAN: A Novel Strategy for Simultaneous Super-resolution Reconstruction and Denoising of Post-stack Seismic Profile","abstract":"Post-stack seismic profiles are images reflecting containing geological structures which provides a critical foundation for understanding the distribution of oil and gas resources. However, due to the limitations of seismic acquisition equipment and data collecting geometry, the post-stack profiles suffer from low resolution and strong noise issues, which severely affects subsequent seismic interpretation. To better enhance the spatial resolution and signal-to-noise ratio of post-seismic profiles, a multi-scale attention encoder-decoder network based on generative adversarial network (MAE-GAN) is proposed. This method improves the resolution of post-stack profiles, and effectively suppresses noises and recovers weak signals as well. A multi-scale residual module is proposed to extract geological features under different receptive fields. At the same time, an attention module is designed to further guide the network to focus on important feature information. Additionally, to better recover the global and local information of post-stack profiles, an adversarial network based on a Markov discriminator is proposed. Finally, by introducing an edge information preservation loss function, the conventional loss function of the Generative Adversarial Network is improved, which enables better recovery of the edge information of the original post-stack profiles. Experimental results on simulated and field post-stack profiles demonstrate that the proposed MAE-GAN method outperforms two advanced convolutional neural network-based methods in noise suppression and weak signal recovery. Furthermore, the profiles reconstructed by the MAE-GAN method preserve more geological structures.","sentences":["Post-stack seismic profiles are images reflecting containing geological structures which provides a critical foundation for understanding the distribution of oil and gas resources.","However, due to the limitations of seismic acquisition equipment and data collecting geometry, the post-stack profiles suffer from low resolution and strong noise issues, which severely affects subsequent seismic interpretation.","To better enhance the spatial resolution and signal-to-noise ratio of post-seismic profiles, a multi-scale attention encoder-decoder network based on generative adversarial network (MAE-GAN) is proposed.","This method improves the resolution of post-stack profiles, and effectively suppresses noises and recovers weak signals as well.","A multi-scale residual module is proposed to extract geological features under different receptive fields.","At the same time, an attention module is designed to further guide the network to focus on important feature information.","Additionally, to better recover the global and local information of post-stack profiles, an adversarial network based on a Markov discriminator is proposed.","Finally, by introducing an edge information preservation loss function, the conventional loss function of the Generative Adversarial Network is improved, which enables better recovery of the edge information of the original post-stack profiles.","Experimental results on simulated and field post-stack profiles demonstrate that the proposed MAE-GAN method outperforms two advanced convolutional neural network-based methods in noise suppression and weak signal recovery.","Furthermore, the profiles reconstructed by the MAE-GAN method preserve more geological structures."],"url":"http://arxiv.org/abs/2405.19767v1","category":"physics.geo-ph"}
{"created":"2024-05-30 07:25:23","title":"Towards Unified Multi-granularity Text Detection with Interactive Attention","abstract":"Existing OCR engines or document image analysis systems typically rely on training separate models for text detection in varying scenarios and granularities, leading to significant computational complexity and resource demands. In this paper, we introduce \"Detect Any Text\" (DAT), an advanced paradigm that seamlessly unifies scene text detection, layout analysis, and document page detection into a cohesive, end-to-end model. This design enables DAT to efficiently manage text instances at different granularities, including *word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the across-granularity interactive attention module, which significantly enhances the representation learning of text instances at varying granularities by correlating structural information across different text queries. As a result, it enables the model to achieve mutually beneficial detection performances across multiple text granularities. Additionally, a prompt-based segmentation module refines detection outcomes for texts of arbitrary curvature and complex layouts, thereby improving DAT's accuracy and expanding its real-world applicability. Experimental results demonstrate that DAT achieves state-of-the-art performances across a variety of text-related benchmarks, including multi-oriented/arbitrarily-shaped scene text detection, document layout analysis and page detection tasks.","sentences":["Existing OCR engines or document image analysis systems typically rely on training separate models for text detection in varying scenarios and granularities, leading to significant computational complexity and resource demands.","In this paper, we introduce \"Detect Any Text\" (DAT), an advanced paradigm that seamlessly unifies scene text detection, layout analysis, and document page detection into a cohesive, end-to-end model.","This design enables DAT to efficiently manage text instances at different granularities, including *word*, *line*, *paragraph* and *page*.","A pivotal innovation in DAT is the across-granularity interactive attention module, which significantly enhances the representation learning of text instances at varying granularities by correlating structural information across different text queries.","As a result, it enables the model to achieve mutually beneficial detection performances across multiple text granularities.","Additionally, a prompt-based segmentation module refines detection outcomes for texts of arbitrary curvature and complex layouts, thereby improving DAT's accuracy and expanding its real-world applicability.","Experimental results demonstrate that DAT achieves state-of-the-art performances across a variety of text-related benchmarks, including multi-oriented/arbitrarily-shaped scene text detection, document layout analysis and page detection tasks."],"url":"http://arxiv.org/abs/2405.19765v1","category":"cs.CV"}
{"created":"2024-05-30 07:16:03","title":"Revisiting CNNs for Trajectory Similarity Learning","abstract":"Similarity search is a fundamental but expensive operator in querying trajectory data, due to its quadratic complexity of distance computation. To mitigate the computational burden for long trajectories, neural networks have been widely employed for similarity learning and each trajectory is encoded as a high-dimensional vector for similarity search with linear complexity. Given the sequential nature of trajectory data, previous efforts have been primarily devoted to the utilization of RNNs or Transformers.   In this paper, we argue that the common practice of treating trajectory as sequential data results in excessive attention to capturing long-term global dependency between two sequences. Instead, our investigation reveals the pivotal role of local similarity, prompting a revisit of simple CNNs for trajectory similarity learning. We introduce ConvTraj, incorporating both 1D and 2D convolutions to capture sequential and geo-distribution features of trajectories, respectively. In addition, we conduct a series of theoretical analyses to justify the effectiveness of ConvTraj. Experimental results on three real-world large-scale datasets demonstrate that ConvTraj achieves state-of-the-art accuracy in trajectory similarity search. Owing to the simple network structure of ConvTraj, the training and inference speed on the Porto dataset with 1.6 million trajectories are increased by at least $240$x and $2.16$x, respectively. The source code and dataset can be found at \\textit{\\url{https://github.com/Proudc/ConvTraj}}.","sentences":["Similarity search is a fundamental but expensive operator in querying trajectory data, due to its quadratic complexity of distance computation.","To mitigate the computational burden for long trajectories, neural networks have been widely employed for similarity learning and each trajectory is encoded as a high-dimensional vector for similarity search with linear complexity.","Given the sequential nature of trajectory data, previous efforts have been primarily devoted to the utilization of RNNs or Transformers.   ","In this paper, we argue that the common practice of treating trajectory as sequential data results in excessive attention to capturing long-term global dependency between two sequences.","Instead, our investigation reveals the pivotal role of local similarity, prompting a revisit of simple CNNs for trajectory similarity learning.","We introduce ConvTraj, incorporating both 1D and 2D convolutions to capture sequential and geo-distribution features of trajectories, respectively.","In addition, we conduct a series of theoretical analyses to justify the effectiveness of ConvTraj.","Experimental results on three real-world large-scale datasets demonstrate that ConvTraj achieves state-of-the-art accuracy in trajectory similarity search.","Owing to the simple network structure of ConvTraj, the training and inference speed on the Porto dataset with 1.6 million trajectories are increased by at least $240$x and $2.16$x, respectively.","The source code and dataset can be found at \\textit{\\url{https://github.com/Proudc/ConvTraj}}."],"url":"http://arxiv.org/abs/2405.19761v1","category":"cs.AI"}
{"created":"2024-05-30 07:06:02","title":"Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise Filtering","abstract":"Recent advances in a generative neural network model extend the development of data augmentation methods. However, the augmentation methods based on the modern generative models fail to achieve notable performance for class imbalance data compared to the conventional model, the SMOTE. We investigate the problem of the generative model for imbalanced classification and introduce a framework to enhance the SMOTE algorithm using Variational Autoencoders (VAE). Our approach systematically quantifies the density of data points in a low-dimensional latent space using the VAE, simultaneously incorporating information on class labels and classification difficulty. Then, the data points potentially degrading the augmentation are systematically excluded, and the neighboring observations are directly augmented on the data space. Empirical studies on several imbalanced datasets represent that this simple process innovatively improves the conventional SMOTE algorithm over the deep learning models. Consequently, we conclude that the selection of minority data and the interpolation in the data space are beneficial for imbalanced classification problems with a relatively small number of data points.","sentences":["Recent advances in a generative neural network model extend the development of data augmentation methods.","However, the augmentation methods based on the modern generative models fail to achieve notable performance for class imbalance data compared to the conventional model, the SMOTE.","We investigate the problem of the generative model for imbalanced classification and introduce a framework to enhance the SMOTE algorithm using Variational Autoencoders (VAE).","Our approach systematically quantifies the density of data points in a low-dimensional latent space using the VAE, simultaneously incorporating information on class labels and classification difficulty.","Then, the data points potentially degrading the augmentation are systematically excluded, and the neighboring observations are directly augmented on the data space.","Empirical studies on several imbalanced datasets represent that this simple process innovatively improves the conventional SMOTE algorithm over the deep learning models.","Consequently, we conclude that the selection of minority data and the interpolation in the data space are beneficial for imbalanced classification problems with a relatively small number of data points."],"url":"http://arxiv.org/abs/2405.19757v1","category":"cs.LG"}
{"created":"2024-05-30 07:02:50","title":"Mitigating annotation shift in cancer classification using single image generative models","abstract":"Artificial Intelligence (AI) has emerged as a valuable tool for assisting radiologists in breast cancer detection and diagnosis. However, the success of AI applications in this domain is restricted by the quantity and quality of available data, posing challenges due to limited and costly data annotation procedures that often lead to annotation shifts. This study simulates, analyses and mitigates annotation shifts in cancer classification in the breast mammography domain. First, a high-accuracy cancer risk prediction model is developed, which effectively distinguishes benign from malignant lesions. Next, model performance is used to quantify the impact of annotation shift. We uncover a substantial impact of annotation shift on multiclass classification performance particularly for malignant lesions. We thus propose a training data augmentation approach based on single-image generative models for the affected class, requiring as few as four in-domain annotations to considerably mitigate annotation shift, while also addressing dataset imbalance. Lastly, we further increase performance by proposing and validating an ensemble architecture based on multiple models trained under different data augmentation regimes. Our study offers key insights into annotation shift in deep learning breast cancer classification and explores the potential of single-image generative models to overcome domain shift challenges.","sentences":["Artificial Intelligence (AI) has emerged as a valuable tool for assisting radiologists in breast cancer detection and diagnosis.","However, the success of AI applications in this domain is restricted by the quantity and quality of available data, posing challenges due to limited and costly data annotation procedures that often lead to annotation shifts.","This study simulates, analyses and mitigates annotation shifts in cancer classification in the breast mammography domain.","First, a high-accuracy cancer risk prediction model is developed, which effectively distinguishes benign from malignant lesions.","Next, model performance is used to quantify the impact of annotation shift.","We uncover a substantial impact of annotation shift on multiclass classification performance particularly for malignant lesions.","We thus propose a training data augmentation approach based on single-image generative models for the affected class, requiring as few as four in-domain annotations to considerably mitigate annotation shift, while also addressing dataset imbalance.","Lastly, we further increase performance by proposing and validating an ensemble architecture based on multiple models trained under different data augmentation regimes.","Our study offers key insights into annotation shift in deep learning breast cancer classification and explores the potential of single-image generative models to overcome domain shift challenges."],"url":"http://arxiv.org/abs/2405.19754v1","category":"cs.CV"}
{"created":"2024-05-30 06:56:11","title":"HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization","abstract":"Diffusion Transformers (DiTs) have recently gained substantial attention in both industrial and academic fields for their superior visual generation capabilities, outperforming traditional diffusion models that use U-Net. However,the enhanced performance of DiTs also comes with high parameter counts and implementation costs, seriously restricting their use on resource-limited devices such as mobile phones. To address these challenges, we introduce the Hybrid Floating-point Quantization for DiT(HQ-DiT), an efficient post-training quantization method that utilizes 4-bit floating-point (FP) precision on both weights and activations for DiT inference. Compared to fixed-point quantization (e.g., INT8), FP quantization, complemented by our proposed clipping range selection mechanism, naturally aligns with the data distribution within DiT, resulting in a minimal quantization error. Furthermore, HQ-DiT also implements a universal identity mathematical transform to mitigate the serious quantization error caused by the outliers. The experimental results demonstrate that DiT can achieve extremely low-precision quantization (i.e., 4 bits) with negligible impact on performance. Our approach marks the first instance where both weights and activations in DiTs are quantized to just 4 bits, with only a 0.12 increase in sFID on ImageNet.","sentences":["Diffusion Transformers (DiTs) have recently gained substantial attention in both industrial and academic fields for their superior visual generation capabilities, outperforming traditional diffusion models that use U-Net.","However,the enhanced performance of DiTs also comes with high parameter counts and implementation costs, seriously restricting their use on resource-limited devices such as mobile phones.","To address these challenges, we introduce the Hybrid Floating-point Quantization for DiT(HQ-DiT), an efficient post-training quantization method that utilizes 4-bit floating-point (FP) precision on both weights and activations for DiT inference.","Compared to fixed-point quantization (e.g., INT8), FP quantization, complemented by our proposed clipping range selection mechanism, naturally aligns with the data distribution within DiT, resulting in a minimal quantization error.","Furthermore, HQ-DiT also implements a universal identity mathematical transform to mitigate the serious quantization error caused by the outliers.","The experimental results demonstrate that DiT can achieve extremely low-precision quantization (i.e., 4 bits) with negligible impact on performance.","Our approach marks the first instance where both weights and activations in DiTs are quantized to just 4 bits, with only a 0.12 increase in sFID on ImageNet."],"url":"http://arxiv.org/abs/2405.19751v1","category":"cs.CV"}
{"created":"2024-05-30 06:52:01","title":"Generating Query Recommendations via LLMs","abstract":"Query recommendation systems are ubiquitous in modern search engines, assisting users in producing effective queries to meet their information needs. However, these systems require a large amount of data to produce good recommendations, such as a large collection of documents to index and query logs. In particular, query logs and user data are not available in cold start scenarios. Query logs are expensive to collect and maintain and require complex and time-consuming cascading pipelines for creating, combining, and ranking recommendations. To address these issues, we frame the query recommendation problem as a generative task, proposing a novel approach called Generative Query Recommendation (GQR). GQR uses an LLM as its foundation and does not require to be trained or fine-tuned to tackle the query recommendation problem. We design a prompt that enables the LLM to understand the specific recommendation task, even using a single example. We then improved our system by proposing a version that exploits query logs called Retriever-Augmented GQR (RA-GQR). RA-GQr dynamically composes its prompt by retrieving similar queries from query logs. GQR approaches reuses a pre-existing neural architecture resulting in a simpler and more ready-to-market approach, even in a cold start scenario. Our proposed GQR obtains state-of-the-art performance in terms of NDCG@10 and clarity score against two commercial search engines and the previous state-of-the-art approach on the Robust04 and ClueWeb09B collections, improving on average the NDCG@10 performance up to ~4% on Robust04 and ClueWeb09B w.r.t the previous best competitor. RA-GQR further improve the NDCG@10 obtaining an increase of ~11%, ~6\\% on Robust04 and ClueWeb09B w.r.t the best competitor. Furthermore, our system obtained ~59% of user preferences in a blind user study, proving that our method produces the most engaging queries.","sentences":["Query recommendation systems are ubiquitous in modern search engines, assisting users in producing effective queries to meet their information needs.","However, these systems require a large amount of data to produce good recommendations, such as a large collection of documents to index and query logs.","In particular, query logs and user data are not available in cold start scenarios.","Query logs are expensive to collect and maintain and require complex and time-consuming cascading pipelines for creating, combining, and ranking recommendations.","To address these issues, we frame the query recommendation problem as a generative task, proposing a novel approach called Generative Query Recommendation (GQR).","GQR uses an LLM as its foundation and does not require to be trained or fine-tuned to tackle the query recommendation problem.","We design a prompt that enables the LLM to understand the specific recommendation task, even using a single example.","We then improved our system by proposing a version that exploits query logs called Retriever-Augmented GQR (RA-GQR).","RA-GQr dynamically composes its prompt by retrieving similar queries from query logs.","GQR approaches reuses a pre-existing neural architecture resulting in a simpler and more ready-to-market approach, even in a cold start scenario.","Our proposed GQR obtains state-of-the-art performance in terms of NDCG@10 and clarity score against two commercial search engines and the previous state-of-the-art approach on the Robust04 and ClueWeb09B collections, improving on average the NDCG@10 performance up to ~4% on Robust04 and ClueWeb09B","w.r.t the previous best competitor.","RA-GQR further improve the NDCG@10 obtaining an increase of ~11%, ~6\\% on Robust04 and ClueWeb09B w.r.t the best competitor.","Furthermore, our system obtained ~59% of user preferences in a blind user study, proving that our method produces the most engaging queries."],"url":"http://arxiv.org/abs/2405.19749v1","category":"cs.IR"}
{"created":"2024-05-30 06:47:55","title":"GaussianPrediction: Dynamic 3D Gaussian Prediction for Motion Extrapolation and Free View Synthesis","abstract":"Forecasting future scenarios in dynamic environments is essential for intelligent decision-making and navigation, a challenge yet to be fully realized in computer vision and robotics. Traditional approaches like video prediction and novel-view synthesis either lack the ability to forecast from arbitrary viewpoints or to predict temporal dynamics. In this paper, we introduce GaussianPrediction, a novel framework that empowers 3D Gaussian representations with dynamic scene modeling and future scenario synthesis in dynamic environments. GaussianPrediction can forecast future states from any viewpoint, using video observations of dynamic scenes. To this end, we first propose a 3D Gaussian canonical space with deformation modeling to capture the appearance and geometry of dynamic scenes, and integrate the lifecycle property into Gaussians for irreversible deformations. To make the prediction feasible and efficient, a concentric motion distillation approach is developed by distilling the scene motion with key points. Finally, a Graph Convolutional Network is employed to predict the motions of key points, enabling the rendering of photorealistic images of future scenarios. Our framework shows outstanding performance on both synthetic and real-world datasets, demonstrating its efficacy in predicting and rendering future environments.","sentences":["Forecasting future scenarios in dynamic environments is essential for intelligent decision-making and navigation, a challenge yet to be fully realized in computer vision and robotics.","Traditional approaches like video prediction and novel-view synthesis either lack the ability to forecast from arbitrary viewpoints or to predict temporal dynamics.","In this paper, we introduce GaussianPrediction, a novel framework that empowers 3D Gaussian representations with dynamic scene modeling and future scenario synthesis in dynamic environments.","GaussianPrediction can forecast future states from any viewpoint, using video observations of dynamic scenes.","To this end, we first propose a 3D Gaussian canonical space with deformation modeling to capture the appearance and geometry of dynamic scenes, and integrate the lifecycle property into Gaussians for irreversible deformations.","To make the prediction feasible and efficient, a concentric motion distillation approach is developed by distilling the scene motion with key points.","Finally, a Graph Convolutional Network is employed to predict the motions of key points, enabling the rendering of photorealistic images of future scenarios.","Our framework shows outstanding performance on both synthetic and real-world datasets, demonstrating its efficacy in predicting and rendering future environments."],"url":"http://arxiv.org/abs/2405.19745v1","category":"cs.CV"}
{"created":"2024-05-30 06:45:23","title":"X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions","abstract":"Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples into these languages can be a solution but unreliable, leading to responses with translation errors and lacking language-specific or cultural knowledge. To address this issue, we propose a novel method to construct cross-lingual instruction following samples with instruction in English and response in low-resource languages. Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses. The candidate cross-lingual instruction tuning samples are further refined and diversified. We have employed this method to build a large-scale cross-lingual instruction tuning dataset on 10 languages, namely X-Instruction. The instruction data built using our method incorporate more language-specific knowledge compared with the naive translation method. Experimental results have shown that the response quality of the model tuned on X-Instruction greatly exceeds the model distilled from a powerful teacher model, reaching or even surpassing the ones of ChatGPT. In addition, we find that models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning.","sentences":["Large language models respond well in high-resource languages like English but struggle in low-resource languages.","It may arise from the lack of high-quality instruction following data in these languages.","Directly translating English samples into these languages can be a solution but unreliable, leading to responses with translation errors and lacking language-specific or cultural knowledge.","To address this issue, we propose a novel method to construct cross-lingual instruction following samples with instruction in English and response in low-resource languages.","Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses.","The candidate cross-lingual instruction tuning samples are further refined and diversified.","We have employed this method to build a large-scale cross-lingual instruction tuning dataset on 10 languages, namely X-Instruction.","The instruction data built using our method incorporate more language-specific knowledge compared with the naive translation method.","Experimental results have shown that the response quality of the model tuned on X-Instruction greatly exceeds the model distilled from a powerful teacher model, reaching or even surpassing the ones of ChatGPT.","In addition, we find that models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning."],"url":"http://arxiv.org/abs/2405.19744v1","category":"cs.CL"}
{"created":"2024-05-30 06:43:55","title":"May the Dance be with You: Dance Generation Framework for Non-Humanoids","abstract":"We hypothesize dance as a motion that forms a visual rhythm from music, where the visual rhythm can be perceived from an optical flow. If an agent can recognize the relationship between visual rhythm and music, it will be able to dance by generating a motion to create a visual rhythm that matches the music. Based on this, we propose a framework for any kind of non-humanoid agents to learn how to dance from human videos. Our framework works in two processes: (1) training a reward model which perceives the relationship between optical flow (visual rhythm) and music from human dance videos, (2) training the non-humanoid dancer based on that reward model, and reinforcement learning. Our reward model consists of two feature encoders for optical flow and music. They are trained based on contrastive learning which makes the higher similarity between concurrent optical flow and music features. With this reward model, the agent learns dancing by getting a higher reward when its action creates an optical flow whose feature has a higher similarity with the given music feature. Experiment results show that generated dance motion can align with the music beat properly, and user study result indicates that our framework is more preferred by humans compared to the baselines. To the best of our knowledge, our work of non-humanoid agents which learn dance from human videos is unprecedented. An example video can be found at https://youtu.be/dOUPvo-O3QY.","sentences":["We hypothesize dance as a motion that forms a visual rhythm from music, where the visual rhythm can be perceived from an optical flow.","If an agent can recognize the relationship between visual rhythm and music, it will be able to dance by generating a motion to create a visual rhythm that matches the music.","Based on this, we propose a framework for any kind of non-humanoid agents to learn how to dance from human videos.","Our framework works in two processes: (1) training a reward model which perceives the relationship between optical flow (visual rhythm) and music from human dance videos, (2) training the non-humanoid dancer based on that reward model, and reinforcement learning.","Our reward model consists of two feature encoders for optical flow and music.","They are trained based on contrastive learning which makes the higher similarity between concurrent optical flow and music features.","With this reward model, the agent learns dancing by getting a higher reward when its action creates an optical flow whose feature has a higher similarity with the given music feature.","Experiment results show that generated dance motion can align with the music beat properly, and user study result indicates that our framework is more preferred by humans compared to the baselines.","To the best of our knowledge, our work of non-humanoid agents which learn dance from human videos is unprecedented.","An example video can be found at https://youtu.be/dOUPvo-O3QY."],"url":"http://arxiv.org/abs/2405.19743v1","category":"cs.CV"}
{"created":"2024-05-30 06:38:32","title":"PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations","abstract":"Expert-designed close-ended benchmarks serve as vital tools in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through knowledge-invariant perturbations. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of transition analyses that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six state-of-the-art LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 21% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, potentially being resolved through rote memorization and leading to inflated performance. We also find that the detailed transition analyses by PertEval could illuminate weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Given these insights, we posit that PertEval can act as an essential tool that, when applied alongside any close-ended benchmark, unveils the true knowledge capacity of LLMs, marking a significant step toward more trustworthy LLM evaluation.","sentences":["Expert-designed close-ended benchmarks serve as vital tools in assessing the knowledge capacity of large language models (LLMs).","Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination.","To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through knowledge-invariant perturbations.","These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details.","Our toolkit further includes a suite of transition analyses that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity.","Six state-of-the-art LLMs are re-evaluated using PertEval.","Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 21% overestimation for GPT-4.","Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, potentially being resolved through rote memorization and leading to inflated performance.","We also find that the detailed transition analyses by PertEval could illuminate weaknesses in existing LLMs' knowledge mastery and guide the development of refinement.","Given these insights, we posit that PertEval can act as an essential tool that, when applied alongside any close-ended benchmark, unveils the true knowledge capacity of LLMs, marking a significant step toward more trustworthy LLM evaluation."],"url":"http://arxiv.org/abs/2405.19740v1","category":"cs.CL"}
{"created":"2024-05-30 06:32:11","title":"Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation","abstract":"As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs consist mainly of simple reasoning forms, with a small proportion ($\\approx 4.7\\%$) of key reasoning steps that truly impact conclusions. However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key reasoning steps, instead imitating the teacher's reasoning forms and making errors or omissions on these steps. To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistak\\textbf{E}-\\textbf{D}riven key reason\\textbf{I}ng step distilla\\textbf{T}ion (\\textbf{EDIT}), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning. Firstly, to expose these crucial steps in CoTs, we design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions. Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood of these steps. Extensive experiments validate the effectiveness of EDIT across both in-domain and out-of-domain benchmark reasoning datasets. Further analysis shows that EDIT can generate high-quality CoTs with more correct key reasoning steps. Notably, we also explore how different mistake patterns affect performance and find that EDIT benefits more from logical errors than from knowledge or mathematical calculation errors in dual CoTs\\footnote{Code can be found at \\url{https://github.com/C-W-D/EDIT}}.","sentences":["As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs).","We find that CoTs consist mainly of simple reasoning forms, with a small proportion ($\\approx 4.7\\%$) of key reasoning steps that truly impact conclusions.","However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key reasoning steps, instead imitating the teacher's reasoning forms and making errors or omissions on these steps.","To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistak\\textbf{E}-\\textbf{D}riven key reason\\textbf{I}ng step distilla\\textbf{T}ion (\\textbf{EDIT}), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning.","Firstly, to expose these crucial steps in CoTs, we design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions.","Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood of these steps.","Extensive experiments validate the effectiveness of EDIT across both in-domain and out-of-domain benchmark reasoning datasets.","Further analysis shows that EDIT can generate high-quality CoTs with more correct key reasoning steps.","Notably, we also explore how different mistake patterns affect performance and find that EDIT benefits more from logical errors than from knowledge or mathematical calculation errors in dual CoTs\\footnote{Code can be found at \\url{https://github.com/C-W-D/EDIT}}."],"url":"http://arxiv.org/abs/2405.19737v1","category":"cs.CL"}
{"created":"2024-05-30 06:31:03","title":"Learning Task-relevant Sequence Representations via Intrinsic Dynamics Characteristics in Reinforcement Learning","abstract":"Learning task-relevant state representations is crucial to solving the problem of scene generalization in visual deep reinforcement learning. Prior work typically establishes a self-supervised auxiliary learner, introducing elements (e.g., rewards and actions) to extract task-relevant state information from observations through behavioral similarity metrics. However, the methods often ignore the inherent relationships between the elements (e.g., dynamics relationships) that are essential for learning accurate representations, and they are also limited to single-step metrics, which impedes the discrimination of short-term similar task/behavior information in long-term dynamics transitions. To solve the issues, we propose an intrinsic dynamic characteristics-driven sequence representation learning method (DSR) over a common DRL frame. Concretely, inspired by the fact of state transition in the underlying system, it constrains the optimization of the encoder via modeling the dynamics equations related to the state transition, which prompts the latent encoding information to satisfy the state transition process and thereby distinguishes state space and noise space. Further, to refine the ability of encoding similar tasks based on dynamics constraints, DSR also sequentially models inherent dynamics equation relationships from the perspective of sequence elements' frequency domain and multi-step prediction. Finally, experimental results show that DSR has achieved a significant performance boost in the Distracting DMControl Benchmark, with an average of 78.9% over the backbone baseline. Further results indicate that it also achieves the best performance in real-world autonomous driving tasks in the CARLA simulator. Moreover, the qualitative analysis results of t-SNE visualization validate that our method possesses superior representation ability on visual tasks.","sentences":["Learning task-relevant state representations is crucial to solving the problem of scene generalization in visual deep reinforcement learning.","Prior work typically establishes a self-supervised auxiliary learner, introducing elements (e.g., rewards and actions) to extract task-relevant state information from observations through behavioral similarity metrics.","However, the methods often ignore the inherent relationships between the elements (e.g., dynamics relationships) that are essential for learning accurate representations, and they are also limited to single-step metrics, which impedes the discrimination of short-term similar task/behavior information in long-term dynamics transitions.","To solve the issues, we propose an intrinsic dynamic characteristics-driven sequence representation learning method (DSR) over a common DRL frame.","Concretely, inspired by the fact of state transition in the underlying system, it constrains the optimization of the encoder via modeling the dynamics equations related to the state transition, which prompts the latent encoding information to satisfy the state transition process and thereby distinguishes state space and noise space.","Further, to refine the ability of encoding similar tasks based on dynamics constraints, DSR also sequentially models inherent dynamics equation relationships from the perspective of sequence elements' frequency domain and multi-step prediction.","Finally, experimental results show that DSR has achieved a significant performance boost in the Distracting DMControl Benchmark, with an average of 78.9% over the backbone baseline.","Further results indicate that it also achieves the best performance in real-world autonomous driving tasks in the CARLA simulator.","Moreover, the qualitative analysis results of t-SNE visualization validate that our method possesses superior representation ability on visual tasks."],"url":"http://arxiv.org/abs/2405.19736v1","category":"cs.AI"}
{"created":"2024-05-30 06:29:46","title":"Search for the decay $B^{0}\\to\u03b3\u03b3$ using Belle and Belle II data","abstract":"We report the result of a search for the rare decay $B^{0} \\to \\gamma \\gamma$ using a combined dataset of $753\\times10^{6}$ $B\\bar{B}$ pairs collected by the Belle experiment and $387\\times10^{6}$ $B\\bar{B}$ pairs collected by the Belle II experiment from decays of the $\\rm \\Upsilon(4S)$ resonance produced in $e^{+}e^{-}$ collisions. A simultaneous fit to the Belle and Belle II data sets yields $11.0^{+6.5}_{-5.5}$ signal events, corresponding to a 2.5$\\sigma$ significance. We determine the branching fraction $\\mathcal{B}(B^{0} \\to \\gamma\\gamma) = (3.7^{+2.2}_{-1.8}(\\rm stat)\\pm0.5(\\rm syst))\\times10^{-8}$ and set a 90% credibility level upper limit of $\\mathcal{B}(B^{0} \\to \\gamma\\gamma) < 6.4\\times10^{-8}$.","sentences":["We report the result of a search for the rare decay $B^{0} \\to \\gamma \\gamma$ using a combined dataset of $753\\times10^{6}$ $B\\bar{B}$ pairs collected by the Belle experiment and $387\\times10^{6}$ $B\\bar{B}$ pairs collected by the Belle II experiment from decays of the $\\rm \\Upsilon(4S)$ resonance produced in $e^{+}e^{-}$ collisions.","A simultaneous fit to the Belle and Belle II data sets yields $11.0^{+6.5}_{-5.5}$ signal events, corresponding to a 2.5$\\sigma$ significance.","We determine the branching fraction $\\mathcal{B}(B^{0} \\to \\gamma\\gamma) = (3.7^{+2.2}_{-1.8}(\\rm stat)\\pm0.5(\\rm syst))\\times10^{-8}$ and set a 90% credibility level upper limit of $\\mathcal{B}(B^{0} \\to \\gamma\\gamma) <","6.4\\times10^{-8}$."],"url":"http://arxiv.org/abs/2405.19734v1","category":"hep-ex"}
{"created":"2024-05-30 06:21:34","title":"Research on Foundation Model for Spatial Data Intelligence: China's 2024 White Paper on Strategic Development of Spatial Data Intelligence","abstract":"This report focuses on spatial data intelligent large models, delving into the principles, methods, and cutting-edge applications of these models. It provides an in-depth discussion on the definition, development history, current status, and trends of spatial data intelligent large models, as well as the challenges they face. The report systematically elucidates the key technologies of spatial data intelligent large models and their applications in urban environments, aerospace remote sensing, geography, transportation, and other scenarios. Additionally, it summarizes the latest application cases of spatial data intelligent large models in themes such as urban development, multimodal systems, remote sensing, smart transportation, and resource environments. Finally, the report concludes with an overview and outlook on the development prospects of spatial data intelligent large models.","sentences":["This report focuses on spatial data intelligent large models, delving into the principles, methods, and cutting-edge applications of these models.","It provides an in-depth discussion on the definition, development history, current status, and trends of spatial data intelligent large models, as well as the challenges they face.","The report systematically elucidates the key technologies of spatial data intelligent large models and their applications in urban environments, aerospace remote sensing, geography, transportation, and other scenarios.","Additionally, it summarizes the latest application cases of spatial data intelligent large models in themes such as urban development, multimodal systems, remote sensing, smart transportation, and resource environments.","Finally, the report concludes with an overview and outlook on the development prospects of spatial data intelligent large models."],"url":"http://arxiv.org/abs/2405.19730v1","category":"cs.AI"}
{"created":"2024-05-30 06:21:11","title":"Dynamic feature selection in medical predictive monitoring by reinforcement learning","abstract":"In this paper, we investigate dynamic feature selection within multivariate time-series scenario, a common occurrence in clinical prediction monitoring where each feature corresponds to a bio-test result. Many existing feature selection methods fall short in effectively leveraging time-series information, primarily because they are designed for static data. Our approach addresses this limitation by enabling the selection of time-varying feature subsets for each patient. Specifically, we employ reinforcement learning to optimize a policy under maximum cost restrictions. The prediction model is subsequently updated using synthetic data generated by trained policy. Our method can seamlessly integrate with non-differentiable prediction models. We conducted experiments on a sizable clinical dataset encompassing regression and classification tasks. The results demonstrate that our approach outperforms strong feature selection baselines, particularly when subjected to stringent cost limitations. Code will be released once paper is accepted.","sentences":["In this paper, we investigate dynamic feature selection within multivariate time-series scenario, a common occurrence in clinical prediction monitoring where each feature corresponds to a bio-test result.","Many existing feature selection methods fall short in effectively leveraging time-series information, primarily because they are designed for static data.","Our approach addresses this limitation by enabling the selection of time-varying feature subsets for each patient.","Specifically, we employ reinforcement learning to optimize a policy under maximum cost restrictions.","The prediction model is subsequently updated using synthetic data generated by trained policy.","Our method can seamlessly integrate with non-differentiable prediction models.","We conducted experiments on a sizable clinical dataset encompassing regression and classification tasks.","The results demonstrate that our approach outperforms strong feature selection baselines, particularly when subjected to stringent cost limitations.","Code will be released once paper is accepted."],"url":"http://arxiv.org/abs/2405.19729v1","category":"cs.LG"}
{"created":"2024-05-30 06:10:10","title":"Encoding and Controlling Global Semantics for Long-form Video Question Answering","abstract":"Seeking answers effectively for long videos is essential to build video question answering (videoQA) systems. Previous methods adaptively select frames and regions from long videos to save computations. However, this fails to reason over the whole sequence of video, leading to sub-optimal performance. To address this problem, we introduce a state space layer (SSL) into multi-modal Transformer to efficiently integrate global semantics of the video, which mitigates the video information loss caused by frame and region selection modules. Our SSL includes a gating unit to enable controllability over the flow of global semantics into visual representations. To further enhance the controllability, we introduce a cross-modal compositional congruence (C^3) objective to encourage global semantics aligned with the question. To rigorously evaluate long-form videoQA capacity, we construct two new benchmarks Ego-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5 minutes and 1.9 hours, respectively. Extensive experiments demonstrate the superiority of our framework on these new as well as existing datasets.","sentences":["Seeking answers effectively for long videos is essential to build video question answering (videoQA) systems.","Previous methods adaptively select frames and regions from long videos to save computations.","However, this fails to reason over the whole sequence of video, leading to sub-optimal performance.","To address this problem, we introduce a state space layer (SSL) into multi-modal Transformer to efficiently integrate global semantics of the video, which mitigates the video information loss caused by frame and region selection modules.","Our SSL includes a gating unit to enable controllability over the flow of global semantics into visual representations.","To further enhance the controllability, we introduce a cross-modal compositional congruence (C^3) objective to encourage global semantics aligned with the question.","To rigorously evaluate long-form videoQA capacity, we construct two new benchmarks Ego-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5 minutes and 1.9 hours, respectively.","Extensive experiments demonstrate the superiority of our framework on these new as well as existing datasets."],"url":"http://arxiv.org/abs/2405.19723v1","category":"cs.CV"}
{"created":"2024-05-30 05:49:38","title":"SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths","abstract":"Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance. We study the choice of the candidate length K and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (an additional 7.2% improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.","sentences":["Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model.","Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round.","However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance.","We study the choice of the candidate length K and formulate it as a Markov Decision Process.","We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value.","Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly.","We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens.","SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold.","We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair.","Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (an additional 7.2% improvement over the baseline speculative decoding).","On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively."],"url":"http://arxiv.org/abs/2405.19715v1","category":"cs.CL"}
{"created":"2024-05-30 05:36:32","title":"Text Guided Image Editing with Automatic Concept Locating and Forgetting","abstract":"With the advancement of image-to-image diffusion models guided by text, significant progress has been made in image editing. However, a persistent challenge remains in seamlessly incorporating objects into images based on textual instructions, without relying on extra user-provided guidance. Text and images are inherently distinct modalities, bringing out difficulties in fully capturing the semantic intent conveyed through language and accurately translating that into the desired visual modifications. Therefore, text-guided image editing models often produce generations with residual object attributes that do not fully align with human expectations. To address this challenge, the models should comprehend the image content effectively away from a disconnect between the provided textual editing prompts and the actual modifications made to the image. In our paper, we propose a novel method called Locate and Forget (LaF), which effectively locates potential target concepts in the image for modification by comparing the syntactic trees of the target prompt and scene descriptions in the input image, intending to forget their existence clues in the generated image. Compared to the baselines, our method demonstrates its superiority in text-guided image editing tasks both qualitatively and quantitatively.","sentences":["With the advancement of image-to-image diffusion models guided by text, significant progress has been made in image editing.","However, a persistent challenge remains in seamlessly incorporating objects into images based on textual instructions, without relying on extra user-provided guidance.","Text and images are inherently distinct modalities, bringing out difficulties in fully capturing the semantic intent conveyed through language and accurately translating that into the desired visual modifications.","Therefore, text-guided image editing models often produce generations with residual object attributes that do not fully align with human expectations.","To address this challenge, the models should comprehend the image content effectively away from a disconnect between the provided textual editing prompts and the actual modifications made to the image.","In our paper, we propose a novel method called Locate and Forget (LaF), which effectively locates potential target concepts in the image for modification by comparing the syntactic trees of the target prompt and scene descriptions in the input image, intending to forget their existence clues in the generated image.","Compared to the baselines, our method demonstrates its superiority in text-guided image editing tasks both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.19708v1","category":"cs.CV"}
{"created":"2024-05-30 05:36:12","title":"DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark","abstract":"Recently, video generation techniques have advanced rapidly. Given the popularity of video content on social media platforms, these models intensify concerns about the spread of fake information. Therefore, there is a growing demand for detectors capable of distinguishing between fake AI-generated videos and mitigating the potential harm caused by fake information. However, the lack of large-scale datasets from the most advanced video generators poses a barrier to the development of such detectors. To address this gap, we introduce the first AI-generated video detection dataset, GenVideo. It features the following characteristics: (1) a large volume of videos, including over one million AI-generated and real videos collected; (2) a rich diversity of generated content and methodologies, covering a broad spectrum of video categories and generation techniques. We conducted extensive studies of the dataset and proposed two evaluation methods tailored for real-world-like scenarios to assess the detectors' performance: the cross-generator video classification task assesses the generalizability of trained detectors on generators; the degraded video classification task evaluates the robustness of detectors to handle videos that have degraded in quality during dissemination. Moreover, we introduced a plug-and-play module, named Detail Mamba (DeMamba), designed to enhance the detectors by identifying AI-generated videos through the analysis of inconsistencies in temporal and spatial dimensions. Our extensive experiments demonstrate DeMamba's superior generalizability and robustness on GenVideo compared to existing detectors. We believe that the GenVideo dataset and the DeMamba module will significantly advance the field of AI-generated video detection. Our code and dataset will be aviliable at \\url{https://github.com/chenhaoxing/DeMamba}.","sentences":["Recently, video generation techniques have advanced rapidly.","Given the popularity of video content on social media platforms, these models intensify concerns about the spread of fake information.","Therefore, there is a growing demand for detectors capable of distinguishing between fake AI-generated videos and mitigating the potential harm caused by fake information.","However, the lack of large-scale datasets from the most advanced video generators poses a barrier to the development of such detectors.","To address this gap, we introduce the first AI-generated video detection dataset, GenVideo.","It features the following characteristics: (1) a large volume of videos, including over one million AI-generated and real videos collected; (2) a rich diversity of generated content and methodologies, covering a broad spectrum of video categories and generation techniques.","We conducted extensive studies of the dataset and proposed two evaluation methods tailored for real-world-like scenarios to assess the detectors' performance: the cross-generator video classification task assesses the generalizability of trained detectors on generators; the degraded video classification task evaluates the robustness of detectors to handle videos that have degraded in quality during dissemination.","Moreover, we introduced a plug-and-play module, named Detail Mamba (DeMamba), designed to enhance the detectors by identifying AI-generated videos through the analysis of inconsistencies in temporal and spatial dimensions.","Our extensive experiments demonstrate DeMamba's superior generalizability and robustness on GenVideo compared to existing detectors.","We believe that the GenVideo dataset and the DeMamba module will significantly advance the field of AI-generated video detection.","Our code and dataset will be aviliable at \\url{https://github.com/chenhaoxing/DeMamba}."],"url":"http://arxiv.org/abs/2405.19707v1","category":"cs.CV"}
{"created":"2024-05-30 05:26:57","title":"Significance of Chain of Thought in Gender Bias Mitigation for English-Dravidian Machine Translation","abstract":"Gender bias in machine translation (MT) systems poses a significant challenge to achieving accurate and inclusive translations. This paper examines gender bias in machine translation systems for languages such as Telugu and Kannada from the Dravidian family, analyzing how gender inflections affect translation accuracy and neutrality using Google Translate and ChatGPT. It finds that while plural forms can reduce bias, individual-centric sentences often maintain the bias due to historical stereotypes. The study evaluates the Chain of Thought processing, noting significant bias mitigation from 80% to 4% in Telugu and from 40% to 0% in Kannada. It also compares Telugu and Kannada translations, emphasizing the need for language specific strategies to address these challenges and suggesting directions for future research to enhance fairness in both data preparation and prompts during inference.","sentences":["Gender bias in machine translation (MT) systems poses a significant challenge to achieving accurate and inclusive translations.","This paper examines gender bias in machine translation systems for languages such as Telugu and Kannada from the Dravidian family, analyzing how gender inflections affect translation accuracy and neutrality using Google Translate and ChatGPT.","It finds that while plural forms can reduce bias, individual-centric sentences often maintain the bias due to historical stereotypes.","The study evaluates the Chain of Thought processing, noting significant bias mitigation from 80% to 4% in Telugu and from 40% to 0% in Kannada.","It also compares Telugu and Kannada translations, emphasizing the need for language specific strategies to address these challenges and suggesting directions for future research to enhance fairness in both data preparation and prompts during inference."],"url":"http://arxiv.org/abs/2405.19701v1","category":"cs.CL"}
{"created":"2024-05-30 05:25:14","title":"Fairness in AI-Driven Recruitment: Challenges, Metrics, Methods, and Future Directions","abstract":"The recruitment process is crucial to an organization's ability to position itself for success, from finding qualified and well-fitting job candidates to impacting its output and culture. Therefore, over the past century, human resources experts and industrial-organizational psychologists have established hiring practices such as attracting candidates with job ads, gauging a candidate's skills with assessments, and using interview questions to assess organizational fit. However, the advent of big data and machine learning has led to a rapid transformation in the traditional recruitment process as many organizations have moved to using artificial intelligence (AI). Given the prevalence of AI-based recruitment, there is growing concern that human biases may carry over to decisions made by these systems, which can amplify the effect through systematic application. Empirical studies have identified prevalent biases in candidate ranking software and chatbot interactions, catalyzing a growing body of research dedicated to AI fairness over the last decade. This paper provides a comprehensive overview of this emerging field by discussing the types of biases encountered in AI-driven recruitment, exploring various fairness metrics and mitigation methods, and examining tools for auditing these systems. We highlight current challenges and outline future directions for developing fair AI recruitment applications, ensuring equitable candidate treatment and enhancing organizational outcomes.","sentences":["The recruitment process is crucial to an organization's ability to position itself for success, from finding qualified and well-fitting job candidates to impacting its output and culture.","Therefore, over the past century, human resources experts and industrial-organizational psychologists have established hiring practices such as attracting candidates with job ads, gauging a candidate's skills with assessments, and using interview questions to assess organizational fit.","However, the advent of big data and machine learning has led to a rapid transformation in the traditional recruitment process as many organizations have moved to using artificial intelligence (AI).","Given the prevalence of AI-based recruitment, there is growing concern that human biases may carry over to decisions made by these systems, which can amplify the effect through systematic application.","Empirical studies have identified prevalent biases in candidate ranking software and chatbot interactions, catalyzing a growing body of research dedicated to AI fairness over the last decade.","This paper provides a comprehensive overview of this emerging field by discussing the types of biases encountered in AI-driven recruitment, exploring various fairness metrics and mitigation methods, and examining tools for auditing these systems.","We highlight current challenges and outline future directions for developing fair AI recruitment applications, ensuring equitable candidate treatment and enhancing organizational outcomes."],"url":"http://arxiv.org/abs/2405.19699v1","category":"cs.CY"}
{"created":"2024-05-30 05:24:20","title":"Bilevel reinforcement learning via the development of hyper-gradient without lower-level convexity","abstract":"Bilevel reinforcement learning (RL), which features intertwined two-level problems, has attracted growing interest recently. The inherent non-convexity of the lower-level RL problem is, however, to be an impediment to developing bilevel optimization methods. By employing the fixed point equation associated with the regularized RL, we characterize the hyper-gradient via fully first-order information, thus circumventing the assumption of lower-level convexity. This, remarkably, distinguishes our development of hyper-gradient from the general AID-based bilevel frameworks since we take advantage of the specific structure of RL problems. Moreover, we propose both model-based and model-free bilevel reinforcement learning algorithms, facilitated by access to the fully first-order hyper-gradient. Both algorithms are provable to enjoy the convergence rate $\\mathcal{O}(\\epsilon^{-1})$. To the best of our knowledge, this is the first time that AID-based bilevel RL gets rid of additional assumptions on the lower-level problem. In addition, numerical experiments demonstrate that the hyper-gradient indeed serves as an integration of exploitation and exploration.","sentences":["Bilevel reinforcement learning (RL), which features intertwined two-level problems, has attracted growing interest recently.","The inherent non-convexity of the lower-level RL problem is, however, to be an impediment to developing bilevel optimization methods.","By employing the fixed point equation associated with the regularized RL, we characterize the hyper-gradient via fully first-order information, thus circumventing the assumption of lower-level convexity.","This, remarkably, distinguishes our development of hyper-gradient from the general AID-based bilevel frameworks since we take advantage of the specific structure of RL problems.","Moreover, we propose both model-based and model-free bilevel reinforcement learning algorithms, facilitated by access to the fully first-order hyper-gradient.","Both algorithms are provable to enjoy the convergence rate $\\mathcal{O}(\\epsilon^{-1})$. To the best of our knowledge, this is the first time that AID-based bilevel RL gets rid of additional assumptions on the lower-level problem.","In addition, numerical experiments demonstrate that the hyper-gradient indeed serves as an integration of exploitation and exploration."],"url":"http://arxiv.org/abs/2405.19697v1","category":"math.OC"}
{"created":"2024-05-30 05:08:15","title":"Grade Like a Human: Rethinking Automated Assessment with Large Language Models","abstract":"While large language models (LLMs) have been used for automated grading, they have not yet achieved the same level of performance as humans, especially when it comes to grading complex questions. Existing research on this topic focuses on a particular step in the grading procedure: grading using predefined rubrics. However, grading is a multifaceted procedure that encompasses other crucial steps, such as grading rubrics design and post-grading review. There has been a lack of systematic research exploring the potential of LLMs to enhance the entire grading~process.   In this paper, we propose an LLM-based grading system that addresses the entire grading procedure, including the following key components: 1) Developing grading rubrics that not only consider the questions but also the student answers, which can more accurately reflect students' performance. 2) Under the guidance of grading rubrics, providing accurate and consistent scores for each student, along with customized feedback. 3) Conducting post-grading review to better ensure accuracy and fairness. Additionally, we collected a new dataset named OS from a university operating system course and conducted extensive experiments on both our new dataset and the widely used Mohler dataset. Experiments demonstrate the effectiveness of our proposed approach, providing some new insights for developing automated grading systems based on LLMs.","sentences":["While large language models (LLMs) have been used for automated grading, they have not yet achieved the same level of performance as humans, especially when it comes to grading complex questions.","Existing research on this topic focuses on a particular step in the grading procedure: grading using predefined rubrics.","However, grading is a multifaceted procedure that encompasses other crucial steps, such as grading rubrics design and post-grading review.","There has been a lack of systematic research exploring the potential of LLMs to enhance the entire grading~process.   ","In this paper, we propose an LLM-based grading system that addresses the entire grading procedure, including the following key components: 1) Developing grading rubrics that not only consider the questions but also the student answers, which can more accurately reflect students' performance.","2) Under the guidance of grading rubrics, providing accurate and consistent scores for each student, along with customized feedback.","3) Conducting post-grading review to better ensure accuracy and fairness.","Additionally, we collected a new dataset named OS from a university operating system course and conducted extensive experiments on both our new dataset and the widely used Mohler dataset.","Experiments demonstrate the effectiveness of our proposed approach, providing some new insights for developing automated grading systems based on LLMs."],"url":"http://arxiv.org/abs/2405.19694v1","category":"cs.AI"}
{"created":"2024-05-30 05:04:33","title":"Diffusion Policies creating a Trust Region for Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) leverages pre-collected datasets to train optimal policies. Diffusion Q-Learning (DQL), introducing diffusion models as a powerful and expressive policy class, significantly boosts the performance of offline RL. However, its reliance on iterative denoising sampling to generate actions slows down both training and inference. While several recent attempts have tried to accelerate diffusion-QL, the improvement in training and/or inference speed often results in degraded performance. In this paper, we introduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which comprises a diffusion policy for pure behavior cloning and a practical one-step policy. We bridge the two polices by a newly introduced diffusion trust region loss. The diffusion policy maintains expressiveness, while the trust region loss directs the one-step policy to explore freely and seek modes within the region defined by the diffusion policy. DTQL eliminates the need for iterative denoising sampling during both training and inference, making it remarkably computationally efficient. We evaluate its effectiveness and algorithmic characteristics against popular Kullback-Leibler (KL) based distillation methods in 2D bandit scenarios and gym tasks. We then show that DTQL could not only outperform other methods on the majority of the D4RL benchmark tasks but also demonstrate efficiency in training and inference speeds. The PyTorch implementation will be made available.","sentences":["Offline reinforcement learning (RL) leverages pre-collected datasets to train optimal policies.","Diffusion Q-Learning (DQL), introducing diffusion models as a powerful and expressive policy class, significantly boosts the performance of offline RL.","However, its reliance on iterative denoising sampling to generate actions slows down both training and inference.","While several recent attempts have tried to accelerate diffusion-QL, the improvement in training and/or inference speed often results in degraded performance.","In this paper, we introduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which comprises a diffusion policy for pure behavior cloning and a practical one-step policy.","We bridge the two polices by a newly introduced diffusion trust region loss.","The diffusion policy maintains expressiveness, while the trust region loss directs the one-step policy to explore freely and seek modes within the region defined by the diffusion policy.","DTQL eliminates the need for iterative denoising sampling during both training and inference, making it remarkably computationally efficient.","We evaluate its effectiveness and algorithmic characteristics against popular Kullback-Leibler (KL) based distillation methods in 2D bandit scenarios and gym tasks.","We then show that DTQL could not only outperform other methods on the majority of the D4RL benchmark tasks but also demonstrate efficiency in training and inference speeds.","The PyTorch implementation will be made available."],"url":"http://arxiv.org/abs/2405.19690v1","category":"cs.LG"}
{"created":"2024-05-30 04:57:03","title":"Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback","abstract":"Large language models (LLMs) have demonstrated remarkable proficiency in a range of natural language processing tasks. Once deployed, LLMs encounter users with personalized factual knowledge, and such personalized knowledge is consistently reflected through users' interactions with the LLMs. To enhance user experience, real-time model personalization is essential, allowing LLMs to adapt user-specific knowledge based on user feedback during human-LLM interactions. Existing methods mostly require back-propagation to finetune the model parameters, which incurs high computational and memory costs. In addition, these methods suffer from low interpretability, which will cause unforeseen impacts on model performance during long-term use, where the user's personalized knowledge is accumulated extensively.To address these challenges, we propose Knowledge Graph Tuning (KGT), a novel approach that leverages knowledge graphs (KGs) to personalize LLMs. KGT extracts personalized factual knowledge triples from users' queries and feedback and optimizes KGs without modifying the LLM parameters. Our method improves computational and memory efficiency by avoiding back-propagation and ensures interpretability by making the KG adjustments comprehensible to humans.Experiments with state-of-the-art LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves personalization performance while reducing latency and GPU memory costs. Ultimately, KGT offers a promising solution of effective, efficient, and interpretable real-time LLM personalization during user interactions with the LLMs.","sentences":["Large language models (LLMs) have demonstrated remarkable proficiency in a range of natural language processing tasks.","Once deployed, LLMs encounter users with personalized factual knowledge, and such personalized knowledge is consistently reflected through users' interactions with the LLMs.","To enhance user experience, real-time model personalization is essential, allowing LLMs to adapt user-specific knowledge based on user feedback during human-LLM interactions.","Existing methods mostly require back-propagation to finetune the model parameters, which incurs high computational and memory costs.","In addition, these methods suffer from low interpretability, which will cause unforeseen impacts on model performance during long-term use, where the user's personalized knowledge is accumulated extensively.","To address these challenges, we propose Knowledge Graph Tuning (KGT), a novel approach that leverages knowledge graphs (KGs) to personalize LLMs.","KGT extracts personalized factual knowledge triples from users' queries and feedback and optimizes KGs without modifying the LLM parameters.","Our method improves computational and memory efficiency by avoiding back-propagation and ensures interpretability by making the KG adjustments comprehensible to humans.","Experiments with state-of-the-art LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves personalization performance while reducing latency and GPU memory costs.","Ultimately, KGT offers a promising solution of effective, efficient, and interpretable real-time LLM personalization during user interactions with the LLMs."],"url":"http://arxiv.org/abs/2405.19686v1","category":"cs.AI"}
{"created":"2024-05-30 04:46:40","title":"A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning","abstract":"Underwater image enhancement (UIE) is a challenging research task in the field of computer vision. Although hundreds of UIE algorithms have been proposed, a comprehensive and systematic review is still lacking. To promote future research, we summarize the UIE task from multiple perspectives. First, the physical models, data construction processes, evaluation metrics, and loss functions are introduced. Second, according to the contributions brought by different literatures, recent proposed algorithms are discussed and classified from six perspectives, namely network architecture, learning strategy, learning stage, assistance task, domain perspective and disentanglement fusion, respectively. Third, considering the inconsistencies in experimental settings in different literatures, a comprehensive and fair comparison does not yet exist. To this end, we quantitatively and qualitatively evaluate state-of-the-art algorithms on multiple benchmark datasets. Finally, issues worthy of further research in the UIE task are raised. A collection of useful materials is available at https://github.com/YuZhao1999/UIE.","sentences":["Underwater image enhancement (UIE) is a challenging research task in the field of computer vision.","Although hundreds of UIE algorithms have been proposed, a comprehensive and systematic review is still lacking.","To promote future research, we summarize the UIE task from multiple perspectives.","First, the physical models, data construction processes, evaluation metrics, and loss functions are introduced.","Second, according to the contributions brought by different literatures, recent proposed algorithms are discussed and classified from six perspectives, namely network architecture, learning strategy, learning stage, assistance task, domain perspective and disentanglement fusion, respectively.","Third, considering the inconsistencies in experimental settings in different literatures, a comprehensive and fair comparison does not yet exist.","To this end, we quantitatively and qualitatively evaluate state-of-the-art algorithms on multiple benchmark datasets.","Finally, issues worthy of further research in the UIE task are raised.","A collection of useful materials is available at https://github.com/YuZhao1999/UIE."],"url":"http://arxiv.org/abs/2405.19684v1","category":"cs.CV"}
{"created":"2024-05-30 04:14:58","title":"View-Consistent Hierarchical 3D SegmentationUsing Ultrametric Feature Fields","abstract":"Large-scale vision foundation models such as Segment Anything (SAM) demonstrate impressive performance in zero-shot image segmentation at multiple levels of granularity. However, these zero-shot predictions are rarely 3D-consistent. As the camera viewpoint changes in a scene, so do the segmentation predictions, as well as the characterizations of ``coarse\" or ``fine\" granularity. In this work, we address the challenging task of lifting multi-granular and view-inconsistent image segmentations into a hierarchical and 3D-consistent representation. We learn a novel feature field within a Neural Radiance Field (NeRF) representing a 3D scene, whose segmentation structure can be revealed at different scales by simply using different thresholds on feature distance. Our key idea is to learn an ultrametric feature space, which unlike a Euclidean space, exhibits transitivity in distance-based grouping, naturally leading to a hierarchical clustering. Put together, our method takes view-inconsistent multi-granularity 2D segmentations as input and produces a hierarchy of 3D-consistent segmentations as output. We evaluate our method and several baselines on synthetic datasets with multi-view images and multi-granular segmentation, showcasing improved accuracy and viewpoint-consistency. We additionally provide qualitative examples of our model's 3D hierarchical segmentations in real world scenes.\\footnote{The code and dataset are available at:","sentences":["Large-scale vision foundation models such as Segment Anything (SAM) demonstrate impressive performance in zero-shot image segmentation at multiple levels of granularity.","However, these zero-shot predictions are rarely 3D-consistent.","As the camera viewpoint changes in a scene, so do the segmentation predictions, as well as the characterizations of ``coarse\" or ``fine\" granularity.","In this work, we address the challenging task of lifting multi-granular and view-inconsistent image segmentations into a hierarchical and 3D-consistent representation.","We learn a novel feature field within a Neural Radiance Field (NeRF) representing a 3D scene, whose segmentation structure can be revealed at different scales by simply using different thresholds on feature distance.","Our key idea is to learn an ultrametric feature space, which unlike a Euclidean space, exhibits transitivity in distance-based grouping, naturally leading to a hierarchical clustering.","Put together, our method takes view-inconsistent multi-granularity 2D segmentations as input and produces a hierarchy of 3D-consistent segmentations as output.","We evaluate our method and several baselines on synthetic datasets with multi-view images and multi-granular segmentation, showcasing improved accuracy and viewpoint-consistency.","We additionally provide qualitative examples of our model's 3D hierarchical segmentations in real world scenes.\\footnote{The code and dataset are available at:"],"url":"http://arxiv.org/abs/2405.19678v1","category":"cs.CV"}
{"created":"2024-05-30 04:11:17","title":"Large Language Model Watermark Stealing With Mixed Integer Programming","abstract":"The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.","sentences":["The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse.","The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold.","However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases.","Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing.","In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack.","We formalize the attack as a mixed integer programming problem with constraints.","We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme.","Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings."],"url":"http://arxiv.org/abs/2405.19677v1","category":"cs.CR"}
{"created":"2024-05-30 03:57:29","title":"Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models","abstract":"AI-driven design problems, such as DNA/protein sequence design, are commonly tackled from two angles: generative modeling, which efficiently captures the feasible design space (e.g., natural images or biological sequences), and model-based optimization, which utilizes reward models for extrapolation. To combine the strengths of both approaches, we adopt a hybrid method that fine-tunes cutting-edge diffusion models by optimizing reward models through RL. Although prior work has explored similar avenues, they primarily focus on scenarios where accurate reward models are accessible. In contrast, we concentrate on an offline setting where a reward model is unknown, and we must learn from static offline datasets, a common scenario in scientific domains. In offline scenarios, existing approaches tend to suffer from overoptimization, as they may be misled by the reward model in out-of-distribution regions. To address this, we introduce a conservative fine-tuning approach, BRAID, by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions. Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models.","sentences":["AI-driven design problems, such as DNA/protein sequence design, are commonly tackled from two angles: generative modeling, which efficiently captures the feasible design space (e.g., natural images or biological sequences), and model-based optimization, which utilizes reward models for extrapolation.","To combine the strengths of both approaches, we adopt a hybrid method that fine-tunes cutting-edge diffusion models by optimizing reward models through RL.","Although prior work has explored similar avenues, they primarily focus on scenarios where accurate reward models are accessible.","In contrast, we concentrate on an offline setting where a reward model is unknown, and we must learn from static offline datasets, a common scenario in scientific domains.","In offline scenarios, existing approaches tend to suffer from overoptimization, as they may be misled by the reward model in out-of-distribution regions.","To address this, we introduce a conservative fine-tuning approach, BRAID, by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions.","Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models."],"url":"http://arxiv.org/abs/2405.19673v1","category":"cs.LG"}
{"created":"2024-05-30 03:18:30","title":"Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D Gaussian","abstract":"3D Gaussian splatting has demonstrated impressive performance in real-time novel view synthesis. However, achieving successful reconstruction from RGB images generally requires multiple input views captured under static conditions. To address the challenge of sparse input views, previous approaches have incorporated depth supervision into the training of 3D Gaussians to mitigate overfitting, using dense predictions from pretrained depth networks as pseudo-ground truth. Nevertheless, depth predictions from monocular depth estimation models inherently exhibit significant uncertainty in specific areas. Relying solely on pixel-wise L2 loss may inadvertently incorporate detrimental noise from these uncertain areas. In this work, we introduce a novel method to supervise the depth distribution of 3D Gaussians, utilizing depth priors with integrated uncertainty estimates. To address these localized errors in depth predictions, we integrate a patch-wise optimal transport strategy to complement traditional L2 loss in depth supervision. Extensive experiments conducted on the LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT, achieves superior novel view synthesis and consistently outperforms state-of-the-art methods.","sentences":["3D Gaussian splatting has demonstrated impressive performance in real-time novel view synthesis.","However, achieving successful reconstruction from RGB images generally requires multiple input views captured under static conditions.","To address the challenge of sparse input views, previous approaches have incorporated depth supervision into the training of 3D Gaussians to mitigate overfitting, using dense predictions from pretrained depth networks as pseudo-ground truth.","Nevertheless, depth predictions from monocular depth estimation models inherently exhibit significant uncertainty in specific areas.","Relying solely on pixel-wise L2 loss may inadvertently incorporate detrimental noise from these uncertain areas.","In this work, we introduce a novel method to supervise the depth distribution of 3D Gaussians, utilizing depth priors with integrated uncertainty estimates.","To address these localized errors in depth predictions, we integrate a patch-wise optimal transport strategy to complement traditional L2 loss in depth supervision.","Extensive experiments conducted on the LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT, achieves superior novel view synthesis and consistently outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.19657v1","category":"cs.CV"}
{"created":"2024-05-30 03:15:59","title":"Accurate and Reliable Predictions with Mutual-Transport Ensemble","abstract":"Deep Neural Networks (DNNs) have achieved remarkable success in a variety of tasks, especially when it comes to prediction accuracy. However, in complex real-world scenarios, particularly in safety-critical applications, high accuracy alone is not enough. Reliable uncertainty estimates are crucial. Modern DNNs, often trained with cross-entropy loss, tend to be overconfident, especially with ambiguous samples. To improve uncertainty calibration, many techniques have been developed, but they often compromise prediction accuracy. To tackle this challenge, we propose the ``mutual-transport ensemble'' (MTE). This approach introduces a co-trained auxiliary model and adaptively regularizes the cross-entropy loss using Kullback-Leibler (KL) divergence between the prediction distributions of the primary and auxiliary models. We conducted extensive studies on various benchmarks to validate the effectiveness of our method. The results show that MTE can simultaneously enhance both accuracy and uncertainty calibration. For example, on the CIFAR-100 dataset, our MTE method on ResNet34/50 achieved significant improvements compared to previous state-of-the-art method, with absolute accuracy increases of 2.4%/3.7%, relative reductions in ECE of $42.3%/29.4%, and relative reductions in classwise-ECE of 11.6%/15.3%.","sentences":["Deep Neural Networks (DNNs) have achieved remarkable success in a variety of tasks, especially when it comes to prediction accuracy.","However, in complex real-world scenarios, particularly in safety-critical applications, high accuracy alone is not enough.","Reliable uncertainty estimates are crucial.","Modern DNNs, often trained with cross-entropy loss, tend to be overconfident, especially with ambiguous samples.","To improve uncertainty calibration, many techniques have been developed, but they often compromise prediction accuracy.","To tackle this challenge, we propose the ``mutual-transport ensemble'' (MTE).","This approach introduces a co-trained auxiliary model and adaptively regularizes the cross-entropy loss using Kullback-Leibler (KL) divergence between the prediction distributions of the primary and auxiliary models.","We conducted extensive studies on various benchmarks to validate the effectiveness of our method.","The results show that MTE can simultaneously enhance both accuracy and uncertainty calibration.","For example, on the CIFAR-100 dataset, our MTE method on ResNet34/50 achieved significant improvements compared to previous state-of-the-art method, with absolute accuracy increases of 2.4%/3.7%, relative reductions in ECE of $42.3%/29.4%, and relative reductions in classwise-ECE of 11.6%/15.3%."],"url":"http://arxiv.org/abs/2405.19656v1","category":"cs.AI"}
{"created":"2024-05-30 03:15:09","title":"Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training","abstract":"Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf multi-modal medical datasets, most existing methods have not thoroughly tapped into such extensive supervision signals. In this paper, we introduce the Med-ST framework for fine-grained spatial and temporal modeling to exploit information from multiple spatial views of chest radiographs and temporal historical records. For spatial modeling, Med-ST employs the Mixture of View Expert (MoVE) architecture to integrate different visual features from both frontal and lateral views. To achieve a more comprehensive alignment, Med-ST not only establishes the global alignment between whole images and texts but also introduces modality-weighted local alignment between text tokens and spatial regions of images. For temporal modeling, we propose a novel cross-modal bidirectional cycle consistency objective by forward mapping classification (FMC) and reverse mapping regression (RMR). By perceiving temporal information from simple to complex, Med-ST can learn temporal semantics. Experimental results across four distinct tasks demonstrate the effectiveness of Med-ST, especially in temporal classification tasks. Our code and model are available at https://github.com/SVT-Yang/MedST.","sentences":["Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports.","Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf multi-modal medical datasets, most existing methods have not thoroughly tapped into such extensive supervision signals.","In this paper, we introduce the Med-ST framework for fine-grained spatial and temporal modeling to exploit information from multiple spatial views of chest radiographs and temporal historical records.","For spatial modeling, Med-ST employs the Mixture of View Expert (MoVE) architecture to integrate different visual features from both frontal and lateral views.","To achieve a more comprehensive alignment, Med-ST not only establishes the global alignment between whole images and texts but also introduces modality-weighted local alignment between text tokens and spatial regions of images.","For temporal modeling, we propose a novel cross-modal bidirectional cycle consistency objective by forward mapping classification (FMC) and reverse mapping regression (RMR).","By perceiving temporal information from simple to complex, Med-ST can learn temporal semantics.","Experimental results across four distinct tasks demonstrate the effectiveness of Med-ST, especially in temporal classification tasks.","Our code and model are available at https://github.com/SVT-Yang/MedST."],"url":"http://arxiv.org/abs/2405.19654v1","category":"cs.AI"}
{"created":"2024-05-30 03:04:57","title":"Few for Many: Tchebycheff Set Scalarization for Many-Objective Optimization","abstract":"Multi-objective optimization can be found in many real-world applications where some conflicting objectives can not be optimized by a single solution. Existing optimization methods often focus on finding a set of Pareto solutions with different optimal trade-offs among the objectives. However, the required number of solutions to well approximate the whole Pareto optimal set could be exponentially large with respect to the number of objectives, which makes these methods unsuitable for handling many optimization objectives. In this work, instead of finding a dense set of Pareto solutions, we propose a novel Tchebycheff set scalarization method to find a few representative solutions (e.g., 5) to cover a large number of objectives (e.g., $>100$) in a collaborative and complementary manner. In this way, each objective can be well addressed by at least one solution in the small solution set. In addition, we further develop a smooth Tchebycheff set scalarization approach for efficient optimization with good theoretical guarantees. Experimental studies on different problems with many optimization objectives demonstrate the effectiveness of our proposed method.","sentences":["Multi-objective optimization can be found in many real-world applications where some conflicting objectives can not be optimized by a single solution.","Existing optimization methods often focus on finding a set of Pareto solutions with different optimal trade-offs among the objectives.","However, the required number of solutions to well approximate the whole Pareto optimal set could be exponentially large with respect to the number of objectives, which makes these methods unsuitable for handling many optimization objectives.","In this work, instead of finding a dense set of Pareto solutions, we propose a novel Tchebycheff set scalarization method to find a few representative solutions (e.g., 5) to cover a large number of objectives (e.g., $>100$) in a collaborative and complementary manner.","In this way, each objective can be well addressed by at least one solution in the small solution set.","In addition, we further develop a smooth Tchebycheff set scalarization approach for efficient optimization with good theoretical guarantees.","Experimental studies on different problems with many optimization objectives demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2405.19650v1","category":"cs.LG"}
{"created":"2024-05-30 03:00:47","title":"Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach","abstract":"Concerns regarding the propensity of Large Language Models (LLMs) to produce inaccurate outputs, also known as hallucinations, have escalated. Detecting them is vital for ensuring the reliability of applications relying on LLM-generated content. Current methods often demand substantial resources and rely on extensive LLMs or employ supervised learning with multidimensional features or intricate linguistic and semantic analyses difficult to reproduce and largely depend on using the same LLM that hallucinated. This paper introduces a supervised learning approach employing two simple classifiers utilizing only four numerical features derived from tokens and vocabulary probabilities obtained from other LLM evaluators, which are not necessarily the same. The method yields promising results, surpassing state-of-the-art outcomes in multiple tasks across three different benchmarks. Additionally, we provide a comprehensive examination of the strengths and weaknesses of our approach, highlighting the significance of the features utilized and the LLM employed as an evaluator. We have released our code publicly at https://github.com/Baylor-AI/HalluDetect.","sentences":["Concerns regarding the propensity of Large Language Models (LLMs) to produce inaccurate outputs, also known as hallucinations, have escalated.","Detecting them is vital for ensuring the reliability of applications relying on LLM-generated content.","Current methods often demand substantial resources and rely on extensive LLMs or employ supervised learning with multidimensional features or intricate linguistic and semantic analyses difficult to reproduce and largely depend on using the same LLM that hallucinated.","This paper introduces a supervised learning approach employing two simple classifiers utilizing only four numerical features derived from tokens and vocabulary probabilities obtained from other LLM evaluators, which are not necessarily the same.","The method yields promising results, surpassing state-of-the-art outcomes in multiple tasks across three different benchmarks.","Additionally, we provide a comprehensive examination of the strengths and weaknesses of our approach, highlighting the significance of the features utilized and the LLM employed as an evaluator.","We have released our code publicly at https://github.com/Baylor-AI/HalluDetect."],"url":"http://arxiv.org/abs/2405.19648v1","category":"cs.CL"}
{"created":"2024-05-30 02:53:19","title":"EgoSurgery-Phase: A Dataset of Surgical Phase Recognition from Egocentric Open Surgery Videos","abstract":"Surgical phase recognition has gained significant attention due to its potential to offer solutions to numerous demands of the modern operating room. However, most existing methods concentrate on minimally invasive surgery (MIS), leaving surgical phase recognition for open surgery understudied. This discrepancy is primarily attributed to the scarcity of publicly available open surgery video datasets for surgical phase recognition. To address this issue, we introduce a new egocentric open surgery video dataset for phase recognition, named EgoSurgery-Phase. This dataset comprises 15 hours of real open surgery videos spanning 9 distinct surgical phases all captured using an egocentric camera attached to the surgeon's head. In addition to video, the EgoSurgery-Phase offers eye gaze. As far as we know, it is the first real open surgery video dataset for surgical phase recognition publicly available. Furthermore, inspired by the notable success of masked autoencoders (MAEs) in video understanding tasks (e.g., action recognition), we propose a gaze-guided masked autoencoder (GGMAE). Considering the regions where surgeons' gaze focuses are often critical for surgical phase recognition (e.g., surgical field), in our GGMAE, the gaze information acts as an empirical semantic richness prior to guiding the masking process, promoting better attention to semantically rich spatial regions. GGMAE significantly improves the previous state-of-the-art recognition method (6.4% in Jaccard) and the masked autoencoder-based method (3.1% in Jaccard) on EgoSurgery-Phase. The dataset will be released at https://github.com/Fujiry0/EgoSurgery.","sentences":["Surgical phase recognition has gained significant attention due to its potential to offer solutions to numerous demands of the modern operating room.","However, most existing methods concentrate on minimally invasive surgery (MIS), leaving surgical phase recognition for open surgery understudied.","This discrepancy is primarily attributed to the scarcity of publicly available open surgery video datasets for surgical phase recognition.","To address this issue, we introduce a new egocentric open surgery video dataset for phase recognition, named EgoSurgery-Phase.","This dataset comprises 15 hours of real open surgery videos spanning 9 distinct surgical phases all captured using an egocentric camera attached to the surgeon's head.","In addition to video, the EgoSurgery-Phase offers eye gaze.","As far as we know, it is the first real open surgery video dataset for surgical phase recognition publicly available.","Furthermore, inspired by the notable success of masked autoencoders (MAEs) in video understanding tasks (e.g., action recognition), we propose a gaze-guided masked autoencoder (GGMAE).","Considering the regions where surgeons' gaze focuses are often critical for surgical phase recognition (e.g., surgical field), in our GGMAE, the gaze information acts as an empirical semantic richness prior to guiding the masking process, promoting better attention to semantically rich spatial regions.","GGMAE significantly improves the previous state-of-the-art recognition method (6.4% in Jaccard) and the masked autoencoder-based method (3.1% in Jaccard) on EgoSurgery-Phase.","The dataset will be released at https://github.com/Fujiry0/EgoSurgery."],"url":"http://arxiv.org/abs/2405.19644v1","category":"cs.CV"}
{"created":"2024-05-30 02:51:29","title":"Few-shot fault diagnosis based on multi-scale graph convolution filtering for industry","abstract":"Industrial equipment fault diagnosis often encounter challenges such as the scarcity of fault data, complex operating conditions, and varied types of failures. Signal analysis, data statistical learning, and conventional deep learning techniques face constraints under these conditions due to their substantial data requirements and the necessity for transfer learning to accommodate new failure modes. To effectively leverage information and extract the intrinsic characteristics of faults across different domains under limited sample conditions, this paper introduces a fault diagnosis approach employing Multi-Scale Graph Convolution Filtering (MSGCF). MSGCF enhances the traditional Graph Neural Network (GNN) framework by integrating both local and global information fusion modules within the graph convolution filter block. This advancement effectively mitigates the over-smoothing issue associated with excessive layering of graph convolutional layers while preserving a broad receptive field. It also reduces the risk of overfitting in few-shot diagnosis, thereby augmenting the model's representational capacity. Experiments on the University of Paderborn bearing dataset (PU) demonstrate that the MSGCF method proposed herein surpasses alternative approaches in accuracy, thereby offering valuable insights for industrial fault diagnosis in few-shot learning scenarios.","sentences":["Industrial equipment fault diagnosis often encounter challenges such as the scarcity of fault data, complex operating conditions, and varied types of failures.","Signal analysis, data statistical learning, and conventional deep learning techniques face constraints under these conditions due to their substantial data requirements and the necessity for transfer learning to accommodate new failure modes.","To effectively leverage information and extract the intrinsic characteristics of faults across different domains under limited sample conditions, this paper introduces a fault diagnosis approach employing Multi-Scale Graph Convolution Filtering (MSGCF).","MSGCF enhances the traditional Graph Neural Network (GNN) framework by integrating both local and global information fusion modules within the graph convolution filter block.","This advancement effectively mitigates the over-smoothing issue associated with excessive layering of graph convolutional layers while preserving a broad receptive field.","It also reduces the risk of overfitting in few-shot diagnosis, thereby augmenting the model's representational capacity.","Experiments on the University of Paderborn bearing dataset (PU) demonstrate that the MSGCF method proposed herein surpasses alternative approaches in accuracy, thereby offering valuable insights for industrial fault diagnosis in few-shot learning scenarios."],"url":"http://arxiv.org/abs/2405.19642v1","category":"cs.AI"}
{"created":"2024-05-30 02:48:00","title":"Reconciling Safety Measurement and Dynamic Assurance","abstract":"We propose a new framework to facilitate dynamic assurance within a safety case approach by associating safety performance measurement with the core assurance artifacts of a safety case. The focus is mainly on the safety architecture, whose underlying risk assessment model gives the concrete link from safety measurement to operational risk. Using an aviation domain example of autonomous taxiing, we describe our approach to derive safety indicators and revise the risk assessment based on safety measurement. We then outline a notion of consistency between a collection of safety indicators and the safety case, as a formal basis for implementing the proposed framework in our tool, AdvoCATE.","sentences":["We propose a new framework to facilitate dynamic assurance within a safety case approach by associating safety performance measurement with the core assurance artifacts of a safety case.","The focus is mainly on the safety architecture, whose underlying risk assessment model gives the concrete link from safety measurement to operational risk.","Using an aviation domain example of autonomous taxiing, we describe our approach to derive safety indicators and revise the risk assessment based on safety measurement.","We then outline a notion of consistency between a collection of safety indicators and the safety case, as a formal basis for implementing the proposed framework in our tool, AdvoCATE."],"url":"http://arxiv.org/abs/2405.19641v1","category":"cs.SE"}
{"created":"2024-05-30 02:33:28","title":"Leveraging Open-Source Large Language Models for encoding Social Determinants of Health using an Intelligent Router","abstract":"Social Determinants of Health (SDOH) play a significant role in patient health outcomes. The Center of Disease Control (CDC) introduced a subset of ICD-10 codes called Z-codes in an attempt to officially recognize and measure SDOH in the health care system. However, these codes are rarely annotated in a patient's Electronic Health Record (EHR), and instead, in many cases, need to be inferred from clinical notes. Previous research has shown that large language models (LLMs) show promise on extracting unstructured data from EHRs. However, with thousands of models to choose from with unique architectures and training sets, it's difficult to choose one model that performs the best on coding tasks. Further, clinical notes contain trusted health information making the use of closed-source language models from commercial vendors difficult, so the identification of open source LLMs that can be run within health organizations and exhibits high performance on SDOH tasks is an urgent problem. Here, we introduce an intelligent routing system for SDOH coding that uses a language model router to direct medical record data to open source LLMs that demonstrate optimal performance on specific SDOH codes. The intelligent routing system exhibits state of the art performance of 97.4% accuracy averaged across 5 codes, including homelessness and food insecurity, on par with closed models such as GPT-4o. In order to train the routing system and validate models, we also introduce a synthetic data generation and validation paradigm to increase the scale of training data without needing privacy protected medical records. Together, we demonstrate an architecture for intelligent routing of inputs to task-optimal language models to achieve high performance across a set of medical coding sub-tasks.","sentences":["Social Determinants of Health (SDOH) play a significant role in patient health outcomes.","The Center of Disease Control (CDC) introduced a subset of ICD-10 codes called Z-codes in an attempt to officially recognize and measure SDOH in the health care system.","However, these codes are rarely annotated in a patient's Electronic Health Record (EHR), and instead, in many cases, need to be inferred from clinical notes.","Previous research has shown that large language models (LLMs) show promise on extracting unstructured data from EHRs.","However, with thousands of models to choose from with unique architectures and training sets, it's difficult to choose one model that performs the best on coding tasks.","Further, clinical notes contain trusted health information making the use of closed-source language models from commercial vendors difficult, so the identification of open source LLMs that can be run within health organizations and exhibits high performance on SDOH tasks is an urgent problem.","Here, we introduce an intelligent routing system for SDOH coding that uses a language model router to direct medical record data to open source LLMs that demonstrate optimal performance on specific SDOH codes.","The intelligent routing system exhibits state of the art performance of 97.4% accuracy averaged across 5 codes, including homelessness and food insecurity, on par with closed models such as GPT-4o.","In order to train the routing system and validate models, we also introduce a synthetic data generation and validation paradigm to increase the scale of training data without needing privacy protected medical records.","Together, we demonstrate an architecture for intelligent routing of inputs to task-optimal language models to achieve high performance across a set of medical coding sub-tasks."],"url":"http://arxiv.org/abs/2405.19631v1","category":"cs.AI"}
{"created":"2024-05-30 02:09:51","title":"Easy Problems That LLMs Get Wrong","abstract":"We introduce a comprehensive Linguistic Benchmark designed to evaluate the limitations of Large Language Models (LLMs) in domains such as logical reasoning, spatial intelligence, and linguistic understanding, among others. Through a series of straightforward questions, it uncovers the significant limitations of well-regarded models to perform tasks that humans manage with ease. It also highlights the potential of prompt engineering to mitigate some errors and underscores the necessity for better training methodologies. Our findings stress the importance of grounding LLMs with human reasoning and common sense, emphasising the need for human-in-the-loop for enterprise applications. We hope this work paves the way for future research to enhance the usefulness and reliability of new models.","sentences":["We introduce a comprehensive Linguistic Benchmark designed to evaluate the limitations of Large Language Models (LLMs) in domains such as logical reasoning, spatial intelligence, and linguistic understanding, among others.","Through a series of straightforward questions, it uncovers the significant limitations of well-regarded models to perform tasks that humans manage with ease.","It also highlights the potential of prompt engineering to mitigate some errors and underscores the necessity for better training methodologies.","Our findings stress the importance of grounding LLMs with human reasoning and common sense, emphasising the need for human-in-the-loop for enterprise applications.","We hope this work paves the way for future research to enhance the usefulness and reliability of new models."],"url":"http://arxiv.org/abs/2405.19616v1","category":"cs.AI"}
{"created":"2024-05-30 02:08:58","title":"Enhancing Exoplanet Ephemerides by Leveraging Professional and Citizen Science Data: A Test Case with WASP-77A b","abstract":"We present an updated ephemeris and physical parameters for the exoplanet WASP-77 A b. In this effort, we combine 64 ground- and space-based transit observations, 6 space-based eclipse observations, and 32 radial velocity observations to produce the most precise orbital solution to date for this target, aiding in the planning of James Webb Space Telescope (JWST) and Ariel observations and atmospheric studies. We report a new orbital period of 1.360029395 +- 5.7e-8 days, a new mid-transit time of 2459957.337860 +- 4.3e-5 BJDTDB (Barycentric Julian Date in the Barycentric Dynamical Time scale; arXiv:1005.4415) and a new mid-eclipse time of 2459956.658192 +- 6.7e-5 BJDTDB. Furthermore, the methods presented in this study reduce the uncertainties in the planet mass to 1.6654 +- 4.5e-3 Mjup and orbital period to 1.360029395 +- 5.7e-8 days by factors of 15.1 and 10.9, respectively. Through a joint fit analysis comparison of transit data taken by space-based and citizen science-led initiatives, our study demonstrates the power of including data collected by citizen scientists compared to a fit of the space-based data alone. Additionally, by including a vast array of citizen science data from ExoClock, Exoplanet Transit Database (ETD), and Exoplanet Watch, we can increase our observational baseline and thus acquire better constraints on the forward propagation of our ephemeris than what is achievable with TESS data alone.","sentences":["We present an updated ephemeris and physical parameters for the exoplanet WASP-77","A b.","In this effort, we combine 64 ground- and space-based transit observations, 6 space-based eclipse observations, and 32 radial velocity observations to produce the most precise orbital solution to date for this target, aiding in the planning of James Webb Space Telescope (JWST) and Ariel observations and atmospheric studies.","We report a new orbital period of 1.360029395 +- 5.7e-8 days, a new mid-transit time of 2459957.337860 +- 4.3e-5 BJDTDB (Barycentric Julian Date in the Barycentric Dynamical Time scale; arXiv:1005.4415) and a new mid-eclipse time of 2459956.658192 +- 6.7e-5 BJDTDB.","Furthermore, the methods presented in this study reduce the uncertainties in the planet mass to 1.6654 +- 4.5e-3 Mjup and orbital period to 1.360029395 +- 5.7e-8 days by factors of 15.1 and 10.9, respectively.","Through a joint fit analysis comparison of transit data taken by space-based and citizen science-led initiatives, our study demonstrates the power of including data collected by citizen scientists compared to a fit of the space-based data alone.","Additionally, by including a vast array of citizen science data from ExoClock, Exoplanet Transit Database (ETD), and Exoplanet Watch, we can increase our observational baseline and thus acquire better constraints on the forward propagation of our ephemeris than what is achievable with TESS data alone."],"url":"http://arxiv.org/abs/2405.19615v1","category":"astro-ph.EP"}
{"created":"2024-05-30 01:47:27","title":"Relation Modeling and Distillation for Learning with Noisy Labels","abstract":"Learning with noisy labels has become an effective strategy for enhancing the robustness of models, which enables models to better tolerate inaccurate data. Existing methods either focus on optimizing the loss function to mitigate the interference from noise, or design procedures to detect potential noise and correct errors. However, their effectiveness is often compromised in representation learning due to the dilemma where models overfit to noisy labels. To address this issue, this paper proposes a relation modeling and distillation framework that models inter-sample relationships via self-supervised learning and employs knowledge distillation to enhance understanding of latent associations, which mitigate the impact of noisy labels. Specifically, the proposed method, termed RMDNet, includes two main modules, where the relation modeling (RM) module implements the contrastive learning technique to learn representations of all data, an unsupervised approach that effectively eliminates the interference of noisy tags on feature extraction. The relation-guided representation learning (RGRL) module utilizes inter-sample relation learned from the RM module to calibrate the representation distribution for noisy samples, which is capable of improving the generalization of the model in the inference phase. Notably, the proposed RMDNet is a plug-and-play framework that can integrate multiple methods to its advantage. Extensive experiments were conducted on two datasets, including performance comparison, ablation study, in-depth analysis and case study. The results show that RMDNet can learn discriminative representations for noisy data, which results in superior performance than the existing methods.","sentences":["Learning with noisy labels has become an effective strategy for enhancing the robustness of models, which enables models to better tolerate inaccurate data.","Existing methods either focus on optimizing the loss function to mitigate the interference from noise, or design procedures to detect potential noise and correct errors.","However, their effectiveness is often compromised in representation learning due to the dilemma where models overfit to noisy labels.","To address this issue, this paper proposes a relation modeling and distillation framework that models inter-sample relationships via self-supervised learning and employs knowledge distillation to enhance understanding of latent associations, which mitigate the impact of noisy labels.","Specifically, the proposed method, termed RMDNet, includes two main modules, where the relation modeling (RM) module implements the contrastive learning technique to learn representations of all data, an unsupervised approach that effectively eliminates the interference of noisy tags on feature extraction.","The relation-guided representation learning (RGRL) module utilizes inter-sample relation learned from the RM module to calibrate the representation distribution for noisy samples, which is capable of improving the generalization of the model in the inference phase.","Notably, the proposed RMDNet is a plug-and-play framework that can integrate multiple methods to its advantage.","Extensive experiments were conducted on two datasets, including performance comparison, ablation study, in-depth analysis and case study.","The results show that RMDNet can learn discriminative representations for noisy data, which results in superior performance than the existing methods."],"url":"http://arxiv.org/abs/2405.19606v1","category":"cs.AI"}
{"created":"2024-05-30 01:30:34","title":"Do spectral cues matter in contrast-based graph self-supervised learning?","abstract":"The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues. However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions or heuristic approaches regarding the spectral domain demonstrate notable enhancements in learning performance. This paradox prompts a critical inquiry into the genuine contribution of spectral information to contrast-based graph self-supervised learning. This study undertakes an extensive investigation into this inquiry, conducting a thorough study of the relationship between spectral characteristics and the learning outcomes of contemporary methodologies. Based on this analysis, we claim that the effectiveness and significance of spectral information need to be questioned. Instead, we revisit simple edge perturbation: random edge dropping designed for node-level self-supervised learning and random edge adding intended for graph-level self-supervised learning. Compelling evidence is presented that these simple yet effective strategies consistently yield superior performance while demanding significantly fewer computational resources compared to all prior spectral augmentation methods. The proposed insights represent a significant leap forward in the field, potentially reshaping the understanding and implementation of graph self-supervised learning.","sentences":["The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues.","However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions or heuristic approaches regarding the spectral domain demonstrate notable enhancements in learning performance.","This paradox prompts a critical inquiry into the genuine contribution of spectral information to contrast-based graph self-supervised learning.","This study undertakes an extensive investigation into this inquiry, conducting a thorough study of the relationship between spectral characteristics and the learning outcomes of contemporary methodologies.","Based on this analysis, we claim that the effectiveness and significance of spectral information need to be questioned.","Instead, we revisit simple edge perturbation: random edge dropping designed for node-level self-supervised learning and random edge adding intended for graph-level self-supervised learning.","Compelling evidence is presented that these simple yet effective strategies consistently yield superior performance while demanding significantly fewer computational resources compared to all prior spectral augmentation methods.","The proposed insights represent a significant leap forward in the field, potentially reshaping the understanding and implementation of graph self-supervised learning."],"url":"http://arxiv.org/abs/2405.19600v1","category":"cs.LG"}
{"created":"2024-05-30 01:27:43","title":"SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors","abstract":"Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights \\(W\\) and inject learnable matrices \\(\\Delta W\\). These \\(\\Delta W\\) matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically show a performance gap compared to full fine-tuning. Although recent PEFT methods have narrowed this gap, they do so at the cost of additional learnable parameters. We propose SVFT, a simple approach that fundamentally differs from existing methods: the structure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\). Specifically, SVFT updates \\(W\\) as a sparse combination of outer products of its singular vectors, training only the coefficients (scales) of these sparse combinations. This approach allows fine-grained control over expressivity through the number of coefficients. Extensive experiments on language and vision benchmarks show that SVFT recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of parameters, outperforming existing methods that only recover up to 85% performance using 0.03 to 0.8% of the trainable parameter budget.","sentences":["Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights \\(W\\) and inject learnable matrices \\(\\Delta W\\).","These \\(\\Delta W\\) matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors.","However, these methods typically show a performance gap compared to full fine-tuning.","Although recent PEFT methods have narrowed this gap, they do so at the cost of additional learnable parameters.","We propose SVFT, a simple approach that fundamentally differs from existing methods: the structure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\).","Specifically, SVFT updates \\(W\\) as a sparse combination of outer products of its singular vectors, training only the coefficients (scales) of these sparse combinations.","This approach allows fine-grained control over expressivity through the number of coefficients.","Extensive experiments on language and vision benchmarks show that SVFT recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of parameters, outperforming existing methods that only recover up to 85% performance using 0.03 to 0.8% of the trainable parameter budget."],"url":"http://arxiv.org/abs/2405.19597v1","category":"cs.LG"}
{"created":"2024-05-30 01:18:50","title":"The RSNA Abdominal Traumatic Injury CT (RATIC) Dataset","abstract":"The RSNA Abdominal Traumatic Injury CT (RATIC) dataset is the largest publicly available collection of adult abdominal CT studies annotated for traumatic injuries. This dataset includes 4,274 studies from 23 institutions across 14 countries. The dataset is freely available for non-commercial use via Kaggle at https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection. Created for the RSNA 2023 Abdominal Trauma Detection competition, the dataset encourages the development of advanced machine learning models for detecting abdominal injuries on CT scans. The dataset encompasses detection and classification of traumatic injuries across multiple organs, including the liver, spleen, kidneys, bowel, and mesentery. Annotations were created by expert radiologists from the American Society of Emergency Radiology (ASER) and Society of Abdominal Radiology (SAR). The dataset is annotated at multiple levels, including the presence of injuries in three solid organs with injury grading, image-level annotations for active extravasations and bowel injury, and voxelwise segmentations of each of the potentially injured organs. With the release of this dataset, we hope to facilitate research and development in machine learning and abdominal trauma that can lead to improved patient care and outcomes.","sentences":["The RSNA Abdominal Traumatic Injury CT (RATIC) dataset is the largest publicly available collection of adult abdominal CT studies annotated for traumatic injuries.","This dataset includes 4,274 studies from 23 institutions across 14 countries.","The dataset is freely available for non-commercial use via Kaggle at https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection.","Created for the RSNA 2023 Abdominal Trauma Detection competition, the dataset encourages the development of advanced machine learning models for detecting abdominal injuries on CT scans.","The dataset encompasses detection and classification of traumatic injuries across multiple organs, including the liver, spleen, kidneys, bowel, and mesentery.","Annotations were created by expert radiologists from the American Society of Emergency Radiology (ASER) and Society of Abdominal Radiology (SAR).","The dataset is annotated at multiple levels, including the presence of injuries in three solid organs with injury grading, image-level annotations for active extravasations and bowel injury, and voxelwise segmentations of each of the potentially injured organs.","With the release of this dataset, we hope to facilitate research and development in machine learning and abdominal trauma that can lead to improved patient care and outcomes."],"url":"http://arxiv.org/abs/2405.19595v1","category":"cs.CV"}
{"created":"2024-05-30 01:11:35","title":"Why Larger Language Models Do In-context Learning Differently?","abstract":"Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of in-context learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis.","sentences":["Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of in-context learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters.","One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context.","This work studies this observation theoretically aiming to improve the understanding of LLM and ICL.","We analyze two stylized settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model).","In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors.","This sheds light on where transformers pay attention to and how that affects ICL.","Preliminary experimental results on large base and chat models provide positive support for our analysis."],"url":"http://arxiv.org/abs/2405.19592v1","category":"cs.LG"}
{"created":"2024-05-30 00:57:06","title":"Weights Augmentation: it has never ever ever ever let her model down","abstract":"Weight play an essential role in deep learning network models. Unlike network structure design, this article proposes the concept of weight augmentation, focusing on weight exploration. The core of Weight Augmentation Strategy (WAS) is to adopt random transformed weight coefficients training and transformed coefficients, named Shadow Weight(SW), for networks that can be used to calculate loss function to affect parameter updates. However, stochastic gradient descent is applied to Plain Weight(PW), which is referred to as the original weight of the network before the random transformation. During training, numerous SW collectively form high-dimensional space, while PW is directly learned from the distribution of SW instead of the data. The weight of the accuracy-oriented mode(AOM) relies on PW, which guarantees the network is highly robust and accurate. The desire-oriented mode(DOM) weight uses SW, which is determined by the network model's unique functions based on WAT's performance desires, such as lower computational complexity, lower sensitivity to particular data, etc. The dual mode be switched at anytime if needed. WAT extends the augmentation technique from data augmentation to weight, and it is easy to understand and implement, but it can improve almost all networks amazingly. Our experimental results show that convolutional neural networks, such as VGG-16, ResNet-18, ResNet-34, GoogleNet, MobilementV2, and Efficientment-Lite, can benefit much at little or no cost. The accuracy of models is on the CIFAR100 and CIFAR10 datasets, which can be evaluated to increase by 7.32\\% and 9.28\\%, respectively, with the highest values being 13.42\\% and 18.93\\%, respectively. In addition, DOM can reduce floating point operations (FLOPs) by up to 36.33\\%. The code is available at https://github.com/zlearh/Weight-Augmentation-Technology.","sentences":["Weight play an essential role in deep learning network models.","Unlike network structure design, this article proposes the concept of weight augmentation, focusing on weight exploration.","The core of Weight Augmentation Strategy (WAS) is to adopt random transformed weight coefficients training and transformed coefficients, named Shadow Weight(SW), for networks that can be used to calculate loss function to affect parameter updates.","However, stochastic gradient descent is applied to Plain Weight(PW), which is referred to as the original weight of the network before the random transformation.","During training, numerous SW collectively form high-dimensional space, while PW is directly learned from the distribution of SW instead of the data.","The weight of the accuracy-oriented mode(AOM) relies on PW, which guarantees the network is highly robust and accurate.","The desire-oriented mode(DOM) weight uses SW, which is determined by the network model's unique functions based on WAT's performance desires, such as lower computational complexity, lower sensitivity to particular data, etc.","The dual mode be switched at anytime if needed.","WAT extends the augmentation technique from data augmentation to weight, and it is easy to understand and implement, but it can improve almost all networks amazingly.","Our experimental results show that convolutional neural networks, such as VGG-16, ResNet-18, ResNet-34, GoogleNet, MobilementV2, and Efficientment-Lite, can benefit much at little or no cost.","The accuracy of models is on the CIFAR100 and CIFAR10 datasets, which can be evaluated to increase by 7.32\\% and 9.28\\%, respectively, with the highest values being 13.42\\% and 18.93\\%, respectively.","In addition, DOM can reduce floating point operations (FLOPs) by up to 36.33\\%.","The code is available at https://github.com/zlearh/Weight-Augmentation-Technology."],"url":"http://arxiv.org/abs/2405.19590v1","category":"cs.LG"}
{"created":"2024-05-30 00:17:44","title":"Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases","abstract":"Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.","sentences":["Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap.","Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE.","However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance.","Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively.","In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis.","Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context.","This additional context enables black-box LLMs to enhance recovery accuracy.","We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively.","These results highlight the effectiveness of our approach in automating and improving binary code analysis."],"url":"http://arxiv.org/abs/2405.19581v1","category":"cs.SE"}
{"created":"2024-05-29 23:45:42","title":"A Deep Convolutional Neural Network-based Model for Aspect and Polarity Classification in Hausa Movie Reviews","abstract":"Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment nuances in text, especially across diverse languages and cultures. This paper introduces a novel Deep Convolutional Neural Network (CNN)-based model tailored for aspect and polarity classification in Hausa movie reviews, an underrepresented language in sentiment analysis research. A comprehensive Hausa ABSA dataset is created, filling a significant gap in resource availability. The dataset, preprocessed using sci-kit-learn for TF-IDF transformation, includes manually annotated aspect-level feature ontology words and sentiment polarity assignments. The proposed model combines CNNs with attention mechanisms for aspect-word prediction, leveraging contextual information and sentiment polarities. With 91% accuracy on aspect term extraction and 92% on sentiment polarity classification, the model outperforms traditional machine models, offering insights into specific aspects and sentiments. This study advances ABSA research, particularly in underrepresented languages, with implications for cross-cultural linguistic research.","sentences":["Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment nuances in text, especially across diverse languages and cultures.","This paper introduces a novel Deep Convolutional Neural Network (CNN)-based model tailored for aspect and polarity classification in Hausa movie reviews, an underrepresented language in sentiment analysis research.","A comprehensive Hausa ABSA dataset is created, filling a significant gap in resource availability.","The dataset, preprocessed using sci-kit-learn for TF-IDF transformation, includes manually annotated aspect-level feature ontology words and sentiment polarity assignments.","The proposed model combines CNNs with attention mechanisms for aspect-word prediction, leveraging contextual information and sentiment polarities.","With 91% accuracy on aspect term extraction and 92% on sentiment polarity classification, the model outperforms traditional machine models, offering insights into specific aspects and sentiments.","This study advances ABSA research, particularly in underrepresented languages, with implications for cross-cultural linguistic research."],"url":"http://arxiv.org/abs/2405.19575v1","category":"cs.CL"}
{"created":"2024-05-29 23:19:28","title":"Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding","abstract":"Vision-Language Models (VLM) can support clinicians by analyzing medical images and engaging in natural language interactions to assist in diagnostic and treatment tasks. However, VLMs often exhibit \"hallucinogenic\" behavior, generating textual outputs not grounded in contextual multimodal information. This challenge is particularly pronounced in the medical domain, where we do not only require VLM outputs to be accurate in single interactions but also to be consistent with clinical reasoning and diagnostic pathways throughout multi-turn conversations. For this purpose, we propose a new alignment algorithm that uses symbolic representations of clinical reasoning to ground VLMs in medical knowledge. These representations are utilized to (i) generate GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM conversations with demonstrations of clinical reasoning, and (ii) create an automatic reward function that evaluates the clinical validity of VLM generations throughout clinician-VLM interactions. Our algorithm eliminates the need for human involvement in training data generation or reward model construction, reducing costs compared to standard reinforcement learning with human feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a conversational VLM finetuned for analyzing bone marrow pathology slides, demonstrating strong performance in multi-turn medical conversations.","sentences":["Vision-Language Models (VLM) can support clinicians by analyzing medical images and engaging in natural language interactions to assist in diagnostic and treatment tasks.","However, VLMs often exhibit \"hallucinogenic\" behavior, generating textual outputs not grounded in contextual multimodal information.","This challenge is particularly pronounced in the medical domain, where we do not only require VLM outputs to be accurate in single interactions but also to be consistent with clinical reasoning and diagnostic pathways throughout multi-turn conversations.","For this purpose, we propose a new alignment algorithm that uses symbolic representations of clinical reasoning to ground VLMs in medical knowledge.","These representations are utilized to (i) generate GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM conversations with demonstrations of clinical reasoning, and (ii) create an automatic reward function that evaluates the clinical validity of VLM generations throughout clinician-VLM interactions.","Our algorithm eliminates the need for human involvement in training data generation or reward model construction, reducing costs compared to standard reinforcement learning with human feedback (RLHF).","We apply our alignment algorithm to develop Dr-LLaVA, a conversational VLM finetuned for analyzing bone marrow pathology slides, demonstrating strong performance in multi-turn medical conversations."],"url":"http://arxiv.org/abs/2405.19567v1","category":"cs.AI"}
{"created":"2024-05-29 23:06:54","title":"Quo Vadis ChatGPT? From Large Language Models to Large Knowledge Models","abstract":"The startling success of ChatGPT and other large language models (LLMs) using transformer-based generative neural network architecture in applications such as natural language processing and image synthesis has many researchers excited about potential opportunities in process systems engineering (PSE). The almost human-like performance of LLMs in these areas is indeed very impressive, surprising, and a major breakthrough. Their capabilities are very useful in certain tasks, such as writing first drafts of documents, code writing assistance, text summarization, etc. However, their success is limited in highly scientific domains as they cannot yet reason, plan, or explain due to their lack of in-depth domain knowledge. This is a problem in domains such as chemical engineering as they are governed by fundamental laws of physics and chemistry (and biology), constitutive relations, and highly technical knowledge about materials, processes, and systems. Although purely data-driven machine learning has its immediate uses, the long-term success of AI in scientific and engineering domains would depend on developing hybrid AI systems that use first principles and technical knowledge effectively. We call these hybrid AI systems Large Knowledge Models (LKMs), as they will not be limited to only NLP-based techniques or NLP-like applications. In this paper, we discuss the challenges and opportunities in developing such systems in chemical engineering.","sentences":["The startling success of ChatGPT and other large language models (LLMs) using transformer-based generative neural network architecture in applications such as natural language processing and image synthesis has many researchers excited about potential opportunities in process systems engineering (PSE).","The almost human-like performance of LLMs in these areas is indeed very impressive, surprising, and a major breakthrough.","Their capabilities are very useful in certain tasks, such as writing first drafts of documents, code writing assistance, text summarization, etc.","However, their success is limited in highly scientific domains as they cannot yet reason, plan, or explain due to their lack of in-depth domain knowledge.","This is a problem in domains such as chemical engineering as they are governed by fundamental laws of physics and chemistry (and biology), constitutive relations, and highly technical knowledge about materials, processes, and systems.","Although purely data-driven machine learning has its immediate uses, the long-term success of AI in scientific and engineering domains would depend on developing hybrid AI systems that use first principles and technical knowledge effectively.","We call these hybrid AI systems Large Knowledge Models (LKMs), as they will not be limited to only NLP-based techniques or NLP-like applications.","In this paper, we discuss the challenges and opportunities in developing such systems in chemical engineering."],"url":"http://arxiv.org/abs/2405.19561v1","category":"cs.AI"}
{"created":"2024-05-29 22:29:37","title":"Point process analysis of geographical diffusion of news in Argentina","abstract":"The diffusion of information plays a crucial role in a society, characterizing the diffusion process is challenging because it is highly non-stationary and varies with the media type. To understand the spreading of newspaper news in Argentina, we collected data from more than 27000 articles published in six main provinces during four months. We classified the articles into 20 thematic axes and obtained a set of 120 time series that capture daily newspaper attention on different topics in different provinces. To analyze the data we use a point process approach. For each topic, $n$, and for all pairs of provinces, $i$ and $j$, we use two measures to quantify the synchronicity of the events, $Q_s(i,j)$, which quantifies the number of events that occur almost simultaneously in $i$ and $j$, and $Q_a(i,j)$, which quantifies the direction of news spreading. We also analyze the dataset using well-known measures to detect correlations and dependencies, computed from the raw time series: undirected measures (linear cross-correlation, $CC$, and nonlinear mutual information, $MI$) and directed measures (linear Granger causality, $GC$, and nonlinear Transfer entropy, $TE$). Our analysis unveils how fast the information diffusion process is, as high values of $Q_{s}$, $CC$, and $MI$ reveal pairs of provinces with very similar and almost simultaneous temporal variations of media attention. On the other hand, $GC$ and $TE$ do not perform well in this context because they often return opposite directions of information transfer. We interpret this as due to three main factors: the characteristics of the data, which is highly non-stationary, the characteristics of the information diffusion process, which is very fast and probably acts at a sub-resolution time scale, and the action of large media companies that act as global, external drivers of information dissemination.","sentences":["The diffusion of information plays a crucial role in a society, characterizing the diffusion process is challenging because it is highly non-stationary and varies with the media type.","To understand the spreading of newspaper news in Argentina, we collected data from more than 27000 articles published in six main provinces during four months.","We classified the articles into 20 thematic axes and obtained a set of 120 time series that capture daily newspaper attention on different topics in different provinces.","To analyze the data we use a point process approach.","For each topic, $n$, and for all pairs of provinces, $i$ and $j$, we use two measures to quantify the synchronicity of the events, $Q_s(i,j)$, which quantifies the number of events that occur almost simultaneously in $i$ and $j$, and $Q_a(i,j)$, which quantifies the direction of news spreading.","We also analyze the dataset using well-known measures to detect correlations and dependencies, computed from the raw time series: undirected measures (linear cross-correlation, $CC$, and nonlinear mutual information, $MI$) and directed measures (linear Granger causality, $GC$, and nonlinear Transfer entropy, $TE$).","Our analysis unveils how fast the information diffusion process is, as high values of $Q_{s}$, $CC$, and $MI$ reveal pairs of provinces with very similar and almost simultaneous temporal variations of media attention.","On the other hand, $GC$ and $TE$ do not perform well in this context because they often return opposite directions of information transfer.","We interpret this as due to three main factors: the characteristics of the data, which is highly non-stationary, the characteristics of the information diffusion process, which is very fast and probably acts at a sub-resolution time scale, and the action of large media companies that act as global, external drivers of information dissemination."],"url":"http://arxiv.org/abs/2405.19552v1","category":"physics.soc-ph"}
{"created":"2024-05-29 22:12:52","title":"One-Shot Safety Alignment for Large Language Models via Optimal Dualization","abstract":"The growing safety concerns surrounding Large Language Models (LLMs) raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety. A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF). For such constrained RLHF, common Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable. This paper presents a dualization perspective that reduces constrained alignment to an equivalent unconstrained alignment problem. We do so by pre-optimizing a smooth and convex dual function that has a closed form. This shortcut eliminates the need for cumbersome primal-dual policy iterations, thus greatly reducing the computational burden and improving training stability. Our strategy leads to two practical algorithms in model-based and preference-based scenarios (MoCAN and PeCAN, respectively). A broad range of experiments demonstrate the effectiveness of our methods.","sentences":["The growing safety concerns surrounding Large Language Models (LLMs) raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety.","A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF).","For such constrained RLHF, common Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable.","This paper presents a dualization perspective that reduces constrained alignment to an equivalent unconstrained alignment problem.","We do so by pre-optimizing a smooth and convex dual function that has a closed form.","This shortcut eliminates the need for cumbersome primal-dual policy iterations, thus greatly reducing the computational burden and improving training stability.","Our strategy leads to two practical algorithms in model-based and preference-based scenarios (MoCAN and PeCAN, respectively).","A broad range of experiments demonstrate the effectiveness of our methods."],"url":"http://arxiv.org/abs/2405.19544v1","category":"cs.AI"}
{"created":"2024-05-29 21:48:56","title":"CheXpert Plus: Hundreds of Thousands of Aligned Radiology Texts, Images and Patients","abstract":"Since the release of the original CheXpert paper five years ago, CheXpert has become one of the most widely used and cited clinical AI datasets. The emergence of vision language models has sparked an increase in demands for sharing reports linked to CheXpert images, along with a growing interest among AI fairness researchers in obtaining demographic data. To address this, CheXpert Plus serves as a new collection of radiology data sources, made publicly available to enhance the scaling, performance, robustness, and fairness of models for all subsequent machine learning tasks in the field of radiology. CheXpert Plus is the largest text dataset publicly released in radiology, with a total of 36 million text tokens, including 13 million impression tokens. To the best of our knowledge, it represents the largest text de-identification effort in radiology, with almost 1 million PHI spans anonymized. It is only the second time that a large-scale English paired dataset has been released in radiology, thereby enabling, for the first time, cross-institution training at scale. All reports are paired with high-quality images in DICOM format, along with numerous image and patient metadata covering various clinical and socio-economic groups, as well as many pathology labels and RadGraph annotations. We hope this dataset will boost research for AI models that can further assist radiologists and help improve medical care. Data is available at the following URL: https://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1 Models are available at the following URL: https://github.com/Stanford-AIMI/chexpert-plus","sentences":["Since the release of the original CheXpert paper five years ago, CheXpert has become one of the most widely used and cited clinical AI datasets.","The emergence of vision language models has sparked an increase in demands for sharing reports linked to CheXpert images, along with a growing interest among AI fairness researchers in obtaining demographic data.","To address this, CheXpert Plus serves as a new collection of radiology data sources, made publicly available to enhance the scaling, performance, robustness, and fairness of models for all subsequent machine learning tasks in the field of radiology.","CheXpert Plus is the largest text dataset publicly released in radiology, with a total of 36 million text tokens, including 13 million impression tokens.","To the best of our knowledge, it represents the largest text de-identification effort in radiology, with almost 1 million PHI spans anonymized.","It is only the second time that a large-scale English paired dataset has been released in radiology, thereby enabling, for the first time, cross-institution training at scale.","All reports are paired with high-quality images in DICOM format, along with numerous image and patient metadata covering various clinical and socio-economic groups, as well as many pathology labels and RadGraph annotations.","We hope this dataset will boost research for AI models that can further assist radiologists and help improve medical care.","Data is available at the following URL: https://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1 Models are available at the following URL: https://github.com/Stanford-AIMI/chexpert-plus"],"url":"http://arxiv.org/abs/2405.19538v1","category":"cs.CL"}
{"created":"2024-05-29 21:39:39","title":"Generating Einstein$\\unicode{x2013}$Podolsky$\\unicode{x2013}$Rosen correlations for teleporting collective spin states in a two dimensional trapped ion crystal","abstract":"We propose the use of phonon$\\unicode{x2013}$mediated interactions as an entanglement resource to engineer Einstein$\\unicode{x2013}$Podolsky$\\unicode{x2013}$Rosen (EPR) correlations and to perform teleportation of collective spin states in two$\\unicode{x2013}$dimensional ion crystals. We emulate continuous variable quantum teleportation protocols between subsystems corresponding to different nuclear spin degrees of freedom. In each of them, a quantum state is encoded in an electronic spin degree of freedom that couples to the vibrational modes of the crystal. We show that high fidelity teleportation of spin-coherent states and their phase-displaced variant, entangled spin-squeezed states, and Dicke states, is possible for realistic experimental conditions in arrays from a few tens to a few hundred ions.","sentences":["We propose the use of phonon$\\unicode{x2013}$mediated interactions as an entanglement resource to engineer Einstein$\\unicode{x2013}$Podolsky$\\unicode{x2013}$Rosen (EPR) correlations and to perform teleportation of collective spin states in two$\\unicode{x2013}$dimensional ion crystals.","We emulate continuous variable quantum teleportation protocols between subsystems corresponding to different nuclear spin degrees of freedom.","In each of them, a quantum state is encoded in an electronic spin degree of freedom that couples to the vibrational modes of the crystal.","We show that high fidelity teleportation of spin-coherent states and their phase-displaced variant, entangled spin-squeezed states, and Dicke states, is possible for realistic experimental conditions in arrays from a few tens to a few hundred ions."],"url":"http://arxiv.org/abs/2405.19536v1","category":"quant-ph"}
{"created":"2024-05-29 21:29:44","title":"Preference Learning Algorithms Do Not Learn Preference Rankings","abstract":"Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited. In this work, we study the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs than less preferred outputs, measured via $\\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 60% on common preference datasets. We furthermore derive the $\\textit{idealized ranking accuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly. We demonstrate that existing models exhibit a significant $\\textit{alignment gap}$ -- $\\textit{i.e.}$, a gap between the observed and idealized ranking accuracies. We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint. Finally, we demonstrate that ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model used in the objective, shedding further light on the differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO) preference learning algorithms.","sentences":["Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited.","In this work, we study the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs than less preferred outputs, measured via $\\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 60% on common preference datasets.","We furthermore derive the $\\textit{idealized ranking accuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly.","We demonstrate that existing models exhibit a significant $\\textit{alignment gap}$ -- $\\textit{i.e.}$, a gap between the observed and idealized ranking accuracies.","We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint.","Finally, we demonstrate that ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model used in the objective, shedding further light on the differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO) preference learning algorithms."],"url":"http://arxiv.org/abs/2405.19534v1","category":"cs.LG"}
{"created":"2024-05-29 21:20:16","title":"Real-Time Dynamic Robot-Assisted Hand-Object Interaction via Motion Primitives","abstract":"Advances in artificial intelligence (AI) have been propelling the evolution of human-robot interaction (HRI) technologies. However, significant challenges remain in achieving seamless interactions, particularly in tasks requiring physical contact with humans. These challenges arise from the need for accurate real-time perception of human actions, adaptive control algorithms for robots, and the effective coordination between human and robotic movements. In this paper, we propose an approach to enhancing physical HRI with a focus on dynamic robot-assisted hand-object interaction (HOI). Our methodology integrates hand pose estimation, adaptive robot control, and motion primitives to facilitate human-robot collaboration. Specifically, we employ a transformer-based algorithm to perform real-time 3D modeling of human hands from single RGB images, based on which a motion primitives model (MPM) is designed to translate human hand motions into robotic actions. The robot's action implementation is dynamically fine-tuned using the continuously updated 3D hand models. Experimental validations, including a ring-wearing task, demonstrate the system's effectiveness in adapting to real-time movements and assisting in precise task executions.","sentences":["Advances in artificial intelligence (AI) have been propelling the evolution of human-robot interaction (HRI) technologies.","However, significant challenges remain in achieving seamless interactions, particularly in tasks requiring physical contact with humans.","These challenges arise from the need for accurate real-time perception of human actions, adaptive control algorithms for robots, and the effective coordination between human and robotic movements.","In this paper, we propose an approach to enhancing physical HRI with a focus on dynamic robot-assisted hand-object interaction (HOI).","Our methodology integrates hand pose estimation, adaptive robot control, and motion primitives to facilitate human-robot collaboration.","Specifically, we employ a transformer-based algorithm to perform real-time 3D modeling of human hands from single RGB images, based on which a motion primitives model (MPM) is designed to translate human hand motions into robotic actions.","The robot's action implementation is dynamically fine-tuned using the continuously updated 3D hand models.","Experimental validations, including a ring-wearing task, demonstrate the system's effectiveness in adapting to real-time movements and assisting in precise task executions."],"url":"http://arxiv.org/abs/2405.19531v1","category":"cs.RO"}
{"created":"2024-05-29 21:00:47","title":"AI Risk Management Should Incorporate Both Safety and Security","abstract":"The exposure of security vulnerabilities in safety-aligned language models, e.g., susceptibility to adversarial attacks, has shed light on the intricate interplay between AI safety and AI security. Although the two disciplines now come together under the overarching goal of AI risk management, they have historically evolved separately, giving rise to differing perspectives. Therefore, in this paper, we advocate that stakeholders in AI risk management should be aware of the nuances, synergies, and interplay between safety and security, and unambiguously take into account the perspectives of both disciplines in order to devise mostly effective and holistic risk mitigation approaches. Unfortunately, this vision is often obfuscated, as the definitions of the basic concepts of \"safety\" and \"security\" themselves are often inconsistent and lack consensus across communities. With AI risk management being increasingly cross-disciplinary, this issue is particularly salient. In light of this conceptual challenge, we introduce a unified reference framework to clarify the differences and interplay between AI safety and AI security, aiming to facilitate a shared understanding and effective collaboration across communities.","sentences":["The exposure of security vulnerabilities in safety-aligned language models, e.g., susceptibility to adversarial attacks, has shed light on the intricate interplay between AI safety and AI security.","Although the two disciplines now come together under the overarching goal of AI risk management, they have historically evolved separately, giving rise to differing perspectives.","Therefore, in this paper, we advocate that stakeholders in AI risk management should be aware of the nuances, synergies, and interplay between safety and security, and unambiguously take into account the perspectives of both disciplines in order to devise mostly effective and holistic risk mitigation approaches.","Unfortunately, this vision is often obfuscated, as the definitions of the basic concepts of \"safety\" and \"security\" themselves are often inconsistent and lack consensus across communities.","With AI risk management being increasingly cross-disciplinary, this issue is particularly salient.","In light of this conceptual challenge, we introduce a unified reference framework to clarify the differences and interplay between AI safety and AI security, aiming to facilitate a shared understanding and effective collaboration across communities."],"url":"http://arxiv.org/abs/2405.19524v1","category":"cs.CR"}
{"created":"2024-05-29 20:59:57","title":"Artificial Intelligence Index Report 2024","abstract":"The 2024 Index is our most comprehensive to date and arrives at an important moment when AI's influence on society has never been more pronounced. This year, we have broadened our scope to more extensively cover essential trends such as technical advancements in AI, public perceptions of the technology, and the geopolitical dynamics surrounding its development. Featuring more original data than ever before, this edition introduces new estimates on AI training costs, detailed analyses of the responsible AI landscape, and an entirely new chapter dedicated to AI's impact on science and medicine. The AI Index report tracks, collates, distills, and visualizes data related to artificial intelligence (AI). Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence. Previous editions have been cited in major newspapers, including the The New York Times, Bloomberg, and The Guardian, have amassed hundreds of academic citations, and been referenced by high-level policymakers in the United States, the United Kingdom, and the European Union, among other places. This year's edition surpasses all previous ones in size, scale, and scope, reflecting the growing significance that AI is coming to hold in all of our lives.","sentences":["The 2024 Index is our most comprehensive to date and arrives at an important moment when AI's influence on society has never been more pronounced.","This year, we have broadened our scope to more extensively cover essential trends such as technical advancements in AI, public perceptions of the technology, and the geopolitical dynamics surrounding its development.","Featuring more original data than ever before, this edition introduces new estimates on AI training costs, detailed analyses of the responsible AI landscape, and an entirely new chapter dedicated to AI's impact on science and medicine.","The AI Index report tracks, collates, distills, and visualizes data related to artificial intelligence (AI).","Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI.","The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence.","Previous editions have been cited in major newspapers, including the The New York Times, Bloomberg, and The Guardian, have amassed hundreds of academic citations, and been referenced by high-level policymakers in the United States, the United Kingdom, and the European Union, among other places.","This year's edition surpasses all previous ones in size, scale, and scope, reflecting the growing significance that AI is coming to hold in all of our lives."],"url":"http://arxiv.org/abs/2405.19522v1","category":"cs.AI"}
{"created":"2024-05-29 20:56:52","title":"Two-layer retrieval augmented generation framework for low-resource medical question-answering: proof of concept using Reddit data","abstract":"Retrieval augmented generation (RAG) provides the capability to constrain generative model outputs, and mitigate the possibility of hallucination, by providing relevant in-context text. The number of tokens a generative large language model (LLM) can incorporate as context is finite, thus limiting the volume of knowledge from which to generate an answer. We propose a two-layer RAG framework for query-focused answer generation and evaluate a proof-of-concept for this framework in the context of query-focused summary generation from social media forums, focusing on emerging drug-related information. The evaluations demonstrate the effectiveness of the two-layer framework in resource constrained settings to enable researchers in obtaining near real-time data from users.","sentences":["Retrieval augmented generation (RAG) provides the capability to constrain generative model outputs, and mitigate the possibility of hallucination, by providing relevant in-context text.","The number of tokens a generative large language model (LLM) can incorporate as context is finite, thus limiting the volume of knowledge from which to generate an answer.","We propose a two-layer RAG framework for query-focused answer generation and evaluate a proof-of-concept for this framework in the context of query-focused summary generation from social media forums, focusing on emerging drug-related information.","The evaluations demonstrate the effectiveness of the two-layer framework in resource constrained settings to enable researchers in obtaining near real-time data from users."],"url":"http://arxiv.org/abs/2405.19519v1","category":"cs.CL"}
{"created":"2024-05-29 20:28:04","title":"MDS-ViTNet: Improving saliency prediction for Eye-Tracking with Vision Transformer","abstract":"In this paper, we present a novel methodology we call MDS-ViTNet (Multi Decoder Saliency by Vision Transformer Network) for enhancing visual saliency prediction or eye-tracking. This approach holds significant potential for diverse fields, including marketing, medicine, robotics, and retail. We propose a network architecture that leverages the Vision Transformer, moving beyond the conventional ImageNet backbone. The framework adopts an encoder-decoder structure, with the encoder utilizing a Swin transformer to efficiently embed most important features. This process involves a Transfer Learning method, wherein layers from the Vision Transformer are converted by the Encoder Transformer and seamlessly integrated into a CNN Decoder. This methodology ensures minimal information loss from the original input image. The decoder employs a multi-decoding technique, utilizing dual decoders to generate two distinct attention maps. These maps are subsequently combined into a singular output via an additional CNN model. Our trained model MDS-ViTNet achieves state-of-the-art results across several benchmarks. Committed to fostering further collaboration, we intend to make our code, models, and datasets accessible to the public.","sentences":["In this paper, we present a novel methodology we call MDS-ViTNet (Multi Decoder Saliency by Vision Transformer Network) for enhancing visual saliency prediction or eye-tracking.","This approach holds significant potential for diverse fields, including marketing, medicine, robotics, and retail.","We propose a network architecture that leverages the Vision Transformer, moving beyond the conventional ImageNet backbone.","The framework adopts an encoder-decoder structure, with the encoder utilizing a Swin transformer to efficiently embed most important features.","This process involves a Transfer Learning method, wherein layers from the Vision Transformer are converted by the Encoder Transformer and seamlessly integrated into a CNN Decoder.","This methodology ensures minimal information loss from the original input image.","The decoder employs a multi-decoding technique, utilizing dual decoders to generate two distinct attention maps.","These maps are subsequently combined into a singular output via an additional CNN model.","Our trained model MDS-ViTNet achieves state-of-the-art results across several benchmarks.","Committed to fostering further collaboration, we intend to make our code, models, and datasets accessible to the public."],"url":"http://arxiv.org/abs/2405.19501v1","category":"cs.CV"}
{"created":"2024-05-29 20:23:57","title":"Machine Psychology: Integrating Operant Conditioning with the Non-Axiomatic Reasoning System for Advancing Artificial General Intelligence Research","abstract":"This paper introduces an interdisciplinary framework called Machine Psychology, which merges principles from operant learning psychology with a specific Artificial Intelligence model, the Non-Axiomatic Reasoning System (NARS), to enhance Artificial General Intelligence (AGI) research. The core premise of this framework is that adaptation is crucial to both biological and artificial intelligence and can be understood through operant conditioning principles. The study assesses this approach via three operant learning tasks using OpenNARS for Applications (ONA): simple discrimination, changing contingencies, and conditional discrimination tasks.   In the simple discrimination task, NARS demonstrated rapid learning, achieving perfect accuracy during both training and testing phases. The changing contingencies task showcased NARS's adaptability, as it successfully adjusted its behavior when task conditions were reversed. In the conditional discrimination task, NARS handled complex learning scenarios effectively, achieving high accuracy by forming and utilizing intricate hypotheses based on conditional cues.   These findings support the application of operant conditioning as a framework for creating adaptive AGI systems. NARS's ability to operate under conditions of insufficient knowledge and resources, coupled with its sensorimotor reasoning capabilities, establishes it as a robust model for AGI. The Machine Psychology framework, by incorporating elements of natural intelligence such as continuous learning and goal-driven behavior, offers a scalable and flexible approach for real-world applications. Future research should investigate using enhanced NARS systems, more advanced tasks, and applying this framework to diverse, complex challenges to further progress the development of human-level AI.","sentences":["This paper introduces an interdisciplinary framework called Machine Psychology, which merges principles from operant learning psychology with a specific Artificial Intelligence model, the Non-Axiomatic Reasoning System (NARS), to enhance Artificial General Intelligence (AGI) research.","The core premise of this framework is that adaptation is crucial to both biological and artificial intelligence and can be understood through operant conditioning principles.","The study assesses this approach via three operant learning tasks using OpenNARS for Applications (ONA): simple discrimination, changing contingencies, and conditional discrimination tasks.   ","In the simple discrimination task, NARS demonstrated rapid learning, achieving perfect accuracy during both training and testing phases.","The changing contingencies task showcased NARS's adaptability, as it successfully adjusted its behavior when task conditions were reversed.","In the conditional discrimination task, NARS handled complex learning scenarios effectively, achieving high accuracy by forming and utilizing intricate hypotheses based on conditional cues.   ","These findings support the application of operant conditioning as a framework for creating adaptive AGI systems.","NARS's ability to operate under conditions of insufficient knowledge and resources, coupled with its sensorimotor reasoning capabilities, establishes it as a robust model for AGI.","The Machine Psychology framework, by incorporating elements of natural intelligence such as continuous learning and goal-driven behavior, offers a scalable and flexible approach for real-world applications.","Future research should investigate using enhanced NARS systems, more advanced tasks, and applying this framework to diverse, complex challenges to further progress the development of human-level AI."],"url":"http://arxiv.org/abs/2405.19498v1","category":"cs.AI"}
{"created":"2024-05-29 20:21:00","title":"Qiskit Code Assistant: Training LLMs for generating Quantum Computing Code","abstract":"Code Large Language Models (Code LLMs) have emerged as powerful tools, revolutionizing the software development landscape by automating the coding process and reducing time and effort required to build applications. This paper focuses on training Code LLMs to specialize in the field of quantum computing. We begin by discussing the unique needs of quantum computing programming, which differ significantly from classical programming approaches or languages. A Code LLM specializing in quantum computing requires a foundational understanding of quantum computing and quantum information theory. However, the scarcity of available quantum code examples and the rapidly evolving field, which necessitates continuous dataset updates, present significant challenges. Moreover, we discuss our work on training Code LLMs to produce high-quality quantum code using the Qiskit library. This work includes an examination of the various aspects of the LLMs used for training and the specific training conditions, as well as the results obtained with our current models. To evaluate our models, we have developed a custom benchmark, similar to HumanEval, which includes a set of tests specifically designed for the field of quantum computing programming using Qiskit. Our findings indicate that our model outperforms existing state-of-the-art models in quantum computing tasks. We also provide examples of code suggestions, comparing our model to other relevant code LLMs. Finally, we introduce a discussion on the potential benefits of Code LLMs for quantum computing computational scientists, researchers, and practitioners. We also explore various features and future work that could be relevant in this context.","sentences":["Code Large Language Models (Code LLMs) have emerged as powerful tools, revolutionizing the software development landscape by automating the coding process and reducing time and effort required to build applications.","This paper focuses on training Code LLMs to specialize in the field of quantum computing.","We begin by discussing the unique needs of quantum computing programming, which differ significantly from classical programming approaches or languages.","A Code LLM specializing in quantum computing requires a foundational understanding of quantum computing and quantum information theory.","However, the scarcity of available quantum code examples and the rapidly evolving field, which necessitates continuous dataset updates, present significant challenges.","Moreover, we discuss our work on training Code LLMs to produce high-quality quantum code using the Qiskit library.","This work includes an examination of the various aspects of the LLMs used for training and the specific training conditions, as well as the results obtained with our current models.","To evaluate our models, we have developed a custom benchmark, similar to HumanEval, which includes a set of tests specifically designed for the field of quantum computing programming using Qiskit.","Our findings indicate that our model outperforms existing state-of-the-art models in quantum computing tasks.","We also provide examples of code suggestions, comparing our model to other relevant code LLMs.","Finally, we introduce a discussion on the potential benefits of Code LLMs for quantum computing computational scientists, researchers, and practitioners.","We also explore various features and future work that could be relevant in this context."],"url":"http://arxiv.org/abs/2405.19495v1","category":"quant-ph"}
{"created":"2024-05-29 19:53:23","title":"Participation in the age of foundation models","abstract":"Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of public services. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to marginalized communities. Participatory approaches hold promise to instead lend agency and decision-making power to marginalized stakeholders. But existing approaches in participatory AI/ML are typically deeply grounded in context - how do we apply these approaches to foundation models, which are, by design, disconnected from context? Our paper interrogates this question.   First, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the \"foundation\" layer, our framework proposes the \"subfloor'' layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain, and the \"surface'' layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate \"subfloor'' layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer.","sentences":["Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of public services.","Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to marginalized communities.","Participatory approaches hold promise to instead lend agency and decision-making power to marginalized stakeholders.","But existing approaches in participatory AI/ML are typically deeply grounded in context - how do we apply these approaches to foundation models, which are, by design, disconnected from context?","Our paper interrogates this question.   ","First, we examine existing attempts at incorporating participation into foundation models.","We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable.","In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation.","In addition to the \"foundation\" layer, our framework proposes the \"subfloor'' layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain, and the \"surface'' layer, in which affected communities shape the use of a foundation model for a specific downstream task.","The intermediate \"subfloor'' layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention.","At the same time, it avoids duplicative effort by scaling input across relevant use cases.","Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer."],"url":"http://arxiv.org/abs/2405.19479v1","category":"cs.CY"}
{"created":"2024-05-29 19:40:27","title":"The Data Minimization Principle in Machine Learning","abstract":"The principle of data minimization aims to reduce the amount of data collected, processed or retained to minimize the potential for misuse, unauthorized access, or data breaches. Rooted in privacy-by-design principles, data minimization has been endorsed by various global data protection regulations. However, its practical implementation remains a challenge due to the lack of a rigorous formulation. This paper addresses this gap and introduces an optimization framework for data minimization based on its legal definitions. It then adapts several optimization algorithms to perform data minimization and conducts a comprehensive evaluation in terms of their compliance with minimization objectives as well as their impact on user privacy. Our analysis underscores the mismatch between the privacy expectations of data minimization and the actual privacy benefits, emphasizing the need for approaches that account for multiple facets of real-world privacy risks.","sentences":["The principle of data minimization aims to reduce the amount of data collected, processed or retained to minimize the potential for misuse, unauthorized access, or data breaches.","Rooted in privacy-by-design principles, data minimization has been endorsed by various global data protection regulations.","However, its practical implementation remains a challenge due to the lack of a rigorous formulation.","This paper addresses this gap and introduces an optimization framework for data minimization based on its legal definitions.","It then adapts several optimization algorithms to perform data minimization and conducts a comprehensive evaluation in terms of their compliance with minimization objectives as well as their impact on user privacy.","Our analysis underscores the mismatch between the privacy expectations of data minimization and the actual privacy benefits, emphasizing the need for approaches that account for multiple facets of real-world privacy risks."],"url":"http://arxiv.org/abs/2405.19471v1","category":"cs.LG"}
{"created":"2024-05-29 19:24:44","title":"Posterior Sampling via Autoregressive Generation","abstract":"Real-world decision-making requires grappling with a perpetual lack of data as environments change; intelligent agents must comprehend uncertainty and actively gather information to resolve it. We propose a new framework for learning bandit algorithms from massive historical data, which we demonstrate in a cold-start recommendation problem. First, we use historical data to pretrain an autoregressive model to predict a sequence of repeated feedback/rewards (e.g., responses to news articles shown to different users over time). In learning to make accurate predictions, the model implicitly learns an informed prior based on rich action features (e.g., article headlines) and how to sharpen beliefs as more rewards are gathered (e.g., clicks as each article is recommended). At decision-time, we autoregressively sample (impute) an imagined sequence of rewards for each action, and choose the action with the largest average imputed reward. Far from a heuristic, our approach is an implementation of Thompson sampling (with a learned prior), a prominent active exploration algorithm. We prove our pretraining loss directly controls online decision-making performance, and we demonstrate our framework on a news recommendation task where we integrate end-to-end fine-tuning of a pretrained language model to process news article headline text to improve performance.","sentences":["Real-world decision-making requires grappling with a perpetual lack of data as environments change; intelligent agents must comprehend uncertainty and actively gather information to resolve it.","We propose a new framework for learning bandit algorithms from massive historical data, which we demonstrate in a cold-start recommendation problem.","First, we use historical data to pretrain an autoregressive model to predict a sequence of repeated feedback/rewards (e.g., responses to news articles shown to different users over time).","In learning to make accurate predictions, the model implicitly learns an informed prior based on rich action features (e.g., article headlines) and how to sharpen beliefs as more rewards are gathered (e.g., clicks as each article is recommended).","At decision-time, we autoregressively sample (impute) an imagined sequence of rewards for each action, and choose the action with the largest average imputed reward.","Far from a heuristic, our approach is an implementation of Thompson sampling (with a learned prior), a prominent active exploration algorithm.","We prove our pretraining loss directly controls online decision-making performance, and we demonstrate our framework on a news recommendation task where we integrate end-to-end fine-tuning of a pretrained language model to process news article headline text to improve performance."],"url":"http://arxiv.org/abs/2405.19466v1","category":"cs.LG"}
{"created":"2024-05-29 19:23:07","title":"Leveraging Generative AI for Smart City Digital Twins: A Survey on the Autonomous Generation of Data, Scenarios, 3D City Models, and Urban Designs","abstract":"The digital transformation of modern cities by integrating advanced information, communication, and computing technologies has marked the epoch of data-driven smart city applications for efficient and sustainable urban management. Despite their effectiveness, these applications often rely on massive amounts of high-dimensional and multi-domain data for monitoring and characterizing different urban sub-systems, presenting challenges in application areas that are limited by data quality and availability, as well as costly efforts for generating urban scenarios and design alternatives. As an emerging research area in deep learning, Generative Artificial Intelligence (AI) models have demonstrated their unique values in data and code generation. This survey paper aims to explore the innovative integration of generative AI techniques and urban digital twins to address challenges in the realm of smart cities in various urban sectors, such as transportation and mobility management, energy system operations, building and infrastructure management, and urban design. The survey starts with the introduction of popular generative AI models with their application areas, followed by a structured review of the existing urban science applications that leverage the autonomous capability of the generative AI techniques to facilitate (a) data augmentation for promoting urban monitoring and predictive analytics, (b) synthetic data and scenario generation, (c) automated 3D city modeling, and (d) generative urban design and optimization. Based on the review, this survey discusses potential opportunities and technical strategies that integrate generative AI models into the next-generation urban digital twins for more reliable, scalable, and automated management of smart cities.","sentences":["The digital transformation of modern cities by integrating advanced information, communication, and computing technologies has marked the epoch of data-driven smart city applications for efficient and sustainable urban management.","Despite their effectiveness, these applications often rely on massive amounts of high-dimensional and multi-domain data for monitoring and characterizing different urban sub-systems, presenting challenges in application areas that are limited by data quality and availability, as well as costly efforts for generating urban scenarios and design alternatives.","As an emerging research area in deep learning, Generative Artificial Intelligence (AI) models have demonstrated their unique values in data and code generation.","This survey paper aims to explore the innovative integration of generative AI techniques and urban digital twins to address challenges in the realm of smart cities in various urban sectors, such as transportation and mobility management, energy system operations, building and infrastructure management, and urban design.","The survey starts with the introduction of popular generative AI models with their application areas, followed by a structured review of the existing urban science applications that leverage the autonomous capability of the generative AI techniques to facilitate (a) data augmentation for promoting urban monitoring and predictive analytics, (b) synthetic data and scenario generation, (c) automated 3D city modeling, and (d) generative urban design and optimization.","Based on the review, this survey discusses potential opportunities and technical strategies that integrate generative AI models into the next-generation urban digital twins for more reliable, scalable, and automated management of smart cities."],"url":"http://arxiv.org/abs/2405.19464v1","category":"cs.AI"}
{"created":"2024-05-29 19:12:08","title":"MemControl: Mitigating Memorization in Medical Diffusion Models via Automated Parameter Selection","abstract":"Diffusion models show a remarkable ability in generating images that closely mirror the training distribution. However, these models are prone to training data memorization, leading to significant privacy, ethical, and legal concerns, particularly in sensitive fields such as medical imaging. We hypothesize that memorization is driven by the overparameterization of deep models, suggesting that regularizing model capacity during fine-tuning could be an effective mitigation strategy. Parameter-efficient fine-tuning (PEFT) methods offer a promising approach to capacity control by selectively updating specific parameters. However, finding the optimal subset of learnable parameters that balances generation quality and memorization remains elusive. To address this challenge, we propose a bi-level optimization framework that guides automated parameter selection by utilizing memorization and generation quality metrics as rewards. Our framework successfully identifies the optimal parameter set to be updated to satisfy the generation-memorization tradeoff. We perform our experiments for the specific task of medical image generation and outperform existing state-of-the-art training-time mitigation strategies by fine-tuning as few as 0.019% of model parameters. Furthermore, we show that the strategies learned through our framework are transferable across different datasets and domains. Our proposed framework is scalable to large datasets and agnostic to the choice of reward functions. Finally, we show that our framework can be combined with existing approaches for further memorization mitigation.","sentences":["Diffusion models show a remarkable ability in generating images that closely mirror the training distribution.","However, these models are prone to training data memorization, leading to significant privacy, ethical, and legal concerns, particularly in sensitive fields such as medical imaging.","We hypothesize that memorization is driven by the overparameterization of deep models, suggesting that regularizing model capacity during fine-tuning could be an effective mitigation strategy.","Parameter-efficient fine-tuning (PEFT) methods offer a promising approach to capacity control by selectively updating specific parameters.","However, finding the optimal subset of learnable parameters that balances generation quality and memorization remains elusive.","To address this challenge, we propose a bi-level optimization framework that guides automated parameter selection by utilizing memorization and generation quality metrics as rewards.","Our framework successfully identifies the optimal parameter set to be updated to satisfy the generation-memorization tradeoff.","We perform our experiments for the specific task of medical image generation and outperform existing state-of-the-art training-time mitigation strategies by fine-tuning as few as 0.019% of model parameters.","Furthermore, we show that the strategies learned through our framework are transferable across different datasets and domains.","Our proposed framework is scalable to large datasets and agnostic to the choice of reward functions.","Finally, we show that our framework can be combined with existing approaches for further memorization mitigation."],"url":"http://arxiv.org/abs/2405.19458v1","category":"cs.CV"}
{"created":"2024-05-29 19:07:42","title":"An Automated Startup Evaluation Pipeline: Startup Success Forecasting Framework (SSFF)","abstract":"Evaluating startups in their early stages is a complex task that requires detailed analysis by experts. While automating this process on a large scale can significantly impact businesses, the inherent complexity poses challenges. This paper addresses this challenge by introducing the Startup Success Forecasting Framework (SSFF), a new automated system that combines traditional machine learning with advanced language models. This intelligent agent-based architecture is designed to reason, act, synthesize, and decide like a venture capitalist to perform the analysis end-to-end. The SSFF is made up of three main parts: - Prediction Block: Uses random forests and neural networks to make predictions. - Analyst Block: Simulates VC analysis scenario and uses SOTA prompting techniques - External Knowledge Block: Gathers real-time information from external sources. This framework requires minimal input data about the founder and startup description, enhances it with additional data from external resources, and performs a detailed analysis with high accuracy, all in an automated manner","sentences":["Evaluating startups in their early stages is a complex task that requires detailed analysis by experts.","While automating this process on a large scale can significantly impact businesses, the inherent complexity poses challenges.","This paper addresses this challenge by introducing the Startup Success Forecasting Framework (SSFF), a new automated system that combines traditional machine learning with advanced language models.","This intelligent agent-based architecture is designed to reason, act, synthesize, and decide like a venture capitalist to perform the analysis end-to-end.","The SSFF is made up of three main parts: - Prediction Block: Uses random forests and neural networks to make predictions.","- Analyst Block: Simulates VC analysis scenario and uses SOTA prompting techniques - External Knowledge Block: Gathers real-time information from external sources.","This framework requires minimal input data about the founder and startup description, enhances it with additional data from external resources, and performs a detailed analysis with high accuracy, all in an automated manner"],"url":"http://arxiv.org/abs/2405.19456v1","category":"cs.AI"}
{"created":"2024-05-29 19:03:27","title":"Optimizing Split Points for Error-Resilient SplitFed Learning","abstract":"Recent advancements in decentralized learning, such as Federated Learning (FL), Split Learning (SL), and Split Federated Learning (SplitFed), have expanded the potentials of machine learning. SplitFed aims to minimize the computational burden on individual clients in FL and parallelize SL while maintaining privacy. This study investigates the resilience of SplitFed to packet loss at model split points. It explores various parameter aggregation strategies of SplitFed by examining the impact of splitting the model at different points-either shallow split or deep split-on the final global model performance. The experiments, conducted on a human embryo image segmentation task, reveal a statistically significant advantage of a deeper split point.","sentences":["Recent advancements in decentralized learning, such as Federated Learning (FL), Split Learning (SL), and Split Federated Learning (SplitFed), have expanded the potentials of machine learning.","SplitFed aims to minimize the computational burden on individual clients in FL and parallelize SL while maintaining privacy.","This study investigates the resilience of SplitFed to packet loss at model split points.","It explores various parameter aggregation strategies of SplitFed by examining the impact of splitting the model at different points-either shallow split or deep split-on the final global model performance.","The experiments, conducted on a human embryo image segmentation task, reveal a statistically significant advantage of a deeper split point."],"url":"http://arxiv.org/abs/2405.19453v1","category":"cs.AI"}
{"created":"2024-05-29 18:45:55","title":"MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions","abstract":"Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats. However, real world scenarios often involve mathematical question answering that requires multi turn or interactive information exchanges, and the performance of LLMs on these tasks is still underexplored. This paper introduces MathChat, a comprehensive benchmark specifically designed to evaluate LLMs across a broader spectrum of mathematical tasks. These tasks are structured to assess the models' abilities in multiturn interactions and open ended generation. We evaluate the performance of various SOTA LLMs on the MathChat benchmark, and we observe that while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding. To address the above limitations of existing LLMs when faced with multiturn and open ended tasks, we develop MathChat sync, a synthetic dialogue based math dataset for LLM finetuning, focusing on improving models' interaction and instruction following capabilities in conversations. Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync. We believe this work outlines one promising direction for improving the multiturn mathematical reasoning abilities of LLMs, thus pushing forward the development of LLMs that are more adept at interactive mathematical problem solving and real world applications.","sentences":["Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats.","However, real world scenarios often involve mathematical question answering that requires multi turn or interactive information exchanges, and the performance of LLMs on these tasks is still underexplored.","This paper introduces MathChat, a comprehensive benchmark specifically designed to evaluate LLMs across a broader spectrum of mathematical tasks.","These tasks are structured to assess the models' abilities in multiturn interactions and open ended generation.","We evaluate the performance of various SOTA LLMs on the MathChat benchmark, and we observe that while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding.","To address the above limitations of existing LLMs when faced with multiturn and open ended tasks, we develop MathChat sync, a synthetic dialogue based math dataset for LLM finetuning, focusing on improving models' interaction and instruction following capabilities in conversations.","Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync.","We believe this work outlines one promising direction for improving the multiturn mathematical reasoning abilities of LLMs, thus pushing forward the development of LLMs that are more adept at interactive mathematical problem solving and real world applications."],"url":"http://arxiv.org/abs/2405.19444v1","category":"cs.AI"}
{"created":"2024-05-29 18:10:36","title":"Conformal Recursive Feature Elimination","abstract":"Unlike traditional statistical methods, Conformal Prediction (CP) allows for the determination of valid and accurate confidence levels associated with individual predictions based only on exchangeability of the data. We here introduce a new feature selection method that takes advantage of the CP framework. Our proposal, named Conformal Recursive Feature Elimination (CRFE), identifies and recursively removes features that increase the non-conformity of a dataset. We also present an automatic stopping criterion for CRFE, as well as a new index to measure consistency between subsets of features. CRFE selections are compared to the classical Recursive Feature Elimination (RFE) method on several multiclass datasets by using multiple partitions of the data. The results show that CRFE clearly outperforms RFE in half of the datasets, while achieving similar performance in the rest. The automatic stopping criterion provides subsets of effective and non-redundant features without computing any classification performance.","sentences":["Unlike traditional statistical methods, Conformal Prediction (CP) allows for the determination of valid and accurate confidence levels associated with individual predictions based only on exchangeability of the data.","We here introduce a new feature selection method that takes advantage of the CP framework.","Our proposal, named Conformal Recursive Feature Elimination (CRFE), identifies and recursively removes features that increase the non-conformity of a dataset.","We also present an automatic stopping criterion for CRFE, as well as a new index to measure consistency between subsets of features.","CRFE selections are compared to the classical Recursive Feature Elimination (RFE) method on several multiclass datasets by using multiple partitions of the data.","The results show that CRFE clearly outperforms RFE in half of the datasets, while achieving similar performance in the rest.","The automatic stopping criterion provides subsets of effective and non-redundant features without computing any classification performance."],"url":"http://arxiv.org/abs/2405.19429v1","category":"cs.CV"}
{"created":"2024-05-29 18:04:59","title":"Evaluating Vision-Language Models on Bistable Images","abstract":"Bistable images, also known as ambiguous or reversible images, present visual stimuli that can be seen in two distinct interpretations, though not simultaneously by the observer. In this study, we conduct the most extensive examination of vision-language models using bistable images to date. We manually gathered a dataset of 29 bistable images, along with their associated labels, and subjected them to 116 different manipulations in brightness, tint, and rotation. We evaluated twelve different models in both classification and generative tasks across six model architectures. Our findings reveal that, with the exception of models from the Idefics family and LLaVA1.5-13b, there is a pronounced preference for one interpretation over another among the models, and minimal variance under image manipulations, with few exceptions on image rotations. Additionally, we compared the model preferences with humans, noting that the models do not exhibit the same continuity biases as humans and often diverge from human initial interpretations. We also investigated the influence of variations in prompts and the use of synonymous labels, discovering that these factors significantly affect model interpretations more than image manipulations showing a higher influence of the language priors on bistable image interpretations compared to image-text training data. All code and data is open sourced.","sentences":["Bistable images, also known as ambiguous or reversible images, present visual stimuli that can be seen in two distinct interpretations, though not simultaneously by the observer.","In this study, we conduct the most extensive examination of vision-language models using bistable images to date.","We manually gathered a dataset of 29 bistable images, along with their associated labels, and subjected them to 116 different manipulations in brightness, tint, and rotation.","We evaluated twelve different models in both classification and generative tasks across six model architectures.","Our findings reveal that, with the exception of models from the Idefics family and LLaVA1.5-13b, there is a pronounced preference for one interpretation over another among the models, and minimal variance under image manipulations, with few exceptions on image rotations.","Additionally, we compared the model preferences with humans, noting that the models do not exhibit the same continuity biases as humans and often diverge from human initial interpretations.","We also investigated the influence of variations in prompts and the use of synonymous labels, discovering that these factors significantly affect model interpretations more than image manipulations showing a higher influence of the language priors on bistable image interpretations compared to image-text training data.","All code and data is open sourced."],"url":"http://arxiv.org/abs/2405.19423v1","category":"cs.CV"}
{"created":"2024-05-29 18:01:58","title":"Using Contrastive Learning with Generative Similarity to Learn Spaces that Capture Human Inductive Biases","abstract":"Humans rely on strong inductive biases to learn from few examples and abstract useful information from sensory data. Instilling such biases in machine learning models has been shown to improve their performance on various benchmarks including few-shot learning, robustness, and alignment. However, finding effective training procedures to achieve that goal can be challenging as psychologically-rich training data such as human similarity judgments are expensive to scale, and Bayesian models of human inductive biases are often intractable for complex, realistic domains. Here, we address this challenge by introducing a Bayesian notion of generative similarity whereby two datapoints are considered similar if they are likely to have been sampled from the same distribution. This measure can be applied to complex generative processes, including probabilistic programs. We show that generative similarity can be used to define a contrastive learning objective even when its exact form is intractable, enabling learning of spatial embeddings that express specific inductive biases. We demonstrate the utility of our approach by showing how it can be used to capture human inductive biases for geometric shapes, and to better distinguish different abstract drawing styles that are parameterized by probabilistic programs.","sentences":["Humans rely on strong inductive biases to learn from few examples and abstract useful information from sensory data.","Instilling such biases in machine learning models has been shown to improve their performance on various benchmarks including few-shot learning, robustness, and alignment.","However, finding effective training procedures to achieve that goal can be challenging as psychologically-rich training data such as human similarity judgments are expensive to scale, and Bayesian models of human inductive biases are often intractable for complex, realistic domains.","Here, we address this challenge by introducing a Bayesian notion of generative similarity whereby two datapoints are considered similar if they are likely to have been sampled from the same distribution.","This measure can be applied to complex generative processes, including probabilistic programs.","We show that generative similarity can be used to define a contrastive learning objective even when its exact form is intractable, enabling learning of spatial embeddings that express specific inductive biases.","We demonstrate the utility of our approach by showing how it can be used to capture human inductive biases for geometric shapes, and to better distinguish different abstract drawing styles that are parameterized by probabilistic programs."],"url":"http://arxiv.org/abs/2405.19420v1","category":"cs.LG"}
{"created":"2024-05-29 18:00:20","title":"VisTA-SR: Improving the Accuracy and Resolution of Low-Cost Thermal Imaging Cameras for Agriculture","abstract":"Thermal cameras are an important tool for agricultural research because they allow for non-invasive measurement of plant temperature, which relates to important photochemical, hydraulic, and agronomic traits. Utilizing low-cost thermal cameras can lower the barrier to introducing thermal imaging in agricultural research and production. This paper presents an approach to improve the temperature accuracy and image quality of low-cost thermal imaging cameras for agricultural applications. Leveraging advancements in computer vision techniques, particularly deep learning networks, we propose a method, called $\\textbf{VisTA-SR}$ ($\\textbf{Vis}$ual \\& $\\textbf{T}$hermal $\\textbf{A}$lignment and $\\textbf{S}$uper-$\\textbf{R}$esolution Enhancement) that combines RGB and thermal images to enhance the capabilities of low-resolution thermal cameras. The research includes calibration and validation of temperature measurements, acquisition of paired image datasets, and the development of a deep learning network tailored for agricultural thermal imaging. Our study addresses the challenges of image enhancement in the agricultural domain and explores the potential of low-cost thermal cameras to replace high-resolution industrial cameras. Experimental results demonstrate the effectiveness of our approach in enhancing temperature accuracy and image sharpness, paving the way for more accessible and efficient thermal imaging solutions in agriculture.","sentences":["Thermal cameras are an important tool for agricultural research because they allow for non-invasive measurement of plant temperature, which relates to important photochemical, hydraulic, and agronomic traits.","Utilizing low-cost thermal cameras can lower the barrier to introducing thermal imaging in agricultural research and production.","This paper presents an approach to improve the temperature accuracy and image quality of low-cost thermal imaging cameras for agricultural applications.","Leveraging advancements in computer vision techniques, particularly deep learning networks, we propose a method, called $\\textbf{VisTA-SR}$ ($\\textbf{Vis}$ual \\& $\\textbf{T}$hermal $\\textbf{A}$lignment and $\\textbf{S}$uper-$\\textbf{R}$esolution Enhancement) that combines RGB and thermal images to enhance the capabilities of low-resolution thermal cameras.","The research includes calibration and validation of temperature measurements, acquisition of paired image datasets, and the development of a deep learning network tailored for agricultural thermal imaging.","Our study addresses the challenges of image enhancement in the agricultural domain and explores the potential of low-cost thermal cameras to replace high-resolution industrial cameras.","Experimental results demonstrate the effectiveness of our approach in enhancing temperature accuracy and image sharpness, paving the way for more accessible and efficient thermal imaging solutions in agriculture."],"url":"http://arxiv.org/abs/2405.19413v1","category":"cs.CV"}
{"created":"2024-05-30 17:59:51","title":"From Zero to Hero: Cold-Start Anomaly Detection","abstract":"When first deploying an anomaly detection system, e.g., to detect out-of-scope queries in chatbots, there are no observed data, making data-driven approaches ineffective. Zero-shot anomaly detection methods offer a solution to such \"cold-start\" cases, but unfortunately they are often not accurate enough. This paper studies the realistic but underexplored cold-start setting where an anomaly detection model is initialized using zero-shot guidance, but subsequently receives a small number of contaminated observations (namely, that may include anomalies). The goal is to make efficient use of both the zero-shot guidance and the observations. We propose ColdFusion, a method that effectively adapts the zero-shot anomaly detector to contaminated observations. To support future development of this new setting, we propose an evaluation suite consisting of evaluation protocols and metrics.","sentences":["When first deploying an anomaly detection system, e.g., to detect out-of-scope queries in chatbots, there are no observed data, making data-driven approaches ineffective.","Zero-shot anomaly detection methods offer a solution to such \"cold-start\" cases, but unfortunately they are often not accurate enough.","This paper studies the realistic but underexplored cold-start setting where an anomaly detection model is initialized using zero-shot guidance, but subsequently receives a small number of contaminated observations (namely, that may include anomalies).","The goal is to make efficient use of both the zero-shot guidance and the observations.","We propose ColdFusion, a method that effectively adapts the zero-shot anomaly detector to contaminated observations.","To support future development of this new setting, we propose an evaluation suite consisting of evaluation protocols and metrics."],"url":"http://arxiv.org/abs/2405.20341v1","category":"cs.LG"}
{"created":"2024-05-30 17:59:10","title":"SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical Videos","abstract":"Accurate tool tracking is essential for the success of computer-assisted intervention. Previous efforts often modeled tool trajectories rigidly, overlooking the dynamic nature of surgical procedures, especially tracking scenarios like out-of-body and out-of-camera views. Addressing this limitation, the new CholecTrack20 dataset provides detailed labels that account for multiple tool trajectories in three perspectives: (1) intraoperative, (2) intracorporeal, and (3) visibility, representing the different types of temporal duration of tool tracks. These fine-grained labels enhance tracking flexibility but also increase the task complexity. Re-identifying tools after occlusion or re-insertion into the body remains challenging due to high visual similarity, especially among tools of the same category. This work recognizes the critical role of the tool operators in distinguishing tool track instances, especially those belonging to the same tool category. The operators' information are however not explicitly captured in surgical videos. We therefore propose SurgiTrack, a novel deep learning method that leverages YOLOv7 for precise tool detection and employs an attention mechanism to model the originating direction of the tools, as a proxy to their operators, for tool re-identification. To handle diverse tool trajectory perspectives, SurgiTrack employs a harmonizing bipartite matching graph, minimizing conflicts and ensuring accurate tool identity association. Experimental results on CholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselines and state-of-the-art methods with real-time inference capability. This work sets a new standard in surgical tool tracking, providing dynamic trajectories for more adaptable and precise assistance in minimally invasive surgeries.","sentences":["Accurate tool tracking is essential for the success of computer-assisted intervention.","Previous efforts often modeled tool trajectories rigidly, overlooking the dynamic nature of surgical procedures, especially tracking scenarios like out-of-body and out-of-camera views.","Addressing this limitation, the new CholecTrack20 dataset provides detailed labels that account for multiple tool trajectories in three perspectives: (1) intraoperative, (2) intracorporeal, and (3) visibility, representing the different types of temporal duration of tool tracks.","These fine-grained labels enhance tracking flexibility but also increase the task complexity.","Re-identifying tools after occlusion or re-insertion into the body remains challenging due to high visual similarity, especially among tools of the same category.","This work recognizes the critical role of the tool operators in distinguishing tool track instances, especially those belonging to the same tool category.","The operators' information are however not explicitly captured in surgical videos.","We therefore propose SurgiTrack, a novel deep learning method that leverages YOLOv7 for precise tool detection and employs an attention mechanism to model the originating direction of the tools, as a proxy to their operators, for tool re-identification.","To handle diverse tool trajectory perspectives, SurgiTrack employs a harmonizing bipartite matching graph, minimizing conflicts and ensuring accurate tool identity association.","Experimental results on CholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselines and state-of-the-art methods with real-time inference capability.","This work sets a new standard in surgical tool tracking, providing dynamic trajectories for more adaptable and precise assistance in minimally invasive surgeries."],"url":"http://arxiv.org/abs/2405.20333v1","category":"cs.CV"}
{"created":"2024-05-30 17:54:35","title":"S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs","abstract":"Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference. However, despite the high speedups they offer, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead. Given limited memory and the necessity of quantization, a high-performing model on a high-end GPU can slow down by up to 7 times. To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping. When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data. Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3. It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM.","sentences":["Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference.","However, despite the high speedups they offer, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead.","Given limited memory and the necessity of quantization, a high-performing model on a high-end GPU can slow down by up to 7 times.","To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping.","When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data.","Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3.","It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM."],"url":"http://arxiv.org/abs/2405.20314v1","category":"cs.CL"}
{"created":"2024-05-30 17:45:36","title":"Gaussian factors, spectra, and $P$-entropy","abstract":"There is a set of continual cardinality of pairwise disjoint Gaussian automorphisms with spectrally isomorphic even factors having Lebesgue spectrum.","sentences":["There is a set of continual cardinality of pairwise disjoint Gaussian automorphisms with spectrally isomorphic even factors having Lebesgue spectrum."],"url":"http://arxiv.org/abs/2405.20297v1","category":"math.DS"}
{"created":"2024-05-30 17:45:27","title":"Critical metrology of minimally accessible anisotropic spin chains","abstract":"We address quantum metrology in critical spin chains with anisotropy and Dzyaloshinskii-Moriya (DM) interaction, and show how local and quasi-local measurements may be exploited to characterize global properties of the systems. In particular, we evaluate the classical (magnetization) and quantum Fisher information of the relevant parameters for the density matrix of a single spin and that of a pair of spins ranging from nearest to sixth-nearest neighbors, to the limiting case of very distant spins. Our results allow us to elucidate the role of the different parameters and to individuate the optimal working regimes for the precise characterization of the system, also clarifying the effects of correlations on the estimation precision.","sentences":["We address quantum metrology in critical spin chains with anisotropy and Dzyaloshinskii-Moriya (DM) interaction, and show how local and quasi-local measurements may be exploited to characterize global properties of the systems.","In particular, we evaluate the classical (magnetization) and quantum Fisher information of the relevant parameters for the density matrix of a single spin and that of a pair of spins ranging from nearest to sixth-nearest neighbors, to the limiting case of very distant spins.","Our results allow us to elucidate the role of the different parameters and to individuate the optimal working regimes for the precise characterization of the system, also clarifying the effects of correlations on the estimation precision."],"url":"http://arxiv.org/abs/2405.20296v1","category":"quant-ph"}
{"created":"2024-05-30 17:17:59","title":"Hyperuniformity in Ashkin-Teller model","abstract":"We show that equilibrium systems in $d$ dimension that obey the inequality $d\\nu> 2,$ known as Harris criterion, exhibit suppressed energy fluctuation in their critical state. Ashkin-Teller model is an example in $d=2$ where the correlation length exponent $\\nu$ varies continuously with the inter-spin interaction strength $\\lambda$ and exceeds the value $d/2$ set by Harris criterion when $\\lambda$ is negative; there, the variance of the subsystem energy across a length scale $l$ varies as $l^{d-\\alpha}$ with hyperuniformity exponent $\\alpha = 2(1-\\nu^{-1}).$ Point configurations constructed by assigning unity to the sites which has coarse-grained energy beyond a threshold value also exhibit suppressed number fluctuation and hyperuniformiyty with same exponent $\\alpha.$","sentences":["We show that equilibrium systems in $d$ dimension that obey the inequality $d\\nu> 2,$ known as Harris criterion, exhibit suppressed energy fluctuation in their critical state.","Ashkin-Teller model is an example in $d=2$ where the correlation length exponent $\\nu$ varies continuously with the inter-spin interaction strength $\\lambda$ and exceeds the value $d/2$ set by Harris criterion when $\\lambda$ is negative; there, the variance of the subsystem energy across a length scale $l$ varies as $l^{d-\\alpha}$ with hyperuniformity exponent $\\alpha = 2(1-\\nu^{-1}).$ Point configurations constructed by assigning unity to the sites which has coarse-grained energy beyond a threshold value also exhibit suppressed number fluctuation and hyperuniformiyty with same exponent $\\alpha.$"],"url":"http://arxiv.org/abs/2405.20266v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-30 17:16:52","title":"Pinning and dipole asymptotics of locally deformed striped phases","abstract":"We investigate the effect of spatial inhomogeneity on perfectly periodic, self-organized striped patterns in spatially extended systems. We demonstrate that inhomogeneities select a specific translate of the striped patterns and induce algebraically decaying, dipole-type farfield deformations. Phase shifts and leading order terms are determined by effective moments of the spatial inhomogeneity. Farfield decay is proportional to the derivatives of the Green's function of an effective Laplacian. Technically, we use mode filters and conjugacies to an effective Laplacian to establish Fredholm properties of the linearization in Kondratiev spaces. Spatial localization in a contraction argument is gained through the use of an explicit deformation ansatz and a subtle cancellation in Bloch wave space.","sentences":["We investigate the effect of spatial inhomogeneity on perfectly periodic, self-organized striped patterns in spatially extended systems.","We demonstrate that inhomogeneities select a specific translate of the striped patterns and induce algebraically decaying, dipole-type farfield deformations.","Phase shifts and leading order terms are determined by effective moments of the spatial inhomogeneity.","Farfield decay is proportional to the derivatives of the Green's function of an effective Laplacian.","Technically, we use mode filters and conjugacies to an effective Laplacian to establish Fredholm properties of the linearization in Kondratiev spaces.","Spatial localization in a contraction argument is gained through the use of an explicit deformation ansatz and a subtle cancellation in Bloch wave space."],"url":"http://arxiv.org/abs/2405.20265v1","category":"math.AP"}
{"created":"2024-05-30 17:16:31","title":"Transmission of multiple pathogens across species","abstract":"We analyse a model that describes the propagation of many pathogens within and between many species. A branching process approximation is used to compute the probability of disease outbreaks. Special cases of aquatic environments with two host species and one or two pathogens are considered both analytically and computationally.","sentences":["We analyse a model that describes the propagation of many pathogens within and between many species.","A branching process approximation is used to compute the probability of disease outbreaks.","Special cases of aquatic environments with two host species and one or two pathogens are considered both analytically and computationally."],"url":"http://arxiv.org/abs/2405.20264v1","category":"q-bio.PE"}
{"created":"2024-05-30 17:16:29","title":"An algebraic proof of the graph orientation problem dichotomy for forbidden tournaments","abstract":"Using the theory of smooth approximations, we give a new and algebraic proof that for any set F of finite tournaments, the problem of orienting a finite graph whilst avoiding all members of F is either in P or NP-complete. We characterize both cases by algebraic conditions.","sentences":["Using the theory of smooth approximations, we give a new and algebraic proof that for any set F of finite tournaments, the problem of orienting a finite graph whilst avoiding all members of F is either in P or NP-complete.","We characterize both cases by algebraic conditions."],"url":"http://arxiv.org/abs/2405.20263v1","category":"math.CO"}
{"created":"2024-05-30 17:14:46","title":"Ancillary Services Provision by Cross-Voltage-Level Power Flow Control using Flexibility Regions","abstract":"The large-scale integration of distributed renewable energy sources into the electricity grid requires the investigation of new methods to ensure stability. For example, Active Distribution Networks (ADNs) can be used at (sub-) transmission levels for emergency operation, provided robust and efficient control is available. This paper investigates the use of Feasible Operating Regions (FORs) and Flexibility Regions (FRs) for Cross-Voltage-Level Power Flow Control (CPFC). The enhancement of network stability due to the provision of ancillary services is illustrated, as is the need for strengthened cooperation between Transmission (TSOs) and Distribution System Operators (DSOs). Optimal power flow methods are considered, focusing on computational advances through PieceWise Linearization (PWL) and convex relaxation techniques aiming to speed up runtime while keeping high accuracy. To illustrate the algorithms' benefits and drawbacks, they are analyzed using exemplary medium voltage grids.","sentences":["The large-scale integration of distributed renewable energy sources into the electricity grid requires the investigation of new methods to ensure stability.","For example, Active Distribution Networks (ADNs) can be used at (sub-) transmission levels for emergency operation, provided robust and efficient control is available.","This paper investigates the use of Feasible Operating Regions (FORs) and Flexibility Regions (FRs) for Cross-Voltage-Level Power Flow Control (CPFC).","The enhancement of network stability due to the provision of ancillary services is illustrated, as is the need for strengthened cooperation between Transmission (TSOs) and Distribution System Operators (DSOs).","Optimal power flow methods are considered, focusing on computational advances through PieceWise Linearization (PWL) and convex relaxation techniques aiming to speed up runtime while keeping high accuracy.","To illustrate the algorithms' benefits and drawbacks, they are analyzed using exemplary medium voltage grids."],"url":"http://arxiv.org/abs/2405.20260v1","category":"eess.SY"}
{"created":"2024-05-30 17:07:43","title":"Static Subspace Approximation for Random Phase Approximation Correlation Energies: Implementation and Performance","abstract":"Developing theoretical understanding of complex reactions and processes at interfaces requires using methods that go beyond semilocal density functional theory to accurately describe the interactions between solvent, reactants and substrates. Methods based on many-body perturbation theory, such as the random phase approximation (RPA), have previously been limited due to their computational complexity. However, this is now a surmountable barrier due to the advances in computational power available, in particular through modern GPU-based supercomputers. In this work, we describe the implementation of RPA calculations within BerkeleyGW and show its favorable computational performance on large complex systems relevant for catalysis and electrochemistry applications. Our implementation builds off of the static subspace approximation which, by employing a compressed representation of the frequency dependent polarizability, enables the evaluation of the RPA correlation energy with significant acceleration and systematically controllable accuracy. We find that the computational cost of calculating the RPA correlation energy scales only linearly with system size for systems containing up to 50 thousand bands, and is expected to scale quadratically thereafter. We also show excellent strong scaling results across several supercomputers, demonstrating the performance and portability of this implementation.","sentences":["Developing theoretical understanding of complex reactions and processes at interfaces requires using methods that go beyond semilocal density functional theory to accurately describe the interactions between solvent, reactants and substrates.","Methods based on many-body perturbation theory, such as the random phase approximation (RPA), have previously been limited due to their computational complexity.","However, this is now a surmountable barrier due to the advances in computational power available, in particular through modern GPU-based supercomputers.","In this work, we describe the implementation of RPA calculations within BerkeleyGW and show its favorable computational performance on large complex systems relevant for catalysis and electrochemistry applications.","Our implementation builds off of the static subspace approximation which, by employing a compressed representation of the frequency dependent polarizability, enables the evaluation of the RPA correlation energy with significant acceleration and systematically controllable accuracy.","We find that the computational cost of calculating the RPA correlation energy scales only linearly with system size for systems containing up to 50 thousand bands, and is expected to scale quadratically thereafter.","We also show excellent strong scaling results across several supercomputers, demonstrating the performance and portability of this implementation."],"url":"http://arxiv.org/abs/2405.20258v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 17:07:07","title":"Conversational Agents to Facilitate Deliberation on Harmful Content in WhatsApp Groups","abstract":"WhatsApp groups have become a hotbed for the propagation of harmful content including misinformation, hate speech, polarizing content, and rumors, especially in Global South countries. Given the platform's end-to-end encryption, moderation responsibilities lie on group admins and members, who rarely contest such content. Another approach is fact-checking, which is unscalable, and can only contest factual content (e.g., misinformation) but not subjective content (e.g., hate speech). Drawing on recent literature, we explore deliberation -- open and inclusive discussion -- as an alternative. We investigate the role of a conversational agent in facilitating deliberation on harmful content in WhatsApp groups. We conducted semi-structured interviews with 21 Indian WhatsApp users, employing a design probe to showcase an example agent. Participants expressed the need for anonymity and recommended AI assistance to reduce the effort required in deliberation. They appreciated the agent's neutrality but pointed out the futility of deliberation in echo chamber groups. Our findings highlight design tensions for such an agent, including privacy versus group dynamics and freedom of speech in private spaces. We discuss the efficacy of deliberation using deliberative theory as a lens, compare deliberation with moderation and fact-checking, and provide design recommendations for future such systems. Ultimately, this work advances CSCW by offering insights into designing deliberative systems for combating harmful content in private group chats on social media.","sentences":["WhatsApp groups have become a hotbed for the propagation of harmful content including misinformation, hate speech, polarizing content, and rumors, especially in Global South countries.","Given the platform's end-to-end encryption, moderation responsibilities lie on group admins and members, who rarely contest such content.","Another approach is fact-checking, which is unscalable, and can only contest factual content (e.g., misinformation) but not subjective content (e.g., hate speech).","Drawing on recent literature, we explore deliberation -- open and inclusive discussion -- as an alternative.","We investigate the role of a conversational agent in facilitating deliberation on harmful content in WhatsApp groups.","We conducted semi-structured interviews with 21 Indian WhatsApp users, employing a design probe to showcase an example agent.","Participants expressed the need for anonymity and recommended AI assistance to reduce the effort required in deliberation.","They appreciated the agent's neutrality but pointed out the futility of deliberation in echo chamber groups.","Our findings highlight design tensions for such an agent, including privacy versus group dynamics and freedom of speech in private spaces.","We discuss the efficacy of deliberation using deliberative theory as a lens, compare deliberation with moderation and fact-checking, and provide design recommendations for future such systems.","Ultimately, this work advances CSCW by offering insights into designing deliberative systems for combating harmful content in private group chats on social media."],"url":"http://arxiv.org/abs/2405.20254v1","category":"cs.HC"}
{"created":"2024-05-30 17:03:08","title":"An Expanding Accretion Disk and a Warm Disk Wind As Seen In the Spectral Evolution of HBC 722","abstract":"We present a comprehensive analysis of the post-outburst evolution of the FU Ori object HBC 722 in optical/near-infrared (NIR) photometry and spectroscopy. Using a modified viscous accretion disk model, we fit the outburst epoch SED to determine the physical parameters of the disk, including $\\dot{M}_\\mathrm{acc} = 10^{-4.0} \\ M_\\odot$ yr$^{-1}$, $R_\\mathrm{inner} = 3.65 \\ R_\\odot$, $i = 79^\\circ$, and a maximum disk temperature of $T_\\mathrm{max} = 5700$ K. We then use a decade of optical/NIR spectra to demonstrate a changing accretion rate drives the visible-range photometric variation, while the NIR shows the outer radius of the active accretion disk expands outward as the outburst progresses. We also identify the major components of the disk system: a plane-parallel disk atmosphere in Keplerian rotation and a 2-part warm disk wind that is collimated near the star and wide-angle at larger radii. The wind is traced by classic wind lines, and appears as a narrow, low-velocity, deep absorption component in several atomic lines spanning the visible spectrum and in the CO 2.29$\\mu$m band. We compare the wind lines to those computed from wind models for other FU Ori systems and rapidly accreting young stellar disks and find a 4000-6000 K wind can explain the observed line profiles. Fitting the progenitor spectrum, we find $M_* = 0.2 \\ M_\\odot$ and $\\dot{M}_\\mathrm{progenitor} = 7.8 \\times 10^{-8} \\ M_\\odot \\ \\mathrm{yr}^{-1}$. Finally, we discuss HBC 722 relative to V960 Mon, another FU Ori object we have previously studied in detail.","sentences":["We present a comprehensive analysis of the post-outburst evolution of the FU Ori object HBC 722 in optical/near-infrared (NIR) photometry and spectroscopy.","Using a modified viscous accretion disk model, we fit the outburst epoch SED to determine the physical parameters of the disk, including $\\dot{M}_\\mathrm{acc} = 10^{-4.0} \\ M_\\odot$ yr$^{-1}$, $R_\\mathrm{inner} = 3.65 \\ R_\\odot$, $i = 79^\\circ$, and a maximum disk temperature of $T_\\mathrm{max} = 5700$ K. We then use a decade of optical/NIR spectra to demonstrate a changing accretion rate drives the visible-range photometric variation, while the NIR shows the outer radius of the active accretion disk expands outward as the outburst progresses.","We also identify the major components of the disk system: a plane-parallel disk atmosphere in Keplerian rotation and a 2-part warm disk wind that is collimated near the star and wide-angle at larger radii.","The wind is traced by classic wind lines, and appears as a narrow, low-velocity, deep absorption component in several atomic lines spanning the visible spectrum and in the CO 2.29$\\mu$m band.","We compare the wind lines to those computed from wind models for other FU Ori systems and rapidly accreting young stellar disks and find a 4000-6000 K wind can explain the observed line profiles.","Fitting the progenitor spectrum, we find $M_*","= 0.2 \\ M_\\odot$ and $\\dot{M}_\\mathrm{progenitor} = 7.8 \\times 10^{-8} \\ M_\\odot \\ \\mathrm{yr}^{-1}$.","Finally, we discuss HBC 722 relative to V960 Mon, another FU Ori object we have previously studied in detail."],"url":"http://arxiv.org/abs/2405.20251v1","category":"astro-ph.SR"}
{"created":"2024-05-30 16:59:43","title":"Image-to-Joint Inverse Kinematic of a Supportive Continuum Arm Using Deep Learning","abstract":"In this work, a deep learning-based technique is used to study the image-to-joint inverse kinematics of a tendon-driven supportive continuum arm. An eye-off-hand configuration is considered by mounting a camera at a fixed pose with respect to the inertial frame attached at the arm base. This camera captures an image for each distinct joint variable at each sampling time to construct the training dataset. This dataset is then employed to adapt a feed-forward deep convolutional neural network, namely the modified VGG-16 model, to estimate the joint variable. One thousand images are recorded to train the deep network, and transfer learning and fine-tuning techniques are applied to the modified VGG-16 to further improve the training. Finally, training is also completed with a larger dataset of images that are affected by various types of noises, changes in illumination, and partial occlusion. The main contribution of this research is the development of an image-to-joint network that can estimate the joint variable given an image of the arm, even if the image is not captured in an ideal condition. The key benefits of this research are twofold: 1) image-to-joint mapping can offer a real-time alternative to computationally complex inverse kinematic mapping through analytical models; and 2) the proposed technique can provide robustness against noise, occlusion, and changes in illumination. The dataset is publicly available on Kaggle.","sentences":["In this work, a deep learning-based technique is used to study the image-to-joint inverse kinematics of a tendon-driven supportive continuum arm.","An eye-off-hand configuration is considered by mounting a camera at a fixed pose with respect to the inertial frame attached at the arm base.","This camera captures an image for each distinct joint variable at each sampling time to construct the training dataset.","This dataset is then employed to adapt a feed-forward deep convolutional neural network, namely the modified VGG-16 model, to estimate the joint variable.","One thousand images are recorded to train the deep network, and transfer learning and fine-tuning techniques are applied to the modified VGG-16 to further improve the training.","Finally, training is also completed with a larger dataset of images that are affected by various types of noises, changes in illumination, and partial occlusion.","The main contribution of this research is the development of an image-to-joint network that can estimate the joint variable given an image of the arm, even if the image is not captured in an ideal condition.","The key benefits of this research are twofold: 1) image-to-joint mapping can offer a real-time alternative to computationally complex inverse kinematic mapping through analytical models; and 2) the proposed technique can provide robustness against noise, occlusion, and changes in illumination.","The dataset is publicly available on Kaggle."],"url":"http://arxiv.org/abs/2405.20248v1","category":"cs.RO"}
{"created":"2024-05-30 16:33:01","title":"Distributed maze exploration using multiple agents and optimal goal assignment","abstract":"Robotic exploration has long captivated researchers aiming to map complex environments efficiently. Techniques such as potential fields and frontier exploration have traditionally been employed in this pursuit, primarily focusing on solitary agents. Recent advancements have shifted towards optimizing exploration efficiency through multiagent systems. However, many existing approaches overlook critical real-world factors, such as broadcast range limitations, communication costs, and coverage overlap. This paper addresses these gaps by proposing a distributed maze exploration strategy (CU-LVP) that assumes constrained broadcast ranges and utilizes Voronoi diagrams for better area partitioning. By adapting traditional multiagent methods to distributed environments with limited broadcast ranges, this study evaluates their performance across diverse maze topologies, demonstrating the efficacy and practical applicability of the proposed method. The code and experimental results supporting this study are available in the following repository: https://github.com/manouslinard/multiagent-exploration/.","sentences":["Robotic exploration has long captivated researchers aiming to map complex environments efficiently.","Techniques such as potential fields and frontier exploration have traditionally been employed in this pursuit, primarily focusing on solitary agents.","Recent advancements have shifted towards optimizing exploration efficiency through multiagent systems.","However, many existing approaches overlook critical real-world factors, such as broadcast range limitations, communication costs, and coverage overlap.","This paper addresses these gaps by proposing a distributed maze exploration strategy (CU-LVP) that assumes constrained broadcast ranges and utilizes Voronoi diagrams for better area partitioning.","By adapting traditional multiagent methods to distributed environments with limited broadcast ranges, this study evaluates their performance across diverse maze topologies, demonstrating the efficacy and practical applicability of the proposed method.","The code and experimental results supporting this study are available in the following repository: https://github.com/manouslinard/multiagent-exploration/."],"url":"http://arxiv.org/abs/2405.20232v1","category":"cs.MA"}
{"created":"2024-05-30 16:30:19","title":"High-order Van Hove singularities and their connection to flat bands","abstract":"The flattening of single-particle band structures plays an important role in the quest for novel quantum states of matter due to the crucial role of interactions. Recent advances in theory and experiment made it possible to construct and tune systems with nearly flat bands, ranging from graphene multilayers and moire' materials to kagome' metals and ruthenates. While theoretical models predict exactly flat bands under certain ideal conditions, evidence was provided that these systems host high-order Van Hove points, i.e., points of high local band flatness and power-law divergence in energy of the density of states. In this review, we examine recent developments in engineering and realising such weakly dispersive bands. We focus on high-order Van Hove singularities and explore their connection to exactly flat bands. We provide classification schemes and discuss interaction effects. We also review experimental evidence for high-order Van Hove singularities and point out future research directions.","sentences":["The flattening of single-particle band structures plays an important role in the quest for novel quantum states of matter due to the crucial role of interactions.","Recent advances in theory and experiment made it possible to construct and tune systems with nearly flat bands, ranging from graphene multilayers and moire' materials to kagome' metals and ruthenates.","While theoretical models predict exactly flat bands under certain ideal conditions, evidence was provided that these systems host high-order Van Hove points, i.e., points of high local band flatness and power-law divergence in energy of the density of states.","In this review, we examine recent developments in engineering and realising such weakly dispersive bands.","We focus on high-order Van Hove singularities and explore their connection to exactly flat bands.","We provide classification schemes and discuss interaction effects.","We also review experimental evidence for high-order Van Hove singularities and point out future research directions."],"url":"http://arxiv.org/abs/2405.20226v1","category":"cond-mat.str-el"}
{"created":"2024-05-30 16:24:35","title":"Astrophysical aspects of $^{12}$C$(p,\u03b3)^{13}$N reaction","abstract":"The Carbon-Nitrogen-Oxygen (CNO) cycle is fundamental to the process of hydrogen burning in stars, serving as a pivotal mechanism. At its core, the primary reaction involves the radiative capture of a proton by $^{12}$C, crucially influencing the isotopic ratio of $^{12}$C to $^{13}$C observed in celestial bodies, including our Solar System. We have addressed this reaction mechanism by extrapolating to low-energy cross sections and S-factors with the aid of astrophysical R-matrix. Our investigation aims to shed light on its implications for nuclear reaction rates, thus influencing the abundance ratio of $^{12}$C to $^{13}$C in the cosmic environment.","sentences":["The Carbon-Nitrogen-Oxygen (CNO) cycle is fundamental to the process of hydrogen burning in stars, serving as a pivotal mechanism.","At its core, the primary reaction involves the radiative capture of a proton by $^{12}$C, crucially influencing the isotopic ratio of $^{12}$C to $^{13}$C observed in celestial bodies, including our Solar System.","We have addressed this reaction mechanism by extrapolating to low-energy cross sections and S-factors with the aid of astrophysical R-matrix.","Our investigation aims to shed light on its implications for nuclear reaction rates, thus influencing the abundance ratio of $^{12}$C to $^{13}$C in the cosmic environment."],"url":"http://arxiv.org/abs/2405.20223v1","category":"nucl-th"}
{"created":"2024-05-30 16:19:13","title":"BeerReview: A Blockchain-enabled Peer Review Platform","abstract":"In an era of increasing concerns over intellectual property rights, traditional peer review systems face challenges including plagiarism, malicious attacks, and unauthorized data access. BeerReview, a blockchain-enabled peer review platform, offers a robust solution, enabling experts and scholars to participate actively in the review process without concerns about plagiarism or security threats. Following the completion of its alpha testing, BeerReview demonstrates the potential for expanded deployment. This platform offers improved convenience and more robust intellectual property protection within the peer review process with open source initiative.","sentences":["In an era of increasing concerns over intellectual property rights, traditional peer review systems face challenges including plagiarism, malicious attacks, and unauthorized data access.","BeerReview, a blockchain-enabled peer review platform, offers a robust solution, enabling experts and scholars to participate actively in the review process without concerns about plagiarism or security threats.","Following the completion of its alpha testing, BeerReview demonstrates the potential for expanded deployment.","This platform offers improved convenience and more robust intellectual property protection within the peer review process with open source initiative."],"url":"http://arxiv.org/abs/2405.20220v1","category":"cs.DC"}
{"created":"2024-05-30 16:19:05","title":"System Identification for Lithium-Ion Batteries with Nonlinear Coupled Electro-Thermal Dynamics via Bayesian Optimization","abstract":"Essential to various practical applications of lithium-ion batteries is the availability of accurate equivalent circuit models. This paper presents a new coupled electro-thermal model for batteries and studies how to extract it from data. We consider the problem of maximum likelihood parameter estimation, which, however, is nontrivial to solve as the model is nonlinear in both its dynamics and measurement. We propose to leverage the Bayesian optimization approach, owing to its machine learning-driven capability in handling complex optimization problems and searching for global optima. To enhance the parameter search efficiency, we dynamically narrow and refine the search space in Bayesian optimization. The proposed system identification approach can efficiently determine the parameters of the coupled electro-thermal model. It is amenable to practical implementation, with few requirements on the experiment, data types, and optimization setups, and well applicable to many other battery models.","sentences":["Essential to various practical applications of lithium-ion batteries is the availability of accurate equivalent circuit models.","This paper presents a new coupled electro-thermal model for batteries and studies how to extract it from data.","We consider the problem of maximum likelihood parameter estimation, which, however, is nontrivial to solve as the model is nonlinear in both its dynamics and measurement.","We propose to leverage the Bayesian optimization approach, owing to its machine learning-driven capability in handling complex optimization problems and searching for global optima.","To enhance the parameter search efficiency, we dynamically narrow and refine the search space in Bayesian optimization.","The proposed system identification approach can efficiently determine the parameters of the coupled electro-thermal model.","It is amenable to practical implementation, with few requirements on the experiment, data types, and optimization setups, and well applicable to many other battery models."],"url":"http://arxiv.org/abs/2405.20219v1","category":"eess.SY"}
{"created":"2024-05-30 16:14:38","title":"The unexpected uses of a bowling pin: anisotropic flow in fixed-target $^{208}$Pb+$^{20}$Ne collisions as a probe of quark-gluon plasma","abstract":"The System for Measuring Overlap with Gas (SMOG2) at the LHCb detector enables the study of fixed-target ion-ion collisions at relativistic energies ($\\sqrt{s_{\\rm NN}}\\sim100$ GeV in the centre-of-mass). With input from \\textit{ab initio} calculations of the structure of $^{16}$O and $^{20}$Ne, we compute 3+1D hydrodynamic predictions for the anisotropic flow of Pb+Ne and Pb+O collisions, to be tested with upcoming LHCb data. This will allow the detailed study of quark-gluon plasma (QGP) formation as well as experimental tests of the predicted nuclear shapes. Elliptic flow ($v_2$) in Pb+Ne collisions is greatly enhanced compared to the Pb+O baseline due to the shape of $^{20}$Ne, which is deformed in a bowling-pin geometry. Owing to the large $^{208}$Pb radius, this effect is seen in a broad centrality range, a unique feature of this collision configuration. Larger elliptic flow further enhances the quadrangular flow ($v_4$) of Pb+Ne collisions via non-linear coupling, and impacts the sign of the kurtosis of the elliptic flow vector distribution ($c_2\\{4\\}$). Exploiting the shape of $^{20}$Ne proves thus an ideal method to investigate the formation of QGP in fixed-target experiments at LHCb, and demonstrates the power of SMOG2 as a tool to image nuclear ground states.","sentences":["The System for Measuring Overlap with Gas (SMOG2) at the LHCb detector enables the study of fixed-target ion-ion collisions at relativistic energies ($\\sqrt{s_{\\rm NN}}\\sim100$ GeV in the centre-of-mass).","With input from \\textit{ab initio} calculations of the structure of $^{16}$O and $^{20}$Ne, we compute 3+1D hydrodynamic predictions for the anisotropic flow of Pb+Ne and Pb+O collisions, to be tested with upcoming LHCb data.","This will allow the detailed study of quark-gluon plasma (QGP) formation as well as experimental tests of the predicted nuclear shapes.","Elliptic flow ($v_2$) in Pb+Ne collisions is greatly enhanced compared to the Pb+O baseline due to the shape of $^{20}$Ne, which is deformed in a bowling-pin geometry.","Owing to the large $^{208}$Pb radius, this effect is seen in a broad centrality range, a unique feature of this collision configuration.","Larger elliptic flow further enhances the quadrangular flow ($v_4$) of Pb+Ne collisions via non-linear coupling, and impacts the sign of the kurtosis of the elliptic flow vector distribution ($c_2\\{4\\}$).","Exploiting the shape of $^{20}$Ne proves thus an ideal method to investigate the formation of QGP in fixed-target experiments at LHCb, and demonstrates the power of SMOG2 as a tool to image nuclear ground states."],"url":"http://arxiv.org/abs/2405.20210v1","category":"nucl-th"}
{"created":"2024-05-30 16:14:22","title":"Lasso-based state estimation for cyber-physical systems under sensor attacks","abstract":"The development of algorithms for secure state estimation in vulnerable cyber-physical systems has been gaining attention in the last years. A consolidated assumption is that an adversary can tamper a relatively small number of sensors. In the literature, block-sparsity methods exploit this prior information to recover the attack locations and the state of the system.   In this paper, we propose an alternative, Lasso-based approach and we analyse its effectiveness. In particular, we theoretically derive conditions that guarantee successful attack/state recovery, independently of established time sparsity patterns. Furthermore, we develop a sparse state observer, by starting from the iterative soft thresholding algorithm for Lasso, to perform online estimation.   Through several numerical experiments, we compare the proposed methods to the state-of-the-art algorithms.","sentences":["The development of algorithms for secure state estimation in vulnerable cyber-physical systems has been gaining attention in the last years.","A consolidated assumption is that an adversary can tamper a relatively small number of sensors.","In the literature, block-sparsity methods exploit this prior information to recover the attack locations and the state of the system.   ","In this paper, we propose an alternative, Lasso-based approach and we analyse its effectiveness.","In particular, we theoretically derive conditions that guarantee successful attack/state recovery, independently of established time sparsity patterns.","Furthermore, we develop a sparse state observer, by starting from the iterative soft thresholding algorithm for Lasso, to perform online estimation.   ","Through several numerical experiments, we compare the proposed methods to the state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2405.20209v1","category":"math.OC"}
{"created":"2024-05-30 16:08:13","title":"Cavity-Enhanced Emission and Absorption of Color Centers in a Diamond Membrane With Selectable Strain","abstract":"Group IV color centers in diamond are among the most promising optically active spin systems with strong optical transitions and long spin coherences. The ground-state splitting of the center is particularly important to suppress the interaction with coherence-limiting phonons, which improves the coherence properties and sets the upper limit for the operating temperature. Negatively charged silicon-vacancy centers have an ordinary ground-state splitting of only 48GHz, resulting in required temperatures below one Kelvin, which can only be achieved by dilution refrigerators. Here, we increase the ground-state splitting by up to an order of magnitude by induced strain in a single-crystal diamond membrane. Furthermore, we demonstrate cavity-assisted spectroscopy enabled by coupling the emitter ensemble with a selectable strain to the mode of a Fabry-Perot microcavity. Calculation of the absorption cross-section yields $\\sigma_{ens} = $4.9*10^-11 cm^2. Together with the Purcell-enhanced twofold reduction in emitter lifetime below 1ns, this makes the system a promising spin-photon interface at moderate temperatures of 4K.","sentences":["Group IV color centers in diamond are among the most promising optically active spin systems with strong optical transitions and long spin coherences.","The ground-state splitting of the center is particularly important to suppress the interaction with coherence-limiting phonons, which improves the coherence properties and sets the upper limit for the operating temperature.","Negatively charged silicon-vacancy centers have an ordinary ground-state splitting of only 48GHz, resulting in required temperatures below one Kelvin, which can only be achieved by dilution refrigerators.","Here, we increase the ground-state splitting by up to an order of magnitude by induced strain in a single-crystal diamond membrane.","Furthermore, we demonstrate cavity-assisted spectroscopy enabled by coupling the emitter ensemble with a selectable strain to the mode of a Fabry-Perot microcavity.","Calculation of the absorption cross-section yields $\\sigma_{ens} = $4.9*10^-11","cm^2.","Together with the Purcell-enhanced twofold reduction in emitter lifetime below 1ns, this makes the system a promising spin-photon interface at moderate temperatures of 4K."],"url":"http://arxiv.org/abs/2405.20205v1","category":"quant-ph"}
{"created":"2024-05-30 15:58:49","title":"Using Large Language Models for Humanitarian Frontline Negotiation: Opportunities and Considerations","abstract":"Humanitarian negotiations in conflict zones, called \\emph{frontline negotiation}, are often highly adversarial, complex, and high-risk. Several best-practices have emerged over the years that help negotiators extract insights from large datasets to navigate nuanced and rapidly evolving scenarios. Recent advances in large language models (LLMs) have sparked interest in the potential for AI to aid decision making in frontline negotiation. Through in-depth interviews with 13 experienced frontline negotiators, we identified their needs for AI-assisted case analysis and creativity support, as well as concerns surrounding confidentiality and model bias. We further explored the potential for AI augmentation of three standard tools used in frontline negotiation planning. We evaluated the quality and stability of our ChatGPT-based negotiation tools in the context of two real cases. Our findings highlight the potential for LLMs to enhance humanitarian negotiations and underscore the need for careful ethical and practical considerations.","sentences":["Humanitarian negotiations in conflict zones, called \\emph{frontline negotiation}, are often highly adversarial, complex, and high-risk.","Several best-practices have emerged over the years that help negotiators extract insights from large datasets to navigate nuanced and rapidly evolving scenarios.","Recent advances in large language models (LLMs) have sparked interest in the potential for AI to aid decision making in frontline negotiation.","Through in-depth interviews with 13 experienced frontline negotiators, we identified their needs for AI-assisted case analysis and creativity support, as well as concerns surrounding confidentiality and model bias.","We further explored the potential for AI augmentation of three standard tools used in frontline negotiation planning.","We evaluated the quality and stability of our ChatGPT-based negotiation tools in the context of two real cases.","Our findings highlight the potential for LLMs to enhance humanitarian negotiations and underscore the need for careful ethical and practical considerations."],"url":"http://arxiv.org/abs/2405.20195v1","category":"cs.HC"}
{"created":"2024-05-30 15:53:22","title":"An Ultra-High Vacuum Scanning Tunneling Microscope with Pulse Tube and Joule-Thomson cooling operating at sub-pm z-noise","abstract":"We describe a compact ultra-high vacuum (UHV) scanning tunneling microscope (STM) system that does not need any external supply of cooling liquids. It achieves temperatures down to 1.5 K and a z-noise down to 300 fmRMS for the frequency range of 0.1 Hz - 5 kHz (feedback loop off). It employs a pulse tube cryocooler (PTC) and a Joule-Thomson (JT) stage inducing only small temperature oscillations at the STM with amplitude below 1 mK. The challenge to combine an effective vibrational decoupling from the PTC with sufficient thermal conduction is tackled by a multipartite approach. We realize a minimal stiffness of the UHV bellows that connect the PTC and the STM chamber. Fine Copper wires mechanically decouple the PTC stages from cooling plates that carry the thermal shields, the JT stage and the STM. Soft springs decouple the STM from the JT stage. Finally, the STM body has an optimized conical shape and is made of the light and stiff material Shapal Hi MSoft such that a strong reduction of low frequency vibrations results for the tunnel junction. The voltage noise in the tunnel junction is 0.12 mV and an RF antenna close to the tunnel junction provides radio frequency excitations up to 40 GHz with amplitudes up to 10 mV.","sentences":["We describe a compact ultra-high vacuum (UHV) scanning tunneling microscope (STM) system that does not need any external supply of cooling liquids.","It achieves temperatures down to 1.5 K and a z-noise down to 300 fmRMS for the frequency range of 0.1 Hz - 5 kHz (feedback loop off).","It employs a pulse tube cryocooler (PTC) and a Joule-Thomson (JT) stage inducing only small temperature oscillations at the STM with amplitude below 1 mK. The challenge to combine an effective vibrational decoupling from the PTC with sufficient thermal conduction is tackled by a multipartite approach.","We realize a minimal stiffness of the UHV bellows that connect the PTC and the STM chamber.","Fine Copper wires mechanically decouple the PTC stages from cooling plates that carry the thermal shields, the JT stage and the STM.","Soft springs decouple the STM from the JT stage.","Finally, the STM body has an optimized conical shape and is made of the light and stiff material Shapal Hi MSoft such that a strong reduction of low frequency vibrations results for the tunnel junction.","The voltage noise in the tunnel junction is 0.12 mV and an RF antenna close to the tunnel junction provides radio frequency excitations up to 40 GHz with amplitudes up to 10 mV."],"url":"http://arxiv.org/abs/2405.20187v1","category":"physics.ins-det"}
{"created":"2024-05-30 15:47:48","title":"Non-intrusive data-driven model order reduction for circuits based on Hammerstein architectures","abstract":"We demonstrate that data-driven system identification techniques can provide a basis for effective, non-intrusive model order reduction (MOR) for common circuits that are key building blocks in microelectronics. Our approach is motivated by the practical operation of these circuits and utilizes a canonical Hammerstein architecture. To demonstrate the approach we develop a parsimonious Hammerstein model for a non-linear CMOS differential amplifier. We train this model on a combination of direct current (DC) and transient Spice (Xyce) circuit simulation data using a novel sequential strategy to identify the static nonlinear and linear dynamical parts of the model. Simulation results show that the Hammerstein model is an effective surrogate for the differential amplifier circuit that accurately and efficiently reproduces its behavior over a wide range of operating points and input frequencies.","sentences":["We demonstrate that data-driven system identification techniques can provide a basis for effective, non-intrusive model order reduction (MOR) for common circuits that are key building blocks in microelectronics.","Our approach is motivated by the practical operation of these circuits and utilizes a canonical Hammerstein architecture.","To demonstrate the approach we develop a parsimonious Hammerstein model for a non-linear CMOS differential amplifier.","We train this model on a combination of direct current (DC) and transient Spice (Xyce) circuit simulation data using a novel sequential strategy to identify the static nonlinear and linear dynamical parts of the model.","Simulation results show that the Hammerstein model is an effective surrogate for the differential amplifier circuit that accurately and efficiently reproduces its behavior over a wide range of operating points and input frequencies."],"url":"http://arxiv.org/abs/2405.20178v1","category":"eess.SY"}
{"created":"2024-05-30 15:45:40","title":"On the nested algebraic Bethe ansatz for spin chains with simple $\\mathfrak{g}$-symmetry","abstract":"We propose a new framework for the nested algebraic Bethe ansatz for a closed, rational spin chain with $\\mathfrak{g}$-symmetry for any simple Lie algebra $\\mathfrak{g}$. Starting the nesting process by removing a single simple root from $\\mathfrak{g}$, we use the residual $U(1)$ charge and the block Gauss decomposition of the $R$-matrix to derive many standard results in the Bethe ansatz, such as the nesting of Yangian algebras, and the AB commutation relation.","sentences":["We propose a new framework for the nested algebraic Bethe ansatz for a closed, rational spin chain with $\\mathfrak{g}$-symmetry for any simple Lie algebra $\\mathfrak{g}$. Starting the nesting process by removing a single simple root from $\\mathfrak{g}$, we use the residual $U(1)$ charge and the block Gauss decomposition of the $R$-matrix to derive many standard results in the Bethe ansatz, such as the nesting of Yangian algebras, and the AB commutation relation."],"url":"http://arxiv.org/abs/2405.20177v1","category":"math-ph"}
{"created":"2024-05-30 15:45:24","title":"The Solar System Notification Alert Processing System (SNAPS): Asteroid Population Outlier Detection","abstract":"The Solar System Notification Alert Processing System (SNAPS) is a ZTF and Rubin Observatory alert broker that will send alerts to the community regarding interesting events in the Solar System. SNAPS is actively monitoring Solar System objects and one of its functions is to compare objects (primarily main belt asteroids) to one another to find those that are outliers relative to the population. In this paper, we use the SNAPShot1 dataset which contains 31,693 objects from ZTF and derive outlier scores for each of these objects. SNAPS employs an unsupervised approach; consequently, to derive outlier rankings for each object, we propose four different outlier metrics such that we can explore variants of outlier scores and add confidence to outlier rankings. We also provide outlier scores for each object in each permutation of 15 feature spaces, between 2 and 15 features, which yields 32,752 total feature spaces. We show that we can derive population outlier rankings each month at Rubin Observatory scale using four Nvidia A100 GPUs, and present several avenues of scientific investigation that can be explored using population outlier detection.","sentences":["The Solar System Notification Alert Processing System (SNAPS) is a ZTF and Rubin Observatory alert broker that will send alerts to the community regarding interesting events in the Solar System.","SNAPS is actively monitoring Solar System objects and one of its functions is to compare objects (primarily main belt asteroids) to one another to find those that are outliers relative to the population.","In this paper, we use the SNAPShot1 dataset which contains 31,693 objects from ZTF and derive outlier scores for each of these objects.","SNAPS employs an unsupervised approach; consequently, to derive outlier rankings for each object, we propose four different outlier metrics such that we can explore variants of outlier scores and add confidence to outlier rankings.","We also provide outlier scores for each object in each permutation of 15 feature spaces, between 2 and 15 features, which yields 32,752 total feature spaces.","We show that we can derive population outlier rankings each month at Rubin Observatory scale using four Nvidia A100 GPUs, and present several avenues of scientific investigation that can be explored using population outlier detection."],"url":"http://arxiv.org/abs/2405.20176v1","category":"astro-ph.EP"}
{"created":"2024-05-30 15:44:28","title":"Eclipse Qrisp QAOA: description and preliminary comparison with Qiskit counterparts","abstract":"This paper focuses on the presentation and evaluation of the high-level quantum programming language Eclipse Qrisp. The presented framework, used for developing and compiling quantum algorithms, is measured in terms of efficiency for its implementation of the Quantum Approximation Optimization Algorithm (QAOA) Module. We measure this efficiency and compare it against two alternative QAOA algorithm implementations using IBM's Qiskit toolkit. The evaluation process has been carried out over a benchmark composed of 15 instances of the well-known Maximum Cut Problem. Through this preliminary experimentation, Eclipse Qrisp demonstrated promising results, outperforming both versions of its counterparts in terms of results quality and circuit complexity.","sentences":["This paper focuses on the presentation and evaluation of the high-level quantum programming language Eclipse Qrisp.","The presented framework, used for developing and compiling quantum algorithms, is measured in terms of efficiency for its implementation of the Quantum Approximation Optimization Algorithm (QAOA) Module.","We measure this efficiency and compare it against two alternative QAOA algorithm implementations using IBM's Qiskit toolkit.","The evaluation process has been carried out over a benchmark composed of 15 instances of the well-known Maximum Cut Problem.","Through this preliminary experimentation, Eclipse Qrisp demonstrated promising results, outperforming both versions of its counterparts in terms of results quality and circuit complexity."],"url":"http://arxiv.org/abs/2405.20173v1","category":"quant-ph"}
{"created":"2024-05-30 15:12:07","title":"Bifurcation enhances temporal information encoding in the olfactory periphery","abstract":"Living systems continually respond to signals from the surrounding environment. Survival requires that their responses adapt quickly and robustly to the changes in the environment. One particularly challenging example is olfactory navigation in turbulent plumes, where animals experience highly intermittent odor signals while odor concentration varies over many length- and timescales. Here, we show theoretically that Drosophila olfactory receptor neurons (ORNs) can exploit proximity to a bifurcation point of their firing dynamics to reliably extract information about the timing and intensity of fluctuations in the odor signal, which have been shown to be critical for odor-guided navigation. Close to the bifurcation, the system is intrinsically invariant to signal variance, and information about the timing, duration, and intensity of odor fluctuations is transferred efficiently. Importantly, we find that proximity to the bifurcation is maintained by mean adaptation alone and therefore does not require any additional feedback mechanism or fine-tuning. Using a biophysical model with calcium-based feedback, we demonstrate that this mechanism can explain the measured adaptation characteristics of Drosophila ORNs.","sentences":["Living systems continually respond to signals from the surrounding environment.","Survival requires that their responses adapt quickly and robustly to the changes in the environment.","One particularly challenging example is olfactory navigation in turbulent plumes, where animals experience highly intermittent odor signals while odor concentration varies over many length- and timescales.","Here, we show theoretically that Drosophila olfactory receptor neurons (ORNs) can exploit proximity to a bifurcation point of their firing dynamics to reliably extract information about the timing and intensity of fluctuations in the odor signal, which have been shown to be critical for odor-guided navigation.","Close to the bifurcation, the system is intrinsically invariant to signal variance, and information about the timing, duration, and intensity of odor fluctuations is transferred efficiently.","Importantly, we find that proximity to the bifurcation is maintained by mean adaptation alone and therefore does not require any additional feedback mechanism or fine-tuning.","Using a biophysical model with calcium-based feedback, we demonstrate that this mechanism can explain the measured adaptation characteristics of Drosophila ORNs."],"url":"http://arxiv.org/abs/2405.20135v1","category":"q-bio.NC"}
{"created":"2024-05-30 15:11:45","title":"Unveiling complex magnetic field configurations in red giant stars","abstract":"Recent measurements of magnetic field strength inside the radiative interior of red giant stars open the way towards the characterization of the geometry of stable large-scale magnetic fields. However, current measurements do not properly constrain the topology of magnetic fields due to degeneracies on the observed magnetic field signature on such $\\ell=1$ mode frequencies. Efforts focused towards unambiguous detections of magnetic field configurations are now key to better understand angular momentum transport in stars. We investigate the detectability of complex magnetic field topologies inside the radiative interior of red giants. We focus on a field composed of a combination of a dipole and a quadrupole (quadrudipole), and on an offset field. We explore the potential of probing such magnetic field topologies from a combined measurement of magnetic signatures on $\\ell=1$ and quadrupolar ($\\ell=2$) mixed mode oscillation frequencies. We first derive the asymptotic theoretical formalism for computing the asymmetric signature in frequency pattern for $\\ell=2$ modes due to a quadrudipole magnetic field. The degeneracy of the quadrudipole with a dipole is lifted when considering both $\\ell=1$ and $\\ell=2$ mode frequencies. In addition to the analytical derivation for the quadrudipole, we present the prospect for complex magnetic field inversions using magnetic sensitivity kernels from standard perturbation analysis for forward modeling. Using this method, we demonstrate that offset fields may be mistaken for weak and centered magnetic fields, resulting in underestimating magnetic field strength in stellar cores. We emphasize the need to characterize $\\ell=2$ mixed-mode frequencies, (along with the currently characterized $\\ell=1$ mixed modes), to unveil the higher-order components of the geometry of buried magnetic fields, and better constrain angular momentum transport inside stars.","sentences":["Recent measurements of magnetic field strength inside the radiative interior of red giant stars open the way towards the characterization of the geometry of stable large-scale magnetic fields.","However, current measurements do not properly constrain the topology of magnetic fields due to degeneracies on the observed magnetic field signature on such $\\ell=1$ mode frequencies.","Efforts focused towards unambiguous detections of magnetic field configurations are now key to better understand angular momentum transport in stars.","We investigate the detectability of complex magnetic field topologies inside the radiative interior of red giants.","We focus on a field composed of a combination of a dipole and a quadrupole (quadrudipole), and on an offset field.","We explore the potential of probing such magnetic field topologies from a combined measurement of magnetic signatures on $\\ell=1$ and quadrupolar ($\\ell=2$) mixed mode oscillation frequencies.","We first derive the asymptotic theoretical formalism for computing the asymmetric signature in frequency pattern for $\\ell=2$ modes due to a quadrudipole magnetic field.","The degeneracy of the quadrudipole with a dipole is lifted when considering both $\\ell=1$ and $\\ell=2$ mode frequencies.","In addition to the analytical derivation for the quadrudipole, we present the prospect for complex magnetic field inversions using magnetic sensitivity kernels from standard perturbation analysis for forward modeling.","Using this method, we demonstrate that offset fields may be mistaken for weak and centered magnetic fields, resulting in underestimating magnetic field strength in stellar cores.","We emphasize the need to characterize $\\ell=2$ mixed-mode frequencies, (along with the currently characterized $\\ell=1$ mixed modes), to unveil the higher-order components of the geometry of buried magnetic fields, and better constrain angular momentum transport inside stars."],"url":"http://arxiv.org/abs/2405.20133v1","category":"astro-ph.SR"}
{"created":"2024-05-30 15:07:59","title":"Realization of a Rydberg-dressed extended Bose Hubbard model","abstract":"The competition of different length scales in quantum many-body systems leads to various novel phenomena, including the emergence of correlated dynamics or non-local order. To access and investigate such effects in an itinerant lattice-based quantum simulator, it has been proposed to introduce tunable extended-range interactions using off-resonant optical coupling to Rydberg states. However, experimental realizations of such \"Rydberg dressing\" have so far mostly concentrated on spin systems without motion. Here, we overcome a number of experimental challenges limiting previous work and realize an effective one-dimensional extended Bose-Hubbard model (eBHM). Harnessing our quantum gas microscope, we probe the correlated out-of-equilibrium dynamics of extended-range repulsively-bound pairs at low filling, and kinetically-constrained \"hard rods\" at half filling. Near equilibrium, we observe density ordering when adiabatically turning on the extended-range interactions. Our results demonstrate the versatility of Rydberg dressing in engineering itinerant optical lattice-based quantum simulators and pave the way to realizing novel light-controlled extended-range interacting quantum many-body systems.","sentences":["The competition of different length scales in quantum many-body systems leads to various novel phenomena, including the emergence of correlated dynamics or non-local order.","To access and investigate such effects in an itinerant lattice-based quantum simulator, it has been proposed to introduce tunable extended-range interactions using off-resonant optical coupling to Rydberg states.","However, experimental realizations of such \"Rydberg dressing\" have so far mostly concentrated on spin systems without motion.","Here, we overcome a number of experimental challenges limiting previous work and realize an effective one-dimensional extended Bose-Hubbard model (eBHM).","Harnessing our quantum gas microscope, we probe the correlated out-of-equilibrium dynamics of extended-range repulsively-bound pairs at low filling, and kinetically-constrained \"hard rods\" at half filling.","Near equilibrium, we observe density ordering when adiabatically turning on the extended-range interactions.","Our results demonstrate the versatility of Rydberg dressing in engineering itinerant optical lattice-based quantum simulators and pave the way to realizing novel light-controlled extended-range interacting quantum many-body systems."],"url":"http://arxiv.org/abs/2405.20128v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-30 14:58:33","title":"Distributed MIMO Precoding with Routing Constraints in Segmented Fronthaul","abstract":"Distributed Multiple-Input and Multiple-Output (D-MIMO) is envisioned to play a significant role in future wireless communication systems as an effective means to improve coverage and capacity. In this paper, we have studied the impact of a practical two-level data routing scheme on radio performance in a downlink D-MIMO scenario with segmented fronthaul. At the first level, a Distributed Unit (DU) is connected to the Aggregating Radio Units (ARUs) that behave as cluster heads for the selected serving RU groups. At the second level, the selected ARUs connect with the additional serving RUs. At each route discovery level, RUs and/or ARUs share information with each other. The aim of the proposed framework is to efficiently select serving RUs and ARUs so that the practical data routing impact for each User Equipment (UE) connection is minimal. The resulting post-routing Signal-to-Interference plus Noise Ratio (SINR) among all UEs is analyzed after the routing constraints have been applied. The results show that limited fronthaul segment capacity causes connection failures with the serving RUs of individual UEs, especially when long routing path lengths are required. Depending on whether the failures occur at the first or the second routing level, a UE may be dropped or its SINR may be reduced. To minimize the DU-ARU connection failures, the segment capacity of the segments closest to the DU is set as double as the remaining segments. When the number of active co-scheduled UEs is kept low enough, practical segment capacities suffice to achieve a zero UE dropping rate. Besides, the proper choice of maximum path length setting should take into account segment capacity and its utilization due to the relation between the two.","sentences":["Distributed Multiple-Input and Multiple-Output (D-MIMO) is envisioned to play a significant role in future wireless communication systems as an effective means to improve coverage and capacity.","In this paper, we have studied the impact of a practical two-level data routing scheme on radio performance in a downlink D-MIMO scenario with segmented fronthaul.","At the first level, a Distributed Unit (DU) is connected to the Aggregating Radio Units (ARUs) that behave as cluster heads for the selected serving RU groups.","At the second level, the selected ARUs connect with the additional serving RUs.","At each route discovery level, RUs and/or ARUs share information with each other.","The aim of the proposed framework is to efficiently select serving RUs and ARUs so that the practical data routing impact for each User Equipment (UE) connection is minimal.","The resulting post-routing Signal-to-Interference plus Noise Ratio (SINR) among all UEs is analyzed after the routing constraints have been applied.","The results show that limited fronthaul segment capacity causes connection failures with the serving RUs of individual UEs, especially when long routing path lengths are required.","Depending on whether the failures occur at the first or the second routing level, a UE may be dropped or its SINR may be reduced.","To minimize the DU-ARU connection failures, the segment capacity of the segments closest to the DU is set as double as the remaining segments.","When the number of active co-scheduled UEs is kept low enough, practical segment capacities suffice to achieve a zero UE dropping rate.","Besides, the proper choice of maximum path length setting should take into account segment capacity and its utilization due to the relation between the two."],"url":"http://arxiv.org/abs/2405.20122v1","category":"eess.SP"}
{"created":"2024-05-30 14:51:01","title":"Phase Transitions in Quantum Many-Body Scars","abstract":"We propose a type of phase transition in quantum many-body systems, which occurs in highly excited quantum many-body scar states, while the rest of the spectrum is largely unaffected. Such scar state phase transitions can be realized by embedding a matrix product state, known to undergo a phase transition, as a scar state into the thermal spectrum of a parent Hamiltonian. We find numerically that the mechanism for the scar state phase transition involves the formation or presence of low-entropy states at energies similar to the scar state in the vicinity of the phase transition point.","sentences":["We propose a type of phase transition in quantum many-body systems, which occurs in highly excited quantum many-body scar states, while the rest of the spectrum is largely unaffected.","Such scar state phase transitions can be realized by embedding a matrix product state, known to undergo a phase transition, as a scar state into the thermal spectrum of a parent Hamiltonian.","We find numerically that the mechanism for the scar state phase transition involves the formation or presence of low-entropy states at energies similar to the scar state in the vicinity of the phase transition point."],"url":"http://arxiv.org/abs/2405.20113v1","category":"quant-ph"}
{"created":"2024-05-30 14:44:38","title":"Complete characterization of symmetric Kubo-Ando operator means satisfying Moln\u00e1r's weak associativity","abstract":"We provide a complete characterization of a subclass of means of positive operators in the class of symmetric Kubo-Ando means that was first introduced and studied in L. Moln\\'ar, ``Characterizations of certain means of positive operators,\" Linear Algebra Appl. 567 (2019) 143-166. In Theorem 6 of that paper, he gives a characterization of this subclass (which we call the Moln\\'ar class of means) in terms of the operator monotone functions representing the means, which includes the geometric mean. Furthermore, he leaves open the problem to determine if the geometric mean is the only such mean in that subclass. Here we give an alternative characterization of the Moln\\'ar class of means in terms of the boundary-values of bounded harmonic functions on certain rectangles which completely characterizes this class of means. Moreover, we use this to construct an explicit example of a mean in the subclass that is not the geometric means thereby solving the open problem of L. Moln\\'ar.","sentences":["We provide a complete characterization of a subclass of means of positive operators in the class of symmetric Kubo-Ando means that was first introduced and studied in L. Moln\\'ar, ``Characterizations of certain means of positive operators,\" Linear Algebra Appl.","567 (2019) 143-166.","In Theorem 6 of that paper, he gives a characterization of this subclass (which we call the Moln\\'ar class of means) in terms of the operator monotone functions representing the means, which includes the geometric mean.","Furthermore, he leaves open the problem to determine if the geometric mean is the only such mean in that subclass.","Here we give an alternative characterization of the Moln\\'ar class of means in terms of the boundary-values of bounded harmonic functions on certain rectangles which completely characterizes this class of means.","Moreover, we use this to construct an explicit example of a mean in the subclass that is not the geometric means thereby solving the open problem of L. Moln\\'ar."],"url":"http://arxiv.org/abs/2405.20108v1","category":"math.FA"}
{"created":"2024-05-30 14:42:13","title":"Object-centric Reconstruction and Tracking of Dynamic Unknown Objects using 3D Gaussian Splatting","abstract":"Generalizable perception is one of the pillars of high-level autonomy in space robotics. Estimating the structure and motion of unknown objects in dynamic environments is fundamental for such autonomous systems. Traditionally, the solutions have relied on prior knowledge of target objects, multiple disparate representations, or low-fidelity outputs unsuitable for robotic operations. This work proposes a novel approach to incrementally reconstruct and track a dynamic unknown object using a unified representation -- a set of 3D Gaussian blobs that describe its geometry and appearance. The differentiable 3D Gaussian Splatting framework is adapted to a dynamic object-centric setting. The input to the pipeline is a sequential set of RGB-D images. 3D reconstruction and 6-DoF pose tracking tasks are tackled using first-order gradient-based optimization. The formulation is simple, requires no pre-training, assumes no prior knowledge of the object or its motion, and is suitable for online applications. The proposed approach is validated on a dataset of 10 unknown spacecraft of diverse geometry and texture under arbitrary relative motion. The experiments demonstrate successful 3D reconstruction and accurate 6-DoF tracking of the target object in proximity operations over a short to medium duration. The causes of tracking drift are discussed and potential solutions are outlined.","sentences":["Generalizable perception is one of the pillars of high-level autonomy in space robotics.","Estimating the structure and motion of unknown objects in dynamic environments is fundamental for such autonomous systems.","Traditionally, the solutions have relied on prior knowledge of target objects, multiple disparate representations, or low-fidelity outputs unsuitable for robotic operations.","This work proposes a novel approach to incrementally reconstruct and track a dynamic unknown object using a unified representation -- a set of 3D Gaussian blobs that describe its geometry and appearance.","The differentiable 3D Gaussian Splatting framework is adapted to a dynamic object-centric setting.","The input to the pipeline is a sequential set of RGB-D images.","3D reconstruction and 6-DoF pose tracking tasks are tackled using first-order gradient-based optimization.","The formulation is simple, requires no pre-training, assumes no prior knowledge of the object or its motion, and is suitable for online applications.","The proposed approach is validated on a dataset of 10 unknown spacecraft of diverse geometry and texture under arbitrary relative motion.","The experiments demonstrate successful 3D reconstruction and accurate 6-DoF tracking of the target object in proximity operations over a short to medium duration.","The causes of tracking drift are discussed and potential solutions are outlined."],"url":"http://arxiv.org/abs/2405.20104v1","category":"cs.RO"}
{"created":"2024-05-30 14:32:18","title":"Few-Photon SUPER: Quantum emitter inversion via two off-resonant photon modes","abstract":"With the realization of controlled quantum systems, exploring excitations beyond the resonant case opens new possibilities. We investigate an extended Jaynes-Cummings model where two photon modes are coupled off-resonantly to a quantum emitter. This allows us to identify few-photon scattering mechanisms that lead to a full inversion of the emitter while transferring off-resonant photons from one mode to another. This behaviour connects to recent measurements of a two-level emitter scattering two off-resonant photons simultaneously. Furthermore, our results can be understood as quantized analogue of the recently developed off-resonant quantum control scheme known as Swing-UP of quantum EmitteR (SUPER). Our intuitive formalism gives a deeper insight into the interaction of a two-level emitter with off-resonant light modes with the prospect of novel photonic applications.","sentences":["With the realization of controlled quantum systems, exploring excitations beyond the resonant case opens new possibilities.","We investigate an extended Jaynes-Cummings model where two photon modes are coupled off-resonantly to a quantum emitter.","This allows us to identify few-photon scattering mechanisms that lead to a full inversion of the emitter while transferring off-resonant photons from one mode to another.","This behaviour connects to recent measurements of a two-level emitter scattering two off-resonant photons simultaneously.","Furthermore, our results can be understood as quantized analogue of the recently developed off-resonant quantum control scheme known as Swing-UP of quantum EmitteR (SUPER).","Our intuitive formalism gives a deeper insight into the interaction of a two-level emitter with off-resonant light modes with the prospect of novel photonic applications."],"url":"http://arxiv.org/abs/2405.20095v1","category":"quant-ph"}
{"created":"2024-05-30 14:31:46","title":"Rapid Wildfire Hotspot Detection Using Self-Supervised Learning on Temporal Remote Sensing Data","abstract":"Rapid detection and well-timed intervention are essential to mitigate the impacts of wildfires. Leveraging remote sensed data from satellite networks and advanced AI models to automatically detect hotspots (i.e., thermal anomalies caused by active fires) is an effective way to build wildfire monitoring systems. In this work, we propose a novel dataset containing time series of remotely sensed data related to European fire events and a Self-Supervised Learning (SSL)-based model able to analyse multi-temporal data and identify hotspots in potentially near real time. We train and evaluate the performance of our model using our dataset and Thraws, a dataset of thermal anomalies including several fire events, obtaining an F1 score of 63.58.","sentences":["Rapid detection and well-timed intervention are essential to mitigate the impacts of wildfires.","Leveraging remote sensed data from satellite networks and advanced AI models to automatically detect hotspots (i.e., thermal anomalies caused by active fires) is an effective way to build wildfire monitoring systems.","In this work, we propose a novel dataset containing time series of remotely sensed data related to European fire events and a Self-Supervised Learning (SSL)-based model able to analyse multi-temporal data and identify hotspots in potentially near real time.","We train and evaluate the performance of our model using our dataset and Thraws, a dataset of thermal anomalies including several fire events, obtaining an F1 score of 63.58."],"url":"http://arxiv.org/abs/2405.20093v1","category":"cs.CV"}
{"created":"2024-05-30 14:16:19","title":"Soft Partitioning of Latent Space for Semantic Channel Equalization","abstract":"Semantic channel equalization has emerged as a solution to address language mismatch in multi-user semantic communications. This approach aims to align the latent spaces of an encoder and a decoder which were not jointly trained and it relies on a partition of the semantic (latent) space into atoms based on the the semantic meaning. In this work we explore the role of the semantic space partition in scenarios where the task structure involves a one-to-many mapping between the semantic space and the action space. In such scenarios, partitioning based on hard inference results results in loss of information which degrades the equalization performance. We propose a soft criterion to derive the atoms of the partition which leverages the soft decoder's output and offers a more comprehensive understanding of the semantic space's structure. Through empirical validation, we demonstrate that soft partitioning yields a more descriptive and regular partition of the space, consequently enhancing the performance of the equalization algorithm.","sentences":["Semantic channel equalization has emerged as a solution to address language mismatch in multi-user semantic communications.","This approach aims to align the latent spaces of an encoder and a decoder which were not jointly trained and it relies on a partition of the semantic (latent) space into atoms based on the the semantic meaning.","In this work we explore the role of the semantic space partition in scenarios where the task structure involves a one-to-many mapping between the semantic space and the action space.","In such scenarios, partitioning based on hard inference results results in loss of information which degrades the equalization performance.","We propose a soft criterion to derive the atoms of the partition which leverages the soft decoder's output and offers a more comprehensive understanding of the semantic space's structure.","Through empirical validation, we demonstrate that soft partitioning yields a more descriptive and regular partition of the space, consequently enhancing the performance of the equalization algorithm."],"url":"http://arxiv.org/abs/2405.20085v1","category":"cs.LG"}
{"created":"2024-05-30 14:07:18","title":"HIP 41378 observed by CHEOPS: Where is planet d?","abstract":"HIP 41378 d is a long-period planet that has only been observed to transit twice, three years apart, with K2. According to stability considerations and a partial detection of the Rossiter-McLaughlin effect, $P_\\mathrm{d} = 278.36$ d has been determined to be the most likely orbital period. We targeted HIP 41378 d with CHEOPS at the predicted transit timing based on $P_\\mathrm{d}= 278.36$ d, but the observations show no transit. We find that large ($>22.4$ hours) transit timing variations (TTVs) could explain this non-detection during the CHEOPS observation window. We also investigated the possibility of an incorrect orbital solution, which would have major implications for our knowledge of this system. If $P_\\mathrm{d} \\neq 278.36$ d, the periods that minimize the eccentricity would be $101.22$ d and $371.14$ d. The shortest orbital period will be tested by TESS, which will observe HIP 41378 in Sector 88 starting in January 2025. Our study shows the importance of a mission like CHEOPS, which today is the only mission able to make long observations (i.e., from space) to track the ephemeris of long-period planets possibly affected by large TTVs.","sentences":["HIP 41378 d is a long-period planet that has only been observed to transit twice, three years apart, with K2.","According to stability considerations and a partial detection of the Rossiter-McLaughlin effect, $P_\\mathrm{d} = 278.36$ d has been determined to be the most likely orbital period.","We targeted HIP 41378 d with CHEOPS at the predicted transit timing based on $P_\\mathrm{d}= 278.36$ d, but the observations show no transit.","We find that large ($>22.4$ hours) transit timing variations (TTVs) could explain this non-detection during the CHEOPS observation window.","We also investigated the possibility of an incorrect orbital solution, which would have major implications for our knowledge of this system.","If $P_\\mathrm{d} \\neq 278.36$ d, the periods that minimize the eccentricity would be $101.22$ d and $371.14$ d. The shortest orbital period will be tested by TESS, which will observe HIP 41378 in Sector 88 starting in January 2025.","Our study shows the importance of a mission like CHEOPS, which today is the only mission able to make long observations (i.e., from space) to track the ephemeris of long-period planets possibly affected by large TTVs."],"url":"http://arxiv.org/abs/2405.20077v1","category":"astro-ph.EP"}
{"created":"2024-05-30 14:06:58","title":"Detection of extragalactic magnetic massive stars","abstract":"Studies of the magnetic characteristics of massive stars have recently received significant attention because they are progenitors of highly magnetised compact objects. Stars initially more massive than about 8M_sun leave behind neutron stars and black holes by the end of their evolution. The merging of binary compact remnant systems produces astrophysical transients detectable by gravitational wave observatories. Studies of magnetic fields in massive stars with low metallicities are of particular interest because they provide important information on the role of magnetic fields in the star formation of the early Universe. While several detections of massive Galactic magnetic stars have been reported in the last few decades, the impact of a low-metallicity environment on the occurrence and strength of stellar magnetic fields has not yet been explored. Because of the similarity between Of?p stars in the Magellanic Clouds (MCs) and Galactic magnetic Of?p stars, which possess globally organised magnetic fields, we searched for magnetic fields in Of?p stars in the MCs. Additionally, we observed the massive contact binary Cl* NGC 346 SSN7 in the Small Magellanic Cloud to test the theoretical scenario that the origin of magnetic fields involves a merger event or a common envelope evolution. We obtained and analysed measurements of the magnetic field in four massive Of?p stars in the MCs and the binary Cl* NGC 346 SSN7 using the ESO/VLT FORS2 spectrograph in spectropolarimetric mode. We detected kilogauss-scale magnetic fields in two Of?p-type stars and in the contact binary Cl* NGC 346 SSN7. These results suggest that the impact of low metallicity on the occurrence and strength of magnetic fields in massive stars is low. However, because the explored stellar sample is very small, additional observations of massive stars in the MCs are necessary.","sentences":["Studies of the magnetic characteristics of massive stars have recently received significant attention because they are progenitors of highly magnetised compact objects.","Stars initially more massive than about 8M_sun leave behind neutron stars and black holes by the end of their evolution.","The merging of binary compact remnant systems produces astrophysical transients detectable by gravitational wave observatories.","Studies of magnetic fields in massive stars with low metallicities are of particular interest because they provide important information on the role of magnetic fields in the star formation of the early Universe.","While several detections of massive Galactic magnetic stars have been reported in the last few decades, the impact of a low-metallicity environment on the occurrence and strength of stellar magnetic fields has not yet been explored.","Because of the similarity between Of?p stars in the Magellanic Clouds (MCs) and Galactic magnetic Of?p stars, which possess globally organised magnetic fields, we searched for magnetic fields in Of?p stars in the MCs.","Additionally, we observed the massive contact binary Cl* NGC 346 SSN7 in the Small Magellanic Cloud to test the theoretical scenario that the origin of magnetic fields involves a merger event or a common envelope evolution.","We obtained and analysed measurements of the magnetic field in four massive Of?p stars in the MCs and the binary Cl* NGC 346 SSN7 using the ESO/VLT FORS2 spectrograph in spectropolarimetric mode.","We detected kilogauss-scale magnetic fields in two Of?p-type stars and in the contact binary Cl* NGC 346 SSN7.","These results suggest that the impact of low metallicity on the occurrence and strength of magnetic fields in massive stars is low.","However, because the explored stellar sample is very small, additional observations of massive stars in the MCs are necessary."],"url":"http://arxiv.org/abs/2405.20076v1","category":"astro-ph.SR"}
{"created":"2024-05-30 14:05:00","title":"Control in the Coefficients of an Obstacle Problem","abstract":"In this work, we consider optimality conditions of an optimal control problem governed by an obstacle problem. Here, we focus on introducing a, matrix valued, control variable as the coefficients of the obstacle problem. As it is well known, obstacle problems can be formulated as a complementarity system and consequently the associated solution operator is not Gateaux differentiable. As a consequence, we utilize a regularization approach to obtain optimality conditions as the limit of optimality conditions of a family of regularized problems.   Due to the coupling of the controlled coefficient with the gradients of the solution to the obstacle problem, weak convergence arguments can not be applied and we need to argue by $H$-convergence. We show, that, based on initial $H$-convergence, a bootstrapping argument can be utilized to prove strong $L^p$-convergence of the control and thus enable the passage to the limit in the optimality conditions.","sentences":["In this work, we consider optimality conditions of an optimal control problem governed by an obstacle problem.","Here, we focus on introducing a, matrix valued, control variable as the coefficients of the obstacle problem.","As it is well known, obstacle problems can be formulated as a complementarity system and consequently the associated solution operator is not Gateaux differentiable.","As a consequence, we utilize a regularization approach to obtain optimality conditions as the limit of optimality conditions of a family of regularized problems.   ","Due to the coupling of the controlled coefficient with the gradients of the solution to the obstacle problem, weak convergence arguments can not be applied and we need to argue by $H$-convergence.","We show, that, based on initial $H$-convergence, a bootstrapping argument can be utilized to prove strong $L^p$-convergence of the control and thus enable the passage to the limit in the optimality conditions."],"url":"http://arxiv.org/abs/2405.20074v1","category":"math.OC"}
{"created":"2024-05-30 14:03:07","title":"Power Allocation for Cell-Free Massive MIMO ISAC Systems with OTFS Signal","abstract":"Applying integrated sensing and communication (ISAC) to a cell-free massive multiple-input multiple-output (CF mMIMO) architecture has attracted increasing attention. This approach equips CF mMIMO networks with sensing capabilities and resolves the problem of unreliable service at cell edges in conventional cellular networks. However, existing studies on CF-ISAC systems have focused on the application of traditional integrated signals. To address this limitation, this study explores the employment of the orthogonal time frequency space (OTFS) signal as a representative of innovative signals in the CF-ISAC system, and the system's overall performance is optimized and evaluated. A universal downlink spectral efficiency (SE) expression is derived regarding multi-antenna access points (APs) and optional sensing beams. To streamline the analysis and optimization of the CF-ISAC system with the OTFS signal, we introduce a lower bound on the achievable SE that is applicable to OTFS-signal-based systems. Based on this, a power allocation algorithm is proposed to maximize the minimum communication signal-to-interference-plus-noise ratio (SINR) of users while guaranteeing a specified sensing SINR value and meeting the per-AP power constraints. The results demonstrate the tightness of the proposed lower bound and the efficiency of the proposed algorithm. Finally, the superiority of using the OTFS signals is verified by a 13-fold expansion of the SE performance gap over the application of orthogonal frequency division multiplexing signals. These findings could guide the future deployment of the CF-ISAC systems, particularly in the field of millimeter waves with a large bandwidth.","sentences":["Applying integrated sensing and communication (ISAC) to a cell-free massive multiple-input multiple-output (CF mMIMO) architecture has attracted increasing attention.","This approach equips CF mMIMO networks with sensing capabilities and resolves the problem of unreliable service at cell edges in conventional cellular networks.","However, existing studies on CF-ISAC systems have focused on the application of traditional integrated signals.","To address this limitation, this study explores the employment of the orthogonal time frequency space (OTFS) signal as a representative of innovative signals in the CF-ISAC system, and the system's overall performance is optimized and evaluated.","A universal downlink spectral efficiency (SE) expression is derived regarding multi-antenna access points (APs) and optional sensing beams.","To streamline the analysis and optimization of the CF-ISAC system with the OTFS signal, we introduce a lower bound on the achievable SE that is applicable to OTFS-signal-based systems.","Based on this, a power allocation algorithm is proposed to maximize the minimum communication signal-to-interference-plus-noise ratio (SINR) of users while guaranteeing a specified sensing SINR value and meeting the per-AP power constraints.","The results demonstrate the tightness of the proposed lower bound and the efficiency of the proposed algorithm.","Finally, the superiority of using the OTFS signals is verified by a 13-fold expansion of the SE performance gap over the application of orthogonal frequency division multiplexing signals.","These findings could guide the future deployment of the CF-ISAC systems, particularly in the field of millimeter waves with a large bandwidth."],"url":"http://arxiv.org/abs/2405.20073v1","category":"cs.IT"}
{"created":"2024-05-30 14:01:02","title":"A Staged Approach using Machine Learning and Uncertainty Quantification to Predict the Risk of Hip Fracture","abstract":"Despite advancements in medical care, hip fractures impose a significant burden on individuals and healthcare systems. This paper focuses on the prediction of hip fracture risk in older and middle-aged adults, where falls and compromised bone quality are predominant factors. We propose a novel staged model that combines advanced imaging and clinical data to improve predictive performance. By using CNNs to extract features from hip DXA images, along with clinical variables, shape measurements, and texture features, our method provides a comprehensive framework for assessing fracture risk. A staged machine learning-based model was developed using two ensemble models: Ensemble 1 (clinical variables only) and Ensemble 2 (clinical variables and DXA imaging features). This staged approach used uncertainty quantification from Ensemble 1 to decide if DXA features are necessary for further prediction. Ensemble 2 exhibited the highest performance, achieving an AUC of 0.9541, an accuracy of 0.9195, a sensitivity of 0.8078, and a specificity of 0.9427. The staged model also performed well, with an AUC of 0.8486, an accuracy of 0.8611, a sensitivity of 0.5578, and a specificity of 0.9249, outperforming Ensemble 1, which had an AUC of 0.5549, an accuracy of 0.7239, a sensitivity of 0.1956, and a specificity of 0.8343. Furthermore, the staged model suggested that 54.49% of patients did not require DXA scanning. It effectively balanced accuracy and specificity, offering a robust solution when DXA data acquisition is not always feasible. Statistical tests confirmed significant differences between the models, highlighting the advantages of the advanced modeling strategies. Our staged approach could identify individuals at risk with a high accuracy but reduce the unnecessary DXA scanning. It has great promise to guide interventions to prevent hip fractures with reduced cost and radiation.","sentences":["Despite advancements in medical care, hip fractures impose a significant burden on individuals and healthcare systems.","This paper focuses on the prediction of hip fracture risk in older and middle-aged adults, where falls and compromised bone quality are predominant factors.","We propose a novel staged model that combines advanced imaging and clinical data to improve predictive performance.","By using CNNs to extract features from hip DXA images, along with clinical variables, shape measurements, and texture features, our method provides a comprehensive framework for assessing fracture risk.","A staged machine learning-based model was developed using two ensemble models: Ensemble 1 (clinical variables only) and Ensemble 2 (clinical variables and DXA imaging features).","This staged approach used uncertainty quantification from Ensemble 1 to decide if DXA features are necessary for further prediction.","Ensemble 2 exhibited the highest performance, achieving an AUC of 0.9541, an accuracy of 0.9195, a sensitivity of 0.8078, and a specificity of 0.9427.","The staged model also performed well, with an AUC of 0.8486, an accuracy of 0.8611, a sensitivity of 0.5578, and a specificity of 0.9249, outperforming Ensemble 1, which had an AUC of 0.5549, an accuracy of 0.7239, a sensitivity of 0.1956, and a specificity of 0.8343.","Furthermore, the staged model suggested that 54.49% of patients did not require DXA scanning.","It effectively balanced accuracy and specificity, offering a robust solution when DXA data acquisition is not always feasible.","Statistical tests confirmed significant differences between the models, highlighting the advantages of the advanced modeling strategies.","Our staged approach could identify individuals at risk with a high accuracy but reduce the unnecessary DXA scanning.","It has great promise to guide interventions to prevent hip fractures with reduced cost and radiation."],"url":"http://arxiv.org/abs/2405.20071v1","category":"physics.med-ph"}
{"created":"2024-05-30 13:57:14","title":"An Efficient Network with Novel Quantization Designed for Massive MIMO CSI Feedback","abstract":"The efficacy of massive multiple-input multiple-output (MIMO) techniques heavily relies on the accuracy of channel state information (CSI) in frequency division duplexing (FDD) systems. Many works focus on CSI compression and quantization methods to enhance CSI reconstruction accuracy with lower feedback overhead. In this letter, we propose CsiConformer, a novel CSI feedback network that combines convolutional operations and self-attention mechanisms to improve CSI feedback accuracy. Additionally, a new quantization module is developed to improve encoding efficiency. Experiment results show that CsiConformer outperforms previous state-of-the-art networks, achieving an average accuracy improvement of 17.67\\% with lower computational overhead.","sentences":["The efficacy of massive multiple-input multiple-output (MIMO) techniques heavily relies on the accuracy of channel state information (CSI) in frequency division duplexing (FDD) systems.","Many works focus on CSI compression and quantization methods to enhance CSI reconstruction accuracy with lower feedback overhead.","In this letter, we propose CsiConformer, a novel CSI feedback network that combines convolutional operations and self-attention mechanisms to improve CSI feedback accuracy.","Additionally, a new quantization module is developed to improve encoding efficiency.","Experiment results show that CsiConformer outperforms previous state-of-the-art networks, achieving an average accuracy improvement of 17.67\\% with lower computational overhead."],"url":"http://arxiv.org/abs/2405.20068v1","category":"eess.SP"}
{"created":"2024-05-30 13:56:58","title":"N-Dimensional Gaussians for Fitting of High Dimensional Functions","abstract":"In the wake of many new ML-inspired approaches for reconstructing and representing high-quality 3D content, recent hybrid and explicitly learned representations exhibit promising performance and quality characteristics. However, their scaling to higher dimensions is challenging, e.g. when accounting for dynamic content with respect to additional parameters such as material properties, illumination, or time. In this paper, we tackle these challenges for an explicit representations based on Gaussian mixture models. With our solutions, we arrive at efficient fitting of compact N-dimensional Gaussian mixtures and enable efficient evaluation at render time: For fast fitting and evaluation, we introduce a high-dimensional culling scheme that efficiently bounds N-D Gaussians, inspired by Locality Sensitive Hashing. For adaptive refinement yet compact representation, we introduce a loss-adaptive density control scheme that incrementally guides the use of additional capacity towards missing details. With these tools we can for the first time represent complex appearance that depends on many input dimensions beyond position or viewing angle within a compact, explicit representation optimized in minutes and rendered in milliseconds.","sentences":["In the wake of many new ML-inspired approaches for reconstructing and representing high-quality 3D content, recent hybrid and explicitly learned representations exhibit promising performance and quality characteristics.","However, their scaling to higher dimensions is challenging, e.g. when accounting for dynamic content with respect to additional parameters such as material properties, illumination, or time.","In this paper, we tackle these challenges for an explicit representations based on Gaussian mixture models.","With our solutions, we arrive at efficient fitting of compact N-dimensional Gaussian mixtures and enable efficient evaluation at render time:","For fast fitting and evaluation, we introduce a high-dimensional culling scheme that efficiently bounds N-D Gaussians, inspired by Locality Sensitive Hashing.","For adaptive refinement yet compact representation, we introduce a loss-adaptive density control scheme that incrementally guides the use of additional capacity towards missing details.","With these tools we can for the first time represent complex appearance that depends on many input dimensions beyond position or viewing angle within a compact, explicit representation optimized in minutes and rendered in milliseconds."],"url":"http://arxiv.org/abs/2405.20067v1","category":"cs.CV"}
{"created":"2024-05-30 13:56:38","title":"Variationally Correct Neural Residual Regression for Parametric PDEs: On the Viability of Controlled Accuracy","abstract":"This paper is about learning the parameter-to-solution map for systems of partial differential equations (PDEs) that depend on a potentially large number of parameters covering all PDE types for which a stable variational formulation (SVF) can be found. A central constituent is the notion of variationally correct residual loss function meaning that its value is always uniformly proportional to the squared solution error in the norm determined by the SVF, hence facilitating rigorous a posteriori accuracy control. It is based on a single variational problem, associated with the family of parameter dependent fiber problems, employing the notion of direct integrals of Hilbert spaces. Since in its original form the loss function is given as a dual test norm of the residual a central objective is to develop equivalent computable expressions. A first critical role is played by hybrid hypothesis classes, whose elements are piecewise polynomial in (low-dimensional) spatio-temporal variables with parameter-dependent coefficients that can be represented, e.g. by neural networks. Second, working with first order SVFs, we distinguish two scenarios: (i) the test space can be chosen as an $L_2$-space (e.g. for elliptic or parabolic problems) so that residuals live in $L_2$ and can be evaluated directly; (ii) when trial and test spaces for the fiber problems (e.g. for transport equations) depend on the parameters, we use ultraweak formulations. In combination with Discontinuous Petrov Galerkin concepts the hybrid format is then instrumental to arrive at variationally correct computable residual loss functions. Our findings are illustrated by numerical experiments representing (i) and (ii), namely elliptic boundary value problems with piecewise constant diffusion coefficients and pure transport equations with parameter dependent convection field.","sentences":["This paper is about learning the parameter-to-solution map for systems of partial differential equations (PDEs) that depend on a potentially large number of parameters covering all PDE types for which a stable variational formulation (SVF) can be found.","A central constituent is the notion of variationally correct residual loss function meaning that its value is always uniformly proportional to the squared solution error in the norm determined by the SVF, hence facilitating rigorous a posteriori accuracy control.","It is based on a single variational problem, associated with the family of parameter dependent fiber problems, employing the notion of direct integrals of Hilbert spaces.","Since in its original form the loss function is given as a dual test norm of the residual a central objective is to develop equivalent computable expressions.","A first critical role is played by hybrid hypothesis classes, whose elements are piecewise polynomial in (low-dimensional) spatio-temporal variables with parameter-dependent coefficients that can be represented, e.g. by neural networks.","Second, working with first order SVFs, we distinguish two scenarios: (i) the test space can be chosen as an $L_2$-space (e.g. for elliptic or parabolic problems) so that residuals live in $L_2$ and can be evaluated directly; (ii) when trial and test spaces for the fiber problems (e.g. for transport equations) depend on the parameters, we use ultraweak formulations.","In combination with Discontinuous Petrov Galerkin concepts the hybrid format is then instrumental to arrive at variationally correct computable residual loss functions.","Our findings are illustrated by numerical experiments representing (i) and (ii), namely elliptic boundary value problems with piecewise constant diffusion coefficients and pure transport equations with parameter dependent convection field."],"url":"http://arxiv.org/abs/2405.20065v1","category":"math.NA"}
{"created":"2024-05-30 13:55:43","title":"1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem","abstract":"Speech emotion recognition is a challenging classification task with natural emotional speech, especially when the distribution of emotion types is imbalanced in the training and test data. In this case, it is more difficult for a model to learn to separate minority classes, resulting in those sometimes being ignored or frequently misclassified. Previous work has utilised class weighted loss for training, but problems remain as it sometimes causes over-fitting for minor classes or under-fitting for major classes. This paper presents the system developed by a multi-site team for the participation in the Odyssey 2024 Emotion Recognition Challenge Track-1. The challenge data has the aforementioned properties and therefore the presented systems aimed to tackle these issues, by introducing focal loss in optimisation when applying class weighted loss. Specifically, the focal loss is further weighted by prior-based class weights. Experimental results show that combining these two approaches brings better overall performance, by sacrificing performance on major classes. The system further employs a majority voting strategy to combine the outputs of an ensemble of 7 models. The models are trained independently, using different acoustic features and loss functions - with the aim to have different properties for different data. Hence these models show different performance preferences on major classes and minor classes. The ensemble system output obtained the best performance in the challenge, ranking top-1 among 68 submissions. It also outperformed all single models in our set. On the Odyssey 2024 Emotion Recognition Challenge Task-1 data the system obtained a Macro-F1 score of 35.69% and an accuracy of 37.32%.","sentences":["Speech emotion recognition is a challenging classification task with natural emotional speech, especially when the distribution of emotion types is imbalanced in the training and test data.","In this case, it is more difficult for a model to learn to separate minority classes, resulting in those sometimes being ignored or frequently misclassified.","Previous work has utilised class weighted loss for training, but problems remain as it sometimes causes over-fitting for minor classes or under-fitting for major classes.","This paper presents the system developed by a multi-site team for the participation in the Odyssey 2024 Emotion Recognition Challenge Track-1.","The challenge data has the aforementioned properties and therefore the presented systems aimed to tackle these issues, by introducing focal loss in optimisation when applying class weighted loss.","Specifically, the focal loss is further weighted by prior-based class weights.","Experimental results show that combining these two approaches brings better overall performance, by sacrificing performance on major classes.","The system further employs a majority voting strategy to combine the outputs of an ensemble of 7 models.","The models are trained independently, using different acoustic features and loss functions - with the aim to have different properties for different data.","Hence these models show different performance preferences on major classes and minor classes.","The ensemble system output obtained the best performance in the challenge, ranking top-1 among 68 submissions.","It also outperformed all single models in our set.","On the Odyssey 2024 Emotion Recognition Challenge Task-1 data the system obtained a Macro-F1 score of 35.69% and an accuracy of 37.32%."],"url":"http://arxiv.org/abs/2405.20064v1","category":"eess.AS"}
{"created":"2024-05-30 13:51:54","title":"Feasibility of meteor surveying from a Venus orbiter","abstract":"Meteor and bolide phenomena caused by the atmospheric ablation of incoming meteoroids are predicted to occur at the planet Venus. Their systematic observation would allow to measure and compare the sub-mm to m meteoroid flux at different locations in the solar system. Using a physical model of atmospheric ablation, we demonstrate that Venus meteors would be brighter, shorter-lived, and appear higher in the atmosphere than Earth meteors. To investigate the feasibility of meteor detection at Venus from an orbiter, we apply the SWARMS survey simulator tool to sets of plausible meteoroid population parameters, atmospheric models and instrument designs suited to the task, such as the Mini-EUSO camera operational on the ISS since 2019. We find that such instrumentation would detect meteors at Venus with a 1.5x to 2.5x higher rate than at Earth. The estimated Venus-Earth detection ratio remains insensitive to variations in the chosen observation orbit and detector characteristics, implying that a meteor survey from Venus orbit is feasible, though contingent on the availability of suitable algorithms and methods for efficient on-board processing and downlinking of the meteor data to Earth. We further show that a hypothetical camera onboard the upcoming EnVision mission to Venus similar to the ISS instrument should detect many times more meteors than needed for an initial characterisation of the large meteoroid population at 0.7 au from the Sun.","sentences":["Meteor and bolide phenomena caused by the atmospheric ablation of incoming meteoroids are predicted to occur at the planet Venus.","Their systematic observation would allow to measure and compare the sub-mm to m meteoroid flux at different locations in the solar system.","Using a physical model of atmospheric ablation, we demonstrate that Venus meteors would be brighter, shorter-lived, and appear higher in the atmosphere than Earth meteors.","To investigate the feasibility of meteor detection at Venus from an orbiter, we apply the SWARMS survey simulator tool to sets of plausible meteoroid population parameters, atmospheric models and instrument designs suited to the task, such as the Mini-EUSO camera operational on the ISS since 2019.","We find that such instrumentation would detect meteors at Venus with a 1.5x to 2.5x higher rate than at Earth.","The estimated Venus-Earth detection ratio remains insensitive to variations in the chosen observation orbit and detector characteristics, implying that a meteor survey from Venus orbit is feasible, though contingent on the availability of suitable algorithms and methods for efficient on-board processing and downlinking of the meteor data to Earth.","We further show that a hypothetical camera onboard the upcoming EnVision mission to Venus similar to the ISS instrument should detect many times more meteors than needed for an initial characterisation of the large meteoroid population at 0.7 au from the Sun."],"url":"http://arxiv.org/abs/2405.20063v1","category":"astro-ph.EP"}
{"created":"2024-05-30 13:46:56","title":"Enhancing Plant Disease Detection: A Novel CNN-Based Approach with Tensor Subspace Learning and HOWSVD-MD","abstract":"Machine learning has revolutionized the field of agricultural science, particularly in the early detection and management of plant diseases, which are crucial for maintaining crop health and productivity. Leveraging advanced algorithms and imaging technologies, researchers are now able to identify and classify plant diseases with unprecedented accuracy and speed. Effective management of tomato diseases is crucial for enhancing agricultural productivity. The development and application of tomato disease classification methods are central to this objective. This paper introduces a cutting-edge technique for the detection and classification of tomato leaf diseases, utilizing insights from the latest pre-trained Convolutional Neural Network (CNN) models. We propose a sophisticated approach within the domain of tensor subspace learning, known as Higher-Order Whitened Singular Value Decomposition (HOWSVD), designed to boost the discriminatory power of the system. Our approach to Tensor Subspace Learning is methodically executed in two phases, beginning with HOWSVD and culminating in Multilinear Discriminant Analysis (MDA). The efficacy of this innovative method was rigorously tested through comprehensive experiments on two distinct datasets, namely PlantVillage and the Taiwan dataset. The findings reveal that HOWSVD-MDA outperforms existing methods, underscoring its capability to markedly enhance the precision and dependability of diagnosing tomato leaf diseases. For instance, up to 98.36\\% and 89.39\\% accuracy scores have been achieved under PlantVillage and the Taiwan datasets, respectively.","sentences":["Machine learning has revolutionized the field of agricultural science, particularly in the early detection and management of plant diseases, which are crucial for maintaining crop health and productivity.","Leveraging advanced algorithms and imaging technologies, researchers are now able to identify and classify plant diseases with unprecedented accuracy and speed.","Effective management of tomato diseases is crucial for enhancing agricultural productivity.","The development and application of tomato disease classification methods are central to this objective.","This paper introduces a cutting-edge technique for the detection and classification of tomato leaf diseases, utilizing insights from the latest pre-trained Convolutional Neural Network (CNN) models.","We propose a sophisticated approach within the domain of tensor subspace learning, known as Higher-Order Whitened Singular Value Decomposition (HOWSVD), designed to boost the discriminatory power of the system.","Our approach to Tensor Subspace Learning is methodically executed in two phases, beginning with HOWSVD and culminating in Multilinear Discriminant Analysis (MDA).","The efficacy of this innovative method was rigorously tested through comprehensive experiments on two distinct datasets, namely PlantVillage and the Taiwan dataset.","The findings reveal that HOWSVD-MDA outperforms existing methods, underscoring its capability to markedly enhance the precision and dependability of diagnosing tomato leaf diseases.","For instance, up to 98.36\\% and 89.39\\% accuracy scores have been achieved under PlantVillage and the Taiwan datasets, respectively."],"url":"http://arxiv.org/abs/2405.20058v1","category":"cs.CV"}
{"created":"2024-05-30 13:43:15","title":"Hypergraph-Aided Task-Resource Matching for Maximizing Value of Task Completion in Collaborative IoT Systems","abstract":"With the growing scale and intrinsic heterogeneity of Internet of Things (IoT) systems, distributed device collaboration becomes essential for effective task completion by dynamically utilizing limited communication and computing resources. However, the separated design and situation-agnostic operation of computing, communication and application layers create a fundamental challenge for rapid task-resource matching, which further deteriorate the overall task completion effectiveness. To overcome this challenge, we utilize hypergraph as a new tool to vertically unify computing, communication, and task aspects of IoT systems for an effective matching by accurately capturing the relationships between tasks and communication and computing resources. Specifically, a state-of-the-art task-resource matching hypergraph (TRM-hypergraph) model is proposed in this paper, which is used to effectively transform the process of allocating complex heterogeneous resources to convoluted tasks into a hypergraph matching problem. Taking into account computational complexity and storage, a game-theoretic hypergraph matching algorithm is proposed via considering the hypergraph matching problem as a non-cooperative multi-player clustering game. Numerical results demonstrate that the proposed TRM-hypergraph model achieves superior performance in matching of tasks and resources compared with comparison algorithms.","sentences":["With the growing scale and intrinsic heterogeneity of Internet of Things (IoT) systems, distributed device collaboration becomes essential for effective task completion by dynamically utilizing limited communication and computing resources.","However, the separated design and situation-agnostic operation of computing, communication and application layers create a fundamental challenge for rapid task-resource matching, which further deteriorate the overall task completion effectiveness.","To overcome this challenge, we utilize hypergraph as a new tool to vertically unify computing, communication, and task aspects of IoT systems for an effective matching by accurately capturing the relationships between tasks and communication and computing resources.","Specifically, a state-of-the-art task-resource matching hypergraph (TRM-hypergraph) model is proposed in this paper, which is used to effectively transform the process of allocating complex heterogeneous resources to convoluted tasks into a hypergraph matching problem.","Taking into account computational complexity and storage, a game-theoretic hypergraph matching algorithm is proposed via considering the hypergraph matching problem as a non-cooperative multi-player clustering game.","Numerical results demonstrate that the proposed TRM-hypergraph model achieves superior performance in matching of tasks and resources compared with comparison algorithms."],"url":"http://arxiv.org/abs/2405.20055v1","category":"eess.SY"}
{"created":"2024-05-30 13:33:13","title":"Towards a characterization of the inverse systems of complete intersections","abstract":"In this paper we give conditions on a homogeneous polynomial for which the associated graded Artin algebra is a complete intersection.","sentences":["In this paper we give conditions on a homogeneous polynomial for which the associated graded Artin algebra is a complete intersection."],"url":"http://arxiv.org/abs/2405.20049v1","category":"math.AC"}
{"created":"2024-05-30 13:27:17","title":"Iterative Learning Control of Fast, Nonlinear, Oscillatory Dynamics (Preprint)","abstract":"The sudden onset of deleterious and oscillatory dynamics (often called instabilities) is a known challenge in many fluid, plasma, and aerospace systems. These dynamics are difficult to address because they are nonlinear, chaotic, and are often too fast for active control schemes. In this work, we develop an alternative active controls system using an iterative, trajectory-optimization and parameter-tuning approach based on Iterative Learning Control (ILC), Time-Lagged Phase Portraits (TLPP) and Gaussian Process Regression (GPR). The novelty of this approach is that it can control a system's dynamics despite the controller being much slower than the dynamics. We demonstrate this controller on the Lorenz system of equations where it iteratively adjusts (tunes) the system's input parameters to successfully reproduce a desired oscillatory trajectory or state. Additionally, we investigate the system's dynamical sensitivity to its control parameters, identify continuous and bounded regions of desired dynamical trajectories, and demonstrate that the controller is robust to missing information and uncontrollable parameters as long as certain requirements are met. The controller presented in this work provides a framework for low-speed control for a variety of fast, nonlinear systems that may aid in instability suppression and mitigation.","sentences":["The sudden onset of deleterious and oscillatory dynamics (often called instabilities) is a known challenge in many fluid, plasma, and aerospace systems.","These dynamics are difficult to address because they are nonlinear, chaotic, and are often too fast for active control schemes.","In this work, we develop an alternative active controls system using an iterative, trajectory-optimization and parameter-tuning approach based on Iterative Learning Control (ILC), Time-Lagged Phase Portraits (TLPP) and Gaussian Process Regression (GPR).","The novelty of this approach is that it can control a system's dynamics despite the controller being much slower than the dynamics.","We demonstrate this controller on the Lorenz system of equations where it iteratively adjusts (tunes) the system's input parameters to successfully reproduce a desired oscillatory trajectory or state.","Additionally, we investigate the system's dynamical sensitivity to its control parameters, identify continuous and bounded regions of desired dynamical trajectories, and demonstrate that the controller is robust to missing information and uncontrollable parameters as long as certain requirements are met.","The controller presented in this work provides a framework for low-speed control for a variety of fast, nonlinear systems that may aid in instability suppression and mitigation."],"url":"http://arxiv.org/abs/2405.20045v1","category":"cs.LG"}
{"created":"2024-05-30 13:25:25","title":"A Point-Neighborhood Learning Framework for Nasal Endoscope Image Segmentation","abstract":"The lesion segmentation on endoscopic images is challenging due to its complex and ambiguous features. Fully-supervised deep learning segmentation methods can receive good performance based on entirely pixel-level labeled dataset but greatly increase experts' labeling burden. Semi-supervised and weakly supervised methods can ease labeling burden, but heavily strengthen the learning difficulty. To alleviate this difficulty, weakly semi-supervised segmentation adopts a new annotation protocol of adding a large number of point annotation samples into a few pixel-level annotation samples. However, existing methods only mine points' limited information while ignoring reliable prior surrounding the point annotations. In this paper, we propose a weakly semi-supervised method called Point-Neighborhood Learning (PNL) framework. To mine the prior of the pixels surrounding the annotated point, we transform a single-point annotation into a circular area named a point-neighborhood. We propose point-neighborhood supervision loss and pseudo-label scoring mechanism to enhance training supervision. Point-neighborhoods are also used to augment the data diversity. Our method greatly improves performance without changing the structure of segmentation network. Comprehensive experiments show the superiority of our method over the other existing methods, demonstrating its effectiveness in point-annotated medical images. The project code will be available on: https://github.com/ParryJay/PNL.","sentences":["The lesion segmentation on endoscopic images is challenging due to its complex and ambiguous features.","Fully-supervised deep learning segmentation methods can receive good performance based on entirely pixel-level labeled dataset but greatly increase experts' labeling burden.","Semi-supervised and weakly supervised methods can ease labeling burden, but heavily strengthen the learning difficulty.","To alleviate this difficulty, weakly semi-supervised segmentation adopts a new annotation protocol of adding a large number of point annotation samples into a few pixel-level annotation samples.","However, existing methods only mine points' limited information while ignoring reliable prior surrounding the point annotations.","In this paper, we propose a weakly semi-supervised method called Point-Neighborhood Learning (PNL) framework.","To mine the prior of the pixels surrounding the annotated point, we transform a single-point annotation into a circular area named a point-neighborhood.","We propose point-neighborhood supervision loss and pseudo-label scoring mechanism to enhance training supervision.","Point-neighborhoods are also used to augment the data diversity.","Our method greatly improves performance without changing the structure of segmentation network.","Comprehensive experiments show the superiority of our method over the other existing methods, demonstrating its effectiveness in point-annotated medical images.","The project code will be available on: https://github.com/ParryJay/PNL."],"url":"http://arxiv.org/abs/2405.20044v1","category":"cs.CV"}
{"created":"2024-05-30 13:19:49","title":"Task-Agnostic Machine Learning-Assisted Inference","abstract":"Machine learning (ML) is playing an increasingly important role in scientific research. In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings. This has also opened up a whole new field of methodological research focusing on integrative approaches that leverage both ML and statistics to tackle data science challenges. One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples and then uses the predicted outcomes in downstream statistical inference. However, existing methods designed to ensure the validity of this type of post-prediction inference are limited to very basic tasks such as linear regression analysis. This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for complex inference tasks and severely constrains the scope of post-prediction inference in real applications. To address this challenge, we propose a novel statistical framework for task-agnostic ML-assisted inference. It provides a post-prediction inference solution that can be easily plugged into almost any established data analysis routine. It delivers valid and efficient inference that is robust to arbitrary choices of ML models, while allowing nearly all existing analytical frameworks to be incorporated into the analysis of ML-predicted outcomes. Through extensive experiments, we showcase the validity, versatility, and superiority of our method compared to existing approaches.","sentences":["Machine learning (ML) is playing an increasingly important role in scientific research.","In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings.","This has also opened up a whole new field of methodological research focusing on integrative approaches that leverage both ML and statistics to tackle data science challenges.","One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples and then uses the predicted outcomes in downstream statistical inference.","However, existing methods designed to ensure the validity of this type of post-prediction inference are limited to very basic tasks such as linear regression analysis.","This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for complex inference tasks and severely constrains the scope of post-prediction inference in real applications.","To address this challenge, we propose a novel statistical framework for task-agnostic ML-assisted inference.","It provides a post-prediction inference solution that can be easily plugged into almost any established data analysis routine.","It delivers valid and efficient inference that is robust to arbitrary choices of ML models, while allowing nearly all existing analytical frameworks to be incorporated into the analysis of ML-predicted outcomes.","Through extensive experiments, we showcase the validity, versatility, and superiority of our method compared to existing approaches."],"url":"http://arxiv.org/abs/2405.20039v1","category":"stat.ML"}
{"created":"2024-05-30 13:19:04","title":"Chirp asymmetry in Zeeman electromagnetically induced transparency","abstract":"The simplest three-level system exhibiting electromagnetically induced transparency (EIT) exhibits an effective conjugation symmetry as well as a permutation symmetry. Breaking conjugation symmetry leads to a distinct chirp asymmetry; the differential response to a frequency increase versus a frequency decrease. Hanle-Zeeman EIT resonance is an ideal platform for testing the theory of chirp asymmetry because so many optical parameters of the system can be changed experimentally. We describe the theory and compare it to an experiment using 87Rb in a buffer gas cell. In contrast with earlier multi-photon chirp asymmetry work this present effort explores the asymmetry at nearly one billionth the earlier chirp rate, yet displays its universal features. Chirp asymmetry may have metrological consequences for understanding systematic dependence on modulation/demodulation parameters.","sentences":["The simplest three-level system exhibiting electromagnetically induced transparency (EIT) exhibits an effective conjugation symmetry as well as a permutation symmetry.","Breaking conjugation symmetry leads to a distinct chirp asymmetry; the differential response to a frequency increase versus a frequency decrease.","Hanle-Zeeman EIT resonance is an ideal platform for testing the theory of chirp asymmetry because so many optical parameters of the system can be changed experimentally.","We describe the theory and compare it to an experiment using 87Rb in a buffer gas cell.","In contrast with earlier multi-photon chirp asymmetry work this present effort explores the asymmetry at nearly one billionth the earlier chirp rate, yet displays its universal features.","Chirp asymmetry may have metrological consequences for understanding systematic dependence on modulation/demodulation parameters."],"url":"http://arxiv.org/abs/2405.20036v1","category":"physics.atom-ph"}
{"created":"2024-05-30 13:16:17","title":"Structure Gaussian SLAM with Manhattan World Hypothesis","abstract":"Gaussian SLAM systems have made significant advancements in improving the efficiency and fidelity of real-time reconstructions. However, these systems often encounter incomplete reconstructions in complex indoor environments, characterized by substantial holes due to unobserved geometry caused by obstacles or limited view angles. To address this challenge, we present Manhattan Gaussian SLAM (MG-SLAM), an RGB-D system that leverages the Manhattan World hypothesis to enhance geometric accuracy and completeness. By seamlessly integrating fused line segments derived from structured scenes, MG-SLAM ensures robust tracking in textureless indoor areas. Moreover, The extracted lines and planar surface assumption allow strategic interpolation of new Gaussians in regions of missing geometry, enabling efficient scene completion. Extensive experiments conducted on both synthetic and real-world scenes demonstrate that these advancements enable our method to achieve state-of-the-art performance, marking a substantial improvement in the capabilities of Gaussian SLAM systems.","sentences":["Gaussian SLAM systems have made significant advancements in improving the efficiency and fidelity of real-time reconstructions.","However, these systems often encounter incomplete reconstructions in complex indoor environments, characterized by substantial holes due to unobserved geometry caused by obstacles or limited view angles.","To address this challenge, we present Manhattan Gaussian SLAM (MG-SLAM), an RGB-D system that leverages the Manhattan World hypothesis to enhance geometric accuracy and completeness.","By seamlessly integrating fused line segments derived from structured scenes, MG-SLAM ensures robust tracking in textureless indoor areas.","Moreover, The extracted lines and planar surface assumption allow strategic interpolation of new Gaussians in regions of missing geometry, enabling efficient scene completion.","Extensive experiments conducted on both synthetic and real-world scenes demonstrate that these advancements enable our method to achieve state-of-the-art performance, marking a substantial improvement in the capabilities of Gaussian SLAM systems."],"url":"http://arxiv.org/abs/2405.20031v1","category":"cs.RO"}
{"created":"2024-05-30 13:11:43","title":"The CFG Complexity of Singleton Sets","abstract":"Let G be a context-free grammar (CFG) in Chomsky normal form. We take the number of rules in G to be the size of G. We also assume all CFGs are in Chomsky normal form.   We consider the question of, given a string w of length n, what is the smallest CFG such that L(G)={w}? We show the following:   1) For all w, |w|=n, there is a CFG of size with O(n/log n) rules, such that L(G)={w}.   2) There exists a string w, |w|=n, such that every CFG G with L(G)={w} is of size Omega(n/log n). We give two proofs of: one nonconstructive, the other constructive.","sentences":["Let G be a context-free grammar (CFG) in Chomsky normal form.","We take the number of rules in G to be the size of G. We also assume all CFGs are in Chomsky normal form.   ","We consider the question of, given a string w of length n, what is the smallest CFG such that L(G)={w}?","We show the following:   1) For all w, |w|=n, there is a CFG of size with O(n/log n) rules, such that L(G)={w}.   ","2) There exists a string w, |w|=n, such that every CFG G with L(G)={w} is of size Omega(n/log n).","We give two proofs of: one nonconstructive, the other constructive."],"url":"http://arxiv.org/abs/2405.20026v1","category":"cs.FL"}
{"created":"2024-05-30 13:05:09","title":"Thermodynamic topology of topological black hole in $F(R)$-ModMax gravity's rainbow","abstract":"In order to include the effect of high energy and topological parameters on black holes in $F(R)$ gravity, we consider two corrections to this gravity: energy-dependent spacetime with different topological constants, and a nonlinear electrodynamics field. In other words, we combine $F(R)$ gravity's rainbow with ModMax nonlinear electrodynamics theory to see the effects of high energy and topological parameters on the physics of black holes. For this purpose, we first extract topological black hole solutions in $F(R)$% -ModMax gravity's rainbow. Then, by considering black holes as thermodynamic systems, we obtain thermodynamic quantities and check the first law of thermodynamics. The effect of the topological parameter on the Hawking temperature and the total mass of black holes is obvious. We also discuss the thermodynamic topology of topological black holes in $F(R)$-ModMax gravity's rainbow using the off-shell free energy method. In this formalism, black holes are assumed to be equivalent to defects in their thermodynamic spaces. For our analysis, we consider two different types of thermodynamic ensembles. These are: fixed $q$ ensemble and fixed $\\phi$ ensemble. We take into account all the different types of curvature hypersurfaces that can be constructed in these black holes. The local and global topology of these black holes are studied by computing the topological charges at the defects in their thermodynamic spaces. Finally, in accordance with their topological charges, we classify the black holes into three topological classes with total winding numbers corresponding to $-1, 0$, and $1$. We observe that the topological classes of these black holes are dependent on the value of the rainbow function, the sign of the scalar curvature, and the choice of ensembles.","sentences":["In order to include the effect of high energy and topological parameters on black holes in $F(R)$ gravity, we consider two corrections to this gravity: energy-dependent spacetime with different topological constants, and a nonlinear electrodynamics field.","In other words, we combine $F(R)$ gravity's rainbow with ModMax nonlinear electrodynamics theory to see the effects of high energy and topological parameters on the physics of black holes.","For this purpose, we first extract topological black hole solutions in $F(R)$% -ModMax gravity's rainbow.","Then, by considering black holes as thermodynamic systems, we obtain thermodynamic quantities and check the first law of thermodynamics.","The effect of the topological parameter on the Hawking temperature and the total mass of black holes is obvious.","We also discuss the thermodynamic topology of topological black holes in $F(R)$-ModMax gravity's rainbow using the off-shell free energy method.","In this formalism, black holes are assumed to be equivalent to defects in their thermodynamic spaces.","For our analysis, we consider two different types of thermodynamic ensembles.","These are: fixed $q$ ensemble and fixed $\\phi$ ensemble.","We take into account all the different types of curvature hypersurfaces that can be constructed in these black holes.","The local and global topology of these black holes are studied by computing the topological charges at the defects in their thermodynamic spaces.","Finally, in accordance with their topological charges, we classify the black holes into three topological classes with total winding numbers corresponding to $-1, 0$, and $1$. We observe that the topological classes of these black holes are dependent on the value of the rainbow function, the sign of the scalar curvature, and the choice of ensembles."],"url":"http://arxiv.org/abs/2405.20022v1","category":"hep-th"}
{"created":"2024-05-30 12:57:35","title":"Safe Multi-agent Reinforcement Learning with Natural Language Constraints","abstract":"The role of natural language constraints in Safe Multi-agent Reinforcement Learning (MARL) is crucial, yet often overlooked. While Safe MARL has vast potential, especially in fields like robotics and autonomous vehicles, its full potential is limited by the need to define constraints in pre-designed mathematical terms, which requires extensive domain expertise and reinforcement learning knowledge, hindering its broader adoption. To address this limitation and make Safe MARL more accessible and adaptable, we propose a novel approach named Safe Multi-agent Reinforcement Learning with Natural Language constraints (SMALL). Our method leverages fine-tuned language models to interpret and process free-form textual constraints, converting them into semantic embeddings that capture the essence of prohibited states and behaviours. These embeddings are then integrated into the multi-agent policy learning process, enabling agents to learn policies that minimize constraint violations while optimizing rewards. To evaluate the effectiveness of SMALL, we introduce the LaMaSafe, a multi-task benchmark designed to assess the performance of multiple agents in adhering to natural language constraints. Empirical evaluations across various environments demonstrate that SMALL achieves comparable rewards and significantly fewer constraint violations, highlighting its effectiveness in understanding and enforcing natural language constraints.","sentences":["The role of natural language constraints in Safe Multi-agent Reinforcement Learning (MARL) is crucial, yet often overlooked.","While Safe MARL has vast potential, especially in fields like robotics and autonomous vehicles, its full potential is limited by the need to define constraints in pre-designed mathematical terms, which requires extensive domain expertise and reinforcement learning knowledge, hindering its broader adoption.","To address this limitation and make Safe MARL more accessible and adaptable, we propose a novel approach named Safe Multi-agent Reinforcement Learning with Natural Language constraints (SMALL).","Our method leverages fine-tuned language models to interpret and process free-form textual constraints, converting them into semantic embeddings that capture the essence of prohibited states and behaviours.","These embeddings are then integrated into the multi-agent policy learning process, enabling agents to learn policies that minimize constraint violations while optimizing rewards.","To evaluate the effectiveness of SMALL, we introduce the LaMaSafe, a multi-task benchmark designed to assess the performance of multiple agents in adhering to natural language constraints.","Empirical evaluations across various environments demonstrate that SMALL achieves comparable rewards and significantly fewer constraint violations, highlighting its effectiveness in understanding and enforcing natural language constraints."],"url":"http://arxiv.org/abs/2405.20018v1","category":"cs.MA"}
{"created":"2024-05-30 12:49:16","title":"Repeatable and Reliable Efforts of Accelerated Risk Assessment","abstract":"Risk assessment of a robot in controlled environments, such as laboratories and proving grounds, is a common means to assess, certify, validate, verify, and characterize the robots' safety performance before, during, and even after their commercialization in the real-world. A standard testing program that acquires the risk estimate is expected to be (i) repeatable, such that it obtains similar risk assessments of the same testing subject among multiple trials or attempts with the similar testing effort by different stakeholders, and (ii) reliable against a variety of testing subjects produced by different vendors and manufacturers. Both repeatability and reliability are fundamental and crucial for a testing algorithm's validity, fairness, and practical feasibility, especially for standardization. However, these properties are rarely satisfied or ensured, especially as the subject robots become more complex, uncertain, and varied. This issue was present in traditional risk assessments through Monte-Carlo sampling, and remains a bottleneck for the recent accelerated risk assessment methods, primarily those using importance sampling. This study aims to enhance existing accelerated testing frameworks by proposing a new algorithm that provably integrates repeatability and reliability with the already established formality and efficiency. It also features demonstrations assessing the risk of instability from frontal impacts, initiated by push-over disturbances on a controlled inverted pendulum and a 7-DoF planar bipedal robot Rabbit managed by various control algorithms.","sentences":["Risk assessment of a robot in controlled environments, such as laboratories and proving grounds, is a common means to assess, certify, validate, verify, and characterize the robots' safety performance before, during, and even after their commercialization in the real-world.","A standard testing program that acquires the risk estimate is expected to be (i) repeatable, such that it obtains similar risk assessments of the same testing subject among multiple trials or attempts with the similar testing effort by different stakeholders, and (ii) reliable against a variety of testing subjects produced by different vendors and manufacturers.","Both repeatability and reliability are fundamental and crucial for a testing algorithm's validity, fairness, and practical feasibility, especially for standardization.","However, these properties are rarely satisfied or ensured, especially as the subject robots become more complex, uncertain, and varied.","This issue was present in traditional risk assessments through Monte-Carlo sampling, and remains a bottleneck for the recent accelerated risk assessment methods, primarily those using importance sampling.","This study aims to enhance existing accelerated testing frameworks by proposing a new algorithm that provably integrates repeatability and reliability with the already established formality and efficiency.","It also features demonstrations assessing the risk of instability from frontal impacts, initiated by push-over disturbances on a controlled inverted pendulum and a 7-DoF planar bipedal robot Rabbit managed by various control algorithms."],"url":"http://arxiv.org/abs/2405.20013v1","category":"cs.RO"}
{"created":"2024-05-30 12:47:33","title":"A construction of homotopically non-trivial embedded spheres for hyperplane arrangements","abstract":"We introduce the notion of locally consistent system of half-spaces for a real hyperplane arrangement. We embed a sphere in the complexified complement by shifting the real unit sphere into the imaginary direction indicated by the half-spaces. We then prove that the sphere is homotopically trivial if and only if the system of half-spaces is globally consistent. To prove its non-triviality, we compute the twisted intersection number of the sphere with a specific, explicitly constructed twisted Borel-Moore cycle.","sentences":["We introduce the notion of locally consistent system of half-spaces for a real hyperplane arrangement.","We embed a sphere in the complexified complement by shifting the real unit sphere into the imaginary direction indicated by the half-spaces.","We then prove that the sphere is homotopically trivial if and only if the system of half-spaces is globally consistent.","To prove its non-triviality, we compute the twisted intersection number of the sphere with a specific, explicitly constructed twisted Borel-Moore cycle."],"url":"http://arxiv.org/abs/2405.20010v1","category":"math.GT"}
{"created":"2024-05-30 12:45:34","title":"Sharing Key Semantics in Transformer Makes Efficient Image Restoration","abstract":"Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information. Notably, the Vision Transformers (ViTs) emergence has further propelled these advancements. When computing, the self-attention mechanism, a cornerstone of ViTs, tends to encompass all global cues, even those from semantically unrelated objects or regions. This inclusivity introduces computational inefficiencies, particularly noticeable with high input resolution, as it requires processing irrelevant information, thereby impeding efficiency. Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction. To address these challenges, we propose boosting IR's performance by sharing the key semantics via Transformer for IR (i.e., SemanIR) in this paper. Specifically, SemanIR initially constructs a sparse yet comprehensive key-semantic dictionary within each transformer stage by establishing essential semantic connections for every degraded patch. Subsequently, this dictionary is shared across all subsequent transformer blocks within the same stage. This strategy optimizes attention calculation within each block by focusing exclusively on semantically related components stored in the key-semantic dictionary. As a result, attention calculation achieves linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements.","sentences":["Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information.","Notably, the Vision Transformers (ViTs) emergence has further propelled these advancements.","When computing, the self-attention mechanism, a cornerstone of ViTs, tends to encompass all global cues, even those from semantically unrelated objects or regions.","This inclusivity introduces computational inefficiencies, particularly noticeable with high input resolution, as it requires processing irrelevant information, thereby impeding efficiency.","Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction.","To address these challenges, we propose boosting IR's performance by sharing the key semantics via Transformer for IR (i.e., SemanIR) in this paper.","Specifically, SemanIR initially constructs a sparse yet comprehensive key-semantic dictionary within each transformer stage by establishing essential semantic connections for every degraded patch.","Subsequently, this dictionary is shared across all subsequent transformer blocks within the same stage.","This strategy optimizes attention calculation within each block by focusing exclusively on semantically related components stored in the key-semantic dictionary.","As a result, attention calculation achieves linear computational complexity within each window.","Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements."],"url":"http://arxiv.org/abs/2405.20008v1","category":"cs.CV"}
{"created":"2024-05-30 12:39:38","title":"Combining physics-informed graph neural network and finite difference for solving forward and inverse spatiotemporal PDEs","abstract":"The great success of Physics-Informed Neural Networks (PINN) in solving partial differential equations (PDEs) has significantly advanced our simulation and understanding of complex physical systems in science and engineering. However, many PINN-like methods are poorly scalable and are limited to in-sample scenarios. To address these challenges, this work proposes a novel discrete approach termed Physics-Informed Graph Neural Network (PIGNN) to solve forward and inverse nonlinear PDEs. In particular, our approach seamlessly integrates the strength of graph neural networks (GNN), physical equations and finite difference to approximate solutions of physical systems. Our approach is compared with the PINN baseline on three well-known nonlinear PDEs (heat, Burgers and FitzHugh-Nagumo). We demonstrate the excellent performance of the proposed method to work with irregular meshes, longer time steps, arbitrary spatial resolutions, varying initial conditions (ICs) and boundary conditions (BCs) by conducting extensive numerical experiments. Numerical results also illustrate the superiority of our approach in terms of accuracy, time extrapolability, generalizability and scalability. The main advantage of our approach is that models trained in small domains with simple settings have excellent fitting capabilities and can be directly applied to more complex situations in large domains.","sentences":["The great success of Physics-Informed Neural Networks (PINN) in solving partial differential equations (PDEs) has significantly advanced our simulation and understanding of complex physical systems in science and engineering.","However, many PINN-like methods are poorly scalable and are limited to in-sample scenarios.","To address these challenges, this work proposes a novel discrete approach termed Physics-Informed Graph Neural Network (PIGNN) to solve forward and inverse nonlinear PDEs.","In particular, our approach seamlessly integrates the strength of graph neural networks (GNN), physical equations and finite difference to approximate solutions of physical systems.","Our approach is compared with the PINN baseline on three well-known nonlinear PDEs (heat, Burgers and FitzHugh-Nagumo).","We demonstrate the excellent performance of the proposed method to work with irregular meshes, longer time steps, arbitrary spatial resolutions, varying initial conditions (ICs) and boundary conditions (BCs) by conducting extensive numerical experiments.","Numerical results also illustrate the superiority of our approach in terms of accuracy, time extrapolability, generalizability and scalability.","The main advantage of our approach is that models trained in small domains with simple settings have excellent fitting capabilities and can be directly applied to more complex situations in large domains."],"url":"http://arxiv.org/abs/2405.20000v1","category":"math.NA"}
{"created":"2024-05-30 11:58:05","title":"Testing in the Evolving World of DL Systems:Insights from Python GitHub Projects","abstract":"In the ever-evolving field of Deep Learning (DL), ensuring project quality and reliability remains a crucial challenge. This research investigates testing practices within DL projects in GitHub. It quantifies the adoption of testing methodologies, focusing on aspects like test automation, the types of tests (e.g., unit, integration, and system), test suite growth rate, and evolution of testing practices across different project versions. We analyze a subset of 300 carefully selected repositories based on quantitative and qualitative criteria. This study reports insights on the prevalence of testing practices in DL projects within the open-source community.","sentences":["In the ever-evolving field of Deep Learning (DL), ensuring project quality and reliability remains a crucial challenge.","This research investigates testing practices within DL projects in GitHub.","It quantifies the adoption of testing methodologies, focusing on aspects like test automation, the types of tests (e.g., unit, integration, and system), test suite growth rate, and evolution of testing practices across different project versions.","We analyze a subset of 300 carefully selected repositories based on quantitative and qualitative criteria.","This study reports insights on the prevalence of testing practices in DL projects within the open-source community."],"url":"http://arxiv.org/abs/2405.19976v1","category":"cs.SE"}
{"created":"2024-05-30 11:56:27","title":"Space-time first-order correlations of an open Bose-Hubbard model with incoherent pump and loss","abstract":"We investigate the correlation properties in the steady state of driven-dissipative interacting bosonic systems in the quantum regime, as for example non-linear photonic cavities. Specifically, we consider the Bose-Hubbard model on a periodic chain and with spatially homogeneous one-body loss and pump within the Markovian approximation. The steady state corresponds to an infinite temperature state at finite chemical potential with diagonal spatial correlations. Nonetheless, we observe a nontrivial behaviour of the space-time two-point correlation function in the steady state, obtained by exact diagonalisation. In particular, we find that the decay width of the propagator is not only renormalised at increasing interactions, as it is the case of a single non-linear resonator, but also at increasing hopping strength. We then compute the full spectral function, finding that it contains both a dispersive free-particle like dispersion at low energy and a doublon branch at energy corresponding to the on-site interactions. We compare with the corresponding calculation for the ground state of a closed quantum system and show that the driven-dissipative nature - determining both the steady state and the dynamical evolution - changes the low-lying part of the spectrum, where noticeably, the dispersion is quadratic instead of linear at small wavevectors.","sentences":["We investigate the correlation properties in the steady state of driven-dissipative interacting bosonic systems in the quantum regime, as for example non-linear photonic cavities.","Specifically, we consider the Bose-Hubbard model on a periodic chain and with spatially homogeneous one-body loss and pump within the Markovian approximation.","The steady state corresponds to an infinite temperature state at finite chemical potential with diagonal spatial correlations.","Nonetheless, we observe a nontrivial behaviour of the space-time two-point correlation function in the steady state, obtained by exact diagonalisation.","In particular, we find that the decay width of the propagator is not only renormalised at increasing interactions, as it is the case of a single non-linear resonator, but also at increasing hopping strength.","We then compute the full spectral function, finding that it contains both a dispersive free-particle like dispersion at low energy and a doublon branch at energy corresponding to the on-site interactions.","We compare with the corresponding calculation for the ground state of a closed quantum system and show that the driven-dissipative nature - determining both the steady state and the dynamical evolution - changes the low-lying part of the spectrum, where noticeably, the dispersion is quadratic instead of linear at small wavevectors."],"url":"http://arxiv.org/abs/2405.19972v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-30 11:50:03","title":"Stable semi-implicit SDC methods for conservation laws","abstract":"Semi-implicit spectral deferred correction (SDC) methods provide a systematic approach to construct time integration methods of arbitrarily high order for nonlinear evolution equations including conservation laws. They converge towards $A$- or even $L$-stable collocation methods, but are often not sufficiently robust themselves. In this paper, a family of SDC methods inspired by an implicit formulation of the Lax-Wendroff method is developed. Compared to fully implicit approaches, the methods have the advantage that they only require the solution of positive definite or semi-definite linear systems. Numerical evidence suggests that the proposed semi-implicit SDC methods with Radau points are $L$-stable up to order 11 and require very little diffusion for orders 13 and 15. The excellent stability and accuracy of these methods is confirmed by numerical experiments with 1D conservation problems, including the convection-diffusion, Burgers, Euler and Navier-Stokes equations.","sentences":["Semi-implicit spectral deferred correction (SDC) methods provide a systematic approach to construct time integration methods of arbitrarily high order for nonlinear evolution equations including conservation laws.","They converge towards $A$- or even $L$-stable collocation methods, but are often not sufficiently robust themselves.","In this paper, a family of SDC methods inspired by an implicit formulation of the Lax-Wendroff method is developed.","Compared to fully implicit approaches, the methods have the advantage that they only require the solution of positive definite or semi-definite linear systems.","Numerical evidence suggests that the proposed semi-implicit SDC methods with Radau points are $L$-stable up to order 11 and require very little diffusion for orders 13 and 15.","The excellent stability and accuracy of these methods is confirmed by numerical experiments with 1D conservation problems, including the convection-diffusion, Burgers, Euler and Navier-Stokes equations."],"url":"http://arxiv.org/abs/2405.19969v1","category":"math.NA"}
{"created":"2024-05-30 11:43:42","title":"Several classes of BCH codes of length $n=\\frac{q^{m}-1}{2}$","abstract":"BCH codes are an important class of cyclic codes, and have wide applications in communication and storage systems. In this paper, we study the negacyclic BCH code and cyclic BCH code of length $n=\\frac{q^m-1}{2}$.For negacyclic BCH code, we give the dimensions of $\\mathcal C_{(n,-1,\\delta,0)}$ for $\\widetilde{\\delta} =a\\frac{q^m-1}{q-1},aq^{m-1}-1$($1\\leq a <\\frac{q-1}{2}$) and $\\widetilde{\\delta} =a\\frac{q^m-1}{q-1}+b\\frac{q^m-1}{q^2-1},aq^{m-1}+(a+b)q^{m-2}-1$ $(2\\mid m,1\\leq a+b \\leq q-1$,$\\left\\lceil \\frac{q-a-2}{2}\\right\\rceil\\geq 1)$. The dimensions of negacyclic BCH codes $\\mathcal C_{(n,-1,\\delta,0)}$ with few nonzeros and $\\mathcal C_{(n,-1,\\delta,b)}$ with $b\\neq 1$ are settled.For cyclic BCH code, we give the weight distributions of extended codes $\\overline{\\mathcal C}_{(n,1,\\delta,1)}$ for $\\delta=\\delta_1,\\delta_2$ and the parameters of dual code $\\mathcal C^{\\perp}_{(n,1,\\delta,1)}$ for $\\delta_2\\leq \\delta \\leq \\delta_1$.","sentences":["BCH codes are an important class of cyclic codes, and have wide applications in communication and storage systems.","In this paper, we study the negacyclic BCH code and cyclic BCH code of length $n=\\frac{q^m-1}{2}$.For negacyclic BCH code, we give the dimensions of $\\mathcal C_{(n,-1,\\delta,0)}$ for $\\widetilde{\\delta} =a\\frac{q^m-1}{q-1},aq^{m-1}-1$($1\\leq a <\\frac{q-1}{2}$) and $\\widetilde{\\delta} =a\\frac{q^m-1}{q-1}+b\\frac{q^m-1}{q^2-1},aq^{m-1}+(a+b)q^{m-2}-1$ $(2\\mid m,1\\leq a+b","\\leq q-1$,$\\left\\lceil \\frac{q-a-2}{2}\\right\\rceil\\geq 1)$.","The dimensions of negacyclic BCH codes $\\mathcal C_{(n,-1,\\delta,0)}$ with few nonzeros and $\\mathcal C_{(n,-1,\\delta,b)}$ with $b\\neq 1$ are settled.","For cyclic BCH code, we give the weight distributions of extended codes $\\overline{\\mathcal C}_{(n,1,\\delta,1)}$ for $\\delta=\\delta_1,\\delta_2$ and the parameters of dual code $\\mathcal C^{\\perp}_{(n,1,\\delta,1)}$ for $\\delta_2\\leq \\delta \\leq \\delta_1$."],"url":"http://arxiv.org/abs/2405.19965v1","category":"cs.IT"}
{"created":"2024-05-30 11:29:34","title":"A study of interacting galaxies from the Arp-Madore Catalogue: Triggering of star formation and nuclear activity","abstract":"We present Gemini Multi-Object Spectrograph (GMOS) spectroscopic observations of 95 galaxies from the Arp & Madore (1987) catalogue of peculiar galaxies. These galaxies have been selected because they appear to be in pairs and small groups. These observations have allowed us to confirm that 60 galaxies are indeed interacting systems. For the confirmed interacting sample, we have built a matched control sample of isolated galaxies. We present an analysis of the stellar populations and nuclear activity in the interacting galaxies and compare them with the isolated galaxies. We find a median light (mass) fraction of 55% (10%) in the interacting galaxies coming from stellar populations younger than 2 Gyr and 28% (3%) in the case of the isolated galaxies. More than half of the interacting galaxies are dominated by this young stellar population, while the isolated ones have most of their light coming from older stellar populations. We used a combination of diagnostic diagrams (BPTs and WHAN) to classify the main ionization mechanisms of the gas. The interacting galaxies in our sample consistently show a higher fraction of active galactic nuclei (AGN) relative to the control sample, which ranges between 1.6 and 4 depending on the combination of diagnostic diagrams employed to classify the galaxies and the number of galaxies considered. Our study provides further observational evidence that interactions drive star formation and nuclear activity in galaxies and can have a significant impact on galaxy evolution.","sentences":["We present Gemini Multi-Object Spectrograph (GMOS) spectroscopic observations of 95 galaxies from the Arp & Madore (1987) catalogue of peculiar galaxies.","These galaxies have been selected because they appear to be in pairs and small groups.","These observations have allowed us to confirm that 60 galaxies are indeed interacting systems.","For the confirmed interacting sample, we have built a matched control sample of isolated galaxies.","We present an analysis of the stellar populations and nuclear activity in the interacting galaxies and compare them with the isolated galaxies.","We find a median light (mass) fraction of 55% (10%) in the interacting galaxies coming from stellar populations younger than 2 Gyr and 28% (3%) in the case of the isolated galaxies.","More than half of the interacting galaxies are dominated by this young stellar population, while the isolated ones have most of their light coming from older stellar populations.","We used a combination of diagnostic diagrams (BPTs and WHAN) to classify the main ionization mechanisms of the gas.","The interacting galaxies in our sample consistently show a higher fraction of active galactic nuclei (AGN) relative to the control sample, which ranges between 1.6 and 4 depending on the combination of diagnostic diagrams employed to classify the galaxies and the number of galaxies considered.","Our study provides further observational evidence that interactions drive star formation and nuclear activity in galaxies and can have a significant impact on galaxy evolution."],"url":"http://arxiv.org/abs/2405.19960v1","category":"astro-ph.GA"}
{"created":"2024-05-30 11:28:18","title":"An ergodic automorphism $\\bf T$ with singular spectrum of $\\bf T^{\\otimes n}$ and Lebesgue one of $\\bf T^{\\otimes (n+1)}$","abstract":"For any natural $n$, and real $\\alpha\\geq 0$ we construct an ergodic automorphism $T$ such that its tensor powers $T^{\\otimes n}$ have singular spectra if $n\\leq 1+\\alpha /2$, and Lebesgue if $n\\, > 1+\\alpha/2$.","sentences":["For any natural $n$, and real $\\alpha\\geq 0$ we construct an ergodic automorphism $T$ such that its tensor powers $T^{\\otimes n}$ have singular spectra if $n\\leq 1+\\alpha /2$, and Lebesgue if $n\\, > 1+\\alpha/2$."],"url":"http://arxiv.org/abs/2405.19959v1","category":"math.DS"}
{"created":"2024-05-30 11:09:38","title":"Universal finite-time scaling in the transcritical, saddle-node, and pitchfork discrete and continuous bifurcations","abstract":"Bifurcations are one of the most remarkable features of dynamical systems. Corral et al. [Sci. Rep. 8(11783), 2018] showed the existence of scaling laws describing the transient (finite-time) dynamics in discrete dynamical systems close to a bifurcation point, following an approach that was valid for the transcritical as well as for the saddle-node bifurcations. We reformulate those previous results and extend them to other discrete and continuous bifurcations, remarkably the pitchfork bifurcation. In contrast to the previous work, we obtain a finite-time bifurcation diagram directly from the scaling law, without a necessary knowledge of the stable fixed point. The derived scaling laws provide a very good and universal description of the transient behavior of the systems for long times and close to the bifurcation points.","sentences":["Bifurcations are one of the most remarkable features of dynamical systems.","Corral et al.","[Sci. Rep. 8(11783), 2018] showed the existence of scaling laws describing the transient (finite-time) dynamics in discrete dynamical systems close to a bifurcation point, following an approach that was valid for the transcritical as well as for the saddle-node bifurcations.","We reformulate those previous results and extend them to other discrete and continuous bifurcations, remarkably the pitchfork bifurcation.","In contrast to the previous work, we obtain a finite-time bifurcation diagram directly from the scaling law, without a necessary knowledge of the stable fixed point.","The derived scaling laws provide a very good and universal description of the transient behavior of the systems for long times and close to the bifurcation points."],"url":"http://arxiv.org/abs/2405.19947v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-30 11:06:21","title":"Multiple sampling and interpolation in a space of polynomials","abstract":"We study sampling and interpolation arrays with multiplicities for the spaces P_k of holomorphic polynomials of degree at most k. We find that the geometric conditions satisfied by these arrays are in accordance with the conditions satisfied by the sampling and interpolating sequences with unbounded multiplicities in the Fock space, which can be seen as a limiting case of the space P_k as k tends to infinity.","sentences":["We study sampling and interpolation arrays with multiplicities for the spaces P_k of holomorphic polynomials of degree at most k.","We find that the geometric conditions satisfied by these arrays are in accordance with the conditions satisfied by the sampling and interpolating sequences with unbounded multiplicities in the Fock space, which can be seen as a limiting case of the space P_k as k tends to infinity."],"url":"http://arxiv.org/abs/2405.19945v1","category":"math.CV"}
{"created":"2024-05-30 11:04:30","title":"Discrete-Time I&I Adaptive Interconnection and Damping Passivity-Based Control for Nonlinearly Parameterized Port-Controlled Hamiltonian Systems","abstract":"In this paper, a discrete-time I&I-based adaptive IDA-PBC controller for uncertain nonlinearly parameterized port-controlled Hamiltonian systems (PCH), where the parameter uncertainties are assumed in the energy function, is constructed. A proper formulation for the uncertain system dynamics is established where the uncertainties appear in nonlinearly parameterized form in the gradient of the Hamiltonian function. The adaptive IDA-PBC controller is constructed considering this formulation. For the adaptation mechanism of the IDA-PBC controller, a discrete-time parameter estimator is derived based on the immersion and invariance (I&I) approach. A structure for a free design function in the I&I-based estimator is proposed including some other free design functions. If these free design functions are selected to satisfy some conditions, derived in this paper, the Lyapunov asymptotic stability of the estimator dynamics is guaranteed. Besides, assuming these conditions are satisfied, local asymptotic stability of the closed-loop system, in a sufficiently large set is shown. The proposed method is applied to the two physical system examples and the performance of the adaptive controller is tested by simulation. It is demonstrated that the performance of the certain IDA-PBC controller is maintained by the adaptive IDA-PBC controller successfully.","sentences":["In this paper, a discrete-time I&I-based adaptive IDA-PBC controller for uncertain nonlinearly parameterized port-controlled Hamiltonian systems (PCH), where the parameter uncertainties are assumed in the energy function, is constructed.","A proper formulation for the uncertain system dynamics is established where the uncertainties appear in nonlinearly parameterized form in the gradient of the Hamiltonian function.","The adaptive IDA-PBC controller is constructed considering this formulation.","For the adaptation mechanism of the IDA-PBC controller, a discrete-time parameter estimator is derived based on the immersion and invariance (I&I) approach.","A structure for a free design function in the I&I-based estimator is proposed including some other free design functions.","If these free design functions are selected to satisfy some conditions, derived in this paper, the Lyapunov asymptotic stability of the estimator dynamics is guaranteed.","Besides, assuming these conditions are satisfied, local asymptotic stability of the closed-loop system, in a sufficiently large set is shown.","The proposed method is applied to the two physical system examples and the performance of the adaptive controller is tested by simulation.","It is demonstrated that the performance of the certain IDA-PBC controller is maintained by the adaptive IDA-PBC controller successfully."],"url":"http://arxiv.org/abs/2405.19944v1","category":"eess.SY"}
{"created":"2024-05-30 10:56:12","title":"Material and size dependent corrections to conductance quantization in anomalous Hall effect from anomaly inflow","abstract":"In quantum anomalous Hall (QAH) systems, the Hall conductance is quantized and the corresponding effective topological theory of the system is the Chern-Simons theory. The conductance quantum is given by the universal constant $e^2/h$ -- the inverse von Klitzing constant -- that is independent of the bulk gap, as well as the size of the system. This picture relies on the assumption that the edge modes are sharply localized at the edge, i.e. they have zero width. We show that considering the physical case where the edge modes have finite localization length $b$, the effective action would not be topological in bulk direction anymore. Due to non-zero $b$ the conductance quantum will be corrected as $(1-\\varepsilon)e^2/h$ where $\\varepsilon$ encompasses the non-universal (i.e. material/sample dependent) part that is determined by the dimensionless ratios $\\frac{gb}{\\hbar v_F}$ and $\\frac{b}{L}$ where $g,v_F,L$ are the bulk gap, Fermi velocity and sample length. To compute the non-universal correction $\\varepsilon$ we use anomaly inflow framework according to which the bulk action produces the correct amount of anomaly inflow that would cancel the anomaly of the chiral edge modes. These corrections place limits on the precision of measurable quantization in units of the inverse von Klitzing constant for QAH systems with smaller sizes and/or smaller bulk gaps. Our result suggests that the failure of precision measurements to reproduce the exact conductance quantum $e^2/h$ is not an annoying sample quality issue, but it contains the quantitative physics of anomaly inflow that can be inferred by the systematic study of such corrections.","sentences":["In quantum anomalous Hall (QAH) systems, the Hall conductance is quantized and the corresponding effective topological theory of the system is the Chern-Simons theory.","The conductance quantum is given by the universal constant $e^2/h$ -- the inverse von Klitzing constant -- that is independent of the bulk gap, as well as the size of the system.","This picture relies on the assumption that the edge modes are sharply localized at the edge, i.e. they have zero width.","We show that considering the physical case where the edge modes have finite localization length $b$, the effective action would not be topological in bulk direction anymore.","Due to non-zero $b$ the conductance quantum will be corrected as $(1-\\varepsilon)e^2/h$ where $\\varepsilon$ encompasses the non-universal (i.e. material/sample dependent) part that is determined by the dimensionless ratios $\\frac{gb}{\\hbar v_F}$ and $\\frac{b}{L}$ where $g,v_F,L$ are the bulk gap, Fermi velocity and sample length.","To compute the non-universal correction $\\varepsilon$ we use anomaly inflow framework according to which the bulk action produces the correct amount of anomaly inflow that would cancel the anomaly of the chiral edge modes.","These corrections place limits on the precision of measurable quantization in units of the inverse von Klitzing constant for QAH systems with smaller sizes and/or smaller bulk gaps.","Our result suggests that the failure of precision measurements to reproduce the exact conductance quantum $e^2/h$ is not an annoying sample quality issue, but it contains the quantitative physics of anomaly inflow that can be inferred by the systematic study of such corrections."],"url":"http://arxiv.org/abs/2405.19935v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 10:47:15","title":"Multi-scale flow, permeability, and heat transport in low-carbon and traditional building materials","abstract":"Permeability and heat transport through building materials ultimately dictates their insulatory performance over a buildings service lifetime. Experiments combining XCT with numerical modelling are an accepted method of studying pore scale processes and have been used extensively in the oil and gas industry to study highly complex reservoir rocks. However, despite the obvious similarities in structure and application, these techniques have not yet been widely adopted by the building and construction industry. An experimental investigation was performed on the pore structure of several building materials, including conventional, historic, and innovative, using XCT and direct numerical simulation. Six samples were imaged at between a 4 and 15 micron resolution inside a micro-CT scanner. The porosity and connectivity were extracted with the grain, throat, and pore size distributions using image analysis. The permeability, velocity, and thermal conductivity were then investigated using GeoChemFoam, our highly-versatile and open source numerical solver. It was found that each material had a unique, heterogeneous and sometimes multi-scale structure that had a large impact on the permeability and thermal conductivity. Furthermore, it was found that the method of including sub-resolution porosity directly effected these bulk property calculations for both parameters, especially in the materials with high structural heterogeneity. This is the first multi-scale study of structure, flow and heat transport on building materials and this workflow could easily be adapted to understand and improve designs in other industries that use porous materials such as fuel cells and batteries technology, lightweight materials and insulation, and semiconductors.","sentences":["Permeability and heat transport through building materials ultimately dictates their insulatory performance over a buildings service lifetime.","Experiments combining XCT with numerical modelling are an accepted method of studying pore scale processes and have been used extensively in the oil and gas industry to study highly complex reservoir rocks.","However, despite the obvious similarities in structure and application, these techniques have not yet been widely adopted by the building and construction industry.","An experimental investigation was performed on the pore structure of several building materials, including conventional, historic, and innovative, using XCT and direct numerical simulation.","Six samples were imaged at between a 4 and 15 micron resolution inside a micro-CT scanner.","The porosity and connectivity were extracted with the grain, throat, and pore size distributions using image analysis.","The permeability, velocity, and thermal conductivity were then investigated using GeoChemFoam, our highly-versatile and open source numerical solver.","It was found that each material had a unique, heterogeneous and sometimes multi-scale structure that had a large impact on the permeability and thermal conductivity.","Furthermore, it was found that the method of including sub-resolution porosity directly effected these bulk property calculations for both parameters, especially in the materials with high structural heterogeneity.","This is the first multi-scale study of structure, flow and heat transport on building materials and this workflow could easily be adapted to understand and improve designs in other industries that use porous materials such as fuel cells and batteries technology, lightweight materials and insulation, and semiconductors."],"url":"http://arxiv.org/abs/2405.19930v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 10:45:10","title":"Periodic forces combined with feedback induce quenching in a bistable oscillator","abstract":"The coexistence of an abnormal rhythm and a normal steady state is often observed in nature (e.g., epilepsy). Such a system is modeled as a bistable oscillator that possesses both a limit cycle and a fixed point. Although bistable oscillators under several perturbations have been addressed in the literature, the mechanism of oscillation quenching, a transition from a limit cycle to a fixed point, has not been fully understood. In this study, we analyze quenching using the extended Stuart-Landau oscillator driven by periodic forces. Numerical simulations suggest that the entrainment to the periodic force induces the amplitude change of a limit cycle. By reducing the system with an averaging method, we investigate the bifurcation structures of the periodically-driven oscillator. We find that oscillation quenching occurs by the homoclinic bifurcation when we use a periodic force combined with quadratic feedback. In conclusion, we develop a state-transition method in a bistable oscillator using periodic forces, which would have the potential for practical applications in controlling and annihilating abnormal oscillations. Moreover, we clarify the rich and diverse bifurcation structures behind periodically-driven bistable oscillators, which we believe would contribute to further understanding the complex behaviors in non-autonomous systems.","sentences":["The coexistence of an abnormal rhythm and a normal steady state is often observed in nature (e.g., epilepsy).","Such a system is modeled as a bistable oscillator that possesses both a limit cycle and a fixed point.","Although bistable oscillators under several perturbations have been addressed in the literature, the mechanism of oscillation quenching, a transition from a limit cycle to a fixed point, has not been fully understood.","In this study, we analyze quenching using the extended Stuart-Landau oscillator driven by periodic forces.","Numerical simulations suggest that the entrainment to the periodic force induces the amplitude change of a limit cycle.","By reducing the system with an averaging method, we investigate the bifurcation structures of the periodically-driven oscillator.","We find that oscillation quenching occurs by the homoclinic bifurcation when we use a periodic force combined with quadratic feedback.","In conclusion, we develop a state-transition method in a bistable oscillator using periodic forces, which would have the potential for practical applications in controlling and annihilating abnormal oscillations.","Moreover, we clarify the rich and diverse bifurcation structures behind periodically-driven bistable oscillators, which we believe would contribute to further understanding the complex behaviors in non-autonomous systems."],"url":"http://arxiv.org/abs/2405.19929v1","category":"nlin.AO"}
{"created":"2024-05-30 10:39:46","title":"Integrated Sensing and Communications Framework for 6G Networks","abstract":"In this paper, we propose a novel integrated sensing and communications (ISAC) framework for the sixth generation (6G) mobile networks, in which we decompose the real physical world into static environment, dynamic targets, and various object materials. The ubiquitous static environment occupies the vast majority of the physical world, for which we design static environment reconstruction (SER) scheme to obtain the layout and point cloud information of static buildings. The dynamic targets floating in static environments create the spatiotemporal transition of the physical world, for which we design comprehensive dynamic target sensing (DTS) scheme to detect, estimate, track, image and recognize the dynamic targets in real-time. The object materials enrich the electromagnetic laws of the physical world, for which we develop object material recognition (OMR) scheme to estimate the electromagnetic coefficient of the objects. Besides, to integrate these sensing functions into existing communications systems, we discuss the interference issues and corresponding solutions for ISAC cellular networks. Furthermore, we develop an ISAC hardware prototype platform that can reconstruct the environmental maps and sense the dynamic targets while maintaining communications services. With all these designs, the proposed ISAC framework can support multifarious emerging applications, such as digital twins, low altitude economy, internet of vehicles, marine management, deformation monitoring, etc.","sentences":["In this paper, we propose a novel integrated sensing and communications (ISAC) framework for the sixth generation (6G) mobile networks, in which we decompose the real physical world into static environment, dynamic targets, and various object materials.","The ubiquitous static environment occupies the vast majority of the physical world, for which we design static environment reconstruction (SER) scheme to obtain the layout and point cloud information of static buildings.","The dynamic targets floating in static environments create the spatiotemporal transition of the physical world, for which we design comprehensive dynamic target sensing (DTS) scheme to detect, estimate, track, image and recognize the dynamic targets in real-time.","The object materials enrich the electromagnetic laws of the physical world, for which we develop object material recognition (OMR) scheme to estimate the electromagnetic coefficient of the objects.","Besides, to integrate these sensing functions into existing communications systems, we discuss the interference issues and corresponding solutions for ISAC cellular networks.","Furthermore, we develop an ISAC hardware prototype platform that can reconstruct the environmental maps and sense the dynamic targets while maintaining communications services.","With all these designs, the proposed ISAC framework can support multifarious emerging applications, such as digital twins, low altitude economy, internet of vehicles, marine management, deformation monitoring, etc."],"url":"http://arxiv.org/abs/2405.19925v1","category":"eess.SP"}
{"created":"2024-05-30 10:39:38","title":"Relative sectional category revisited","abstract":"The concept of relative sectional category expands upon classical sectional category theory by incorporating the pullback of a fibration along a map. Our paper aims not only to explore this extension but also to thoroughly investigate its properties. We seek to uncover how the relative sectional category unifies several homotopic numerical invariants found in recent literature. These include the topological complexity of maps according to Murillo-Wu or Scott, relative topological complexity as defined by Farber, and homotopic distance for continuous maps in the sense of Mac\\'{\\i}as-Virg\\'os and Mosquera-Lois, among others.","sentences":["The concept of relative sectional category expands upon classical sectional category theory by incorporating the pullback of a fibration along a map.","Our paper aims not only to explore this extension but also to thoroughly investigate its properties.","We seek to uncover how the relative sectional category unifies several homotopic numerical invariants found in recent literature.","These include the topological complexity of maps according to Murillo-Wu or Scott, relative topological complexity as defined by Farber, and homotopic distance for continuous maps in the sense of Mac\\'{\\i}as-Virg\\'os and Mosquera-Lois, among others."],"url":"http://arxiv.org/abs/2405.19924v1","category":"math.AT"}
{"created":"2024-05-30 10:33:14","title":"MCDS-VSS: Moving Camera Dynamic Scene Video Semantic Segmentation by Filtering with Self-Supervised Geometry and Motion","abstract":"Autonomous systems, such as self-driving cars, rely on reliable semantic environment perception for decision making. Despite great advances in video semantic segmentation, existing approaches ignore important inductive biases and lack structured and interpretable internal representations. In this work, we propose MCDS-VSS, a structured filter model that learns in a self-supervised manner to estimate scene geometry and ego-motion of the camera, while also estimating the motion of external objects. Our model leverages these representations to improve the temporal consistency of semantic segmentation without sacrificing segmentation accuracy. MCDS-VSS follows a prediction-fusion approach in which scene geometry and camera motion are first used to compensate for ego-motion, then residual flow is used to compensate motion of dynamic objects, and finally the predicted scene features are fused with the current features to obtain a temporally consistent scene segmentation. Our model parses automotive scenes into multiple decoupled interpretable representations such as scene geometry, ego-motion, and object motion. Quantitative evaluation shows that MCDS-VSS achieves superior temporal consistency on video sequences while retaining competitive segmentation performance.","sentences":["Autonomous systems, such as self-driving cars, rely on reliable semantic environment perception for decision making.","Despite great advances in video semantic segmentation, existing approaches ignore important inductive biases and lack structured and interpretable internal representations.","In this work, we propose MCDS-VSS, a structured filter model that learns in a self-supervised manner to estimate scene geometry and ego-motion of the camera, while also estimating the motion of external objects.","Our model leverages these representations to improve the temporal consistency of semantic segmentation without sacrificing segmentation accuracy.","MCDS-VSS follows a prediction-fusion approach in which scene geometry and camera motion are first used to compensate for ego-motion, then residual flow is used to compensate motion of dynamic objects, and finally the predicted scene features are fused with the current features to obtain a temporally consistent scene segmentation.","Our model parses automotive scenes into multiple decoupled interpretable representations such as scene geometry, ego-motion, and object motion.","Quantitative evaluation shows that MCDS-VSS achieves superior temporal consistency on video sequences while retaining competitive segmentation performance."],"url":"http://arxiv.org/abs/2405.19921v1","category":"cs.CV"}
{"created":"2024-05-30 10:23:18","title":"Implementation of B$^\u03b1$ Gates","abstract":"In this brief report, we discuss the characteristics of B$^{\\alpha}$ gates. We propose an experimental scheme to implement B$^{\\alpha}$ gates in ion-trap system. In this scheme, we assume that only a single vibrational mode contributes to spin-spin coupling. This scheme is an extension of a recently proposed scheme to realize XY-type interaction in ion-trap system. With the successful implementation of this scheme, B$^{\\alpha}$ gates can be used for doing quantum computation in ion-trap quantum computers.","sentences":["In this brief report, we discuss the characteristics of B$^{\\alpha}$ gates.","We propose an experimental scheme to implement B$^{\\alpha}$ gates in ion-trap system.","In this scheme, we assume that only a single vibrational mode contributes to spin-spin coupling.","This scheme is an extension of a recently proposed scheme to realize XY-type interaction in ion-trap system.","With the successful implementation of this scheme, B$^{\\alpha}$ gates can be used for doing quantum computation in ion-trap quantum computers."],"url":"http://arxiv.org/abs/2405.19913v1","category":"quant-ph"}
{"created":"2024-05-30 10:19:45","title":"Long-Range Correlations in Elastic Moduli and Local Stresses at the Unjamming Transition","abstract":"We explore the behavior of spatially heterogeneous elastic moduli as well as the correlations between local moduli in model solids with short-range repulsive potentials. We show through numerical simulations that local elastic moduli exhibit long-range correlations, similar to correlations in the local stresses. Specifically, the correlations in local shear moduli exhibit anisotropic behavior at large lengthscales characterized by pinch-point singularities in Fourier space, displaying a structural pattern akin to shear stress correlations. Focussing on two-dimensional jammed solids approaching the unjamming transition, we show that stress correlations exhibit universal properties, characterized by a quadratic $p^2$ dependence of the correlations as the pressure $p$ approaches zero, independent of the details of the model. In contrast, the modulus correlations exhibit a power-law dependence with different exponents depending on the specific interaction potential. Furthermore, we illustrate that while affine responses lack long-range correlations, the total modulus, which encompasses non-affine behavior, exhibits long-range correlations.","sentences":["We explore the behavior of spatially heterogeneous elastic moduli as well as the correlations between local moduli in model solids with short-range repulsive potentials.","We show through numerical simulations that local elastic moduli exhibit long-range correlations, similar to correlations in the local stresses.","Specifically, the correlations in local shear moduli exhibit anisotropic behavior at large lengthscales characterized by pinch-point singularities in Fourier space, displaying a structural pattern akin to shear stress correlations.","Focussing on two-dimensional jammed solids approaching the unjamming transition, we show that stress correlations exhibit universal properties, characterized by a quadratic $p^2$ dependence of the correlations as the pressure $p$ approaches zero, independent of the details of the model.","In contrast, the modulus correlations exhibit a power-law dependence with different exponents depending on the specific interaction potential.","Furthermore, we illustrate that while affine responses lack long-range correlations, the total modulus, which encompasses non-affine behavior, exhibits long-range correlations."],"url":"http://arxiv.org/abs/2405.19908v1","category":"cond-mat.soft"}
{"created":"2024-05-30 10:16:43","title":"Yangian for cotangent Lie algebras and spectral $R$-matrices","abstract":"In this paper, we present a canonical quantization of Lie bialgebra structures on the formal power series $\\mathfrak{d}[\\![t]\\!]$ with coefficients in the cotangent Lie algebra $\\mathfrak{d} = T^*\\mathfrak{g} = \\mathfrak{g} \\ltimes \\mathfrak{g}^*$ to a simple complex Lie algebra $\\mathfrak{g}$. We prove that these quantizations produce twists to the natural analog of the Yangian for $\\mathfrak{d}$. Moreover, we construct spectral $R$-matrices for these twisted Yangians as compositions of twisting matrices.   The motivation for the construction of these twisted Yangians over $\\mathfrak{d}$ comes from certain 4d holomorphic-topological gauge theory. More precisely, we show that pertubative line operators in this theory can be realized as representations of these Yangians. Moreover, the comultiplications of these Yangians correspond to the monodial structure of the category of line operators.","sentences":["In this paper, we present a canonical quantization of Lie bialgebra structures on the formal power series $\\mathfrak{d}[\\![t]\\!]$ with coefficients in the cotangent Lie algebra $\\mathfrak{d} = T^*\\mathfrak{g} = \\mathfrak{g} \\ltimes \\mathfrak{g}^*$ to a simple complex Lie algebra $\\mathfrak{g}$.","We prove that these quantizations produce twists to the natural analog of the Yangian for $\\mathfrak{d}$. Moreover, we construct spectral $R$-matrices for these twisted Yangians as compositions of twisting matrices.   ","The motivation for the construction of these twisted Yangians over $\\mathfrak{d}$ comes from certain 4d holomorphic-topological gauge theory.","More precisely, we show that pertubative line operators in this theory can be realized as representations of these Yangians.","Moreover, the comultiplications of these Yangians correspond to the monodial structure of the category of line operators."],"url":"http://arxiv.org/abs/2405.19906v1","category":"math.QA"}
{"created":"2024-05-30 10:09:52","title":"The backreaction of stellar wobbling on accretion discs of massive protostars","abstract":"In recent years, it has been demonstrated that massive stars see their infant circumstellar medium shaped into a large, irradiated, gravitationally unstable accretion disc during their early formation phase. Such discs constitute the gas reservoir in which nascent high-mass stars gain substantial fraction of their mass by episodic accretion of dense gaseous circumstellar clumps. We aim to evaluate the effects of stellar motion, caused by the disc non-axisymmetric gravitational field, on the disc evolution and its spatial morphology. In particular, we analyze the disc propensity to gravitational instability and fragmentation, and also disc appearance on synthetic millimeter-band images pertinent to the alma facility. We employed three-dimensional radiation-hydrodynamical simulations of the surroundings of a young massive star in the non-inertial spherical coordinate system, adopting the highest spatial resolution to date and including the indirect star-disc gravitational potential caused by the asymmetries in the circumstellar disc. The resulting disc were postprocessed with the radiation transfer tool RADMC-3D and CASA softwear to obtain disc synthetic images. The redistribution of angular momentum in the system makes the disc smaller and rounder, reduces the number of circumstellar gaseous clumps formed via disc gravitational fragmentation, and prevents the ejection of gaseous clumps from the disc. The synthetic predictive images at millimeter wavelengths of the accretion disc including stellar wobbling are in better agreement with the observations of the surroundings of massive young stellar objects, namely, AFGL 4176 mm1, G17.64+0.16 and G353.273, than our numerical hydrodynamics simulations omitting this physical mechanism. Our work confirms that stellar wobbling is an essential ingredient to account for in numerical simulations of accretion discs of massive protostars.","sentences":["In recent years, it has been demonstrated that massive stars see their infant circumstellar medium shaped into a large, irradiated, gravitationally unstable accretion disc during their early formation phase.","Such discs constitute the gas reservoir in which nascent high-mass stars gain substantial fraction of their mass by episodic accretion of dense gaseous circumstellar clumps.","We aim to evaluate the effects of stellar motion, caused by the disc non-axisymmetric gravitational field, on the disc evolution and its spatial morphology.","In particular, we analyze the disc propensity to gravitational instability and fragmentation, and also disc appearance on synthetic millimeter-band images pertinent to the alma facility.","We employed three-dimensional radiation-hydrodynamical simulations of the surroundings of a young massive star in the non-inertial spherical coordinate system, adopting the highest spatial resolution to date and including the indirect star-disc gravitational potential caused by the asymmetries in the circumstellar disc.","The resulting disc were postprocessed with the radiation transfer tool RADMC-3D and CASA softwear to obtain disc synthetic images.","The redistribution of angular momentum in the system makes the disc smaller and rounder, reduces the number of circumstellar gaseous clumps formed via disc gravitational fragmentation, and prevents the ejection of gaseous clumps from the disc.","The synthetic predictive images at millimeter wavelengths of the accretion disc including stellar wobbling are in better agreement with the observations of the surroundings of massive young stellar objects, namely, AFGL 4176 mm1, G17.64+0.16 and G353.273, than our numerical hydrodynamics simulations omitting this physical mechanism.","Our work confirms that stellar wobbling is an essential ingredient to account for in numerical simulations of accretion discs of massive protostars."],"url":"http://arxiv.org/abs/2405.19905v1","category":"astro-ph.SR"}
{"created":"2024-05-30 10:09:49","title":"Statistical physics of principal minors: Cavity approach","abstract":"Determinants are useful to represent the state of an interacting system of (effectively) repulsive and independent elements, like fermions in a quantum system and training samples in a learning problem. A computationally challenging problem is to compute the sum of powers of principal minors of a matrix which is relevant to the study of critical behaviors in quantum fermionic systems and finding a subset of maximally informative training data for a learning algorithm. Specifically, principal minors of positive square matrices can be considered as statistical weights of a random point process on the set of the matrix indices. The probability of each subset of the indices is in general proportional to a positive power of the determinant of the associated sub-matrix. We use Gaussian representation of the determinants for symmetric and positive matrices to estimate the partition function (or free energy) and the entropy of principal minors within the Bethe approximation. The results are expected to be asymptotically exact for diagonally dominant matrices with locally tree-like structures. We consider the Laplacian matrix of random regular graphs of degree $K=2,3,4$ and exactly characterize the structure of the relevant minors in a mean-field model of such matrices. No (finite-temperature) phase transition is observed in this class of diagonally dominant matrices by increasing the positive power of the principal minors, which here plays the role of an inverse temperature.","sentences":["Determinants are useful to represent the state of an interacting system of (effectively) repulsive and independent elements, like fermions in a quantum system and training samples in a learning problem.","A computationally challenging problem is to compute the sum of powers of principal minors of a matrix which is relevant to the study of critical behaviors in quantum fermionic systems and finding a subset of maximally informative training data for a learning algorithm.","Specifically, principal minors of positive square matrices can be considered as statistical weights of a random point process on the set of the matrix indices.","The probability of each subset of the indices is in general proportional to a positive power of the determinant of the associated sub-matrix.","We use Gaussian representation of the determinants for symmetric and positive matrices to estimate the partition function (or free energy) and the entropy of principal minors within the Bethe approximation.","The results are expected to be asymptotically exact for diagonally dominant matrices with locally tree-like structures.","We consider the Laplacian matrix of random regular graphs of degree $K=2,3,4$ and exactly characterize the structure of the relevant minors in a mean-field model of such matrices.","No (finite-temperature) phase transition is observed in this class of diagonally dominant matrices by increasing the positive power of the principal minors, which here plays the role of an inverse temperature."],"url":"http://arxiv.org/abs/2405.19904v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-30 10:08:17","title":"On a new family of weighted Gaussian processes: an application to bat telemetry data","abstract":"In this article we use a covariance function that arises from limit of fluctuations of the rescaled occupation time process of a branching particle system, to introduce a family of weighted long-range dependence Gaussian processes. In particular, we consider two subfamilies for which we show that the process is not a semimartingale, that the processes exhibit long-range dependence and have long-range memory of logarithmic order. Finally, we illustrate that this family of processes is useful for modeling real world data.","sentences":["In this article we use a covariance function that arises from limit of fluctuations of the rescaled occupation time process of a branching particle system, to introduce a family of weighted long-range dependence Gaussian processes.","In particular, we consider two subfamilies for which we show that the process is not a semimartingale, that the processes exhibit long-range dependence and have long-range memory of logarithmic order.","Finally, we illustrate that this family of processes is useful for modeling real world data."],"url":"http://arxiv.org/abs/2405.19903v1","category":"math.PR"}
{"created":"2024-05-30 09:50:49","title":"Infinite rank module categories over finite dimensional $\\mathfrak{sl}_2$-modules in Lie-algebraic context","abstract":"We study locally finitary realizations of simple transitive module categories of infinite rank over the monoidal category $\\mathscr{C}$ of finite dimensional modules for the complex Lie algebra $\\mathfrak{sl}_2$. Combinatorics of such realizations is governed by six infinite Coxeter diagrams. We show that five of these are realizable in our setup, while one (type $B_\\infty$) is not. We also describe the $\\mathscr{C}$-module subcategories of $\\mathfrak{sl}_2$-mod generated by simple modules as well as the $\\mathscr{C}$-module categories coming from the natural action of $\\mathscr{C}$ on the categories of finite dimensional modules over Lie subalgebras of $\\mathfrak{sl}_2$.","sentences":["We study locally finitary realizations of simple transitive module categories of infinite rank over the monoidal category $\\mathscr{C}$ of finite dimensional modules for the complex Lie algebra $\\mathfrak{sl}_2$. Combinatorics of such realizations is governed by six infinite Coxeter diagrams.","We show that five of these are realizable in our setup, while one (type $B_\\infty$) is not.","We also describe the $\\mathscr{C}$-module subcategories of $\\mathfrak{sl}_2$-mod generated by simple modules as well as the $\\mathscr{C}$-module categories coming from the natural action of $\\mathscr{C}$ on the categories of finite dimensional modules over Lie subalgebras of $\\mathfrak{sl}_2$."],"url":"http://arxiv.org/abs/2405.19894v1","category":"math.RT"}
{"created":"2024-05-30 09:46:59","title":"Deep Joint Semantic Coding and Beamforming for Near-Space Airship-Borne Massive MIMO Network","abstract":"Near-space airship-borne communication network is recognized to be an indispensable component of the future integrated ground-air-space network thanks to airships' advantage of long-term residency at stratospheric altitudes, but it urgently needs reliable and efficient Airship-to-X link. To improve the transmission efficiency and capacity, this paper proposes to integrate semantic communication with massive multiple-input multiple-output (MIMO) technology. Specifically, we propose a deep joint semantic coding and beamforming (JSCBF) scheme for airship-based massive MIMO image transmission network in space, in which semantics from both source and channel are fused to jointly design the semantic coding and physical layer beamforming. First, we design two semantic extraction networks to extract semantics from image source and channel state information, respectively. Then, we propose a semantic fusion network that can fuse these semantics into complex-valued semantic features for subsequent physical-layer transmission. To efficiently transmit the fused semantic features at the physical layer, we then propose the hybrid data and model-driven semantic-aware beamforming networks. At the receiver, a semantic decoding network is designed to reconstruct the transmitted images. Finally, we perform end-to-end deep learning to jointly train all the modules, using the image reconstruction quality at the receivers as a metric. The proposed deep JSCBF scheme fully combines the efficient source compressibility and robust error correction capability of semantic communication with the high spectral efficiency of massive MIMO, achieving a significant performance improvement over existing approaches.","sentences":["Near-space airship-borne communication network is recognized to be an indispensable component of the future integrated ground-air-space network thanks to airships' advantage of long-term residency at stratospheric altitudes, but it urgently needs reliable and efficient Airship-to-X link.","To improve the transmission efficiency and capacity, this paper proposes to integrate semantic communication with massive multiple-input multiple-output (MIMO) technology.","Specifically, we propose a deep joint semantic coding and beamforming (JSCBF) scheme for airship-based massive MIMO image transmission network in space, in which semantics from both source and channel are fused to jointly design the semantic coding and physical layer beamforming.","First, we design two semantic extraction networks to extract semantics from image source and channel state information, respectively.","Then, we propose a semantic fusion network that can fuse these semantics into complex-valued semantic features for subsequent physical-layer transmission.","To efficiently transmit the fused semantic features at the physical layer, we then propose the hybrid data and model-driven semantic-aware beamforming networks.","At the receiver, a semantic decoding network is designed to reconstruct the transmitted images.","Finally, we perform end-to-end deep learning to jointly train all the modules, using the image reconstruction quality at the receivers as a metric.","The proposed deep JSCBF scheme fully combines the efficient source compressibility and robust error correction capability of semantic communication with the high spectral efficiency of massive MIMO, achieving a significant performance improvement over existing approaches."],"url":"http://arxiv.org/abs/2405.19889v1","category":"eess.SP"}
{"created":"2024-05-30 09:46:36","title":"Parrot: Efficient Serving of LLM-based Applications with Semantic Variable","abstract":"The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software. Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task. However, they have to use the over-simplified request-level API provided by today's public LLM services, losing essential application-level information. Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications.   This paper introduces Parrot, an LLM service system that focuses on the end-to-end experience of LLM-based applications. Parrot proposes Semantic Variable, a unified abstraction to expose application-level knowledge to public LLM services. A Semantic Variable annotates an input/output variable in the prompt of a request, and creates the data pipeline when connecting multiple LLM requests, providing a natural way to program LLM applications. Exposing Semantic Variables to the public LLM service allows it to perform conventional data flow analysis to uncover the correlation across multiple LLM requests. This correlation opens a brand-new optimization space for the end-to-end performance of LLM-based applications. Extensive evaluations demonstrate that Parrot can achieve up to an order-of-magnitude improvement for popular and practical use cases of LLM applications.","sentences":["The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software.","Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task.","However, they have to use the over-simplified request-level API provided by today's public LLM services, losing essential application-level information.","Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications.   ","This paper introduces Parrot, an LLM service system that focuses on the end-to-end experience of LLM-based applications.","Parrot proposes Semantic Variable, a unified abstraction to expose application-level knowledge to public LLM services.","A Semantic Variable annotates an input/output variable in the prompt of a request, and creates the data pipeline when connecting multiple LLM requests, providing a natural way to program LLM applications.","Exposing Semantic Variables to the public LLM service allows it to perform conventional data flow analysis to uncover the correlation across multiple LLM requests.","This correlation opens a brand-new optimization space for the end-to-end performance of LLM-based applications.","Extensive evaluations demonstrate that Parrot can achieve up to an order-of-magnitude improvement for popular and practical use cases of LLM applications."],"url":"http://arxiv.org/abs/2405.19888v1","category":"cs.LG"}
{"created":"2024-05-30 09:43:13","title":"Quantum Thermalization via Travelling Waves","abstract":"Isolated quantum many-body systems which thermalize under their own dynamics are expected to act as their own thermal baths, thereby bringing their local subsystems to thermal equilibrium. Here we show that the infinite-dimensional limit of a quantum lattice model, as described by Dynamical Mean-Field theory (DMFT), provides a natural framework to understand this self-consistent thermalization process. Using the Fermi-Hubbard model as working example, we demonstrate that the emergence of a self-consistent bath thermalising the system is characterized by a sharp thermalization front, moving balistically and separating the initial condition from the long-time thermal fixed point. We characterize the full DMFT dynamics through an effective temperature for which we derive a travelling-wave equation of the Fisher-Kolmogorov-Petrovsky-Piskunov (FKPP) type. This equation allows to predict the asymptotic shape of the front and its velocity, which match perfectly the full DMFT numerics. Our results provide a new angle to understand the onset of quantum thermalisation in closed isolated systems.","sentences":["Isolated quantum many-body systems which thermalize under their own dynamics are expected to act as their own thermal baths, thereby bringing their local subsystems to thermal equilibrium.","Here we show that the infinite-dimensional limit of a quantum lattice model, as described by Dynamical Mean-Field theory (DMFT), provides a natural framework to understand this self-consistent thermalization process.","Using the Fermi-Hubbard model as working example, we demonstrate that the emergence of a self-consistent bath thermalising the system is characterized by a sharp thermalization front, moving balistically and separating the initial condition from the long-time thermal fixed point.","We characterize the full DMFT dynamics through an effective temperature for which we derive a travelling-wave equation of the Fisher-Kolmogorov-Petrovsky-Piskunov (FKPP) type.","This equation allows to predict the asymptotic shape of the front and its velocity, which match perfectly the full DMFT numerics.","Our results provide a new angle to understand the onset of quantum thermalisation in closed isolated systems."],"url":"http://arxiv.org/abs/2405.19884v1","category":"cond-mat.str-el"}
{"created":"2024-05-30 09:42:54","title":"From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems","abstract":"In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world. To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively. Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting. Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning. Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret. As a remedy, we introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small. Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors.","sentences":["In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world.","To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively.","Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting.","Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning.","Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret.","As a remedy, we introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small.","Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors."],"url":"http://arxiv.org/abs/2405.19883v1","category":"cs.LG"}
{"created":"2024-05-30 09:41:10","title":"PixOOD: Pixel-Level Out-of-Distribution Detection","abstract":"We propose a dense image prediction out-of-distribution detection algorithm, called PixOOD, which does not require training on samples of anomalous data and is not designed for a specific application which avoids traditional training biases. In order to model the complex intra-class variability of the in-distribution data at the pixel level, we propose an online data condensation algorithm which is more robust than standard K-means and is easily trainable through SGD. We evaluate PixOOD on a wide range of problems. It achieved state-of-the-art results on four out of seven datasets, while being competitive on the rest. The source code is available at https://github.com/vojirt/PixOOD.","sentences":["We propose a dense image prediction out-of-distribution detection algorithm, called PixOOD, which does not require training on samples of anomalous data and is not designed for a specific application which avoids traditional training biases.","In order to model the complex intra-class variability of the in-distribution data at the pixel level, we propose an online data condensation algorithm which is more robust than standard K-means and is easily trainable through SGD.","We evaluate PixOOD on a wide range of problems.","It achieved state-of-the-art results on four out of seven datasets, while being competitive on the rest.","The source code is available at https://github.com/vojirt/PixOOD."],"url":"http://arxiv.org/abs/2405.19882v1","category":"cs.CV"}
{"created":"2024-05-30 09:30:12","title":"Composition operators between Toeplitz kernels","abstract":"Recently, we proved that the image of a Toeplitz kernel of dimension $>1$ under composition by an inner function is nearly $S^*$-invariant if and only if the inner function is an automorphism. In this paper, we build on this work and describe the minimal Toeplitz kernel containing the image of a Toeplitz kernel under a composition operator with an inner symbol. Furthermore, this work is extended to the minimal Toeplitz kernel containing the image of a Toeplitz kernel under a weighted composition operator. In particular, the corresponding cases for minimal model spaces are also formulated, generalizing known work on the action of composition operators on model spaces. Finally, the equivalences between Toeplitz kernels are used to formulate the specific maximal vectors for several Toeplitz kernels with symbols expressed in terms of composition operators and inner functions.","sentences":["Recently, we proved that the image of a Toeplitz kernel of dimension $>1$ under composition by an inner function is nearly $S^*$-invariant if and only if the inner function is an automorphism.","In this paper, we build on this work and describe the minimal Toeplitz kernel containing the image of a Toeplitz kernel under a composition operator with an inner symbol.","Furthermore, this work is extended to the minimal Toeplitz kernel containing the image of a Toeplitz kernel under a weighted composition operator.","In particular, the corresponding cases for minimal model spaces are also formulated, generalizing known work on the action of composition operators on model spaces.","Finally, the equivalences between Toeplitz kernels are used to formulate the specific maximal vectors for several Toeplitz kernels with symbols expressed in terms of composition operators and inner functions."],"url":"http://arxiv.org/abs/2405.19875v1","category":"math.FA"}
{"created":"2024-05-30 09:27:51","title":"Computing Elastic Tensors of Amorphous Materials from First-Principles","abstract":"Advancements in modern semiconductor devices increasingly depend on the utilization of amorphous materials and the reduction of material thickness, pushing the boundaries of their physical capabilities. The mechanical properties of these thin layers are critical in determining both the operational efficacy and mechanical integrity of these devices. Unlike bulk crystalline materials, whose calculation techniques are well-established, amorphous materials present a challenge due to the significant variation in atomic topology and their non-affine transformations under external strain. This study introduces a novel method for computing the elastic tensor of amorphous materials, applicable to both bulk and ultra-thin films in the linear elastic regime using Density Functional Theory. We exemplify this method with a-SiO2, a commonly used dielectric. Our approach accounts for the structural disorder inherent in amorphous systems, which, while contributing to remarkable material properties, complicates traditional elastic tensor computation. We propose a solution involving the inability of atomic positions to relax under internal relaxation, near the boundaries of the computational unit cell, ensuring the affine transformations necessary for linear elasticity. This method's efficacy is demonstrated through its alignment with classical Young's modulus measurements, and has potential for broad application in fields such as Technology Computer Aided Design and stress analysis via Raman spectra. The revised technique for assessing the mechanical properties of amorphous materials opens new avenues for exploring their impact on device reliability and functionality.","sentences":["Advancements in modern semiconductor devices increasingly depend on the utilization of amorphous materials and the reduction of material thickness, pushing the boundaries of their physical capabilities.","The mechanical properties of these thin layers are critical in determining both the operational efficacy and mechanical integrity of these devices.","Unlike bulk crystalline materials, whose calculation techniques are well-established, amorphous materials present a challenge due to the significant variation in atomic topology and their non-affine transformations under external strain.","This study introduces a novel method for computing the elastic tensor of amorphous materials, applicable to both bulk and ultra-thin films in the linear elastic regime using Density Functional Theory.","We exemplify this method with a-SiO2, a commonly used dielectric.","Our approach accounts for the structural disorder inherent in amorphous systems, which, while contributing to remarkable material properties, complicates traditional elastic tensor computation.","We propose a solution involving the inability of atomic positions to relax under internal relaxation, near the boundaries of the computational unit cell, ensuring the affine transformations necessary for linear elasticity.","This method's efficacy is demonstrated through its alignment with classical Young's modulus measurements, and has potential for broad application in fields such as Technology Computer Aided Design and stress analysis via Raman spectra.","The revised technique for assessing the mechanical properties of amorphous materials opens new avenues for exploring their impact on device reliability and functionality."],"url":"http://arxiv.org/abs/2405.19873v1","category":"physics.app-ph"}
{"created":"2024-05-30 09:23:48","title":"On Vessel Location Forecasting and the Effect of Federated Learning","abstract":"The wide spread of Automatic Identification System (AIS) has motivated several maritime analytics operations. Vessel Location Forecasting (VLF) is one of the most critical operations for maritime awareness. However, accurate VLF is a challenging problem due to the complexity and dynamic nature of maritime traffic conditions. Furthermore, as privacy concerns and restrictions have grown, training data has become increasingly fragmented, resulting in dispersed databases of several isolated data silos among different organizations, which in turn decreases the quality of learning models. In this paper, we propose an efficient VLF solution based on LSTM neural networks, in two variants, namely Nautilus and FedNautilus for the centralized and the federated learning approach, respectively. We also demonstrate the superiority of the centralized approach with respect to current state of the art and discuss the advantages and disadvantages of the federated against the centralized approach.","sentences":["The wide spread of Automatic Identification System (AIS) has motivated several maritime analytics operations.","Vessel Location Forecasting (VLF) is one of the most critical operations for maritime awareness.","However, accurate VLF is a challenging problem due to the complexity and dynamic nature of maritime traffic conditions.","Furthermore, as privacy concerns and restrictions have grown, training data has become increasingly fragmented, resulting in dispersed databases of several isolated data silos among different organizations, which in turn decreases the quality of learning models.","In this paper, we propose an efficient VLF solution based on LSTM neural networks, in two variants, namely Nautilus and FedNautilus for the centralized and the federated learning approach, respectively.","We also demonstrate the superiority of the centralized approach with respect to current state of the art and discuss the advantages and disadvantages of the federated against the centralized approach."],"url":"http://arxiv.org/abs/2405.19870v1","category":"cs.LG"}
{"created":"2024-05-30 09:22:31","title":"Semantic Landmark Detection & Classification Using Neural Networks For 3D In-Air Sonar","abstract":"In challenging environments where traditional sensing modalities struggle, in-air sonar offers resilience to optical interference. Placing a priori known landmarks in these environments can eliminate accumulated errors in autonomous mobile systems such as Simultaneous Localization and Mapping (SLAM) and autonomous navigation. We present a novel approach using a convolutional neural network to detect and classify ten different reflector landmarks with varying radii using in-air 3D sonar. Additionally, the network predicts the orientation angle of the detected landmarks. The neural network is trained on cochleograms, representing echoes received by the sensor in a time-frequency domain. Experimental results in cluttered indoor settings show promising performance. The CNN achieves a 97.3% classification accuracy on the test dataset, accurately detecting both the presence and absence of landmarks. Moreover, the network predicts landmark orientation angles with an RMSE lower than 10 degrees, enhancing the utility in SLAM and autonomous navigation applications. This advancement improves the robustness and accuracy of autonomous systems in challenging environments.","sentences":["In challenging environments where traditional sensing modalities struggle, in-air sonar offers resilience to optical interference.","Placing a priori known landmarks in these environments can eliminate accumulated errors in autonomous mobile systems such as Simultaneous Localization and Mapping (SLAM) and autonomous navigation.","We present a novel approach using a convolutional neural network to detect and classify ten different reflector landmarks with varying radii using in-air 3D sonar.","Additionally, the network predicts the orientation angle of the detected landmarks.","The neural network is trained on cochleograms, representing echoes received by the sensor in a time-frequency domain.","Experimental results in cluttered indoor settings show promising performance.","The CNN achieves a 97.3% classification accuracy on the test dataset, accurately detecting both the presence and absence of landmarks.","Moreover, the network predicts landmark orientation angles with an RMSE lower than 10 degrees, enhancing the utility in SLAM and autonomous navigation applications.","This advancement improves the robustness and accuracy of autonomous systems in challenging environments."],"url":"http://arxiv.org/abs/2405.19869v1","category":"cs.RO"}
{"created":"2024-05-30 09:17:56","title":"Subgroups of word hyperbolic groups in dimension 2 over arbitrary rings","abstract":"In 1996, Gersten proved that finitely presented subgroups of a word hyperbolic group of integral cohomological dimension 2 are hyperbolic. We use isoperimetric inequalities over arbitrary rings to extend this result to any ring. In particular, we study the discrete isoperimetric function and show that its linearity is equivalent to hyperbolicity, which is also equivalent to it being subquadratic. We further use these ideas to obtain conditions for subgroups of higher rank hyperbolic groups to be again higher rank hyperbolic of the same rank. The appendix discusses the equivalence between isoperimetric inequalities and coning inequalities in the simplicial setting and the general setting, leading to combinatorial definitions of higher rank hyperbolicity in the setting of simplicial complexes and allowing us to give elementary definitions of higher rank hyperbolic groups.","sentences":["In 1996, Gersten proved that finitely presented subgroups of a word hyperbolic group of integral cohomological dimension 2 are hyperbolic.","We use isoperimetric inequalities over arbitrary rings to extend this result to any ring.","In particular, we study the discrete isoperimetric function and show that its linearity is equivalent to hyperbolicity, which is also equivalent to it being subquadratic.","We further use these ideas to obtain conditions for subgroups of higher rank hyperbolic groups to be again higher rank hyperbolic of the same rank.","The appendix discusses the equivalence between isoperimetric inequalities and coning inequalities in the simplicial setting and the general setting, leading to combinatorial definitions of higher rank hyperbolicity in the setting of simplicial complexes and allowing us to give elementary definitions of higher rank hyperbolic groups."],"url":"http://arxiv.org/abs/2405.19866v1","category":"math.GR"}
{"created":"2024-05-30 09:12:53","title":"Katsura's self-similar groupoid actions, Putnam's binary factors, and their limit spaces","abstract":"We show that the dynamical system associated by Putnam to a pair of graph embeddings is identical to the shift map on the limit space of a self-similar groupoid action on a graph. Moreover, performing a certain out-split on said graph gives rise to a Katsura groupoid action on the out-split graph whose associated limit space dynamical system is conjugate to the previous one. We characterise the self-similar properties of these groupoids in terms of properties of their defining data, two matrices $A$, $B$. We prove a large class of the associated limit spaces are bundles of circles and points which fibre over a totally disconnected space, and the dynamics restricted to each circle is of the form $z\\to z^{n}$. Moreover, we find a planar embedding of these spaces, thereby answering a question Putnam posed in his paper.","sentences":["We show that the dynamical system associated by Putnam to a pair of graph embeddings is identical to the shift map on the limit space of a self-similar groupoid action on a graph.","Moreover, performing a certain out-split on said graph gives rise to a Katsura groupoid action on the out-split graph whose associated limit space dynamical system is conjugate to the previous one.","We characterise the self-similar properties of these groupoids in terms of properties of their defining data, two matrices $A$, $B$. We prove a large class of the associated limit spaces are bundles of circles and points which fibre over a totally disconnected space, and the dynamics restricted to each circle is of the form $z\\to z^{n}$.","Moreover, we find a planar embedding of these spaces, thereby answering a question Putnam posed in his paper."],"url":"http://arxiv.org/abs/2405.19863v1","category":"math.DS"}
{"created":"2024-05-30 09:05:55","title":"A frustrated antipolar phase analogous to classical spin liquids","abstract":"The study of magnetic frustration in classical spin systems was motivated by the prediction and discovery of classical spin liquid states. These uncommon magnetic phases are characterized by a massive degeneracy of their ground state implying a finite magnetic entropy at zero temperature. While the classical spin liquid state was originally predicted in the Ising triangular lattice antiferromagnet in 1950, this state has never been experimentally observed in any triangular magnets. We report here the discovery of an electric analogue of classical spin liquids on a triangular lattice of uniaxial electric dipoles in EuAl$_{12}$O$_{19}$. This new type of frustrated antipolar phase is characterized by a highly degenerate state at low temperature implying an absence of long-range antiferroelectric order, despite short range antipolar correlations. Its dynamics are governed by a thermally activated process, slowing down upon cooling towards a complete freezing at zero temperature.","sentences":["The study of magnetic frustration in classical spin systems was motivated by the prediction and discovery of classical spin liquid states.","These uncommon magnetic phases are characterized by a massive degeneracy of their ground state implying a finite magnetic entropy at zero temperature.","While the classical spin liquid state was originally predicted in the Ising triangular lattice antiferromagnet in 1950, this state has never been experimentally observed in any triangular magnets.","We report here the discovery of an electric analogue of classical spin liquids on a triangular lattice of uniaxial electric dipoles in EuAl$_{12}$O$_{19}$. This new type of frustrated antipolar phase is characterized by a highly degenerate state at low temperature implying an absence of long-range antiferroelectric order, despite short range antipolar correlations.","Its dynamics are governed by a thermally activated process, slowing down upon cooling towards a complete freezing at zero temperature."],"url":"http://arxiv.org/abs/2405.19859v1","category":"cond-mat.str-el"}
{"created":"2024-05-30 09:05:06","title":"Position Error Bound for Cooperative Sensing in MIMO-OFDM Networks","abstract":"Only the chairs can edit This paper investigates the fundamental limits of target position estimation accuracy of joint sensing and communication (JSC) networks comprising several monostatic base stations (BSs) that cooperate to localize targets. Specifically, each BS adopts a multiple-input multiple-output (MIMO)-orthogonal frequency division multiplexing (OFDM) scheme with a multi-beam radiation pattern to partition power between communication and sensing tasks. Building on prior works, we derive a general framework to evaluate the positioning accuracy of a target in networks with an arbitrary number of cooperating BSs and arbitrary geometrical configurations using Fisher information. Numerical results demonstrate the benefits of cooperation between BSs in improving target localization accuracy and provide insights into the relationships between various system parameters, which may aid in designing JSC networks.","sentences":["Only the chairs can edit This paper investigates the fundamental limits of target position estimation accuracy of joint sensing and communication (JSC) networks comprising several monostatic base stations (BSs) that cooperate to localize targets.","Specifically, each BS adopts a multiple-input multiple-output (MIMO)-orthogonal frequency division multiplexing (OFDM) scheme with a multi-beam radiation pattern to partition power between communication and sensing tasks.","Building on prior works, we derive a general framework to evaluate the positioning accuracy of a target in networks with an arbitrary number of cooperating BSs and arbitrary geometrical configurations using Fisher information.","Numerical results demonstrate the benefits of cooperation between BSs in improving target localization accuracy and provide insights into the relationships between various system parameters, which may aid in designing JSC networks."],"url":"http://arxiv.org/abs/2405.19858v1","category":"eess.SP"}
{"created":"2024-05-30 09:04:40","title":"Biodiversity data standards for the organization and dissemination of complex research projects and digital twins: a guide","abstract":"Biodiversity data are substantially increasing, spurred by technological advances and community (citizen) science initiatives. To integrate data is, likewise, becoming more commonplace. Open science promotes open sharing and data usage. Data standardization is an instrument for the organization and integration of biodiversity data, which is required for complex research projects and digital twins. However, just like with an actual instrument, there is a learning curve to understanding the data standards field. Here we provide a guide, for data providers and data users, on the logistics of compiling and utilizing biodiversity data. We emphasize data standards, because they are integral to data integration. Three primary avenues for compiling biodiversity data are compared, explaining the importance of research infrastructures for coordinated long-term data aggregation. We exemplify the Biodiversity Digital Twin (BioDT) as a case study. Four approaches to data standardization are presented in terms of the balance between practical constraints and the advancement of the data standards field. We aim for this paper to guide and raise awareness of the existing issues related to data standardization, and especially how data standards are key to data interoperability, i.e., machine accessibility. The future is promising for computational biodiversity advancements, such as with the BioDT project, but it rests upon the shoulders of machine actionability and readability, and that requires data standards for computational communication.","sentences":["Biodiversity data are substantially increasing, spurred by technological advances and community (citizen) science initiatives.","To integrate data is, likewise, becoming more commonplace.","Open science promotes open sharing and data usage.","Data standardization is an instrument for the organization and integration of biodiversity data, which is required for complex research projects and digital twins.","However, just like with an actual instrument, there is a learning curve to understanding the data standards field.","Here we provide a guide, for data providers and data users, on the logistics of compiling and utilizing biodiversity data.","We emphasize data standards, because they are integral to data integration.","Three primary avenues for compiling biodiversity data are compared, explaining the importance of research infrastructures for coordinated long-term data aggregation.","We exemplify the Biodiversity Digital Twin (BioDT) as a case study.","Four approaches to data standardization are presented in terms of the balance between practical constraints and the advancement of the data standards field.","We aim for this paper to guide and raise awareness of the existing issues related to data standardization, and especially how data standards are key to data interoperability, i.e., machine accessibility.","The future is promising for computational biodiversity advancements, such as with the BioDT project, but it rests upon the shoulders of machine actionability and readability, and that requires data standards for computational communication."],"url":"http://arxiv.org/abs/2405.19857v1","category":"q-bio.OT"}
{"created":"2024-05-30 09:00:59","title":"On a problem of Pavlovi\u0107 involving harmonic quasiconformal mappings","abstract":"We obtain a sharp result on order of certain affine and linear invariant families of harmonic quasiconformal mappings with bounded Schwarzian norm. This problem is motivated by the work of Chuaqui, Hern\\'{a}ndez and Mart\\'{i}n [Math. Ann. 367: 1099--1122, 2017]. Firstly, for $K\\ge1$, we construct a harmonic $K$-quasiconformal counterpart of the classical Koebe function and use it to formulate the corresponding conjectures. Then we consider Hardy spaces $H^p$ of harmonic quasiconformal mappings by applying results for quasiconformal mappings obtained by Astala and Koskela [Pure Appl. Math. Q. 7: 19--50, 2011]. In particular, we determine the optimal order of the family of harmonic quasiconformal mappings with bounded Schwarzian norm to belong to a harmonic Hardy space. This partially solves an open problem posed by Pavlovi\\'{c} in 2014. Finally, we derive pre-Schwarzian and Schwarzian norm estimates of certain harmonic mappings.","sentences":["We obtain a sharp result on order of certain affine and linear invariant families of harmonic quasiconformal mappings with bounded Schwarzian norm.","This problem is motivated by the work of Chuaqui, Hern\\'{a}ndez and Mart\\'{i}n","[Math. Ann. 367: 1099--1122, 2017].","Firstly, for $K\\ge1$, we construct a harmonic $K$-quasiconformal counterpart of the classical Koebe function and use it to formulate the corresponding conjectures.","Then we consider Hardy spaces $H^p$ of harmonic quasiconformal mappings by applying results for quasiconformal mappings obtained by Astala and Koskela","[Pure Appl.","Math.","Q. 7: 19--50, 2011].","In particular, we determine the optimal order of the family of harmonic quasiconformal mappings with bounded Schwarzian norm to belong to a harmonic Hardy space.","This partially solves an open problem posed by Pavlovi\\'{c} in 2014.","Finally, we derive pre-Schwarzian and Schwarzian norm estimates of certain harmonic mappings."],"url":"http://arxiv.org/abs/2405.19852v1","category":"math.CV"}
{"created":"2024-05-30 08:58:18","title":"Guardians of DNS Integrity: A Remote Method for Identifying DNSSEC Validators Across the Internet","abstract":"DNS Security Extensions (DNSSEC) provide the most effective way to fight DNS cache poisoning attacks. Yet, very few DNS resolvers perform DNSSEC validation. Identifying such systems is non-trivial and the existing methods are not suitable for Internet-scale measurements. In this paper, we propose a novel remote technique for identifying DNSSEC-validating resolvers. The proposed method consists of two steps. In the first step, we identify open resolvers by scanning 3.1 billion end hosts and request every non-forwarder to resolve one correct and seven deliberately misconfigured domains. We then build a classifier that discriminates validators from non-validators based on query patterns and DNS response codes. We find that while most open resolvers are DNSSEC-enabled, less than 18% in IPv4 (38% in IPv6) validate received responses. In the second step, we remotely identify closed non-forwarders in networks that do not have inbound Source Address Validation (SAV) in place. Using the classifier built in step one, we identify 37.4% IPv4 (42.9% IPv6) closed DNSSEC validators and cross-validate the results using RIPE Atlas probes. Finally, we show that the discovered (non)-validators actively send requests to DNS root servers, suggesting that we deal with operational recursive resolvers rather than misconfigured machines.","sentences":["DNS Security Extensions (DNSSEC) provide the most effective way to fight DNS cache poisoning attacks.","Yet, very few DNS resolvers perform DNSSEC validation.","Identifying such systems is non-trivial and the existing methods are not suitable for Internet-scale measurements.","In this paper, we propose a novel remote technique for identifying DNSSEC-validating resolvers.","The proposed method consists of two steps.","In the first step, we identify open resolvers by scanning 3.1 billion end hosts and request every non-forwarder to resolve one correct and seven deliberately misconfigured domains.","We then build a classifier that discriminates validators from non-validators based on query patterns and DNS response codes.","We find that while most open resolvers are DNSSEC-enabled, less than 18% in IPv4 (38% in IPv6) validate received responses.","In the second step, we remotely identify closed non-forwarders in networks that do not have inbound Source Address Validation (SAV) in place.","Using the classifier built in step one, we identify 37.4% IPv4 (42.9% IPv6) closed DNSSEC validators and cross-validate the results using RIPE Atlas probes.","Finally, we show that the discovered (non)-validators actively send requests to DNS root servers, suggesting that we deal with operational recursive resolvers rather than misconfigured machines."],"url":"http://arxiv.org/abs/2405.19851v1","category":"cs.CR"}
{"created":"2024-05-30 08:54:29","title":"Some remarks on Brauer Classes of K3-type","abstract":"An element in the Brauer group of a general complex projective $K3$ surface $S$ defines a sublattice of the transcendental lattice of $S$. We consider those elements of prime order for which this sublattice is Hodge-isometric to the transcendental lattice of another K3 surface $X$. We recall that this defines a finite map between moduli spaces of polarized K3 surfaces and we compute its degree. We show how the Picard lattice of $X$ determines the Picard lattice of $S$ in the case that the Picard number of $X$ is two.","sentences":["An element in the Brauer group of a general complex projective $K3$ surface $S$ defines a sublattice of the transcendental lattice of $S$. We consider those elements of prime order for which this sublattice is Hodge-isometric to the transcendental lattice of another K3 surface $X$. We recall that this defines a finite map between moduli spaces of polarized K3 surfaces and we compute its degree.","We show how the Picard lattice of $X$ determines the Picard lattice of $S$ in the case that the Picard number of $X$ is two."],"url":"http://arxiv.org/abs/2405.19848v1","category":"math.AG"}
{"created":"2024-05-30 08:50:53","title":"Assessing the impact of weather-induced uncertainties in large-scale electricity systems","abstract":"The future energy system will largely depend on volatile renewable energy sources and temperature-dependent loads, which makes the weather a central influencing factor. This article presents a novel approach for simulating weather scenarios for robust large-scale power system analysis. By applying different signal analysis methods, historical weather data is decomposed into its spectral components, processed appropriately, and then used to generate random, self-consistent weather data. In this process, any weather parameters of different locations can be considered, while their respective dependencies are mapped. The added value is demonstrated by coupling with a state-of-the-art large-scale energy system model for Europe. It is shown that the integrated consideration of different weather influences allows a quantification of the range of fluctuation of various parameters - such as the feed-in of wind and solar power - and thus provides the basis for future resilient grid planning approaches.","sentences":["The future energy system will largely depend on volatile renewable energy sources and temperature-dependent loads, which makes the weather a central influencing factor.","This article presents a novel approach for simulating weather scenarios for robust large-scale power system analysis.","By applying different signal analysis methods, historical weather data is decomposed into its spectral components, processed appropriately, and then used to generate random, self-consistent weather data.","In this process, any weather parameters of different locations can be considered, while their respective dependencies are mapped.","The added value is demonstrated by coupling with a state-of-the-art large-scale energy system model for Europe.","It is shown that the integrated consideration of different weather influences allows a quantification of the range of fluctuation of various parameters - such as the feed-in of wind and solar power - and thus provides the basis for future resilient grid planning approaches."],"url":"http://arxiv.org/abs/2405.19845v1","category":"eess.SY"}
{"created":"2024-05-30 08:49:32","title":"The First Photometric Analysis of Two Low Mass Ratio Contact Binary Systems In TESS Survey","abstract":"Low mass-ratio (q) contact binary systems are progenitors of stellar mergers such as blue straggles (BS) or fast-rotating FK Com stars. In this study, we present the first light curve analysis of two newly identified low mass-ratio contact binary systems, TIC 55007847 and TIC 63597006, that are identified from TESS. Both stars are classified as A-subtype contact binaries. We obtained the precise orbit periods for the two objects by using the O-C method, i.e. P=0.6117108 d for TIC 55007847 and P=0.7008995 d for TIC 63597006, respectively, and found an obvious periodic signal in the O-C curve of TIC 63597006. We suggest that the periodic signal comes from a third body. We further use the Markov Chain Monte Carlo (MCMC) method with PHOEBE to derive the photometric solutions for the two binaries. The photometric solution for this object shows that the contribution of the third body is about 6%. Our analysis revealed that TIC 55007847 has an extremely low mass ratio of q=0.08. By calculating the ratio of spin angular momentum to the orbital angular momentum Js/Jo, we found that TIC 55007847 is very close to the instability threshold with Js/Jo = 0.31, indicating that it may merge into a single, fast-rotating star in the future. For TIC 63597006, q=0.14 and Js/Jo=0.15. This object is in a relatively stable evolutionary status at present.","sentences":["Low mass-ratio (q) contact binary systems are progenitors of stellar mergers such as blue straggles (BS) or fast-rotating FK Com stars.","In this study, we present the first light curve analysis of two newly identified low mass-ratio contact binary systems, TIC 55007847 and TIC 63597006, that are identified from TESS.","Both stars are classified as A-subtype contact binaries.","We obtained the precise orbit periods for the two objects by using the O-C method, i.e. P=0.6117108 d for TIC 55007847 and P=0.7008995 d for TIC 63597006, respectively, and found an obvious periodic signal in the O-C curve of TIC 63597006.","We suggest that the periodic signal comes from a third body.","We further use the Markov Chain Monte Carlo (MCMC) method with PHOEBE to derive the photometric solutions for the two binaries.","The photometric solution for this object shows that the contribution of the third body is about 6%.","Our analysis revealed that TIC 55007847 has an extremely low mass ratio of q=0.08.","By calculating the ratio of spin angular momentum to the orbital angular momentum Js/Jo, we found that TIC 55007847 is very close to the instability threshold with Js/Jo = 0.31, indicating that it may merge into a single, fast-rotating star in the future.","For TIC 63597006, q=0.14 and Js/Jo=0.15.","This object is in a relatively stable evolutionary status at present."],"url":"http://arxiv.org/abs/2405.19841v1","category":"astro-ph.SR"}
{"created":"2024-05-30 08:49:06","title":"VELOcities of CEpheids (VELOCE) II. Systematic Search for Spectroscopic Binary Cepheids","abstract":"Classical Cepheids provide valuable insights into the evolution of stellar multiplicity among intermediate-mass stars. Here, we present a systematic investigation of single-lined spectroscopic binaries (SB1) based on high-precision velocities measured by the VELOcities of CEpheids (VELOCE) project. We detected 76 (29%) SB1 systems among the 258 Milky Way Cepheids in the first VELOCE data release, 32 (43%) of which were not previously known to be SB1 systems. We determined 30 precise and 3 tentative orbital solutions, 18 (53%) of which are reported for the first time. This large set of Cepheid orbits provides a detailed view of the eccentricity e and orbital period Porb distribution among evolved intermediate-mass stars, ranging from e=[0.0, 0.8] and Porb=[240, 9 000] d. Orbital motion on timescales exceeding the 11 yr VELOCE baseline was investigated using a template fitting technique applied to literature data. Particularly interesting objects include a) R Cru, the Cepheid with the shortest orbital period in the Milky Way (240 d), b) ASAS J103158-5814.7, a short-period overtone Cepheid exhibiting time-dependent pulsation amplitudes as well as orbital motion, c) 17 triple systems with outer visual companions, among other interesting objects. Most VELOCE Cepheids (21/23) that exhibit evidence for a companion based on Gaia proper motion anomaly are also spectroscopic binaries, whereas the remaining do not exhibit significant (> 3-sigma) orbital RV variations. Gaia quality flags, notably the Renormalized Unit Weight Error (RUWE), do not allow to reliably identify Cepheid binaries, although statistically the average RUWE of SB1 Cepheids is slightly higher than that of non-SB1 Cepheids. Comparison with Gaia photometric amplitudes in G, Bp, and Rp also does not allow to identify spectroscopic binaries among the full VELOCE sample.","sentences":["Classical Cepheids provide valuable insights into the evolution of stellar multiplicity among intermediate-mass stars.","Here, we present a systematic investigation of single-lined spectroscopic binaries (SB1) based on high-precision velocities measured by the VELOcities of CEpheids (VELOCE) project.","We detected 76 (29%) SB1 systems among the 258 Milky Way Cepheids in the first VELOCE data release, 32 (43%) of which were not previously known to be SB1 systems.","We determined 30 precise and 3 tentative orbital solutions, 18 (53%) of which are reported for the first time.","This large set of Cepheid orbits provides a detailed view of the eccentricity e and orbital period Porb distribution among evolved intermediate-mass stars, ranging from e=[0.0, 0.8] and Porb=[240, 9 000] d. Orbital motion on timescales exceeding the 11 yr VELOCE baseline was investigated using a template fitting technique applied to literature data.","Particularly interesting objects include a) R Cru, the Cepheid with the shortest orbital period in the Milky Way (240 d), b) ASAS J103158-5814.7, a short-period overtone Cepheid exhibiting time-dependent pulsation amplitudes as well as orbital motion, c) 17 triple systems with outer visual companions, among other interesting objects.","Most VELOCE Cepheids (21/23) that exhibit evidence for a companion based on Gaia proper motion anomaly are also spectroscopic binaries, whereas the remaining do not exhibit significant (> 3-sigma) orbital RV variations.","Gaia quality flags, notably the Renormalized Unit Weight Error (RUWE), do not allow to reliably identify Cepheid binaries, although statistically the average RUWE of SB1 Cepheids is slightly higher than that of non-SB1 Cepheids.","Comparison with Gaia photometric amplitudes in G, Bp, and Rp also does not allow to identify spectroscopic binaries among the full VELOCE sample."],"url":"http://arxiv.org/abs/2405.19840v1","category":"astro-ph.SR"}
{"created":"2024-05-30 08:45:45","title":"The Merit of River Network Topology for Neural Flood Forecasting","abstract":"Climate change exacerbates riverine floods, which occur with higher frequency and intensity than ever. The much-needed forecasting systems typically rely on accurate river discharge predictions. To this end, the SOTA data-driven approaches treat forecasting at spatially distributed gauge stations as isolated problems, even within the same river network. However, incorporating the known topology of the river network into the prediction model has the potential to leverage the adjacency relationship between gauges. Thus, we model river discharge for a network of gauging stations with GNNs and compare the forecasting performance achieved by different adjacency definitions. Our results show that the model fails to benefit from the river network topology information, both on the entire network and small subgraphs. The learned edge weights correlate with neither of the static definitions and exhibit no regular pattern. Furthermore, the GNNs struggle to predict sudden, narrow discharge spikes. Our work hints at a more general underlying phenomenon of neural prediction not always benefitting from graphical structure and may inspire a systematic study of the conditions under which this happens.","sentences":["Climate change exacerbates riverine floods, which occur with higher frequency and intensity than ever.","The much-needed forecasting systems typically rely on accurate river discharge predictions.","To this end, the SOTA data-driven approaches treat forecasting at spatially distributed gauge stations as isolated problems, even within the same river network.","However, incorporating the known topology of the river network into the prediction model has the potential to leverage the adjacency relationship between gauges.","Thus, we model river discharge for a network of gauging stations with GNNs and compare the forecasting performance achieved by different adjacency definitions.","Our results show that the model fails to benefit from the river network topology information, both on the entire network and small subgraphs.","The learned edge weights correlate with neither of the static definitions and exhibit no regular pattern.","Furthermore, the GNNs struggle to predict sudden, narrow discharge spikes.","Our work hints at a more general underlying phenomenon of neural prediction not always benefitting from graphical structure and may inspire a systematic study of the conditions under which this happens."],"url":"http://arxiv.org/abs/2405.19836v1","category":"cs.LG"}
{"created":"2024-05-30 08:35:46","title":"The variable radio jet of the accreting neutron star the Rapid Burster","abstract":"The Rapid Burster is a unique neutron star low-mass X-ray binary system, showing both thermonuclear Type-I and accretion-driven Type-II X-ray bursts. Recent studies have demonstrated how coordinated observations of X-ray and radio variability can constrain jet properties of accreting neutron stars - particularly when the X-ray variability is dominated by discrete changes. We present a simultaneous VLA, Swift, and INTEGRAL observing campaign of the Rapid Burster to investigate whether its jet responds to Type-II bursts. We observe the radio counterpart of the X-ray binary at its faintest-detected radio luminosity, while the X-ray observations reveal prolific, fast X-ray bursting. A time-resolved analysis reveals that the radio counterpart varies significantly between observing scans, displaying a fractional variability of $38 \\pm 5$%. The radio faintness of the system prevents the robust identification of a causal relation between individual Type-II bursts and the evolution of the radio jet. However, based on a comparison of its low radio luminosity with archival Rapid Burster observations and other accreting neutron stars, and on a qualitative assessment of the X-ray and radio light curves, we explore the presence of a tentative connection between bursts and jet: i.e., the Type-II bursts may weaken or strengthen the jet. The former of those two scenarios would fit with magneto-rotational jet models; we discuss three lines of future research to establish this potential relation between Type-II bursts and jets more confidently.","sentences":["The Rapid Burster is a unique neutron star low-mass X-ray binary system, showing both thermonuclear Type-I and accretion-driven Type-II X-ray bursts.","Recent studies have demonstrated how coordinated observations of X-ray and radio variability can constrain jet properties of accreting neutron stars - particularly when the X-ray variability is dominated by discrete changes.","We present a simultaneous VLA, Swift, and INTEGRAL observing campaign of the Rapid Burster to investigate whether its jet responds to Type-II bursts.","We observe the radio counterpart of the X-ray binary at its faintest-detected radio luminosity, while the X-ray observations reveal prolific, fast X-ray bursting.","A time-resolved analysis reveals that the radio counterpart varies significantly between observing scans, displaying a fractional variability of $38 \\pm 5$%.","The radio faintness of the system prevents the robust identification of a causal relation between individual Type-II bursts and the evolution of the radio jet.","However, based on a comparison of its low radio luminosity with archival Rapid Burster observations and other accreting neutron stars, and on a qualitative assessment of the X-ray and radio light curves, we explore the presence of a tentative connection between bursts and jet: i.e., the Type-II bursts may weaken or strengthen the jet.","The former of those two scenarios would fit with magneto-rotational jet models; we discuss three lines of future research to establish this potential relation between Type-II bursts and jets more confidently."],"url":"http://arxiv.org/abs/2405.19827v1","category":"astro-ph.HE"}
{"created":"2024-05-30 08:34:12","title":"Measurement of the production and elliptic flow of (anti)nuclei in Xe-Xe collisions at $\\sqrt{s_{\\rm NN}}$ = 5.44 TeV","abstract":"Measurements of (anti)deuteron and (anti)$^3$He production in the rapidity range $ |y| < $ 0.5 as a function of the transverse momentum and event multiplicity in Xe$-$Xe collisions at a center-of-mass energy per nucleon$-$nucleon pair of $\\sqrt{s_{\\rm NN}}$ = 5.44 TeV are presented. The coalescence parameters $B_2$ and $B_3$ are measured as a function of the transverse momentum per nucleon. The ratios between (anti)deuteron and (anti)$^3$He yields and those of (anti)protons and pions are reported as a function of the mean charged-particle multiplicity density, and compared with two implementations of the statistical hadronization model (SHM) and with coalescence predictions. The elliptic flow of (anti)deuterons is measured for the first time in Xe$-$Xe collisions and shows features similar to those already observed in Pb$-$Pb collisions, i.e., the mass ordering at low transverse momentum and the meson$-$baryon grouping at intermediate transverse momentum. The production of nuclei is particularly sensitive to the chemical freeze-out temperature of the system created in the collision, which is extracted from a grand-canonical-ensemble-based thermal fit, performed for the first time including light nuclei along with light-flavor hadrons in Xe$-$Xe collisions. The extracted chemical freeze-out temperature $T_{\\rm chem}$ = (154.2 $\\pm$ 1.1) MeV in Xe$-$Xe collisions is similar to that observed in Pb$-$Pb collisions and close to the crossover temperature predicted by lattice QCD calculations.","sentences":["Measurements of (anti)deuteron and (anti)$^3$He production in the rapidity range $ |y| < $ 0.5 as a function of the transverse momentum and event multiplicity in Xe$-$Xe collisions at a center-of-mass energy per nucleon$-$nucleon pair of $\\sqrt{s_{\\rm NN}}$ = 5.44 TeV are presented.","The coalescence parameters $B_2$ and $B_3$ are measured as a function of the transverse momentum per nucleon.","The ratios between (anti)deuteron and (anti)$^3$He yields and those of (anti)protons and pions are reported as a function of the mean charged-particle multiplicity density, and compared with two implementations of the statistical hadronization model (SHM) and with coalescence predictions.","The elliptic flow of (anti)deuterons is measured for the first time in Xe$-$Xe collisions and shows features similar to those already observed in Pb$-$Pb collisions, i.e., the mass ordering at low transverse momentum and the meson$-$baryon grouping at intermediate transverse momentum.","The production of nuclei is particularly sensitive to the chemical freeze-out temperature of the system created in the collision, which is extracted from a grand-canonical-ensemble-based thermal fit, performed for the first time including light nuclei along with light-flavor hadrons in Xe$-$Xe collisions.","The extracted chemical freeze-out temperature $T_{\\rm chem}$ = (154.2 $\\pm$ 1.1) MeV in Xe$-$Xe collisions is similar to that observed in Pb$-$Pb collisions and close to the crossover temperature predicted by lattice QCD calculations."],"url":"http://arxiv.org/abs/2405.19826v1","category":"nucl-ex"}
{"created":"2024-05-30 08:24:00","title":"Performance Examination of Symbolic Aggregate Approximation in IoT Applications","abstract":"Symbolic Aggregate approXimation (SAX) is a common dimensionality reduction approach for time-series data which has been employed in a variety of domains, including classification and anomaly detection in time-series data. Domains also include shape recognition where the shape outline is converted into time-series data forinstance epoch classification of archived arrowheads. In this paper we propose a dimensionality reduction and shape recognition approach based on the SAX algorithm, an application which requires responses on cost efficient, IoT-like, platforms. The challenge is largely dealing with the computational expense of the SAX algorithm in IoT-like applications, from simple time-series dimension reduction through shape recognition. The approach is based on lowering the dimensional space while capturing and preserving the most representative features of the shape. We present three scenarios of increasing computational complexity backing up our statements with measurement of performance characteristics","sentences":["Symbolic Aggregate approXimation (SAX) is a common dimensionality reduction approach for time-series data which has been employed in a variety of domains, including classification and anomaly detection in time-series data.","Domains also include shape recognition where the shape outline is converted into time-series data forinstance epoch classification of archived arrowheads.","In this paper we propose a dimensionality reduction and shape recognition approach based on the SAX algorithm, an application which requires responses on cost efficient, IoT-like, platforms.","The challenge is largely dealing with the computational expense of the SAX algorithm in IoT-like applications, from simple time-series dimension reduction through shape recognition.","The approach is based on lowering the dimensional space while capturing and preserving the most representative features of the shape.","We present three scenarios of increasing computational complexity backing up our statements with measurement of performance characteristics"],"url":"http://arxiv.org/abs/2405.19817v1","category":"cs.CV"}
{"created":"2024-05-30 08:21:18","title":"SLAM-based Joint Calibration of Multiple Asynchronous Microphone Arrays and Sound Source Localization","abstract":"Robot audition systems with multiple microphone arrays have many applications in practice. However, accurate calibration of multiple microphone arrays remains challenging because there are many unknown parameters to be identified, including the relative transforms (i.e., orientation, translation) and asynchronous factors (i.e., initial time offset and sampling clock difference) between microphone arrays. To tackle these challenges, in this paper, we adopt batch simultaneous localization and mapping (SLAM) for joint calibration of multiple asynchronous microphone arrays and sound source localization. Using the Fisher information matrix (FIM) approach, we first conduct the observability analysis (i.e., parameter identifiability) of the above-mentioned calibration problem and establish necessary/sufficient conditions under which the FIM and the Jacobian matrix have full column rank, which implies the identifiability of the unknown parameters. We also discover several scenarios where the unknown parameters are not uniquely identifiable. Subsequently, we propose an effective framework to initialize the unknown parameters, which is used as the initial guess in batch SLAM for multiple microphone arrays calibration, aiming to further enhance optimization accuracy and convergence. Extensive numerical simulations and real experiments have been conducted to verify the performance of the proposed method. The experiment results show that the proposed pipeline achieves higher accuracy with fast convergence in comparison to methods that use the noise-corrupted ground truth of the unknown parameters as the initial guess in the optimization and other existing frameworks.","sentences":["Robot audition systems with multiple microphone arrays have many applications in practice.","However, accurate calibration of multiple microphone arrays remains challenging because there are many unknown parameters to be identified, including the relative transforms (i.e., orientation, translation) and asynchronous factors (i.e., initial time offset and sampling clock difference) between microphone arrays.","To tackle these challenges, in this paper, we adopt batch simultaneous localization and mapping (SLAM) for joint calibration of multiple asynchronous microphone arrays and sound source localization.","Using the Fisher information matrix (FIM) approach, we first conduct the observability analysis (i.e., parameter identifiability) of the above-mentioned calibration problem and establish necessary/sufficient conditions under which the FIM and the Jacobian matrix have full column rank, which implies the identifiability of the unknown parameters.","We also discover several scenarios where the unknown parameters are not uniquely identifiable.","Subsequently, we propose an effective framework to initialize the unknown parameters, which is used as the initial guess in batch SLAM for multiple microphone arrays calibration, aiming to further enhance optimization accuracy and convergence.","Extensive numerical simulations and real experiments have been conducted to verify the performance of the proposed method.","The experiment results show that the proposed pipeline achieves higher accuracy with fast convergence in comparison to methods that use the noise-corrupted ground truth of the unknown parameters as the initial guess in the optimization and other existing frameworks."],"url":"http://arxiv.org/abs/2405.19813v1","category":"cs.RO"}
{"created":"2024-05-30 08:20:34","title":"Approximate Global Convergence of Independent Learning in Multi-Agent Systems","abstract":"Independent learning (IL), despite being a popular approach in practice to achieve scalability in large-scale multi-agent systems, usually lacks global convergence guarantees. In this paper, we study two representative algorithms, independent $Q$-learning and independent natural actor-critic, within value-based and policy-based frameworks, and provide the first finite-sample analysis for approximate global convergence. The results imply a sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ up to an error term that captures the dependence among agents and characterizes the fundamental limit of IL in achieving global convergence. To establish the result, we develop a novel approach for analyzing IL by constructing a separable Markov decision process (MDP) for convergence analysis and then bounding the gap due to model difference between the separable MDP and the original one. Moreover, we conduct numerical experiments using a synthetic MDP and an electric vehicle charging example to verify our theoretical findings and to demonstrate the practical applicability of IL.","sentences":["Independent learning (IL), despite being a popular approach in practice to achieve scalability in large-scale multi-agent systems, usually lacks global convergence guarantees.","In this paper, we study two representative algorithms, independent $Q$-learning and independent natural actor-critic, within value-based and policy-based frameworks, and provide the first finite-sample analysis for approximate global convergence.","The results imply a sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ up to an error term that captures the dependence among agents and characterizes the fundamental limit of IL in achieving global convergence.","To establish the result, we develop a novel approach for analyzing IL by constructing a separable Markov decision process (MDP) for convergence analysis and then bounding the gap due to model difference between the separable MDP and the original one.","Moreover, we conduct numerical experiments using a synthetic MDP and an electric vehicle charging example to verify our theoretical findings and to demonstrate the practical applicability of IL."],"url":"http://arxiv.org/abs/2405.19811v1","category":"cs.LG"}
{"created":"2024-05-30 08:14:34","title":"Complexity of Deciding Injectivity and Surjectivity of ReLU Neural Networks","abstract":"Neural networks with ReLU activation play a key role in modern machine learning. In view of safety-critical applications, the verification of trained networks is of great importance and necessitates a thorough understanding of essential properties of the function computed by a ReLU network, including characteristics like injectivity and surjectivity. Recently, Puthawala et al. [JMLR 2022] came up with a characterization for injectivity of a ReLU layer, which implies an exponential time algorithm. However, the exact computational complexity of deciding injectivity remained open. We answer this question by proving coNP-completeness of deciding injectivity of a ReLU layer. On the positive side, as our main result, we present a parameterized algorithm which yields fixed-parameter tractability of the problem with respect to the input dimension. In addition, we also characterize surjectivity for two-layer ReLU networks with one-dimensional output. Remarkably, the decision problem turns out to be the complement of a basic network verification task. We prove NP-hardness for surjectivity, implying a stronger hardness result than previously known for the network verification problem. Finally, we reveal interesting connections to computational convexity by formulating the surjectivity problem as a zonotope containment problem","sentences":["Neural networks with ReLU activation play a key role in modern machine learning.","In view of safety-critical applications, the verification of trained networks is of great importance and necessitates a thorough understanding of essential properties of the function computed by a ReLU network, including characteristics like injectivity and surjectivity.","Recently, Puthawala et al.","[JMLR 2022] came up with a characterization for injectivity of a ReLU layer, which implies an exponential time algorithm.","However, the exact computational complexity of deciding injectivity remained open.","We answer this question by proving coNP-completeness of deciding injectivity of a ReLU layer.","On the positive side, as our main result, we present a parameterized algorithm which yields fixed-parameter tractability of the problem with respect to the input dimension.","In addition, we also characterize surjectivity for two-layer ReLU networks with one-dimensional output.","Remarkably, the decision problem turns out to be the complement of a basic network verification task.","We prove NP-hardness for surjectivity, implying a stronger hardness result than previously known for the network verification problem.","Finally, we reveal interesting connections to computational convexity by formulating the surjectivity problem as a zonotope containment problem"],"url":"http://arxiv.org/abs/2405.19805v1","category":"cs.CC"}
{"created":"2024-05-30 08:11:01","title":"Modeling of Nitric Oxide Infrared radiative flux in lower thermosphere: a machine learning perspective","abstract":"Nitric Oxide (NO) significantly impacts energy distribution and chemical processes in the mesosphere and lower thermosphere (MLT). During geomagnetic storms, a substantial influx of energy in the thermosphere leads to an increase in NO infrared emissions. Accurately predicting the radiative flux of Nitric Oxide is crucial for understanding the thermospheric energy budget, particularly during extreme space weather events. With advancements in computational techniques, machine learning (ML) has become a highly effective tool for space weather forecasting. This effort becomes even more worthwhile considering the availability of two decades of continuous NO infrared emissions measurement by TIMED/SABER along with several other key thermospheric variables. We present the scheme of development of an ML-based predictive model for Nitric Oxide Infrared Radiative Flux (NOIRF). Various ML algorithms have been tested for better predictive ability, and an optimized model (NOEMLM) has been developed for the study of NOIRF. This model is able to extract the underlying relationships between the input features and effectively predict the NOIRF. The NOEMLM predictions have very good agreements with SABER observation during quiet time as well as geomagnetic storms. In comparison with the existing TIEGCM model, NOEMLM has very good performance, especially during extreme space weather conditions. The results of this study suggest that utilizing geomagnetic and space weather indices with ML/AI can serve as superior parameters for studying the upper atmosphere, as compared to focusing on specific species having complex chemical processes and associated uncertainties in constituents. ML techniques can effectively carry out the analysis with greater ease than traditional chemical studies.","sentences":["Nitric Oxide (NO) significantly impacts energy distribution and chemical processes in the mesosphere and lower thermosphere (MLT).","During geomagnetic storms, a substantial influx of energy in the thermosphere leads to an increase in NO infrared emissions.","Accurately predicting the radiative flux of Nitric Oxide is crucial for understanding the thermospheric energy budget, particularly during extreme space weather events.","With advancements in computational techniques, machine learning (ML) has become a highly effective tool for space weather forecasting.","This effort becomes even more worthwhile considering the availability of two decades of continuous NO infrared emissions measurement by TIMED/SABER along with several other key thermospheric variables.","We present the scheme of development of an ML-based predictive model for Nitric Oxide Infrared Radiative Flux (NOIRF).","Various ML algorithms have been tested for better predictive ability, and an optimized model (NOEMLM) has been developed for the study of NOIRF.","This model is able to extract the underlying relationships between the input features and effectively predict the NOIRF.","The NOEMLM predictions have very good agreements with SABER observation during quiet time as well as geomagnetic storms.","In comparison with the existing TIEGCM model, NOEMLM has very good performance, especially during extreme space weather conditions.","The results of this study suggest that utilizing geomagnetic and space weather indices with ML/AI can serve as superior parameters for studying the upper atmosphere, as compared to focusing on specific species having complex chemical processes and associated uncertainties in constituents.","ML techniques can effectively carry out the analysis with greater ease than traditional chemical studies."],"url":"http://arxiv.org/abs/2405.19801v1","category":"physics.space-ph"}
{"created":"2024-05-30 08:10:50","title":"Unsupervised Mutual Learning of Dialogue Discourse Parsing and Topic Segmentation","abstract":"The advancement of large language models (LLMs) has propelled the development of dialogue systems. Unlike the popular ChatGPT-like assistant model, which only satisfies the user's preferences, task-oriented dialogue systems have also faced new requirements and challenges in the broader business field. They are expected to provide correct responses at each dialogue turn, at the same time, achieve the overall goal defined by the task. By understanding rhetorical structures and topic structures via topic segmentation and discourse parsing, a dialogue system may do a better planning to achieve both objectives. However, while both structures belong to discourse structure in linguistics, rhetorical structure and topic structure are mostly modeled separately or with one assisting the other in the prior work. The interaction between these two structures has not been considered for joint modeling and mutual learning. Furthermore, unsupervised learning techniques to achieve the above are not well explored. To fill this gap, we propose an unsupervised mutual learning framework of two structures leveraging the global and local connections between them. We extend the topic modeling between non-adjacent discourse units to ensure global structural relevance with rhetorical structures. We also incorporate rhetorical structures into the topic structure through a graph neural network model to ensure local coherence consistency. Finally, we utilize the similarity between the two fused structures for mutual learning. The experimental results demonstrate that our methods outperform all strong baselines on two dialogue rhetorical datasets (STAC and Molweni), as well as dialogue topic datasets (Doc2Dial and TIAGE).","sentences":["The advancement of large language models (LLMs) has propelled the development of dialogue systems.","Unlike the popular ChatGPT-like assistant model, which only satisfies the user's preferences, task-oriented dialogue systems have also faced new requirements and challenges in the broader business field.","They are expected to provide correct responses at each dialogue turn, at the same time, achieve the overall goal defined by the task.","By understanding rhetorical structures and topic structures via topic segmentation and discourse parsing, a dialogue system may do a better planning to achieve both objectives.","However, while both structures belong to discourse structure in linguistics, rhetorical structure and topic structure are mostly modeled separately or with one assisting the other in the prior work.","The interaction between these two structures has not been considered for joint modeling and mutual learning.","Furthermore, unsupervised learning techniques to achieve the above are not well explored.","To fill this gap, we propose an unsupervised mutual learning framework of two structures leveraging the global and local connections between them.","We extend the topic modeling between non-adjacent discourse units to ensure global structural relevance with rhetorical structures.","We also incorporate rhetorical structures into the topic structure through a graph neural network model to ensure local coherence consistency.","Finally, we utilize the similarity between the two fused structures for mutual learning.","The experimental results demonstrate that our methods outperform all strong baselines on two dialogue rhetorical datasets (STAC and Molweni), as well as dialogue topic datasets (Doc2Dial and TIAGE)."],"url":"http://arxiv.org/abs/2405.19799v1","category":"cs.CL"}
{"created":"2024-05-30 08:10:29","title":"Mixed radix numeration bases: H\u00f6rner's rule, Yang-Baxter equation and Furstenberg's conjecture","abstract":"Mixed radix bases in numeration is a very old notion but it is rarely studied on its own or in relation with concrete problems related to number theory. Starting from the natural question of the conversion of a basis to another for integers as well as polynomials, we use mixed radix bases to introduce two-dimensional arrays with suitable filling rules. These arrays provide algorithms of conversion which uses only a finite number of euclidean division to convert from one basis to another; it is interesting to note that these algorithms are generalizations of the well-known H\\\"orner's rule of quick evaluation of polynomials. The two-dimensional arrays with local transformations are reminiscent from statistical mechanics models: we show that changes between three numeration basis are related to the set-theoretical Yang-Baxter equation and this is, up to our knowledge, the first time that such a structure is described in number theory. As an illustration, we reinterpret well-known results around Furstenberg's conjecture in terms of Yang-Baxter transformations between mixed radix bases, hence opening the way to alternative approaches.","sentences":["Mixed radix bases in numeration is a very old notion but it is rarely studied on its own or in relation with concrete problems related to number theory.","Starting from the natural question of the conversion of a basis to another for integers as well as polynomials, we use mixed radix bases to introduce two-dimensional arrays with suitable filling rules.","These arrays provide algorithms of conversion which uses only a finite number of euclidean division to convert from one basis to another; it is interesting to note that these algorithms are generalizations of the well-known H\\\"orner's rule of quick evaluation of polynomials.","The two-dimensional arrays with local transformations are reminiscent from statistical mechanics models: we show that changes between three numeration basis are related to the set-theoretical Yang-Baxter equation and this is, up to our knowledge, the first time that such a structure is described in number theory.","As an illustration, we reinterpret well-known results around Furstenberg's conjecture in terms of Yang-Baxter transformations between mixed radix bases, hence opening the way to alternative approaches."],"url":"http://arxiv.org/abs/2405.19798v1","category":"math-ph"}
{"created":"2024-05-30 08:07:33","title":"Growth of the Moon due to bodies ejected from the Earth","abstract":"The evolution of the orbits of bodies ejected from the Earth has been studied at the stage of its accumulation and early evolution after impacts of large planetesimals. In the considered variants of calculations of the motion of bodies ejected from the Earth, most of the bodies left the Hill sphere of the Earth and moved in heliocentric orbits. Their dynamical lifetime reached several hundred million years. At higher ejection velocities vej the probabilities of collisions of bodies with the Earth and Moon were generally lower. Over the entire considered time interval at the ejection velocity vej, equal to 11.5, 12 and 14 km/s, the values of the probability of a collision of a body with the Earth were approximately 0.3, 0.2 and 0.15-0.2, respectively. At ejection velocities vej<11.25 km/s, i.e., slightly exceeding a parabolic velocity, most of the ejected bodies fell back to the Earth. The probability of a collision of a body ejected from the Earth with the Moon moving in its present orbit was approximately 15-35 times less than that with the Earth at vej>11.5 km/s. The probability of a collision of such bodies with the Moon was mainly about 0.004-0.008 at ejection velocities of at least 14 km/s and about 0.006-0.01 at vej=12 km/s. It was larger at lower ejection velocities and was in the range of 0.01-0.02 at vej=11.3 km/s. The Moon may contain material ejected from the Earth during the accumulation of the Earth and during the late heavy bombardment. At the same time, as obtained in our calculations, the bodies ejected from the Earth and falling on the Moon embryo would not be enough for the Moon to grow to its present mass from a small embryo moving along the present orbit of the Moon. This result argues in favor of the formation of a lunar embryo and its further growth to most of the present mass of the Moon near the Earth.","sentences":["The evolution of the orbits of bodies ejected from the Earth has been studied at the stage of its accumulation and early evolution after impacts of large planetesimals.","In the considered variants of calculations of the motion of bodies ejected from the Earth, most of the bodies left the Hill sphere of the Earth and moved in heliocentric orbits.","Their dynamical lifetime reached several hundred million years.","At higher ejection velocities vej the probabilities of collisions of bodies with the Earth and Moon were generally lower.","Over the entire considered time interval at the ejection velocity vej, equal to 11.5, 12 and 14 km/s, the values of the probability of a collision of a body with the Earth were approximately 0.3, 0.2 and 0.15-0.2, respectively.","At ejection velocities vej<11.25 km/s, i.e., slightly exceeding a parabolic velocity, most of the ejected bodies fell back to the Earth.","The probability of a collision of a body ejected from the Earth with the Moon moving in its present orbit was approximately 15-35 times less than that with the Earth at vej>11.5 km/s. The probability of a collision of such bodies with the Moon was mainly about 0.004-0.008 at ejection velocities of at least 14 km/s and about 0.006-0.01 at vej=12 km/s.","It was larger at lower ejection velocities and was in the range of 0.01-0.02 at vej=11.3 km/s. The Moon may contain material ejected from the Earth during the accumulation of the Earth and during the late heavy bombardment.","At the same time, as obtained in our calculations, the bodies ejected from the Earth and falling on the Moon embryo would not be enough for the Moon to grow to its present mass from a small embryo moving along the present orbit of the Moon.","This result argues in favor of the formation of a lunar embryo and its further growth to most of the present mass of the Moon near the Earth."],"url":"http://arxiv.org/abs/2405.19797v1","category":"astro-ph.EP"}
{"created":"2024-05-30 17:59:45","title":"Mixed finite element methods for fourth order obstacle problems in linearised elasticity","abstract":"This paper is devoted to the study of a novel mixed Finite Element Method for approximating the solutions of fourth order variational problems subjected to a constraint.   The first problem we consider consists in establishing the convergence of the error of the numerical approximation of the solution of a biharmonic obstacle problem. The contents of this section are meant to generalise the approach originally proposed by Ciarlet \\& Raviart, and then complemented by Ciarlet \\& Glowinski.   The second problem we consider amounts to studying a two-dimensional variational problem for linearly elastic shallow shells subjected to remaining confined in a prescribed half-space. We first study the case where the parametrisation of the middle surface for the linearly elastic shallow shell under consideration has non-zero curvature, and we observe that the numerical approximation of this model via a mixed Finite Element Method based on conforming elements requires the implementation of the additional constraint according to which the gradient matrix of the dual variable has to be symmetric. However, differently from the biharmonic obstacle problem previously studied, we show that the numerical implementation of this result cannot be implemented by solely resorting to Courant triangles.   Finally, we show that if the middle surface of the linearly elastic shallow shell under consideration is flat, the symmetry constraint required for formulating the constrained mixed variational problem announced in the second part of the paper is not required, and the solution can thus be approximated by solely resorting to Courant triangles.   The theoretical results we derived are complemented by a series of numerical experiments.","sentences":["This paper is devoted to the study of a novel mixed Finite Element Method for approximating the solutions of fourth order variational problems subjected to a constraint.   ","The first problem we consider consists in establishing the convergence of the error of the numerical approximation of the solution of a biharmonic obstacle problem.","The contents of this section are meant to generalise the approach originally proposed by Ciarlet \\& Raviart, and then complemented by Ciarlet \\& Glowinski.   ","The second problem we consider amounts to studying a two-dimensional variational problem for linearly elastic shallow shells subjected to remaining confined in a prescribed half-space.","We first study the case where the parametrisation of the middle surface for the linearly elastic shallow shell under consideration has non-zero curvature, and we observe that the numerical approximation of this model via a mixed Finite Element Method based on conforming elements requires the implementation of the additional constraint according to which the gradient matrix of the dual variable has to be symmetric.","However, differently from the biharmonic obstacle problem previously studied, we show that the numerical implementation of this result cannot be implemented by solely resorting to Courant triangles.   ","Finally, we show that if the middle surface of the linearly elastic shallow shell under consideration is flat, the symmetry constraint required for formulating the constrained mixed variational problem announced in the second part of the paper is not required, and the solution can thus be approximated by solely resorting to Courant triangles.   ","The theoretical results we derived are complemented by a series of numerical experiments."],"url":"http://arxiv.org/abs/2405.20338v1","category":"math.NA"}
{"created":"2024-05-30 17:59:05","title":"Taxonomy of Infinite Distance Limits","abstract":"The Emergent String Conjecture constrains the possible types of light towers in infinite-distance limits in quantum gravity moduli spaces. In this paper, we use these constraints to restrict the geometry of the scalar charge-to-mass vectors $(-\\vec{\\nabla}\\log m)$ of the light towers and the analogous vector $(-\\vec{\\nabla}\\log\\Lambda_{\\text{QG}})$ of the species scale. We derive taxonomic rules that these vectors must satisfy in each duality frame. Under certain assumptions, this allows us to classify the ways in which different duality frames can fit together globally in the moduli space in terms of a finite list of polytopes. Many of these polytopes arise in known string theory compactifications, while others suggest either undiscovered corners of the landscape or new swampland constraints.","sentences":["The Emergent String Conjecture constrains the possible types of light towers in infinite-distance limits in quantum gravity moduli spaces.","In this paper, we use these constraints to restrict the geometry of the scalar charge-to-mass vectors $(-\\vec{\\nabla}\\log m)$ of the light towers and the analogous vector $(-\\vec{\\nabla}\\log\\Lambda_{\\text{QG}})$ of the species scale.","We derive taxonomic rules that these vectors must satisfy in each duality frame.","Under certain assumptions, this allows us to classify the ways in which different duality frames can fit together globally in the moduli space in terms of a finite list of polytopes.","Many of these polytopes arise in known string theory compactifications, while others suggest either undiscovered corners of the landscape or new swampland constraints."],"url":"http://arxiv.org/abs/2405.20332v1","category":"hep-th"}
{"created":"2024-05-30 17:52:52","title":"A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D Reconstruction","abstract":"Learning 3D scene representation from a single-view image is a long-standing fundamental problem in computer vision, with the inherent ambiguity in predicting contents unseen from the input view. Built on the recently proposed 3D Gaussian Splatting (3DGS), the Splatter Image method has made promising progress on fast single-image novel view synthesis via learning a single 3D Gaussian for each pixel based on the U-Net feature map of an input image. However, it has limited expressive power to represent occluded components that are not observable in the input view. To address this problem, this paper presents a Hierarchical Splatter Image method in which a pixel is worth more than one 3D Gaussians. Specifically,   each pixel is represented by a parent 3D Gaussian and a small number of child 3D Gaussians. Parent 3D Gaussians are learned as done in the vanilla Splatter Image. Child 3D Gaussians are learned via a lightweight Multi-Layer Perceptron (MLP) which takes as input the projected image features of a parent 3D Gaussian and the embedding of a target camera view. Both parent and child 3D Gaussians are learned end-to-end in a stage-wise way. The joint condition of input image features from eyes of the parent Gaussians and the target camera position facilitates learning to allocate child Gaussians to ``see the unseen'', recovering the occluded details that are often missed by parent Gaussians.   In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D datasets with state-of-the-art performance obtained, especially showing promising capabilities of reconstructing occluded contents in the input view.","sentences":["Learning 3D scene representation from a single-view image is a long-standing fundamental problem in computer vision, with the inherent ambiguity in predicting contents unseen from the input view.","Built on the recently proposed 3D Gaussian Splatting (3DGS), the Splatter Image method has made promising progress on fast single-image novel view synthesis via learning a single 3D Gaussian for each pixel based on the U-Net feature map of an input image.","However, it has limited expressive power to represent occluded components that are not observable in the input view.","To address this problem, this paper presents a Hierarchical Splatter Image method in which a pixel is worth more than one 3D Gaussians.","Specifically,   each pixel is represented by a parent 3D Gaussian and a small number of child 3D Gaussians.","Parent 3D Gaussians are learned as done in the vanilla Splatter Image.","Child 3D Gaussians are learned via a lightweight Multi-Layer Perceptron (MLP) which takes as input the projected image features of a parent 3D Gaussian and the embedding of a target camera view.","Both parent and child 3D Gaussians are learned end-to-end in a stage-wise way.","The joint condition of input image features from eyes of the parent Gaussians and the target camera position facilitates learning to allocate child Gaussians to ``see the unseen'', recovering the occluded details that are often missed by parent Gaussians.   ","In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D datasets with state-of-the-art performance obtained, especially showing promising capabilities of reconstructing occluded contents in the input view."],"url":"http://arxiv.org/abs/2405.20310v1","category":"cs.CV"}
{"created":"2024-05-30 17:48:24","title":"Summer Amplification of Wind Stilling and Land Warming Compounds Energy Risks in a Changing Climate","abstract":"Wind energy is key for reducing greenhouse gas emissions and meeting increasing energy demand. Long-term changes in wind resources were extensively studied but notable gaps remain. Meanwhile, the temporal changes and compounding risks received little attention. Here we analyze large-ensemble climate simulations and show that anthropogenic warming can contribute to significant wind stilling, particularly in the warm season of the northern hemisphere midlatitudes. The stilling is related to the amplified warming of the midlatitude land and mid-to-upper troposphere. At representative US sites, the stilling will unlikely threaten the cost-competitiveness of onshore wind relative to fossil energy. However, its impact can exceed a large interest increase and make wind energy less cost-competitive than other clean energy. Moreover, the summertime stilling coincides with surging cooling demand in the midlatitudes and may bring challenges to energy security. The findings about wind stilling and its implications may help inform future energy investments.","sentences":["Wind energy is key for reducing greenhouse gas emissions and meeting increasing energy demand.","Long-term changes in wind resources were extensively studied but notable gaps remain.","Meanwhile, the temporal changes and compounding risks received little attention.","Here we analyze large-ensemble climate simulations and show that anthropogenic warming can contribute to significant wind stilling, particularly in the warm season of the northern hemisphere midlatitudes.","The stilling is related to the amplified warming of the midlatitude land and mid-to-upper troposphere.","At representative US sites, the stilling will unlikely threaten the cost-competitiveness of onshore wind relative to fossil energy.","However, its impact can exceed a large interest increase and make wind energy less cost-competitive than other clean energy.","Moreover, the summertime stilling coincides with surging cooling demand in the midlatitudes and may bring challenges to energy security.","The findings about wind stilling and its implications may help inform future energy investments."],"url":"http://arxiv.org/abs/2405.20302v1","category":"physics.ao-ph"}
{"created":"2024-05-30 17:41:32","title":"Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness","abstract":"The security threat of backdoor attacks is a central concern for deep neural networks (DNNs). Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense. Additionally, vanilla fine-tuning with those clean data can help recover the lost clean accuracy. However, the behavior of clean unlearning is still under-explored, and vanilla fine-tuning unintentionally induces back the backdoor effect. In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data; 2) the neurons of the backdoored model are more active (i.e., larger changes in gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning. Then, we propose an effective two-stage defense method. In the first stage, an efficient Neuron Weight Change (NWC)-based Backdoor Reinitialization is proposed based on observation 1). In the second stage, based on observation 2), we design an Activeness-Aware Fine-Tuning to replace the vanilla fine-tuning. Extensive experiments, involving eight backdoor attacks on three benchmark datasets, demonstrate the superior performance of our proposed method compared to recent state-of-the-art backdoor defense approaches.","sentences":["The security threat of backdoor attacks is a central concern for deep neural networks (DNNs).","Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense.","Additionally, vanilla fine-tuning with those clean data can help recover the lost clean accuracy.","However, the behavior of clean unlearning is still under-explored, and vanilla fine-tuning unintentionally induces back the backdoor effect.","In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data; 2) the neurons of the backdoored model are more active (i.e., larger changes in gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning.","Then, we propose an effective two-stage defense method.","In the first stage, an efficient Neuron Weight Change (NWC)-based Backdoor Reinitialization is proposed based on observation 1).","In the second stage, based on observation 2), we design an Activeness-Aware Fine-Tuning to replace the vanilla fine-tuning.","Extensive experiments, involving eight backdoor attacks on three benchmark datasets, demonstrate the superior performance of our proposed method compared to recent state-of-the-art backdoor defense approaches."],"url":"http://arxiv.org/abs/2405.20291v1","category":"cs.CR"}
{"created":"2024-05-30 17:07:38","title":"A test for a local formation of finite groups to be a formation of soluble groups with the Shemetkov property","abstract":"L.A. Shemetkov posed a Problem 9.74 in Kourovka Notebook to find all local formations $\\mathfrak{F}$ of finite groups such that every finite minimal non-$\\mathfrak{F}$-group is either a Schmidt group or a group of prime order. All known solutions to this problem are obtained under the assumption that every minimal non-$\\mathfrak{F}$-group is soluble. Using the above mentioned solutions we present a polynomial in $n$ time check for a local formation $\\mathfrak{F}$ with bounded $\\pi(\\mathfrak{F})$ to be a formation of soluble groups with the Shemtkov property where $n=\\max \\pi(\\mathfrak{F})$.","sentences":["L.A. Shemetkov posed a Problem 9.74 in Kourovka Notebook to find all local formations $\\mathfrak{F}$ of finite groups such that every finite minimal non-$\\mathfrak{F}$-group is either a Schmidt group or a group of prime order.","All known solutions to this problem are obtained under the assumption that every minimal non-$\\mathfrak{F}$-group is soluble.","Using the above mentioned solutions we present a polynomial in $n$ time check for a local formation $\\mathfrak{F}$ with bounded $\\pi(\\mathfrak{F})$ to be a formation of soluble groups with the Shemtkov property where $n=\\max \\pi(\\mathfrak{F})$."],"url":"http://arxiv.org/abs/2405.20257v1","category":"math.GR"}
{"created":"2024-05-30 16:16:19","title":"Filter Design for Estimation of Stellar Metallicity: Insights from Experiments with Gaia XP Spectra","abstract":"We search for an optimal filter design for the estimation of stellar metallicity, based on synthetic photometry from Gaia XP spectra convolved with a series of filter-transmission curves defined by different central wavelengths and bandwidths. Unlike previous designs based solely on maximizing metallicity sensitivity, we find that the optimal solution provides a balance between the sensitivity and uncertainty of the spectra. With this optimal filter design, the best precision of metallicity estimates for relatively bright ($G \\sim 11.5$) stars is excellent, $\\sigma_{\\rm [Fe/H]} = 0.034$\\,dex for FGK dwarf stars, superior to that obtained utilizing custom sensitivity-optimized filters (e.g., SkyMapper\\,$v$). By selecting hundreds of high-probabability member stars of the open cluster M67, our analysis reveals that the intrinsic photometric-metallicity scatter of these cluster members is only 0.036\\,dex, consistent with this level of precision. Our results clearly demonstrate that the internal precision of photometric-metallicity estimates can be extremely high, even providing the opportunity to perform chemical tagging for very large numbers of field stars in the Milky Way. This experiment shows that it is crucial to take into account uncertainty alongside the sensitivity when designing filters for measuring the stellar metallicity and other parameters.","sentences":["We search for an optimal filter design for the estimation of stellar metallicity, based on synthetic photometry from Gaia XP spectra convolved with a series of filter-transmission curves defined by different central wavelengths and bandwidths.","Unlike previous designs based solely on maximizing metallicity sensitivity, we find that the optimal solution provides a balance between the sensitivity and uncertainty of the spectra.","With this optimal filter design, the best precision of metallicity estimates for relatively bright ($G \\sim 11.5$) stars is excellent, $\\sigma_{\\rm [Fe/H]} = 0.034$\\,dex for FGK dwarf stars, superior to that obtained utilizing custom sensitivity-optimized filters (e.g., SkyMapper\\,$v$).","By selecting hundreds of high-probabability member stars of the open cluster M67, our analysis reveals that the intrinsic photometric-metallicity scatter of these cluster members is only 0.036\\,dex, consistent with this level of precision.","Our results clearly demonstrate that the internal precision of photometric-metallicity estimates can be extremely high, even providing the opportunity to perform chemical tagging for very large numbers of field stars in the Milky Way.","This experiment shows that it is crucial to take into account uncertainty alongside the sensitivity when designing filters for measuring the stellar metallicity and other parameters."],"url":"http://arxiv.org/abs/2405.20212v1","category":"astro-ph.SR"}
{"created":"2024-05-30 16:08:53","title":"Measurements of jet cross-section ratios in 13 TeV proton--proton collisions with ATLAS","abstract":"Measurements of jet cross-section ratios between inclusive bins of jet multiplicity are performed in 140 fb$^{-1}$ of proton--proton collisions with $\\sqrt{s}=13$ TeV center-of-mass energy, recorded with the ATLAS detector at CERN's Large Hadron Collider. Observables that are sensitive the energy-scale and angular distribution of radiation due to the strong interaction in the final state are measured double-differentially, in bins of jet multiplicity, and are unfolded to account for acceptance and detector-related effects. Additionally, the scalar sum of the two leading jets' transverse momenta is measured triple-differentially, in bins of the third jet's transverse momentum as well as bins of jet multiplicity. The measured distributions are used to construct ratios of the inclusive jet-multiplicity bins, which have been shown to be sensitive to the strong coupling $\\alpha_{\\textrm S}$ while being less sensitive than other observables to systematic uncertainties and parton distribution functions. The measured distributions are compared with state-of-the-art QCD calculations, including next-to-next-to-leading-order predictions. Studies leading to reduced jet energy scale uncertainties significantly improve the precision of this work, and are documented herein.","sentences":["Measurements of jet cross-section ratios between inclusive bins of jet multiplicity are performed in 140 fb$^{-1}$ of proton--proton collisions with $\\sqrt{s}=13$ TeV center-of-mass energy, recorded with the ATLAS detector at CERN's Large Hadron Collider.","Observables that are sensitive the energy-scale and angular distribution of radiation due to the strong interaction in the final state are measured double-differentially, in bins of jet multiplicity, and are unfolded to account for acceptance and detector-related effects.","Additionally, the scalar sum of the two leading jets' transverse momenta is measured triple-differentially, in bins of the third jet's transverse momentum as well as bins of jet multiplicity.","The measured distributions are used to construct ratios of the inclusive jet-multiplicity bins, which have been shown to be sensitive to the strong coupling $\\alpha_{\\textrm S}$ while being less sensitive than other observables to systematic uncertainties and parton distribution functions.","The measured distributions are compared with state-of-the-art QCD calculations, including next-to-next-to-leading-order predictions.","Studies leading to reduced jet energy scale uncertainties significantly improve the precision of this work, and are documented herein."],"url":"http://arxiv.org/abs/2405.20206v1","category":"hep-ex"}
{"created":"2024-05-30 16:05:37","title":"The development of drug resistance in metastatic tumours under chemotherapy: an evolutionary perspective","abstract":"We present a mathematical model of the evolutionary dynamics of a metastatic tumour under chemotherapy, comprising non-local partial differential equations for the phenotype-structured cell populations in the primary tumour and its metastasis. These equations are coupled with a physiologically-based pharmacokinetic model of drug delivery, implementing a realistic delivery schedule. The model is carefully calibrated from the literature, focusing on BRAF-mutated melanoma treated with Dabrafenib as a case study. By means of long-time asymptotic analysis, global sensitivity analysis and numerical simulations, we explore the impact of cell migration from the primary to the metastatic site, physiological aspects of the tumour sites and drug dose on the development of drug resistance and treatment efficacy. Our findings provide a possible explanation for empirical evidence indicating that chemotherapy may foster metastatic spread and that metastatic sites may be less impacted by chemotherapy.","sentences":["We present a mathematical model of the evolutionary dynamics of a metastatic tumour under chemotherapy, comprising non-local partial differential equations for the phenotype-structured cell populations in the primary tumour and its metastasis.","These equations are coupled with a physiologically-based pharmacokinetic model of drug delivery, implementing a realistic delivery schedule.","The model is carefully calibrated from the literature, focusing on BRAF-mutated melanoma treated with Dabrafenib as a case study.","By means of long-time asymptotic analysis, global sensitivity analysis and numerical simulations, we explore the impact of cell migration from the primary to the metastatic site, physiological aspects of the tumour sites and drug dose on the development of drug resistance and treatment efficacy.","Our findings provide a possible explanation for empirical evidence indicating that chemotherapy may foster metastatic spread and that metastatic sites may be less impacted by chemotherapy."],"url":"http://arxiv.org/abs/2405.20203v1","category":"q-bio.CB"}
{"created":"2024-05-30 15:41:39","title":"Lethal surface ozone concentrations are possible on habitable zone exoplanets","abstract":"Ozone ($\\textrm{O}_3$) is important for the survival of life on Earth because it shields the surface from ionising ultraviolet (UV) radiation. However, the existence of $\\textrm{O}_3$ in Earth's atmosphere is not always beneficial. Resulting from anthropogenic activity, $\\textrm{O}_3$ exists as a biologically harmful pollutant at the surface when it forms in the presence of sunlight and other pollutants. As a strong oxidiser, $\\textrm{O}_3$ can be lethal to several different organisms; thus, when assessing the potential habitability of an exoplanet, a key part is determining whether toxic gases could be present at its surface. Using the Whole Atmosphere Community Climate Model version 6 (WACCM6; a three-dimensional chemistry-climate model), twelve atmospheric simulations of the terrestrial exoplanet TRAPPIST-1 e are performed with a variety of $\\textrm{O}_2$ concentrations and assuming two different stellar spectra proposed in the literature. Four atmospheric simulations of the exoplanet Proxima Centauri b are also included. Some scenarios for both exoplanets exhibit time-averaged surface $\\textrm{O}_3$ mixing ratios exceeding harmful levels of 40 ppbv, with 2200 ppbv the maximum concentration found in the cases simulated. These concentrations are toxic and can be fatal to most life on Earth. In other scenarios $\\textrm{O}_3$ remains under harmful limits over a significant fraction of the surface, despite there being present regions which may prove inhospitable. In the case that $\\textrm{O}_3$ is detected in a terrestrial exoplanet's atmosphere, determining the surface concentration is an important step when evaluating a planet's habitability.","sentences":["Ozone ($\\textrm{O}_3$) is important for the survival of life on Earth because it shields the surface from ionising ultraviolet (UV) radiation.","However, the existence of $\\textrm{O}_3$ in Earth's atmosphere is not always beneficial.","Resulting from anthropogenic activity, $\\textrm{O}_3$ exists as a biologically harmful pollutant at the surface when it forms in the presence of sunlight and other pollutants.","As a strong oxidiser, $\\textrm{O}_3$ can be lethal to several different organisms; thus, when assessing the potential habitability of an exoplanet, a key part is determining whether toxic gases could be present at its surface.","Using the Whole Atmosphere Community Climate Model version 6 (WACCM6; a three-dimensional chemistry-climate model), twelve atmospheric simulations of the terrestrial exoplanet TRAPPIST-1 e are performed with a variety of $\\textrm{O}_2$ concentrations and assuming two different stellar spectra proposed in the literature.","Four atmospheric simulations of the exoplanet Proxima Centauri b are also included.","Some scenarios for both exoplanets exhibit time-averaged surface $\\textrm{O}_3$ mixing ratios exceeding harmful levels of 40 ppbv, with 2200 ppbv the maximum concentration found in the cases simulated.","These concentrations are toxic and can be fatal to most life on Earth.","In other scenarios $\\textrm{O}_3$ remains under harmful limits over a significant fraction of the surface, despite there being present regions which may prove inhospitable.","In the case that $\\textrm{O}_3$ is detected in a terrestrial exoplanet's atmosphere, determining the surface concentration is an important step when evaluating a planet's habitability."],"url":"http://arxiv.org/abs/2405.20167v1","category":"astro-ph.EP"}
{"created":"2024-05-30 15:39:19","title":"Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation","abstract":"We study reinforcement learning with multinomial logistic (MNL) function approximation where the underlying transition probability kernel of the Markov decision processes (MDPs) is parametrized by an unknown transition core with features of state and action. For the finite horizon episodic setting with inhomogeneous state transitions, we propose provably efficient algorithms with randomized exploration having frequentist regret guarantees. For our first algorithm, $\\texttt{RRL-MNL}$, we adapt optimistic sampling to ensure the optimism of the estimated value function with sufficient frequency and establish that $\\texttt{RRL-MNL}$ is both statistically and computationally efficient, achieving a $\\tilde{O}(\\kappa^{-1} d^{\\frac{3}{2}} H^{\\frac{3}{2}} \\sqrt{T})$ frequentist regret bound with constant-time computational cost per episode. Here, $d$ is the dimension of the transition core, $H$ is the horizon length, $T$ is the total number of steps, and $\\kappa$ is a problem-dependent constant. Despite the simplicity and practicality of $\\texttt{RRL-MNL}$, its regret bound scales with $\\kappa^{-1}$, which is potentially large in the worst case. To improve the dependence on $\\kappa^{-1}$, we propose $\\texttt{ORRL-MNL}$, which estimates the value function using local gradient information of the MNL transition model. We show that its frequentist regret bound is $\\tilde{O}(d^{\\frac{3}{2}} H^{\\frac{3}{2}} \\sqrt{T} + \\kappa^{-1} d^2 H^2)$. To the best of our knowledge, these are the first randomized RL algorithms for the MNL transition model that achieve both computational and statistical efficiency. Numerical experiments demonstrate the superior performance of the proposed algorithms.","sentences":["We study reinforcement learning with multinomial logistic (MNL) function approximation where the underlying transition probability kernel of the Markov decision processes (MDPs) is parametrized by an unknown transition core with features of state and action.","For the finite horizon episodic setting with inhomogeneous state transitions, we propose provably efficient algorithms with randomized exploration having frequentist regret guarantees.","For our first algorithm, $\\texttt{RRL-MNL}$, we adapt optimistic sampling to ensure the optimism of the estimated value function with sufficient frequency and establish that $\\texttt{RRL-MNL}$ is both statistically and computationally efficient, achieving a $\\tilde{O}(\\kappa^{-1} d^{\\frac{3}{2}} H^{\\frac{3}{2}} \\sqrt{T})$ frequentist regret bound with constant-time computational cost per episode.","Here, $d$ is the dimension of the transition core, $H$ is the horizon length, $T$ is the total number of steps, and $\\kappa$ is a problem-dependent constant.","Despite the simplicity and practicality of $\\texttt{RRL-MNL}$, its regret bound scales with $\\kappa^{-1}$, which is potentially large in the worst case.","To improve the dependence on $\\kappa^{-1}$, we propose $\\texttt{ORRL-MNL}$, which estimates the value function using local gradient information of the MNL transition model.","We show that its frequentist regret bound is $\\tilde{O}(d^{\\frac{3}{2}} H^{\\frac{3}{2}} \\sqrt{T} + \\kappa^{-1} d^2 H^2)$. To the best of our knowledge, these are the first randomized RL algorithms for the MNL transition model that achieve both computational and statistical efficiency.","Numerical experiments demonstrate the superior performance of the proposed algorithms."],"url":"http://arxiv.org/abs/2405.20165v1","category":"stat.ML"}
{"created":"2024-05-30 15:39:12","title":"Item response parameter estimation performance using Gaussian quadrature and Laplace","abstract":"Item parameter estimation in pharmacometric item response theory (IRT) models is predominantly performed using the Laplace estimation algorithm as implemented in NONMEM. In psychometrics a wide range of different software tools, including several packages for the open-source software R for implementation of IRT are also available. Each have their own set of benefits and limitations and to date a systematic comparison of the primary estimation algorithms has not been evaluated. A simulation study evaluating varying number of hypothetical sample sizes and item scenarios at baseline was performed using both Laplace and Gauss-hermite quadrature (GHQ-EM). In scenarios with at least 20 items and more than 100 subjects, item parameters were estimated with good precision and were similar between estimation algorithms as demonstrated by several measures of bias and precision. The minimal differences observed for certain parameters or sample size scenarios were reduced when translating to the total score scale. The ease of use, speed of estimation and relative accuracy of the GHQ-EM method employed in mirt make it an appropriate alternative or supportive analytical approach to NONMEM for potential pharmacometrics IRT applications.","sentences":["Item parameter estimation in pharmacometric item response theory (IRT) models is predominantly performed using the Laplace estimation algorithm as implemented in NONMEM.","In psychometrics a wide range of different software tools, including several packages for the open-source software R for implementation of IRT are also available.","Each have their own set of benefits and limitations and to date a systematic comparison of the primary estimation algorithms has not been evaluated.","A simulation study evaluating varying number of hypothetical sample sizes and item scenarios at baseline was performed using both Laplace and Gauss-hermite quadrature (GHQ-EM).","In scenarios with at least 20 items and more than 100 subjects, item parameters were estimated with good precision and were similar between estimation algorithms as demonstrated by several measures of bias and precision.","The minimal differences observed for certain parameters or sample size scenarios were reduced when translating to the total score scale.","The ease of use, speed of estimation and relative accuracy of the GHQ-EM method employed in mirt make it an appropriate alternative or supportive analytical approach to NONMEM for potential pharmacometrics IRT applications."],"url":"http://arxiv.org/abs/2405.20164v1","category":"stat.ME"}
{"created":"2024-05-30 15:33:32","title":"Landslide mapping from Sentinel-2 imagery through change detection","abstract":"Landslides are one of the most critical and destructive geohazards. Widespread development of human activities and settlements combined with the effects of climate change on weather are resulting in a high increase in the frequency and destructive power of landslides, making them a major threat to human life and the economy. In this paper, we explore methodologies to map newly-occurred landslides using Sentinel-2 imagery automatically. All approaches presented are framed as a bi-temporal change detection problem, requiring only a pair of Sentinel-2 images, taken respectively before and after a landslide-triggering event. Furthermore, we introduce a novel deep learning architecture for fusing Sentinel-2 bi-temporal image pairs with Digital Elevation Model (DEM) data, showcasing its promising performances w.r.t. other change detection models in the literature. As a parallel task, we address limitations in existing datasets by creating a novel geodatabase, which includes manually validated open-access landslide inventories over heterogeneous ecoregions of the world. We release both code and dataset with an open-source license.","sentences":["Landslides are one of the most critical and destructive geohazards.","Widespread development of human activities and settlements combined with the effects of climate change on weather are resulting in a high increase in the frequency and destructive power of landslides, making them a major threat to human life and the economy.","In this paper, we explore methodologies to map newly-occurred landslides using Sentinel-2 imagery automatically.","All approaches presented are framed as a bi-temporal change detection problem, requiring only a pair of Sentinel-2 images, taken respectively before and after a landslide-triggering event.","Furthermore, we introduce a novel deep learning architecture for fusing Sentinel-2 bi-temporal image pairs with Digital Elevation Model (DEM) data, showcasing its promising performances w.r.t.","other change detection models in the literature.","As a parallel task, we address limitations in existing datasets by creating a novel geodatabase, which includes manually validated open-access landslide inventories over heterogeneous ecoregions of the world.","We release both code and dataset with an open-source license."],"url":"http://arxiv.org/abs/2405.20161v1","category":"cs.CV"}
{"created":"2024-05-30 15:28:07","title":"Decoherence-assisted quantum key distribution","abstract":"We present a theoretical and experimental study of a controllable decoherence-assisted quantum key distribution scheme. Our method is based on the possibility of introducing controllable decoherence to polarization qubits using the spatial degree of freedom of light. We show that our method reduces the amount of information that an eavesdropper can obtain in the BB84 protocol under the entangling probe attack. We demonstrate experimentally that Alice and Bob can agree on a scheme to that gives low values of the quantum bit error rate, despite the presence of a large amount of decoherence in the transmission channel of the BB84 protocol.","sentences":["We present a theoretical and experimental study of a controllable decoherence-assisted quantum key distribution scheme.","Our method is based on the possibility of introducing controllable decoherence to polarization qubits using the spatial degree of freedom of light.","We show that our method reduces the amount of information that an eavesdropper can obtain in the BB84 protocol under the entangling probe attack.","We demonstrate experimentally that Alice and Bob can agree on a scheme to that gives low values of the quantum bit error rate, despite the presence of a large amount of decoherence in the transmission channel of the BB84 protocol."],"url":"http://arxiv.org/abs/2405.20153v1","category":"quant-ph"}
{"created":"2024-05-30 15:24:56","title":"Can the a.c.s. notion and the GLT theory handle approximated PDEs/FDEs with either moving or unbounded domains?","abstract":"In the current note we consider matrix-sequences $\\{B_{n,t}\\}_n$ of increasing sizes depending on $n$ and equipped with a parameter $t>0$. For every fixed $t>0$, we assume that each $\\{B_{n,t}\\}_n$ possesses a canonical spectral/singular values symbol $f_t$ defined on $D_t\\subset \\R^{d}$ of finite measure, $d\\ge 1$. Furthermore, we assume that $ \\{ \\{ B_{n,t}\\} : \\, t > 0 \\} $ is an approximating class of sequences (a.c.s.) for $ \\{ A_n \\} $ and that $ \\bigcup_{t > 0} D_t = D $ with $ D_{t + 1} \\supset D_t $. Under such assumptions and via the notion of a.c.s, we prove results on the canonical distributions of $ \\{ A_n \\} $, whose symbol, when it exists, can be defined on the, possibly unbounded, domain $D$ of finite or even infinite measure.   We then extend the concept of a.c.s. to the case where the approximating sequence $ \\{ B_{n,t}\\}_n $ has possibly a different dimension than the one of $ \\{ A_n\\} $. This concept seems to be particularly natural when dealing, e.g., with the approximation both of a partial differential equation (PDE) and of its (possibly unbounded, or moving) domain $D$, using an exhausting sequence of domains $\\{ D_t \\}$.   Examples coming from approximated PDEs/FDEs with either moving or unbounded domains are presented in connection with the classical and the new notion of a.c.s., while numerical tests and a list of open questions conclude the present work.","sentences":["In the current note we consider matrix-sequences $\\{B_{n,t}\\}_n$ of increasing sizes depending on $n$ and equipped with a parameter $t>0$. For every fixed $t>0$, we assume that each $\\{B_{n,t}\\}_n$ possesses a canonical spectral/singular values symbol $f_t$ defined on $D_t\\subset \\R^{d}$ of finite measure, $d\\ge 1$.","Furthermore, we assume that $ \\{ \\{ B_{n,t}\\} : \\, t > 0 \\} $ is an approximating class of sequences (a.c.s.)","for $ \\{ A_n \\} $ and that $ \\bigcup_{t > 0} D_t = D $ with $ D_{t + 1} \\supset D_t $.","Under such assumptions and via the notion of a.c.s, we prove results on the canonical distributions of $ \\{ A_n \\} $, whose symbol, when it exists, can be defined on the, possibly unbounded, domain $D$ of finite or even infinite measure.   ","We then extend the concept of a.c.s. to the case where the approximating sequence $ \\{ B_{n,t}\\}_n $ has possibly a different dimension than the one of $ \\{ A_n\\} $.","This concept seems to be particularly natural when dealing, e.g., with the approximation both of a partial differential equation (PDE) and of its (possibly unbounded, or moving) domain $D$, using an exhausting sequence of domains $\\{ D_t \\}$.   Examples coming from approximated PDEs/FDEs with either moving or unbounded domains are presented in connection with the classical and the new notion of a.c.s., while numerical tests and a list of open questions conclude the present work."],"url":"http://arxiv.org/abs/2405.20150v1","category":"math.NA"}
{"created":"2024-05-30 15:15:12","title":"The key science drivers for the Atacama Large Aperture Submillimeter Telescope (AtLAST)","abstract":"Sub-mm and mm wavelengths provide a unique view of the Universe, from the gas and dust that fills and surrounds galaxies to the chromosphere of our own Sun. Current single-dish facilities have presented a tantalising view of the brightest (sub-)mm sources, and interferometers have provided the exquisite resolution necessary to analyse the details in small fields, but there are still many open questions that cannot be answered with current facilities: Where are all the baryons? How do structures interact with their environments? What does the time-varying (sub-)mm sky look like? In order to make major advances on these questions and others, what is needed now is a facility capable of rapidly mapping the sky spatially, spectrally, and temporally, which can only be done by a high throughput, single-dish observatory. An extensive design study for this new facility is currently being undertaken. In this paper, we focus on the key science drivers and the requirements they place on the observatory. As a 50m single dish telescope with a 1-2{\\deg} field of view, the strength of the Atacama Large Aperture Submillimeter Telescope (AtLAST) is in science where a large field of view, highly multiplexed instrumentation and sensitivity to faint large-scale structure is important. AtLAST aims to be a sustainable, upgradeable, multipurpose facility that will deliver orders of magnitude increases in sensitivity and mapping speeds over current and planned telescopes.","sentences":["Sub-mm and mm wavelengths provide a unique view of the Universe, from the gas and dust that fills and surrounds galaxies to the chromosphere of our own Sun.","Current single-dish facilities have presented a tantalising view of the brightest (sub-)mm sources, and interferometers have provided the exquisite resolution necessary to analyse the details in small fields, but there are still many open questions that cannot be answered with current facilities: Where are all the baryons?","How do structures interact with their environments?","What does the time-varying (sub-)mm sky look like?","In order to make major advances on these questions and others, what is needed now is a facility capable of rapidly mapping the sky spatially, spectrally, and temporally, which can only be done by a high throughput, single-dish observatory.","An extensive design study for this new facility is currently being undertaken.","In this paper, we focus on the key science drivers and the requirements they place on the observatory.","As a 50m single dish telescope with a 1-2{\\deg} field of view, the strength of the Atacama Large Aperture Submillimeter Telescope (AtLAST) is in science where a large field of view, highly multiplexed instrumentation and sensitivity to faint large-scale structure is important.","AtLAST aims to be a sustainable, upgradeable, multipurpose facility that will deliver orders of magnitude increases in sensitivity and mapping speeds over current and planned telescopes."],"url":"http://arxiv.org/abs/2405.20140v1","category":"astro-ph.IM"}
{"created":"2024-05-30 15:07:30","title":"Federated and Transfer Learning for Cancer Detection Based on Image Analysis","abstract":"This review article discusses the roles of federated learning (FL) and transfer learning (TL) in cancer detection based on image analysis. These two strategies powered by machine learning have drawn a lot of attention due to their potential to increase the precision and effectiveness of cancer diagnosis in light of the growing importance of machine learning techniques in cancer detection. FL enables the training of machine learning models on data distributed across multiple sites without the need for centralized data sharing, while TL allows for the transfer of knowledge from one task to another. A comprehensive assessment of the two methods, including their strengths, and weaknesses is presented. Moving on, their applications in cancer detection are discussed, including potential directions for the future. Finally, this article offers a thorough description of the functions of TL and FL in image-based cancer detection. The authors also make insightful suggestions for additional study in this rapidly developing area.","sentences":["This review article discusses the roles of federated learning (FL) and transfer learning (TL) in cancer detection based on image analysis.","These two strategies powered by machine learning have drawn a lot of attention due to their potential to increase the precision and effectiveness of cancer diagnosis in light of the growing importance of machine learning techniques in cancer detection.","FL enables the training of machine learning models on data distributed across multiple sites without the need for centralized data sharing, while TL allows for the transfer of knowledge from one task to another.","A comprehensive assessment of the two methods, including their strengths, and weaknesses is presented.","Moving on, their applications in cancer detection are discussed, including potential directions for the future.","Finally, this article offers a thorough description of the functions of TL and FL in image-based cancer detection.","The authors also make insightful suggestions for additional study in this rapidly developing area."],"url":"http://arxiv.org/abs/2405.20126v1","category":"cs.CV"}
{"created":"2024-05-30 15:07:30","title":"SPAM: Stochastic Proximal Point Method with Momentum Variance Reduction for Non-convex Cross-Device Federated Learning","abstract":"Cross-device training is a crucial subfield of federated learning, where the number of clients can reach into the billions. Standard approaches and local methods are prone to issues such as client drift and insensitivity to data similarities. We propose a novel algorithm (SPAM) for cross-device federated learning with non-convex losses, which solves both issues. We provide sharp analysis under second-order (Hessian) similarity, a condition satisfied by a variety of machine learning problems in practice. Additionally, we extend our results to the partial participation setting, where a cohort of selected clients communicate with the server at each communication round. Our method is the first in its kind, that does not require the smoothness of the objective and provably benefits from clients having similar data.","sentences":["Cross-device training is a crucial subfield of federated learning, where the number of clients can reach into the billions.","Standard approaches and local methods are prone to issues such as client drift and insensitivity to data similarities.","We propose a novel algorithm (SPAM) for cross-device federated learning with non-convex losses, which solves both issues.","We provide sharp analysis under second-order (Hessian) similarity, a condition satisfied by a variety of machine learning problems in practice.","Additionally, we extend our results to the partial participation setting, where a cohort of selected clients communicate with the server at each communication round.","Our method is the first in its kind, that does not require the smoothness of the objective and provably benefits from clients having similar data."],"url":"http://arxiv.org/abs/2405.20127v1","category":"math.OC"}
{"created":"2024-05-30 15:02:51","title":"Haloes, other dark matter candidates and astrophysical implications","abstract":"It is possible that a multi-component dark matter model is required if primordial black holes only contribute to a fraction of the energy density in dark matter. This is increasingly more likely with respect to the case of $f_{\\rm PBH} = 1$, since there is only one remaining window, on asteroid-mass scales, where primordial black holes can make up all of the dark matter. A mixed dark matter model can lead to interesting observables that come about due to the interactions between primordial black holes and the second dark matter component. This can provide unique signatures of the presence of primordial black holes and increase the prospects of detection or improvement of constraints in the mass ranges where $f_{\\rm PBH} < 1$, whilst simultaneously exploring the remaining open parameter space for other dark matter candidates.","sentences":["It is possible that a multi-component dark matter model is required if primordial black holes only contribute to a fraction of the energy density in dark matter.","This is increasingly more likely with respect to the case of $f_{\\rm PBH} = 1$, since there is only one remaining window, on asteroid-mass scales, where primordial black holes can make up all of the dark matter.","A mixed dark matter model can lead to interesting observables that come about due to the interactions between primordial black holes and the second dark matter component.","This can provide unique signatures of the presence of primordial black holes and increase the prospects of detection or improvement of constraints in the mass ranges where $f_{\\rm PBH} < 1$, whilst simultaneously exploring the remaining open parameter space for other dark matter candidates."],"url":"http://arxiv.org/abs/2405.20125v1","category":"astro-ph.CO"}
{"created":"2024-05-30 14:56:25","title":"Lattice QCD calculation of the pion distribution amplitude with domain wall fermions at physical pion mass","abstract":"We present a direct lattice QCD calculation of the $x$-dependence of the pion distribution amplitude (DA), which is performed using the quasi-DA in large momentum effective theory on a domain-wall fermion ensemble at physical quark masses and spacing $a\\approx 0.084$ fm. The bare quais-DA matrix elements are renormalized in the hybrid scheme and matched to $\\overline{\\rm MS}$ with a subtraction of the leading renormalon in the Wilson-line mass. For the first time, we include threshold resummation in the perturbative matching onto the light-cone DA, which resums the large logarithms in the soft gluon limit at next-to-next-to-leading log. The resummed results show controlled scale-variation uncertainty within the range of momentum fraction $x\\in[0.25,0.75]$ at the largest pion momentum $P_z\\approx 1.85$~GeV. In addition, we apply the same analysis to quasi-DAs from a highly-improved-staggered-quark ensemble at physical pion mass and $a=0.076$ fm. By comparison we find with $2\\sigma$ confidence level that the DA obtained from chiral fermions is flatter and lower near $x=0.5$.","sentences":["We present a direct lattice QCD calculation of the $x$-dependence of the pion distribution amplitude (DA), which is performed using the quasi-DA in large momentum effective theory on a domain-wall fermion ensemble at physical quark masses and spacing $a\\approx 0.084$ fm.","The bare quais-DA matrix elements are renormalized in the hybrid scheme and matched to $\\overline{\\rm MS}$ with a subtraction of the leading renormalon in the Wilson-line mass.","For the first time, we include threshold resummation in the perturbative matching onto the light-cone DA, which resums the large logarithms in the soft gluon limit at next-to-next-to-leading log.","The resummed results show controlled scale-variation uncertainty within the range of momentum fraction $x\\in[0.25,0.75]$ at the largest pion momentum $P_z\\approx 1.85$~GeV. In addition, we apply the same analysis to quasi-DAs from a highly-improved-staggered-quark ensemble at physical pion mass and $a=0.076$ fm.","By comparison we find with $2\\sigma$ confidence level that the DA obtained from chiral fermions is flatter and lower near $x=0.5$."],"url":"http://arxiv.org/abs/2405.20120v1","category":"hep-lat"}
{"created":"2024-05-30 14:55:47","title":"Strain-Induced Changes of Electronic and Optical Properties of Zr-based MXenes","abstract":"Zr-based MXenes recently attracted attention because of its experimental preparation showing temperature stability, mechanical strength, and promising energy, sensoric, and electrochemistry applications. However, necessary theoretical predictions at a precise/predictive level are complicated due to essential excitonic features and strong electron correlation (i.e., a necessity to go beyond standard density functional theory, DFT). Contrary to the prevailing focus on oxygen-terminated MXenes and standard predictions of other Zr-based MXenes as conductors, based on the hybrid DFT and GW many-body perturbational theory, we were able to find seven different semiconductors (five of them for their equilibrium geometry and two others under slight tensile biaxial strain) in the case of two- and three-layered Zr$_2$CT$_2$ and Zr$_3$C$_2$T$_2$ configurations with various terminations (T = O, F, S, Cl). We observed semiconductor-to-conductor transition induced by strain in the majority of such Zr-based MXenes at experimentally achievable strain range. Furthermore, using the Bethe-Salpeter equation (BSE), we demonstrated that selected semiconducting Zr-based MXenes possess high optical absorption efficiency (20-30%) in the visible light range, underscoring their potential in photonic applications. The high sensitivity of Zr-based MXenes to external conditions and functionalization combined with the thermal stability make the materials promising for applications at operational temperatures in electronic and optical technologies.","sentences":["Zr-based MXenes recently attracted attention because of its experimental preparation showing temperature stability, mechanical strength, and promising energy, sensoric, and electrochemistry applications.","However, necessary theoretical predictions at a precise/predictive level are complicated due to essential excitonic features and strong electron correlation (i.e., a necessity to go beyond standard density functional theory, DFT).","Contrary to the prevailing focus on oxygen-terminated MXenes and standard predictions of other Zr-based MXenes as conductors, based on the hybrid DFT and GW many-body perturbational theory, we were able to find seven different semiconductors (five of them for their equilibrium geometry and two others under slight tensile biaxial strain) in the case of two- and three-layered Zr$_2$CT$_2$ and Zr$_3$C$_2$T$_2$ configurations with various terminations (T = O, F, S, Cl).","We observed semiconductor-to-conductor transition induced by strain in the majority of such Zr-based MXenes at experimentally achievable strain range.","Furthermore, using the Bethe-Salpeter equation (BSE), we demonstrated that selected semiconducting Zr-based MXenes possess high optical absorption efficiency (20-30%) in the visible light range, underscoring their potential in photonic applications.","The high sensitivity of Zr-based MXenes to external conditions and functionalization combined with the thermal stability make the materials promising for applications at operational temperatures in electronic and optical technologies."],"url":"http://arxiv.org/abs/2405.20119v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 14:16:32","title":"Analysis of a multi-target linear shrinkage covariance estimator","abstract":"Multi-target linear shrinkage is an extension of the standard single-target linear shrinkage for covariance estimation. We combine several constant matrices - the targets - with the sample covariance matrix. We derive the oracle and a \\textit{bona fide} multi-target linear shrinkage estimator with exact and empirical mean. In both settings, we proved its convergence towards the oracle under Kolmogorov asymptotics. Finally, we show empirically that it outperforms other standard estimators in various situations.","sentences":["Multi-target linear shrinkage is an extension of the standard single-target linear shrinkage for covariance estimation.","We combine several constant matrices - the targets - with the sample covariance matrix.","We derive the oracle and a \\textit{bona fide} multi-target linear shrinkage estimator with exact and empirical mean.","In both settings, we proved its convergence towards the oracle under Kolmogorov asymptotics.","Finally, we show empirically that it outperforms other standard estimators in various situations."],"url":"http://arxiv.org/abs/2405.20086v1","category":"math.ST"}
{"created":"2024-05-30 13:48:01","title":"Detailed nuclear structure calculations for coherent elastic neutrino-nucleus scattering","abstract":"Any discovery of `new physics' in the neutrino sector first requires a precise prediction of the expected Standard Model cross section. Currently, Coherent Elastic neutrino-Nucleus Scattering (CEvNS) experiments are statistics limited. However, as new and future experiments scale up, it will be necessary to improve the theoretical predictions. Here we review the calculation of the CEvNS cross section in a consistent theory of hadronic currents and compute the relevant nuclear form factors using the nuclear shell model. The uncertainty on the form factors is explored by repeating the calculation for various shell model interactions and with Skyme-Hartree-Fock evaluations of the Weak-charge radii. We then refine the Standard Model predictions for the recent experimental results of the COHERENT experiment. We find that our cross sections are in good agreement with previous predictions, but with significantly smaller uncertainties - by up to a factor of 10. Near-future CEvNS experiments will meaningfully benefit from improved predictions through an increased sensitivity to new-physics signals.","sentences":["Any discovery of `new physics' in the neutrino sector first requires a precise prediction of the expected Standard Model cross section.","Currently, Coherent Elastic neutrino-Nucleus Scattering (CEvNS) experiments are statistics limited.","However, as new and future experiments scale up, it will be necessary to improve the theoretical predictions.","Here we review the calculation of the CEvNS cross section in a consistent theory of hadronic currents and compute the relevant nuclear form factors using the nuclear shell model.","The uncertainty on the form factors is explored by repeating the calculation for various shell model interactions and with Skyme-Hartree-Fock evaluations of the Weak-charge radii.","We then refine the Standard Model predictions for the recent experimental results of the COHERENT experiment.","We find that our cross sections are in good agreement with previous predictions, but with significantly smaller uncertainties - by up to a factor of 10.","Near-future CEvNS experiments will meaningfully benefit from improved predictions through an increased sensitivity to new-physics signals."],"url":"http://arxiv.org/abs/2405.20060v1","category":"hep-ph"}
{"created":"2024-05-30 13:20:24","title":"Search for non-resonant Higgs boson pair production in final states with leptons, taus, and photons in $pp$ collisions at $\\sqrt{s}$ = 13 TeV with the ATLAS detector","abstract":"A search is presented for non-resonant Higgs boson pair production, targeting the $bbZZ$, 4$V$ ($V$ = $W$ or $Z$), $VV\\tau\\tau$, 4$\\tau$, $\\gamma\\gamma VV$ and $\\gamma\\gamma\\tau\\tau$ decay channels. Events are categorised based on the multiplicity of light charged leptons (electrons or muons), hadronically decaying tau leptons, and photons. The search is based on a data sample of proton-proton collisions at $\\sqrt{s}$ = 13 TeV recorded with the ATLAS detector during Run 2 of the Large Hadron Collider, corresponding to an integrated luminosity of 140 fb$^{-1}$. No evidence of the signal is found and the observed (expected) upper limit on the cross-section for non-resonant Higgs boson pair production is determined to be 17 (11) times the Standard Model predicted cross-section at 95% confidence level under the background-only hypothesis. The observed (expected) constraints on the $HHH$ coupling modifier, $\\kappa_{\\lambda}$, are determined to be $-6.2 < \\kappa_{\\lambda} < 11.6$ ($-4.5 < \\kappa_{\\lambda} < 9.6$) at 95% confidence level, assuming the Standard Model for the expected limits and that new physics would only affect $\\kappa_{\\lambda}$.","sentences":["A search is presented for non-resonant Higgs boson pair production, targeting the $bbZZ$, 4$V$ ($V$ = $W$ or $Z$), $VV\\tau\\tau$, 4$\\tau$, $\\gamma\\gamma VV$ and $\\gamma\\gamma\\tau\\tau$ decay channels.","Events are categorised based on the multiplicity of light charged leptons (electrons or muons), hadronically decaying tau leptons, and photons.","The search is based on a data sample of proton-proton collisions at $\\sqrt{s}$ = 13 TeV recorded with the ATLAS detector during Run 2 of the Large Hadron Collider, corresponding to an integrated luminosity of 140 fb$^{-1}$. No evidence of the signal is found and the observed (expected) upper limit on the cross-section for non-resonant Higgs boson pair production is determined to be 17 (11) times the Standard Model predicted cross-section at 95% confidence level under the background-only hypothesis.","The observed (expected) constraints on the $HHH$ coupling modifier, $\\kappa_{\\lambda}$, are determined to be $-6.2 < \\kappa_{\\lambda} < 11.6$ ($-4.5 < \\kappa_{\\lambda} < 9.6$) at 95% confidence level, assuming the Standard Model for the expected limits and that new physics would only affect $\\kappa_{\\lambda}$."],"url":"http://arxiv.org/abs/2405.20040v1","category":"hep-ex"}
{"created":"2024-05-30 13:12:53","title":"SEA Cache: A Performance-Efficient Countermeasure for Contention-based Attacks","abstract":"Many cache designs have been proposed to guard against contention-based side-channel attacks. One well-known type of cache is the randomized remapping cache. Many randomized remapping caches provide fixed or over protection, which leads to permanent performance degradation, or they provide flexible protection, but sacrifice performance against strong contention-based attacks. To improve the secure cache design, we extend an existing secure cache design, CEASER-SH cache, and propose the SEA cache. The novel cache configurations in both caches are logical associativity, which allows the cache line to be placed not only in its mapped cache set but also in the subsequent cache sets. SEA cache allows each user or each process to have a different local logical associativity. Hence, only those users or processes that request extra protection against contention-based attacks are protected with high logical associativity. Other users or processes can access the cache with lower latency and higher performance. Compared to a CEASER-SH cache with logical associativity of 8, an SEA cache with logical associativity of 1 for normal protection users and 16 for high protection users has a Cycles Per Instruction penalty that is about 0.6% less for users under normal protections and provides better security against contention-based attacks. Based on a 45nm technology library, and compared to a conventional cache, we estimate the power overhead is about 20% and the area overhead is 3.4%.","sentences":["Many cache designs have been proposed to guard against contention-based side-channel attacks.","One well-known type of cache is the randomized remapping cache.","Many randomized remapping caches provide fixed or over protection, which leads to permanent performance degradation, or they provide flexible protection, but sacrifice performance against strong contention-based attacks.","To improve the secure cache design, we extend an existing secure cache design, CEASER-SH cache, and propose the SEA cache.","The novel cache configurations in both caches are logical associativity, which allows the cache line to be placed not only in its mapped cache set but also in the subsequent cache sets.","SEA cache allows each user or each process to have a different local logical associativity.","Hence, only those users or processes that request extra protection against contention-based attacks are protected with high logical associativity.","Other users or processes can access the cache with lower latency and higher performance.","Compared to a CEASER-SH cache with logical associativity of 8, an SEA cache with logical associativity of 1 for normal protection users and 16 for high protection users has a Cycles Per Instruction penalty that is about 0.6% less for users under normal protections and provides better security against contention-based attacks.","Based on a 45nm technology library, and compared to a conventional cache, we estimate the power overhead is about 20% and the area overhead is 3.4%."],"url":"http://arxiv.org/abs/2405.20027v1","category":"cs.CR"}
{"created":"2024-05-30 12:47:34","title":"Optical activity and phase transformations in \u03b3/\u03b2 Ga2O3 bilayers under annealing","abstract":"Gallium oxide (Ga2O3) can be crystallized in several polymorphs exhibiting different physical properties. In this work, polymorphic structures consisting of the cubic defective spinel (gamma) film on the top of the monoclinic (beta) substrate were fabricated by disorder-induced ordering, known to be a practical way to stack these polymorphs together. Such bilayer structures were annealed to investigate the optical properties and phase transformations. Specifically, photoluminescence and diffuse reflectance spectroscopies were combined with transmission electron microscopy, Rutherford backscattering/channeling spectrometry and x-ray diffraction to monitor the evolutions. As a result we observe a two-stage annealing kinetics in gamma/beta Ga2O3 bilayers associated with the epitaxial gamma-to-beta regrowth at the interface at temperatures below 700 {\\deg}C and a non-planar gamma-to-beta phase transformation starting at higher temperatures. Thus, the present data enhance understanding of the polymorphism in Ga2O3, interconnecting the phase transformation kinetics with the evolution of the optical properties.","sentences":["Gallium oxide (Ga2O3) can be crystallized in several polymorphs exhibiting different physical properties.","In this work, polymorphic structures consisting of the cubic defective spinel (gamma) film on the top of the monoclinic (beta) substrate were fabricated by disorder-induced ordering, known to be a practical way to stack these polymorphs together.","Such bilayer structures were annealed to investigate the optical properties and phase transformations.","Specifically, photoluminescence and diffuse reflectance spectroscopies were combined with transmission electron microscopy, Rutherford backscattering/channeling spectrometry and x-ray diffraction to monitor the evolutions.","As a result we observe a two-stage annealing kinetics in gamma/beta Ga2O3 bilayers associated with the epitaxial gamma-to-beta regrowth at the interface at temperatures below 700 {\\deg}C and a non-planar gamma-to-beta phase transformation starting at higher temperatures.","Thus, the present data enhance understanding of the polymorphism in Ga2O3, interconnecting the phase transformation kinetics with the evolution of the optical properties."],"url":"http://arxiv.org/abs/2405.20011v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 12:43:10","title":"SWIFT: A Monotonic, Flux-Form Semi-Lagrangian Tracer Transport Scheme for Flow with Large Courant Numbers","abstract":"Local conservation of mass and entropy are becoming increasingly desirable properties for modern numerical weather and climate models. This work presents a Flux-Form Semi-Lagrangian (FFSL) transport scheme, called SWIFT, that facilitates this conservation for tracer variables, whilst maintaining other vital properties such as preservation of a constant, monotonicity and positivity. Importantly, these properties all hold for large Courant numbers and multi-dimensional flow, making the scheme appropriate for use within a dynamical core which takes large time steps.   The SWIFT scheme presented here can be seen as an evolution of the FFSL methods of Leonard et al and Lin and Rood. Two-dimensional and three-dimensional schemes consist of a splitting into a sequence of one-dimensional calculations. The new SWIFT splitting presented here allows monotonic and positivity properties from the one-dimensional calculations to be inherited by the multi-dimensional scheme. These one-dimensional calculations involve separating the mass flux into terms that correspond to integer and fractional parts of the Courant number. Key to achieving conservation is coupling the transport of tracers to the transport of the fluid density, through re-use of the discrete mass flux that was calculated from the fluid density in the transport of the tracers. This work also describes how these properties can still be attained when the tracer is vertically-staggered from the density in a Charney-Phillips grid.","sentences":["Local conservation of mass and entropy are becoming increasingly desirable properties for modern numerical weather and climate models.","This work presents a Flux-Form Semi-Lagrangian (FFSL) transport scheme, called SWIFT, that facilitates this conservation for tracer variables, whilst maintaining other vital properties such as preservation of a constant, monotonicity and positivity.","Importantly, these properties all hold for large Courant numbers and multi-dimensional flow, making the scheme appropriate for use within a dynamical core which takes large time steps.   ","The SWIFT scheme presented here can be seen as an evolution of the FFSL methods of Leonard et al and Lin and Rood.","Two-dimensional and three-dimensional schemes consist of a splitting into a sequence of one-dimensional calculations.","The new SWIFT splitting presented here allows monotonic and positivity properties from the one-dimensional calculations to be inherited by the multi-dimensional scheme.","These one-dimensional calculations involve separating the mass flux into terms that correspond to integer and fractional parts of the Courant number.","Key to achieving conservation is coupling the transport of tracers to the transport of the fluid density, through re-use of the discrete mass flux that was calculated from the fluid density in the transport of the tracers.","This work also describes how these properties can still be attained when the tracer is vertically-staggered from the density in a Charney-Phillips grid."],"url":"http://arxiv.org/abs/2405.20006v1","category":"math.NA"}
{"created":"2024-05-30 12:41:00","title":"Prospect of measuring the top quark mass through energy correlators","abstract":"Reaching a high precision of the top quark mass is an important task of the Large Hadron Collider. We perform a feasibility study of measuring the top quark mass through the three-point energy correlator. The expected sensitivity of the top quark mass in the boosted regime is presented. We further introduce its application to the low top $p_\\text{T}$ regime and demonstrate that both the W boson and the top quark masses could be extracted from this single observable. Compared to traditional observables, the energy correlator shows robustness to uncertainties that usually dominate experimental measurements and provides a promising way to improve experimental precision.","sentences":["Reaching a high precision of the top quark mass is an important task of the Large Hadron Collider.","We perform a feasibility study of measuring the top quark mass through the three-point energy correlator.","The expected sensitivity of the top quark mass in the boosted regime is presented.","We further introduce its application to the low top $p_\\text{T}$ regime and demonstrate that both the W boson and the top quark masses could be extracted from this single observable.","Compared to traditional observables, the energy correlator shows robustness to uncertainties that usually dominate experimental measurements and provides a promising way to improve experimental precision."],"url":"http://arxiv.org/abs/2405.20001v1","category":"hep-ph"}
{"created":"2024-05-30 12:11:40","title":"Hydrodynamics of a hard-core non-polar active lattice gas","abstract":"We present a fluctuating hydrodynamic description of a non-polar active lattice gas model with excluded volume interactions that exhibits motility-induced phase separation under appropriate conditions. For quasi-one dimension and higher, stability analysis of the noiseless hydrodynamics gives quantitative bounds on the phase boundary of the motility-induced phase separation in terms of spinodal and binodal. Inclusion of the multiplicative noise in the fluctuating hydrodynamics describes the exponentially decaying two-point correlations in the stationary-state homogeneous phase. Our hydrodynamic description and theoretical predictions based on it are in excellent agreement with our Monte-Carlo simulations and pseudo-spectral iteration of the hydrodynamics equations. Our construction of hydrodynamics for this model is not suitable in strictly one-dimension with single-file constraints, and we argue that this breakdown is associated with micro-phase separation.","sentences":["We present a fluctuating hydrodynamic description of a non-polar active lattice gas model with excluded volume interactions that exhibits motility-induced phase separation under appropriate conditions.","For quasi-one dimension and higher, stability analysis of the noiseless hydrodynamics gives quantitative bounds on the phase boundary of the motility-induced phase separation in terms of spinodal and binodal.","Inclusion of the multiplicative noise in the fluctuating hydrodynamics describes the exponentially decaying two-point correlations in the stationary-state homogeneous phase.","Our hydrodynamic description and theoretical predictions based on it are in excellent agreement with our Monte-Carlo simulations and pseudo-spectral iteration of the hydrodynamics equations.","Our construction of hydrodynamics for this model is not suitable in strictly one-dimension with single-file constraints, and we argue that this breakdown is associated with micro-phase separation."],"url":"http://arxiv.org/abs/2405.19984v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-30 11:59:58","title":"Consistent Submodular Maximization","abstract":"Maximizing monotone submodular functions under cardinality constraints is a classic optimization task with several applications in data mining and machine learning. In this paper we study this problem in a dynamic environment with consistency constraints: elements arrive in a streaming fashion and the goal is maintaining a constant approximation to the optimal solution while having a stable solution (i.e., the number of changes between two consecutive solutions is bounded). We provide algorithms in this setting with different trade-offs between consistency and approximation quality. We also complement our theoretical results with an experimental analysis showing the effectiveness of our algorithms in real-world instances.","sentences":["Maximizing monotone submodular functions under cardinality constraints is a classic optimization task with several applications in data mining and machine learning.","In this paper we study this problem in a dynamic environment with consistency constraints: elements arrive in a streaming fashion and the goal is maintaining a constant approximation to the optimal solution while having a stable solution (i.e., the number of changes between two consecutive solutions is bounded).","We provide algorithms in this setting with different trade-offs between consistency and approximation quality.","We also complement our theoretical results with an experimental analysis showing the effectiveness of our algorithms in real-world instances."],"url":"http://arxiv.org/abs/2405.19977v1","category":"cs.DS"}
{"created":"2024-05-30 11:57:26","title":"Comet 81/P Wild 2: changes in the spin axis orientation during the last five apparitions","abstract":"Comet 81P (Wild 2) is characterized by the presence of a prominent-fan shaped dust emission originating from an active source at high latitude on the nucleus, whose axis is assumed to coincide with the comet's rotation axis. Therefore, several authors estimated the spin axis orientation of 81P in past apparitions based on the polar jet model. By measuring the PAs of the fan on CCD images taken with different telescopes during the 2009-10 and 2022-23 apparitions, we estimated a position of the comet's spin axis at RA=295.0{\\deg}$\\pm$ 7.5{\\deg}, Dec=14.5{\\deg}$\\pm$ 4.0{\\deg} for the 2009-10 apparition and at RA=296.7{\\deg}$\\pm$ 2.0{\\deg}, Dec=17.3{\\deg}$\\pm$ 2.5{\\deg} for the 2022-23 apparition. Despite some degree of uncertainty of the estimate for the 2009-10 apparition, we interpolated the estimate for 2009-10 and 2022-23 with the published data of the previous apparition of 1997, to assess the presence and the extent of a drift of the pole since the 1997 passage. The analysis over a long time span of five consecutive apparitions confirms previous observations that the spin axis of comet 81P is subject to a slow drift with variable rate, probably connected to outgassing-induced jet forces and the related non-gravitational perturbations of its orbital period.","sentences":["Comet 81P (Wild 2) is characterized by the presence of a prominent-fan shaped dust emission originating from an active source at high latitude on the nucleus, whose axis is assumed to coincide with the comet's rotation axis.","Therefore, several authors estimated the spin axis orientation of 81P in past apparitions based on the polar jet model.","By measuring the PAs of the fan on CCD images taken with different telescopes during the 2009-10 and 2022-23 apparitions, we estimated a position of the comet's spin axis at RA=295.0{\\deg}$\\pm$ 7.5{\\deg}, Dec=14.5{\\deg}$\\pm$ 4.0{\\deg} for the 2009-10 apparition and at RA=296.7{\\deg}$\\pm$ 2.0{\\deg}, Dec=17.3{\\deg}$\\pm$ 2.5{\\deg} for the 2022-23 apparition.","Despite some degree of uncertainty of the estimate for the 2009-10 apparition, we interpolated the estimate for 2009-10 and 2022-23 with the published data of the previous apparition of 1997, to assess the presence and the extent of a drift of the pole since the 1997 passage.","The analysis over a long time span of five consecutive apparitions confirms previous observations that the spin axis of comet 81P is subject to a slow drift with variable rate, probably connected to outgassing-induced jet forces and the related non-gravitational perturbations of its orbital period."],"url":"http://arxiv.org/abs/2405.19975v1","category":"astro-ph.EP"}
{"created":"2024-05-30 11:57:00","title":"A Triumvirate of AI Driven Theoretical Discovery","abstract":"Recent years have seen the dramatic rise of the usage of AI algorithms in pure mathematics and fundamental sciences such as theoretical physics. This is perhaps counter-intuitive since mathematical sciences require the rigorous definitions, derivations, and proofs, in contrast to the experimental sciences which rely on the modelling of data with error-bars. In this Perspective, we categorize the approaches to mathematical discovery as \"top-down\", \"bottom-up\" and \"meta-mathematics\", as inspired by historical examples. We review some of the progress over the last few years, comparing and contrasting both the advances and the short-comings in each approach. We argue that while the theorist is in no way in danger of being replaced by AI in the near future, the hybrid of human expertise and AI algorithms will become an integral part of theoretical discovery.","sentences":["Recent years have seen the dramatic rise of the usage of AI algorithms in pure mathematics and fundamental sciences such as theoretical physics.","This is perhaps counter-intuitive since mathematical sciences require the rigorous definitions, derivations, and proofs, in contrast to the experimental sciences which rely on the modelling of data with error-bars.","In this Perspective, we categorize the approaches to mathematical discovery as \"top-down\", \"bottom-up\" and \"meta-mathematics\", as inspired by historical examples.","We review some of the progress over the last few years, comparing and contrasting both the advances and the short-comings in each approach.","We argue that while the theorist is in no way in danger of being replaced by AI in the near future, the hybrid of human expertise and AI algorithms will become an integral part of theoretical discovery."],"url":"http://arxiv.org/abs/2405.19973v1","category":"math.HO"}
{"created":"2024-05-30 11:03:08","title":"Coherent Control of Spontaneous Emission for a giant driven $\u039b $-type three-level atom","abstract":"Quantum optics with giant atoms provides a new approach for implementing optical memory devices at the atomic scale. Here, we theoretically study the relaxation dynamics of a single driven three-level atom interacting with a one-dimensional waveguide, via two coupling points. Under certain conditions, after the long-time dynamics, we found that the population of giant atom can either maintain stable values or exhibit regular periodic oscillation behavior, while photons can be trapped in the region of giant atoms. This phenomenon is not achievable using a two-level atom with two legs. It is worth noting that the atomic excitation probability of a stable bound state is a constant value, which is determined by the size of the atom. Crucially, the size of the atom (the distance between the two coupling points) is much larger than the wavelength of the light field, which is a necessary condition for the existence of oscillating bound states.","sentences":["Quantum optics with giant atoms provides a new approach for implementing optical memory devices at the atomic scale.","Here, we theoretically study the relaxation dynamics of a single driven three-level atom interacting with a one-dimensional waveguide, via two coupling points.","Under certain conditions, after the long-time dynamics, we found that the population of giant atom can either maintain stable values or exhibit regular periodic oscillation behavior, while photons can be trapped in the region of giant atoms.","This phenomenon is not achievable using a two-level atom with two legs.","It is worth noting that the atomic excitation probability of a stable bound state is a constant value, which is determined by the size of the atom.","Crucially, the size of the atom (the distance between the two coupling points) is much larger than the wavelength of the light field, which is a necessary condition for the existence of oscillating bound states."],"url":"http://arxiv.org/abs/2405.19942v1","category":"quant-ph"}
{"created":"2024-05-30 10:59:15","title":"On the Uncertainty Principle for Metaplectic Transformations","abstract":"We explore the new proofs and extensions of the Heisenberg Uncertainty Principle introduced by A.~Widgerson & Y.~Widgerson in [MR4229152], developed in [MR4453622] by N.C.~Dias, F.~Luef and J.N.~Prata and also in [MR4337266] by Y.~Tang. In particular we give here a proof of the Uncertainty Principle for operators in the Metaplectic group in any dimension.","sentences":["We explore the new proofs and extensions of the Heisenberg Uncertainty Principle introduced by A.~Widgerson & Y.~Widgerson in [MR4229152], developed in [MR4453622] by N.C.~Dias, F.~Luef and J.N.~Prata and also in [MR4337266] by Y.~Tang.","In particular we give here a proof of the Uncertainty Principle for operators in the Metaplectic group in any dimension."],"url":"http://arxiv.org/abs/2405.19938v1","category":"math.SP"}
{"created":"2024-05-30 10:56:32","title":"Free-ranging dogs quickly learn to recognize a rewarding person","abstract":"Individual human recognition is important for species that live in close proximity to humans. Numerous studies on domesticated species and urban-adapted birds have highlighted this ability. One such species which is heavily reliant on humans is the free-ranging dog. Very little knowledge exists on the amount of time taken by free-ranging dogs to learn and remember individual humans. Due to their territorial nature, they have a high probability of encountering the same people multiple times on the streets. Being able to distinguish individual humans might be helpful in making decisions regarding people from whom to beg for food or social reward. We investigated if free-ranging dogs are capable of identifying the person rewarding them and the amount of time required for them to learn it. We conducted field trials on randomly selected adult free-ranging dogs in West Bengal, India. On Day 1, a choice test was conducted. The experimenter chosen did not provide reward while the other experimenter provided a piece of boiled chicken followed by petting. The person giving reward on Day 1 served as the correct choice on four subsequent days of training. Day 6 was the test day when none of the experimenters had a reward. We analyzed the choice made by the dogs, the time taken to approach during the choice tests, and the socialization index, which was calculated based on the intensity of affiliative behaviour shown towards the experimenters. The dogs made correct choices at a significantly higher rate on the fifth and sixth days, as compared to Day 2, suggesting learning. This is the first study aiming to understand the time taken for individual human recognition in free-ranging dogs and can serve as the scaffold for future studies to understand the dog-human relationship in open environments, like urban ecosystems.","sentences":["Individual human recognition is important for species that live in close proximity to humans.","Numerous studies on domesticated species and urban-adapted birds have highlighted this ability.","One such species which is heavily reliant on humans is the free-ranging dog.","Very little knowledge exists on the amount of time taken by free-ranging dogs to learn and remember individual humans.","Due to their territorial nature, they have a high probability of encountering the same people multiple times on the streets.","Being able to distinguish individual humans might be helpful in making decisions regarding people from whom to beg for food or social reward.","We investigated if free-ranging dogs are capable of identifying the person rewarding them and the amount of time required for them to learn it.","We conducted field trials on randomly selected adult free-ranging dogs in West Bengal, India.","On Day 1, a choice test was conducted.","The experimenter chosen did not provide reward while the other experimenter provided a piece of boiled chicken followed by petting.","The person giving reward on Day 1 served as the correct choice on four subsequent days of training.","Day 6 was the test day when none of the experimenters had a reward.","We analyzed the choice made by the dogs, the time taken to approach during the choice tests, and the socialization index, which was calculated based on the intensity of affiliative behaviour shown towards the experimenters.","The dogs made correct choices at a significantly higher rate on the fifth and sixth days, as compared to Day 2, suggesting learning.","This is the first study aiming to understand the time taken for individual human recognition in free-ranging dogs and can serve as the scaffold for future studies to understand the dog-human relationship in open environments, like urban ecosystems."],"url":"http://arxiv.org/abs/2405.19936v1","category":"q-bio.OT"}
{"created":"2024-05-30 10:30:44","title":"Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled Learning","abstract":"While Positive-Unlabeled (PU) learning is vital in many real-world scenarios, its application to graph data still remains under-explored. We unveil that a critical challenge for PU learning on graph lies on the edge heterophily, which directly violates the irreducibility assumption for Class-Prior Estimation (class prior is essential for building PU learning algorithms) and degenerates the latent label inference on unlabeled nodes during classifier training. In response to this challenge, we introduce a new method, named Graph PU Learning with Label Propagation Loss (GPL). Specifically, GPL considers learning from PU nodes along with an intermediate heterophily reduction, which helps mitigate the negative impact of the heterophilic structure. We formulate this procedure as a bilevel optimization that reduces heterophily in the inner loop and efficiently learns a classifier in the outer loop. Extensive experiments across a variety of datasets have shown that GPL significantly outperforms baseline methods, confirming its effectiveness and superiority.","sentences":["While Positive-Unlabeled (PU) learning is vital in many real-world scenarios, its application to graph data still remains under-explored.","We unveil that a critical challenge for PU learning on graph lies on the edge heterophily, which directly violates the irreducibility assumption for Class-Prior Estimation (class prior is essential for building PU learning algorithms) and degenerates the latent label inference on unlabeled nodes during classifier training.","In response to this challenge, we introduce a new method, named Graph PU Learning with Label Propagation Loss (GPL).","Specifically, GPL considers learning from PU nodes along with an intermediate heterophily reduction, which helps mitigate the negative impact of the heterophilic structure.","We formulate this procedure as a bilevel optimization that reduces heterophily in the inner loop and efficiently learns a classifier in the outer loop.","Extensive experiments across a variety of datasets have shown that GPL significantly outperforms baseline methods, confirming its effectiveness and superiority."],"url":"http://arxiv.org/abs/2405.19919v1","category":"cs.LG"}
{"created":"2024-05-30 10:29:34","title":"Integrated diode lasers for the generation of sub-GHz repetition rate frequency combs","abstract":"We demonstrate absorber-free passive and hybrid mode-locking at sub-GHz repetition rates using a hybrid integrated extended cavity diode laser around 1550 nm. The laser is based on InP as gain medium and a long Si$_3$N$_4$ feedback circuit, with three highly frequency selective microring resonators. The feedback resonators not only increases the cavity length up to 0.6 m to achieve sub-GHz repetition rates but also serve as a dispersive narrowband mirror for sharp spectral filtering, which enables Fourier domain mode-locking. We observe passive mode-locking with repetition rates below 500 MHz, with $\\approx$ 15 comb lines at around 0.2 mW total power. To stabilize the repetition rate, hybrid mode-locking is demonstrated by weak RF modulation of the diode current at frequencies around 500 MHz. The RF injection reduces the Lorentzian linewidth component from 8.9 kHz to a detection limited value around 300 mHz. To measure the locking range of the repetition rate, the injected RF frequency is tuned with regard to the passive mode-locking frequency and the injected RF power is varied. The locking range increases approximately as a square-root function of the injected RF power. At 1 mW injection a wide locking range of about 80 MHz is obtained. We observe the laser maintaining stable mode-locking also when the DC diode pump current is increased from 40 mA to 190 mA, provided that the cavity length is maintained constant with thermo-refractive tuning.","sentences":["We demonstrate absorber-free passive and hybrid mode-locking at sub-GHz repetition rates using a hybrid integrated extended cavity diode laser around 1550 nm.","The laser is based on InP as gain medium and a long Si$_3$N$_4$ feedback circuit, with three highly frequency selective microring resonators.","The feedback resonators not only increases the cavity length up to 0.6 m to achieve sub-GHz repetition rates but also serve as a dispersive narrowband mirror for sharp spectral filtering, which enables Fourier domain mode-locking.","We observe passive mode-locking with repetition rates below 500 MHz, with $\\approx$ 15 comb lines at around 0.2 mW total power.","To stabilize the repetition rate, hybrid mode-locking is demonstrated by weak RF modulation of the diode current at frequencies around 500 MHz.","The RF injection reduces the Lorentzian linewidth component from 8.9 kHz to a detection limited value around 300 mHz.","To measure the locking range of the repetition rate, the injected RF frequency is tuned with regard to the passive mode-locking frequency and the injected RF power is varied.","The locking range increases approximately as a square-root function of the injected RF power.","At 1 mW injection a wide locking range of about 80 MHz is obtained.","We observe the laser maintaining stable mode-locking also when the DC diode pump current is increased from 40 mA to 190 mA, provided that the cavity length is maintained constant with thermo-refractive tuning."],"url":"http://arxiv.org/abs/2405.19916v1","category":"physics.optics"}
{"created":"2024-05-30 10:23:16","title":"Robust Kernel Hypothesis Testing under Data Corruption","abstract":"We propose two general methods for constructing robust permutation tests under data corruption. The proposed tests effectively control the non-asymptotic type I error under data corruption, and we prove their consistency in power under minimal conditions. This contributes to the practical deployment of hypothesis tests for real-world applications with potential adversarial attacks. One of our methods inherently ensures differential privacy, further broadening its applicability to private data analysis. For the two-sample and independence settings, we show that our kernel robust tests are minimax optimal, in the sense that they are guaranteed to be non-asymptotically powerful against alternatives uniformly separated from the null in the kernel MMD and HSIC metrics at some optimal rate (tight with matching lower bound). Finally, we provide publicly available implementations and empirically illustrate the practicality of our proposed tests.","sentences":["We propose two general methods for constructing robust permutation tests under data corruption.","The proposed tests effectively control the non-asymptotic type I error under data corruption, and we prove their consistency in power under minimal conditions.","This contributes to the practical deployment of hypothesis tests for real-world applications with potential adversarial attacks.","One of our methods inherently ensures differential privacy, further broadening its applicability to private data analysis.","For the two-sample and independence settings, we show that our kernel robust tests are minimax optimal, in the sense that they are guaranteed to be non-asymptotically powerful against alternatives uniformly separated from the null in the kernel MMD and HSIC metrics at some optimal rate (tight with matching lower bound).","Finally, we provide publicly available implementations and empirically illustrate the practicality of our proposed tests."],"url":"http://arxiv.org/abs/2405.19912v1","category":"stat.ML"}
{"created":"2024-05-30 10:19:02","title":"DFT study of structural, electronic and optical properties of 2D MgO monolayer under bi-axial mechanical strain","abstract":"The structural, electronic, and dielectric (optical) properties of graphene-like 2D MgO monolayer have been explored through first-principles calculations under bi-axial tensile and compressive mechanical strain within a range of -10% to +10%. Our findings revealed that the pristine MgO monolayer is an indirect band gap semiconducting material and the semiconducting mature of MgO monolayer remains consistent under both compressive and tensile mechanical strain. This nature of MgO is confirmed through partial density of states (PDOS) as well as electronic band structure. PDOS exhibits the contribution of different atomic orbitals in bond formation and nature of bond, while band structure provides insight into electron transitions between energy levels of valance and conduction bands. All optical parameters (dielectric function, reflectivity, energy loss, refractive index, extinction coefficient and absorption) are plotted in an energy range 0-15 eV. Within this energy interval, MgO possesses the highest value of the refractive index (2.13) at 3.12 eV energy. Also, a detailed analysis of changes in the geometrical structure of MgO monolayer is provided.","sentences":["The structural, electronic, and dielectric (optical) properties of graphene-like 2D MgO monolayer have been explored through first-principles calculations under bi-axial tensile and compressive mechanical strain within a range of -10% to +10%.","Our findings revealed that the pristine MgO monolayer is an indirect band gap semiconducting material and the semiconducting mature of MgO monolayer remains consistent under both compressive and tensile mechanical strain.","This nature of MgO is confirmed through partial density of states (PDOS) as well as electronic band structure.","PDOS exhibits the contribution of different atomic orbitals in bond formation and nature of bond, while band structure provides insight into electron transitions between energy levels of valance and conduction bands.","All optical parameters (dielectric function, reflectivity, energy loss, refractive index, extinction coefficient and absorption) are plotted in an energy range 0-15 eV. Within this energy interval, MgO possesses the highest value of the refractive index (2.13) at 3.12 eV energy.","Also, a detailed analysis of changes in the geometrical structure of MgO monolayer is provided."],"url":"http://arxiv.org/abs/2405.19907v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 09:57:21","title":"Uncertainty relations for overcomplete measurements from generalized equiangular tight frames","abstract":"The current study aims to examine uncertainty relations for overcomplete measurements from generalized equiangular tight frames. Informationally overcomplete measurements are a valuable tool in quantum information processing, including tomography and state estimation. The maximal sets of mutually unbiased bases are the most common case of informationally overcomplete measurements. The existence of $d+1$ of mutually unbiased bases is proved for $d$ being a prime power. More general classes of informationally overcomplete measurements have been proposed for various purposes. Such measurements are typically characterized by some inner structure maintaining the required properties. It leads to restrictions imposed on generated probabilities. To apply the measurements of interest, these restrictions should be converted into information-theoretic terms. To describe the amount of uncertainty quantitatively, we use the Tsallis and R\\'{e}nyi entropies as well as probabilities of separate outcomes. The obtained results are based on estimation of the index of coincidence. The derived relations are briefly exemplified.","sentences":["The current study aims to examine uncertainty relations for overcomplete measurements from generalized equiangular tight frames.","Informationally overcomplete measurements are a valuable tool in quantum information processing, including tomography and state estimation.","The maximal sets of mutually unbiased bases are the most common case of informationally overcomplete measurements.","The existence of $d+1$ of mutually unbiased bases is proved for $d$ being a prime power.","More general classes of informationally overcomplete measurements have been proposed for various purposes.","Such measurements are typically characterized by some inner structure maintaining the required properties.","It leads to restrictions imposed on generated probabilities.","To apply the measurements of interest, these restrictions should be converted into information-theoretic terms.","To describe the amount of uncertainty quantitatively, we use the Tsallis and R\\'{e}nyi entropies as well as probabilities of separate outcomes.","The obtained results are based on estimation of the index of coincidence.","The derived relations are briefly exemplified."],"url":"http://arxiv.org/abs/2405.19900v1","category":"quant-ph"}
{"created":"2024-05-30 09:50:38","title":"Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts","abstract":"In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge intensive tasks, where retrieval augmented generation (RAG) can be of help. Nevertheless, existing retrieval augmented models typically use similarity as a bridge between queries and documents and follow a retrieve then read procedure. In this work, we argue that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation. To this end, we propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework. To begin with, beyond existing similarity oriented thought, we embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts. Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought. Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.","sentences":["In recent years, large language models (LLMs) have made remarkable achievements in various domains.","However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge intensive tasks, where retrieval augmented generation (RAG) can be of help.","Nevertheless, existing retrieval augmented models typically use similarity as a bridge between queries and documents and follow a retrieve then read procedure.","In this work, we argue that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation.","To this end, we propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework.","To begin with, beyond existing similarity oriented thought, we embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts.","Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought.","Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation.","Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag."],"url":"http://arxiv.org/abs/2405.19893v1","category":"cs.LG"}
{"created":"2024-05-30 09:43:59","title":"Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning","abstract":"Reinforcement learning is able to obtain generalized low-level robot policies on diverse robotics datasets in embodied learning scenarios, and Transformer has been widely used to model time-varying features. However, it still suffers from the issues of low data efficiency and high inference latency. In this paper, we propose to investigate the task from a new perspective of the frequency domain. We first observe that the energy density in the frequency domain of a robot's trajectory is mainly concentrated in the low-frequency part. Then, we present the Fourier Controller Network (FCNet), a new network that utilizes the Short-Time Fourier Transform (STFT) to extract and encode time-varying features through frequency domain interpolation. We further achieve parallel training and efficient recurrent inference by using FFT and Sliding DFT methods in the model architecture for real-time decision-making. Comprehensive analyses in both simulated (e.g., D4RL) and real-world environments (e.g., robot locomotion) demonstrate FCNet's substantial efficiency and effectiveness over existing methods such as Transformer, e.g., FCNet outperforms Transformer on multi-environmental robotics datasets of all types of sizes (from 1.9M to 120M). The project page and code can be found https://thkkk.github.io/fcnet.","sentences":["Reinforcement learning is able to obtain generalized low-level robot policies on diverse robotics datasets in embodied learning scenarios, and Transformer has been widely used to model time-varying features.","However, it still suffers from the issues of low data efficiency and high inference latency.","In this paper, we propose to investigate the task from a new perspective of the frequency domain.","We first observe that the energy density in the frequency domain of a robot's trajectory is mainly concentrated in the low-frequency part.","Then, we present the Fourier Controller Network (FCNet), a new network that utilizes the Short-Time Fourier Transform (STFT) to extract and encode time-varying features through frequency domain interpolation.","We further achieve parallel training and efficient recurrent inference by using FFT and Sliding DFT methods in the model architecture for real-time decision-making.","Comprehensive analyses in both simulated (e.g., D4RL) and real-world environments (e.g., robot locomotion) demonstrate FCNet's substantial efficiency and effectiveness over existing methods such as Transformer, e.g., FCNet outperforms Transformer on multi-environmental robotics datasets of all types of sizes (from 1.9M to 120M).","The project page and code can be found https://thkkk.github.io/fcnet."],"url":"http://arxiv.org/abs/2405.19885v1","category":"cs.LG"}
{"created":"2024-05-30 09:39:52","title":"Pressure-induced superconductivity in La$_{4}$Ni$_{3}$O$_{10+\u03b4}$ ($\u03b4$ = 0.04 and -0.01)","abstract":"The superconducting transition temperatures, $T_{\\mathrm{c}}$, of La$_{4}$Ni$_{3}$O$_{10+\\delta}$($\\delta$ = 0.04 and -0.01) were determined under various pressures up to 124.9 GPa by electrical resistance measurements with a diamond anvil cell. $T_{\\mathrm{c}}$ exhibits a strong dependence on oxygen content within the pressure range of approximately 20 GPa and 80 GPa. At 48.0 GPa, $T_{\\mathrm{c}}$ of La$_{4}$Ni$_{3}$O$_{10.04}$ peaks at 36 K, marking the highest $T_{\\mathrm{c}}$ reported thus far.","sentences":["The superconducting transition temperatures, $T_{\\mathrm{c}}$, of La$_{4}$Ni$_{3}$O$_{10+\\delta}$($\\delta$ = 0.04 and -0.01) were determined under various pressures up to 124.9 GPa by electrical resistance measurements with a diamond anvil cell.","$T_{\\mathrm{c}}$ exhibits a strong dependence on oxygen content within the pressure range of approximately 20 GPa and 80 GPa.","At 48.0 GPa, $T_{\\mathrm{c}}$ of La$_{4}$Ni$_{3}$O$_{10.04}$ peaks at 36 K, marking the highest $T_{\\mathrm{c}}$ reported thus far."],"url":"http://arxiv.org/abs/2405.19880v1","category":"cond-mat.supr-con"}
{"created":"2024-05-30 09:34:31","title":"Learning from Random Demonstrations: Offline Reinforcement Learning with Importance-Sampled Diffusion Models","abstract":"Generative models such as diffusion have been employed as world models in offline reinforcement learning to generate synthetic data for more effective learning. Existing work either generates diffusion models one-time prior to training or requires additional interaction data to update it. In this paper, we propose a novel approach for offline reinforcement learning with closed-loop policy evaluation and world-model adaptation. It iteratively leverages a guided diffusion world model to directly evaluate the offline target policy with actions drawn from it, and then performs an importance-sampled world model update to adaptively align the world model with the updated policy. We analyzed the performance of the proposed method and provided an upper bound on the return gap between our method and the real environment under an optimal policy. The result sheds light on various factors affecting learning performance. Evaluations in the D4RL environment show significant improvement over state-of-the-art baselines, especially when only random or medium-expertise demonstrations are available -- thus requiring improved alignment between the world model and offline policy evaluation.","sentences":["Generative models such as diffusion have been employed as world models in offline reinforcement learning to generate synthetic data for more effective learning.","Existing work either generates diffusion models one-time prior to training or requires additional interaction data to update it.","In this paper, we propose a novel approach for offline reinforcement learning with closed-loop policy evaluation and world-model adaptation.","It iteratively leverages a guided diffusion world model to directly evaluate the offline target policy with actions drawn from it, and then performs an importance-sampled world model update to adaptively align the world model with the updated policy.","We analyzed the performance of the proposed method and provided an upper bound on the return gap between our method and the real environment under an optimal policy.","The result sheds light on various factors affecting learning performance.","Evaluations in the D4RL environment show significant improvement over state-of-the-art baselines, especially when only random or medium-expertise demonstrations are available -- thus requiring improved alignment between the world model and offline policy evaluation."],"url":"http://arxiv.org/abs/2405.19878v1","category":"cs.LG"}
{"created":"2024-05-30 09:14:41","title":"Reduced Rank Regression for Mixed Predictor and Response Variables","abstract":"In this paper, we propose the generalized mixed reduced rank regression method, GMR$^3$ for short. GMR$^3$ is a regression method for a mix of numeric, binary and ordinal response variables. The predictor variables can be a mix of binary, nominal, ordinal, and numeric variables. For dealing with the categorical predictors we use optimal scaling. A majorization-minimization algorithm is derived for maximum likelihood estimation under a local independence assumption. We discuss in detail model selection for the dimensionality or rank, and the selection of predictor variables. We show an application of GMR$^3$ using the Eurobarometer Surveys data set of 2023.","sentences":["In this paper, we propose the generalized mixed reduced rank regression method, GMR$^3$ for short.","GMR$^3$ is a regression method for a mix of numeric, binary and ordinal response variables.","The predictor variables can be a mix of binary, nominal, ordinal, and numeric variables.","For dealing with the categorical predictors we use optimal scaling.","A majorization-minimization algorithm is derived for maximum likelihood estimation under a local independence assumption.","We discuss in detail model selection for the dimensionality or rank, and the selection of predictor variables.","We show an application of GMR$^3$ using the Eurobarometer Surveys data set of 2023."],"url":"http://arxiv.org/abs/2405.19865v1","category":"stat.ME"}
{"created":"2024-05-30 09:02:04","title":"Correlated Electronic Structure and Density-Wave Gap in Trilayer Nickelate La4Ni3O10","abstract":"The discovery of pressurized superconductivity at 80 K in La3Ni2O7 officially brings nickelates into the family of high-temperature superconductors, which gives rise to not only new insights but also mysteries in the strongly correlated superconductivity. More recently, the sibling compound La4Ni3O10 was also shown to be superconducting below about 25 K under pressure, further boosting the popularity of nickelates in the Ruddlesden-Popper phase. In this study, combining high-resolution angle-resolved photoemission spectroscopy and ab initio calculation, we systematically investigate the electronic structures of La4Ni3O10 at ambient pressure. We reveal a high resemblance of La4Ni3O10 with La3Ni2O7 in the orbital-dependent fermiology and electronic structure, suggesting a similar electronic correlation between the two compounds. The temperature-dependent measurements imply an orbital-dependent energy gap related to the density-wave transition in La4Ni3O10. By comparing the theoretical pressure-dependent electronic structure, clues about the superconducting high-pressure phase can be deduced from the ambient measurements, providing crucial information for deciphering the unconventional superconductivity in nickelates.","sentences":["The discovery of pressurized superconductivity at 80 K in La3Ni2O7 officially brings nickelates into the family of high-temperature superconductors, which gives rise to not only new insights but also mysteries in the strongly correlated superconductivity.","More recently, the sibling compound La4Ni3O10 was also shown to be superconducting below about 25 K under pressure, further boosting the popularity of nickelates in the Ruddlesden-Popper phase.","In this study, combining high-resolution angle-resolved photoemission spectroscopy and ab initio calculation, we systematically investigate the electronic structures of La4Ni3O10 at ambient pressure.","We reveal a high resemblance of La4Ni3O10 with La3Ni2O7 in the orbital-dependent fermiology and electronic structure, suggesting a similar electronic correlation between the two compounds.","The temperature-dependent measurements imply an orbital-dependent energy gap related to the density-wave transition in La4Ni3O10.","By comparing the theoretical pressure-dependent electronic structure, clues about the superconducting high-pressure phase can be deduced from the ambient measurements, providing crucial information for deciphering the unconventional superconductivity in nickelates."],"url":"http://arxiv.org/abs/2405.19853v1","category":"cond-mat.supr-con"}
{"created":"2024-05-30 08:50:37","title":"How Gold to Make the Golden Snitch: Designing the \"Game Changer\" in Esports","abstract":"Many battling games utilize a special item (e.g. Roshan in Defense of the Ancients 2 (DOTA 2), Baron Nashor in League of Legends (LOL), Golden Snitch in Quidditch) as a potential ``Game Changer''. The reward of this item can enable the underdog to make a comeback. However, if the reward is excessively high, the whole game may devolve into a chase for the ``Game Changer''. Our research initiates with a Quidditch case study, a fictional sport in Harry Potter series, wherein we architect the Golden Snitch's reward to maximize the audience's surprise. Surprisingly, we discover that for equally competent teams, the optimal Snitch reward is zero. Moreover, we establish that under most circumstances the optimal score aligns with the game's expected duration multiplied by the teams' strength difference. Finally, we explore the correlation between the ``Game Changer's'' reward and audience surprise in Multiplayer Online Battle Arena (MOBA) games including DOTA 2 and LOL, finding that the optimal reward escalates with increasing team strength inequality.","sentences":["Many battling games utilize a special item (e.g. Roshan in Defense of the Ancients 2 (DOTA 2), Baron Nashor in League of Legends (LOL), Golden Snitch in Quidditch) as a potential ``Game Changer''.","The reward of this item can enable the underdog to make a comeback.","However, if the reward is excessively high, the whole game may devolve into a chase for the ``Game Changer''.","Our research initiates with a Quidditch case study, a fictional sport in Harry Potter series, wherein we architect the Golden Snitch's reward to maximize the audience's surprise.","Surprisingly, we discover that for equally competent teams, the optimal Snitch reward is zero.","Moreover, we establish that under most circumstances the optimal score aligns with the game's expected duration multiplied by the teams' strength difference.","Finally, we explore the correlation between the ``Game Changer's'' reward and audience surprise in Multiplayer Online Battle Arena (MOBA) games including DOTA 2 and LOL, finding that the optimal reward escalates with increasing team strength inequality."],"url":"http://arxiv.org/abs/2405.19843v1","category":"cs.GT"}
{"created":"2024-05-30 08:44:41","title":"A structured L-BFGS method with diagonal scaling and its application to image registration","abstract":"We devise an L-BFGS method for optimization problems in which the objective is the sum of two functions, where the Hessian of the first function is computationally unavailable while the Hessian of the second function has a computationally available approximation that allows for cheap matrix-vector products. This is a prototypical setting for many inverse problems. The proposed L-BFGS method exploits the structure of the objective to construct a more accurate Hessian approximation than in standard L-BFGS. In contrast to existing works on structured L-BFGS, we choose the first part of the seed matrix, which approximates the Hessian of the first function, as a diagonal matrix rather than a multiple of the identity. We derive two suitable formulas for the coefficients of the diagonal matrix and show that this boosts performance on real-life image registration problems, which are highly non-convex inverse problems. The new method converges globally and linearly on non-convex problems under mild assumptions in a general Hilbert space setting, making it applicable to a broad class of inverse problems. An implementation of the method is freely available.","sentences":["We devise an L-BFGS method for optimization problems in which the objective is the sum of two functions, where the Hessian of the first function is computationally unavailable while the Hessian of the second function has a computationally available approximation that allows for cheap matrix-vector products.","This is a prototypical setting for many inverse problems.","The proposed L-BFGS method exploits the structure of the objective to construct a more accurate Hessian approximation than in standard L-BFGS.","In contrast to existing works on structured L-BFGS, we choose the first part of the seed matrix, which approximates the Hessian of the first function, as a diagonal matrix rather than a multiple of the identity.","We derive two suitable formulas for the coefficients of the diagonal matrix and show that this boosts performance on real-life image registration problems, which are highly non-convex inverse problems.","The new method converges globally and linearly on non-convex problems under mild assumptions in a general Hilbert space setting, making it applicable to a broad class of inverse problems.","An implementation of the method is freely available."],"url":"http://arxiv.org/abs/2405.19834v1","category":"math.OC"}
{"created":"2024-05-30 08:44:12","title":"KITRO: Refining Human Mesh by 2D Clues and Kinematic-tree Rotation","abstract":"2D keypoints are commonly used as an additional cue to refine estimated 3D human meshes. Current methods optimize the pose and shape parameters with a reprojection loss on the provided 2D keypoints. Such an approach, while simple and intuitive, has limited effectiveness because the optimal solution is hard to find in ambiguous parameter space and may sacrifice depth. Additionally, divergent gradients from distal joints complicate and deviate the refinement of proximal joints in the kinematic chain. To address these, we introduce Kinematic-Tree Rotation (KITRO), a novel mesh refinement strategy that explicitly models depth and human kinematic-tree structure. KITRO treats refinement from a bone-wise perspective. Unlike previous methods which perform gradient-based optimizations, our method calculates bone directions in closed form. By accounting for the 2D pose, bone length, and parent joint's depth, the calculation results in two possible directions for each child joint. We then use a decision tree to trace binary choices for all bones along the human skeleton's kinematic-tree to select the most probable hypothesis. Our experiments across various datasets and baseline models demonstrate that KITRO significantly improves 3D joint estimation accuracy and achieves an ideal 2D fit simultaneously. Our code available at: https://github.com/MartaYang/KITRO.","sentences":["2D keypoints are commonly used as an additional cue to refine estimated 3D human meshes.","Current methods optimize the pose and shape parameters with a reprojection loss on the provided 2D keypoints.","Such an approach, while simple and intuitive, has limited effectiveness because the optimal solution is hard to find in ambiguous parameter space and may sacrifice depth.","Additionally, divergent gradients from distal joints complicate and deviate the refinement of proximal joints in the kinematic chain.","To address these, we introduce Kinematic-Tree Rotation (KITRO), a novel mesh refinement strategy that explicitly models depth and human kinematic-tree structure.","KITRO treats refinement from a bone-wise perspective.","Unlike previous methods which perform gradient-based optimizations, our method calculates bone directions in closed form.","By accounting for the 2D pose, bone length, and parent joint's depth, the calculation results in two possible directions for each child joint.","We then use a decision tree to trace binary choices for all bones along the human skeleton's kinematic-tree to select the most probable hypothesis.","Our experiments across various datasets and baseline models demonstrate that KITRO significantly improves 3D joint estimation accuracy and achieves an ideal 2D fit simultaneously.","Our code available at: https://github.com/MartaYang/KITRO."],"url":"http://arxiv.org/abs/2405.19833v1","category":"cs.CV"}
{"created":"2024-05-30 08:32:49","title":"Bayesian Uncertainty Quantification for Anaerobic Digestion models","abstract":"Uncertainty quantification is critical for ensuring adequate predictive power of computational models used in biology. Focusing on two anaerobic digestion models, this article introduces a novel generalized Bayesian procedure, called VarBUQ, ensuring a correct tradeoff between flexibility and computational cost. A benchmark against three existing methods (Fisher's information, bootstrapping and Beale's criteria) was conducted using synthetic data. This Bayesian procedure offered a good compromise between fitting ability and confidence estimation, while the other methods proved to be repeatedly overconfident. The method's performances notably benefitted from inductive bias brought by the prior distribution, although it requires careful construction. This article advocates for more systematic consideration of uncertainty for anaerobic digestion models and showcases a new, computationally efficient Bayesian method. To facilitate future implementations, a Python package called 'aduq' is made available.","sentences":["Uncertainty quantification is critical for ensuring adequate predictive power of computational models used in biology.","Focusing on two anaerobic digestion models, this article introduces a novel generalized Bayesian procedure, called VarBUQ, ensuring a correct tradeoff between flexibility and computational cost.","A benchmark against three existing methods (Fisher's information, bootstrapping and Beale's criteria) was conducted using synthetic data.","This Bayesian procedure offered a good compromise between fitting ability and confidence estimation, while the other methods proved to be repeatedly overconfident.","The method's performances notably benefitted from inductive bias brought by the prior distribution, although it requires careful construction.","This article advocates for more systematic consideration of uncertainty for anaerobic digestion models and showcases a new, computationally efficient Bayesian method.","To facilitate future implementations, a Python package called 'aduq' is made available."],"url":"http://arxiv.org/abs/2405.19824v1","category":"q-bio.BM"}
{"created":"2024-05-30 08:26:47","title":"Gated Fields: Learning Scene Reconstruction from Gated Videos","abstract":"Reconstructing outdoor 3D scenes from temporal observations is a challenge that recent work on neural fields has offered a new avenue for. However, existing methods that recover scene properties, such as geometry, appearance, or radiance, solely from RGB captures often fail when handling poorly-lit or texture-deficient regions. Similarly, recovering scenes with scanning LiDAR sensors is also difficult due to their low angular sampling rate which makes recovering expansive real-world scenes difficult. Tackling these gaps, we introduce Gated Fields - a neural scene reconstruction method that utilizes active gated video sequences. To this end, we propose a neural rendering approach that seamlessly incorporates time-gated capture and illumination. Our method exploits the intrinsic depth cues in the gated videos, achieving precise and dense geometry reconstruction irrespective of ambient illumination conditions. We validate the method across day and night scenarios and find that Gated Fields compares favorably to RGB and LiDAR reconstruction methods. Our code and datasets are available at https://light.princeton.edu/gatedfields/.","sentences":["Reconstructing outdoor 3D scenes from temporal observations is a challenge that recent work on neural fields has offered a new avenue for.","However, existing methods that recover scene properties, such as geometry, appearance, or radiance, solely from RGB captures often fail when handling poorly-lit or texture-deficient regions.","Similarly, recovering scenes with scanning LiDAR sensors is also difficult due to their low angular sampling rate which makes recovering expansive real-world scenes difficult.","Tackling these gaps, we introduce Gated Fields - a neural scene reconstruction method that utilizes active gated video sequences.","To this end, we propose a neural rendering approach that seamlessly incorporates time-gated capture and illumination.","Our method exploits the intrinsic depth cues in the gated videos, achieving precise and dense geometry reconstruction irrespective of ambient illumination conditions.","We validate the method across day and night scenarios and find that Gated Fields compares favorably to RGB and LiDAR reconstruction methods.","Our code and datasets are available at https://light.princeton.edu/gatedfields/."],"url":"http://arxiv.org/abs/2405.19819v1","category":"cs.CV"}
{"created":"2024-05-30 08:17:00","title":"MetaCURL: Non-stationary Concave Utility Reinforcement Learning","abstract":"We explore online learning in episodic loop-free Markov decision processes on non-stationary environments (changing losses and probability transitions). Our focus is on the Concave Utility Reinforcement Learning problem (CURL), an extension of classical RL for handling convex performance criteria in state-action distributions induced by agent policies. While various machine learning problems can be written as CURL, its non-linearity invalidates traditional Bellman equations. Despite recent solutions to classical CURL, none address non-stationary MDPs. This paper introduces MetaCURL, the first CURL algorithm for non-stationary MDPs. It employs a meta-algorithm running multiple black-box algorithms instances over different intervals, aggregating outputs via a sleeping expert framework. The key hurdle is partial information due to MDP uncertainty. Under partial information on the probability transitions (uncertainty and non-stationarity coming only from external noise, independent of agent state-action pairs), we achieve optimal dynamic regret without prior knowledge of MDP changes. Unlike approaches for RL, MetaCURL handles full adversarial losses, not just stochastic ones. We believe our approach for managing non-stationarity with experts can be of interest to the RL community.","sentences":["We explore online learning in episodic loop-free Markov decision processes on non-stationary environments (changing losses and probability transitions).","Our focus is on the Concave Utility Reinforcement Learning problem (CURL), an extension of classical RL for handling convex performance criteria in state-action distributions induced by agent policies.","While various machine learning problems can be written as CURL, its non-linearity invalidates traditional Bellman equations.","Despite recent solutions to classical CURL, none address non-stationary MDPs.","This paper introduces MetaCURL, the first CURL algorithm for non-stationary MDPs.","It employs a meta-algorithm running multiple black-box algorithms instances over different intervals, aggregating outputs via a sleeping expert framework.","The key hurdle is partial information due to MDP uncertainty.","Under partial information on the probability transitions (uncertainty and non-stationarity coming only from external noise, independent of agent state-action pairs), we achieve optimal dynamic regret without prior knowledge of MDP changes.","Unlike approaches for RL, MetaCURL handles full adversarial losses, not just stochastic ones.","We believe our approach for managing non-stationarity with experts can be of interest to the RL community."],"url":"http://arxiv.org/abs/2405.19807v1","category":"cs.LG"}
{"created":"2024-05-30 08:00:28","title":"Trembling Motion of Exciton-Polaritons Close to the Rashba-Dresselhaus Regime","abstract":"We report the experimental emulation of trembling quantum motion, or Zitterbewegung, of exciton polaritons in a perovskite microcavity at room temperature. By introducing liquid crystal molecules into the microcavity, we achieve spinor states with synthetic Rashba-Dresselhaus spin-orbit coupling and tunable energy splitting. Under a resonant excitation, the polariton fluid exhibits clear trembling motion perpendicular to its flowing direction, accompanied by a unique spin pattern resembling interlocked fingers. Furthermore, leveraging on the sizable tunability of energy gaps by external electrical voltages, we observe the continuous transition of polariton Zitterbewegung from relativistic (small gaps) to non-relativistic (large gaps) regimes. Our findings pave the way for using exciton polaritons in the emulation of relativistic quantum physics.","sentences":["We report the experimental emulation of trembling quantum motion, or Zitterbewegung, of exciton polaritons in a perovskite microcavity at room temperature.","By introducing liquid crystal molecules into the microcavity, we achieve spinor states with synthetic Rashba-Dresselhaus spin-orbit coupling and tunable energy splitting.","Under a resonant excitation, the polariton fluid exhibits clear trembling motion perpendicular to its flowing direction, accompanied by a unique spin pattern resembling interlocked fingers.","Furthermore, leveraging on the sizable tunability of energy gaps by external electrical voltages, we observe the continuous transition of polariton Zitterbewegung from relativistic (small gaps) to non-relativistic (large gaps) regimes.","Our findings pave the way for using exciton polaritons in the emulation of relativistic quantum physics."],"url":"http://arxiv.org/abs/2405.19791v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 07:56:07","title":"Unidirectional charge orders induced by oxygen vacancies on SrTiO$_3$(001)","abstract":"The discovery of high-mobility two-dimensional electron gas and low carrier density superconductivity in multiple SrTiO$_3$-based heterostructures has stimulated intense interest in the surface properties of SrTiO$_3$. The recent discovery of high-T$_c$ superconductivity in the monolayer FeSe/SrTiO$_3$ aroused the upsurge and underscored the atomic precision probe of the surface structure. By performing atomically resolved cryogenic scanning tunneling microscopy/spectroscopy characterization on dual-TiO$_{2}$-${\\delta}$-terminated SrTiO$_3$(001) surfaces with ($\\sqrt{13}$ $\\times$ $\\sqrt{13}$), c(4 $\\times$ 2), mixed (2 $\\times$ 1), and (2 $\\times$ 2) reconstructions, we disclosed universally broken rotational symmetry and contrasting bias- and temperature-dependent electronic states for apical and equatorial oxygen sites. With the sequentially evolved surface reconstructions and simultaneously increasing equatorial oxygen vacancies, the surface anisotropy reduces, and the work function lowers. Intriguingly, unidirectional stripe orders appear on the c(4 $\\times$ 2) surface, whereas local (4 $\\times$ 4) order emerges and eventually forms long-range unidirectional c(4 $\\times$ 4) charge order on the (2 $\\times$ 2) surface. This work reveals robust unidirectional charge orders induced by oxygen vacancies due to strong and delicate electronic-lattice interaction under broken rotational symmetry, providing insights into understanding the complex behaviors in perovskite oxide-based heterostructures.","sentences":["The discovery of high-mobility two-dimensional electron gas and low carrier density superconductivity in multiple SrTiO$_3$-based heterostructures has stimulated intense interest in the surface properties of SrTiO$_3$. The recent discovery of high-T$_c$ superconductivity in the monolayer FeSe/SrTiO$_3$ aroused the upsurge and underscored the atomic precision probe of the surface structure.","By performing atomically resolved cryogenic scanning tunneling microscopy/spectroscopy characterization on dual-TiO$_{2}$-${\\delta}$-terminated SrTiO$_3$(001) surfaces with ($\\sqrt{13}$ $\\times$ $\\sqrt{13}$), c(4 $\\times$ 2), mixed (2 $\\times$ 1), and (2 $\\times$ 2) reconstructions, we disclosed universally broken rotational symmetry and contrasting bias- and temperature-dependent electronic states for apical and equatorial oxygen sites.","With the sequentially evolved surface reconstructions and simultaneously increasing equatorial oxygen vacancies, the surface anisotropy reduces, and the work function lowers.","Intriguingly, unidirectional stripe orders appear on the c(4 $\\times$ 2) surface, whereas local (4 $\\times$ 4) order emerges and eventually forms long-range unidirectional c(4 $\\times$ 4) charge order on the (2 $\\times$ 2) surface.","This work reveals robust unidirectional charge orders induced by oxygen vacancies due to strong and delicate electronic-lattice interaction under broken rotational symmetry, providing insights into understanding the complex behaviors in perovskite oxide-based heterostructures."],"url":"http://arxiv.org/abs/2405.19788v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 07:49:02","title":"Recurrent Deep Kernel Learning of Dynamical Systems","abstract":"Digital twins require computationally-efficient reduced-order models (ROMs) that can accurately describe complex dynamics of physical assets. However, constructing ROMs from noisy high-dimensional data is challenging. In this work, we propose a data-driven, non-intrusive method that utilizes stochastic variational deep kernel learning (SVDKL) to discover low-dimensional latent spaces from data and a recurrent version of SVDKL for representing and predicting the evolution of latent dynamics. The proposed method is demonstrated with two challenging examples -- a double pendulum and a reaction-diffusion system. Results show that our framework is capable of (i) denoising and reconstructing measurements, (ii) learning compact representations of system states, (iii) predicting system evolution in low-dimensional latent spaces, and (iv) quantifying modeling uncertainties.","sentences":["Digital twins require computationally-efficient reduced-order models (ROMs) that can accurately describe complex dynamics of physical assets.","However, constructing ROMs from noisy high-dimensional data is challenging.","In this work, we propose a data-driven, non-intrusive method that utilizes stochastic variational deep kernel learning (SVDKL) to discover low-dimensional latent spaces from data and a recurrent version of SVDKL for representing and predicting the evolution of latent dynamics.","The proposed method is demonstrated with two challenging examples -- a double pendulum and a reaction-diffusion system.","Results show that our framework is capable of (i) denoising and reconstructing measurements, (ii) learning compact representations of system states, (iii) predicting system evolution in low-dimensional latent spaces, and (iv) quantifying modeling uncertainties."],"url":"http://arxiv.org/abs/2405.19785v1","category":"cs.LG"}
{"created":"2024-05-30 07:44:31","title":"Automatic Graph Topology-Aware Transformer","abstract":"Existing efforts are dedicated to designing many topologies and graph-aware strategies for the graph Transformer, which greatly improve the model's representation capabilities. However, manually determining the suitable Transformer architecture for a specific graph dataset or task requires extensive expert knowledge and laborious trials. This paper proposes an evolutionary graph Transformer architecture search framework (EGTAS) to automate the construction of strong graph Transformers. We build a comprehensive graph Transformer search space with the micro-level and macro-level designs. EGTAS evolves graph Transformer topologies at the macro level and graph-aware strategies at the micro level. Furthermore, a surrogate model based on generic architectural coding is proposed to directly predict the performance of graph Transformers, substantially reducing the evaluation cost of evolutionary search. We demonstrate the efficacy of EGTAS across a range of graph-level and node-level tasks, encompassing both small-scale and large-scale graph datasets. Experimental results and ablation studies show that EGTAS can construct high-performance architectures that rival state-of-the-art manual and automated baselines.","sentences":["Existing efforts are dedicated to designing many topologies and graph-aware strategies for the graph Transformer, which greatly improve the model's representation capabilities.","However, manually determining the suitable Transformer architecture for a specific graph dataset or task requires extensive expert knowledge and laborious trials.","This paper proposes an evolutionary graph Transformer architecture search framework (EGTAS) to automate the construction of strong graph Transformers.","We build a comprehensive graph Transformer search space with the micro-level and macro-level designs.","EGTAS evolves graph Transformer topologies at the macro level and graph-aware strategies at the micro level.","Furthermore, a surrogate model based on generic architectural coding is proposed to directly predict the performance of graph Transformers, substantially reducing the evaluation cost of evolutionary search.","We demonstrate the efficacy of EGTAS across a range of graph-level and node-level tasks, encompassing both small-scale and large-scale graph datasets.","Experimental results and ablation studies show that EGTAS can construct high-performance architectures that rival state-of-the-art manual and automated baselines."],"url":"http://arxiv.org/abs/2405.19779v1","category":"cs.NE"}
{"created":"2024-05-30 07:41:07","title":"Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network","abstract":"Style transfer aims to render an image with the artistic features of a style image, while maintaining the original structure. Various methods have been put forward for this task, but some challenges still exist. For instance, it is difficult for CNN-based methods to handle global information and long-range dependencies between input images, for which transformer-based methods have been proposed. Although transformers can better model the relationship between content and style images, they require high-cost hardware and time-consuming inference. To address these issues, we design a novel transformer model that includes only the encoder, thus significantly reducing the computational cost. In addition, we also find that existing style transfer methods may lead to images under-stylied or missing content. In order to achieve better stylization, we design a content feature extractor and a style feature extractor, based on which pure content and style images can be fed to the transformer. Finally, we propose a novel network termed Puff-Net, i.e., pure content and style feature fusion network. Through qualitative and quantitative experiments, we demonstrate the advantages of our model compared to state-of-the-art ones in the literature.","sentences":["Style transfer aims to render an image with the artistic features of a style image, while maintaining the original structure.","Various methods have been put forward for this task, but some challenges still exist.","For instance, it is difficult for CNN-based methods to handle global information and long-range dependencies between input images, for which transformer-based methods have been proposed.","Although transformers can better model the relationship between content and style images, they require high-cost hardware and time-consuming inference.","To address these issues, we design a novel transformer model that includes only the encoder, thus significantly reducing the computational cost.","In addition, we also find that existing style transfer methods may lead to images under-stylied or missing content.","In order to achieve better stylization, we design a content feature extractor and a style feature extractor, based on which pure content and style images can be fed to the transformer.","Finally, we propose a novel network termed Puff-Net, i.e., pure content and style feature fusion network.","Through qualitative and quantitative experiments, we demonstrate the advantages of our model compared to state-of-the-art ones in the literature."],"url":"http://arxiv.org/abs/2405.19775v1","category":"cs.CV"}
{"created":"2024-05-30 07:34:05","title":"MIP-DD: A Delta Debugger for Mixed Integer Programming Solvers","abstract":"The recent performance improvements in mixed-integer programming (MIP) went along with a significantly increased complexity of the codes of MIP solvers, which poses challenges in fixing implementation errors. Traditionally, debugging in MIP solvers is done by either adding assertions, debug solution checks, or using a bidirectional debugger. Especially in larger instances, none of these approaches guarantees success since still a deep understanding of the code is required. In this paper, we introduce MIP-DD, a solver-independent tool, which is, to the best of our knowledge, the first open-source delta debugger for MIP. Delta debugging is a hypothesis-trial-result approach to isolate the cause of a solver failure. MIP-DD simplifies MIP instances while maintaining the undesired behavior and already supported and motivated fixes for many bugs in the SCIP releases 8.0.4, 8.1.0, and 9.0.0. This translates to an increase of approximately 71.% more bugfixes than in the same time period before and including some fixes of long-known issues. As we highlight in selected case studies, instances triggering fundamental bugs in SCIP can typically be reduced to a few variables and constraints in less than an hour. This makes it significantly easier to manually trace and check the solution process on the resulting simplified instances. A promising future application of MIP-DD is the analysis of performance bottlenecks, which could very well benefit from simple adversarial instances.","sentences":["The recent performance improvements in mixed-integer programming (MIP) went along with a significantly increased complexity of the codes of MIP solvers, which poses challenges in fixing implementation errors.","Traditionally, debugging in MIP solvers is done by either adding assertions, debug solution checks, or using a bidirectional debugger.","Especially in larger instances, none of these approaches guarantees success since still a deep understanding of the code is required.","In this paper, we introduce MIP-DD, a solver-independent tool, which is, to the best of our knowledge, the first open-source delta debugger for MIP.","Delta debugging is a hypothesis-trial-result approach to isolate the cause of a solver failure.","MIP-DD simplifies MIP instances while maintaining the undesired behavior and already supported and motivated fixes for many bugs in the SCIP releases 8.0.4, 8.1.0, and 9.0.0.","This translates to an increase of approximately 71.% more bugfixes than in the same time period before and including some fixes of long-known issues.","As we highlight in selected case studies, instances triggering fundamental bugs in SCIP can typically be reduced to a few variables and constraints in less than an hour.","This makes it significantly easier to manually trace and check the solution process on the resulting simplified instances.","A promising future application of MIP-DD is the analysis of performance bottlenecks, which could very well benefit from simple adversarial instances."],"url":"http://arxiv.org/abs/2405.19770v1","category":"math.OC"}
{"created":"2024-05-30 07:11:20","title":"Identifiability of a statistical model with two latent vectors: Importance of the dimensionality relation and application to graph embedding","abstract":"Identifiability of statistical models is a key notion in unsupervised representation learning. Recent work of nonlinear independent component analysis (ICA) employs auxiliary data and has established identifiable conditions. This paper proposes a statistical model of two latent vectors with single auxiliary data generalizing nonlinear ICA, and establishes various identifiability conditions. Unlike previous work, the two latent vectors in the proposed model can have arbitrary dimensions, and this property enables us to reveal an insightful dimensionality relation among two latent vectors and auxiliary data in identifiability conditions. Furthermore, surprisingly, we prove that the indeterminacies of the proposed model has the same as \\emph{linear} ICA under certain conditions: The elements in the latent vector can be recovered up to their permutation and scales. Next, we apply the identifiability theory to a statistical model for graph data. As a result, one of the identifiability conditions includes an appealing implication: Identifiability of the statistical model could depend on the maximum value of link weights in graph data. Then, we propose a practical method for identifiable graph embedding. Finally, we numerically demonstrate that the proposed method well-recovers the latent vectors and model identifiability clearly depends on the maximum value of link weights, which supports the implication of our theoretical results","sentences":["Identifiability of statistical models is a key notion in unsupervised representation learning.","Recent work of nonlinear independent component analysis (ICA) employs auxiliary data and has established identifiable conditions.","This paper proposes a statistical model of two latent vectors with single auxiliary data generalizing nonlinear ICA, and establishes various identifiability conditions.","Unlike previous work, the two latent vectors in the proposed model can have arbitrary dimensions, and this property enables us to reveal an insightful dimensionality relation among two latent vectors and auxiliary data in identifiability conditions.","Furthermore, surprisingly, we prove that the indeterminacies of the proposed model has the same as \\emph{linear} ICA under certain conditions: The elements in the latent vector can be recovered up to their permutation and scales.","Next, we apply the identifiability theory to a statistical model for graph data.","As a result, one of the identifiability conditions includes an appealing implication: Identifiability of the statistical model could depend on the maximum value of link weights in graph data.","Then, we propose a practical method for identifiable graph embedding.","Finally, we numerically demonstrate that the proposed method well-recovers the latent vectors and model identifiability clearly depends on the maximum value of link weights, which supports the implication of our theoretical results"],"url":"http://arxiv.org/abs/2405.19760v1","category":"stat.ML"}
{"created":"2024-05-30 07:03:26","title":"Developing a Comprehensive Measurement Tool for Assessing the Rate of BIM Adoption in the Construction Industry","abstract":"Building Information Modeling (BIM) is a crucial technology in the construction industry, offering benefits such as enhanced collaboration, real-time decision-making, and significant cost and time savings. Despite its advantages, BIM adoption faces numerous barriers. This study aims to create a reliable tool to assess the Rate of BIM Adoption (RBA), drawing on Attributes of Innovation theory and empirical data from the literature. This research integrates theoretical insights with empirical data, providing quantitative items to measure BAR in the construction industry. The quantitative approach helps decision-makers and policymakers to mandate BIM and establish appropriate implementation standards. Its implications are significant for the construction industry, policymakers, and the academic community, offering a systematic approach to assess BIM adoption, identify barriers, and implement targeted strategies. The reliability of this approach is ensured through a solid theoretical foundation, item development, pilot testing, and statistical analysis, making it a valuable resource for improving BIM implementation and fostering innovation in the construction industry.","sentences":["Building Information Modeling (BIM) is a crucial technology in the construction industry, offering benefits such as enhanced collaboration, real-time decision-making, and significant cost and time savings.","Despite its advantages, BIM adoption faces numerous barriers.","This study aims to create a reliable tool to assess the Rate of BIM Adoption (RBA), drawing on Attributes of Innovation theory and empirical data from the literature.","This research integrates theoretical insights with empirical data, providing quantitative items to measure BAR in the construction industry.","The quantitative approach helps decision-makers and policymakers to mandate BIM and establish appropriate implementation standards.","Its implications are significant for the construction industry, policymakers, and the academic community, offering a systematic approach to assess BIM adoption, identify barriers, and implement targeted strategies.","The reliability of this approach is ensured through a solid theoretical foundation, item development, pilot testing, and statistical analysis, making it a valuable resource for improving BIM implementation and fostering innovation in the construction industry."],"url":"http://arxiv.org/abs/2405.19755v1","category":"cs.RO"}
{"created":"2024-05-30 06:24:14","title":"Two Optimizers Are Better Than One: LLM Catalyst for Enhancing Gradient-Based Optimization","abstract":"Learning a skill generally relies on both practical experience by doer and insightful high-level guidance by instructor. Will this strategy also work well for solving complex non-convex optimization problems? Here, a common gradient-based optimizer acts like a disciplined doer, making locally optimal update at each step. Recent methods utilize large language models (LLMs) to optimize solutions for concrete problems by inferring from natural language instructions, akin to a high-level instructor. In this paper, we show that these two optimizers are complementary to each other, suggesting a collaborative optimization approach. The gradient-based optimizer and LLM-based optimizer are combined in an interleaved manner. We instruct LLMs using task descriptions and timely optimization trajectories recorded during gradient-based optimization. Inferred results from LLMs are used as restarting points for the next stage of gradient optimization. By leveraging both the locally rigorous gradient-based optimizer and the high-level deductive LLM-based optimizer, our combined optimization method consistently yields improvements over competitive baseline prompt tuning methods. Our results demonstrate the synergistic effect of conventional gradient-based optimization and the inference ability of LLMs. The code is released at https://github.com/guozix/LLM-catalyst.","sentences":["Learning a skill generally relies on both practical experience by doer and insightful high-level guidance by instructor.","Will this strategy also work well for solving complex non-convex optimization problems?","Here, a common gradient-based optimizer acts like a disciplined doer, making locally optimal update at each step.","Recent methods utilize large language models (LLMs) to optimize solutions for concrete problems by inferring from natural language instructions, akin to a high-level instructor.","In this paper, we show that these two optimizers are complementary to each other, suggesting a collaborative optimization approach.","The gradient-based optimizer and LLM-based optimizer are combined in an interleaved manner.","We instruct LLMs using task descriptions and timely optimization trajectories recorded during gradient-based optimization.","Inferred results from LLMs are used as restarting points for the next stage of gradient optimization.","By leveraging both the locally rigorous gradient-based optimizer and the high-level deductive LLM-based optimizer, our combined optimization method consistently yields improvements over competitive baseline prompt tuning methods.","Our results demonstrate the synergistic effect of conventional gradient-based optimization and the inference ability of LLMs.","The code is released at https://github.com/guozix/LLM-catalyst."],"url":"http://arxiv.org/abs/2405.19732v1","category":"cs.CV"}
{"created":"2024-05-30 06:07:09","title":"Generalized Bayesian Nash Equilibrium with Continuous Type and Action Spaces","abstract":"Bayesian game is a strategic decision-making model where each player's type parameter characterizing its own objective is private information: each player knows its own type but not its rivals' types, and Bayesian Nash equilibrium (BNE) is an outcome of this game where each player makes a strategic optimal decision according to its own type under the Nash conjecture. In this paper, we advance the literature by considering a generalized Bayesian game where each player's action space depends on its own type parameter and the rivals' actions. This reflects the fact that in practical applications, a firm's feasible action is often related to its own type (e.g. marginal cost) and the rivals' actions (e.g. common resource constraints in a competitive market). Under some moderate conditions, we demonstrate existence of continuous generalized Bayesian Nash equilibria (GBNE) and uniqueness of such an equilibrium when each player's action space is only dependent on its type. In the case that each player's action space is also dependent on rivals' actions, we give a simple example to show that uniqueness of GBNE is not guaranteed under standard monotone conditions. To compute an approximate GBNE, we restrict each player's response function to the space of polynomial functions of its type parameter and consequently convert the GBNE problem to a stochastic generalized Nash equilibrium problem (SGNE). To justify the approximation, we discuss convergence of the approximation scheme. Some preliminary numerical test results show that the approximation scheme works well.","sentences":["Bayesian game is a strategic decision-making model where each player's type parameter characterizing its own objective is private information: each player knows its own type but not its rivals' types, and Bayesian Nash equilibrium (BNE) is an outcome of this game where each player makes a strategic optimal decision according to its own type under the Nash conjecture.","In this paper, we advance the literature by considering a generalized Bayesian game where each player's action space depends on its own type parameter and the rivals' actions.","This reflects the fact that in practical applications, a firm's feasible action is often related to its own type (e.g. marginal cost) and the rivals' actions (e.g. common resource constraints in a competitive market).","Under some moderate conditions, we demonstrate existence of continuous generalized Bayesian Nash equilibria (GBNE) and uniqueness of such an equilibrium when each player's action space is only dependent on its type.","In the case that each player's action space is also dependent on rivals' actions, we give a simple example to show that uniqueness of GBNE is not guaranteed under standard monotone conditions.","To compute an approximate GBNE, we restrict each player's response function to the space of polynomial functions of its type parameter and consequently convert the GBNE problem to a stochastic generalized Nash equilibrium problem (SGNE).","To justify the approximation, we discuss convergence of the approximation scheme.","Some preliminary numerical test results show that the approximation scheme works well."],"url":"http://arxiv.org/abs/2405.19721v1","category":"math.OC"}
{"created":"2024-05-30 06:06:13","title":"A study on star formation in rotating and magnetized filamentary molecular clouds","abstract":"A 2D dynamic model is utilized to investigate star formation in rotating filamentary molecular clouds (FMCs) amidst magnetic fields. The study reveals that the emergence of field stars is possible under both weak and strong magnetic fields due to the presence of low-density structures. The presence of a strong rotation in a strongly magnetized FMC Plays a crucial role in forming a combination of two binary pairs and two field stars. An intermediate-density structure in the presence of a moderate magnetic field can form a combination of binary stars and field stars, whereas, in very high dense filamentary molecular clouds, a low magnetic field may help to form binary stars or stellar associations. In a rapidly rotating and weakly magnetized highly dense cloud binary stars are formed, whereas, strong rotation and moderate magnetic field help to form stellar associations","sentences":["A 2D dynamic model is utilized to investigate star formation in rotating filamentary molecular clouds (FMCs) amidst magnetic fields.","The study reveals that the emergence of field stars is possible under both weak and strong magnetic fields due to the presence of low-density structures.","The presence of a strong rotation in a strongly magnetized FMC Plays a crucial role in forming a combination of two binary pairs and two field stars.","An intermediate-density structure in the presence of a moderate magnetic field can form a combination of binary stars and field stars, whereas, in very high dense filamentary molecular clouds, a low magnetic field may help to form binary stars or stellar associations.","In a rapidly rotating and weakly magnetized highly dense cloud binary stars are formed, whereas, strong rotation and moderate magnetic field help to form stellar associations"],"url":"http://arxiv.org/abs/2405.19720v1","category":"astro-ph.GA"}
{"created":"2024-05-30 05:41:45","title":"Optical Extinctions of Inter-Arm Molecular Clouds in M31: A Pilot Study for the Upcoming CSST Observations","abstract":"Recent sub-millimeter dust thermal emission observations have unveiled a significant number of inter-arm massive molecular clouds in M31.However,the effectiveness of this technique is limited to its sensitivity,making it challenging to study more distant galaxies.This study introduces an alternative approach,utilizing optical extinctions derived from space-based telescopes,with a focus on the forthcoming China Space Station Telescope(CSST).We first demonstrate the capability of this method by constructing dust extinction maps for 17 inter-arm massive molecular clouds in M31 using the Panchromatic Hubble Andromeda Treasury(PHAT) data.Our analysis reveals that inter-arm massive molecular clouds with an optical extinction(AV) greater than 1.6 mag exhibit a notable AV excess,facilitating their identification.The majority of these inter-arm massive molecular clouds show an AV around 1 mag,aligning with measurements from our JCMT data.Further validation using a mock CSST RGB star catalog confirms the method's effectiveness.We show that the derived AV values using CSST z and y photometries align more closely with the input values.Molecular clouds with AV>1.6 mag can also be identified using the CSST mock data.We thus claim that future CSST observation could provide an effective way for the detection of inter-arm massive molecular clouds with significant optical extinction in nearby galaxies.","sentences":["Recent sub-millimeter dust thermal emission observations have unveiled a significant number of inter-arm massive molecular clouds in M31.However,the effectiveness of this technique is limited to its sensitivity,making it challenging to study more distant galaxies.","This study introduces an alternative approach,utilizing optical extinctions derived from space-based telescopes,with a focus on the forthcoming China Space Station Telescope(CSST).We first demonstrate the capability of this method by constructing dust extinction maps for 17 inter-arm massive molecular clouds in M31 using the Panchromatic Hubble Andromeda Treasury(PHAT) data.","Our analysis reveals that inter-arm massive molecular clouds with an optical extinction(AV) greater than 1.6 mag exhibit a notable AV excess,facilitating their identification.","The majority of these inter-arm massive molecular clouds show an AV around 1 mag,aligning with measurements from our JCMT data.","Further validation using a mock CSST RGB star catalog confirms the method's effectiveness.","We show that the derived AV values using CSST z and y photometries align more closely with the input values.","Molecular clouds with AV>1.6 mag can also be identified using the CSST mock data.","We thus claim that future CSST observation could provide an effective way for the detection of inter-arm massive molecular clouds with significant optical extinction in nearby galaxies."],"url":"http://arxiv.org/abs/2405.19710v1","category":"astro-ph.GA"}
{"created":"2024-05-30 05:35:57","title":"Bridging eResearch Infrastructure and Experimental Materials Science Process in the Quantum Data Hub","abstract":"Experimental materials science is experiencing significant growth due to automated experimentation and AI techniques. Integrated autonomous platforms are emerging, combining generative models, robotics, simulations, and automated systems for material synthesis. However, two major challenges remain: democratizing access to these technologies and creating accessible infrastructure for under-resourced scientists. This paper introduces the Quantum Data Hub (QDH), a community-accessible research infrastructure aimed at researchers working with quantum materials. QDH integrates with the National Data Platform, adhering to FAIR principles while proposing additional UNIT principles for usability, navigability, interpretability, and timeliness. The QDH facilitates collaboration and extensibility, allowing seamless integration of new researchers, instruments, and data into the system.","sentences":["Experimental materials science is experiencing significant growth due to automated experimentation and AI techniques.","Integrated autonomous platforms are emerging, combining generative models, robotics, simulations, and automated systems for material synthesis.","However, two major challenges remain: democratizing access to these technologies and creating accessible infrastructure for under-resourced scientists.","This paper introduces the Quantum Data Hub (QDH), a community-accessible research infrastructure aimed at researchers working with quantum materials.","QDH integrates with the National Data Platform, adhering to FAIR principles while proposing additional UNIT principles for usability, navigability, interpretability, and timeliness.","The QDH facilitates collaboration and extensibility, allowing seamless integration of new researchers, instruments, and data into the system."],"url":"http://arxiv.org/abs/2405.19706v1","category":"cs.SE"}
{"created":"2024-05-30 05:29:40","title":"Universal Online Convex Optimization with $1$ Projection per Round","abstract":"To address the uncertainty in function types, recent progress in online convex optimization (OCO) has spurred the development of universal algorithms that simultaneously attain minimax rates for multiple types of convex functions. However, for a $T$-round online problem, state-of-the-art methods typically conduct $O(\\log T)$ projections onto the domain in each round, a process potentially time-consuming with complicated feasible sets. In this paper, inspired by the black-box reduction of Cutkosky and Orabona (2018), we employ a surrogate loss defined over simpler domains to develop universal OCO algorithms that only require $1$ projection. Embracing the framework of prediction with expert advice, we maintain a set of experts for each type of functions and aggregate their predictions via a meta-algorithm. The crux of our approach lies in a uniquely designed expert-loss for strongly convex functions, stemming from an innovative decomposition of the regret into the meta-regret and the expert-regret. Our analysis sheds new light on the surrogate loss, facilitating a rigorous examination of the discrepancy between the regret of the original loss and that of the surrogate loss, and carefully controlling meta-regret under the strong convexity condition. In this way, with only $1$ projection per round, we establish optimal regret bounds for general convex, exponentially concave, and strongly convex functions simultaneously. Furthermore, we enhance the expert-loss to exploit the smoothness property, and demonstrate that our algorithm can attain small-loss regret for multiple types of convex and smooth functions.","sentences":["To address the uncertainty in function types, recent progress in online convex optimization (OCO) has spurred the development of universal algorithms that simultaneously attain minimax rates for multiple types of convex functions.","However, for a $T$-round online problem, state-of-the-art methods typically conduct $O(\\log T)$ projections onto the domain in each round, a process potentially time-consuming with complicated feasible sets.","In this paper, inspired by the black-box reduction of Cutkosky and Orabona (2018), we employ a surrogate loss defined over simpler domains to develop universal OCO algorithms that only require $1$ projection.","Embracing the framework of prediction with expert advice, we maintain a set of experts for each type of functions and aggregate their predictions via a meta-algorithm.","The crux of our approach lies in a uniquely designed expert-loss for strongly convex functions, stemming from an innovative decomposition of the regret into the meta-regret and the expert-regret.","Our analysis sheds new light on the surrogate loss, facilitating a rigorous examination of the discrepancy between the regret of the original loss and that of the surrogate loss, and carefully controlling meta-regret under the strong convexity condition.","In this way, with only $1$ projection per round, we establish optimal regret bounds for general convex, exponentially concave, and strongly convex functions simultaneously.","Furthermore, we enhance the expert-loss to exploit the smoothness property, and demonstrate that our algorithm can attain small-loss regret for multiple types of convex and smooth functions."],"url":"http://arxiv.org/abs/2405.19705v1","category":"cs.LG"}
{"created":"2024-05-30 05:04:01","title":"Uncertainty-aware sign language video retrieval with probability distribution modeling","abstract":"Sign language video retrieval plays a key role in facilitating information access for the deaf community. Despite significant advances in video-text retrieval, the complexity and inherent uncertainty of sign language preclude the direct application of these techniques. Previous methods achieve the mapping between sign language video and text through fine-grained modal alignment. However, due to the scarcity of fine-grained annotation, the uncertainty inherent in sign language video is underestimated, limiting the further development of sign language retrieval tasks. To address this challenge, we propose a novel Uncertainty-aware Probability Distribution Retrieval (UPRet), that conceptualizes the mapping process of sign language video and text in terms of probability distributions, explores their potential interrelationships, and enables flexible mappings. Experiments on three benchmarks demonstrate the effectiveness of our method, which achieves state-of-the-art results on How2Sign (59.1%), PHOENIX-2014T (72.0%), and CSL-Daily (78.4%).","sentences":["Sign language video retrieval plays a key role in facilitating information access for the deaf community.","Despite significant advances in video-text retrieval, the complexity and inherent uncertainty of sign language preclude the direct application of these techniques.","Previous methods achieve the mapping between sign language video and text through fine-grained modal alignment.","However, due to the scarcity of fine-grained annotation, the uncertainty inherent in sign language video is underestimated, limiting the further development of sign language retrieval tasks.","To address this challenge, we propose a novel Uncertainty-aware Probability Distribution Retrieval (UPRet), that conceptualizes the mapping process of sign language video and text in terms of probability distributions, explores their potential interrelationships, and enables flexible mappings.","Experiments on three benchmarks demonstrate the effectiveness of our method, which achieves state-of-the-art results on How2Sign (59.1%), PHOENIX-2014T (72.0%), and CSL-Daily (78.4%)."],"url":"http://arxiv.org/abs/2405.19689v1","category":"cs.CV"}
{"created":"2024-05-30 04:57:54","title":"Autonomous Driving with Spiking Neural Networks","abstract":"Autonomous driving demands an integrated approach that encompasses perception, prediction, and planning, all while operating under strict energy constraints to enhance scalability and environmental sustainability. We present Spiking Autonomous Driving (\\name{}), the first unified Spiking Neural Network (SNN) to address the energy challenges faced by autonomous driving systems through its event-driven and energy-efficient nature. SAD is trained end-to-end and consists of three main modules: perception, which processes inputs from multi-view cameras to construct a spatiotemporal bird's eye view; prediction, which utilizes a novel dual-pathway with spiking neurons to forecast future states; and planning, which generates safe trajectories considering predicted occupancy, traffic rules, and ride comfort. Evaluated on the nuScenes dataset, SAD achieves competitive performance in perception, prediction, and planning tasks, while drawing upon the energy efficiency of SNNs. This work highlights the potential of neuromorphic computing to be applied to energy-efficient autonomous driving, a critical step toward sustainable and safety-critical automotive technology. Our code is available at \\url{https://github.com/ridgerchu/SAD}.","sentences":["Autonomous driving demands an integrated approach that encompasses perception, prediction, and planning, all while operating under strict energy constraints to enhance scalability and environmental sustainability.","We present Spiking Autonomous Driving (\\name{}), the first unified Spiking Neural Network (SNN) to address the energy challenges faced by autonomous driving systems through its event-driven and energy-efficient nature.","SAD is trained end-to-end and consists of three main modules: perception, which processes inputs from multi-view cameras to construct a spatiotemporal bird's eye view; prediction, which utilizes a novel dual-pathway with spiking neurons to forecast future states; and planning, which generates safe trajectories considering predicted occupancy, traffic rules, and ride comfort.","Evaluated on the nuScenes dataset, SAD achieves competitive performance in perception, prediction, and planning tasks, while drawing upon the energy efficiency of SNNs.","This work highlights the potential of neuromorphic computing to be applied to energy-efficient autonomous driving, a critical step toward sustainable and safety-critical automotive technology.","Our code is available at \\url{https://github.com/ridgerchu/SAD}."],"url":"http://arxiv.org/abs/2405.19687v1","category":"cs.NE"}
{"created":"2024-05-30 04:40:13","title":"Breaking Indistinguishability with Transfer Learning: A First Look at SPECK32/64 Lightweight Block Ciphers","abstract":"In this research, we introduce MIND-Crypt, a novel attack framework that uses deep learning (DL) and transfer learning (TL) to challenge the indistinguishability of block ciphers, specifically SPECK32/64 encryption algorithm in CBC mode (Cipher Block Chaining) against Known Plaintext Attacks (KPA). Our methodology includes training a DL model with ciphertexts of two messages encrypted using the same key. The selected messages have the same byte-length and differ by only one bit at the binary level. This DL model employs a residual network architecture. For the TL, we use the trained DL model as a feature extractor, and these features are then used to train a shallow machine learning, such as XGBoost. This dual strategy aims to distinguish ciphertexts of two encrypted messages, addressing traditional cryptanalysis challenges.   Our findings demonstrate that the DL model achieves an accuracy of approximately 99% under consistent cryptographic conditions (Same Key or Rounds) with the SPECK32/64 cipher. However, performance degrades to random guessing levels (50%) when tested with ciphertext generated from different keys or different encryption rounds of SPECK32/64. To enhance the results, the DL model requires retraining with different keys or encryption rounds using larger datasets (10^7 samples). To overcome this limitation, we implement TL, achieving an accuracy of about 53% with just 10,000 samples, which is better than random guessing. Further training with 580,000 samples increases accuracy to nearly 99%, showing a substantial reduction in data requirements by over 94%. This shows that an attacker can utilize machine learning models to break indistinguishability by accessing pairs of plaintexts and their corresponding ciphertexts encrypted with the same key, without directly interacting with the communicating parties.","sentences":["In this research, we introduce MIND-Crypt, a novel attack framework that uses deep learning (DL) and transfer learning (TL) to challenge the indistinguishability of block ciphers, specifically SPECK32/64 encryption algorithm in CBC mode (Cipher Block Chaining) against Known Plaintext Attacks (KPA).","Our methodology includes training a DL model with ciphertexts of two messages encrypted using the same key.","The selected messages have the same byte-length and differ by only one bit at the binary level.","This DL model employs a residual network architecture.","For the TL, we use the trained DL model as a feature extractor, and these features are then used to train a shallow machine learning, such as XGBoost.","This dual strategy aims to distinguish ciphertexts of two encrypted messages, addressing traditional cryptanalysis challenges.   ","Our findings demonstrate that the DL model achieves an accuracy of approximately 99% under consistent cryptographic conditions (Same Key or Rounds) with the SPECK32/64 cipher.","However, performance degrades to random guessing levels (50%) when tested with ciphertext generated from different keys or different encryption rounds of SPECK32/64.","To enhance the results, the DL model requires retraining with different keys or encryption rounds using larger datasets (10^7 samples).","To overcome this limitation, we implement TL, achieving an accuracy of about 53% with just 10,000 samples, which is better than random guessing.","Further training with 580,000 samples increases accuracy to nearly 99%, showing a substantial reduction in data requirements by over 94%.","This shows that an attacker can utilize machine learning models to break indistinguishability by accessing pairs of plaintexts and their corresponding ciphertexts encrypted with the same key, without directly interacting with the communicating parties."],"url":"http://arxiv.org/abs/2405.19683v1","category":"cs.CR"}
{"created":"2024-05-30 03:46:59","title":"GaussianRoom: Improving 3D Gaussian Splatting with SDF Guidance and Monocular Cues for Indoor Scene Reconstruction","abstract":"Recently, 3D Gaussian Splatting(3DGS) has revolutionized neural rendering with its high-quality rendering and real-time speed. However, when it comes to indoor scenes with a significant number of textureless areas, 3DGS yields incomplete and noisy reconstruction results due to the poor initialization of the point cloud and under-constrained optimization. Inspired by the continuity of signed distance field (SDF), which naturally has advantages in modeling surfaces, we present a unified optimizing framework integrating neural SDF with 3DGS. This framework incorporates a learnable neural SDF field to guide the densification and pruning of Gaussians, enabling Gaussians to accurately model scenes even with poor initialized point clouds. At the same time, the geometry represented by Gaussians improves the efficiency of the SDF field by piloting its point sampling. Additionally, we regularize the optimization with normal and edge priors to eliminate geometry ambiguity in textureless areas and improve the details. Extensive experiments in ScanNet and ScanNet++ show that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis.","sentences":["Recently, 3D Gaussian Splatting(3DGS) has revolutionized neural rendering with its high-quality rendering and real-time speed.","However, when it comes to indoor scenes with a significant number of textureless areas, 3DGS yields incomplete and noisy reconstruction results due to the poor initialization of the point cloud and under-constrained optimization.","Inspired by the continuity of signed distance field (SDF), which naturally has advantages in modeling surfaces, we present a unified optimizing framework integrating neural SDF with 3DGS.","This framework incorporates a learnable neural SDF field to guide the densification and pruning of Gaussians, enabling Gaussians to accurately model scenes even with poor initialized point clouds.","At the same time, the geometry represented by Gaussians improves the efficiency of the SDF field by piloting its point sampling.","Additionally, we regularize the optimization with normal and edge priors to eliminate geometry ambiguity in textureless areas and improve the details.","Extensive experiments in ScanNet and ScanNet++ show that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis."],"url":"http://arxiv.org/abs/2405.19671v1","category":"cs.CV"}
{"created":"2024-05-30 03:38:44","title":"Texture-guided Coding for Deep Features","abstract":"With the rapid development of machine vision technology in recent years, many researchers have begun to focus on feature compression that is better suited for machine vision tasks. The target of feature compression is deep features, which arise from convolution in the middle layer of a pre-trained convolutional neural network. However, due to the large volume of data and high level of abstraction of deep features, their application is primarily limited to machine-centric scenarios, which poses significant constraints in situations requiring human-computer interaction. This paper investigates features and textures and proposes a texture-guided feature compression strategy based on their characteristics. Specifically, the strategy comprises feature layers and texture layers. The feature layers serve the machine, including a feature selection module and a feature reconstruction network. With the assistance of texture images, they selectively compress and transmit channels relevant to visual tasks, reducing feature data while providing high-quality features for the machine. The texture layers primarily serve humans and consist of an image reconstruction network. This image reconstruction network leverages features and texture images to reconstruct preview images for humans. Our method fully exploits the characteristics of texture and features. It eliminates feature redundancy, reconstructs high-quality preview images for humans, and supports decision-making. The experimental results demonstrate excellent performance when employing our proposed method to compress the deep features.","sentences":["With the rapid development of machine vision technology in recent years, many researchers have begun to focus on feature compression that is better suited for machine vision tasks.","The target of feature compression is deep features, which arise from convolution in the middle layer of a pre-trained convolutional neural network.","However, due to the large volume of data and high level of abstraction of deep features, their application is primarily limited to machine-centric scenarios, which poses significant constraints in situations requiring human-computer interaction.","This paper investigates features and textures and proposes a texture-guided feature compression strategy based on their characteristics.","Specifically, the strategy comprises feature layers and texture layers.","The feature layers serve the machine, including a feature selection module and a feature reconstruction network.","With the assistance of texture images, they selectively compress and transmit channels relevant to visual tasks, reducing feature data while providing high-quality features for the machine.","The texture layers primarily serve humans and consist of an image reconstruction network.","This image reconstruction network leverages features and texture images to reconstruct preview images for humans.","Our method fully exploits the characteristics of texture and features.","It eliminates feature redundancy, reconstructs high-quality preview images for humans, and supports decision-making.","The experimental results demonstrate excellent performance when employing our proposed method to compress the deep features."],"url":"http://arxiv.org/abs/2405.19669v1","category":"cs.CV"}
{"created":"2024-05-30 03:38:31","title":"AutoBreach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization","abstract":"Despite the widespread application of large language models (LLMs) across various tasks, recent studies indicate that they are susceptible to jailbreak attacks, which can render their defense mechanisms ineffective. However, previous jailbreak research has frequently been constrained by limited universality, suboptimal efficiency, and a reliance on manual crafting. In response, we rethink the approach to jailbreaking LLMs and formally define three essential properties from the attacker' s perspective, which contributes to guiding the design of jailbreak methods. We further introduce AutoBreach, a novel method for jailbreaking LLMs that requires only black-box access. Inspired by the versatility of wordplay, AutoBreach employs a wordplay-guided mapping rule sampling strategy to generate a variety of universal mapping rules for creating adversarial prompts. This generation process leverages LLMs' automatic summarization and reasoning capabilities, thus alleviating the manual burden. To boost jailbreak success rates, we further suggest sentence compression and chain-of-thought-based mapping rules to correct errors and wordplay misinterpretations in target LLMs. Additionally, we propose a two-stage mapping rule optimization strategy that initially optimizes mapping rules before querying target LLMs to enhance the efficiency of AutoBreach. AutoBreach can efficiently identify security vulnerabilities across various LLMs, including three proprietary models: Claude-3, GPT-3.5, GPT-4 Turbo, and two LLMs' web platforms: Bingchat, GPT-4 Web, achieving an average success rate of over 80% with fewer than 10 queries","sentences":["Despite the widespread application of large language models (LLMs) across various tasks, recent studies indicate that they are susceptible to jailbreak attacks, which can render their defense mechanisms ineffective.","However, previous jailbreak research has frequently been constrained by limited universality, suboptimal efficiency, and a reliance on manual crafting.","In response, we rethink the approach to jailbreaking LLMs and formally define three essential properties from the attacker' s perspective, which contributes to guiding the design of jailbreak methods.","We further introduce AutoBreach, a novel method for jailbreaking LLMs that requires only black-box access.","Inspired by the versatility of wordplay, AutoBreach employs a wordplay-guided mapping rule sampling strategy to generate a variety of universal mapping rules for creating adversarial prompts.","This generation process leverages LLMs' automatic summarization and reasoning capabilities, thus alleviating the manual burden.","To boost jailbreak success rates, we further suggest sentence compression and chain-of-thought-based mapping rules to correct errors and wordplay misinterpretations in target LLMs.","Additionally, we propose a two-stage mapping rule optimization strategy that initially optimizes mapping rules before querying target LLMs to enhance the efficiency of AutoBreach.","AutoBreach can efficiently identify security vulnerabilities across various LLMs, including three proprietary models: Claude-3, GPT-3.5, GPT-4 Turbo, and two LLMs' web platforms: Bingchat, GPT-4 Web, achieving an average success rate of over 80% with fewer than 10 queries"],"url":"http://arxiv.org/abs/2405.19668v1","category":"cs.CV"}
{"created":"2024-05-30 03:36:46","title":"Reconciling Model Multiplicity for Downstream Decision Making","abstract":"We consider the problem of model multiplicity in downstream decision-making, a setting where two predictive models of equivalent accuracy cannot agree on the best-response action for a downstream loss function. We show that even when the two predictive models approximately agree on their individual predictions almost everywhere, it is still possible for their induced best-response actions to differ on a substantial portion of the population. We address this issue by proposing a framework that calibrates the predictive models with regard to both the downstream decision-making problem and the individual probability prediction. Specifically, leveraging tools from multi-calibration, we provide an algorithm that, at each time-step, first reconciles the differences in individual probability prediction, then calibrates the updated models such that they are indistinguishable from the true probability distribution to the decision-maker. We extend our results to the setting where one does not have direct access to the true probability distribution and instead relies on a set of i.i.d data to be the empirical distribution. Finally, we provide a set of experiments to empirically evaluate our methods: compared to existing work, our proposed algorithm creates a pair of predictive models with both improved downstream decision-making losses and agrees on their best-response actions almost everywhere.","sentences":["We consider the problem of model multiplicity in downstream decision-making, a setting where two predictive models of equivalent accuracy cannot agree on the best-response action for a downstream loss function.","We show that even when the two predictive models approximately agree on their individual predictions almost everywhere, it is still possible for their induced best-response actions to differ on a substantial portion of the population.","We address this issue by proposing a framework that calibrates the predictive models with regard to both the downstream decision-making problem and the individual probability prediction.","Specifically, leveraging tools from multi-calibration, we provide an algorithm that, at each time-step, first reconciles the differences in individual probability prediction, then calibrates the updated models such that they are indistinguishable from the true probability distribution to the decision-maker.","We extend our results to the setting where one does not have direct access to the true probability distribution and instead relies on a set of i.i.d data to be the empirical distribution.","Finally, we provide a set of experiments to empirically evaluate our methods: compared to existing work, our proposed algorithm creates a pair of predictive models with both improved downstream decision-making losses and agrees on their best-response actions almost everywhere."],"url":"http://arxiv.org/abs/2405.19667v1","category":"cs.LG"}
{"created":"2024-05-30 03:33:49","title":"A novel fault localization with data refinement for hydroelectric units","abstract":"Due to the scarcity of fault samples and the complexity of non-linear and non-smooth characteristics data in hydroelectric units, most of the traditional hydroelectric unit fault localization methods are difficult to carry out accurate localization. To address these problems, a sparse autoencoder (SAE)-generative adversarial network (GAN)-wavelet noise reduction (WNR)- manifold-boosted deep learning (SG-WMBDL) based fault localization method for hydroelectric units is proposed. To overcome the data scarcity, a SAE is embedded into the GAN to generate more high-quality samples in the data generation module. Considering the signals involving non-linear and non-smooth characteristics, the improved WNR which combining both soft and hard thresholding and local linear embedding (LLE) are utilized to the data preprocessing module in order to reduce the noise and effectively capture the local features. In addition, to seek higher performance, the novel Adaptive Boost (AdaBoost) combined with multi deep learning is proposed to achieve accurate fault localization. The experimental results show that the SG-WMBDL can locate faults for hydroelectric units under a small number of fault samples with non-linear and non-smooth characteristics on higher precision and accuracy compared to other frontier methods, which verifies the effectiveness and practicality of the proposed method.","sentences":["Due to the scarcity of fault samples and the complexity of non-linear and non-smooth characteristics data in hydroelectric units, most of the traditional hydroelectric unit fault localization methods are difficult to carry out accurate localization.","To address these problems, a sparse autoencoder (SAE)-generative adversarial network (GAN)-wavelet noise reduction (WNR)- manifold-boosted deep learning (SG-WMBDL) based fault localization method for hydroelectric units is proposed.","To overcome the data scarcity, a SAE is embedded into the GAN to generate more high-quality samples in the data generation module.","Considering the signals involving non-linear and non-smooth characteristics, the improved WNR which combining both soft and hard thresholding and local linear embedding (LLE) are utilized to the data preprocessing module in order to reduce the noise and effectively capture the local features.","In addition, to seek higher performance, the novel Adaptive Boost (AdaBoost) combined with multi deep learning is proposed to achieve accurate fault localization.","The experimental results show that the SG-WMBDL can locate faults for hydroelectric units under a small number of fault samples with non-linear and non-smooth characteristics on higher precision and accuracy compared to other frontier methods, which verifies the effectiveness and practicality of the proposed method."],"url":"http://arxiv.org/abs/2405.19665v1","category":"eess.SY"}
{"created":"2024-05-30 03:33:40","title":"Quantum Zeno Effect on Genuine Tripartite Nonlocality and Entanglement in Quantum Dissipative System","abstract":"As a precious global resource in quantum information, genuine tripartite nonlocality(GTN) can be quantified by violating Svetlichny inequality. However, there is still no analytical expression for the general three-qubit states due to the difficulty of theoretical calculations. In this paper, we achieve highly accurate quantization of GTN for arbitrary three-qubit quantum states numerically. As an example, we study the dynamics of GTN and genuine tripartite entanglement(GTE) for the W state. Moreover, the complementarity of GTN is verified by examining the nonlocality between the tripartite and the bipartite. Finally, we also find a useful strategy to protect the correlation of GTN and GTE under decoherence by utilizing the Zeno effect.","sentences":["As a precious global resource in quantum information, genuine tripartite nonlocality(GTN) can be quantified by violating Svetlichny inequality.","However, there is still no analytical expression for the general three-qubit states due to the difficulty of theoretical calculations.","In this paper, we achieve highly accurate quantization of GTN for arbitrary three-qubit quantum states numerically.","As an example, we study the dynamics of GTN and genuine tripartite entanglement(GTE) for the W state.","Moreover, the complementarity of GTN is verified by examining the nonlocality between the tripartite and the bipartite.","Finally, we also find a useful strategy to protect the correlation of GTN and GTE under decoherence by utilizing the Zeno effect."],"url":"http://arxiv.org/abs/2405.19664v1","category":"quant-ph"}
{"created":"2024-05-30 03:12:04","title":"SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems","abstract":"Data-driven simulation surrogates help computational scientists study complex systems. They can also help inform impactful policy decisions. We introduce a learning framework for surrogate modeling where language is used to interface with the underlying system being simulated. We call a language description of a system a \"system caption\", or SysCap. To address the lack of datasets of paired natural language SysCaps and simulation runs, we use large language models (LLMs) to synthesize high-quality captions. Using our framework, we train multimodal text and timeseries regression models for two real-world simulators of complex energy systems. Our experiments demonstrate the feasibility of designing language interfaces for real-world surrogate models at comparable accuracy to standard baselines. We qualitatively and quantitatively show that SysCaps unlock text-prompt-style surrogate modeling and new generalization abilities beyond what was previously possible. We will release the generated SysCaps datasets and our code to support follow-on studies.","sentences":["Data-driven simulation surrogates help computational scientists study complex systems.","They can also help inform impactful policy decisions.","We introduce a learning framework for surrogate modeling where language is used to interface with the underlying system being simulated.","We call a language description of a system a \"system caption\", or SysCap.","To address the lack of datasets of paired natural language SysCaps and simulation runs, we use large language models (LLMs) to synthesize high-quality captions.","Using our framework, we train multimodal text and timeseries regression models for two real-world simulators of complex energy systems.","Our experiments demonstrate the feasibility of designing language interfaces for real-world surrogate models at comparable accuracy to standard baselines.","We qualitatively and quantitatively show that SysCaps unlock text-prompt-style surrogate modeling and new generalization abilities beyond what was previously possible.","We will release the generated SysCaps datasets and our code to support follow-on studies."],"url":"http://arxiv.org/abs/2405.19653v1","category":"cs.LG"}
{"created":"2024-05-30 02:59:49","title":"FTS: A Framework to Find a Faithful TimeSieve","abstract":"The field of time series forecasting has garnered significant attention in recent years, prompting the development of advanced models like TimeSieve, which demonstrates impressive performance. However, an analysis reveals certain unfaithfulness issues, including high sensitivity to random seeds and minute input noise perturbations. Recognizing these challenges, we embark on a quest to define the concept of \\textbf{\\underline{F}aithful \\underline{T}ime\\underline{S}ieve \\underline{(FTS)}}, a model that consistently delivers reliable and robust predictions. To address these issues, we propose a novel framework aimed at identifying and rectifying unfaithfulness in TimeSieve. Our framework is designed to enhance the model's stability and resilience, ensuring that its outputs are less susceptible to the aforementioned factors. Experimentation validates the effectiveness of our proposed framework, demonstrating improved faithfulness in the model's behavior. Looking forward, we plan to expand our experimental scope to further validate and optimize our algorithm, ensuring comprehensive faithfulness across a wide range of scenarios. Ultimately, we aspire to make this framework can be applied to enhance the faithfulness of not just TimeSieve but also other state-of-the-art temporal methods, thereby contributing to the reliability and robustness of temporal modeling as a whole.","sentences":["The field of time series forecasting has garnered significant attention in recent years, prompting the development of advanced models like TimeSieve, which demonstrates impressive performance.","However, an analysis reveals certain unfaithfulness issues, including high sensitivity to random seeds and minute input noise perturbations.","Recognizing these challenges, we embark on a quest to define the concept of \\textbf{\\underline{F}aithful \\underline{T}ime\\underline{S}ieve \\underline{(FTS)}}, a model that consistently delivers reliable and robust predictions.","To address these issues, we propose a novel framework aimed at identifying and rectifying unfaithfulness in TimeSieve.","Our framework is designed to enhance the model's stability and resilience, ensuring that its outputs are less susceptible to the aforementioned factors.","Experimentation validates the effectiveness of our proposed framework, demonstrating improved faithfulness in the model's behavior.","Looking forward, we plan to expand our experimental scope to further validate and optimize our algorithm, ensuring comprehensive faithfulness across a wide range of scenarios.","Ultimately, we aspire to make this framework can be applied to enhance the faithfulness of not just TimeSieve but also other state-of-the-art temporal methods, thereby contributing to the reliability and robustness of temporal modeling as a whole."],"url":"http://arxiv.org/abs/2405.19647v1","category":"cs.LG"}
{"created":"2024-05-30 02:53:11","title":"Quantum Circuit Tensors and Enumerators with Applications to Quantum Fault Tolerance","abstract":"We extend the recently introduced notion of tensor enumerator to the circuit enumerator. We provide a mathematical framework that offers a novel method for analyzing circuits and error models without resorting to Monte Carlo techniques. We introduce an analogue of the Poisson summation formula for stabilizer codes, facilitating a method for the exact computation of the number of error paths within the syndrome extraction circuit of the code that does not require direct enumeration. We demonstrate the efficacy of our approach by explicitly providing the number of error paths in a distance five surface code under various error models, a task previously deemed infeasible via simulation. We also show our circuit enumerator is related to the process matrix of a channel through a type of MacWilliams identity.","sentences":["We extend the recently introduced notion of tensor enumerator to the circuit enumerator.","We provide a mathematical framework that offers a novel method for analyzing circuits and error models without resorting to Monte Carlo techniques.","We introduce an analogue of the Poisson summation formula for stabilizer codes, facilitating a method for the exact computation of the number of error paths within the syndrome extraction circuit of the code that does not require direct enumeration.","We demonstrate the efficacy of our approach by explicitly providing the number of error paths in a distance five surface code under various error models, a task previously deemed infeasible via simulation.","We also show our circuit enumerator is related to the process matrix of a channel through a type of MacWilliams identity."],"url":"http://arxiv.org/abs/2405.19643v1","category":"quant-ph"}
{"created":"2024-05-30 02:44:31","title":"Generalized BER Performance Analysis for SIC-based Uplink NOMA Systems","abstract":"Non-orthogonal multiple access (NOMA) is widely recognized for its spectral and energy efficiency, which allows more users to share the network resources more effectively. This paper provides a generalized bit error rate (BER) performance analysis of successive interference cancellation (SIC)-based uplink NOMA systems under Rayleigh fading channels, taking into account error propagation resulting from SIC imperfections. Exact closed-form BER expressions are initially derived for scenarios with 2 and 3 users using quadrature phase shift keying (QPSK) modulation. These expressions are then generalized to encompass any arbitrary rectangular/square M-ary quadrature amplitude modulation (M-QAM) order, number of NOMA users, and number of BS antennas. Additionally, by utilizing the derived closed-form BER expressions, a simple and practically feasible power allocation (PA) technique is devised to minimize the sum bit error rate of the users and optimize the SIC-based NOMA detection at the base-station (BS). The derived closed-form expressions are corroborated through Monte Carlo simulations. It is demonstrated that these expressions can be effective for optimal uplink PA to ensure optimized SIC detection that mitigates error floors. It is also shown that significant performance improvements are achieved regardless of the users' decoding order, making uplink SIC-based NOMA a viable approach.","sentences":["Non-orthogonal multiple access (NOMA) is widely recognized for its spectral and energy efficiency, which allows more users to share the network resources more effectively.","This paper provides a generalized bit error rate (BER) performance analysis of successive interference cancellation (SIC)-based uplink NOMA systems under Rayleigh fading channels, taking into account error propagation resulting from SIC imperfections.","Exact closed-form BER expressions are initially derived for scenarios with 2 and 3 users using quadrature phase shift keying (QPSK) modulation.","These expressions are then generalized to encompass any arbitrary rectangular/square M-ary quadrature amplitude modulation (M-QAM) order, number of NOMA users, and number of BS antennas.","Additionally, by utilizing the derived closed-form BER expressions, a simple and practically feasible power allocation (PA) technique is devised to minimize the sum bit error rate of the users and optimize the SIC-based NOMA detection at the base-station (BS).","The derived closed-form expressions are corroborated through Monte Carlo simulations.","It is demonstrated that these expressions can be effective for optimal uplink PA to ensure optimized SIC detection that mitigates error floors.","It is also shown that significant performance improvements are achieved regardless of the users' decoding order, making uplink SIC-based NOMA a viable approach."],"url":"http://arxiv.org/abs/2405.19639v1","category":"eess.SP"}
{"created":"2024-05-30 02:42:58","title":"Learning Robust Correlation with Foundation Model for Weakly-Supervised Few-Shot Segmentation","abstract":"Existing few-shot segmentation (FSS) only considers learning support-query correlation and segmenting unseen categories under the precise pixel masks. However, the cost of a large number of pixel masks during training is expensive. This paper considers a more challenging scenario, weakly-supervised few-shot segmentation (WS-FSS), which only provides category ($i.e.$ image-level) labels. It requires the model to learn robust support-query information when the generated mask is inaccurate. In this work, we design a Correlation Enhancement Network (CORENet) with foundation model, which utilizes multi-information guidance to learn robust correlation. Specifically, correlation-guided transformer (CGT) utilizes self-supervised ViT tokens to learn robust correlation from both local and global perspectives. From the perspective of semantic categories, the class-guided module (CGM) guides the model to locate valuable correlations through the pre-trained CLIP. Finally, the embedding-guided module (EGM) implicitly guides the model to supplement the inevitable information loss during the correlation learning by the original appearance embedding and finally generates the query mask. Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ have shown that CORENet exhibits excellent performance compared to existing methods.","sentences":["Existing few-shot segmentation (FSS) only considers learning support-query correlation and segmenting unseen categories under the precise pixel masks.","However, the cost of a large number of pixel masks during training is expensive.","This paper considers a more challenging scenario, weakly-supervised few-shot segmentation (WS-FSS), which only provides category ($i.e.$ image-level) labels.","It requires the model to learn robust support-query information when the generated mask is inaccurate.","In this work, we design a Correlation Enhancement Network (CORENet) with foundation model, which utilizes multi-information guidance to learn robust correlation.","Specifically, correlation-guided transformer (CGT) utilizes self-supervised ViT tokens to learn robust correlation from both local and global perspectives.","From the perspective of semantic categories, the class-guided module (CGM) guides the model to locate valuable correlations through the pre-trained CLIP.","Finally, the embedding-guided module (EGM) implicitly guides the model to supplement the inevitable information loss during the correlation learning by the original appearance embedding and finally generates the query mask.","Extensive experiments on PASCAL-5$^i$ and COCO-20$^i$ have shown that CORENet exhibits excellent performance compared to existing methods."],"url":"http://arxiv.org/abs/2405.19638v1","category":"cs.CV"}
{"created":"2024-05-30 02:42:27","title":"Inference in semiparametric formation models for directed networks","abstract":"We propose a semiparametric model for dyadic link formations in directed networks. The model contains a set of degree parameters that measure different effects of popularity or outgoingness across nodes, a regression parameter vector that reflects the homophily effect resulting from the nodal attributes or pairwise covariates associated with edges, and a set of latent random noises with unknown distributions. Our interest lies in inferring the unknown degree parameters and homophily parameters. The dimension of the degree parameters increases with the number of nodes. Under the high-dimensional regime, we develop a kernel-based least squares approach to estimate the unknown parameters. The major advantage of our estimator is that it does not encounter the incidental parameter problem for the homophily parameters. We prove consistency of all the resulting estimators of the degree parameters and homophily parameters. We establish high-dimensional central limit theorems for the proposed estimators and provide several applications of our general theory, including testing the existence of degree heterogeneity, testing sparse signals and recovering the support. Simulation studies and a real data application are conducted to illustrate the finite sample performance of the proposed methods.","sentences":["We propose a semiparametric model for dyadic link formations in directed networks.","The model contains a set of degree parameters that measure different effects of popularity or outgoingness across nodes, a regression parameter vector that reflects the homophily effect resulting from the nodal attributes or pairwise covariates associated with edges, and a set of latent random noises with unknown distributions.","Our interest lies in inferring the unknown degree parameters and homophily parameters.","The dimension of the degree parameters increases with the number of nodes.","Under the high-dimensional regime, we develop a kernel-based least squares approach to estimate the unknown parameters.","The major advantage of our estimator is that it does not encounter the incidental parameter problem for the homophily parameters.","We prove consistency of all the resulting estimators of the degree parameters and homophily parameters.","We establish high-dimensional central limit theorems for the proposed estimators and provide several applications of our general theory, including testing the existence of degree heterogeneity, testing sparse signals and recovering the support.","Simulation studies and a real data application are conducted to illustrate the finite sample performance of the proposed methods."],"url":"http://arxiv.org/abs/2405.19637v1","category":"stat.ME"}
{"created":"2024-05-30 02:23:50","title":"Position: CXL Shared Memory Programming: Barely Distributed and Almost Persistent","abstract":"While Compute Express Link (CXL) enables support for cache-coherent shared memory among multiple nodes, it also introduces new types of failures--processes can fail before data does, or data might fail before a process does. The lack of a failure model for CXL-based shared memory makes it challenging to understand and mitigate these failures.   To solve these challenges, in this paper, we describe a model categorizing and handling the CXL-based shared memory's failures: data and process failures. Data failures in CXL-based shared memory render data inaccessible or inconsistent for a currently running application. We argue that such failures are unlike data failures in distributed storage systems and require CXL-specific handling. To address this, we look into traditional data failure mitigation techniques like erasure coding and replication and propose new solutions to better handle data failures in CXL-based shared memory systems. Next, we look into process failures and compare the failures and potential solutions with PMEM's failure model and programming solutions. We argue that although PMEM shares some of CXL's characteristics, it does not fully address CXL's volatile nature and low access latencies. Finally, taking inspiration from PMEM programming solutions, we propose techniques to handle these new failures.   Thus, this paper is the first work to define the CXL-based shared memory failure model and propose tailored solutions that address challenges specific to CXL-based systems.","sentences":["While Compute Express Link (CXL) enables support for cache-coherent shared memory among multiple nodes, it also introduces new types of failures--processes can fail before data does, or data might fail before a process does.","The lack of a failure model for CXL-based shared memory makes it challenging to understand and mitigate these failures.   ","To solve these challenges, in this paper, we describe a model categorizing and handling the CXL-based shared memory's failures: data and process failures.","Data failures in CXL-based shared memory render data inaccessible or inconsistent for a currently running application.","We argue that such failures are unlike data failures in distributed storage systems and require CXL-specific handling.","To address this, we look into traditional data failure mitigation techniques like erasure coding and replication and propose new solutions to better handle data failures in CXL-based shared memory systems.","Next, we look into process failures and compare the failures and potential solutions with PMEM's failure model and programming solutions.","We argue that although PMEM shares some of CXL's characteristics, it does not fully address CXL's volatile nature and low access latencies.","Finally, taking inspiration from PMEM programming solutions, we propose techniques to handle these new failures.   ","Thus, this paper is the first work to define the CXL-based shared memory failure model and propose tailored solutions that address challenges specific to CXL-based systems."],"url":"http://arxiv.org/abs/2405.19626v1","category":"cs.DC"}
{"created":"2024-05-30 02:23:46","title":"A possible spin Jahn-Teller material: ordered pseudobrookite FeTi2O5","abstract":"We investigated the spin-lattice coupling in orthorhombic pseudobrookite FeTi2O5 single crystal with highly ordered $ Fe^{2+} / Ti^{4+}$ occupation, which consists of quasi-1D S=2 chains running along a-axis. Both the magnetization and specific heat measurements confirm that the antiferromagnetic phase transition of FeTi2O5 occurs at TN = 42 K. The structural distortions were also observed around TN in the thermal expansion $ \\Delta L / L (T)$ data. Moreover, the magnetic field was found to strongly affect the thermal expansion both along chains and in the perpendicular direction clearly signaling a substantial magnetoelastic coupling, which was recently proposed to be the origin of a rare spin Jahn-Teller effect, when frustration is lifted via additional lattice distortions. Experimentally observed change in the thermal conductivity slope around TN is usually associated with the orbital ordering, but DFT+U calculations do not detect modification of the orbital structure across the transition. However, the first-principles calculation results confirm that FeTi2O5 is a quasi-1D magnet with a ratio of frustrating inter-chain to intra-chain exchanges $ J' / J = 0 . 0 3$ and a substantial single-ion anisotropy (A = 4K) of easy-axis type making this material interesting for studying quantum criticality in transverse magnetic fields.","sentences":["We investigated the spin-lattice coupling in orthorhombic pseudobrookite FeTi2O5","single crystal with highly ordered $ Fe^{2+} / Ti^{4+}$ occupation, which consists of quasi-1D S=2 chains running along a-axis.","Both the magnetization and specific heat measurements confirm that the antiferromagnetic phase transition of FeTi2O5 occurs at TN = 42","K. The structural distortions were also observed around TN in the thermal expansion $ \\Delta L / L (T)$ data.","Moreover, the magnetic field was found to strongly affect the thermal expansion both along chains and in the perpendicular direction clearly signaling a substantial magnetoelastic coupling, which was recently proposed to be the origin of a rare spin Jahn-Teller effect, when frustration is lifted via additional lattice distortions.","Experimentally observed change in the thermal conductivity slope around TN is usually associated with the orbital ordering, but DFT+U calculations do not detect modification of the orbital structure across the transition.","However, the first-principles calculation results confirm that FeTi2O5 is a quasi-1D magnet with a ratio of frustrating inter-chain to intra-chain exchanges $ J' / J = 0 .","0","3$ and a substantial single-ion anisotropy (A = 4K) of easy-axis type making this material interesting for studying quantum criticality in transverse magnetic fields."],"url":"http://arxiv.org/abs/2405.19625v1","category":"cond-mat.str-el"}
{"created":"2024-05-30 02:20:04","title":"A Novel Approach for Automated Design Information Mining from Issue Logs","abstract":"Software architectures are usually meticulously designed to address multiple quality concerns and support long-term maintenance. However, due to the imbalance between the cost and value for developers to document design rationales (i.e., the design alternatives and the underlying arguments for making or rejecting decisions), these rationales are often obsolete or even missing. The lack of design knowledge has motivated a number of studies to extract design information from various platforms in recent years. Unfortunately, despite the wealth of discussion records related to design information provided by platforms like open-source communities, existing research often overlooks the underlying arguments behind alternatives due to challenges such as the intricate semantics of discussions and the lack of benchmarks for design rationale extraction. In this paper, we propose a novel method, named by DRMiner, to automatically mine latent design rationales from developers' live discussion in open-source community (i.e., issue logs in Jira). To better identify solutions and the arguments supporting them, DRMiner skillfully decomposes the problem into multiple text classification tasks and tackles them using prompt tuning of language models and customized text-related features. To evaluate DRMiner, we acquire issue logs from Cassandra, Flink, and Solr repositories in Jira, and then annotate and process them under a rigorous scheme, ultimately forming a dataset for design rationale mining. Experimental results show that DRMiner achieves an F1 score of 65% for mining design rationales, outperforming all baselines with a 7% improvement over GPT-4.0. Furthermore, we investigate the usefulness of the design rationales mined by DRMiner for automated program repair (APR) and find that the design rationales significantly enhance APR, achieving 14 times higher full-match repairs on average.","sentences":["Software architectures are usually meticulously designed to address multiple quality concerns and support long-term maintenance.","However, due to the imbalance between the cost and value for developers to document design rationales (i.e., the design alternatives and the underlying arguments for making or rejecting decisions), these rationales are often obsolete or even missing.","The lack of design knowledge has motivated a number of studies to extract design information from various platforms in recent years.","Unfortunately, despite the wealth of discussion records related to design information provided by platforms like open-source communities, existing research often overlooks the underlying arguments behind alternatives due to challenges such as the intricate semantics of discussions and the lack of benchmarks for design rationale extraction.","In this paper, we propose a novel method, named by DRMiner, to automatically mine latent design rationales from developers' live discussion in open-source community (i.e., issue logs in Jira).","To better identify solutions and the arguments supporting them, DRMiner skillfully decomposes the problem into multiple text classification tasks and tackles them using prompt tuning of language models and customized text-related features.","To evaluate DRMiner, we acquire issue logs from Cassandra, Flink, and Solr repositories in Jira, and then annotate and process them under a rigorous scheme, ultimately forming a dataset for design rationale mining.","Experimental results show that DRMiner achieves an F1 score of 65% for mining design rationales, outperforming all baselines with a 7% improvement over GPT-4.0.","Furthermore, we investigate the usefulness of the design rationales mined by DRMiner for automated program repair (APR) and find that the design rationales significantly enhance APR, achieving 14 times higher full-match repairs on average."],"url":"http://arxiv.org/abs/2405.19623v1","category":"cs.SE"}
{"created":"2024-05-30 02:19:24","title":"On shortest products for nonnegative matrix mortality","abstract":"Given a finite set of matrices with integer entries, the matrix mortality problem asks if there exists a product of these matrices equal to the zero matrix. We consider a special case of this problem where all entries of the matrices are nonnegative. This case is equivalent to the NFA mortality problem, which, given an NFA, asks for a word $w$ such that the image of every state under $w$ is the empty set. The size of the alphabet of the NFA is then equal to the number of matrices in the set. We study the length of shortest such words depending on the size of the alphabet. We show that this length for an NFA with $n$ states can be at least $2^n - 1$, $2^{(n - 4)/2}$ and $2^{(n - 2)/3}$ if the size of the alphabet is, respectively, equal to $n$, three and two.","sentences":["Given a finite set of matrices with integer entries, the matrix mortality problem asks if there exists a product of these matrices equal to the zero matrix.","We consider a special case of this problem where all entries of the matrices are nonnegative.","This case is equivalent to the NFA mortality problem, which, given an NFA, asks for a word $w$ such that the image of every state under $w$ is the empty set.","The size of the alphabet of the NFA is then equal to the number of matrices in the set.","We study the length of shortest such words depending on the size of the alphabet.","We show that this length for an NFA with $n$ states can be at least $2^n - 1$, $2^{(n - 4)/2}$ and $2^{(n - 2)/3}$ if the size of the alphabet is, respectively, equal to $n$, three and two."],"url":"http://arxiv.org/abs/2405.19622v1","category":"cs.DM"}
{"created":"2024-05-30 01:48:37","title":"Generic transverse stability of kink structures in atomic and optical nonlinear media with competing attractive and repulsive interactions","abstract":"We demonstrate the existence and stability of one-dimensional (1D) topological kink configurations immersed in higher-dimensional bosonic gases and nonlinear optical setups. Our analysis pertains, in particular, to the two- and three-dimensional extended Gross-Pitaevskii models with quantum fluctuations describing droplet-bearing environments but also to the two-dimensional cubic-quintic nonlinear Schr\\\"odinger equation containing higher-order corrections to the nonlinear refractive index. Contrary to the generic dark soliton transverse instability, the kink structures are generically robust under the interplay of low-amplitude attractive and high-amplitude repulsive interactions. A quasi-1D effective potential picture dictates the existence of these defects, while their stability is obtained through linearization analysis and direct dynamics in the presence of external fluctuations showcasing their unprecedented resilience. These generic (across different models) findings should be detectable in current cold atom and optics experiments. They also offer insights towards controlling topological excitations and their usage in topological quantum computers.","sentences":["We demonstrate the existence and stability of one-dimensional (1D) topological kink configurations immersed in higher-dimensional bosonic gases and nonlinear optical setups.","Our analysis pertains, in particular, to the two- and three-dimensional extended Gross-Pitaevskii models with quantum fluctuations describing droplet-bearing environments but also to the two-dimensional cubic-quintic nonlinear Schr\\\"odinger equation containing higher-order corrections to the nonlinear refractive index.","Contrary to the generic dark soliton transverse instability, the kink structures are generically robust under the interplay of low-amplitude attractive and high-amplitude repulsive interactions.","A quasi-1D effective potential picture dictates the existence of these defects, while their stability is obtained through linearization analysis and direct dynamics in the presence of external fluctuations showcasing their unprecedented resilience.","These generic (across different models) findings should be detectable in current cold atom and optics experiments.","They also offer insights towards controlling topological excitations and their usage in topological quantum computers."],"url":"http://arxiv.org/abs/2405.19607v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-30 01:38:10","title":"Rogue wave patterns associated with Adler--Moser polynomials featuring multiple roots in the nonlinear Schr\u00f6dinger equation","abstract":"In this work, we analyze the asymptotic behaviors of high-order rogue wave solutions with multiple large parameters and discover novel rogue wave patterns, including claw-like, OTR-type, TTR-type, semi-modified TTR-type, and their modified patterns. A correlation is established between these rogue wave patterns and the root structures of the Adler--Moser polynomials with multiple roots. At the positions in the $(x,t)$-plane corresponding to single roots of the Adler--Moser polynomials, these high-order rogue wave patterns asymptotically approach first-order rogue waves. At the positions in the $(x,t)$-plane corresponding to multiple roots of the Adler--Moser polynomials, these rogue wave patterns asymptotically tend toward lower-order fundamental rogue waves, dispersed first-order rogue waves, or mixed structures of these rogue waves. These structures are related to the root structures of special Adler--Moser polynomials with new free parameters, such as the Yablonskii--Vorob'ev polynomial hierarchy, among others. Notably, the positions of the fundamental lower-order rogue waves or mixed structures in these rogue wave patterns can be controlled freely under specific conditions.","sentences":["In this work, we analyze the asymptotic behaviors of high-order rogue wave solutions with multiple large parameters and discover novel rogue wave patterns, including claw-like, OTR-type, TTR-type, semi-modified TTR-type, and their modified patterns.","A correlation is established between these rogue wave patterns and the root structures of the Adler--Moser polynomials with multiple roots.","At the positions in the $(x,t)$-plane corresponding to single roots of the Adler--Moser polynomials, these high-order rogue wave patterns asymptotically approach first-order rogue waves.","At the positions in the $(x,t)$-plane corresponding to multiple roots of the Adler--Moser polynomials, these rogue wave patterns asymptotically tend toward lower-order fundamental rogue waves, dispersed first-order rogue waves, or mixed structures of these rogue waves.","These structures are related to the root structures of special Adler--Moser polynomials with new free parameters, such as the Yablonskii--Vorob'ev polynomial hierarchy, among others.","Notably, the positions of the fundamental lower-order rogue waves or mixed structures in these rogue wave patterns can be controlled freely under specific conditions."],"url":"http://arxiv.org/abs/2405.19602v1","category":"nlin.SI"}
{"created":"2024-05-30 17:50:04","title":"Group Robust Preference Optimization in Reward-free RLHF","abstract":"Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data. While these data often come from diverse labelers' groups (e.g., different demographics, ethnicities, company teams, etc.), traditional RLHF approaches adopt a \"one-size-fits-all\" approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups. To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups' preferences robustly. Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance. To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss. We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class. By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies compared to non-robust baselines.","sentences":["Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data.","While these data often come from diverse labelers' groups (e.g., different demographics, ethnicities, company teams, etc.), traditional RLHF approaches adopt a \"one-size-fits-all\" approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups.","To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups' preferences robustly.","Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance.","To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss.","We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class.","By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies compared to non-robust baselines."],"url":"http://arxiv.org/abs/2405.20304v1","category":"cs.CL"}
{"created":"2024-05-30 15:45:03","title":"Tropical Expressivity of Neural Networks","abstract":"We propose an algebraic geometric framework to study the expressivity of linear activation neural networks. A particular quantity that has been actively studied in the field of deep learning is the number of linear regions, which gives an estimate of the information capacity of the architecture. To study and evaluate information capacity and expressivity, we work in the setting of tropical geometry -- a combinatorial and polyhedral variant of algebraic geometry -- where there are known connections between tropical rational maps and feedforward neural networks. Our work builds on and expands this connection to capitalize on the rich theory of tropical geometry to characterize and study various architectural aspects of neural networks. Our contributions are threefold: we provide a novel tropical geometric approach to selecting sampling domains among linear regions; an algebraic result allowing for a guided restriction of the sampling domain for network architectures with symmetries; and an open source library to analyze neural networks as tropical Puiseux rational maps. We provide a comprehensive set of proof-of-concept numerical experiments demonstrating the breadth of neural network architectures to which tropical geometric theory can be applied to reveal insights on expressivity characteristics of a network. Our work provides the foundations for the adaptation of both theory and existing software from computational tropical geometry and symbolic computation to deep learning.","sentences":["We propose an algebraic geometric framework to study the expressivity of linear activation neural networks.","A particular quantity that has been actively studied in the field of deep learning is the number of linear regions, which gives an estimate of the information capacity of the architecture.","To study and evaluate information capacity and expressivity, we work in the setting of tropical geometry -- a combinatorial and polyhedral variant of algebraic geometry -- where there are known connections between tropical rational maps and feedforward neural networks.","Our work builds on and expands this connection to capitalize on the rich theory of tropical geometry to characterize and study various architectural aspects of neural networks.","Our contributions are threefold: we provide a novel tropical geometric approach to selecting sampling domains among linear regions; an algebraic result allowing for a guided restriction of the sampling domain for network architectures with symmetries; and an open source library to analyze neural networks as tropical Puiseux rational maps.","We provide a comprehensive set of proof-of-concept numerical experiments demonstrating the breadth of neural network architectures to which tropical geometric theory can be applied to reveal insights on expressivity characteristics of a network.","Our work provides the foundations for the adaptation of both theory and existing software from computational tropical geometry and symbolic computation to deep learning."],"url":"http://arxiv.org/abs/2405.20174v1","category":"cs.LG"}
{"created":"2024-05-30 15:23:34","title":"Heidelberg-Boston @ SIGTYP 2024 Shared Task: Enhancing Low-Resource Language Analysis With Character-Aware Hierarchical Transformers","abstract":"Historical languages present unique challenges to the NLP community, with one prominent hurdle being the limited resources available in their closed corpora. This work describes our submission to the constrained subtask of the SIGTYP 2024 shared task, focusing on PoS tagging, morphological tagging, and lemmatization for 13 historical languages. For PoS and morphological tagging we adapt a hierarchical tokenization method from Sun et al. (2023) and combine it with the advantages of the DeBERTa-V3 architecture, enabling our models to efficiently learn from every character in the training data. We also demonstrate the effectiveness of character-level T5 models on the lemmatization task. Pre-trained from scratch with limited data, our models achieved first place in the constrained subtask, nearly reaching the performance levels of the unconstrained task's winner. Our code is available at https://github.com/bowphs/SIGTYP-2024-hierarchical-transformers","sentences":["Historical languages present unique challenges to the NLP community, with one prominent hurdle being the limited resources available in their closed corpora.","This work describes our submission to the constrained subtask of the SIGTYP 2024 shared task, focusing on PoS tagging, morphological tagging, and lemmatization for 13 historical languages.","For PoS and morphological tagging we adapt a hierarchical tokenization method from Sun et al.","(2023)","and combine it with the advantages of the DeBERTa-V3 architecture, enabling our models to efficiently learn from every character in the training data.","We also demonstrate the effectiveness of character-level T5 models on the lemmatization task.","Pre-trained from scratch with limited data, our models achieved first place in the constrained subtask, nearly reaching the performance levels of the unconstrained task's winner.","Our code is available at https://github.com/bowphs/SIGTYP-2024-hierarchical-transformers"],"url":"http://arxiv.org/abs/2405.20145v1","category":"cs.CL"}
{"created":"2024-05-30 14:40:35","title":"Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks","abstract":"Safety, security, and compliance are essential requirements when aligning large language models (LLMs). However, many seemingly aligned LLMs are soon shown to be susceptible to jailbreak attacks. These attacks aim to circumvent the models' safety guardrails and security mechanisms by introducing jailbreak prompts into malicious queries. In response to these challenges, this paper introduces Defensive Prompt Patch (DPP), a novel prompt-based defense mechanism specifically designed to protect LLMs against such sophisticated jailbreak strategies. Unlike previous approaches, which have often compromised the utility of the model for the sake of safety, DPP is designed to achieve a minimal Attack Success Rate (ASR) while preserving the high utility of LLMs. Our method uses strategically designed interpretable suffix prompts that effectively thwart a wide range of standard and adaptive jailbreak techniques. Empirical results conducted on LLAMA-2-7B-Chat and Mistral-7B-Instruct-v0.2 models demonstrate the robustness and adaptability of DPP, showing significant reductions in ASR with negligible impact on utility. Our approach not only outperforms existing defense strategies in balancing safety and functionality, but also provides a scalable and interpretable solution applicable to various LLM platforms.","sentences":["Safety, security, and compliance are essential requirements when aligning large language models (LLMs).","However, many seemingly aligned LLMs are soon shown to be susceptible to jailbreak attacks.","These attacks aim to circumvent the models' safety guardrails and security mechanisms by introducing jailbreak prompts into malicious queries.","In response to these challenges, this paper introduces Defensive Prompt Patch (DPP), a novel prompt-based defense mechanism specifically designed to protect LLMs against such sophisticated jailbreak strategies.","Unlike previous approaches, which have often compromised the utility of the model for the sake of safety, DPP is designed to achieve a minimal Attack Success Rate (ASR) while preserving the high utility of LLMs.","Our method uses strategically designed interpretable suffix prompts that effectively thwart a wide range of standard and adaptive jailbreak techniques.","Empirical results conducted on LLAMA-2-7B-Chat and Mistral-7B-Instruct-v0.2 models demonstrate the robustness and adaptability of DPP, showing significant reductions in ASR with negligible impact on utility.","Our approach not only outperforms existing defense strategies in balancing safety and functionality, but also provides a scalable and interpretable solution applicable to various LLM platforms."],"url":"http://arxiv.org/abs/2405.20099v1","category":"cs.CR"}
{"created":"2024-05-30 13:56:55","title":"A theory of stratification learning","abstract":"Given i.i.d. sample from a stratified mixture of immersed manifolds of different dimensions, we study the minimax estimation of the underlying stratified structure. We provide a constructive algorithm allowing to estimate each mixture component at its optimal dimension-specific rate adaptively. The method is based on an ascending hierarchical co-detection of points belonging to different layers, which also identifies the number of layers and their dimensions, assigns each data point to a layer accurately, and estimates tangent spaces optimally. These results hold regardless of any ambient assumption on the manifolds or on their intersection configurations. They open the way to a broad clustering framework, where each mixture component models a cluster emanating from a specific nonlinear correlation phenomenon.","sentences":["Given i.i.d. sample from a stratified mixture of immersed manifolds of different dimensions, we study the minimax estimation of the underlying stratified structure.","We provide a constructive algorithm allowing to estimate each mixture component at its optimal dimension-specific rate adaptively.","The method is based on an ascending hierarchical co-detection of points belonging to different layers, which also identifies the number of layers and their dimensions, assigns each data point to a layer accurately, and estimates tangent spaces optimally.","These results hold regardless of any ambient assumption on the manifolds or on their intersection configurations.","They open the way to a broad clustering framework, where each mixture component models a cluster emanating from a specific nonlinear correlation phenomenon."],"url":"http://arxiv.org/abs/2405.20066v1","category":"math.ST"}
{"created":"2024-05-30 13:13:12","title":"A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of $\u0398(T^{2/3})$ and its Application to Best-of-Both-Worlds","abstract":"Follow-the-Regularized-Leader (FTRL) is a powerful framework for various online learning problems. By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment. However, most existing adaptive learning rates are for online learning problems with a minimax regret of $\\Theta(\\sqrt{T})$ for the number of rounds $T$, and there are only a few studies on adaptive learning rates for problems with a minimax regret of $\\Theta(T^{2/3})$, which include several important problems dealing with indirect feedback. To address this limitation, we establish a new adaptive learning rate framework for problems with a minimax regret of $\\Theta(T^{2/3})$. Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of $\\Theta(T^{2/3})$. As applications of this framework, we consider two major problems dealing with indirect feedback: partial monitoring and graph bandits. We show that FTRL with our learning rate and the Tsallis entropy regularizer improves existing Best-of-Both-Worlds (BOBW) regret upper bounds, which achieve simultaneous optimality in the stochastic and adversarial regimes. The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of $\\Theta(T^{2/3})$.","sentences":["Follow-the-Regularized-Leader (FTRL) is a powerful framework for various online learning problems.","By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment.","However, most existing adaptive learning rates are for online learning problems with a minimax regret of $\\Theta(\\sqrt{T})$ for the number of rounds $T$, and there are only a few studies on adaptive learning rates for problems with a minimax regret of $\\Theta(T^{2/3})$, which include several important problems dealing with indirect feedback.","To address this limitation, we establish a new adaptive learning rate framework for problems with a minimax regret of $\\Theta(T^{2/3})$.","Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of $\\Theta(T^{2/3})$. As applications of this framework, we consider two major problems dealing with indirect feedback: partial monitoring and graph bandits.","We show that FTRL with our learning rate and the Tsallis entropy regularizer improves existing Best-of-Both-Worlds (BOBW) regret upper bounds, which achieve simultaneous optimality in the stochastic and adversarial regimes.","The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of $\\Theta(T^{2/3})$."],"url":"http://arxiv.org/abs/2405.20028v1","category":"cs.LG"}
{"created":"2024-05-30 12:09:41","title":"Propagation, dissipation and breakdown in quantum anomalous Hall edge states probed by microwave edge plasmons","abstract":"The quantum anomalous Hall (QAH) effect, with its single chiral, topologically protected edge state, offers a platform for flying Majorana states as well as non-reciprocal microwave devices. While recent research showed the non-reciprocity of edge plasmons in Cr-doped $\\mathrm{(Bi_xSb_\\text{1-x})_2Te_3}$, the understanding of their dissipation remains incomplete. Our study explores edge plasmon dissipation in V-doped $\\mathrm{(Bi_xSb_\\text{1-x})_2Te_3}$ films, analyzing microwave transmission across various conditions. We identify interactions with charge puddles as a primary source of dissipation, providing insights critical for developing improved QAH-based technologies.","sentences":["The quantum anomalous Hall (QAH) effect, with its single chiral, topologically protected edge state, offers a platform for flying Majorana states as well as non-reciprocal microwave devices.","While recent research showed the non-reciprocity of edge plasmons in Cr-doped $\\mathrm{(Bi_xSb_\\text{1-x})_2Te_3}$, the understanding of their dissipation remains incomplete.","Our study explores edge plasmon dissipation in V-doped $\\mathrm{(Bi_xSb_\\text{1-x})_2Te_3}$ films, analyzing microwave transmission across various conditions.","We identify interactions with charge puddles as a primary source of dissipation, providing insights critical for developing improved QAH-based technologies."],"url":"http://arxiv.org/abs/2405.19983v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 10:30:07","title":"Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition","abstract":"We address a novel cross-domain few-shot learning task (CD-FSL) with multimodal input and unlabeled target data for egocentric action recognition. This paper simultaneously tackles two critical challenges associated with egocentric action recognition in CD-FSL settings: (1) the extreme domain gap in egocentric videos (\\eg, daily life vs. industrial domain) and (2) the computational cost for real-world applications. We propose MM-CDFSL, a domain-adaptive and computationally efficient approach designed to enhance adaptability to the target domain and improve inference speed. To address the first challenge, we propose the incorporation of multimodal distillation into the student RGB model using teacher models. Each teacher model is trained independently on source and target data for its respective modality. Leveraging only unlabeled target data during multimodal distillation enhances the student model's adaptability to the target domain. We further introduce ensemble masked inference, a technique that reduces the number of input tokens through masking. In this approach, ensemble prediction mitigates the performance degradation caused by masking, effectively addressing the second issue. Our approach outperformed the state-of-the-art CD-FSL approaches with a substantial margin on multiple egocentric datasets, improving by an average of 6.12/6.10 points for 1-shot/5-shot settings while achieving $2.2$ times faster inference speed. Project page: https://masashi-hatano.github.io/MM-CDFSL/","sentences":["We address a novel cross-domain few-shot learning task (CD-FSL) with multimodal input and unlabeled target data for egocentric action recognition.","This paper simultaneously tackles two critical challenges associated with egocentric action recognition in CD-FSL settings: (1) the extreme domain gap in egocentric videos (\\eg, daily life vs. industrial domain) and (2) the computational cost for real-world applications.","We propose MM-CDFSL, a domain-adaptive and computationally efficient approach designed to enhance adaptability to the target domain and improve inference speed.","To address the first challenge, we propose the incorporation of multimodal distillation into the student RGB model using teacher models.","Each teacher model is trained independently on source and target data for its respective modality.","Leveraging only unlabeled target data during multimodal distillation enhances the student model's adaptability to the target domain.","We further introduce ensemble masked inference, a technique that reduces the number of input tokens through masking.","In this approach, ensemble prediction mitigates the performance degradation caused by masking, effectively addressing the second issue.","Our approach outperformed the state-of-the-art CD-FSL approaches with a substantial margin on multiple egocentric datasets, improving by an average of 6.12/6.10 points for 1-shot/5-shot settings while achieving $2.2$ times faster inference speed.","Project page: https://masashi-hatano.github.io/MM-CDFSL/"],"url":"http://arxiv.org/abs/2405.19917v1","category":"cs.CV"}
{"created":"2024-05-30 09:47:33","title":"Improving the Fidelity of CNOT Circuits on NISQ Hardware","abstract":"We introduce an improved CNOT synthesis algorithm that considers nearest-neighbour interactions and CNOT gate error rates in noisy intermediate-scale quantum (NISQ) hardware. Compared to IBM's Qiskit compiler, it improves the fidelity of a synthesized CNOT circuit by about 2 times on average (up to 9 times). It lowers the synthesized CNOT count by a factor of 13 on average (up to a factor of 162).   Our contribution is twofold. First, we define a $\\textsf{Cost}$ function by approximating the average gate fidelity $F_{avg}$. According to the simulation results, $\\textsf{Cost}$ fits the error probability of a noisy CNOT circuit, $\\textsf{Prob} = 1 - F_{avg}$, much tighter than the commonly used cost functions. On IBM's fake Nairobi backend, it matches $\\textsf{Prob}$ to within $10^{-3}$. On other backends, it fits $\\textsf{Prob}$ to within $10^{-1}$. $\\textsf{Cost}$ accurately quantifies the dynamic error characteristics and shows remarkable scalability. Second, we propose a noise-aware CNOT routing algorithm, NAPermRowCol, by adapting the leading Steiner-tree-based connectivity-aware CNOT synthesis algorithms. A weighted edge is used to encode a CNOT gate error rate and $\\textsf{Cost}$-instructed heuristics are applied to each reduction step. NAPermRowCol does not use ancillary qubits and is not restricted to certain initial qubit maps. Compared with algorithms that are noise-agnostic, it improves the fidelity of a synthesized CNOT circuit across varied NISQ hardware. Depending on the benchmark circuit and the IBM backend selected, it lowers the synthesized CNOT count up to $56.95\\%$ compared to ROWCOL and up to $21.62\\%$ compared to PermRowCol. It reduces the synthesis $\\textsf{Cost}$ up to $25.71\\%$ compared to ROWCOL and up to $9.12\\%$ compared to PermRowCol. Our method can be extended to route a more general quantum circuit, giving a powerful new tool for compiling on NISQ devices.","sentences":["We introduce an improved CNOT synthesis algorithm that considers nearest-neighbour interactions and CNOT gate error rates in noisy intermediate-scale quantum (NISQ) hardware.","Compared to IBM's Qiskit compiler, it improves the fidelity of a synthesized CNOT circuit by about 2 times on average (up to 9 times).","It lowers the synthesized CNOT count by a factor of 13 on average (up to a factor of 162).   ","Our contribution is twofold.","First, we define a $\\textsf{Cost}$ function by approximating the average gate fidelity $F_{avg}$.","According to the simulation results, $\\textsf{Cost}$ fits the error probability of a noisy CNOT circuit, $\\textsf{Prob} = 1 - F_{avg}$, much tighter than the commonly used cost functions.","On IBM's fake Nairobi backend, it matches $\\textsf{Prob}$ to within $10^{-3}$. On other backends, it fits $\\textsf{Prob}$ to within $10^{-1}$. $\\textsf{Cost}$ accurately quantifies the dynamic error characteristics and shows remarkable scalability.","Second, we propose a noise-aware CNOT routing algorithm, NAPermRowCol, by adapting the leading Steiner-tree-based connectivity-aware CNOT synthesis algorithms.","A weighted edge is used to encode a CNOT gate error rate and $\\textsf{Cost}$-instructed heuristics are applied to each reduction step.","NAPermRowCol does not use ancillary qubits and is not restricted to certain initial qubit maps.","Compared with algorithms that are noise-agnostic, it improves the fidelity of a synthesized CNOT circuit across varied NISQ hardware.","Depending on the benchmark circuit and the IBM backend selected, it lowers the synthesized CNOT count up to $56.95\\%$ compared to ROWCOL and up to $21.62\\%$ compared to PermRowCol.","It reduces the synthesis $\\textsf{Cost}$ up to $25.71\\%$ compared to ROWCOL and up to $9.12\\%$ compared to PermRowCol.","Our method can be extended to route a more general quantum circuit, giving a powerful new tool for compiling on NISQ devices."],"url":"http://arxiv.org/abs/2405.19891v1","category":"quant-ph"}
{"created":"2024-05-30 08:50:41","title":"The Neumann boundary condition for the two-dimensional Lax-Wendroff scheme. II","abstract":"We study the stability of a two-dimensional Lax-Wendroff scheme in a quarter-plane. Following our previous work, we aim here at adapting the energy method in order to study second order extrapolation boundary conditions. We first show on the one-dimensional problem why modifying the energy is a necessity in order to obtain stability estimates. We then study the two-dimensional case and propose a modified energy as well as second order extrapolation boundary and corner conditions in order to maintain second order accuracy and stability of the whole scheme, including near the corner.","sentences":["We study the stability of a two-dimensional Lax-Wendroff scheme in a quarter-plane.","Following our previous work, we aim here at adapting the energy method in order to study second order extrapolation boundary conditions.","We first show on the one-dimensional problem why modifying the energy is a necessity in order to obtain stability estimates.","We then study the two-dimensional case and propose a modified energy as well as second order extrapolation boundary and corner conditions in order to maintain second order accuracy and stability of the whole scheme, including near the corner."],"url":"http://arxiv.org/abs/2405.19844v1","category":"math.NA"}
{"created":"2024-05-30 08:45:09","title":"Anomalously Strong Size Effect on Thermal Conductivity of Diamond Microparticles","abstract":"Diamond has the known highest thermal conductivity of around \\SI{2000}{\\watt\\per\\meter\\per\\kelvin} and is therefore widely used for heat dissipation. In practical applications, synthetic diamond microparticles are usually assumed to have similar thermal conductivity to that of bulk diamond because the particle size is larger than theoretical phonon mean free path so that boundary scattering of heat-carrying phonons is absent. In this report, we find the thermal conductivity of diamond microparticles anomalously depends on their sizes. Thermal conductivity of diamond microparticles increases from \\SI{400}{\\watt\\per\\meter\\per\\kelvin} to \\SI{2000}{\\watt\\per\\meter\\per\\kelvin} with the size growing from \\SI{20}{\\micro\\meter} to \\SI{300}{\\micro\\meter}. We attribute the abnormally strong size effect to the long-range defects during the growth process based on analysis of point defects, dislocations, and thermal penetration depth dependence of thermal conductivity. Our results play a vital role in the design of diamond composites and in the improvement of thermal conductivity of synthetic diamonds.","sentences":["Diamond has the known highest thermal conductivity of around \\SI{2000}{\\watt\\per\\meter\\per\\kelvin} and is therefore widely used for heat dissipation.","In practical applications, synthetic diamond microparticles are usually assumed to have similar thermal conductivity to that of bulk diamond because the particle size is larger than theoretical phonon mean free path so that boundary scattering of heat-carrying phonons is absent.","In this report, we find the thermal conductivity of diamond microparticles anomalously depends on their sizes.","Thermal conductivity of diamond microparticles increases from \\SI{400}{\\watt\\per\\meter\\per\\kelvin} to \\SI{2000}{\\watt\\per\\meter\\per\\kelvin} with the size growing from \\SI{20}{\\micro\\meter} to \\SI{300}{\\micro\\meter}.","We attribute the abnormally strong size effect to the long-range defects during the growth process based on analysis of point defects, dislocations, and thermal penetration depth dependence of thermal conductivity.","Our results play a vital role in the design of diamond composites and in the improvement of thermal conductivity of synthetic diamonds."],"url":"http://arxiv.org/abs/2405.19835v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 07:34:05","title":"All-In-One Medical Image Restoration via Task-Adaptive Routing","abstract":"Although single-task medical image restoration (MedIR) has witnessed remarkable success, the limited generalizability of these methods poses a substantial obstacle to wider application. In this paper, we focus on the task of all-in-one medical image restoration, aiming to address multiple distinct MedIR tasks with a single universal model. Nonetheless, due to significant differences between different MedIR tasks, training a universal model often encounters task interference issues, where different tasks with shared parameters may conflict with each other in the gradient update direction. This task interference leads to deviation of the model update direction from the optimal path, thereby affecting the model's performance. To tackle this issue, we propose a task-adaptive routing strategy, allowing conflicting tasks to select different network paths in spatial and channel dimensions, thereby mitigating task interference. Experimental results demonstrate that our proposed \\textbf{A}ll-in-one \\textbf{M}edical \\textbf{I}mage \\textbf{R}estoration (\\textbf{AMIR}) network achieves state-of-the-art performance in three MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis, both in single-task and all-in-one settings. The code and data will be available at \\href{https://github.com/Yaziwel/All-In-One-Medical-Image-Restoration-via-Task-Adaptive-Routing.git}{https://github.com/Yaziwel/AMIR}.","sentences":["Although single-task medical image restoration (MedIR) has witnessed remarkable success, the limited generalizability of these methods poses a substantial obstacle to wider application.","In this paper, we focus on the task of all-in-one medical image restoration, aiming to address multiple distinct MedIR tasks with a single universal model.","Nonetheless, due to significant differences between different MedIR tasks, training a universal model often encounters task interference issues, where different tasks with shared parameters may conflict with each other in the gradient update direction.","This task interference leads to deviation of the model update direction from the optimal path, thereby affecting the model's performance.","To tackle this issue, we propose a task-adaptive routing strategy, allowing conflicting tasks to select different network paths in spatial and channel dimensions, thereby mitigating task interference.","Experimental results demonstrate that our proposed \\textbf{A}ll-in-one \\textbf{M}edical \\textbf{I}mage \\textbf{R}estoration (\\textbf{AMIR}) network achieves state-of-the-art performance in three MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis, both in single-task and all-in-one settings.","The code and data will be available at \\href{https://github.com/Yaziwel/All-In-One-Medical-Image-Restoration-via-Task-Adaptive-Routing.git}{https://github.com/Yaziwel/AMIR}."],"url":"http://arxiv.org/abs/2405.19769v1","category":"cs.CV"}
{"created":"2024-05-30 06:31:03","title":"Twin Deformable Point Convolutions for Point Cloud Semantic Segmentation in Remote Sensing Scenes","abstract":"Thanks to the application of deep learning technology in point cloud processing of the remote sensing field, point cloud segmentation has become a research hotspot in recent years, which can be applied to real-world 3D, smart cities, and other fields. Although existing solutions have made unprecedented progress, they ignore the inherent characteristics of point clouds in remote sensing fields that are strictly arranged according to latitude, longitude, and altitude, which brings great convenience to the segmentation of point clouds in remote sensing fields. To consider this property cleverly, we propose novel convolution operators, termed Twin Deformable point Convolutions (TDConvs), which aim to achieve adaptive feature learning by learning deformable sampling points in the latitude-longitude plane and altitude direction, respectively. First, to model the characteristics of the latitude-longitude plane, we propose a Cylinder-wise Deformable point Convolution (CyDConv) operator, which generates a two-dimensional cylinder map by constructing a cylinder-like grid in the latitude-longitude direction. Furthermore, to better integrate the features of the latitude-longitude plane and the spatial geometric features, we perform a multi-scale fusion of the extracted latitude-longitude features and spatial geometric features, and realize it through the aggregation of adjacent point features of different scales. In addition, a Sphere-wise Deformable point Convolution (SpDConv) operator is introduced to adaptively offset the sampling points in three-dimensional space by constructing a sphere grid structure, aiming at modeling the characteristics in the altitude direction. Experiments on existing popular benchmarks conclude that our TDConvs achieve the best segmentation performance, surpassing the existing state-of-the-art methods.","sentences":["Thanks to the application of deep learning technology in point cloud processing of the remote sensing field, point cloud segmentation has become a research hotspot in recent years, which can be applied to real-world 3D, smart cities, and other fields.","Although existing solutions have made unprecedented progress, they ignore the inherent characteristics of point clouds in remote sensing fields that are strictly arranged according to latitude, longitude, and altitude, which brings great convenience to the segmentation of point clouds in remote sensing fields.","To consider this property cleverly, we propose novel convolution operators, termed Twin Deformable point Convolutions (TDConvs), which aim to achieve adaptive feature learning by learning deformable sampling points in the latitude-longitude plane and altitude direction, respectively.","First, to model the characteristics of the latitude-longitude plane, we propose a Cylinder-wise Deformable point Convolution (CyDConv) operator, which generates a two-dimensional cylinder map by constructing a cylinder-like grid in the latitude-longitude direction.","Furthermore, to better integrate the features of the latitude-longitude plane and the spatial geometric features, we perform a multi-scale fusion of the extracted latitude-longitude features and spatial geometric features, and realize it through the aggregation of adjacent point features of different scales.","In addition, a Sphere-wise Deformable point Convolution (SpDConv) operator is introduced to adaptively offset the sampling points in three-dimensional space by constructing a sphere grid structure, aiming at modeling the characteristics in the altitude direction.","Experiments on existing popular benchmarks conclude that our TDConvs achieve the best segmentation performance, surpassing the existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.19735v1","category":"cs.CV"}
{"created":"2024-05-30 05:43:01","title":"SimiSketch: Efficiently Estimating Similarity of streaming Multisets","abstract":"The challenge of estimating similarity between sets has been a significant concern in data science, finding diverse applications across various domains. However, previous approaches, such as MinHash, have predominantly centered around hashing techniques, which are well-suited for sets but less naturally adaptable to multisets, a common occurrence in scenarios like network streams and text data. Moreover, with the increasing prevalence of data arriving in streaming patterns, many existing methods struggle to handle cases where set items are presented in a continuous stream. Consequently, our focus in this paper is on the challenging scenario of multisets with item streams. To address this, we propose SimiSketch, a sketching algorithm designed to tackle this specific problem. The paper begins by presenting two simpler versions that employ intuitive sketches for similarity estimation. Subsequently, we formally introduce SimiSketch and leverage SALSA to enhance accuracy. To validate our algorithms, we conduct extensive testing on synthetic datasets, real-world network traffic, and text articles. Our experiment shows that compared with the state-of-the-art, SimiSketch can improve the accuracy by up to 42 times, and increase the throughput by up to 360 times. The complete source code is open-sourced and available on GitHub for reference.","sentences":["The challenge of estimating similarity between sets has been a significant concern in data science, finding diverse applications across various domains.","However, previous approaches, such as MinHash, have predominantly centered around hashing techniques, which are well-suited for sets but less naturally adaptable to multisets, a common occurrence in scenarios like network streams and text data.","Moreover, with the increasing prevalence of data arriving in streaming patterns, many existing methods struggle to handle cases where set items are presented in a continuous stream.","Consequently, our focus in this paper is on the challenging scenario of multisets with item streams.","To address this, we propose SimiSketch, a sketching algorithm designed to tackle this specific problem.","The paper begins by presenting two simpler versions that employ intuitive sketches for similarity estimation.","Subsequently, we formally introduce SimiSketch and leverage SALSA to enhance accuracy.","To validate our algorithms, we conduct extensive testing on synthetic datasets, real-world network traffic, and text articles.","Our experiment shows that compared with the state-of-the-art, SimiSketch can improve the accuracy by up to 42 times, and increase the throughput by up to 360 times.","The complete source code is open-sourced and available on GitHub for reference."],"url":"http://arxiv.org/abs/2405.19711v1","category":"cs.DS"}
{"created":"2024-05-30 05:15:38","title":"Distribution Aligned Semantics Adaption for Lifelong Person Re-Identification","abstract":"In real-world scenarios, person Re-IDentification (Re-ID) systems need to be adaptable to changes in space and time. Therefore, the adaptation of Re-ID models to new domains while preserving previously acquired knowledge is crucial, known as Lifelong person Re-IDentification (LReID). Advanced LReID methods rely on replaying exemplars from old domains and applying knowledge distillation in logits with old models. However, due to privacy concerns, retaining previous data is inappropriate. Additionally, the fine-grained and open-set characteristics of Re-ID limit the effectiveness of the distillation paradigm for accumulating knowledge. We argue that a Re-ID model trained on diverse and challenging pedestrian images at a large scale can acquire robust and general human semantic knowledge. These semantics can be readily utilized as shared knowledge for lifelong applications. In this paper, we identify the challenges and discrepancies associated with adapting a pre-trained model to each application domain, and introduce the Distribution Aligned Semantics Adaption (DASA) framework. It efficiently adjusts Batch Normalization (BN) to mitigate interference from data distribution discrepancy and freezes the pre-trained convolutional layers to preserve shared knowledge. Additionally, we propose the lightweight Semantics Adaption (SA) module, which effectively adapts learned semantics to enhance pedestrian representations. Extensive experiments demonstrate the remarkable superiority of our proposed framework over advanced LReID methods, and it exhibits significantly reduced storage consumption. DASA presents a novel and cost-effective perspective on effectively adapting pre-trained models for LReID.","sentences":["In real-world scenarios, person Re-IDentification (Re-ID) systems need to be adaptable to changes in space and time.","Therefore, the adaptation of Re-ID models to new domains while preserving previously acquired knowledge is crucial, known as Lifelong person Re-IDentification (LReID).","Advanced LReID methods rely on replaying exemplars from old domains and applying knowledge distillation in logits with old models.","However, due to privacy concerns, retaining previous data is inappropriate.","Additionally, the fine-grained and open-set characteristics of Re-ID limit the effectiveness of the distillation paradigm for accumulating knowledge.","We argue that a Re-ID model trained on diverse and challenging pedestrian images at a large scale can acquire robust and general human semantic knowledge.","These semantics can be readily utilized as shared knowledge for lifelong applications.","In this paper, we identify the challenges and discrepancies associated with adapting a pre-trained model to each application domain, and introduce the Distribution Aligned Semantics Adaption (DASA) framework.","It efficiently adjusts Batch Normalization (BN) to mitigate interference from data distribution discrepancy and freezes the pre-trained convolutional layers to preserve shared knowledge.","Additionally, we propose the lightweight Semantics Adaption (SA) module, which effectively adapts learned semantics to enhance pedestrian representations.","Extensive experiments demonstrate the remarkable superiority of our proposed framework over advanced LReID methods, and it exhibits significantly reduced storage consumption.","DASA presents a novel and cost-effective perspective on effectively adapting pre-trained models for LReID."],"url":"http://arxiv.org/abs/2405.19695v1","category":"cs.CV"}
{"created":"2024-05-30 04:37:57","title":"Fully Test-Time Adaptation for Monocular 3D Object Detection","abstract":"Monocular 3D object detection (Mono 3Det) aims to identify 3D objects from a single RGB image. However, existing methods often assume training and test data follow the same distribution, which may not hold in real-world test scenarios. To address the out-of-distribution (OOD) problems, we explore a new adaptation paradigm for Mono 3Det, termed Fully Test-time Adaptation. It aims to adapt a well-trained model to unlabeled test data by handling potential data distribution shifts at test time without access to training data and test labels. However, applying this paradigm in Mono 3Det poses significant challenges due to OOD test data causing a remarkable decline in object detection scores. This decline conflicts with the pre-defined score thresholds of existing detection methods, leading to severe object omissions (i.e., rare positive detections and many false negatives). Consequently, the limited positive detection and plenty of noisy predictions cause test-time adaptation to fail in Mono 3Det. To handle this problem, we propose a novel Monocular Test-Time Adaptation (MonoTTA) method, based on two new strategies. 1) Reliability-driven adaptation: we empirically find that high-score objects are still reliable and the optimization of high-score objects can enhance confidence across all detections. Thus, we devise a self-adaptive strategy to identify reliable objects for model adaptation, which discovers potential objects and alleviates omissions. 2) Noise-guard adaptation: since high-score objects may be scarce, we develop a negative regularization term to exploit the numerous low-score objects via negative learning, preventing overfitting to noise and trivial solutions. Experimental results show that MonoTTA brings significant performance gains for Mono 3Det models in OOD test scenarios, approximately 190% gains by average on KITTI and 198% gains on nuScenes.","sentences":["Monocular 3D object detection (Mono 3Det) aims to identify 3D objects from a single RGB image.","However, existing methods often assume training and test data follow the same distribution, which may not hold in real-world test scenarios.","To address the out-of-distribution (OOD) problems, we explore a new adaptation paradigm for Mono 3Det, termed Fully Test-time Adaptation.","It aims to adapt a well-trained model to unlabeled test data by handling potential data distribution shifts at test time without access to training data and test labels.","However, applying this paradigm in Mono 3Det poses significant challenges due to OOD test data causing a remarkable decline in object detection scores.","This decline conflicts with the pre-defined score thresholds of existing detection methods, leading to severe object omissions (i.e., rare positive detections and many false negatives).","Consequently, the limited positive detection and plenty of noisy predictions cause test-time adaptation to fail in Mono 3Det.","To handle this problem, we propose a novel Monocular Test-Time Adaptation (MonoTTA) method, based on two new strategies.","1) Reliability-driven adaptation: we empirically find that high-score objects are still reliable and the optimization of high-score objects can enhance confidence across all detections.","Thus, we devise a self-adaptive strategy to identify reliable objects for model adaptation, which discovers potential objects and alleviates omissions.","2) Noise-guard adaptation: since high-score objects may be scarce, we develop a negative regularization term to exploit the numerous low-score objects via negative learning, preventing overfitting to noise and trivial solutions.","Experimental results show that MonoTTA brings significant performance gains for Mono 3Det models in OOD test scenarios, approximately 190% gains by average on KITTI and 198% gains on nuScenes."],"url":"http://arxiv.org/abs/2405.19682v1","category":"cs.CV"}
{"created":"2024-05-30 04:04:36","title":"Knowledge-grounded Adaptation Strategy for Vision-language Models: Building Unique Case-set for Screening Mammograms for Residents Training","abstract":"A visual-language model (VLM) pre-trained on natural images and text pairs poses a significant barrier when applied to medical contexts due to domain shift. Yet, adapting or fine-tuning these VLMs for medical use presents considerable hurdles, including domain misalignment, limited access to extensive datasets, and high-class imbalances. Hence, there is a pressing need for strategies to effectively adapt these VLMs to the medical domain, as such adaptations would prove immensely valuable in healthcare applications. In this study, we propose a framework designed to adeptly tailor VLMs to the medical domain, employing selective sampling and hard-negative mining techniques for enhanced performance in retrieval tasks. We validate the efficacy of our proposed approach by implementing it across two distinct VLMs: the in-domain VLM (MedCLIP) and out-of-domain VLMs (ALBEF). We assess the performance of these models both in their original off-the-shelf state and after undergoing our proposed training strategies, using two extensive datasets containing mammograms and their corresponding reports. Our evaluation spans zero-shot, few-shot, and supervised scenarios. Through our approach, we observe a notable enhancement in Recall@K performance for the image-text retrieval task.","sentences":["A visual-language model (VLM) pre-trained on natural images and text pairs poses a significant barrier when applied to medical contexts due to domain shift.","Yet, adapting or fine-tuning these VLMs for medical use presents considerable hurdles, including domain misalignment, limited access to extensive datasets, and high-class imbalances.","Hence, there is a pressing need for strategies to effectively adapt these VLMs to the medical domain, as such adaptations would prove immensely valuable in healthcare applications.","In this study, we propose a framework designed to adeptly tailor VLMs to the medical domain, employing selective sampling and hard-negative mining techniques for enhanced performance in retrieval tasks.","We validate the efficacy of our proposed approach by implementing it across two distinct VLMs: the in-domain VLM (MedCLIP) and out-of-domain VLMs (ALBEF).","We assess the performance of these models both in their original off-the-shelf state and after undergoing our proposed training strategies, using two extensive datasets containing mammograms and their corresponding reports.","Our evaluation spans zero-shot, few-shot, and supervised scenarios.","Through our approach, we observe a notable enhancement in Recall@K performance for the image-text retrieval task."],"url":"http://arxiv.org/abs/2405.19675v1","category":"cs.CV"}
{"created":"2024-05-30 03:44:54","title":"One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models","abstract":"Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune the LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capacities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across nine question-answering tasks demonstrate the superiority of our approach.","sentences":["Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content.","Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune the LLMs to adapt to RAG scenarios.","Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters.","This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality.","To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG.","By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capacities.","Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method.","Comprehensive experiments across nine question-answering tasks demonstrate the superiority of our approach."],"url":"http://arxiv.org/abs/2405.19670v1","category":"cs.CL"}
{"created":"2024-05-30 03:32:44","title":"MGCP: A Multi-Grained Correlation based Prediction Network for Multivariate Time Series","abstract":"Multivariate time series prediction is widely used in daily life, which poses significant challenges due to the complex correlations that exist at multi-grained levels. Unfortunately, the majority of current time series prediction models fail to simultaneously learn the correlations of multivariate time series at multi-grained levels, resulting in suboptimal performance. To address this, we propose a Multi-Grained Correlations-based Prediction (MGCP) Network, which simultaneously considers the correlations at three granularity levels to enhance prediction performance. Specifically, MGCP utilizes Adaptive Fourier Neural Operators and Graph Convolutional Networks to learn the global spatiotemporal correlations and inter-series correlations, enabling the extraction of potential features from multivariate time series at fine-grained and medium-grained levels. Additionally, MGCP employs adversarial training with an attention mechanism-based predictor and conditional discriminator to optimize prediction results at coarse-grained level, ensuring high fidelity between the generated forecast results and the actual data distribution. Finally, we compare MGCP with several state-of-the-art time series prediction algorithms on real-world benchmark datasets, and our results demonstrate the generality and effectiveness of the proposed model.","sentences":["Multivariate time series prediction is widely used in daily life, which poses significant challenges due to the complex correlations that exist at multi-grained levels.","Unfortunately, the majority of current time series prediction models fail to simultaneously learn the correlations of multivariate time series at multi-grained levels, resulting in suboptimal performance.","To address this, we propose a Multi-Grained Correlations-based Prediction (MGCP) Network, which simultaneously considers the correlations at three granularity levels to enhance prediction performance.","Specifically, MGCP utilizes Adaptive Fourier Neural Operators and Graph Convolutional Networks to learn the global spatiotemporal correlations and inter-series correlations, enabling the extraction of potential features from multivariate time series at fine-grained and medium-grained levels.","Additionally, MGCP employs adversarial training with an attention mechanism-based predictor and conditional discriminator to optimize prediction results at coarse-grained level, ensuring high fidelity between the generated forecast results and the actual data distribution.","Finally, we compare MGCP with several state-of-the-art time series prediction algorithms on real-world benchmark datasets, and our results demonstrate the generality and effectiveness of the proposed model."],"url":"http://arxiv.org/abs/2405.19661v1","category":"cs.LG"}
{"created":"2024-05-30 03:11:21","title":"Dual sparse training framework: inducing activation map sparsity via Transformed $\\ell1$ regularization","abstract":"Although deep convolutional neural networks have achieved rapid development, it is challenging to widely promote and apply these models on low-power devices, due to computational and storage limitations. To address this issue, researchers have proposed techniques such as model compression, activation sparsity induction, and hardware accelerators. This paper presents a method to induce the sparsity of activation maps based on Transformed $\\ell1$ regularization, so as to improve the research in the field of activation sparsity induction. Further, the method is innovatively combined with traditional pruning, constituting a dual sparse training framework. Compared to previous methods, Transformed $\\ell1$ can achieve higher sparsity and better adapt to different network structures. Experimental results show that the method achieves improvements by more than 20\\% in activation map sparsity on most models and corresponding datasets without compromising the accuracy. Specifically, it achieves a 27.52\\% improvement for ResNet18 on the ImageNet dataset, and a 44.04\\% improvement for LeNet5 on the MNIST dataset. In addition, the dual sparse training framework can greatly reduce the computational load and provide potential for reducing the required storage during runtime. Specifically, the ResNet18 and ResNet50 models obtained by the dual sparse training framework respectively reduce 81.7\\% and 84.13\\% of multiplicative floating-point operations, while maintaining accuracy and a low pruning rate.","sentences":["Although deep convolutional neural networks have achieved rapid development, it is challenging to widely promote and apply these models on low-power devices, due to computational and storage limitations.","To address this issue, researchers have proposed techniques such as model compression, activation sparsity induction, and hardware accelerators.","This paper presents a method to induce the sparsity of activation maps based on Transformed $\\ell1$ regularization, so as to improve the research in the field of activation sparsity induction.","Further, the method is innovatively combined with traditional pruning, constituting a dual sparse training framework.","Compared to previous methods, Transformed $\\ell1$ can achieve higher sparsity and better adapt to different network structures.","Experimental results show that the method achieves improvements by more than 20\\% in activation map sparsity on most models and corresponding datasets without compromising the accuracy.","Specifically, it achieves a 27.52\\% improvement for ResNet18 on the ImageNet dataset, and a 44.04\\% improvement for LeNet5 on the MNIST dataset.","In addition, the dual sparse training framework can greatly reduce the computational load and provide potential for reducing the required storage during runtime.","Specifically, the ResNet18 and ResNet50 models obtained by the dual sparse training framework respectively reduce 81.7\\% and 84.13\\% of multiplicative floating-point operations, while maintaining accuracy and a low pruning rate."],"url":"http://arxiv.org/abs/2405.19652v1","category":"cs.CV"}
{"created":"2024-05-30 03:02:23","title":"Towards Deeper Understanding of PPR-based Embedding Approaches: A Topological Perspective","abstract":"Node embedding learns low-dimensional vectors for nodes in the graph. Recent state-of-the-art embedding approaches take Personalized PageRank (PPR) as the proximity measure and factorize the PPR matrix or its adaptation to generate embeddings. However, little previous work analyzes what information is encoded by these approaches, and how the information correlates with their superb performance in downstream tasks. In this work, we first show that state-of-the-art embedding approaches that factorize a PPR-related matrix can be unified into a closed-form framework. Then, we study whether the embeddings generated by this strategy can be inverted to better recover the graph topology information than random-walk based embeddings. To achieve this, we propose two methods for recovering graph topology via PPR-based embeddings, including the analytical method and the optimization method. Extensive experimental results demonstrate that the embeddings generated by factorizing a PPR-related matrix maintain more topological information, such as common edges and community structures, than that generated by random walks, paving a new way to systematically comprehend why PPR-based node embedding approaches outperform random walk-based alternatives in various downstream tasks. To the best of our knowledge, this is the first work that focuses on the interpretability of PPR-based node embedding approaches.","sentences":["Node embedding learns low-dimensional vectors for nodes in the graph.","Recent state-of-the-art embedding approaches take Personalized PageRank (PPR) as the proximity measure and factorize the PPR matrix or its adaptation to generate embeddings.","However, little previous work analyzes what information is encoded by these approaches, and how the information correlates with their superb performance in downstream tasks.","In this work, we first show that state-of-the-art embedding approaches that factorize a PPR-related matrix can be unified into a closed-form framework.","Then, we study whether the embeddings generated by this strategy can be inverted to better recover the graph topology information than random-walk based embeddings.","To achieve this, we propose two methods for recovering graph topology via PPR-based embeddings, including the analytical method and the optimization method.","Extensive experimental results demonstrate that the embeddings generated by factorizing a PPR-related matrix maintain more topological information, such as common edges and community structures, than that generated by random walks, paving a new way to systematically comprehend why PPR-based node embedding approaches outperform random walk-based alternatives in various downstream tasks.","To the best of our knowledge, this is the first work that focuses on the interpretability of PPR-based node embedding approaches."],"url":"http://arxiv.org/abs/2405.19649v1","category":"cs.LG"}
{"created":"2024-05-30 01:42:36","title":"LBT SHARK-VIS Observes a Major Resurfacing Event on Io","abstract":"Since volcanic activity was first discovered on Io from Voyager images in 1979, changes on Io's surface have been monitored from both spacecraft and ground-based telescopes. Here, we present the highest spatial resolution images of Io ever obtained from a ground-based telescope. These images, acquired by the SHARK-VIS instrument on the Large Binocular Telescope, show evidence of a major resurfacing event on Io's trailing hemisphere. When compared to the most recent spacecraft images, the SHARK-VIS images show that a plume deposit from a powerful eruption at Pillan Patera has covered part of the long-lived Pele plume deposit. Although this type of resurfacing event may be common on Io, few have been detected due to the rarity of spacecraft visits and the previously low spatial resolution available from Earth-based telescopes. The SHARK-VIS instrument ushers in a new era of high resolution imaging of Io's surface using adaptive optics at visible wavelengths.","sentences":["Since volcanic activity was first discovered on Io from Voyager images in 1979, changes on Io's surface have been monitored from both spacecraft and ground-based telescopes.","Here, we present the highest spatial resolution images of Io ever obtained from a ground-based telescope.","These images, acquired by the SHARK-VIS instrument on the Large Binocular Telescope, show evidence of a major resurfacing event on Io's trailing hemisphere.","When compared to the most recent spacecraft images, the SHARK-VIS images show that a plume deposit from a powerful eruption at Pillan Patera has covered part of the long-lived Pele plume deposit.","Although this type of resurfacing event may be common on Io, few have been detected due to the rarity of spacecraft visits and the previously low spatial resolution available from Earth-based telescopes.","The SHARK-VIS instrument ushers in a new era of high resolution imaging of Io's surface using adaptive optics at visible wavelengths."],"url":"http://arxiv.org/abs/2405.19604v1","category":"astro-ph.EP"}
{"created":"2024-05-30 01:30:13","title":"Hybrid Quantum Algorithm for Simulating Real-Time Thermal Correlation Functions","abstract":"We present a hybrid Path Integral Monte Carlo (hPIMC) algorithm to calculate real-time quantum thermal correlation functions and demonstrate its application to open quantum systems. The hPIMC algorithm leverages the successes of classical PIMC as a computational tool for high-dimensional system studies by exactly simulating dissipation using the Feynman-Vernon influence functional on a classical computer. We achieve a quantum speed-up over the classical algorithm by computing short-time matrix elements of the quantum propagator on a quantum computer. We show that the component of imaginary-time evolution can be performed accurately using the recently developed Probabilistic Imaginary-Time Evolution (PITE) algorithm, and we introduce a novel low-depth circuit for approximate real-time evolution under the kinetic energy operator using a Discrete Variable Representation (DVR). We test the accuracy of the approximation by computing the position-position thermal correlation function of a proton transfer reaction.","sentences":["We present a hybrid Path Integral Monte Carlo (hPIMC) algorithm to calculate real-time quantum thermal correlation functions and demonstrate its application to open quantum systems.","The hPIMC algorithm leverages the successes of classical PIMC as a computational tool for high-dimensional system studies by exactly simulating dissipation using the Feynman-Vernon influence functional on a classical computer.","We achieve a quantum speed-up over the classical algorithm by computing short-time matrix elements of the quantum propagator on a quantum computer.","We show that the component of imaginary-time evolution can be performed accurately using the recently developed Probabilistic Imaginary-Time Evolution (PITE) algorithm, and we introduce a novel low-depth circuit for approximate real-time evolution under the kinetic energy operator using a Discrete Variable Representation (DVR).","We test the accuracy of the approximation by computing the position-position thermal correlation function of a proton transfer reaction."],"url":"http://arxiv.org/abs/2405.19599v1","category":"quant-ph"}
{"created":"2024-05-30 00:32:51","title":"SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation","abstract":"Acquiring a multi-task imitation policy in 3D manipulation poses challenges in terms of scene understanding and action prediction. Current methods employ both 3D representation and multi-view 2D representation to predict the poses of the robot's end-effector. However, they still require a considerable amount of high-quality robot trajectories, and suffer from limited generalization in unseen tasks and inefficient execution in long-horizon reasoning. In this paper, we propose SAM-E, a novel architecture for robot manipulation by leveraging a vision-foundation model for generalizable scene understanding and sequence imitation for long-term action reasoning. Specifically, we adopt Segment Anything (SAM) pre-trained on a huge number of images and promptable masks as the foundation model for extracting task-relevant features, and employ parameter-efficient fine-tuning on robot data for a better understanding of embodied scenarios. To address long-horizon reasoning, we develop a novel multi-channel heatmap that enables the prediction of the action sequence in a single pass, notably enhancing execution efficiency. Experimental results from various instruction-following tasks demonstrate that SAM-E achieves superior performance with higher execution efficiency compared to the baselines, and also significantly improves generalization in few-shot adaptation to new tasks.","sentences":["Acquiring a multi-task imitation policy in 3D manipulation poses challenges in terms of scene understanding and action prediction.","Current methods employ both 3D representation and multi-view 2D representation to predict the poses of the robot's end-effector.","However, they still require a considerable amount of high-quality robot trajectories, and suffer from limited generalization in unseen tasks and inefficient execution in long-horizon reasoning.","In this paper, we propose SAM-E, a novel architecture for robot manipulation by leveraging a vision-foundation model for generalizable scene understanding and sequence imitation for long-term action reasoning.","Specifically, we adopt Segment Anything (SAM) pre-trained on a huge number of images and promptable masks as the foundation model for extracting task-relevant features, and employ parameter-efficient fine-tuning on robot data for a better understanding of embodied scenarios.","To address long-horizon reasoning, we develop a novel multi-channel heatmap that enables the prediction of the action sequence in a single pass, notably enhancing execution efficiency.","Experimental results from various instruction-following tasks demonstrate that SAM-E achieves superior performance with higher execution efficiency compared to the baselines, and also significantly improves generalization in few-shot adaptation to new tasks."],"url":"http://arxiv.org/abs/2405.19586v1","category":"cs.CV"}
{"created":"2024-05-30 00:27:52","title":"The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms","abstract":"We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution.","sentences":["We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates.","We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs.","We then investigate in detail two adaptive learning rates -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem.","When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD.","Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues.","For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution."],"url":"http://arxiv.org/abs/2405.19585v1","category":"math.OC"}
{"created":"2024-05-29 21:53:24","title":"Canonical Correlation Analysis as Reduced Rank Regression in High Dimensions","abstract":"Canonical Correlation Analysis (CCA) is a widespread technique for discovering linear relationships between two sets of variables $X \\in \\mathbb{R}^{n \\times p}$ and $Y \\in \\mathbb{R}^{n \\times q}$. In high dimensions however, standard estimates of the canonical directions cease to be consistent without assuming further structure. In this setting, a possible solution consists in leveraging the presumed sparsity of the solution: only a subset of the covariates span the canonical directions. While the last decade has seen a proliferation of sparse CCA methods, practical challenges regarding the scalability and adaptability of these methods still persist. To circumvent these issues, this paper suggests an alternative strategy that uses reduced rank regression to estimate the canonical directions when one of the datasets is high-dimensional while the other remains low-dimensional. By casting the problem of estimating the canonical direction as a regression problem, our estimator is able to leverage the rich statistics literature on high-dimensional regression and is easily adaptable to accommodate a wider range of structural priors. Our proposed solution maintains computational efficiency and accuracy, even in the presence of very high-dimensional data. We validate the benefits of our approach through a series of simulated experiments and further illustrate its practicality by applying it to three real-world datasets.","sentences":["Canonical Correlation Analysis (CCA) is a widespread technique for discovering linear relationships between two sets of variables $X \\in \\mathbb{R}^{n \\times p}$ and $Y \\in \\mathbb{R}^{n \\times q}$. In high dimensions however, standard estimates of the canonical directions cease to be consistent without assuming further structure.","In this setting, a possible solution consists in leveraging the presumed sparsity of the solution: only a subset of the covariates span the canonical directions.","While the last decade has seen a proliferation of sparse CCA methods, practical challenges regarding the scalability and adaptability of these methods still persist.","To circumvent these issues, this paper suggests an alternative strategy that uses reduced rank regression to estimate the canonical directions when one of the datasets is high-dimensional while the other remains low-dimensional.","By casting the problem of estimating the canonical direction as a regression problem, our estimator is able to leverage the rich statistics literature on high-dimensional regression and is easily adaptable to accommodate a wider range of structural priors.","Our proposed solution maintains computational efficiency and accuracy, even in the presence of very high-dimensional data.","We validate the benefits of our approach through a series of simulated experiments and further illustrate its practicality by applying it to three real-world datasets."],"url":"http://arxiv.org/abs/2405.19539v1","category":"stat.ME"}
{"created":"2024-05-29 20:43:25","title":"Data-Efficient Discovery of Hyperelastic TPMS Metamaterials with Extreme Energy Dissipation","abstract":"Triply periodic minimal surfaces (TPMS) are a class of metamaterials with a variety of applications and well-known primitives. We present a new method for discovering novel microscale TPMS structures with exceptional energy-dissipation capabilities, achieving double the energy absorption of the best existing TPMS primitive structure. Our approach employs a parametric representation, allowing seamless interpolation between structures and representing a rich TPMS design space. We show that simulations are intractable for optimizing microscale hyperelastic structures, and instead propose a sample-efficient computational strategy for rapidly discovering structures with extreme energy dissipation using limited amounts of empirical data from 3D-printed and tested microscale metamaterials. This strategy ensures high-fidelity results but involves time-consuming 3D printing and testing. To address this, we leverage an uncertainty-aware Deep Ensembles model to predict microstructure behaviors and identify which structures to 3D-print and test next. We iteratively refine our model through batch Bayesian optimization, selecting structures for fabrication that maximize exploration of the performance space and exploitation of our energy-dissipation objective. Using our method, we produce the first open-source dataset of hyperelastic microscale TPMS structures, including a set of novel structures that demonstrate extreme energy dissipation capabilities. We show several potential applications of these structures in protective equipment and bone implants.","sentences":["Triply periodic minimal surfaces (TPMS) are a class of metamaterials with a variety of applications and well-known primitives.","We present a new method for discovering novel microscale TPMS structures with exceptional energy-dissipation capabilities, achieving double the energy absorption of the best existing TPMS primitive structure.","Our approach employs a parametric representation, allowing seamless interpolation between structures and representing a rich TPMS design space.","We show that simulations are intractable for optimizing microscale hyperelastic structures, and instead propose a sample-efficient computational strategy for rapidly discovering structures with extreme energy dissipation using limited amounts of empirical data from 3D-printed and tested microscale metamaterials.","This strategy ensures high-fidelity results but involves time-consuming 3D printing and testing.","To address this, we leverage an uncertainty-aware Deep Ensembles model to predict microstructure behaviors and identify which structures to 3D-print and test next.","We iteratively refine our model through batch Bayesian optimization, selecting structures for fabrication that maximize exploration of the performance space and exploitation of our energy-dissipation objective.","Using our method, we produce the first open-source dataset of hyperelastic microscale TPMS structures, including a set of novel structures that demonstrate extreme energy dissipation capabilities.","We show several potential applications of these structures in protective equipment and bone implants."],"url":"http://arxiv.org/abs/2405.19507v1","category":"cs.GR"}
{"created":"2024-05-29 20:04:23","title":"Online Nonparametric Supervised Learning for Massive Data","abstract":"Despite their benefits in terms of simplicity, low computational cost and data requirement, parametric machine learning algorithms, such as linear discriminant analysis, quadratic discriminant analysis or logistic regression, suffer from serious drawbacks including linearity, poor fit of features to the usually imposed normal distribution and high dimensionality. Batch kernel-based nonparametric classifier, which overcomes the linearity and normality of features constraints, represent an interesting alternative for supervised classification problem. However, it suffers from the ``curse of dimension\". The problem can be alleviated by the explosive sample size in the era of big data, while large-scale data size presents some challenges in the storage of data and the calculation of the classifier. These challenges make the classical batch nonparametric classifier no longer applicable. This motivates us to develop a fast algorithm adapted to the real-time calculation of the nonparametric classifier in massive as well as streaming data frameworks. This online classifier includes two steps. First, we consider an online principle components analysis to reduce the dimension of the features with a very low computation cost. Then, a stochastic approximation algorithm is deployed to obtain a real-time calculation of the nonparametric classifier. The proposed methods are evaluated and compared to some commonly used machine learning algorithms for real-time fetal well-being monitoring. The study revealed that, in terms of accuracy, the offline (or Batch), as well as, the online classifiers are good competitors to the random forest algorithm. Moreover, we show that the online classifier gives the best trade-off accuracy/computation cost compared to the offline classifier.","sentences":["Despite their benefits in terms of simplicity, low computational cost and data requirement, parametric machine learning algorithms, such as linear discriminant analysis, quadratic discriminant analysis or logistic regression, suffer from serious drawbacks including linearity, poor fit of features to the usually imposed normal distribution and high dimensionality.","Batch kernel-based nonparametric classifier, which overcomes the linearity and normality of features constraints, represent an interesting alternative for supervised classification problem.","However, it suffers from the ``curse of dimension\".","The problem can be alleviated by the explosive sample size in the era of big data, while large-scale data size presents some challenges in the storage of data and the calculation of the classifier.","These challenges make the classical batch nonparametric classifier no longer applicable.","This motivates us to develop a fast algorithm adapted to the real-time calculation of the nonparametric classifier in massive as well as streaming data frameworks.","This online classifier includes two steps.","First, we consider an online principle components analysis to reduce the dimension of the features with a very low computation cost.","Then, a stochastic approximation algorithm is deployed to obtain a real-time calculation of the nonparametric classifier.","The proposed methods are evaluated and compared to some commonly used machine learning algorithms for real-time fetal well-being monitoring.","The study revealed that, in terms of accuracy, the offline (or Batch), as well as, the online classifiers are good competitors to the random forest algorithm.","Moreover, we show that the online classifier gives the best trade-off accuracy/computation cost compared to the offline classifier."],"url":"http://arxiv.org/abs/2405.19486v1","category":"stat.ML"}
{"created":"2024-05-29 19:23:53","title":"RAP: Efficient Text-Video Retrieval with Sparse-and-Correlated Adapter","abstract":"Text-Video Retrieval (TVR) aims to align relevant video content with natural language queries. To date, most state-of-the-art TVR methods learn image-to-video transfer learning based on large-scale pre-trained visionlanguage models (e.g., CLIP). However, fully fine-tuning these pre-trained models for TVR incurs prohibitively expensive computation costs. To this end, we propose to conduct efficient text-video Retrieval with a sparse-andcorrelated AdaPter (RAP), i.e., fine-tuning the pre-trained model with a few parameterized layers. To accommodate the text-video scenario, we equip our RAP with two indispensable characteristics: temporal sparsity and correlation. Specifically, we propose a low-rank modulation module to refine the per-image features from the frozen CLIP backbone, which accentuates salient frames within the video features while alleviating temporal redundancy. Besides, we introduce an asynchronous self-attention mechanism that first selects the top responsive visual patches and augments the correlation modeling between them with learnable temporal and patch offsets. Extensive experiments on four TVR datasets demonstrate that RAP achieves superior or comparable performance compared to the fully fine-tuned counterpart and other parameter-efficient fine-tuning methods.","sentences":["Text-Video Retrieval (TVR) aims to align relevant video content with natural language queries.","To date, most state-of-the-art TVR methods learn image-to-video transfer learning based on large-scale pre-trained visionlanguage models (e.g., CLIP).","However, fully fine-tuning these pre-trained models for TVR incurs prohibitively expensive computation costs.","To this end, we propose to conduct efficient text-video Retrieval with a sparse-andcorrelated AdaPter (RAP), i.e., fine-tuning the pre-trained model with a few parameterized layers.","To accommodate the text-video scenario, we equip our RAP with two indispensable characteristics: temporal sparsity and correlation.","Specifically, we propose a low-rank modulation module to refine the per-image features from the frozen CLIP backbone, which accentuates salient frames within the video features while alleviating temporal redundancy.","Besides, we introduce an asynchronous self-attention mechanism that first selects the top responsive visual patches and augments the correlation modeling between them with learnable temporal and patch offsets.","Extensive experiments on four TVR datasets demonstrate that RAP achieves superior or comparable performance compared to the fully fine-tuned counterpart and other parameter-efficient fine-tuning methods."],"url":"http://arxiv.org/abs/2405.19465v1","category":"cs.CV"}
{"created":"2024-05-29 19:21:17","title":"Clustering-Based Validation Splits for Domain Generalisation","abstract":"This paper considers the problem of model selection under domain shift. In this setting, it is proposed that a high maximum mean discrepancy (MMD) between the training and validation sets increases the generalisability of selected models. A data splitting algorithm based on kernel k-means clustering, which maximises this objective, is presented. The algorithm leverages linear programming to control the size, label, and (optionally) group distributions of the splits, and comes with convergence guarantees. The technique consistently outperforms alternative splitting strategies across a range of datasets and training algorithms, for both domain generalisation (DG) and unsupervised domain adaptation (UDA) tasks. Analysis also shows the MMD between the training and validation sets to be strongly rank-correlated ($\\rho=0.63$) with test domain accuracy, further substantiating the validity of this approach.","sentences":["This paper considers the problem of model selection under domain shift.","In this setting, it is proposed that a high maximum mean discrepancy (MMD) between the training and validation sets increases the generalisability of selected models.","A data splitting algorithm based on kernel k-means clustering, which maximises this objective, is presented.","The algorithm leverages linear programming to control the size, label, and (optionally)","group distributions of the splits, and comes with convergence guarantees.","The technique consistently outperforms alternative splitting strategies across a range of datasets and training algorithms, for both domain generalisation (DG) and unsupervised domain adaptation (UDA) tasks.","Analysis also shows the MMD between the training and validation sets to be strongly rank-correlated ($\\rho=0.63$) with test domain accuracy, further substantiating the validity of this approach."],"url":"http://arxiv.org/abs/2405.19461v1","category":"cs.LG"}
{"created":"2024-05-29 18:37:38","title":"Asymptotic expansion for the Fourier coefficients associated with the inverse of the modular discriminant function $\u0394$","abstract":"There have been a plethora of investigations carried out in studying inequalities for the Fourier coefficients of weakly holomorphic modular forms, for example, on the partition function. Recently, Bringmann, Kane, Rolen, and Tripp studied asymptotics for the $k$-colored partition function and more generally, for the fractional partitions arising from the Nekrasov-Okounkov formula which in turn allowed them to prove generalized multiplicative inequalities. Motivated by their idea to find interesting inequalities for the $k$-colored partition functions, denoted by $p_k(n)$, in this paper, we prove a family of inequalities for the $p_{24}(n)$. The main aim of this paper is to study the asymptotic expansion with an effective estimate for the error bound regarding the Fourier coefficients of the modular form $1/\\Delta$ (up to a constant $c$ and a power of $q$), where $\\Delta$ is the modular discriminant function and a well-known combinatorial interpretation for the associated coefficient sequence is called $24$-colored partitions, denoted by $p_{24}(n)$. Consequently, we show that $p_{24}(n)$ satisfies $2$-$\\log$-concavity, Tur\\'{a}n inequality of order $3$, and Laguerre inequalities of order $m$ with $2\\le m\\le 8$ eventually. Our method of estimations for the error term of the asymptotic expansion for $p_{24}(n)$ can be adapted in a more general paradigm where the Fourier coefficients of a certain class of Dedekind-eta quotients which are essentially a modular form of negative weight, admit a Rademacher type exact formula involving the $I$-Bessel function of positive order.","sentences":["There have been a plethora of investigations carried out in studying inequalities for the Fourier coefficients of weakly holomorphic modular forms, for example, on the partition function.","Recently, Bringmann, Kane, Rolen, and Tripp studied asymptotics for the $k$-colored partition function and more generally, for the fractional partitions arising from the Nekrasov-Okounkov formula which in turn allowed them to prove generalized multiplicative inequalities.","Motivated by their idea to find interesting inequalities for the $k$-colored partition functions, denoted by $p_k(n)$, in this paper, we prove a family of inequalities for the $p_{24}(n)$. The main aim of this paper is to study the asymptotic expansion with an effective estimate for the error bound regarding the Fourier coefficients of the modular form $1/\\Delta$ (up to a constant $c$ and a power of $q$), where $\\Delta$ is the modular discriminant function and a well-known combinatorial interpretation for the associated coefficient sequence is called $24$-colored partitions, denoted by $p_{24}(n)$. Consequently, we show that $p_{24}(n)$ satisfies $2$-$\\log$-concavity, Tur\\'{a}n inequality of order $3$, and Laguerre inequalities of order $m$ with $2\\le m\\le 8$ eventually.","Our method of estimations for the error term of the asymptotic expansion for $p_{24}(n)$ can be adapted in a more general paradigm where the Fourier coefficients of a certain class of Dedekind-eta quotients which are essentially a modular form of negative weight, admit a Rademacher type exact formula involving the $I$-Bessel function of positive order."],"url":"http://arxiv.org/abs/2405.19441v1","category":"math.NT"}
{"created":"2024-05-29 18:08:37","title":"Adaptive In-conversation Team Building for Language Model Agents","abstract":"Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs. It allows for a flexible yet structured approach to problem-solving and can help reduce redundancy and enhance output diversity. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering.","sentences":["Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art.","It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively?","Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent.","It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs.","It allows for a flexible yet structured approach to problem-solving and can help reduce redundancy and enhance output diversity.","A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering."],"url":"http://arxiv.org/abs/2405.19425v1","category":"cs.CL"}
{"created":"2024-05-29 18:00:24","title":"An Ingot-like class of WaveFront Sensors for Laser Guide Stars","abstract":"Full sky coverage Adaptive Optics on Extremely Large Telescopes requires the adoption of several Laser Guide Stars as references.With such large apertures, the apparent elongation of the beacons is absolutely significant.With few exceptions,WaveFront Sensors designed for Natural Guide Stars are adapted and used in suboptimal mode in this context. We analyse and describe the geometrical properties of a class of WaveFront Sensors that are specifically designed to deal with Laser Guide Stars propagated from a location in the immediate vicinity of the telescope aperture. We describe in three dimensions the loci where the light of the Laser Guide Stars would focus in the focal volume located behind the focal plane (where astronomical objects are reimaged). We also describe the properties of several types of optomechanical devices that, through refraction and reflections, act as perturbers for this new class of pupil plane sensors, which we call ingot WaveFront Sensor. We give the recipes both for the most reasonable complex version of these WaveFront Sensors, with 6 pupils, and for the simplest one, with only 3 pupils. Both of them are referred to the ELT case. Elements to have a qualitative idea of how the sensitivity of such a new class of sensors compared to conventional ones are outlined. We present a new class of WaveFront Sensors, by carrying out the extension to the case of elongated sources at finite distance of the pyramid WaveFront Sensor and pointing out which advantages of the pyramid are retained and how it can be adopted to optimize the sensing.","sentences":["Full sky coverage Adaptive Optics on Extremely Large Telescopes requires the adoption of several Laser Guide Stars as references.","With such large apertures, the apparent elongation of the beacons is absolutely significant.","With few exceptions,WaveFront Sensors designed for Natural Guide Stars are adapted and used in suboptimal mode in this context.","We analyse and describe the geometrical properties of a class of WaveFront Sensors that are specifically designed to deal with Laser Guide Stars propagated from a location in the immediate vicinity of the telescope aperture.","We describe in three dimensions the loci where the light of the Laser Guide Stars would focus in the focal volume located behind the focal plane (where astronomical objects are reimaged).","We also describe the properties of several types of optomechanical devices that, through refraction and reflections, act as perturbers for this new class of pupil plane sensors, which we call ingot WaveFront Sensor.","We give the recipes both for the most reasonable complex version of these WaveFront Sensors, with 6 pupils, and for the simplest one, with only 3 pupils.","Both of them are referred to the ELT case.","Elements to have a qualitative idea of how the sensitivity of such a new class of sensors compared to conventional ones are outlined.","We present a new class of WaveFront Sensors, by carrying out the extension to the case of elongated sources at finite distance of the pyramid WaveFront Sensor and pointing out which advantages of the pyramid are retained and how it can be adopted to optimize the sensing."],"url":"http://arxiv.org/abs/2405.19415v1","category":"astro-ph.IM"}
{"created":"2024-05-29 09:12:44","title":"NeuralODEs for VLEO simulations: Introducing thermoNET for Thermosphere Modeling","abstract":"We introduce a novel neural architecture termed thermoNET, designed to represent thermospheric density in satellite orbital propagation using a reduced amount of differentiable computations. Due to the appearance of a neural network on the right-hand side of the equations of motion, the resulting satellite dynamics is governed by a NeuralODE, a neural Ordinary Differential Equation, characterized by its fully differentiable nature, allowing the derivation of variational equations (hence of the state transition matrix) and facilitating its use in connection to advanced numerical techniques such as Taylor-based numerical propagation and differential algebraic techniques. Efficient training of the network parameters occurs through two distinct approaches. In the first approach, the network undergoes training independently of spacecraft dynamics, engaging in a pure regression task against ground truth models, including JB-08 and NRLMSISE-00. In the second paradigm, network parameters are learned based on observed dynamics, adapting through ODE sensitivities. In both cases, the outcome is a flexible, compact model of the thermosphere density greatly enhancing numerical propagation efficiency while maintaining accuracy in the orbital predictions.","sentences":["We introduce a novel neural architecture termed thermoNET, designed to represent thermospheric density in satellite orbital propagation using a reduced amount of differentiable computations.","Due to the appearance of a neural network on the right-hand side of the equations of motion, the resulting satellite dynamics is governed by a NeuralODE, a neural Ordinary Differential Equation, characterized by its fully differentiable nature, allowing the derivation of variational equations (hence of the state transition matrix) and facilitating its use in connection to advanced numerical techniques such as Taylor-based numerical propagation and differential algebraic techniques.","Efficient training of the network parameters occurs through two distinct approaches.","In the first approach, the network undergoes training independently of spacecraft dynamics, engaging in a pure regression task against ground truth models, including JB-08 and NRLMSISE-00.","In the second paradigm, network parameters are learned based on observed dynamics, adapting through ODE sensitivities.","In both cases, the outcome is a flexible, compact model of the thermosphere density greatly enhancing numerical propagation efficiency while maintaining accuracy in the orbital predictions."],"url":"http://arxiv.org/abs/2405.19384v1","category":"astro-ph.EP"}
{"created":"2024-05-30 17:59:51","title":"Evaluating Approximations of Count Distributions and Forecasts for Poisson-Lindley Integer Autoregressive Processes","abstract":"Although many time series are realizations from discrete processes, it is often that a continuous Gaussian model is implemented for modeling and forecasting the data, resulting in incoherent forecasts. Forecasts using a Poisson-Lindley integer autoregressive (PLINAR) model are compared to variations of Gaussian forecasts via simulation by equating relevant moments of the marginals of the PLINAR to the Gaussian AR. To illustrate utility, the methods discussed are applied and compared using a discrete series with model parameters being estimated using each of conditional least squares, Yule-Walker, and maximum likelihood.","sentences":["Although many time series are realizations from discrete processes, it is often that a continuous Gaussian model is implemented for modeling and forecasting the data, resulting in incoherent forecasts.","Forecasts using a Poisson-Lindley integer autoregressive (PLINAR) model are compared to variations of Gaussian forecasts via simulation by equating relevant moments of the marginals of the PLINAR to the Gaussian AR.","To illustrate utility, the methods discussed are applied and compared using a discrete series with model parameters being estimated using each of conditional least squares, Yule-Walker, and maximum likelihood."],"url":"http://arxiv.org/abs/2405.20342v1","category":"stat.ME"}
{"created":"2024-05-30 17:43:35","title":"Multi-headed lattices and Green functions","abstract":"Lattice geometries and random walks on them are of great interest for their applications in different fields such as physics, chemistry, and computer science. In this work, we focus on multi-headed lattices and study properties of the Green functions for these lattices such as the associated differential equations and the P\\'olya numbers. In particular, we complete the analysis of three missing cases in dimensions no larger than five. Our results are built upon an automatic machinery of creative telescoping.","sentences":["Lattice geometries and random walks on them are of great interest for their applications in different fields such as physics, chemistry, and computer science.","In this work, we focus on multi-headed lattices and study properties of the Green functions for these lattices such as the associated differential equations and the P\\'olya numbers.","In particular, we complete the analysis of three missing cases in dimensions no larger than five.","Our results are built upon an automatic machinery of creative telescoping."],"url":"http://arxiv.org/abs/2405.20294v1","category":"math.CO"}
{"created":"2024-05-30 16:01:11","title":"Well-posedness of Hibler's parabolic-hyperbolic sea ice model","abstract":"This paper proves the local-in-time strong well-posedness of a parabolic-hyperbolic regularized version of Hibler's sea ice model. The latter model is the most frequently used sea ice model in climate science. Lagrangian coordinates are employed to handle the hyperbolic terms in the balance laws. The resulting problem is regarded as a quasilinear non-autonomous evolution equation. Maximal $\\mathrm{L}^p$-regularity of the underlying linearized problem is obtained on an anisotropic ground space in order to deal with the lack of regularization in the balance laws.","sentences":["This paper proves the local-in-time strong well-posedness of a parabolic-hyperbolic regularized version of Hibler's sea ice model.","The latter model is the most frequently used sea ice model in climate science.","Lagrangian coordinates are employed to handle the hyperbolic terms in the balance laws.","The resulting problem is regarded as a quasilinear non-autonomous evolution equation.","Maximal $\\mathrm{L}^p$-regularity of the underlying linearized problem is obtained on an anisotropic ground space in order to deal with the lack of regularization in the balance laws."],"url":"http://arxiv.org/abs/2405.20198v1","category":"math.AP"}
{"created":"2024-05-30 15:53:09","title":"Aharonov-Bohm flux and dual gaps effects on energy levels in graphene magnetic quantum dots","abstract":"We address the question of how the Aharonov-Bohm flux $\\Phi_{AB}$ can affect the energy levels of graphene magnetic quantum dots (GMQDs) of radius $R$. To answer this question, we consider GMQDs induced by a magnetic field $B$ and subjected to two different gaps - an internal gap $\\Delta_1$ and an external gap $\\Delta_2$. After determining the eigenspinors and ensuring continuity at the boundary of the GMQDs, we formulate an analytical equation describing the corresponding energy levels. Our results show that the energy levels can exhibit either a symmetric or an asymmetric behavior depending on the valleys $K$ and $K'$ together with the quantum angular momentum $m$. In addition, we find that $\\Phi_{AB}$ causes an increase in the band gap width when $\\Delta_1$ is present inside the GMQDs. This effect is less significant when a gap is present outside, resulting in a longer lifetime of the confined electronic states. Further increases in \\(\\Phi_{AB}\\) reduce the number of levels between the conduction and valence bands, thereby increasing the band gap. These results demonstrate that the electronic properties of graphene can be tuned by the presence of the AB flux, offering the potential to control the behavior of graphene-based quantum devices.","sentences":["We address the question of how the Aharonov-Bohm flux $\\Phi_{AB}$ can affect the energy levels of graphene magnetic quantum dots (GMQDs) of radius $R$. To answer this question, we consider GMQDs induced by a magnetic field $B$ and subjected to two different gaps - an internal gap $\\Delta_1$ and an external gap $\\Delta_2$. After determining the eigenspinors and ensuring continuity at the boundary of the GMQDs, we formulate an analytical equation describing the corresponding energy levels.","Our results show that the energy levels can exhibit either a symmetric or an asymmetric behavior depending on the valleys $K$ and $K'$ together with the quantum angular momentum $m$. In addition, we find that $\\Phi_{AB}$ causes an increase in the band gap width when $\\Delta_1$ is present inside the GMQDs.","This effect is less significant when a gap is present outside, resulting in a longer lifetime of the confined electronic states.","Further increases in \\(\\Phi_{AB}\\) reduce the number of levels between the conduction and valence bands, thereby increasing the band gap.","These results demonstrate that the electronic properties of graphene can be tuned by the presence of the AB flux, offering the potential to control the behavior of graphene-based quantum devices."],"url":"http://arxiv.org/abs/2405.20186v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 15:50:02","title":"The Parton Branching evolution package uPDFevolv2","abstract":"uPDFevolv2 is a software package designed for evolving collinear and Transverse Momentum Dependent (TMD) parton densities using the DGLAP evolution equation. A comprehensive description of both the theoretical framework and technical implementation is given, accompanied by a detailed guide on program usage, focusing on customizable parameters.   This report is as a technical release note for uPDFevolv version 2.5.03.","sentences":["uPDFevolv2 is a software package designed for evolving collinear and Transverse Momentum Dependent (TMD) parton densities using the DGLAP evolution equation.","A comprehensive description of both the theoretical framework and technical implementation is given, accompanied by a detailed guide on program usage, focusing on customizable parameters.   ","This report is as a technical release note for uPDFevolv version 2.5.03."],"url":"http://arxiv.org/abs/2405.20185v1","category":"hep-ph"}
{"created":"2024-05-30 15:33:10","title":"Spectral adjoint-based assimilation of sparse data in unsteady simulations of turbulent flows","abstract":"The URANS equations provide a computationally efficient tool to simulate unsteady turbulent flows for a wide range of applications. To account for the errors introduced by the turbulence closure model, recent works have adopted data assimilation (DA) to enhance their predictive capabilities. Recognizing the challenges posed by the computational cost of 4DVar DA for unsteady flows, we propose a 3DVar DA framework that incorporates a time-discrete Fourier transform of the URANS equations, facilitating the use of the stationary discrete adjoint method in Fourier space.   Central to our methodology is the introduction of a corrective, divergence-free, and unsteady forcing term, derived from a Fourier series expansion, into the URANS equations. This term aims at mitigating discrepancies in the modeled divergence of Reynolds stresses, allowing for the tuning of stationary parameters across different Fourier modes.   Our implementation is built upon an extended version of the coupled URANS solver in OpenFOAM, enhanced to compute adjoint variables and gradients. This design choice ensures straightforward applicability to various flow setups and solvers, eliminating the need for specialized harmonic solvers. A gradient-based optimizer is employed to minimize discrepancies between simulated results and sparse velocity reference data.   The effectiveness of our approach is demonstrated through its application to flow around a two-dimensional circular cylinder at a Reynolds number of 3900. Our results highlight the method's ability to reconstruct mean flow accurately and improve the vortex shedding frequency through the assimilation of zeroth mode data. Additionally, the assimilation of first mode data further enhances the simulation's capability to capture low-frequency dynamics of the flow, and finally, it runs efficiently by leveraging a coarse mesh.","sentences":["The URANS equations provide a computationally efficient tool to simulate unsteady turbulent flows for a wide range of applications.","To account for the errors introduced by the turbulence closure model, recent works have adopted data assimilation (DA) to enhance their predictive capabilities.","Recognizing the challenges posed by the computational cost of 4DVar DA for unsteady flows, we propose a 3DVar DA framework that incorporates a time-discrete Fourier transform of the URANS equations, facilitating the use of the stationary discrete adjoint method in Fourier space.   ","Central to our methodology is the introduction of a corrective, divergence-free, and unsteady forcing term, derived from a Fourier series expansion, into the URANS equations.","This term aims at mitigating discrepancies in the modeled divergence of Reynolds stresses, allowing for the tuning of stationary parameters across different Fourier modes.   ","Our implementation is built upon an extended version of the coupled URANS solver in OpenFOAM, enhanced to compute adjoint variables and gradients.","This design choice ensures straightforward applicability to various flow setups and solvers, eliminating the need for specialized harmonic solvers.","A gradient-based optimizer is employed to minimize discrepancies between simulated results and sparse velocity reference data.   ","The effectiveness of our approach is demonstrated through its application to flow around a two-dimensional circular cylinder at a Reynolds number of 3900.","Our results highlight the method's ability to reconstruct mean flow accurately and improve the vortex shedding frequency through the assimilation of zeroth mode data.","Additionally, the assimilation of first mode data further enhances the simulation's capability to capture low-frequency dynamics of the flow, and finally, it runs efficiently by leveraging a coarse mesh."],"url":"http://arxiv.org/abs/2405.20160v1","category":"physics.flu-dyn"}
{"created":"2024-05-30 15:09:01","title":"Bandwidth and focal radius with positive isotropic curvature","abstract":"This paper investigates quantitative metric inequalities for manifolds with positive isotropic curvature (PIC). Our results include upper bounds on the bandwidth and focal radius of hypersurfaces in PIC manifolds, contingent on boundary convexities and Betti numbers. The proof is based on exploiting the spectral properties of a twisted de Rham-Hodge operator on manifolds with boundary.","sentences":["This paper investigates quantitative metric inequalities for manifolds with positive isotropic curvature (PIC).","Our results include upper bounds on the bandwidth and focal radius of hypersurfaces in PIC manifolds, contingent on boundary convexities and Betti numbers.","The proof is based on exploiting the spectral properties of a twisted de Rham-Hodge operator on manifolds with boundary."],"url":"http://arxiv.org/abs/2405.20129v1","category":"math.DG"}
{"created":"2024-05-30 14:54:26","title":"Infinite 3D Landmarks: Improving Continuous 2D Facial Landmark Detection","abstract":"In this paper, we examine 3 important issues in the practical use of state-of-the-art facial landmark detectors and show how a combination of specific architectural modifications can directly improve their accuracy and temporal stability. First, many facial landmark detectors require face normalization as a preprocessing step, which is accomplished by a separately-trained neural network that crops and resizes the face in the input image. There is no guarantee that this pre-trained network performs the optimal face normalization for landmark detection. We instead analyze the use of a spatial transformer network that is trained alongside the landmark detector in an unsupervised manner, and jointly learn optimal face normalization and landmark detection. Second, we show that modifying the output head of the landmark predictor to infer landmarks in a canonical 3D space can further improve accuracy. To convert the predicted 3D landmarks into screen-space, we additionally predict the camera intrinsics and head pose from the input image. As a side benefit, this allows to predict the 3D face shape from a given image only using 2D landmarks as supervision, which is useful in determining landmark visibility among other things. Finally, when training a landmark detector on multiple datasets at the same time, annotation inconsistencies across datasets forces the network to produce a suboptimal average. We propose to add a semantic correction network to address this issue. This additional lightweight neural network is trained alongside the landmark detector, without requiring any additional supervision. While the insights of this paper can be applied to most common landmark detectors, we specifically target a recently-proposed continuous 2D landmark detector to demonstrate how each of our additions leads to meaningful improvements over the state-of-the-art on standard benchmarks.","sentences":["In this paper, we examine 3 important issues in the practical use of state-of-the-art facial landmark detectors and show how a combination of specific architectural modifications can directly improve their accuracy and temporal stability.","First, many facial landmark detectors require face normalization as a preprocessing step, which is accomplished by a separately-trained neural network that crops and resizes the face in the input image.","There is no guarantee that this pre-trained network performs the optimal face normalization for landmark detection.","We instead analyze the use of a spatial transformer network that is trained alongside the landmark detector in an unsupervised manner, and jointly learn optimal face normalization and landmark detection.","Second, we show that modifying the output head of the landmark predictor to infer landmarks in a canonical 3D space can further improve accuracy.","To convert the predicted 3D landmarks into screen-space, we additionally predict the camera intrinsics and head pose from the input image.","As a side benefit, this allows to predict the 3D face shape from a given image only using 2D landmarks as supervision, which is useful in determining landmark visibility among other things.","Finally, when training a landmark detector on multiple datasets at the same time, annotation inconsistencies across datasets forces the network to produce a suboptimal average.","We propose to add a semantic correction network to address this issue.","This additional lightweight neural network is trained alongside the landmark detector, without requiring any additional supervision.","While the insights of this paper can be applied to most common landmark detectors, we specifically target a recently-proposed continuous 2D landmark detector to demonstrate how each of our additions leads to meaningful improvements over the state-of-the-art on standard benchmarks."],"url":"http://arxiv.org/abs/2405.20117v1","category":"cs.CV"}
{"created":"2024-05-30 14:41:39","title":"Fill in the Gap! Combining Self-supervised Representation Learning with Neural Audio Synthesis for Speech Inpainting","abstract":"Most speech self-supervised learning (SSL) models are trained with a pretext task which consists in predicting missing parts of the input signal, either future segments (causal prediction) or segments masked anywhere within the input (non-causal prediction). Learned speech representations can then be efficiently transferred to downstream tasks (e.g., automatic speech or speaker recognition). In the present study, we investigate the use of a speech SSL model for speech inpainting, that is reconstructing a missing portion of a speech signal from its surrounding context, i.e., fulfilling a downstream task that is very similar to the pretext task. To that purpose, we combine an SSL encoder, namely HuBERT, with a neural vocoder, namely HiFiGAN, playing the role of a decoder. In particular, we propose two solutions to match the HuBERT output with the HiFiGAN input, by freezing one and fine-tuning the other, and vice versa. Performance of both approaches was assessed in single- and multi-speaker settings, for both informed and blind inpainting configurations (i.e., the position of the mask is known or unknown, respectively), with different objective metrics and a perceptual evaluation. Performances show that if both solutions allow to correctly reconstruct signal portions up to the size of 200ms (and even 400ms in some cases), fine-tuning the SSL encoder provides a more accurate signal reconstruction in the single-speaker setting case, while freezing it (and training the neural vocoder instead) is a better strategy when dealing with multi-speaker data.","sentences":["Most speech self-supervised learning (SSL) models are trained with a pretext task which consists in predicting missing parts of the input signal, either future segments (causal prediction) or segments masked anywhere within the input (non-causal prediction).","Learned speech representations can then be efficiently transferred to downstream tasks (e.g., automatic speech or speaker recognition).","In the present study, we investigate the use of a speech SSL model for speech inpainting, that is reconstructing a missing portion of a speech signal from its surrounding context, i.e., fulfilling a downstream task that is very similar to the pretext task.","To that purpose, we combine an SSL encoder, namely HuBERT, with a neural vocoder, namely HiFiGAN, playing the role of a decoder.","In particular, we propose two solutions to match the HuBERT output with the HiFiGAN input, by freezing one and fine-tuning the other, and vice versa.","Performance of both approaches was assessed in single- and multi-speaker settings, for both informed and blind inpainting configurations (i.e., the position of the mask is known or unknown, respectively), with different objective metrics and a perceptual evaluation.","Performances show that if both solutions allow to correctly reconstruct signal portions up to the size of 200ms (and even 400ms in some cases), fine-tuning the SSL encoder provides a more accurate signal reconstruction in the single-speaker setting case, while freezing it (and training the neural vocoder instead) is a better strategy when dealing with multi-speaker data."],"url":"http://arxiv.org/abs/2405.20101v1","category":"cs.SD"}
{"created":"2024-05-30 14:32:06","title":"Low-dimensional approximations of the conditional law of Volterra processes: a non-positive curvature approach","abstract":"Predicting the conditional evolution of Volterra processes with stochastic volatility is a crucial challenge in mathematical finance. While deep neural network models offer promise in approximating the conditional law of such processes, their effectiveness is hindered by the curse of dimensionality caused by the infinite dimensionality and non-smooth nature of these problems. To address this, we propose a two-step solution. Firstly, we develop a stable dimension reduction technique, projecting the law of a reasonably broad class of Volterra process onto a low-dimensional statistical manifold of non-positive sectional curvature. Next, we introduce a sequentially deep learning model tailored to the manifold's geometry, which we show can approximate the projected conditional law of the Volterra process. Our model leverages an auxiliary hypernetwork to dynamically update its internal parameters, allowing it to encode non-stationary dynamics of the Volterra process, and it can be interpreted as a gating mechanism in a mixture of expert models where each expert is specialized at a specific point in time. Our hypernetwork further allows us to achieve approximation rates that would seemingly only be possible with very large networks.","sentences":["Predicting the conditional evolution of Volterra processes with stochastic volatility is a crucial challenge in mathematical finance.","While deep neural network models offer promise in approximating the conditional law of such processes, their effectiveness is hindered by the curse of dimensionality caused by the infinite dimensionality and non-smooth nature of these problems.","To address this, we propose a two-step solution.","Firstly, we develop a stable dimension reduction technique, projecting the law of a reasonably broad class of Volterra process onto a low-dimensional statistical manifold of non-positive sectional curvature.","Next, we introduce a sequentially deep learning model tailored to the manifold's geometry, which we show can approximate the projected conditional law of the Volterra process.","Our model leverages an auxiliary hypernetwork to dynamically update its internal parameters, allowing it to encode non-stationary dynamics of the Volterra process, and it can be interpreted as a gating mechanism in a mixture of expert models where each expert is specialized at a specific point in time.","Our hypernetwork further allows us to achieve approximation rates that would seemingly only be possible with very large networks."],"url":"http://arxiv.org/abs/2405.20094v1","category":"math.NA"}
{"created":"2024-05-30 13:23:02","title":"CycleFormer : TSP Solver Based on Language Modeling","abstract":"We propose a new transformer model for the Traveling Salesman Problem (TSP) called CycleFormer. We identified distinctive characteristics that need to be considered when applying a conventional transformer model to TSP and aimed to fully incorporate these elements into the TSP-specific transformer. Unlike the token sets in typical language models, which are limited and static, the token (node) set in TSP is unlimited and dynamic. To exploit this fact to the fullest, we equated the encoder output with the decoder linear layer and directly connected the context vector of the encoder to the decoder encoding. Additionally, we added a positional encoding to the encoder tokens that reflects the two-dimensional nature of TSP, and devised a circular positional encoding for the decoder tokens that considers the cyclic properties of a tour. By incorporating these ideas, CycleFormer outperforms state-of-the-art (SOTA) transformer models for TSP from TSP-50 to TSP-500. Notably, on TSP-500, the optimality gap was reduced by approximately 2.8 times, from 3.09% to 1.10%, compared to the existing SOTA. The code will be made available at https://github.com/Giventicket/CycleFormer.","sentences":["We propose a new transformer model for the Traveling Salesman Problem (TSP) called CycleFormer.","We identified distinctive characteristics that need to be considered when applying a conventional transformer model to TSP and aimed to fully incorporate these elements into the TSP-specific transformer.","Unlike the token sets in typical language models, which are limited and static, the token (node) set in TSP is unlimited and dynamic.","To exploit this fact to the fullest, we equated the encoder output with the decoder linear layer and directly connected the context vector of the encoder to the decoder encoding.","Additionally, we added a positional encoding to the encoder tokens that reflects the two-dimensional nature of TSP, and devised a circular positional encoding for the decoder tokens that considers the cyclic properties of a tour.","By incorporating these ideas, CycleFormer outperforms state-of-the-art (SOTA) transformer models for TSP from TSP-50 to TSP-500.","Notably, on TSP-500, the optimality gap was reduced by approximately 2.8 times, from 3.09% to 1.10%, compared to the existing SOTA.","The code will be made available at https://github.com/Giventicket/CycleFormer."],"url":"http://arxiv.org/abs/2405.20042v1","category":"cs.LG"}
{"created":"2024-05-30 13:13:48","title":"A Random Forest-based Prediction Model for Turning Points in Antagonistic event-group Competitions","abstract":"At present, most of the prediction studies related to antagonistic event-group competitions focus on the prediction of competition results, and less on the prediction of the competition process, which can not provide real-time feedback of the athletes' state information in the actual competition, and thus can not analyze the changes of the competition situation. In order to solve this problem, this paper proposes a prediction model based on Random Forest for the turning point of the antagonistic event-group. Firstly, the quantitative equation of competitive potential energy is proposed; Secondly, the quantitative value of competitive potential energy is obtained by using the dynamic combination of weights method, and the turning point of the competition situation of the antagonistic event-group is marked according to the quantitative time series graph; Finally, the random forest prediction model based on the optimisation of the KM-SMOTE algorithm and the grid search method is established. The experimental analysis shows that: the quantitative equation of competitive potential energy can effectively reflect the dynamic situation of the competition; The model can effectively predict the turning point of the competition situation of the antagonistic event-group, and the recall rate of the model in the test set is 86.13%; the model has certain significance for the future study of the competition situation of the antagonistic event-group.","sentences":["At present, most of the prediction studies related to antagonistic event-group competitions focus on the prediction of competition results, and less on the prediction of the competition process, which can not provide real-time feedback of the athletes' state information in the actual competition, and thus can not analyze the changes of the competition situation.","In order to solve this problem, this paper proposes a prediction model based on Random Forest for the turning point of the antagonistic event-group.","Firstly, the quantitative equation of competitive potential energy is proposed; Secondly, the quantitative value of competitive potential energy is obtained by using the dynamic combination of weights method, and the turning point of the competition situation of the antagonistic event-group is marked according to the quantitative time series graph; Finally, the random forest prediction model based on the optimisation of the KM-SMOTE algorithm and the grid search method is established.","The experimental analysis shows that: the quantitative equation of competitive potential energy can effectively reflect the dynamic situation of the competition; The model can effectively predict the turning point of the competition situation of the antagonistic event-group, and the recall rate of the model in the test set is 86.13%; the model has certain significance for the future study of the competition situation of the antagonistic event-group."],"url":"http://arxiv.org/abs/2405.20029v1","category":"cs.LG"}
{"created":"2024-05-30 12:59:12","title":"About truncated Chebyshev spectral method for solving numerical differentiation and summation","abstract":"The problems of optimal recovering univariate functions and their derivatives are studied. To solve these problems, two variants of the truncation method are constructed, which are order-optimal both in the sense of accuracy and in terms of the amount of involved Galerkin information. For numerical summation, it has been established how the parameters characterizing the problem being solved affect its stability.","sentences":["The problems of optimal recovering univariate functions and their derivatives are studied.","To solve these problems, two variants of the truncation method are constructed, which are order-optimal both in the sense of accuracy and in terms of the amount of involved Galerkin information.","For numerical summation, it has been established how the parameters characterizing the problem being solved affect its stability."],"url":"http://arxiv.org/abs/2405.20020v1","category":"math.NA"}
{"created":"2024-05-30 12:27:31","title":"High-order parallel-in-time method for the monodomain equation in cardiac electrophysiology","abstract":"Simulation of the monodomain equation, crucial for modeling the heart's electrical activity, faces scalability limits when traditional numerical methods only parallelize in space. To optimize the use of large multi-processor computers by distributing the computational load more effectively, time parallelization is essential. We introduce a high-order parallel-in-time method addressing the substantial computational challenges posed by the stiff, multiscale, and nonlinear nature of cardiac dynamics. Our method combines the semi-implicit and exponential spectral deferred correction methods, yielding a hybrid method that is extended to parallel-in-time employing the PFASST framework. We thoroughly evaluate the stability, accuracy, and robustness of the proposed parallel-in-time method through extensive numerical experiments, using practical ionic models such as the ten-Tusscher-Panfilov. The results underscore the method's potential to significantly enhance real-time and high-fidelity simulations in biomedical research and clinical applications.","sentences":["Simulation of the monodomain equation, crucial for modeling the heart's electrical activity, faces scalability limits when traditional numerical methods only parallelize in space.","To optimize the use of large multi-processor computers by distributing the computational load more effectively, time parallelization is essential.","We introduce a high-order parallel-in-time method addressing the substantial computational challenges posed by the stiff, multiscale, and nonlinear nature of cardiac dynamics.","Our method combines the semi-implicit and exponential spectral deferred correction methods, yielding a hybrid method that is extended to parallel-in-time employing the PFASST framework.","We thoroughly evaluate the stability, accuracy, and robustness of the proposed parallel-in-time method through extensive numerical experiments, using practical ionic models such as the ten-Tusscher-Panfilov.","The results underscore the method's potential to significantly enhance real-time and high-fidelity simulations in biomedical research and clinical applications."],"url":"http://arxiv.org/abs/2405.19994v1","category":"math.NA"}
{"created":"2024-05-30 12:06:00","title":"Color dipole cross-section from unifying the color dipole picture and improved saturation models","abstract":"We present an analysis of the color-dipole picture (CDP) for determination of the gluon density at low-$x$ which is obtained from the Altarelli-Martinelli equation by expansion at distinct points of expansion. The dipole cross-sections with respect to the improved saturation model of Bartels-Golec-Biernat-Kowalski (BGK) are obtained in a wide range of transverse sizes $r$ and compared with the Golec-Biernat-W$\\ddot{\\mathrm{u}}$sthoff (GBW) model. We find that the model gives a good description of the dipole cross-section at large $r$ which confirms saturation and matches the perturbative QCD result at a small $r$ due to the significant role of the running of the gluon distribution. The transition between these regions occurs with decreasing transverse sizes with a decrease of Bjorken $x$ and dependence on the expansion point.","sentences":["We present an analysis of the color-dipole picture (CDP) for determination of the gluon density at low-$x$ which is obtained from the Altarelli-Martinelli equation by expansion at distinct points of expansion.","The dipole cross-sections with respect to the improved saturation model of Bartels-Golec-Biernat-Kowalski (BGK) are obtained in a wide range of transverse sizes $r$ and compared with the Golec-Biernat-W$\\ddot{\\mathrm{u}}$sthoff (GBW) model.","We find that the model gives a good description of the dipole cross-section at large $r$ which confirms saturation and matches the perturbative QCD result at a small $r$ due to the significant role of the running of the gluon distribution.","The transition between these regions occurs with decreasing transverse sizes with a decrease of Bjorken $x$ and dependence on the expansion point."],"url":"http://arxiv.org/abs/2405.19981v1","category":"hep-ph"}
{"created":"2024-05-30 10:44:45","title":"BAN: Detecting Backdoors Activated by Adversarial Neuron Noise","abstract":"Backdoor attacks on deep learning represent a recent threat that has gained significant attention in the research community. Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios. State-of-the-art backdoor inversion recovers a mask in the feature space to locate prominent backdoor features, where benign and backdoor features can be disentangled. However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features. To tackle these shortcomings, this paper improves backdoor feature inversion for backdoor detection by incorporating extra neuron activation information. In particular, we adversarially increase the loss of backdoored models with respect to weights to activate the backdoor effect, based on which we can easily differentiate backdoored and clean models. Experimental results demonstrate our defense, BAN, is 1.37$\\times$ (on CIFAR-10) and 5.11$\\times$ (on ImageNet200) more efficient with 9.99% higher detect success rate than the state-of-the-art defense BTI-DBF. Our code and trained models are publicly available.\\url{https://anonymous.4open.science/r/ban-4B32}","sentences":["Backdoor attacks on deep learning represent a recent threat that has gained significant attention in the research community.","Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios.","State-of-the-art backdoor inversion recovers a mask in the feature space to locate prominent backdoor features, where benign and backdoor features can be disentangled.","However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features.","To tackle these shortcomings, this paper improves backdoor feature inversion for backdoor detection by incorporating extra neuron activation information.","In particular, we adversarially increase the loss of backdoored models with respect to weights to activate the backdoor effect, based on which we can easily differentiate backdoored and clean models.","Experimental results demonstrate our defense, BAN, is 1.37$\\times$ (on CIFAR-10) and 5.11$\\times$ (on ImageNet200) more efficient with 9.99% higher detect success rate than the state-of-the-art defense BTI-DBF.","Our code and trained models are publicly available.\\url{https://anonymous.4open.science/r/ban-4B32}"],"url":"http://arxiv.org/abs/2405.19928v1","category":"cs.LG"}
{"created":"2024-05-30 10:06:06","title":"Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection","abstract":"Label noise, commonly found in real-world datasets, has a detrimental impact on a model's generalization. To effectively detect incorrectly labeled instances, previous works have mostly relied on distinguishable training signals, such as training loss, as indicators to differentiate between clean and noisy labels. However, they have limitations in that the training signals incompletely reveal the model's behavior and are not effectively generalized to various noise types, resulting in limited detection accuracy. In this paper, we propose DynaCor framework that distinguishes incorrectly labeled instances from correctly labeled ones based on the dynamics of the training signals. To cope with the absence of supervision for clean and noisy labels, DynaCor first introduces a label corruption strategy that augments the original dataset with intentionally corrupted labels, enabling indirect simulation of the model's behavior on noisy labels. Then, DynaCor learns to identify clean and noisy instances by inducing two clearly distinguishable clusters from the latent representations of training dynamics. Our comprehensive experiments show that DynaCor outperforms the state-of-the-art competitors and shows strong robustness to various noise types and noise rates.","sentences":["Label noise, commonly found in real-world datasets, has a detrimental impact on a model's generalization.","To effectively detect incorrectly labeled instances, previous works have mostly relied on distinguishable training signals, such as training loss, as indicators to differentiate between clean and noisy labels.","However, they have limitations in that the training signals incompletely reveal the model's behavior and are not effectively generalized to various noise types, resulting in limited detection accuracy.","In this paper, we propose DynaCor framework that distinguishes incorrectly labeled instances from correctly labeled ones based on the dynamics of the training signals.","To cope with the absence of supervision for clean and noisy labels, DynaCor first introduces a label corruption strategy that augments the original dataset with intentionally corrupted labels, enabling indirect simulation of the model's behavior on noisy labels.","Then, DynaCor learns to identify clean and noisy instances by inducing two clearly distinguishable clusters from the latent representations of training dynamics.","Our comprehensive experiments show that DynaCor outperforms the state-of-the-art competitors and shows strong robustness to various noise types and noise rates."],"url":"http://arxiv.org/abs/2405.19902v1","category":"cs.LG"}
{"created":"2024-05-30 09:53:10","title":"Fast Numerical Approximation of Linear, Second-Order Hyperbolic Problems Using Model Order Reduction and the Laplace Transform","abstract":"We extend our previous work [F. Henr\\'iquez and J. S. Hesthaven, arXiv:2403.02847 (2024)] to the linear, second-order wave equation in bounded domains. This technique, referred to as the Laplace Transform Reduced Basis (LT-RB) method, uses two widely known mathematical tools to construct a fast and efficient method for the solution of linear, time-dependent problems: The Laplace transform and the Reduced Basis method, hence the name.   The application of the Laplace transform yields a time-independent problem parametrically depending on the Laplace variable. Following the two-phase paradigm of the RB method, firstly in an offline stage we sample the Laplace parameter, compute the full-order or high-fidelity solution, and then resort to a Proper Orthogonal Decomposition (POD) to extract a basis of reduced dimension. Then, in an online phase, we project the time-dependent problem onto this basis and proceed to solve the evolution problem using any suitable time-stepping method. We prove exponential convergence of the reduced solution computed by the LT-RB method toward the high-fidelity one as the dimension of the reduced space increases.   Finally, we present a set of numerical experiments portraying the performance of the method in terms of accuracy and, in particular, speed-up when compared to the full-order model.","sentences":["We extend our previous work [F. Henr\\'iquez and J. S. Hesthaven, arXiv:2403.02847 (2024)] to the linear, second-order wave equation in bounded domains.","This technique, referred to as the Laplace Transform Reduced Basis (LT-RB) method, uses two widely known mathematical tools to construct a fast and efficient method for the solution of linear, time-dependent problems: The Laplace transform and the Reduced Basis method, hence the name.   ","The application of the Laplace transform yields a time-independent problem parametrically depending on the Laplace variable.","Following the two-phase paradigm of the RB method, firstly in an offline stage we sample the Laplace parameter, compute the full-order or high-fidelity solution, and then resort to a Proper Orthogonal Decomposition (POD) to extract a basis of reduced dimension.","Then, in an online phase, we project the time-dependent problem onto this basis and proceed to solve the evolution problem using any suitable time-stepping method.","We prove exponential convergence of the reduced solution computed by the LT-RB method toward the high-fidelity one as the dimension of the reduced space increases.   ","Finally, we present a set of numerical experiments portraying the performance of the method in terms of accuracy and, in particular, speed-up when compared to the full-order model."],"url":"http://arxiv.org/abs/2405.19896v1","category":"math.NA"}
{"created":"2024-05-30 09:46:30","title":"Modulational Instability of the Coupled Nonlinear volatility and option price model","abstract":"We study the Coupled Nonlinear volatility and option price model via both Modulational instability analysis and direct simulations. Since the coupling term for both the volatility and the option price equation is the same, the MI results are dependent on it, and the stability of the volatility exists for the same condition as that of the price. The numerical simulations are done to comfirm the conditions of MI","sentences":["We study the Coupled Nonlinear volatility and option price model via both Modulational instability analysis and direct simulations.","Since the coupling term for both the volatility and the option price equation is the same, the MI results are dependent on it, and the stability of the volatility exists for the same condition as that of the price.","The numerical simulations are done to comfirm the conditions of MI"],"url":"http://arxiv.org/abs/2405.19887v1","category":"nlin.PS"}
{"created":"2024-05-30 09:30:28","title":"IReNe: Instant Recoloring in Neural Radiance Fields","abstract":"Advances in NERFs have allowed for 3D scene reconstructions and novel view synthesis. Yet, efficiently editing these representations while retaining photorealism is an emerging challenge. Recent methods face three primary limitations: they're slow for interactive use, lack precision at object boundaries, and struggle to ensure multi-view consistency. We introduce IReNe to address these limitations, enabling swift, near real-time color editing in NeRF. Leveraging a pre-trained NeRF model and a single training image with user-applied color edits, IReNe swiftly adjusts network parameters in seconds. This adjustment allows the model to generate new scene views, accurately representing the color changes from the training image while also controlling object boundaries and view-specific effects. Object boundary control is achieved by integrating a trainable segmentation module into the model. The process gains efficiency by retraining only the weights of the last network layer. We observed that neurons in this layer can be classified into those responsible for view-dependent appearance and those contributing to diffuse appearance. We introduce an automated classification approach to identify these neuron types and exclusively fine-tune the weights of the diffuse neurons. This further accelerates training and ensures consistent color edits across different views. A thorough validation on a new dataset, with edited object colors, shows significant quantitative and qualitative advancements over competitors, accelerating speeds by 5x to 500x.","sentences":["Advances in NERFs have allowed for 3D scene reconstructions and novel view synthesis.","Yet, efficiently editing these representations while retaining photorealism is an emerging challenge.","Recent methods face three primary limitations: they're slow for interactive use, lack precision at object boundaries, and struggle to ensure multi-view consistency.","We introduce IReNe to address these limitations, enabling swift, near real-time color editing in NeRF.","Leveraging a pre-trained NeRF model and a single training image with user-applied color edits, IReNe swiftly adjusts network parameters in seconds.","This adjustment allows the model to generate new scene views, accurately representing the color changes from the training image while also controlling object boundaries and view-specific effects.","Object boundary control is achieved by integrating a trainable segmentation module into the model.","The process gains efficiency by retraining only the weights of the last network layer.","We observed that neurons in this layer can be classified into those responsible for view-dependent appearance and those contributing to diffuse appearance.","We introduce an automated classification approach to identify these neuron types and exclusively fine-tune the weights of the diffuse neurons.","This further accelerates training and ensures consistent color edits across different views.","A thorough validation on a new dataset, with edited object colors, shows significant quantitative and qualitative advancements over competitors, accelerating speeds by 5x to 500x."],"url":"http://arxiv.org/abs/2405.19876v1","category":"cs.CV"}
{"created":"2024-05-30 08:47:59","title":"Measurement of ${}_\u039b^{3}\\mathrm{H}$ production in Pb-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}$ = 5.02 TeV","abstract":"The first measurement of $_{\\Lambda}^{3}\\mathrm{H}$ and $^3_ {\\overline{\\Lambda}}\\overline{\\mathrm{H}}$ differential production with respect to transverse momentum and centrality in Pb$-$Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}=5.02$~TeV is presented. The $_{\\Lambda}^{3}\\mathrm{H}$ has been reconstructed via its two-charged-body decay channel, i.e., $_{\\Lambda}^{3}\\mathrm{H} \\rightarrow {}^{3}\\mathrm{He} + \\pi^{-}$. A Blast-Wave model fit of the $p_{\\rm T}$-differential spectra of all nuclear species measured by the ALICE collaboration suggests that the $_{\\Lambda}^{3}\\mathrm{H}$ kinetic freeze-out surface is consistent with that of other nuclei. The ratio between the integrated yields of $_{\\Lambda}^{3}\\mathrm{H}$ and $^3\\mathrm{He}$ is compared to predictions from the statistical hadronisation model and the coalescence model, with the latter being favoured by the presented measurements.","sentences":["The first measurement of $_{\\Lambda}^{3}\\mathrm{H}$ and $^3_ {\\overline{\\Lambda}}\\overline{\\mathrm{H}}$ differential production with respect to transverse momentum and centrality in Pb$-$Pb collisions at $\\sqrt{s_{\\mathrm{NN}}}=5.02$~TeV is presented.","The $_{\\Lambda}^{3}\\mathrm{H}$ has been reconstructed via its two-charged-body decay channel, i.e., $_{\\Lambda}^{3}\\mathrm{H} \\rightarrow {}^{3}\\mathrm{He} + \\pi^{-}$. A Blast-Wave model fit of the $p_{\\rm T}$-differential spectra of all nuclear species measured by the ALICE collaboration suggests that the $_{\\Lambda}^{3}\\mathrm{H}$ kinetic freeze-out surface is consistent with that of other nuclei.","The ratio between the integrated yields of $_{\\Lambda}^{3}\\mathrm{H}$ and $^3\\mathrm{He}$ is compared to predictions from the statistical hadronisation model and the coalescence model, with the latter being favoured by the presented measurements."],"url":"http://arxiv.org/abs/2405.19839v1","category":"nucl-ex"}
{"created":"2024-05-30 08:41:33","title":"Just Rewrite It Again: A Post-Processing Method for Enhanced Semantic Similarity and Privacy Preservation of Differentially Private Rewritten Text","abstract":"The study of Differential Privacy (DP) in Natural Language Processing often views the task of text privatization as a $\\textit{rewriting}$ task, in which sensitive input texts are rewritten to hide explicit or implicit private information. In order to evaluate the privacy-preserving capabilities of a DP text rewriting mechanism, $\\textit{empirical privacy}$ tests are frequently employed. In these tests, an adversary is modeled, who aims to infer sensitive information (e.g., gender) about the author behind a (privatized) text. Looking to improve the empirical protections provided by DP rewriting methods, we propose a simple post-processing method based on the goal of aligning rewritten texts with their original counterparts, where DP rewritten texts are rewritten $\\textit{again}$. Our results shown that such an approach not only produces outputs that are more semantically reminiscent of the original inputs, but also texts which score on average better in empirical privacy evaluations. Therefore, our approach raises the bar for DP rewriting methods in their empirical privacy evaluations, providing an extra layer of protection against malicious adversaries.","sentences":["The study of Differential Privacy (DP) in Natural Language Processing often views the task of text privatization as a $\\textit{rewriting}$ task, in which sensitive input texts are rewritten to hide explicit or implicit private information.","In order to evaluate the privacy-preserving capabilities of a DP text rewriting mechanism, $\\textit{empirical privacy}$ tests are frequently employed.","In these tests, an adversary is modeled, who aims to infer sensitive information (e.g., gender) about the author behind a (privatized) text.","Looking to improve the empirical protections provided by DP rewriting methods, we propose a simple post-processing method based on the goal of aligning rewritten texts with their original counterparts, where DP rewritten texts are rewritten $\\textit{again}$. Our results shown that such an approach not only produces outputs that are more semantically reminiscent of the original inputs, but also texts which score on average better in empirical privacy evaluations.","Therefore, our approach raises the bar for DP rewriting methods in their empirical privacy evaluations, providing an extra layer of protection against malicious adversaries."],"url":"http://arxiv.org/abs/2405.19831v1","category":"cs.CL"}
{"created":"2024-05-30 08:22:35","title":"Equivalence between non-commutative harmonic oscillators and two-photon quantum Rabi models","abstract":"We prove that the non-commutative harmonic oscillator on $L^2(\\mathbb{R})\\otimes\\mathbb{C}^2$ introduced by Parmeggiani and Wakayama is equivalent to the two-photon quantum Rabi model, and they are also equivalent to a holomorphic differential equation on the unit disk. The confluence process of this differential equation and the relation with the one-photon quantum Rabi model are also discussed.","sentences":["We prove that the non-commutative harmonic oscillator on $L^2(\\mathbb{R})\\otimes\\mathbb{C}^2$ introduced by Parmeggiani and Wakayama is equivalent to the two-photon quantum Rabi model, and they are also equivalent to a holomorphic differential equation on the unit disk.","The confluence process of this differential equation and the relation with the one-photon quantum Rabi model are also discussed."],"url":"http://arxiv.org/abs/2405.19814v1","category":"math-ph"}
{"created":"2024-05-30 07:58:37","title":"Muckenhoupt Weights Meet Brezis--Seeger--Van Schaftingen--Yung Formulae in Ball Banach Function Spaces","abstract":"In this article, via first establishing a weighted variant of the profound and far-reaching inequality obtained by A. Cohen, W. Dahmen, I. Daubechies, and R. DeVore in 2003, the authors give two new characterizations of Muckenhoupt weights. As an application, the authors further establish a representation formula of gradients with sharp parameters in ball Banach function spaces, which extends the famous formula obtained by H. Brezis, A. Seeger, J. Van Schaftingen, and P.-L. Yung in 2021 from classical Sobolev spaces to various different Sobolev-type spaces and gives an affirmative answer to the question in page 29 of [Calc. Var. Partial Differential Equations 62 (2023), Paper No. 234]. The most novelty of this article exists in subtly revealing the mutual equivalences among the Muckenhoupt weight, the weighted variant of the inequality of Cohen et al., and the weighted upper estimate of the formula of Brezis et al.","sentences":["In this article, via first establishing a weighted variant of the profound and far-reaching inequality obtained by A. Cohen, W. Dahmen, I. Daubechies, and R. DeVore in 2003, the authors give two new characterizations of Muckenhoupt weights.","As an application, the authors further establish a representation formula of gradients with sharp parameters in ball Banach function spaces, which extends the famous formula obtained by H. Brezis, A. Seeger, J. Van Schaftingen, and P.-L. Yung in 2021 from classical Sobolev spaces to various different Sobolev-type spaces and gives an affirmative answer to the question in page 29 of [Calc.","Var.","Partial Differential Equations 62 (2023), Paper No. 234].","The most novelty of this article exists in subtly revealing the mutual equivalences among the Muckenhoupt weight, the weighted variant of the inequality of Cohen et al., and the weighted upper estimate of the formula of Brezis et al."],"url":"http://arxiv.org/abs/2405.19790v1","category":"math.CA"}
{"created":"2024-05-30 07:36:47","title":"New Exponential operators connected with a^2+x^2: a generalization of Post-Widder and Ismail May","abstract":"The present study offers a general exponential operator connected with a^2+x^2; for positive real \"a\". We estimate the asymptotic formula for simultaneous and ordinary approximation of the constructed operator. In the last section, we graphically interpret the created operator's convergence to two periodic functions \"x sin(x)\" and \"-x/2*cos(pi*x)\". We also consider the limiting case a tends to 0; which provides Post-Widder operator. In addition, we analyze each particular case of the defined operator and determine the optimal value of \"a\", that would yield the greatest approximation; this facilitates us to contrast the well-known operators existing in the literature, especially the Post-Widder operator and the operator due to Ismail-May.","sentences":["The present study offers a general exponential operator connected with a^2+x^2; for positive real \"a\".","We estimate the asymptotic formula for simultaneous and ordinary approximation of the constructed operator.","In the last section, we graphically interpret the created operator's convergence to two periodic functions \"x sin(x)\" and \"-x/2*cos(pi*x)\".","We also consider the limiting case a tends to 0; which provides Post-Widder operator.","In addition, we analyze each particular case of the defined operator and determine the optimal value of \"a\", that would yield the greatest approximation; this facilitates us to contrast the well-known operators existing in the literature, especially the Post-Widder operator and the operator due to Ismail-May."],"url":"http://arxiv.org/abs/2405.19772v1","category":"math.FA"}
{"created":"2024-05-30 07:09:42","title":"Periodic localized traveling waves in the two-dimensional suspension bridge equation","abstract":"In the dynamics generated by the suspension bridge equation, traveling waves are an essential feature. The existing literature focuses primarily on the idealized one-dimensional case, while traveling structures in two spatial dimensions have only been studied via numerical simulations. We use computer-assisted proof methods based on a Newton-Kantorovich type argument to find and prove periodic localized traveling waves in two dimensions. The main obstacle is the exponential nonlinearity in combination with the resulting large amplitude of the localized waves. Our analysis hinges on establishing computable bounds to control the aliasing error in the computed Fourier coefficients. This leads to existence proofs of different traveling wave solutions, accompanied by small, explicit, rigorous bounds on the deficiency of numerical approximations. This approach is directly extendable to other wave equation models and elliptic partial differential equations with analytic nonlinearities, in two as well as in higher dimensions.","sentences":["In the dynamics generated by the suspension bridge equation, traveling waves are an essential feature.","The existing literature focuses primarily on the idealized one-dimensional case, while traveling structures in two spatial dimensions have only been studied via numerical simulations.","We use computer-assisted proof methods based on a Newton-Kantorovich type argument to find and prove periodic localized traveling waves in two dimensions.","The main obstacle is the exponential nonlinearity in combination with the resulting large amplitude of the localized waves.","Our analysis hinges on establishing computable bounds to control the aliasing error in the computed Fourier coefficients.","This leads to existence proofs of different traveling wave solutions, accompanied by small, explicit, rigorous bounds on the deficiency of numerical approximations.","This approach is directly extendable to other wave equation models and elliptic partial differential equations with analytic nonlinearities, in two as well as in higher dimensions."],"url":"http://arxiv.org/abs/2405.19759v1","category":"math.AP"}
{"created":"2024-05-30 06:52:44","title":"Sloshing and spiral structures breeding a putative radio mini-halo in the environment of a cool-core cluster Abell 795","abstract":"Spiral structures and cold fronts in X-rays are frequently observed in cool core galaxy clusters. However, studies on radio mini-haloes associated with such spirals and their physical connections are rare. Here, we present the detection of an extended diffuse radio emission entrained in the X-ray spiral structure in a known cool core cluster Abell 795 (A795). Though the cool core is a sign of the relaxed nature of the clusters, our re-analysed 30 ks Chandra X-ray data of cluster A795 confirms the presence of an interesting log spiral structure of X-ray deficit region complemented by an X-ray excess counter spiral in the residual map, exposing its dynamical activity. Our new analysis of 150 $\\&$ 325 MHz GMRT archival data of the cluster confirms the detection of a $\\sim180$ kpc ultra-steep ($\\alpha\\sim-2.7$) diffuse radio structure which was previously reported as a candidate radio mini halo from low sensitive survey maps. This radio emission spans the entire spiral structure ($\\sim186$ kpc), enclosed by two previously reported cold fronts. Furthermore, SDSS DR13 optical spectra, as well as GALEX's FUV data, show a considerably low total star formation rate of 2.52 M$_{\\odot}$ yr$^{-1}$ and having no significant variation in metallicity distribution. We argued that the two-phase (hot and cold) plasma at the cluster core with differential velocity has probably caused the spiral formation and has redistributed the secondary electrons from the central BCG or the pre-accelerated electrons which have been (re-)accelerated by the sloshing turbulence to form the observed candidate radio mini-halo structure. This has been supported by a few previous studies that indicate spiral formation and sloshing turbulence may quench star formation and facilitate smooth metallicity distribution by mixing the gas in the core.","sentences":["Spiral structures and cold fronts in X-rays are frequently observed in cool core galaxy clusters.","However, studies on radio mini-haloes associated with such spirals and their physical connections are rare.","Here, we present the detection of an extended diffuse radio emission entrained in the X-ray spiral structure in a known cool core cluster Abell 795 (A795).","Though the cool core is a sign of the relaxed nature of the clusters, our re-analysed 30 ks Chandra X-ray data of cluster A795 confirms the presence of an interesting log spiral structure of X-ray deficit region complemented by an X-ray excess counter spiral in the residual map, exposing its dynamical activity.","Our new analysis of 150 $\\&$ 325 MHz GMRT archival data of the cluster confirms the detection of a $\\sim180$ kpc ultra-steep ($\\alpha\\sim-2.7$) diffuse radio structure which was previously reported as a candidate radio mini halo from low sensitive survey maps.","This radio emission spans the entire spiral structure ($\\sim186$ kpc), enclosed by two previously reported cold fronts.","Furthermore, SDSS DR13 optical spectra, as well as GALEX's FUV data, show a considerably low total star formation rate of 2.52 M$_{\\odot}$ yr$^{-1}$","and having no significant variation in metallicity distribution.","We argued that the two-phase (hot and cold) plasma at the cluster core with differential velocity has probably caused the spiral formation and has redistributed the secondary electrons from the central BCG or the pre-accelerated electrons which have been (re-)accelerated by the sloshing turbulence to form the observed candidate radio mini-halo structure.","This has been supported by a few previous studies that indicate spiral formation and sloshing turbulence may quench star formation and facilitate smooth metallicity distribution by mixing the gas in the core."],"url":"http://arxiv.org/abs/2405.19750v1","category":"astro-ph.GA"}
{"created":"2024-05-30 06:40:40","title":"CMC surfaces of revolution, Elliptic curves, and Weierstrass-$\\wp$ functions","abstract":"This paper establishes an interesting connection between the family of CMC surfaces of revolution in $\\mathbb E_1^3$ and some specific families of elliptic curves. As a consequence of this connection, we show in the class of spacelike CMC surfaces of revolution in the $\\mathbb E_1^3$, only spacelike cylinders and standard hyperboloids are algebraic. We also show that a similar connection exists between CMC surfaces of revolution in $\\mathbb E^3$ and elliptic curves. Further, we use this to reestablish the fact that the only CMC algebraic surfaces of revolution in $\\mathbb E^3$ are spheres and right circular cylinders.","sentences":["This paper establishes an interesting connection between the family of CMC surfaces of revolution in $\\mathbb E_1^3$ and some specific families of elliptic curves.","As a consequence of this connection, we show in the class of spacelike CMC surfaces of revolution in the $\\mathbb E_1^3$, only spacelike cylinders and standard hyperboloids are algebraic.","We also show that a similar connection exists between CMC surfaces of revolution in $\\mathbb E^3$ and elliptic curves.","Further, we use this to reestablish the fact that the only CMC algebraic surfaces of revolution in $\\mathbb E^3$ are spheres and right circular cylinders."],"url":"http://arxiv.org/abs/2405.19742v1","category":"math.DG"}
{"created":"2024-05-30 06:24:21","title":"Unraveling the global behavior of equation of state by explicit finite nuclei constraints","abstract":"We obtain posterior distribution of equations of state (EOSs) across a broad range of density by imposing explicitly the constraints from precisely measured fundamental properties of finite nuclei, in combination with the experimental data from heavy-ion collisions and the astrophysical observations of radius, tidal deformability and minimum-maximum mass of neutron stars. The acquired EOSs exhibit a distinct global behavior compared to those usually obtained by imposing the finite nuclei constraints implicitly through empirical values of selected key parameters describing symmetric nuclear matter and symmetry energy in the vicinity of the saturation density. The explicit treatment of finite nuclei constraints yields softer EOSs at low densities which eventually become stiffer to meet the maximum mass criteria. The Kullback-Leibler divergence has been used to perform a quantitative comparison of the distributions of neutron star properties resulting from the EOSs obtained from implicit and explicit finite nuclei constraints.","sentences":["We obtain posterior distribution of equations of state (EOSs) across a broad range of density by imposing explicitly the constraints from precisely measured fundamental properties of finite nuclei, in combination with the experimental data from heavy-ion collisions and the astrophysical observations of radius, tidal deformability and minimum-maximum mass of neutron stars.","The acquired EOSs exhibit a distinct global behavior compared to those usually obtained by imposing the finite nuclei constraints implicitly through empirical values of selected key parameters describing symmetric nuclear matter and symmetry energy in the vicinity of the saturation density.","The explicit treatment of finite nuclei constraints yields softer EOSs at low densities which eventually become stiffer to meet the maximum mass criteria.","The Kullback-Leibler divergence has been used to perform a quantitative comparison of the distributions of neutron star properties resulting from the EOSs obtained from implicit and explicit finite nuclei constraints."],"url":"http://arxiv.org/abs/2405.19733v1","category":"nucl-th"}
{"created":"2024-05-30 06:12:27","title":"Llarull's theorem on punctured sphere with $L^\\infty$ metric","abstract":"The classical Llarull theorem states that a smooth metric on $n$-sphere cannot have scalar curvature no less than $n(n-1)$ and dominate the standard spherical metric at the same time unless it is the standard spherical metric. In this work, we prove that Llarull's rigidity theorem holds for $L^{\\infty}$ metrics on spheres with finitely many points punctured. This is related to a question of Gromov.","sentences":["The classical Llarull theorem states that a smooth metric on $n$-sphere cannot have scalar curvature no less than $n(n-1)$ and dominate the standard spherical metric at the same time unless it is the standard spherical metric.","In this work, we prove that Llarull's rigidity theorem holds for $L^{\\infty}$ metrics on spheres with finitely many points punctured.","This is related to a question of Gromov."],"url":"http://arxiv.org/abs/2405.19724v1","category":"math.DG"}
{"created":"2024-05-30 06:05:56","title":"The Isospectral Problem for p-widths: An Application of Zoll Metrics","abstract":"We pose the isospectral problem for the $p$-widths: Is a riemannian manifold $(M^n, g)$ uniquely determined by its $p$-widths, $\\{\\omega_p(M,g)\\}_{p=1}^{\\infty}$? We construct many counterexamples on $S^2$ using Zoll metrics and the fact that geodesic $p$-widths are given by unions of immersed geodesics.","sentences":["We pose the isospectral problem for the $p$-widths: Is a riemannian manifold $(M^n, g)$ uniquely determined by its $p$-widths, $\\{\\omega_p(M,g)\\}_{p=1}^{\\infty}$?","We construct many counterexamples on $S^2$ using Zoll metrics and the fact that geodesic $p$-widths are given by unions of immersed geodesics."],"url":"http://arxiv.org/abs/2405.19719v1","category":"math.DG"}
{"created":"2024-05-30 05:45:00","title":"Status of QCD precision predictions for Drell-Yan processes","abstract":"We compute differential distributions for Drell-Yan processes at the LHC and the Tevatron colliders at next-to-next-to-leading order in perturbative QCD, including fiducial cuts on the decay leptons in the final state. The comparison of predictions obtained with four different codes shows excellent agreement, once linear power corrections from the fiducial cuts are included in those codes that rely on phase-space slicing subtraction schemes. For $Z$-boson production we perform a detailed study of the symmetric cuts on the transverse momenta of the decay leptons. Predictions at fixed order in perturbative QCD for those symmetric cuts, typically imposed in experiments, suffer from an instability. We show how this can be remedied by an all-order resummation of the fiducial transverse momentum spectrum, and we comment on the choice of cuts for future experimental analyses.","sentences":["We compute differential distributions for Drell-Yan processes at the LHC and the Tevatron colliders at next-to-next-to-leading order in perturbative QCD, including fiducial cuts on the decay leptons in the final state.","The comparison of predictions obtained with four different codes shows excellent agreement, once linear power corrections from the fiducial cuts are included in those codes that rely on phase-space slicing subtraction schemes.","For $Z$-boson production we perform a detailed study of the symmetric cuts on the transverse momenta of the decay leptons.","Predictions at fixed order in perturbative QCD for those symmetric cuts, typically imposed in experiments, suffer from an instability.","We show how this can be remedied by an all-order resummation of the fiducial transverse momentum spectrum, and we comment on the choice of cuts for future experimental analyses."],"url":"http://arxiv.org/abs/2405.19714v1","category":"hep-ph"}
{"created":"2024-05-30 05:43:09","title":"HINT: Learning Complete Human Neural Representations from Limited Viewpoints","abstract":"No augmented application is possible without animated humanoid avatars. At the same time, generating human replicas from real-world monocular hand-held or robotic sensor setups is challenging due to the limited availability of views. Previous work showed the feasibility of virtual avatars but required the presence of 360 degree views of the targeted subject. To address this issue, we propose HINT, a NeRF-based algorithm able to learn a detailed and complete human model from limited viewing angles. We achieve this by introducing a symmetry prior, regularization constraints, and training cues from large human datasets. In particular, we introduce a sagittal plane symmetry prior to the appearance of the human, directly supervise the density function of the human model using explicit 3D body modeling, and leverage a co-learned human digitization network as additional supervision for the unseen angles. As a result, our method can reconstruct complete humans even from a few viewing angles, increasing performance by more than 15% PSNR compared to previous state-of-the-art algorithms.","sentences":["No augmented application is possible without animated humanoid avatars.","At the same time, generating human replicas from real-world monocular hand-held or robotic sensor setups is challenging due to the limited availability of views.","Previous work showed the feasibility of virtual avatars but required the presence of 360 degree views of the targeted subject.","To address this issue, we propose HINT, a NeRF-based algorithm able to learn a detailed and complete human model from limited viewing angles.","We achieve this by introducing a symmetry prior, regularization constraints, and training cues from large human datasets.","In particular, we introduce a sagittal plane symmetry prior to the appearance of the human, directly supervise the density function of the human model using explicit 3D body modeling, and leverage a co-learned human digitization network as additional supervision for the unseen angles.","As a result, our method can reconstruct complete humans even from a few viewing angles, increasing performance by more than 15% PSNR compared to previous state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2405.19712v1","category":"cs.CV"}
{"created":"2024-05-30 05:05:51","title":"Neural Network Gauge Field Transformation for 4D SU(3) gauge fields","abstract":"We construct neural networks that work for any Lie group and maintain gauge covariance, enabling smooth, invertible gauge field transformations. We implement these transformations for 4D SU(3) lattice gauge fields and explore their use in HMC. We focus on developing loss functions and optimizing the transformations. We show the effects on HMC's molecular dynamics and discuss the scalability of the approach.","sentences":["We construct neural networks that work for any Lie group and maintain gauge covariance, enabling smooth, invertible gauge field transformations.","We implement these transformations for 4D SU(3) lattice gauge fields and explore their use in HMC.","We focus on developing loss functions and optimizing the transformations.","We show the effects on HMC's molecular dynamics and discuss the scalability of the approach."],"url":"http://arxiv.org/abs/2405.19692v1","category":"hep-lat"}
{"created":"2024-05-30 04:57:55","title":"DNPM: A Neural Parametric Model for the Synthesis of Facial Geometric Details","abstract":"Parametric 3D models have enabled a wide variety of computer vision and graphics tasks, such as modeling human faces, bodies and hands. In 3D face modeling, 3DMM is the most widely used parametric model, but can't generate fine geometric details solely from identity and expression inputs. To tackle this limitation, we propose a neural parametric model named DNPM for the facial geometric details, which utilizes deep neural network to extract latent codes from facial displacement maps encoding details and wrinkles. Built upon DNPM, a novel 3DMM named Detailed3DMM is proposed, which augments traditional 3DMMs by including the synthesis of facial details only from the identity and expression inputs. Moreover, we show that DNPM and Detailed3DMM can facilitate two downstream applications: speech-driven detailed 3D facial animation and 3D face reconstruction from a degraded image. Extensive experiments have shown the usefulness of DNPM and Detailed3DMM, and the progressiveness of two proposed applications.","sentences":["Parametric 3D models have enabled a wide variety of computer vision and graphics tasks, such as modeling human faces, bodies and hands.","In 3D face modeling, 3DMM is the most widely used parametric model, but can't generate fine geometric details solely from identity and expression inputs.","To tackle this limitation, we propose a neural parametric model named DNPM for the facial geometric details, which utilizes deep neural network to extract latent codes from facial displacement maps encoding details and wrinkles.","Built upon DNPM, a novel 3DMM named Detailed3DMM is proposed, which augments traditional 3DMMs by including the synthesis of facial details only from the identity and expression inputs.","Moreover, we show that DNPM and Detailed3DMM can facilitate two downstream applications: speech-driven detailed 3D facial animation and 3D face reconstruction from a degraded image.","Extensive experiments have shown the usefulness of DNPM and Detailed3DMM, and the progressiveness of two proposed applications."],"url":"http://arxiv.org/abs/2405.19688v1","category":"cs.CV"}
{"created":"2024-05-30 04:47:48","title":"Identifying Functional Brain Networks of Spatiotemporal Wide-Field Calcium Imaging Data via a Long Short-Term Memory Autoencoder","abstract":"Wide-field calcium imaging (WFCI) that records neural calcium dynamics allows for identification of functional brain networks (FBNs) in mice that express genetically encoded calcium indicators. Estimating FBNs from WFCI data is commonly achieved by use of seed-based correlation (SBC) analysis and independent component analysis (ICA). These two methods are conceptually distinct and each possesses limitations. Recent success of unsupervised representation learning in neuroimage analysis motivates the investigation of such methods to identify FBNs. In this work, a novel approach referred as LSTM-AER, is proposed in which a long short-term memory (LSTM) autoencoder (AE) is employed to learn spatial-temporal latent embeddings from WFCI data, followed by an ordinary least square regression (R) to estimate FBNs. The goal of this study is to elucidate and illustrate, qualitatively and quantitatively, the FBNs identified by use of the LSTM-AER method and compare them to those from traditional SBC and ICA. It was observed that spatial FBN maps produced from LSTM-AER resembled those derived by SBC and ICA while better accounting for intra-subject variation, data from a single hemisphere, shorter epoch lengths and tunable number of latent components. The results demonstrate the potential of unsupervised deep learning-based approaches to identifying and mapping FBNs.","sentences":["Wide-field calcium imaging (WFCI) that records neural calcium dynamics allows for identification of functional brain networks (FBNs) in mice that express genetically encoded calcium indicators.","Estimating FBNs from WFCI data is commonly achieved by use of seed-based correlation (SBC) analysis and independent component analysis (ICA).","These two methods are conceptually distinct and each possesses limitations.","Recent success of unsupervised representation learning in neuroimage analysis motivates the investigation of such methods to identify FBNs.","In this work, a novel approach referred as LSTM-AER, is proposed in which a long short-term memory (LSTM) autoencoder (AE) is employed to learn spatial-temporal latent embeddings from WFCI data, followed by an ordinary least square regression (R) to estimate FBNs.","The goal of this study is to elucidate and illustrate, qualitatively and quantitatively, the FBNs identified by use of the LSTM-AER method and compare them to those from traditional SBC and ICA.","It was observed that spatial FBN maps produced from LSTM-AER resembled those derived by SBC and ICA while better accounting for intra-subject variation, data from a single hemisphere, shorter epoch lengths and tunable number of latent components.","The results demonstrate the potential of unsupervised deep learning-based approaches to identifying and mapping FBNs."],"url":"http://arxiv.org/abs/2405.19685v1","category":"eess.IV"}
{"created":"2024-05-30 04:27:36","title":"Bayesian Online Natural Gradient (BONG)","abstract":"We propose a novel approach to sequential Bayesian inference based on variational Bayes. The key insight is that, in the online setting, we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep); instead we can optimize just the expected log-likelihood, performing a single step of natural gradient descent starting at the prior predictive. We prove this method recovers exact Bayesian inference if the model is conjugate, and empirically outperforms other online VB methods in the non-conjugate setting, such as online learning for neural networks, especially when controlling for computational costs.","sentences":["We propose a novel approach to sequential Bayesian inference based on variational Bayes.","The key insight is that, in the online setting, we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep); instead we can optimize just the expected log-likelihood, performing a single step of natural gradient descent starting at the prior predictive.","We prove this method recovers exact Bayesian inference if the model is conjugate, and empirically outperforms other online VB methods in the non-conjugate setting, such as online learning for neural networks, especially when controlling for computational costs."],"url":"http://arxiv.org/abs/2405.19681v1","category":"stat.ML"}
{"created":"2024-05-30 04:00:35","title":"On blow-up for the supercritical defocusing nonlinear wave equation","abstract":"In this paper, we consider the defocusing nonlinear wave equation $-\\partial_t^2u+\\Delta u=|u|^{p-1}u$ in $\\mathbb R\\times \\mathbb R^d$. Building on our companion work ({\\it \\small Self-similar imploding solutions of the relativistic Euler equations}), we prove that for $d=4, p\\geq 29$ and $d\\geq 5, p\\geq 17$, there exists a smooth complex-valued solution that blows up in finite time.","sentences":["In this paper, we consider the defocusing nonlinear wave equation $-\\partial_t^2u+\\Delta u=|u|^{p-1}u$ in $\\mathbb R\\times","\\mathbb R^d$. Building on our companion work ({\\it \\small Self-similar imploding solutions of the relativistic Euler equations}), we prove that for $d=4, p\\geq 29$ and $d\\geq 5, p\\geq 17$, there exists a smooth complex-valued solution that blows up in finite time."],"url":"http://arxiv.org/abs/2405.19674v1","category":"math.AP"}
{"created":"2024-05-30 03:33:09","title":"Gauge-invariant formulation for the gravitational wave equations","abstract":"A gauge-invariant formulation for the gravitational wave equations is presented. Using this approach, weak, plane wave solutions in a vacuum are derived in various theories. These include general relativity with two modes of polarization with helicity $\\pm 2$, Yang's theory with three modes of polarization with helicity $\\pm 2$ and $0$, and so-called \"general metric theories\" with six modes of polarization with helicity $\\pm 2$, $\\pm 1$, and two $0$'s. To identify the polarizations of gravitational waves, it is explicitly demonstrated how the gauge-invariant approach reproduces the earlier results.","sentences":["A gauge-invariant formulation for the gravitational wave equations is presented.","Using this approach, weak, plane wave solutions in a vacuum are derived in various theories.","These include general relativity with two modes of polarization with helicity $\\pm 2$, Yang's theory with three modes of polarization with helicity $\\pm 2$ and $0$, and so-called \"general metric theories\" with six modes of polarization with helicity $\\pm 2$, $\\pm 1$, and two $0$'s.","To identify the polarizations of gravitational waves, it is explicitly demonstrated how the gauge-invariant approach reproduces the earlier results."],"url":"http://arxiv.org/abs/2405.19662v1","category":"gr-qc"}
{"created":"2024-05-30 03:15:24","title":"Electric-field-tuned binding energies of trions in silicene, germanene, and stanene monolayers","abstract":"We predict a formation of intravalley and intervalley controllable trions in buckled 2D materials such as silicene, germanene and stanene monolayers in an external electric field. A study is performed within the framework of a nonrelativistic potential model using the method of hyperspherical harmonics (HH). We solve the three-body Schr\\\"{o}dinger equation with the Rytova-Keldysh potential by expanding the wave functions of a trion in terms of the HH. A resultant system of coupled differential equations are solved numerically. Controllable ground state energies of intravalley and intervalley trions by the external electric field are presented. The dependencies of the binding energy (BE) of trions in the silicene, germanene and stanene as a function of the electric field are qualitatively similar. BEs of trions formed by $A$ and $B$ excitons have a non-negligible differences which slightly increase as the electric field increases. It is demonstrated that trion BEs can be controlled by the external electric field and the dielectric environment has a significant effect on the trion BE","sentences":["We predict a formation of intravalley and intervalley controllable trions in buckled 2D materials such as silicene, germanene and stanene monolayers in an external electric field.","A study is performed within the framework of a nonrelativistic potential model using the method of hyperspherical harmonics (HH).","We solve the three-body Schr\\\"{o}dinger equation with the Rytova-Keldysh potential by expanding the wave functions of a trion in terms of the HH.","A resultant system of coupled differential equations are solved numerically.","Controllable ground state energies of intravalley and intervalley trions by the external electric field are presented.","The dependencies of the binding energy (BE) of trions in the silicene, germanene and stanene as a function of the electric field are qualitatively similar.","BEs of trions formed by $A$ and $B$ excitons have a non-negligible differences which slightly increase as the electric field increases.","It is demonstrated that trion BEs can be controlled by the external electric field and the dielectric environment has a significant effect on the trion BE"],"url":"http://arxiv.org/abs/2405.19655v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 02:27:19","title":"Deep Learning Model for Detecting Abnormal Corn Kernels","abstract":"This research aims to detect the physical characteristics of corn kernels and analyze images using a deep learning model. The data analysis based on the CRISP-DM framework which consists of six steps, business understanding, data understanding, data preparation, modelling, evaluation, and deployment. The business goal reduces the cost of the separation of abnormal corn kernels. The dataset comprises 1,800 images of corn kernels and divided equally between normal and abnormal corn kernels. The dataset was divided into three subsets: 1,000 images for training the deep learning model, 600 images for validation and 200 images for testing. The tools for analysis in this research are Jupyter Lab, Python, TensorFlow Keras, and Convolutional Neural Networks. The results revealed that the deep learning model achieved the accuracy rate of 99% in differentiating between normal and abnormal corn kernel images that is a highly effective model in this context.","sentences":["This research aims to detect the physical characteristics of corn kernels and analyze images using a deep learning model.","The data analysis based on the CRISP-DM framework which consists of six steps, business understanding, data understanding, data preparation, modelling, evaluation, and deployment.","The business goal reduces the cost of the separation of abnormal corn kernels.","The dataset comprises 1,800 images of corn kernels and divided equally between normal and abnormal corn kernels.","The dataset was divided into three subsets: 1,000 images for training the deep learning model, 600 images for validation and 200 images for testing.","The tools for analysis in this research are Jupyter Lab, Python, TensorFlow Keras, and Convolutional Neural Networks.","The results revealed that the deep learning model achieved the accuracy rate of 99% in differentiating between normal and abnormal corn kernel images that is a highly effective model in this context."],"url":"http://arxiv.org/abs/2405.19628v1","category":"cs.CE"}
{"created":"2024-05-30 02:20:54","title":"Single spin asymmetry $ A _ { U L } ^ { \\sin ( 2 \u03c6_ { h } ) }$ in dihadron production in SIDIS","abstract":"The paper calculates the helicity-dependent dihadron fragmentation function (DiFF), by extending the dihadron spectator model and examine the single longitudinal spin asymmetry $A^{\\sin(2\\phi_h)}_{UL}$ from dihadron in semi-inclusive inelastic scattering (SIDIS). This function elucidates the relationship between the longitudinal polarization of the fragmented quark and the transverse momentum of the resulting hadron pairs. A study by the COMPASS collaboration detected a minimal signal in their experimental search for this azimuthal asymmetry in SIDIS. Here, we use the spectator model to calculate the unknown T-odd dihadron fragmentation function $H_1^\\perp$. Adopting collinear factorization to describe the data, avoiding the transverse momentum dependent factorization and the associated resummation effects, helping us understand the asymmetry and explaining why the signal is so weak. We involve the approach of transverse momentum dependence in the model calculations, in order to formulate the differential cross sections and the spin asymmetries in terms of the collinear parton distributions and the collinear DiFFs. A transverse momentum factor analysis method was used, in which the transverse momentum of the final hadron pairs was not integrated. The asymmetry of $sin(2\\phi_h)$ in COMPASS kinematics was calculated and compared with experimental data. In addition, predictions for the same asymmetry are also presented for HERMES and the Electron Ion Collider.","sentences":["The paper calculates the helicity-dependent dihadron fragmentation function (DiFF), by extending the dihadron spectator model and examine the single longitudinal spin asymmetry $A^{\\sin(2\\phi_h)}_{UL}$ from dihadron in semi-inclusive inelastic scattering (SIDIS).","This function elucidates the relationship between the longitudinal polarization of the fragmented quark and the transverse momentum of the resulting hadron pairs.","A study by the COMPASS collaboration detected a minimal signal in their experimental search for this azimuthal asymmetry in SIDIS.","Here, we use the spectator model to calculate the unknown T-odd dihadron fragmentation function $H_1^\\perp$. Adopting collinear factorization to describe the data, avoiding the transverse momentum dependent factorization and the associated resummation effects, helping us understand the asymmetry and explaining why the signal is so weak.","We involve the approach of transverse momentum dependence in the model calculations, in order to formulate the differential cross sections and the spin asymmetries in terms of the collinear parton distributions and the collinear DiFFs.","A transverse momentum factor analysis method was used, in which the transverse momentum of the final hadron pairs was not integrated.","The asymmetry of $sin(2\\phi_h)$ in COMPASS kinematics was calculated and compared with experimental data.","In addition, predictions for the same asymmetry are also presented for HERMES and the Electron Ion Collider."],"url":"http://arxiv.org/abs/2405.19624v1","category":"hep-ph"}
{"created":"2024-05-30 02:13:56","title":"SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation","abstract":"The well-established modular autonomous driving system is decoupled into different standalone tasks, e.g. perception, prediction and planning, suffering from information loss and error accumulation across modules. In contrast, end-to-end paradigms unify multi-tasks into a fully differentiable framework, allowing for optimization in a planning-oriented spirit. Despite the great potential of end-to-end paradigms, both the performance and efficiency of existing methods are not satisfactory, particularly in terms of planning safety. We attribute this to the computationally expensive BEV (bird's eye view) features and the straightforward design for prediction and planning. To this end, we explore the sparse representation and review the task design for end-to-end autonomous driving, proposing a new paradigm named SparseDrive. Concretely, SparseDrive consists of a symmetric sparse perception module and a parallel motion planner. The sparse perception module unifies detection, tracking and online mapping with a symmetric model architecture, learning a fully sparse representation of the driving scene. For motion prediction and planning, we review the great similarity between these two tasks, leading to a parallel design for motion planner. Based on this parallel design, which models planning as a multi-modal problem, we propose a hierarchical planning selection strategy , which incorporates a collision-aware rescore module, to select a rational and safe trajectory as the final planning output. With such effective designs, SparseDrive surpasses previous state-of-the-arts by a large margin in performance of all tasks, while achieving much higher training and inference efficiency. Code will be avaliable at https://github.com/swc-17/SparseDrive for facilitating future research.","sentences":["The well-established modular autonomous driving system is decoupled into different standalone tasks, e.g. perception, prediction and planning, suffering from information loss and error accumulation across modules.","In contrast, end-to-end paradigms unify multi-tasks into a fully differentiable framework, allowing for optimization in a planning-oriented spirit.","Despite the great potential of end-to-end paradigms, both the performance and efficiency of existing methods are not satisfactory, particularly in terms of planning safety.","We attribute this to the computationally expensive BEV (bird's eye view) features and the straightforward design for prediction and planning.","To this end, we explore the sparse representation and review the task design for end-to-end autonomous driving, proposing a new paradigm named SparseDrive.","Concretely, SparseDrive consists of a symmetric sparse perception module and a parallel motion planner.","The sparse perception module unifies detection, tracking and online mapping with a symmetric model architecture, learning a fully sparse representation of the driving scene.","For motion prediction and planning, we review the great similarity between these two tasks, leading to a parallel design for motion planner.","Based on this parallel design, which models planning as a multi-modal problem, we propose a hierarchical planning selection strategy , which incorporates a collision-aware rescore module, to select a rational and safe trajectory as the final planning output.","With such effective designs, SparseDrive surpasses previous state-of-the-arts by a large margin in performance of all tasks, while achieving much higher training and inference efficiency.","Code will be avaliable at https://github.com/swc-17/SparseDrive for facilitating future research."],"url":"http://arxiv.org/abs/2405.19620v1","category":"cs.CV"}
{"created":"2024-05-30 02:13:28","title":"Simple and explicit constructions of semi-discrete surfaces and discrete surfaces","abstract":"We give a simple and explicit constructions of various semi-discrete surfaces and discrete $K$-surfaces in terms of the Jacobi elliptic functions using $\\tau$-functions. Their periodicities are also determined.","sentences":["We give a simple and explicit constructions of various semi-discrete surfaces and discrete $K$-surfaces in terms of the Jacobi elliptic functions using $\\tau$-functions.","Their periodicities are also determined."],"url":"http://arxiv.org/abs/2405.19619v1","category":"math.DG"}
{"created":"2024-05-30 02:10:39","title":"On the closure of curvature in 2D flamelet theory","abstract":"So far, flamelet theory has treated curvature as an independent parameter requiring specific means for closure. In this work, it is shown how the adoption of a two-dimensional orthogonal composition space allows obtaining formal mathematical relations between the flame curvatures and the gradients of the conditioning scalars (also called flamelet coordinates). With these, both curvatures become a flame response to the underlying flow, which conveniently allows removing them from the corresponding set of flamelet equations. While the demonstration is performed in the context of partially premixed flames, the approach is general and applicable to any orthogonal coordinate system.","sentences":["So far, flamelet theory has treated curvature as an independent parameter requiring specific means for closure.","In this work, it is shown how the adoption of a two-dimensional orthogonal composition space allows obtaining formal mathematical relations between the flame curvatures and the gradients of the conditioning scalars (also called flamelet coordinates).","With these, both curvatures become a flame response to the underlying flow, which conveniently allows removing them from the corresponding set of flamelet equations.","While the demonstration is performed in the context of partially premixed flames, the approach is general and applicable to any orthogonal coordinate system."],"url":"http://arxiv.org/abs/2405.19617v1","category":"physics.flu-dyn"}
{"created":"2024-05-30 02:06:14","title":"On the persistence properties for the fractionary BBM equation with low dispersion in weighted Sobolev spaces","abstract":"We consider the initial value problem associated to the low dispersion fractionary Benjamin-Bona-Mahony equation, fBBM. Our aim is to establish local persistence results in weighted Sobolev spaces and to obtain unique continuation results that imply that those results above are sharp. Hence, arbitrary polynomial type decay is not preserved by the fBBM flow.","sentences":["We consider the initial value problem associated to the low dispersion fractionary Benjamin-Bona-Mahony equation, fBBM.","Our aim is to establish local persistence results in weighted Sobolev spaces and to obtain unique continuation results that imply that those results above are sharp.","Hence, arbitrary polynomial type decay is not preserved by the fBBM flow."],"url":"http://arxiv.org/abs/2405.19613v1","category":"math.AP"}
{"created":"2024-05-30 01:56:49","title":"Factor Augmented Tensor-on-Tensor Neural Networks","abstract":"This paper studies the prediction task of tensor-on-tensor regression in which both covariates and responses are multi-dimensional arrays (a.k.a., tensors) across time with arbitrary tensor order and data dimension. Existing methods either focused on linear models without accounting for possibly nonlinear relationships between covariates and responses, or directly employed black-box deep learning algorithms that failed to utilize the inherent tensor structure. In this work, we propose a Factor Augmented Tensor-on-Tensor Neural Network (FATTNN) that integrates tensor factor models into deep neural networks. We begin with summarizing and extracting useful predictive information (represented by the ``factor tensor'') from the complex structured tensor covariates, and then proceed with the prediction task using the estimated factor tensor as input of a temporal convolutional neural network. The proposed methods effectively handle nonlinearity between complex data structures, and improve over traditional statistical models and conventional deep learning approaches in both prediction accuracy and computational cost. By leveraging tensor factor models, our proposed methods exploit the underlying latent factor structure to enhance the prediction, and in the meantime, drastically reduce the data dimensionality that speeds up the computation. The empirical performances of our proposed methods are demonstrated via simulation studies and real-world applications to three public datasets. Numerical results show that our proposed algorithms achieve substantial increases in prediction accuracy and significant reductions in computational time compared to benchmark methods.","sentences":["This paper studies the prediction task of tensor-on-tensor regression in which both covariates and responses are multi-dimensional arrays (a.k.a., tensors) across time with arbitrary tensor order and data dimension.","Existing methods either focused on linear models without accounting for possibly nonlinear relationships between covariates and responses, or directly employed black-box deep learning algorithms that failed to utilize the inherent tensor structure.","In this work, we propose a Factor Augmented Tensor-on-Tensor Neural Network (FATTNN) that integrates tensor factor models into deep neural networks.","We begin with summarizing and extracting useful predictive information (represented by the ``factor tensor'') from the complex structured tensor covariates, and then proceed with the prediction task using the estimated factor tensor as input of a temporal convolutional neural network.","The proposed methods effectively handle nonlinearity between complex data structures, and improve over traditional statistical models and conventional deep learning approaches in both prediction accuracy and computational cost.","By leveraging tensor factor models, our proposed methods exploit the underlying latent factor structure to enhance the prediction, and in the meantime, drastically reduce the data dimensionality that speeds up the computation.","The empirical performances of our proposed methods are demonstrated via simulation studies and real-world applications to three public datasets.","Numerical results show that our proposed algorithms achieve substantial increases in prediction accuracy and significant reductions in computational time compared to benchmark methods."],"url":"http://arxiv.org/abs/2405.19610v1","category":"stat.ML"}
{"created":"2024-05-30 01:51:46","title":"Estimating the Accuracy of the Variational Energy: Zeeman Effect in Hydrogen Atom","abstract":"For a hydrogen atom subjected to a constant magnetic field, we report the first numerical realization of the two-dimensional Non-Linearization Procedure (NLP) to estimate the accuracy of the variational energy associated with a trial function. Relevant equations of the NLP, which resemble those describing a dielectric medium with a space-dependent permittivity, are solved numerically with high accuracy using an orthogonal collocation method. As an illustration, we consider the trial function proposed in del Valle et al., Phys. Rev. A 103, (2021) 032820, and establish the accuracy of the variational energy and the local accuracy of the trial function for a strength of the magnetic field $\\gamma=1$ a.u. Additionally, we investigate the accuracy of the cusp parameter and quadrupole moment calculated by means of the trial function.","sentences":["For a hydrogen atom subjected to a constant magnetic field, we report the first numerical realization of the two-dimensional Non-Linearization Procedure (NLP) to estimate the accuracy of the variational energy associated with a trial function.","Relevant equations of the NLP, which resemble those describing a dielectric medium with a space-dependent permittivity, are solved numerically with high accuracy using an orthogonal collocation method.","As an illustration, we consider the trial function proposed in del Valle et al., Phys.","Rev. A 103, (2021) 032820, and establish the accuracy of the variational energy and the local accuracy of the trial function for a strength of the magnetic field $\\gamma=1$ a.u.","Additionally, we investigate the accuracy of the cusp parameter and quadrupole moment calculated by means of the trial function."],"url":"http://arxiv.org/abs/2405.19608v1","category":"physics.atom-ph"}
{"created":"2024-05-30 01:17:14","title":"Factorized Approximation to the IMSRG(3)","abstract":"We describe an approximation to the in-medium similarity renormalization group (IMSRG) method in which we include the effects of intermediate three-body operators arising within nested commutators. As an initial step, we present the relevant equations for two nested commutators, all of which can be factorized so that the method scales like the standard IMSRG(2) approximation, enabling large-scale calculations. We test the accuracy of this approximation scheme, and apply it to the isotopic chains of carbon, sulfur and nickel isotopic chains. We obtain an improved description of spectroscopy, and a reduced dependence on the choice of the valence space. In addition, we provide an explanation of the relative importance of the diagram topologies included, with an eye toward assessing the impact of remaining omitted terms.","sentences":["We describe an approximation to the in-medium similarity renormalization group (IMSRG) method in which we include the effects of intermediate three-body operators arising within nested commutators.","As an initial step, we present the relevant equations for two nested commutators, all of which can be factorized so that the method scales like the standard IMSRG(2) approximation, enabling large-scale calculations.","We test the accuracy of this approximation scheme, and apply it to the isotopic chains of carbon, sulfur and nickel isotopic chains.","We obtain an improved description of spectroscopy, and a reduced dependence on the choice of the valence space.","In addition, we provide an explanation of the relative importance of the diagram topologies included, with an eye toward assessing the impact of remaining omitted terms."],"url":"http://arxiv.org/abs/2405.19594v1","category":"nucl-th"}
{"created":"2024-05-30 00:39:40","title":"Bounding the softwired parsimony score of a phylogenetic network","abstract":"In comparison to phylogenetic trees, phylogenetic networks are more suitable to represent complex evolutionary histories of species whose past includes reticulation such as hybridisation or lateral gene transfer. However, the reconstruction of phylogenetic networks remains challenging and computationally expensive due to their intricate structural properties. For example, the small parsimony problem that is solvable in polynomial time for phylogenetic trees, becomes NP-hard on phylogenetic networks under softwired and parental parsimony, even for a single binary character and structurally constrained networks. To calculate the parsimony score of a phylogenetic network $N$, these two parsimony notions consider different exponential-size sets of phylogenetic trees that can be extracted from $N$ and infer the minimum parsimony score over all trees in the set. In this paper, we ask: What is the maximum difference between the parsimony score of any phylogenetic tree that is contained in the set of considered trees and a phylogenetic tree whose parsimony score equates to the parsimony score of $N$? Given a gap-free sequence alignment of multi-state characters and a rooted binary level-$k$ phylogenetic network, we use the novel concept of an informative blob to show that this difference is bounded by $k+1$ times the softwired parsimony score of $N$. In particular, the difference is independent of the alignment length and the number of character states. We show that an analogous bound can be obtained for the softwired parsimony score of semi-directed networks, while under parental parsimony on the other hand, such a bound does not hold.","sentences":["In comparison to phylogenetic trees, phylogenetic networks are more suitable to represent complex evolutionary histories of species whose past includes reticulation such as hybridisation or lateral gene transfer.","However, the reconstruction of phylogenetic networks remains challenging and computationally expensive due to their intricate structural properties.","For example, the small parsimony problem that is solvable in polynomial time for phylogenetic trees, becomes NP-hard on phylogenetic networks under softwired and parental parsimony, even for a single binary character and structurally constrained networks.","To calculate the parsimony score of a phylogenetic network $N$, these two parsimony notions consider different exponential-size sets of phylogenetic trees that can be extracted from $N$ and infer the minimum parsimony score over all trees in the set.","In this paper, we ask: What is the maximum difference between the parsimony score of any phylogenetic tree that is contained in the set of considered trees and a phylogenetic tree whose parsimony score equates to the parsimony score of $N$?","Given a gap-free sequence alignment of multi-state characters and a rooted binary level-$k$ phylogenetic network, we use the novel concept of an informative blob to show that this difference is bounded by $k+1$ times the softwired parsimony score of $N$. In particular, the difference is independent of the alignment length and the number of character states.","We show that an analogous bound can be obtained for the softwired parsimony score of semi-directed networks, while under parental parsimony on the other hand, such a bound does not hold."],"url":"http://arxiv.org/abs/2405.19587v1","category":"q-bio.PE"}
{"created":"2024-05-30 00:22:48","title":"Existence, Uniqueness and Asymptotic Dynamics of Nonlinear Schr\u00f6dinger Equations With Quasi-Periodic Initial Data: I. The Standard NLS","abstract":"This is the first part of a two-paper series studying nonlinear Schr\\\"odinger equations with quasi-periodic initial data. In this paper, we consider the standard nonlinear Schr\\\"odinger equation. Under the assumption that the Fourier coefficients of the initial data obey a power-law upper bound, we establish local existence of a solution that retains quasi-periodicity in space with a slightly weaker Fourier decay. Moreover, the solution is shown to be unique within this class of quasi-periodic functions. In addition, for the nonlinear Schr\\\"odinger equation with small nonlinearity, within the time scale, as the small parameter of nonlinearity tends to zero, we prove that the nonlinear solution converges asymptotically to the linear solution with respect to both the sup-norm $\\|\\cdot\\|_{L_x^\\infty(\\mathbb R)}$ and the Sobolev-norm $\\|\\cdot\\|_{H^s_x(\\mathbb R)}$.   The proof proceeds via a consideration of an associated infinite system of coupled ordinary differential equations for the Fourier coefficients and a combinatorial analysis of the resulting tree expansion of the coefficients. For this purpose, we introduce a Feynman diagram for the Picard iteration and $\\ast^{[\\cdot]}$ to denote the complex conjugate label.","sentences":["This is the first part of a two-paper series studying nonlinear Schr\\\"odinger equations with quasi-periodic initial data.","In this paper, we consider the standard nonlinear Schr\\\"odinger equation.","Under the assumption that the Fourier coefficients of the initial data obey a power-law upper bound, we establish local existence of a solution that retains quasi-periodicity in space with a slightly weaker Fourier decay.","Moreover, the solution is shown to be unique within this class of quasi-periodic functions.","In addition, for the nonlinear Schr\\\"odinger equation with small nonlinearity, within the time scale, as the small parameter of nonlinearity tends to zero, we prove that the nonlinear solution converges asymptotically to the linear solution with respect to both the sup-norm $\\|\\cdot\\|_{L_x^\\infty(\\mathbb R)}$ and the Sobolev-norm $\\|\\cdot\\|_{H^s_x(\\mathbb R)}$.   ","The proof proceeds via a consideration of an associated infinite system of coupled ordinary differential equations for the Fourier coefficients and a combinatorial analysis of the resulting tree expansion of the coefficients.","For this purpose, we introduce a Feynman diagram for the Picard iteration and $\\ast^{[\\cdot]}$ to denote the complex conjugate label."],"url":"http://arxiv.org/abs/2405.19583v1","category":"math.AP"}
{"created":"2024-05-29 22:44:00","title":"Numerical analysis of a 1/2-equation model of turbulence","abstract":"The recent 1/2-equation model of turbulence is a simplification of the standard Kolmogorov-Prandtl 1-equation URANS model. Surprisingly, initial numerical tests indicated that the 1/2-equation model produces comparable velocity statistics at reduced cost. It is also a test problem and first step for developing numerical analysis to address a full 1-equation model. This report begins the numerical analysis of the 1/2 equation model. Stability, convergence and error estimates are proven for a semi-discrete and fully discrete approximation. Finally, numerical tests are conducted to validate our convergence theory.","sentences":["The recent 1/2-equation model of turbulence is a simplification of the standard Kolmogorov-Prandtl 1-equation URANS model.","Surprisingly, initial numerical tests indicated that the 1/2-equation model produces comparable velocity statistics at reduced cost.","It is also a test problem and first step for developing numerical analysis to address a full 1-equation model.","This report begins the numerical analysis of the 1/2 equation model.","Stability, convergence and error estimates are proven for a semi-discrete and fully discrete approximation.","Finally, numerical tests are conducted to validate our convergence theory."],"url":"http://arxiv.org/abs/2405.19554v1","category":"math.NA"}
{"created":"2024-05-29 22:15:23","title":"Two concurrent exponential decays in a vacuum capacitor-resistor circuit: effect of surface charge","abstract":"The relaxation in a vacuum capacitor-resistor circuit is comprised of two exponential decays, one caused by surface charge and the other by the decay of energy stored between the capacitor plates. A simple phenomenological model of this relaxation is shown to be supported by measurements even though Maxwell's equations are difficult to apply in this case. Similar behavior is also observed for polypropylene capacitors, indicating that this surface charge effect is applicable to all capacitors and potentially other circuit components.","sentences":["The relaxation in a vacuum capacitor-resistor circuit is comprised of two exponential decays, one caused by surface charge and the other by the decay of energy stored between the capacitor plates.","A simple phenomenological model of this relaxation is shown to be supported by measurements even though Maxwell's equations are difficult to apply in this case.","Similar behavior is also observed for polypropylene capacitors, indicating that this surface charge effect is applicable to all capacitors and potentially other circuit components."],"url":"http://arxiv.org/abs/2405.19545v1","category":"physics.app-ph"}
{"created":"2024-05-29 21:36:24","title":"Stochastic thermodynamics of ecosystems","abstract":"We investigate the thermodynamics as well as the population dynamics of ecosystems based on a stochastic approach in which the number of individuals of the several species of the ecosystem are treated as stochastic variables. The several species are connected by feeding relationships that are understood as unidirectional processes in which a certain amount of biomass is exchanged between species. We show that the equations for the averages in the numbers of individuals are that given by the deterministic approach. We determine the fluxes of mass, energy, and entropy as well as the rate of the entropy production. This last quantity, which has a central role in the present stochastic approach, is obtained by a formula appropriate for unidirectional transitions. The flux of energy across the ecosystem is shown to be proportional to the flux of entropy to the environment.","sentences":["We investigate the thermodynamics as well as the population dynamics of ecosystems based on a stochastic approach in which the number of individuals of the several species of the ecosystem are treated as stochastic variables.","The several species are connected by feeding relationships that are understood as unidirectional processes in which a certain amount of biomass is exchanged between species.","We show that the equations for the averages in the numbers of individuals are that given by the deterministic approach.","We determine the fluxes of mass, energy, and entropy as well as the rate of the entropy production.","This last quantity, which has a central role in the present stochastic approach, is obtained by a formula appropriate for unidirectional transitions.","The flux of energy across the ecosystem is shown to be proportional to the flux of entropy to the environment."],"url":"http://arxiv.org/abs/2405.19535v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-29 21:10:30","title":"On decay and regularity of solutions to the modified 2D Zakharov-Kuznetsov equation","abstract":"This work is devoted to study the relation between regularity and decay for solutions of the two-dimensional modified Zakharov-Kuznetsov equation in the weighted Sobolev spaces $Z_{s,(r_1,r_2)}:=H^s(\\R^2)\\cap L^2((1+|x|^{2r_1}+|y|^{2r_2})dxdy)$.","sentences":["This work is devoted to study the relation between regularity and decay for solutions of the two-dimensional modified Zakharov-Kuznetsov equation in the weighted Sobolev spaces $Z_{s,(r_1,r_2)}:=H^s(\\R^2)\\cap L^2((1+|x|^{2r_1}+|y|^{2r_2})dxdy)$."],"url":"http://arxiv.org/abs/2405.19526v1","category":"math.AP"}
{"created":"2024-05-29 20:40:20","title":"MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings","abstract":"Neural embedding models have become a fundamental component of modern information retrieval (IR) pipelines. These models produce a single embedding $x \\in \\mathbb{R}^d$ per data-point, allowing for fast retrieval via highly optimized maximum inner product search (MIPS) algorithms. Recently, beginning with the landmark ColBERT paper, multi-vector models, which produce a set of embedding per data point, have achieved markedly superior performance for IR tasks. Unfortunately, using these models for IR is computationally expensive due to the increased complexity of multi-vector retrieval and scoring.   In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a retrieval mechanism which reduces multi-vector similarity search to single-vector similarity search. This enables the usage of off-the-shelf MIPS solvers for multi-vector retrieval. MUVERA asymmetrically generates Fixed Dimensional Encodings (FDEs) of queries and documents, which are vectors whose inner product approximates multi-vector similarity. We prove that FDEs give high-quality $\\epsilon$-approximations, thus providing the first single-vector proxy for multi-vector similarity with theoretical guarantees. Empirically, we find that FDEs achieve the same recall as prior state-of-the-art heuristics while retrieving 2-5$\\times$ fewer candidates. Compared to prior state of the art implementations, MUVERA achieves consistently good end-to-end recall and latency across a diverse set of the BEIR retrieval datasets, achieving an average of 10$\\%$ improved recall with $90\\%$ lower latency.","sentences":["Neural embedding models have become a fundamental component of modern information retrieval (IR) pipelines.","These models produce a single embedding $x \\in \\mathbb{R}^d$ per data-point, allowing for fast retrieval via highly optimized maximum inner product search (MIPS) algorithms.","Recently, beginning with the landmark ColBERT paper, multi-vector models, which produce a set of embedding per data point, have achieved markedly superior performance for IR tasks.","Unfortunately, using these models for IR is computationally expensive due to the increased complexity of multi-vector retrieval and scoring.   ","In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a retrieval mechanism which reduces multi-vector similarity search to single-vector similarity search.","This enables the usage of off-the-shelf MIPS solvers for multi-vector retrieval.","MUVERA asymmetrically generates Fixed Dimensional Encodings (FDEs) of queries and documents, which are vectors whose inner product approximates multi-vector similarity.","We prove that FDEs give high-quality $\\epsilon$-approximations, thus providing the first single-vector proxy for multi-vector similarity with theoretical guarantees.","Empirically, we find that FDEs achieve the same recall as prior state-of-the-art heuristics while retrieving 2-5$\\times$ fewer candidates.","Compared to prior state of the art implementations, MUVERA achieves consistently good end-to-end recall and latency across a diverse set of the BEIR retrieval datasets, achieving an average of 10$\\%$ improved recall with $90\\%$ lower latency."],"url":"http://arxiv.org/abs/2405.19504v1","category":"cs.DS"}
{"created":"2024-05-29 20:05:46","title":"A Full-duplex Speech Dialogue Scheme Based On Large Language Models","abstract":"We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction. It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states. The perception and motor function modules operate simultaneously, allowing the system to simultaneously speak and listen to the user. The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM. All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time. In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than 3 folds compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions. Running a LLM with only 8 billion parameters, our system exhibits a 8% higher interruption precision rate than the best available commercial LLM for voice-based dialogue.","sentences":["We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction.","It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states.","The perception and motor function modules operate simultaneously, allowing the system to simultaneously speak and listen to the user.","The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM.","All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time.","In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than 3 folds compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions.","Running a LLM with only 8 billion parameters, our system exhibits a 8% higher interruption precision rate than the best available commercial LLM for voice-based dialogue."],"url":"http://arxiv.org/abs/2405.19487v1","category":"cs.CL"}
{"created":"2024-05-29 19:58:11","title":"Caustics of a Paraboloid and Apollonius Problem","abstract":"We study caustics of an elliptical paraboloid and the history of their various representations from 3D models in XIX century to the recent computer graphics. In the paper two ways of generating the surface, one with cartesian coordinates using formula for principal curvatures, and the other one with parabolic coordinates using Seidel's formula were demonstrated. By finding the intersection curves of these caustics with the paraboloid we extend the solution of F. Caspari for classical Apollonius problem about the number of concurrent normals to the points of the paraboloid itself. A complete classification of all possible cases of intersections of these caustics with their paraboloid is given.","sentences":["We study caustics of an elliptical paraboloid and the history of their various representations from 3D models in XIX century to the recent computer graphics.","In the paper two ways of generating the surface, one with cartesian coordinates using formula for principal curvatures, and the other one with parabolic coordinates using Seidel's formula were demonstrated.","By finding the intersection curves of these caustics with the paraboloid we extend the solution of F. Caspari for classical Apollonius problem about the number of concurrent normals to the points of the paraboloid itself.","A complete classification of all possible cases of intersections of these caustics with their paraboloid is given."],"url":"http://arxiv.org/abs/2405.19484v1","category":"math.DG"}
{"created":"2024-05-29 19:57:02","title":"IMEX methods for thin-film equations and Cahn-Hilliard equations with variable mobility","abstract":"We explore a class of splitting schemes employing implicit-explicit (IMEX) time-stepping to achieve accurate and energy-stable solutions for thin-film equations and Cahn-Hilliard models with variable mobility. This splitting method incorporates a linear, constant coefficient implicit step, facilitating efficient computational implementation. We investigate the influence of stabilizing splitting parameters on the numerical solution computationally, considering various initial conditions. Furthermore, we generate energy-stability plots for the proposed methods, examining different choices of splitting parameter values and timestep sizes. These methods enhance the accuracy of the original bi-harmonic-modified (BHM) approach, while preserving its energy-decreasing property and achieving second-order accuracy. We present numerical experiments to illustrate the performance of the proposed methods.","sentences":["We explore a class of splitting schemes employing implicit-explicit (IMEX) time-stepping to achieve accurate and energy-stable solutions for thin-film equations and Cahn-Hilliard models with variable mobility.","This splitting method incorporates a linear, constant coefficient implicit step, facilitating efficient computational implementation.","We investigate the influence of stabilizing splitting parameters on the numerical solution computationally, considering various initial conditions.","Furthermore, we generate energy-stability plots for the proposed methods, examining different choices of splitting parameter values and timestep sizes.","These methods enhance the accuracy of the original bi-harmonic-modified (BHM) approach, while preserving its energy-decreasing property and achieving second-order accuracy.","We present numerical experiments to illustrate the performance of the proposed methods."],"url":"http://arxiv.org/abs/2405.19483v1","category":"math.NA"}
{"created":"2024-05-29 19:55:10","title":"Malliavian differentiablity and smoothness of density for SDES with locally Lipschitz coefficients","abstract":"We study Malliavin differentiability for the solutions of a stochastic differential equation with drift of super-linear growth. Assuming we have a monotone drift with polynomial growth, we prove Malliavin differentiability of any order. As a consequence of this result, under the H\\\"ormander's hypothesis we prove that the density of the solution's law with respect to the Lebesgue measure is infinitely differentiable. To avoid non-integrability problems due to the unbounded drift, we follow an approach based on the concepts of Ray Absolute Continuity and Stochastic Gate\\^aux Differentiability.","sentences":["We study Malliavin differentiability for the solutions of a stochastic differential equation with drift of super-linear growth.","Assuming we have a monotone drift with polynomial growth, we prove Malliavin differentiability of any order.","As a consequence of this result, under the H\\\"ormander's hypothesis we prove that the density of the solution's law with respect to the Lebesgue measure is infinitely differentiable.","To avoid non-integrability problems due to the unbounded drift, we follow an approach based on the concepts of Ray Absolute Continuity and Stochastic Gate\\^aux Differentiability."],"url":"http://arxiv.org/abs/2405.19482v1","category":"math.PR"}
{"created":"2024-05-29 19:52:27","title":"New sector morphologies emerge from anisotropic colony growth","abstract":"Competition during range expansions is of great interest from both practical and theoretical view points. Experimentally, range expansions are often studied in homogeneous Petri dishes, which lack spatial anisotropy that might be present in realistic populations. Here, we analyze a model of anisotropic growth, based on coupled Kardar-Parisi-Zhang and Fisher-Kolmogorov-Petrovsky-Piskunov equations that describe surface growth and lateral competition. Compared to a previous study of isotropic growth, anisotropy relaxes a constraint between parameters of the model. We completely characterize spatial patterns and invasion velocities in this generalized model. In particular, we find that strong anisotropy results in a distinct morphology of spatial invasion with a kink in the displaced strain ahead of the boundary between the strains. This morphology of the out-competed strain is similar to a shock wave and serves as a signature of anisotropic growth.","sentences":["Competition during range expansions is of great interest from both practical and theoretical view points.","Experimentally, range expansions are often studied in homogeneous Petri dishes, which lack spatial anisotropy that might be present in realistic populations.","Here, we analyze a model of anisotropic growth, based on coupled Kardar-Parisi-Zhang and Fisher-Kolmogorov-Petrovsky-Piskunov equations that describe surface growth and lateral competition.","Compared to a previous study of isotropic growth, anisotropy relaxes a constraint between parameters of the model.","We completely characterize spatial patterns and invasion velocities in this generalized model.","In particular, we find that strong anisotropy results in a distinct morphology of spatial invasion with a kink in the displaced strain ahead of the boundary between the strains.","This morphology of the out-competed strain is similar to a shock wave and serves as a signature of anisotropic growth."],"url":"http://arxiv.org/abs/2405.19478v1","category":"nlin.PS"}
{"created":"2024-05-29 19:48:48","title":"An Ultrametric for Cartesian Differential Categories for Taylor Series Convergence","abstract":"Cartesian differential categories provide a categorical framework for multivariable differential calculus and also the categorical semantics of the differential $\\lambda$-calculus. Taylor series expansion is an important concept for both differential calculus and the differential $\\lambda$-calculus. In differential calculus, a function is equal to its Taylor series if its sequence of Taylor polynomials converges to the function in the analytic sense. On the other hand, for the differential $\\lambda$-calculus, one works in a setting with an appropriate notion of algebraic infinite sums to formalize Taylor series expansion. In this paper, we provide a formal theory of Taylor series in an arbitrary Cartesian differential category without the need for converging limits or infinite sums. We begin by developing the notion of Taylor polynomials of maps in a Cartesian differential category and then show how comparing Taylor polynomials of maps induces an ultrapseudometric on the homsets. We say that a Cartesian differential category is Taylor if maps are entirely determined by their Taylor polynomials. The main results of this paper are that in a Taylor Cartesian differential category, the induced ultrapseudometrics are ultrametrics and that for every map $f$, its Taylor series converges to $f$ with respect to this ultrametric. This framework recaptures both Taylor series expansion in differential calculus via analytic methods and in categorical models of the differential $\\lambda$-calculus (or Differential Linear Logic) via infinite sums.","sentences":["Cartesian differential categories provide a categorical framework for multivariable differential calculus and also the categorical semantics of the differential $\\lambda$-calculus.","Taylor series expansion is an important concept for both differential calculus and the differential $\\lambda$-calculus.","In differential calculus, a function is equal to its Taylor series if its sequence of Taylor polynomials converges to the function in the analytic sense.","On the other hand, for the differential $\\lambda$-calculus, one works in a setting with an appropriate notion of algebraic infinite sums to formalize Taylor series expansion.","In this paper, we provide a formal theory of Taylor series in an arbitrary Cartesian differential category without the need for converging limits or infinite sums.","We begin by developing the notion of Taylor polynomials of maps in a Cartesian differential category and then show how comparing Taylor polynomials of maps induces an ultrapseudometric on the homsets.","We say that a Cartesian differential category is Taylor if maps are entirely determined by their Taylor polynomials.","The main results of this paper are that in a Taylor Cartesian differential category, the induced ultrapseudometrics are ultrametrics and that for every map $f$, its Taylor series converges to $f$ with respect to this ultrametric.","This framework recaptures both Taylor series expansion in differential calculus via analytic methods and in categorical models of the differential $\\lambda$-calculus (or Differential Linear Logic) via infinite sums."],"url":"http://arxiv.org/abs/2405.19474v1","category":"math.CT"}
{"created":"2024-05-29 19:24:54","title":"Analysis of the Fractional Relativistic Polytropic Gas Sphere","abstract":"Many stellar configurations, including white dwarfs, neutron stars, black holes, supermassive stars, and star clusters, rely on relativistic effects. The Tolman-Oppenheimer-Volkoff (TOV) equation of the polytropic gas sphere is ultimately a hydrostatic equilibrium equation developed from the general relativity framework. In the modified Rieman Liouville (mRL) frame, we formulate the fractional TOV (FTOV) equations and introduce an analytical solution. Using power series expansions to solve the fractional TOV equations yields a limited physical range to the convergent power series solution. Therefore, the two techniques of Euler-Abel transformation and Pade approximation have been combined to improve the convergence of the obtained series solutions. For all possible values of the relativistic parameters (\\sigma), we calculated twenty fractional gas models for the polytropic indexes n=0, 0.5, 1, 1.5, 2. Investigating the impacts of fractional and relativistic parameters on the models revealed fascinating phenomena; the two effects for n=0.5 are that the sphere's volume and mass decrease with increasing \\sigma and the fractional parameter (\\alpha). For n=1, the volume decreases when \\sigma=0.1 and then increases when \\sigma=0.2 and 0.3. The volume of the sphere reduces as both \\sigma and \\alpha increase for n=1.5 and n=2. We calculated the maximum mass and the corresponding minimum radius of the white dwarfs modeled with polytropic index n=3 and several fractional and relativistic parameter values. We obtained a mass limit for the white dwarfs somewhat near the Chandrasekhar limit for the integer models with small relativistic parameters (\\alpha=1, \\sigma=0.001). The situation is altered by lowering the fractional parameter; the mass limit increases to Mlimit=1.63348 M at \\alpha=0.95 and \\sigma=0.001.","sentences":["Many stellar configurations, including white dwarfs, neutron stars, black holes, supermassive stars, and star clusters, rely on relativistic effects.","The Tolman-Oppenheimer-Volkoff (TOV) equation of the polytropic gas sphere is ultimately a hydrostatic equilibrium equation developed from the general relativity framework.","In the modified Rieman Liouville (mRL) frame, we formulate the fractional TOV (FTOV) equations and introduce an analytical solution.","Using power series expansions to solve the fractional TOV equations yields a limited physical range to the convergent power series solution.","Therefore, the two techniques of Euler-Abel transformation and Pade approximation have been combined to improve the convergence of the obtained series solutions.","For all possible values of the relativistic parameters (\\sigma), we calculated twenty fractional gas models for the polytropic indexes n=0, 0.5, 1, 1.5, 2.","Investigating the impacts of fractional and relativistic parameters on the models revealed fascinating phenomena; the two effects for n=0.5 are that the sphere's volume and mass decrease with increasing \\sigma and the fractional parameter (\\alpha).","For n=1, the volume decreases when \\sigma=0.1 and then increases when \\sigma=0.2 and 0.3.","The volume of the sphere reduces as both \\sigma and \\alpha increase for n=1.5 and n=2.","We calculated the maximum mass and the corresponding minimum radius of the white dwarfs modeled with polytropic index n=3 and several fractional and relativistic parameter values.","We obtained a mass limit for the white dwarfs somewhat near the Chandrasekhar limit for the integer models with small relativistic parameters (\\alpha=1, \\sigma=0.001).","The situation is altered by lowering the fractional parameter; the mass limit increases to Mlimit=1.63348 M at \\alpha=0.95 and \\sigma=0.001."],"url":"http://arxiv.org/abs/2405.19467v1","category":"gr-qc"}
{"created":"2024-05-29 19:21:49","title":"Critical Learning Periods: Leveraging Early Training Dynamics for Efficient Data Pruning","abstract":"Neural Machine Translation models are extremely data and compute-hungry. However, not all data points contribute equally to model training and generalization. Data pruning to remove the low-value data points has the benefit of drastically reducing the compute budget without significant drop in model performance. In this paper, we propose a new data pruning technique: Checkpoints Across Time (CAT), that leverages early model training dynamics to identify the most relevant data points for model performance. We benchmark CAT against several data pruning techniques including COMET-QE, LASER and LaBSE. We find that CAT outperforms the benchmarks on Indo-European languages on multiple test sets. When applied to English-German, English-French and English-Swahili translation tasks, CAT achieves comparable performance to using the full dataset, while pruning up to 50% of training data. We inspect the data points that CAT selects and find that it tends to favour longer sentences and sentences with unique or rare words.","sentences":["Neural Machine Translation models are extremely data and compute-hungry.","However, not all data points contribute equally to model training and generalization.","Data pruning to remove the low-value data points has the benefit of drastically reducing the compute budget without significant drop in model performance.","In this paper, we propose a new data pruning technique: Checkpoints Across Time (CAT), that leverages early model training dynamics to identify the most relevant data points for model performance.","We benchmark CAT against several data pruning techniques including COMET-QE, LASER and LaBSE.","We find that CAT outperforms the benchmarks on Indo-European languages on multiple test sets.","When applied to English-German, English-French and English-Swahili translation tasks, CAT achieves comparable performance to using the full dataset, while pruning up to 50% of training data.","We inspect the data points that CAT selects and find that it tends to favour longer sentences and sentences with unique or rare words."],"url":"http://arxiv.org/abs/2405.19462v1","category":"cs.CL"}
{"created":"2024-05-29 19:16:14","title":"Evaluating Micro Parsons Problems as Exam Questions","abstract":"Parsons problems are a type of programming activity that present learners with blocks of existing code and requiring them to arrange those blocks to form a program rather than write the code from scratch. Micro Parsons problems extend this concept by having students assemble segments of code to form a single line of code rather than an entire program. Recent investigations into micro Parsons problems have primarily focused on supporting learners leaving open the question of micro Parsons efficacy as an exam item and how students perceive it when preparing for exams.   To fill this gap, we included a variety of micro Parsons problems on four exams in an introductory programming course taught in Python. We use Item Response Theory to investigate the difficulty of the micro Parsons problems as well as the ability of the questions to differentiate between high and low ability students. We then compare these results to results for related questions where students are asked to write a single line of code from scratch. Finally, we conduct a thematic analysis of the survey responses to investigate how students' perceptions of micro Parsons both when practicing for exams and as they appear on exams.","sentences":["Parsons problems are a type of programming activity that present learners with blocks of existing code and requiring them to arrange those blocks to form a program rather than write the code from scratch.","Micro Parsons problems extend this concept by having students assemble segments of code to form a single line of code rather than an entire program.","Recent investigations into micro Parsons problems have primarily focused on supporting learners leaving open the question of micro Parsons efficacy as an exam item and how students perceive it when preparing for exams.   ","To fill this gap, we included a variety of micro Parsons problems on four exams in an introductory programming course taught in Python.","We use Item Response Theory to investigate the difficulty of the micro Parsons problems as well as the ability of the questions to differentiate between high and low ability students.","We then compare these results to results for related questions where students are asked to write a single line of code from scratch.","Finally, we conduct a thematic analysis of the survey responses to investigate how students' perceptions of micro Parsons both when practicing for exams and as they appear on exams."],"url":"http://arxiv.org/abs/2405.19460v1","category":"cs.HC"}
{"created":"2024-05-29 19:05:11","title":"Deep Grokking: Would Deep Neural Networks Generalize Better?","abstract":"Recent research on the grokking phenomenon has illuminated the intricacies of neural networks' training dynamics and their generalization behaviors. Grokking refers to a sharp rise of the network's generalization accuracy on the test set, which occurs long after an extended overfitting phase, during which the network perfectly fits the training set. While the existing research primarily focus on shallow networks such as 2-layer MLP and 1-layer Transformer, we explore grokking on deep networks (e.g. 12-layer MLP). We empirically replicate the phenomenon and find that deep neural networks can be more susceptible to grokking than its shallower counterparts. Meanwhile, we observe an intriguing multi-stage generalization phenomenon when increase the depth of the MLP model where the test accuracy exhibits a secondary surge, which is scarcely seen on shallow models. We further uncover compelling correspondences between the decreasing of feature ranks and the phase transition from overfitting to the generalization stage during grokking. Additionally, we find that the multi-stage generalization phenomenon often aligns with a double-descent pattern in feature ranks. These observations suggest that internal feature rank could serve as a more promising indicator of the model's generalization behavior compared to the weight-norm. We believe our work is the first one to dive into grokking in deep neural networks, and investigate the relationship of feature rank and generalization performance.","sentences":["Recent research on the grokking phenomenon has illuminated the intricacies of neural networks' training dynamics and their generalization behaviors.","Grokking refers to a sharp rise of the network's generalization accuracy on the test set, which occurs long after an extended overfitting phase, during which the network perfectly fits the training set.","While the existing research primarily focus on shallow networks such as 2-layer MLP and 1-layer Transformer, we explore grokking on deep networks (e.g. 12-layer MLP).","We empirically replicate the phenomenon and find that deep neural networks can be more susceptible to grokking than its shallower counterparts.","Meanwhile, we observe an intriguing multi-stage generalization phenomenon when increase the depth of the MLP model where the test accuracy exhibits a secondary surge, which is scarcely seen on shallow models.","We further uncover compelling correspondences between the decreasing of feature ranks and the phase transition from overfitting to the generalization stage during grokking.","Additionally, we find that the multi-stage generalization phenomenon often aligns with a double-descent pattern in feature ranks.","These observations suggest that internal feature rank could serve as a more promising indicator of the model's generalization behavior compared to the weight-norm.","We believe our work is the first one to dive into grokking in deep neural networks, and investigate the relationship of feature rank and generalization performance."],"url":"http://arxiv.org/abs/2405.19454v1","category":"cs.LG"}
{"created":"2024-05-29 18:46:54","title":"Hall-like behaviour of higher rank Chern-Simons theory of fractons","abstract":"Fracton phases of matter constitute an interesting point of contact between condensed matter and high-energy physics. The limited mobility property of fracton quasiparticles finds applications in many different contexts, including quantum information, spin liquids, elasticity, hydrodynamics, gravity and holography. In this paper we adopt a field theoretical approach to investigate the three dimensional action of a rank-2 symmetric tensor field invariant under the covariant fracton symmetry. The theory appears as a non-topological higher rank generalization of the ordinary Chern-Simons model, depending only on the traceless part of the tensor gauge field. After defining a field strength, a rank-2 traceless ``electric'' field and a ``magnetic'' vector field are identified, in analogy with the standard Chern-Simons ones. Once matter is introduced, a Hall-like behaviour with fractonic features emerges. In particular, our model shows a Hall-like dipole current, together with a vectorial ``flux-attachment'' relation for dipoles. This gives a possible starting point for a fracton - vortex duality. A gauge-fixing term is then introduced, from which propagators are computed and the counting of the degrees of freedom is performed. Finally, the energy-momentum tensor is shown to be conserved and the integrated energy density is proved to be zero, which reminds the topological nature of the standard Chern-Simons model.","sentences":["Fracton phases of matter constitute an interesting point of contact between condensed matter and high-energy physics.","The limited mobility property of fracton quasiparticles finds applications in many different contexts, including quantum information, spin liquids, elasticity, hydrodynamics, gravity and holography.","In this paper we adopt a field theoretical approach to investigate the three dimensional action of a rank-2 symmetric tensor field invariant under the covariant fracton symmetry.","The theory appears as a non-topological higher rank generalization of the ordinary Chern-Simons model, depending only on the traceless part of the tensor gauge field.","After defining a field strength, a rank-2 traceless ``electric'' field and a ``magnetic'' vector field are identified, in analogy with the standard Chern-Simons ones.","Once matter is introduced, a Hall-like behaviour with fractonic features emerges.","In particular, our model shows a Hall-like dipole current, together with a vectorial ``flux-attachment'' relation for dipoles.","This gives a possible starting point for a fracton - vortex duality.","A gauge-fixing term is then introduced, from which propagators are computed and the counting of the degrees of freedom is performed.","Finally, the energy-momentum tensor is shown to be conserved and the integrated energy density is proved to be zero, which reminds the topological nature of the standard Chern-Simons model."],"url":"http://arxiv.org/abs/2405.19446v1","category":"hep-th"}
{"created":"2024-05-29 18:46:22","title":"Shock Wave Formation in Radiative Plasmas","abstract":"The temporal evolution of weak shocks in radiative media is theoretically investigated in this work. The structure of radiative shocks has traditionally been studied in a stationary framework. Their systematic classification is complex because layers of optically thin and thick regions alternate to form a radiatively-driven precursor and a temperature-relaxation layer, between which the hydrodynamic shock is embedded. In this work, we analyze the formation of weak shocks when two radiative plasmas with different pressures are put in contact. Applying a reductive perturbative method yields a Burgers-type equation that governs the temporal evolution of the perturbed variables including the radiation field. The conditions upon which optically thin and thick solutions exist have been derived and expressed as a function of the shock strength and Boltzmann number. Below a certain Boltzmann number threshold, weak shocks always become optically thick asymptotically in time, while thin solutions appear as transitory structures. The existence of an optically thin regime is related to the presence of an overdense layer in the compressed material. Scaling laws for the characteristic formation time and shock width are provided for each regime. The theoretical analysis is supported by FLASH simulations, and a comprehensive test-case has been designed to benchmark radiative hydrodynamic codes.","sentences":["The temporal evolution of weak shocks in radiative media is theoretically investigated in this work.","The structure of radiative shocks has traditionally been studied in a stationary framework.","Their systematic classification is complex because layers of optically thin and thick regions alternate to form a radiatively-driven precursor and a temperature-relaxation layer, between which the hydrodynamic shock is embedded.","In this work, we analyze the formation of weak shocks when two radiative plasmas with different pressures are put in contact.","Applying a reductive perturbative method yields a Burgers-type equation that governs the temporal evolution of the perturbed variables including the radiation field.","The conditions upon which optically thin and thick solutions exist have been derived and expressed as a function of the shock strength and Boltzmann number.","Below a certain Boltzmann number threshold, weak shocks always become optically thick asymptotically in time, while thin solutions appear as transitory structures.","The existence of an optically thin regime is related to the presence of an overdense layer in the compressed material.","Scaling laws for the characteristic formation time and shock width are provided for each regime.","The theoretical analysis is supported by FLASH simulations, and a comprehensive test-case has been designed to benchmark radiative hydrodynamic codes."],"url":"http://arxiv.org/abs/2405.19445v1","category":"physics.plasm-ph"}
{"created":"2024-05-29 18:36:59","title":"On the Convergence of Multi-objective Optimization under Generalized Smoothness","abstract":"Multi-objective optimization (MOO) is receiving more attention in various fields such as multi-task learning. Recent works provide some effective algorithms with theoretical analysis but they are limited by the standard $L$-smooth or bounded-gradient assumptions, which are typically unsatisfactory for neural networks, such as recurrent neural networks (RNNs) and transformers. In this paper, we study a more general and realistic class of $\\ell$-smooth loss functions, where $\\ell$ is a general non-decreasing function of gradient norm. We develop two novel single-loop algorithms for $\\ell$-smooth MOO problems, Generalized Smooth Multi-objective Gradient descent (GSMGrad) and its stochastic variant, Stochastic Generalized Smooth Multi-objective Gradient descent (SGSMGrad), which approximate the conflict-avoidant (CA) direction that maximizes the minimum improvement among objectives. We provide a comprehensive convergence analysis of both algorithms and show that they converge to an $\\epsilon$-accurate Pareto stationary point with a guaranteed $\\epsilon$-level average CA distance (i.e., the gap between the updating direction and the CA direction) over all iterations, where totally $\\mathcal{O}(\\epsilon^{-2})$ and $\\mathcal{O}(\\epsilon^{-4})$ samples are needed for deterministic and stochastic settings, respectively. Our algorithms can also guarantee a tighter $\\epsilon$-level CA distance in each iteration using more samples. Moreover, we propose a practical variant of GSMGrad named GSMGrad-FA using only constant-level time and space, while achieving the same performance guarantee as GSMGrad. Our experiments validate our theory and demonstrate the effectiveness of the proposed methods.","sentences":["Multi-objective optimization (MOO) is receiving more attention in various fields such as multi-task learning.","Recent works provide some effective algorithms with theoretical analysis but they are limited by the standard $L$-smooth or bounded-gradient assumptions, which are typically unsatisfactory for neural networks, such as recurrent neural networks (RNNs) and transformers.","In this paper, we study a more general and realistic class of $\\ell$-smooth loss functions, where $\\ell$ is a general non-decreasing function of gradient norm.","We develop two novel single-loop algorithms for $\\ell$-smooth MOO problems, Generalized Smooth Multi-objective Gradient descent (GSMGrad) and its stochastic variant, Stochastic Generalized Smooth Multi-objective Gradient descent (SGSMGrad), which approximate the conflict-avoidant (CA) direction that maximizes the minimum improvement among objectives.","We provide a comprehensive convergence analysis of both algorithms and show that they converge to an $\\epsilon$-accurate Pareto stationary point with a guaranteed $\\epsilon$-level average CA distance (i.e., the gap between the updating direction and the CA direction) over all iterations, where totally $\\mathcal{O}(\\epsilon^{-2})$ and $\\mathcal{O}(\\epsilon^{-4})$ samples are needed for deterministic and stochastic settings, respectively.","Our algorithms can also guarantee a tighter $\\epsilon$-level CA distance in each iteration using more samples.","Moreover, we propose a practical variant of GSMGrad named GSMGrad-FA using only constant-level time and space, while achieving the same performance guarantee as GSMGrad.","Our experiments validate our theory and demonstrate the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2405.19440v1","category":"cs.LG"}
{"created":"2024-05-29 18:33:16","title":"Unveiling baryon charge carriers through charge stopping in isobar collisions","abstract":"Utilizing a comprehensive (3+1)D relativistic hydrodynamic framework with multiple conserved charge currents and charge-dependent Lattice-QCD-based equation of state, we study the baryon and electric charge number deposition at mid rapidity in isobar Ru+Ru and Zr+Zr collisions at the center of mass energy $\\sqrt{s_\\mathrm{NN}}=200$ GeV. Comparing our predictions with upcoming experimental data from the Relativistic Heavy Ion Collider will shed light on the existence of baryon junctions.","sentences":["Utilizing a comprehensive (3+1)D relativistic hydrodynamic framework with multiple conserved charge currents and charge-dependent Lattice-QCD-based equation of state, we study the baryon and electric charge number deposition at mid rapidity in isobar Ru+Ru and Zr+Zr collisions at the center of mass energy $\\sqrt{s_\\mathrm{NN}}=200$ GeV. Comparing our predictions with upcoming experimental data from the Relativistic Heavy Ion Collider will shed light on the existence of baryon junctions."],"url":"http://arxiv.org/abs/2405.19439v1","category":"nucl-th"}
{"created":"2024-05-29 18:23:04","title":"Quantitative hydrodynamics for a generalized contact model","abstract":"We derive a quantitative version of the hydrodynamic limit for an interacting particle system inspired by integrate-and-fire neuron models. More precisely, we show that the $L^2$-speed of convergence of the empirical density of states in a generalized contact process defined over a $d$-dimensional torus of size $n$ is of the optimal order $\\mathcal O(n^{d/2})$. In addition, we show that the typical fluctuations around the aforementioned hydrodynamic limit are Gaussian, and governed by a inhomogeneous stochastic linear equation.","sentences":["We derive a quantitative version of the hydrodynamic limit for an interacting particle system inspired by integrate-and-fire neuron models.","More precisely, we show that the $L^2$-speed of convergence of the empirical density of states in a generalized contact process defined over a $d$-dimensional torus of size $n$ is of the optimal order $\\mathcal O(n^{d/2})$.","In addition, we show that the typical fluctuations around the aforementioned hydrodynamic limit are Gaussian, and governed by a inhomogeneous stochastic linear equation."],"url":"http://arxiv.org/abs/2405.19437v1","category":"math.PR"}
{"created":"2024-05-29 18:16:32","title":"Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals","abstract":"While current automated essay scoring (AES) methods show high agreement with human raters, their scoring mechanisms are not fully explored. Our proposed method, using counterfactual intervention assisted by Large Language Models (LLMs), reveals that when scoring essays, BERT-like models primarily focus on sentence-level features, while LLMs are attuned to conventions, language complexity, as well as organization, indicating a more comprehensive alignment with scoring rubrics. Moreover, LLMs can discern counterfactual interventions during feedback. Our approach improves understanding of neural AES methods and can also apply to other domains seeking transparency in model-driven decisions. The codes and data will be released at GitHub.","sentences":["While current automated essay scoring (AES) methods show high agreement with human raters, their scoring mechanisms are not fully explored.","Our proposed method, using counterfactual intervention assisted by Large Language Models (LLMs), reveals that when scoring essays, BERT-like models primarily focus on sentence-level features, while LLMs are attuned to conventions, language complexity, as well as organization, indicating a more comprehensive alignment with scoring rubrics.","Moreover, LLMs can discern counterfactual interventions during feedback.","Our approach improves understanding of neural AES methods and can also apply to other domains seeking transparency in model-driven decisions.","The codes and data will be released at GitHub."],"url":"http://arxiv.org/abs/2405.19433v1","category":"cs.CL"}
{"created":"2024-05-29 18:14:15","title":"Acceleration from a clustering environment","abstract":"We study the effects of correlations in a random environment on a random walker. The dependence of its asymptotic speed on the correlations is a nonperturbative effect as it is not captured by a homogeneous version of the same environment. For a slowly cooling environment, the buildup of correlations modifies the walker's speed and, by so, realizes acceleration. We remark on the possible relevance in the discussion of cosmic acceleration as traditionally started from the Friedmann equations, which, from a statistical mechanical point of view, would amount to a mean-field approximation. Our environment is much simpler though, with transition rates sampled from the one-dimensional Ising model and allowing exact results and detailed velocity characteristics.","sentences":["We study the effects of correlations in a random environment on a random walker.","The dependence of its asymptotic speed on the correlations is a nonperturbative effect as it is not captured by a homogeneous version of the same environment.","For a slowly cooling environment, the buildup of correlations modifies the walker's speed and, by so, realizes acceleration.","We remark on the possible relevance in the discussion of cosmic acceleration as traditionally started from the Friedmann equations, which, from a statistical mechanical point of view, would amount to a mean-field approximation.","Our environment is much simpler though, with transition rates sampled from the one-dimensional Ising model and allowing exact results and detailed velocity characteristics."],"url":"http://arxiv.org/abs/2405.19432v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-29 18:00:01","title":"Ground state phases of the two-dimension electron gas with a unified variational approach","abstract":"The two-dimensional electron gas (2DEG) is a fundamental model, which is drawing increasing interest because of recent advances in experimental and theoretical studies of 2D materials. Current understanding of the ground state of the 2DEG relies on quantum Monte Carlo calculations, based on variational comparisons of different ansatze for different phases. We use a single variational ansatz, a general backflow-type wave function using a message-passing neural quantum state architecture, for a unified description across the entire density range. The variational optimization consistently leads to lower ground-state energies than previous best results. Transition into a Wigner crystal (WC) phase occurs automatically at rs = 37 +/- 1, a density lower than currently believed. Between the liquid and WC phases, the same ansatz and variational search strongly suggest the existence of intermediate states in a broad range of densities, with enhanced short-range nematic spin correlations.","sentences":["The two-dimensional electron gas (2DEG) is a fundamental model, which is drawing increasing interest because of recent advances in experimental and theoretical studies of 2D materials.","Current understanding of the ground state of the 2DEG relies on quantum Monte Carlo calculations, based on variational comparisons of different ansatze for different phases.","We use a single variational ansatz, a general backflow-type wave function using a message-passing neural quantum state architecture, for a unified description across the entire density range.","The variational optimization consistently leads to lower ground-state energies than previous best results.","Transition into a Wigner crystal (WC) phase occurs automatically at rs = 37 +/- 1, a density lower than currently believed.","Between the liquid and WC phases, the same ansatz and variational search strongly suggest the existence of intermediate states in a broad range of densities, with enhanced short-range nematic spin correlations."],"url":"http://arxiv.org/abs/2405.19397v1","category":"cond-mat.str-el"}
{"created":"2024-05-29 18:00:01","title":"Neural Scaling Laws From Large-N Field Theory: Solvable Model Beyond the Ridgeless Limit","abstract":"Many machine learning models based on neural networks exhibit scaling laws: their performance scales as power laws with respect to the sizes of the model and training data set. We use large-N field theory methods to solve a model recently proposed by Maloney, Roberts and Sully which provides a simplified setting to study neural scaling laws. Our solution extends the result in this latter paper to general nonzero values of the ridge parameter, which are essential to regularize the behavior of the model. In addition to obtaining new and more precise scaling laws, we also uncover a duality transformation at the diagrams level which explains the symmetry between model and training data set sizes. The same duality underlies recent efforts to design neural networks to simulate quantum field theories.","sentences":["Many machine learning models based on neural networks exhibit scaling laws: their performance scales as power laws with respect to the sizes of the model and training data set.","We use large-N field theory methods to solve a model recently proposed by Maloney, Roberts and Sully which provides a simplified setting to study neural scaling laws.","Our solution extends the result in this latter paper to general nonzero values of the ridge parameter, which are essential to regularize the behavior of the model.","In addition to obtaining new and more precise scaling laws, we also uncover a duality transformation at the diagrams level which explains the symmetry between model and training data set sizes.","The same duality underlies recent efforts to design neural networks to simulate quantum field theories."],"url":"http://arxiv.org/abs/2405.19398v1","category":"hep-th"}
{"created":"2024-05-29 18:00:01","title":"Harmonic $1$-forms on real loci of Calabi-Yau manifolds","abstract":"We numerically study whether there exist nowhere vanishing harmonic $1$-forms on the real locus of some carefully constructed examples of Calabi-Yau manifolds, which would then give rise to potentially new examples of $G_2$-manifolds and an explicit description of their metrics. We do this in two steps: first, we use a neural network to compute an approximate Calabi-Yau metric on each manifold. Second, we use another neural network to compute an approximately harmonic $1$-form with respect to the approximate metric, and then inspect the found solution. On two manifolds existence of a nowhere vanishing harmonic $1$-form can be ruled out using differential geometry. The real locus of a third manifold is diffeomorphic to $S^1 \\times S^2$, and our numerics suggest that when the Calabi-Yau metric is close to a singular limit, then it admits a nowhere vanishing harmonic $1$-form. We explain how such an approximate solution could potentially be used in a numerically verified proof for the fact that our example manifold must admit a nowhere vanishing harmonic $1$-form.","sentences":["We numerically study whether there exist nowhere vanishing harmonic $1$-forms on the real locus of some carefully constructed examples of Calabi-Yau manifolds, which would then give rise to potentially new examples of $G_2$-manifolds and an explicit description of their metrics.","We do this in two steps: first, we use a neural network to compute an approximate Calabi-Yau metric on each manifold.","Second, we use another neural network to compute an approximately harmonic $1$-form with respect to the approximate metric, and then inspect the found solution.","On two manifolds existence of a nowhere vanishing harmonic $1$-form can be ruled out using differential geometry.","The real locus of a third manifold is diffeomorphic to $S^1 \\times S^2$, and our numerics suggest that when the Calabi-Yau metric is close to a singular limit, then it admits a nowhere vanishing harmonic $1$-form.","We explain how such an approximate solution could potentially be used in a numerically verified proof for the fact that our example manifold must admit a nowhere vanishing harmonic $1$-form."],"url":"http://arxiv.org/abs/2405.19402v1","category":"math.DG"}
{"created":"2024-05-29 17:02:23","title":"Thermodynamically Informed Multimodal Learning of High-Dimensional Free Energy Models in Molecular Coarse Graining","abstract":"We present a differentiable formalism for learning free energies that is capable of capturing arbitrarily complex model dependencies on coarse-grained coordinates and finite-temperature response to variation of general system parameters. This is done by endowing models with explicit dependence on temperature and parameters and by exploiting exact differential thermodynamic relationships between the free energy, ensemble averages, and response properties. Formally, we derive an approach for learning high-dimensional cumulant generating functions using statistical estimates of their derivatives, which are observable cumulants of the underlying random variable. The proposed formalism opens ways to resolve several outstanding challenges in bottom-up molecular coarse graining dealing with multiple minima and state dependence. This is realized by using additional differential relationships in the loss function to significantly improve the learning of free energies, while exactly preserving the Boltzmann distribution governing the corresponding fine-grain all-atom system. As an example, we go beyond the standard force-matching procedure to demonstrate how leveraging the thermodynamic relationship between free energy and values of ensemble averaged all-atom potential energy improves the learning efficiency and accuracy of the free energy model. The result is significantly better sampling statistics of structural distribution functions. The theoretical framework presented here is demonstrated via implementations in both kernel-based and neural network machine learning regression methods and opens new ways to train accurate machine learning models for studying thermodynamic and response properties of complex molecular systems.","sentences":["We present a differentiable formalism for learning free energies that is capable of capturing arbitrarily complex model dependencies on coarse-grained coordinates and finite-temperature response to variation of general system parameters.","This is done by endowing models with explicit dependence on temperature and parameters and by exploiting exact differential thermodynamic relationships between the free energy, ensemble averages, and response properties.","Formally, we derive an approach for learning high-dimensional cumulant generating functions using statistical estimates of their derivatives, which are observable cumulants of the underlying random variable.","The proposed formalism opens ways to resolve several outstanding challenges in bottom-up molecular coarse graining dealing with multiple minima and state dependence.","This is realized by using additional differential relationships in the loss function to significantly improve the learning of free energies, while exactly preserving the Boltzmann distribution governing the corresponding fine-grain all-atom system.","As an example, we go beyond the standard force-matching procedure to demonstrate how leveraging the thermodynamic relationship between free energy and values of ensemble averaged all-atom potential energy improves the learning efficiency and accuracy of the free energy model.","The result is significantly better sampling statistics of structural distribution functions.","The theoretical framework presented here is demonstrated via implementations in both kernel-based and neural network machine learning regression methods and opens new ways to train accurate machine learning models for studying thermodynamic and response properties of complex molecular systems."],"url":"http://arxiv.org/abs/2405.19386v1","category":"physics.comp-ph"}
{"created":"2024-05-30 17:46:23","title":"Scaling White-Box Transformers for Vision","abstract":"CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address. Specifically, we propose CRATE-$\\alpha$, featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE. Through extensive experiments, we demonstrate that CRATE-$\\alpha$ can effectively scale with larger model sizes and datasets. For example, our CRATE-$\\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our CRATE-$\\alpha$-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE-$\\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images. The project page is https://rayjryang.github.io/CRATE-alpha/.","sentences":["CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability.","Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address.","Specifically, we propose CRATE-$\\alpha$, featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE.","Through extensive experiments, we demonstrate that CRATE-$\\alpha$ can effectively scale with larger model sizes and datasets.","For example, our CRATE-$\\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%.","Meanwhile, when scaling further, our CRATE-$\\alpha$-L obtains an ImageNet classification accuracy of 85.1%.","More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE-$\\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images.","The project page is https://rayjryang.github.io/CRATE-alpha/."],"url":"http://arxiv.org/abs/2405.20299v1","category":"cs.CV"}
{"created":"2024-05-30 16:41:39","title":"BEAST DB: Grand-Canonical Database of Electrocatalyst Properties","abstract":"We present BEAST DB, an open-source database comprised of ab initio electrochemical data computed using grand-canonical density functional theory in implicit solvent at consistent calculation parameters. The database contains over 20,000 surface calculations and covers a broad set of heterogeneous catalyst materials and electrochemical reactions. Calculations were performed at self-consistent fixed potential as well as constant charge to facilitate comparisons to the computational hydrogen electrode. This article presents common use cases of the database to rationalize trends in catalyst activity, screen catalyst material spaces, understand elementary mechanistic steps, analyze electronic structure, and train machine learning models to predict higher fidelity properties. Users can interact graphically with the database by querying for individual calculations to gain granular understanding of reaction steps or by querying for an entire reaction pathway on a given material using an interactive reaction pathway tool. BEAST DB will be periodically updated, with planned future updates to include advanced electronic structure data, surface speciation studies, and greater reaction coverage.","sentences":["We present BEAST DB, an open-source database comprised of ab initio electrochemical data computed using grand-canonical density functional theory in implicit solvent at consistent calculation parameters.","The database contains over 20,000 surface calculations and covers a broad set of heterogeneous catalyst materials and electrochemical reactions.","Calculations were performed at self-consistent fixed potential as well as constant charge to facilitate comparisons to the computational hydrogen electrode.","This article presents common use cases of the database to rationalize trends in catalyst activity, screen catalyst material spaces, understand elementary mechanistic steps, analyze electronic structure, and train machine learning models to predict higher fidelity properties.","Users can interact graphically with the database by querying for individual calculations to gain granular understanding of reaction steps or by querying for an entire reaction pathway on a given material using an interactive reaction pathway tool.","BEAST DB will be periodically updated, with planned future updates to include advanced electronic structure data, surface speciation studies, and greater reaction coverage."],"url":"http://arxiv.org/abs/2405.20239v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 16:18:29","title":"Data-efficient fine-tuning of foundational models for first-principles quality sublimation enthalpies","abstract":"Calculating sublimation enthalpies of molecular crystal polymorphs is relevant to a wide range of technological applications. However, predicting these quantities at first-principles accuracy -- even with the aid of machine learning potentials -- is a challenge that requires sub-kJ/mol accuracy in the potential energy surface and finite-temperature sampling. We present an accurate and data-efficient protocol based on fine-tuning of the foundational MACE-MP-0 model and showcase its capabilities on sublimation enthalpies and physical properties of ice polymorphs. Our approach requires only a few tens of training structures to achieve sub-kJ/mol accuracy in the sublimation enthalpies and sub 1 % error in densities for polymorphs at finite temperature and pressure. Exploiting this data efficiency, we explore simulations of hexagonal ice at the random phase approximation level of theory at experimental temperatures and pressures, calculating its physical properties, like pair correlation function and density, with good agreement with experiments. Our approach provides a way forward for predicting the stability of molecular crystals at finite thermodynamic conditions with the accuracy of correlated electronic structure theory.","sentences":["Calculating sublimation enthalpies of molecular crystal polymorphs is relevant to a wide range of technological applications.","However, predicting these quantities at first-principles accuracy -- even with the aid of machine learning potentials -- is a challenge that requires sub-kJ/mol accuracy in the potential energy surface and finite-temperature sampling.","We present an accurate and data-efficient protocol based on fine-tuning of the foundational MACE-MP-0 model and showcase its capabilities on sublimation enthalpies and physical properties of ice polymorphs.","Our approach requires only a few tens of training structures to achieve sub-kJ/mol accuracy in the sublimation enthalpies and sub 1 % error in densities for polymorphs at finite temperature and pressure.","Exploiting this data efficiency, we explore simulations of hexagonal ice at the random phase approximation level of theory at experimental temperatures and pressures, calculating its physical properties, like pair correlation function and density, with good agreement with experiments.","Our approach provides a way forward for predicting the stability of molecular crystals at finite thermodynamic conditions with the accuracy of correlated electronic structure theory."],"url":"http://arxiv.org/abs/2405.20217v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 13:38:52","title":"Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads","abstract":"Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.","sentences":["Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control.","By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences.","Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts.","To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head.","We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO).","Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone."],"url":"http://arxiv.org/abs/2405.20053v1","category":"cs.CL"}
{"created":"2024-05-30 09:45:18","title":"Federated Learning with Multi-resolution Model Broadcast","abstract":"In federated learning, a server must periodically broadcast a model to the agents. We propose to use multi-resolution coding and modulation (also known as non-uniform modulation) for this purpose. In the simplest instance, broadcast transmission is used, whereby all agents are targeted with one and the same transmission (typically without any particular favored beam direction), which is coded using multi-resolution coding/modulation. This enables high-SNR agents, with high path gains to the server, to receive a more accurate model than the low-SNR agents do, without consuming more downlink resources. As one implementation, we use transmission with a non-uniform 8-PSK constellation, where a high-SNR receiver (agent) can separate all 8 constellation points (hence receive 3 bits) whereas a low-SNR receiver can only separate 4 points (hence receive 2 bits). By encoding the least significant information in the third bit, the high-SNR receivers can obtain the model with higher accuracy, while the low-SNR receiver can still obtain the model although with reduced accuracy, thereby facilitating at least some basic participation of the low-SNR receiver. We show the effectiveness of our proposed scheme via experimentation using federated learning with the MNIST data-set.","sentences":["In federated learning, a server must periodically broadcast a model to the agents.","We propose to use multi-resolution coding and modulation (also known as non-uniform modulation) for this purpose.","In the simplest instance, broadcast transmission is used, whereby all agents are targeted with one and the same transmission (typically without any particular favored beam direction), which is coded using multi-resolution coding/modulation.","This enables high-SNR agents, with high path gains to the server, to receive a more accurate model than the low-SNR agents do, without consuming more downlink resources.","As one implementation, we use transmission with a non-uniform 8-PSK constellation, where a high-SNR receiver (agent) can separate all 8 constellation points (hence receive 3 bits) whereas a low-SNR receiver can only separate 4 points (hence receive 2 bits).","By encoding the least significant information in the third bit, the high-SNR receivers can obtain the model with higher accuracy, while the low-SNR receiver can still obtain the model although with reduced accuracy, thereby facilitating at least some basic participation of the low-SNR receiver.","We show the effectiveness of our proposed scheme via experimentation using federated learning with the MNIST data-set."],"url":"http://arxiv.org/abs/2405.19886v1","category":"cs.NI"}
{"created":"2024-05-30 09:03:23","title":"RTGen: Generating Region-Text Pairs for Open-Vocabulary Object Detection","abstract":"Open-vocabulary object detection (OVD) requires solid modeling of the region-semantic relationship, which could be learned from massive region-text pairs. However, such data is limited in practice due to significant annotation costs. In this work, we propose RTGen to generate scalable open-vocabulary region-text pairs and demonstrate its capability to boost the performance of open-vocabulary object detection. RTGen includes both text-to-region and region-to-text generation processes on scalable image-caption data. The text-to-region generation is powered by image inpainting, directed by our proposed scene-aware inpainting guider for overall layout harmony. For region-to-text generation, we perform multiple region-level image captioning with various prompts and select the best matching text according to CLIP similarity. To facilitate detection training on region-text pairs, we also introduce a localization-aware region-text contrastive loss that learns object proposals tailored with different localization qualities. Extensive experiments demonstrate that our RTGen can serve as a scalable, semantically rich, and effective source for open-vocabulary object detection and continue to improve the model performance when more data is utilized, delivering superior performance compared to the existing state-of-the-art methods.","sentences":["Open-vocabulary object detection (OVD) requires solid modeling of the region-semantic relationship, which could be learned from massive region-text pairs.","However, such data is limited in practice due to significant annotation costs.","In this work, we propose RTGen to generate scalable open-vocabulary region-text pairs and demonstrate its capability to boost the performance of open-vocabulary object detection.","RTGen includes both text-to-region and region-to-text generation processes on scalable image-caption data.","The text-to-region generation is powered by image inpainting, directed by our proposed scene-aware inpainting guider for overall layout harmony.","For region-to-text generation, we perform multiple region-level image captioning with various prompts and select the best matching text according to CLIP similarity.","To facilitate detection training on region-text pairs, we also introduce a localization-aware region-text contrastive loss that learns object proposals tailored with different localization qualities.","Extensive experiments demonstrate that our RTGen can serve as a scalable, semantically rich, and effective source for open-vocabulary object detection and continue to improve the model performance when more data is utilized, delivering superior performance compared to the existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.19854v1","category":"cs.CV"}
{"created":"2024-05-30 08:54:57","title":"Modelling and Forecasting Energy Market Volatility Using GARCH and Machine Learning Approach","abstract":"This paper presents a comparative analysis of univariate and multivariate GARCH-family models and machine learning algorithms in modeling and forecasting the volatility of major energy commodities: crude oil, gasoline, heating oil, and natural gas. It uses a comprehensive dataset incorporating financial, macroeconomic, and environmental variables to assess predictive performance and discusses volatility persistence and transmission across these commodities. Aspects of volatility persistence and transmission, traditionally examined by GARCH-class models, are jointly explored using the SHAP (Shapley Additive exPlanations) method. The findings reveal that machine learning models demonstrate superior out-of-sample forecasting performance compared to traditional GARCH models. Machine learning models tend to underpredict, while GARCH models tend to overpredict energy market volatility, suggesting a hybrid use of both types of models. There is volatility transmission from crude oil to the gasoline and heating oil markets. The volatility transmission in the natural gas market is less prevalent.","sentences":["This paper presents a comparative analysis of univariate and multivariate GARCH-family models and machine learning algorithms in modeling and forecasting the volatility of major energy commodities: crude oil, gasoline, heating oil, and natural gas.","It uses a comprehensive dataset incorporating financial, macroeconomic, and environmental variables to assess predictive performance and discusses volatility persistence and transmission across these commodities.","Aspects of volatility persistence and transmission, traditionally examined by GARCH-class models, are jointly explored using the SHAP (Shapley Additive exPlanations) method.","The findings reveal that machine learning models demonstrate superior out-of-sample forecasting performance compared to traditional GARCH models.","Machine learning models tend to underpredict, while GARCH models tend to overpredict energy market volatility, suggesting a hybrid use of both types of models.","There is volatility transmission from crude oil to the gasoline and heating oil markets.","The volatility transmission in the natural gas market is less prevalent."],"url":"http://arxiv.org/abs/2405.19849v1","category":"econ.EM"}
{"created":"2024-05-30 08:16:22","title":"Preference Alignment with Flow Matching","abstract":"We present Preference Flow Matching (PFM), a new framework for preference-based reinforcement learning (PbRL) that streamlines the integration of preferences into an arbitrary class of pre-trained models. Existing PbRL methods require fine-tuning pre-trained models, which presents challenges such as scalability, inefficiency, and the need for model modifications, especially with black-box APIs like GPT-4. In contrast, PFM utilizes flow matching techniques to directly learn from preference data, thereby reducing the dependency on extensive fine-tuning of pre-trained models. By leveraging flow-based models, PFM transforms less preferred data into preferred outcomes, and effectively aligns model outputs with human preferences without relying on explicit or implicit reward function estimation, thus avoiding common issues like overfitting in reward models. We provide theoretical insights that support our method's alignment with standard PbRL objectives. Experimental results indicate the practical effectiveness of our method, offering a new direction in aligning a pre-trained model to preference.","sentences":["We present Preference Flow Matching (PFM), a new framework for preference-based reinforcement learning (PbRL) that streamlines the integration of preferences into an arbitrary class of pre-trained models.","Existing PbRL methods require fine-tuning pre-trained models, which presents challenges such as scalability, inefficiency, and the need for model modifications, especially with black-box APIs like GPT-4.","In contrast, PFM utilizes flow matching techniques to directly learn from preference data, thereby reducing the dependency on extensive fine-tuning of pre-trained models.","By leveraging flow-based models, PFM transforms less preferred data into preferred outcomes, and effectively aligns model outputs with human preferences without relying on explicit or implicit reward function estimation, thus avoiding common issues like overfitting in reward models.","We provide theoretical insights that support our method's alignment with standard PbRL objectives.","Experimental results indicate the practical effectiveness of our method, offering a new direction in aligning a pre-trained model to preference."],"url":"http://arxiv.org/abs/2405.19806v1","category":"cs.LG"}
{"created":"2024-05-30 07:58:01","title":"Estimating before Debiasing: A Bayesian Approach to Detaching Prior Bias in Federated Semi-Supervised Learning","abstract":"Federated Semi-Supervised Learning (FSSL) leverages both labeled and unlabeled data on clients to collaboratively train a model.In FSSL, the heterogeneous data can introduce prediction bias into the model, causing the model's prediction to skew towards some certain classes. Existing FSSL methods primarily tackle this issue by enhancing consistency in model parameters or outputs. However, as the models themselves are biased, merely constraining their consistency is not sufficient to alleviate prediction bias. In this paper, we explore this bias from a Bayesian perspective and demonstrate that it principally originates from label prior bias within the training data. Building upon this insight, we propose a debiasing method for FSSL named FedDB. FedDB utilizes the Average Prediction Probability of Unlabeled Data (APP-U) to approximate the biased prior.During local training, FedDB employs APP-U to refine pseudo-labeling through Bayes' theorem, thereby significantly reducing the label prior bias. Concurrently, during the model aggregation, FedDB uses APP-U from participating clients to formulate unbiased aggregate weights, thereby effectively diminishing bias in the global model. Experimental results show that FedDB can surpass existing FSSL methods. The code is available at https://github.com/GuogangZhu/FedDB.","sentences":["Federated Semi-Supervised Learning (FSSL) leverages both labeled and unlabeled data on clients to collaboratively train a model.","In FSSL, the heterogeneous data can introduce prediction bias into the model, causing the model's prediction to skew towards some certain classes.","Existing FSSL methods primarily tackle this issue by enhancing consistency in model parameters or outputs.","However, as the models themselves are biased, merely constraining their consistency is not sufficient to alleviate prediction bias.","In this paper, we explore this bias from a Bayesian perspective and demonstrate that it principally originates from label prior bias within the training data.","Building upon this insight, we propose a debiasing method for FSSL named FedDB.","FedDB utilizes the Average Prediction Probability of Unlabeled Data (APP-U) to approximate the biased prior.","During local training, FedDB employs APP-U to refine pseudo-labeling through Bayes' theorem, thereby significantly reducing the label prior bias.","Concurrently, during the model aggregation, FedDB uses APP-U from participating clients to formulate unbiased aggregate weights, thereby effectively diminishing bias in the global model.","Experimental results show that FedDB can surpass existing FSSL methods.","The code is available at https://github.com/GuogangZhu/FedDB."],"url":"http://arxiv.org/abs/2405.19789v1","category":"cs.LG"}
{"created":"2024-05-30 07:38:58","title":"VQA Training Sets are Self-play Environments for Generating Few-shot Pools","abstract":"Large-language models and large-vision models are increasingly capable of solving compositional reasoning tasks, as measured by breakthroughs in visual-question answering benchmarks. However, state-of-the-art solutions often involve careful construction of large pre-training and fine-tuning datasets, which can be expensive. The use of external tools, whether other ML models, search engines, or APIs, can significantly improve performance by breaking down high-level reasoning questions into sub-questions that are answerable by individual tools, but this approach has similar dataset construction costs to teach fine-tuned models how to use the available tools. We propose a technique in which existing training sets can be directly used for constructing computational environments with task metrics as rewards. This enables a model to autonomously teach itself to use itself or another model as a tool. By doing so, we augment training sets by integrating external signals. The proposed method starts with zero-shot prompts and iteratively refines them by selecting few-shot examples that maximize the task metric on the training set. Our experiments showcase how Gemini learns how to use itself, or another smaller and specialized model such as ScreenAI, to iteratively improve performance on training sets. Our approach successfully generalizes and improves upon zeroshot performance on charts, infographics, and document visual question-answering datasets","sentences":["Large-language models and large-vision models are increasingly capable of solving compositional reasoning tasks, as measured by breakthroughs in visual-question answering benchmarks.","However, state-of-the-art solutions often involve careful construction of large pre-training and fine-tuning datasets, which can be expensive.","The use of external tools, whether other ML models, search engines, or APIs, can significantly improve performance by breaking down high-level reasoning questions into sub-questions that are answerable by individual tools, but this approach has similar dataset construction costs to teach fine-tuned models how to use the available tools.","We propose a technique in which existing training sets can be directly used for constructing computational environments with task metrics as rewards.","This enables a model to autonomously teach itself to use itself or another model as a tool.","By doing so, we augment training sets by integrating external signals.","The proposed method starts with zero-shot prompts and iteratively refines them by selecting few-shot examples that maximize the task metric on the training set.","Our experiments showcase how Gemini learns how to use itself, or another smaller and specialized model such as ScreenAI, to iteratively improve performance on training sets.","Our approach successfully generalizes and improves upon zeroshot performance on charts, infographics, and document visual question-answering datasets"],"url":"http://arxiv.org/abs/2405.19773v1","category":"cs.CV"}
{"created":"2024-05-30 07:36:03","title":"Data Service Maximization in Integrated Terrestrial-Non-Terrestrial 6G Networks: A Deep Reinforcement Learning Approach","abstract":"Integrating terrestrial and non-terrestrial networks has emerged as a promising paradigm to fulfill the constantly growing demand for connectivity, low transmission delay, and quality of services (QoS). This integration brings together the strengths of terrestrial and non-terrestrial networks, such as the reliability of terrestrial networks, broad coverage, and service continuity of non-terrestrial networks like low earth orbit (LEO) satellites. In this work, we study a data service maximization problem in an integrated terrestrial-non-terrestrial network (I-TNT) where the ground base stations (GBSs) and LEO satellites cooperatively serve the coexisting aerial users (AUs) and ground users (GUs). Then, by considering the spectrum scarcity, interference, and QoS requirements of the users, we jointly optimize the user association, AUE's trajectory, and power allocation. To tackle the formulated mixed-integer non-convex problem, we disintegrate it into two subproblems: 1) user association problem and 2) trajectory and power allocation problem. Since the user association problem is a binary integer programming problem, we use the standard convex optimization method to solve it. Meanwhile, the trajectory and power allocation problem is solved by the deep deterministic policy gradient (DDPG) method to cope with the problem's non-convexity and dynamic network environments. Then, the two subproblems are alternately solved by the proposed iterative algorithm. By comparing with the baselines in the existing literature, extensive simulations are conducted to evaluate the performance of the proposed framework.","sentences":["Integrating terrestrial and non-terrestrial networks has emerged as a promising paradigm to fulfill the constantly growing demand for connectivity, low transmission delay, and quality of services (QoS).","This integration brings together the strengths of terrestrial and non-terrestrial networks, such as the reliability of terrestrial networks, broad coverage, and service continuity of non-terrestrial networks like low earth orbit (LEO) satellites.","In this work, we study a data service maximization problem in an integrated terrestrial-non-terrestrial network (I-TNT) where the ground base stations (GBSs) and LEO satellites cooperatively serve the coexisting aerial users (AUs) and ground users (GUs).","Then, by considering the spectrum scarcity, interference, and QoS requirements of the users, we jointly optimize the user association, AUE's trajectory, and power allocation.","To tackle the formulated mixed-integer non-convex problem, we disintegrate it into two subproblems: 1) user association problem and 2) trajectory and power allocation problem.","Since the user association problem is a binary integer programming problem, we use the standard convex optimization method to solve it.","Meanwhile, the trajectory and power allocation problem is solved by the deep deterministic policy gradient (DDPG) method to cope with the problem's non-convexity and dynamic network environments.","Then, the two subproblems are alternately solved by the proposed iterative algorithm.","By comparing with the baselines in the existing literature, extensive simulations are conducted to evaluate the performance of the proposed framework."],"url":"http://arxiv.org/abs/2405.19771v1","category":"cs.NI"}
{"created":"2024-05-30 07:19:31","title":"Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding","abstract":"Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the objective mismatch issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks. To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding. Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared with RLHF models, the improvement averages at 0.69%. These results reveal the effectiveness of our method for LLMs in NLU tasks. Code and data available at: https://github.com/MagiaSN/ACL2024_RLLR.","sentences":["Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities.","However, RLHF encounters numerous challenges, including the objective mismatch issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks.","To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks.","By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding.","Experiments conducted on five diverse foundation models across eight tasks showcase promising results.","In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54%.","Compared with RLHF models, the improvement averages at 0.69%.","These results reveal the effectiveness of our method for LLMs in NLU tasks.","Code and data available at: https://github.com/MagiaSN/ACL2024_RLLR."],"url":"http://arxiv.org/abs/2405.19763v1","category":"cs.CL"}
{"created":"2024-05-30 07:08:40","title":"InterPreT: Interactive Predicate Learning from Language Feedback for Generalizable Task Planning","abstract":"Learning abstract state representations and knowledge is crucial for long-horizon robot planning. We present InterPreT, an LLM-powered framework for robots to learn symbolic predicates from language feedback of human non-experts during embodied interaction. The learned predicates provide relational abstractions of the environment state, facilitating the learning of symbolic operators that capture action preconditions and effects. By compiling the learned predicates and operators into a PDDL domain on-the-fly, InterPreT allows effective planning toward arbitrary in-domain goals using a PDDL planner. In both simulated and real-world robot manipulation domains, we demonstrate that InterPreT reliably uncovers the key predicates and operators governing the environment dynamics. Although learned from simple training tasks, these predicates and operators exhibit strong generalization to novel tasks with significantly higher complexity. In the most challenging generalization setting, InterPreT attains success rates of 73% in simulation and 40% in the real world, substantially outperforming baseline methods.","sentences":["Learning abstract state representations and knowledge is crucial for long-horizon robot planning.","We present InterPreT, an LLM-powered framework for robots to learn symbolic predicates from language feedback of human non-experts during embodied interaction.","The learned predicates provide relational abstractions of the environment state, facilitating the learning of symbolic operators that capture action preconditions and effects.","By compiling the learned predicates and operators into a PDDL domain on-the-fly, InterPreT allows effective planning toward arbitrary in-domain goals using a PDDL planner.","In both simulated and real-world robot manipulation domains, we demonstrate that InterPreT reliably uncovers the key predicates and operators governing the environment dynamics.","Although learned from simple training tasks, these predicates and operators exhibit strong generalization to novel tasks with significantly higher complexity.","In the most challenging generalization setting, InterPreT attains success rates of 73% in simulation and 40% in the real world, substantially outperforming baseline methods."],"url":"http://arxiv.org/abs/2405.19758v1","category":"cs.RO"}
{"created":"2024-05-30 06:56:48","title":"Understanding Memory-Regret Trade-Off for Streaming Stochastic Multi-Armed Bandits","abstract":"We study the stochastic multi-armed bandit problem in the $P$-pass streaming model. In this problem, the $n$ arms are present in a stream and at most $m<n$ arms and their statistics can be stored in the memory. We give a complete characterization of the optimal regret in terms of $m, n$ and $P$. Specifically, we design an algorithm with $\\tilde O\\left((n-m)^{1+\\frac{2^{P}-2}{2^{P+1}-1}} n^{\\frac{2-2^{P+1}}{2^{P+1}-1}} T^{\\frac{2^P}{2^{P+1}-1}}\\right)$ regret and complement it with an $\\tilde \\Omega\\left((n-m)^{1+\\frac{2^{P}-2}{2^{P+1}-1}} n^{\\frac{2-2^{P+1}}{2^{P+1}-1}} T^{\\frac{2^P}{2^{P+1}-1}}\\right)$ lower bound when the number of rounds $T$ is sufficiently large. Our results are tight up to a logarithmic factor in $n$ and $P$.","sentences":["We study the stochastic multi-armed bandit problem in the $P$-pass streaming model.","In this problem, the $n$ arms are present in a stream and at most $m<n$ arms and their statistics can be stored in the memory.","We give a complete characterization of the optimal regret in terms of $m, n$ and $P$. Specifically, we design an algorithm with $\\tilde O\\left((n-m)^{1+\\frac{2^{P}-2}{2^{P+1}-1}} n^{\\frac{2-2^{P+1}}{2^{P+1}-1}} T^{\\frac{2^P}{2^{P+1}-1}}\\right)$ regret and complement it with an $\\tilde \\Omega\\left((n-m)^{1+\\frac{2^{P}-2}{2^{P+1}-1}} n^{\\frac{2-2^{P+1}}{2^{P+1}-1}} T^{\\frac{2^P}{2^{P+1}-1}}\\right)$ lower bound when the number of rounds $T$ is sufficiently large.","Our results are tight up to a logarithmic factor in $n$ and $P$."],"url":"http://arxiv.org/abs/2405.19752v1","category":"cs.LG"}
{"created":"2024-05-30 06:50:28","title":"Understanding and mitigating difficulties in posterior predictive evaluation","abstract":"Predictive posterior densities (PPDs) are of interest in approximate Bayesian inference. Typically, these are estimated by simple Monte Carlo (MC) averages using samples from the approximate posterior. We observe that the signal-to-noise ratio (SNR) of such estimators can be extremely low. An analysis for exact inference reveals SNR decays exponentially as there is an increase in (a) the mismatch between training and test data, (b) the dimensionality of the latent space, or (c) the size of the test data relative to the training data. Further analysis extends these results to approximate inference. To remedy the low SNR problem, we propose replacing simple MC sampling with importance sampling using a proposal distribution optimized at test time on a variational proxy for the SNR and demonstrate that this yields greatly improved estimates.","sentences":["Predictive posterior densities (PPDs) are of interest in approximate Bayesian inference.","Typically, these are estimated by simple Monte Carlo (MC) averages using samples from the approximate posterior.","We observe that the signal-to-noise ratio (SNR) of such estimators can be extremely low.","An analysis for exact inference reveals SNR decays exponentially as there is an increase in (a) the mismatch between training and test data, (b) the dimensionality of the latent space, or (c) the size of the test data relative to the training data.","Further analysis extends these results to approximate inference.","To remedy the low SNR problem, we propose replacing simple MC sampling with importance sampling using a proposal distribution optimized at test time on a variational proxy for the SNR and demonstrate that this yields greatly improved estimates."],"url":"http://arxiv.org/abs/2405.19747v1","category":"cs.LG"}
{"created":"2024-05-30 06:49:59","title":"DenseSeg: Joint Learning for Semantic Segmentation and Landmark Detection Using Dense Image-to-Shape Representation","abstract":"Purpose: Semantic segmentation and landmark detection are fundamental tasks of medical image processing, facilitating further analysis of anatomical objects. Although deep learning-based pixel-wise classification has set a new-state-of-the-art for segmentation, it falls short in landmark detection, a strength of shape-based approaches.   Methods: In this work, we propose a dense image-to-shape representation that enables the joint learning of landmarks and semantic segmentation by employing a fully convolutional architecture. Our method intuitively allows the extraction of arbitrary landmarks due to its representation of anatomical correspondences. We benchmark our method against the state-of-the-art for semantic segmentation (nnUNet), a shape-based approach employing geometric deep learning and a CNN-based method for landmark detection.   Results: We evaluate our method on two medical dataset: one common benchmark featuring the lungs, heart, and clavicle from thorax X-rays, and another with 17 different bones in the paediatric wrist. While our method is on pair with the landmark detection baseline in the thorax setting (error in mm of $2.6\\pm0.9$ vs $2.7\\pm0.9$), it substantially surpassed it in the more complex wrist setting ($1.1\\pm0.6$ vs $1.9\\pm0.5$).   Conclusion: We demonstrate that dense geometric shape representation is beneficial for challenging landmark detection tasks and outperforms previous state-of-the-art using heatmap regression. While it does not require explicit training on the landmarks themselves, allowing for the addition of new landmarks without necessitating retraining.}","sentences":["Purpose: Semantic segmentation and landmark detection are fundamental tasks of medical image processing, facilitating further analysis of anatomical objects.","Although deep learning-based pixel-wise classification has set a new-state-of-the-art for segmentation, it falls short in landmark detection, a strength of shape-based approaches.   ","Methods: In this work, we propose a dense image-to-shape representation that enables the joint learning of landmarks and semantic segmentation by employing a fully convolutional architecture.","Our method intuitively allows the extraction of arbitrary landmarks due to its representation of anatomical correspondences.","We benchmark our method against the state-of-the-art for semantic segmentation (nnUNet), a shape-based approach employing geometric deep learning and a CNN-based method for landmark detection.   ","Results:","We evaluate our method on two medical dataset: one common benchmark featuring the lungs, heart, and clavicle from thorax X-rays, and another with 17 different bones in the paediatric wrist.","While our method is on pair with the landmark detection baseline in the thorax setting (error in mm of $2.6\\pm0.9$ vs $2.7\\pm0.9$), it substantially surpassed it in the more complex wrist setting ($1.1\\pm0.6$ vs $1.9\\pm0.5$).   ","Conclusion: We demonstrate that dense geometric shape representation is beneficial for challenging landmark detection tasks and outperforms previous state-of-the-art using heatmap regression.","While it does not require explicit training on the landmarks themselves, allowing for the addition of new landmarks without necessitating retraining.}"],"url":"http://arxiv.org/abs/2405.19746v1","category":"cs.CV"}
{"created":"2024-05-30 06:39:18","title":"Teaching the apparent motion of Sun and stars across four European countries","abstract":"In the context of the European Erasmus+ project Teaching ASTronomy at the Educational level (TASTE), we investigated the extent to which a learning module at school and a set of activities during a planetarium visit help students to gain insight in the Apparent Motion of the Sun and Stars. Therefore, we have set up a two treatment study with a pretest posttest design. In the four participating countries (Belgium, Germany, Greece and Italy), secondary school students studied the concept of the celestial globe at school using newly designed learning materials. By using a latent class analysis, we identified different classes of student answers on the AMoSS test. We show how students evolve from one class to another between pretest and posttest. Overall the results of the pretest and posttest show that a good understanding of the different aspects of the apparent motion of celestial bodies is difficult to achieve.","sentences":["In the context of the European Erasmus+ project Teaching ASTronomy at the Educational level (TASTE), we investigated the extent to which a learning module at school and a set of activities during a planetarium visit help students to gain insight in the Apparent Motion of the Sun and Stars.","Therefore, we have set up a two treatment study with a pretest posttest design.","In the four participating countries (Belgium, Germany, Greece and Italy), secondary school students studied the concept of the celestial globe at school using newly designed learning materials.","By using a latent class analysis, we identified different classes of student answers on the AMoSS test.","We show how students evolve from one class to another between pretest and posttest.","Overall the results of the pretest and posttest show that a good understanding of the different aspects of the apparent motion of celestial bodies is difficult to achieve."],"url":"http://arxiv.org/abs/2405.19741v1","category":"physics.ed-ph"}
{"created":"2024-05-30 06:15:08","title":"Quantum Visual Feature Encoding Revisited","abstract":"Although quantum machine learning has been introduced for a while, its applications in computer vision are still limited. This paper, therefore, revisits the quantum visual encoding strategies, the initial step in quantum machine learning. Investigating the root cause, we uncover that the existing quantum encoding design fails to ensure information preservation of the visual features after the encoding process, thus complicating the learning process of the quantum machine learning models. In particular, the problem, termed \"Quantum Information Gap\" (QIG), leads to a gap of information between classical and corresponding quantum features. We provide theoretical proof and practical demonstrations of that found and underscore the significance of QIG, as it directly impacts the performance of quantum machine learning algorithms. To tackle this challenge, we introduce a simple but efficient new loss function named Quantum Information Preserving (QIP) to minimize this gap, resulting in enhanced performance of quantum machine learning algorithms. Extensive experiments validate the effectiveness of our approach, showcasing superior performance compared to current methodologies and consistently achieving state-of-the-art results in quantum modeling.","sentences":["Although quantum machine learning has been introduced for a while, its applications in computer vision are still limited.","This paper, therefore, revisits the quantum visual encoding strategies, the initial step in quantum machine learning.","Investigating the root cause, we uncover that the existing quantum encoding design fails to ensure information preservation of the visual features after the encoding process, thus complicating the learning process of the quantum machine learning models.","In particular, the problem, termed \"Quantum Information Gap\" (QIG), leads to a gap of information between classical and corresponding quantum features.","We provide theoretical proof and practical demonstrations of that found and underscore the significance of QIG, as it directly impacts the performance of quantum machine learning algorithms.","To tackle this challenge, we introduce a simple but efficient new loss function named Quantum Information Preserving (QIP) to minimize this gap, resulting in enhanced performance of quantum machine learning algorithms.","Extensive experiments validate the effectiveness of our approach, showcasing superior performance compared to current methodologies and consistently achieving state-of-the-art results in quantum modeling."],"url":"http://arxiv.org/abs/2405.19725v1","category":"quant-ph"}
{"created":"2024-05-30 05:29:12","title":"Enhancing Sufficient Dimension Reduction via Hellinger Correlation","abstract":"In this work, we develop a new theory and method for sufficient dimension reduction (SDR) in single-index models, where SDR is a sub-field of supervised dimension reduction based on conditional independence. Our work is primarily motivated by the recent introduction of the Hellinger correlation as a dependency measure. Utilizing this measure, we develop a method capable of effectively detecting the dimension reduction subspace, complete with theoretical justification. Through extensive numerical experiments, we demonstrate that our proposed method significantly enhances and outperforms existing SDR methods. This improvement is largely attributed to our proposed method's deeper understanding of data dependencies and the refinement of existing SDR techniques.","sentences":["In this work, we develop a new theory and method for sufficient dimension reduction (SDR) in single-index models, where SDR is a sub-field of supervised dimension reduction based on conditional independence.","Our work is primarily motivated by the recent introduction of the Hellinger correlation as a dependency measure.","Utilizing this measure, we develop a method capable of effectively detecting the dimension reduction subspace, complete with theoretical justification.","Through extensive numerical experiments, we demonstrate that our proposed method significantly enhances and outperforms existing SDR methods.","This improvement is largely attributed to our proposed method's deeper understanding of data dependencies and the refinement of existing SDR techniques."],"url":"http://arxiv.org/abs/2405.19704v1","category":"stat.ML"}
{"created":"2024-05-30 05:27:46","title":"Towards a Better Evaluation of Out-of-Domain Generalization","abstract":"The objective of Domain Generalization (DG) is to devise algorithms and models capable of achieving high performance on previously unseen test distributions. In the pursuit of this objective, average measure has been employed as the prevalent measure for evaluating models and comparing algorithms in the existing DG studies. Despite its significance, a comprehensive exploration of the average measure has been lacking and its suitability in approximating the true domain generalization performance has been questionable. In this study, we carefully investigate the limitations inherent in the average measure and propose worst+gap measure as a robust alternative. We establish theoretical grounds of the proposed measure by deriving two theorems starting from two different assumptions. We conduct extensive experimental investigations to compare the proposed worst+gap measure with the conventional average measure. Given the indispensable need to access the true DG performance for studying measures, we modify five existing datasets to come up with SR-CMNIST, C-Cats&Dogs, L-CIFAR10, PACS-corrupted, and VLCS-corrupted datasets. The experiment results unveil an inferior performance of the average measure in approximating the true DG performance and confirm the robustness of the theoretically supported worst+gap measure.","sentences":["The objective of Domain Generalization (DG) is to devise algorithms and models capable of achieving high performance on previously unseen test distributions.","In the pursuit of this objective, average measure has been employed as the prevalent measure for evaluating models and comparing algorithms in the existing DG studies.","Despite its significance, a comprehensive exploration of the average measure has been lacking and its suitability in approximating the true domain generalization performance has been questionable.","In this study, we carefully investigate the limitations inherent in the average measure and propose worst+gap measure as a robust alternative.","We establish theoretical grounds of the proposed measure by deriving two theorems starting from two different assumptions.","We conduct extensive experimental investigations to compare the proposed worst+gap measure with the conventional average measure.","Given the indispensable need to access the true DG performance for studying measures, we modify five existing datasets to come up with SR-CMNIST, C-Cats&Dogs, L-CIFAR10, PACS-corrupted, and VLCS-corrupted datasets.","The experiment results unveil an inferior performance of the average measure in approximating the true DG performance and confirm the robustness of the theoretically supported worst+gap measure."],"url":"http://arxiv.org/abs/2405.19703v1","category":"cs.LG"}
{"created":"2024-05-30 05:04:46","title":"Designing Prompt Analytics Dashboards to Analyze Student-ChatGPT Interactions in EFL Writing","abstract":"While ChatGPT has significantly impacted education by offering personalized resources for students, its integration into educational settings poses unprecedented risks, such as inaccuracies and biases in AI-generated content, plagiarism and over-reliance on AI, and privacy and security issues. To help teachers address such risks, we conducted a two-phase iterative design process that comprises surveys, interviews, and prototype demonstration involving six EFL (English as a Foreign Language) teachers, who integrated ChatGPT into semester-long English essay writing classes. Based on the needs identified during the initial survey and interviews, we developed a prototype of Prompt Analytics Dashboard (PAD) that integrates the essay editing history and chat logs between students and ChatGPT. Teacher's feedback on the prototype informs additional features and unmet needs for designing future PAD, which helps them (1) analyze contextual analysis of student behaviors, (2) design an overall learning loop, and (3) develop their teaching skills.","sentences":["While ChatGPT has significantly impacted education by offering personalized resources for students, its integration into educational settings poses unprecedented risks, such as inaccuracies and biases in AI-generated content, plagiarism and over-reliance on AI, and privacy and security issues.","To help teachers address such risks, we conducted a two-phase iterative design process that comprises surveys, interviews, and prototype demonstration involving six EFL (English as a Foreign Language) teachers, who integrated ChatGPT into semester-long English essay writing classes.","Based on the needs identified during the initial survey and interviews, we developed a prototype of Prompt Analytics Dashboard (PAD) that integrates the essay editing history and chat logs between students and ChatGPT.","Teacher's feedback on the prototype informs additional features and unmet needs for designing future PAD, which helps them (1) analyze contextual analysis of student behaviors, (2) design an overall learning loop, and (3) develop their teaching skills."],"url":"http://arxiv.org/abs/2405.19691v1","category":"cs.HC"}
{"created":"2024-05-30 17:28:23","title":"Arbitrary State Preparation via Quantum Walks","abstract":"Continuous-time quantum walks (CTQWs) on dynamic graphs, referred to as dynamic CTQWs, are a recently introduced universal model of computation that offers a new paradigm in which to envision quantum algorithms. In this work we develop a mapping from dynamic CTQWs to the gate model of computation in the form of an algorithm to convert arbitrary single edge walks and single self loop walks, which are the fundamental building blocks of dynamic CTQWs, to their circuit model counterparts. We use this mapping to introduce an arbitrary quantum state preparation framework based on dynamic CTQWs. Our approach utilizes global information about the target state, relates state preparation to finding the optimal path in a graph, and leads to optimizations in the reduction of controls that are not as obvious in other approaches. Interestingly, classical optimization problems such as the minimal hitting set, minimum spanning tree, and shortest Hamiltonian path problems arise in our framework. We test our methods against uniformly controlled rotations methods, used by Qiskit, and find ours requires fewer CX gates when the target state has a polynomial number of non-zero amplitudes.","sentences":["Continuous-time quantum walks (CTQWs) on dynamic graphs, referred to as dynamic CTQWs, are a recently introduced universal model of computation that offers a new paradigm in which to envision quantum algorithms.","In this work we develop a mapping from dynamic CTQWs to the gate model of computation in the form of an algorithm to convert arbitrary single edge walks and single self loop walks, which are the fundamental building blocks of dynamic CTQWs, to their circuit model counterparts.","We use this mapping to introduce an arbitrary quantum state preparation framework based on dynamic CTQWs.","Our approach utilizes global information about the target state, relates state preparation to finding the optimal path in a graph, and leads to optimizations in the reduction of controls that are not as obvious in other approaches.","Interestingly, classical optimization problems such as the minimal hitting set, minimum spanning tree, and shortest Hamiltonian path problems arise in our framework.","We test our methods against uniformly controlled rotations methods, used by Qiskit, and find ours requires fewer CX gates when the target state has a polynomial number of non-zero amplitudes."],"url":"http://arxiv.org/abs/2405.20273v1","category":"quant-ph"}
{"created":"2024-05-30 15:55:04","title":"SPARE: Symmetrized Point-to-Plane Distance for Robust Non-Rigid Registration","abstract":"Existing optimization-based methods for non-rigid registration typically minimize an alignment error metric based on the point-to-point or point-to-plane distance between corresponding point pairs on the source surface and target surface. However, these metrics can result in slow convergence or a loss of detail. In this paper, we propose SPARE, a novel formulation that utilizes a symmetrized point-to-plane distance for robust non-rigid registration. The symmetrized point-to-plane distance relies on both the positions and normals of the corresponding points, resulting in a more accurate approximation of the underlying geometry and can achieve higher accuracy than existing methods. To solve this optimization problem efficiently, we propose an alternating minimization solver using a majorization-minimization strategy. Moreover, for effective initialization of the solver, we incorporate a deformation graph-based coarse alignment that improves registration quality and efficiency. Extensive experiments show that the proposed method greatly improves the accuracy of non-rigid registration problems and maintains relatively high solution efficiency. The code is publicly available at https://github.com/yaoyx689/spare.","sentences":["Existing optimization-based methods for non-rigid registration typically minimize an alignment error metric based on the point-to-point or point-to-plane distance between corresponding point pairs on the source surface and target surface.","However, these metrics can result in slow convergence or a loss of detail.","In this paper, we propose SPARE, a novel formulation that utilizes a symmetrized point-to-plane distance for robust non-rigid registration.","The symmetrized point-to-plane distance relies on both the positions and normals of the corresponding points, resulting in a more accurate approximation of the underlying geometry and can achieve higher accuracy than existing methods.","To solve this optimization problem efficiently, we propose an alternating minimization solver using a majorization-minimization strategy.","Moreover, for effective initialization of the solver, we incorporate a deformation graph-based coarse alignment that improves registration quality and efficiency.","Extensive experiments show that the proposed method greatly improves the accuracy of non-rigid registration problems and maintains relatively high solution efficiency.","The code is publicly available at https://github.com/yaoyx689/spare."],"url":"http://arxiv.org/abs/2405.20188v1","category":"cs.CV"}
{"created":"2024-05-30 15:10:30","title":"LinApart: optimizing the univariate partial fraction decomposition","abstract":"We present LinApart, a routine designed for efficiently performing the univariate partial fraction decomposition of large symbolic expressions. Our method is based on an explicit closed formula for the decomposition of rational functions with fully factorized denominators. We provide implementations in both the Wolfram Mathematica and C languages, made available at https://github.com/fekeshazy/LinApart . The routine can provide very significant performance gains over available tools such as the Apart command in Mathematica.","sentences":["We present LinApart, a routine designed for efficiently performing the univariate partial fraction decomposition of large symbolic expressions.","Our method is based on an explicit closed formula for the decomposition of rational functions with fully factorized denominators.","We provide implementations in both the Wolfram Mathematica and C languages, made available at https://github.com/fekeshazy/LinApart .","The routine can provide very significant performance gains over available tools such as the Apart command in Mathematica."],"url":"http://arxiv.org/abs/2405.20130v1","category":"cs.SC"}
{"created":"2024-05-30 14:47:34","title":"Autonomous programmable microscopic electronic lablets optimized with digital control","abstract":"Lablets are autonomous microscopic particles with programmable CMOS electronics that can control electrokinetic phenomena and electrochemical reactions in solution via actuator and sensor microelectrodes. In this paper, we describe the design and fabrication of optimized singulated lablets (CMOS3) with dimensions 140x140x50 micrometers carrying an integrated coplanar encapsulated supercapacitor as a rechargeable power supply. The lablets are designed to allow docking to one another or to a smart surface for interchange of energy, electronic information, and chemicals. The paper focusses on the digital and analog design of the lablets to allow significant programmable functionality in a microscopic footprint, including the control of autonomous actuation and sensing up to the level of being able to support a complete lablet self-reproduction life cycle, although experimentally this remains to be proven. The potential of lablets in autonomous sensing and control and for evolutionary experimentation are discussed.","sentences":["Lablets are autonomous microscopic particles with programmable CMOS electronics that can control electrokinetic phenomena and electrochemical reactions in solution via actuator and sensor microelectrodes.","In this paper, we describe the design and fabrication of optimized singulated lablets (CMOS3) with dimensions 140x140x50 micrometers carrying an integrated coplanar encapsulated supercapacitor as a rechargeable power supply.","The lablets are designed to allow docking to one another or to a smart surface for interchange of energy, electronic information, and chemicals.","The paper focusses on the digital and analog design of the lablets to allow significant programmable functionality in a microscopic footprint, including the control of autonomous actuation and sensing up to the level of being able to support a complete lablet self-reproduction life cycle, although experimentally this remains to be proven.","The potential of lablets in autonomous sensing and control and for evolutionary experimentation are discussed."],"url":"http://arxiv.org/abs/2405.20110v1","category":"cs.RO"}
{"created":"2024-05-30 11:45:47","title":"Influence of deposition parameters on the plasmonic properties of gold nanoantennas fabricated by focused ion beam lithography","abstract":"The behavior of plasmonic antennas is influenced by a variety of factors, including their size, shape, and material. Even minor changes in the deposition parameters during the thin film preparation process may have a significant impact on the dielectric function of the film, and thus on the plasmonic properties of the resulting antenna. In this work, we deposited gold thin films with thicknesses of 20 nm, 30 nm, and 40 nm at various deposition rates using an ion-beam-assisted deposition. We evaluate their morphology and crystallography by atomic force microscopy, X ray diffraction, and transmission electron microscopy. Next, we examined the ease of fabricating plasmonic antennas using focused-ion-beam lithography. Finally, we evaluate their plasmonic properties by electron energy loss spectroscopy measurements of individual antennas. Our results show that the optimal gold thin film for plasmonic antenna fabrication of a thickness of 20 and 30 nm should be deposited at the deposition rate of around 0.1 nm/s. The thicker 40 nm film should be deposited at a higher deposition rate like 0.3 nm/s.","sentences":["The behavior of plasmonic antennas is influenced by a variety of factors, including their size, shape, and material.","Even minor changes in the deposition parameters during the thin film preparation process may have a significant impact on the dielectric function of the film, and thus on the plasmonic properties of the resulting antenna.","In this work, we deposited gold thin films with thicknesses of 20 nm, 30 nm, and 40 nm at various deposition rates using an ion-beam-assisted deposition.","We evaluate their morphology and crystallography by atomic force microscopy, X ray diffraction, and transmission electron microscopy.","Next, we examined the ease of fabricating plasmonic antennas using focused-ion-beam lithography.","Finally, we evaluate their plasmonic properties by electron energy loss spectroscopy measurements of individual antennas.","Our results show that the optimal gold thin film for plasmonic antenna fabrication of a thickness of 20 and 30 nm should be deposited at the deposition rate of around 0.1 nm/s. The thicker 40 nm film should be deposited at a higher deposition rate like 0.3 nm/s."],"url":"http://arxiv.org/abs/2405.19966v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 08:18:45","title":"Study of the behaviour of Nesterov Accelerated Gradient in a non convex setting: the strongly quasar convex case","abstract":"We study the convergence of Nesterov Accelerated Gradient (NAG) minimization algorithm applied to a class of non convex functions called strongly quasar convex functions, which can exhibit highly non convex behaviour. We show that in the case of strongly quasar convex functions, NAG can achieve an accelerated convergence speed at the cost of a lower curvature assumption. We provide a continuous analysis through high resolution ODEs, in which negative friction may appear. Finally, we investigate connections with a weaker class of non convex functions (smooth Polyak-\\L ojasiewicz functions) by characterizing the gap between this class and the one of smooth strongly quasar convex functions.","sentences":["We study the convergence of Nesterov Accelerated Gradient (NAG) minimization algorithm applied to a class of non convex functions called strongly quasar convex functions, which can exhibit highly non convex behaviour.","We show that in the case of strongly quasar convex functions, NAG can achieve an accelerated convergence speed at the cost of a lower curvature assumption.","We provide a continuous analysis through high resolution ODEs, in which negative friction may appear.","Finally, we investigate connections with a weaker class of non convex functions (smooth Polyak-\\L ojasiewicz functions) by characterizing the gap between this class and the one of smooth strongly quasar convex functions."],"url":"http://arxiv.org/abs/2405.19809v1","category":"math.OC"}
{"created":"2024-05-30 08:12:21","title":"Dynamic Factor Analysis of High-dimensional Recurrent Events","abstract":"Recurrent event time data arise in many studies, including biomedicine, public health, marketing, and social media analysis. High-dimensional recurrent event data involving large numbers of event types and observations become prevalent with the advances in information technology. This paper proposes a semiparametric dynamic factor model for the dimension reduction and prediction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.","sentences":["Recurrent event time data arise in many studies, including biomedicine, public health, marketing, and social media analysis.","High-dimensional recurrent event data involving large numbers of event types and observations become prevalent with the advances in information technology.","This paper proposes a semiparametric dynamic factor model for the dimension reduction and prediction of high-dimensional recurrent event data.","The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies.","A nearly rate-optimal smoothing-based estimator is proposed.","An information criterion that consistently selects the number of factors is also developed.","Simulation studies demonstrate the effectiveness of these inference tools.","The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained."],"url":"http://arxiv.org/abs/2405.19803v1","category":"stat.ME"}
{"created":"2024-05-30 04:19:20","title":"Efficient Trajectory Inference in Wasserstein Space Using Consecutive Averaging","abstract":"Capturing data from dynamic processes through cross-sectional measurements is seen in many fields such as computational biology. Trajectory inference deals with the challenge of reconstructing continuous processes from such observations. In this work, we propose methods for B-spline approximation and interpolation of point clouds through consecutive averaging that is instrinsic to the Wasserstein space. Combining subdivision schemes with optimal transport-based geodesic, our methods carry out trajectory inference at a chosen level of precision and smoothness, and can automatically handle scenarios where particles undergo division over time. We rigorously evaluate our method by providing convergence guarantees and testing it on simulated cell data characterized by bifurcations and merges, comparing its performance against state-of-the-art trajectory inference and interpolation methods. The results not only underscore the effectiveness of our method in inferring trajectories, but also highlight the benefit of performing interpolation and approximation that respect the inherent geometric properties of the data.","sentences":["Capturing data from dynamic processes through cross-sectional measurements is seen in many fields such as computational biology.","Trajectory inference deals with the challenge of reconstructing continuous processes from such observations.","In this work, we propose methods for B-spline approximation and interpolation of point clouds through consecutive averaging that is instrinsic to the Wasserstein space.","Combining subdivision schemes with optimal transport-based geodesic, our methods carry out trajectory inference at a chosen level of precision and smoothness, and can automatically handle scenarios where particles undergo division over time.","We rigorously evaluate our method by providing convergence guarantees and testing it on simulated cell data characterized by bifurcations and merges, comparing its performance against state-of-the-art trajectory inference and interpolation methods.","The results not only underscore the effectiveness of our method in inferring trajectories, but also highlight the benefit of performing interpolation and approximation that respect the inherent geometric properties of the data."],"url":"http://arxiv.org/abs/2405.19679v1","category":"cs.LG"}
{"created":"2024-05-30 02:39:56","title":"Creating Language-driven Spatial Variations of Icon Images","abstract":"Editing 2D icon images can require significant manual effort from designers. It involves manipulating multiple geometries while maintaining the logical or physical coherence of the objects depicted in the image. Previous language driven image editing methods can change the texture and geometry of objects in the image but fail at producing spatial variations, i.e. modifying spatial relations between objects while maintaining their identities. We present a language driven editing method that can produce spatial variations of icon images. Our method takes in an icon image along with a user's editing request text prompt and outputs an edited icon image reflecting the user's editing request. Our method is designed based on two key observations: (1) A user's editing requests can be translated by a large language model (LLM), with help from a domain specific language (DSL) library, into to a set of geometrical constraints defining the relationships between segments in an icon image. (2) Optimizing the affine transformations of the segments with respect to these geometrical constraints can produce icon images that fulfill the editing request and preserve overall physical and logical coherence. Quantitative and qualitative results show that our system outperforms multiple baselines, enabling natural editing of icon images.","sentences":["Editing 2D icon images can require significant manual effort from designers.","It involves manipulating multiple geometries while maintaining the logical or physical coherence of the objects depicted in the image.","Previous language driven image editing methods can change the texture and geometry of objects in the image but fail at producing spatial variations, i.e. modifying spatial relations between objects while maintaining their identities.","We present a language driven editing method that can produce spatial variations of icon images.","Our method takes in an icon image along with a user's editing request text prompt and outputs an edited icon image reflecting the user's editing request.","Our method is designed based on two key observations: (1) A user's editing requests can be translated by a large language model (LLM), with help from a domain specific language (DSL) library, into to a set of geometrical constraints defining the relationships between segments in an icon image.","(2) Optimizing the affine transformations of the segments with respect to these geometrical constraints can produce icon images that fulfill the editing request and preserve overall physical and logical coherence.","Quantitative and qualitative results show that our system outperforms multiple baselines, enabling natural editing of icon images."],"url":"http://arxiv.org/abs/2405.19636v1","category":"cs.GR"}
{"created":"2024-05-30 02:08:45","title":"TAMBRIDGE: Bridging Frame-Centered Tracking and 3D Gaussian Splatting for Enhanced SLAM","abstract":"The limited robustness of 3D Gaussian Splatting (3DGS) to motion blur and camera noise, along with its poor real-time performance, restricts its application in robotic SLAM tasks. Upon analysis, the primary causes of these issues are the density of views with motion blur and the cumulative errors in dense pose estimation from calculating losses based on noisy original images and rendering results, which increase the difficulty of 3DGS rendering convergence. Thus, a cutting-edge 3DGS-based SLAM system is introduced, leveraging the efficiency and flexibility of 3DGS to achieve real-time performance while remaining robust against sensor noise, motion blur, and the challenges posed by long-session SLAM. Central to this approach is the Fusion Bridge module, which seamlessly integrates tracking-centered ORB Visual Odometry with mapping-centered online 3DGS. Precise pose initialization is enabled by this module through joint optimization of re-projection and rendering loss, as well as strategic view selection, enhancing rendering convergence in large-scale scenes. Extensive experiments demonstrate state-of-the-art rendering quality and localization accuracy, positioning this system as a promising solution for real-world robotics applications that require stable, near-real-time performance. Our project is available at https://ZeldaFromHeaven.github.io/TAMBRIDGE/","sentences":["The limited robustness of 3D Gaussian Splatting (3DGS) to motion blur and camera noise, along with its poor real-time performance, restricts its application in robotic SLAM tasks.","Upon analysis, the primary causes of these issues are the density of views with motion blur and the cumulative errors in dense pose estimation from calculating losses based on noisy original images and rendering results, which increase the difficulty of 3DGS rendering convergence.","Thus, a cutting-edge 3DGS-based SLAM system is introduced, leveraging the efficiency and flexibility of 3DGS to achieve real-time performance while remaining robust against sensor noise, motion blur, and the challenges posed by long-session SLAM.","Central to this approach is the Fusion Bridge module, which seamlessly integrates tracking-centered ORB Visual Odometry with mapping-centered online 3DGS.","Precise pose initialization is enabled by this module through joint optimization of re-projection and rendering loss, as well as strategic view selection, enhancing rendering convergence in large-scale scenes.","Extensive experiments demonstrate state-of-the-art rendering quality and localization accuracy, positioning this system as a promising solution for real-world robotics applications that require stable, near-real-time performance.","Our project is available at https://ZeldaFromHeaven.github.io/TAMBRIDGE/"],"url":"http://arxiv.org/abs/2405.19614v1","category":"cs.RO"}
{"created":"2024-05-29 23:38:12","title":"Blind Image Restoration via Fast Diffusion Inversion","abstract":"Recently, various methods have been proposed to solve Image Restoration (IR) tasks using a pre-trained diffusion model leading to state-of-the-art performance. However, most of these methods assume that the degradation operator in the IR task is completely known. Furthermore, a common characteristic among these approaches is that they alter the diffusion sampling process in order to satisfy the consistency with the degraded input image. This choice has recently been shown to be sub-optimal and to cause the restored image to deviate from the data manifold. To address these issues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD) a blind IR method that jointly optimizes for the degradation model parameters and the restored image. To ensure that the restored images lie onto the data manifold, we propose a novel sampling technique on a pre-trained diffusion model. A key idea in our method is not to modify the reverse sampling, i.e., not to alter all the intermediate latents, once an initial noise is sampled. This is ultimately equivalent to casting the IR task as an optimization problem in the space of the input noise. Moreover, to mitigate the computational cost associated with inverting a fully unrolled diffusion model, we leverage the inherent capability of these models to skip ahead in the forward diffusion process using large time steps. We experimentally validate BIRD on several image restoration tasks and show that it achieves state of the art performance on all of them. Our code is available at https://github.com/hamadichihaoui/BIRD.","sentences":["Recently, various methods have been proposed to solve Image Restoration (IR) tasks using a pre-trained diffusion model leading to state-of-the-art performance.","However, most of these methods assume that the degradation operator in the IR task is completely known.","Furthermore, a common characteristic among these approaches is that they alter the diffusion sampling process in order to satisfy the consistency with the degraded input image.","This choice has recently been shown to be sub-optimal and to cause the restored image to deviate from the data manifold.","To address these issues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD) a blind IR method that jointly optimizes for the degradation model parameters and the restored image.","To ensure that the restored images lie onto the data manifold, we propose a novel sampling technique on a pre-trained diffusion model.","A key idea in our method is not to modify the reverse sampling, i.e., not to alter all the intermediate latents, once an initial noise is sampled.","This is ultimately equivalent to casting the IR task as an optimization problem in the space of the input noise.","Moreover, to mitigate the computational cost associated with inverting a fully unrolled diffusion model, we leverage the inherent capability of these models to skip ahead in the forward diffusion process using large time steps.","We experimentally validate BIRD on several image restoration tasks and show that it achieves state of the art performance on all of them.","Our code is available at https://github.com/hamadichihaoui/BIRD."],"url":"http://arxiv.org/abs/2405.19572v1","category":"cs.CV"}
{"created":"2024-05-29 23:26:30","title":"Distributed Online Planning for Min-Max Problems in Networked Markov Games","abstract":"Min-max problems are important in multi-agent sequential decision-making because they improve the performance of the worst-performing agent in the network. However, solving the multi-agent min-max problem is challenging. We propose a modular, distributed, online planning-based algorithm that is able to approximate the solution of the min-max objective in networked Markov games, assuming that the agents communicate within a network topology and the transition and reward functions are neighborhood-dependent. This set-up is encountered in the multi-robot setting. Our method consists of two phases at every planning step. In the first phase, each agent obtains sample returns based on its local reward function, by performing online planning. Using the samples from online planning, each agent constructs a concave approximation of its underlying local return as a function of only the action of its neighborhood at the next planning step. In the second phase, the agents deploy a distributed optimization framework that converges to the optimal immediate next action for each agent, based on the function approximations of the first phase. We demonstrate our algorithm's performance through formation control simulations.","sentences":["Min-max problems are important in multi-agent sequential decision-making because they improve the performance of the worst-performing agent in the network.","However, solving the multi-agent min-max problem is challenging.","We propose a modular, distributed, online planning-based algorithm that is able to approximate the solution of the min-max objective in networked Markov games, assuming that the agents communicate within a network topology and the transition and reward functions are neighborhood-dependent.","This set-up is encountered in the multi-robot setting.","Our method consists of two phases at every planning step.","In the first phase, each agent obtains sample returns based on its local reward function, by performing online planning.","Using the samples from online planning, each agent constructs a concave approximation of its underlying local return as a function of only the action of its neighborhood at the next planning step.","In the second phase, the agents deploy a distributed optimization framework that converges to the optimal immediate next action for each agent, based on the function approximations of the first phase.","We demonstrate our algorithm's performance through formation control simulations."],"url":"http://arxiv.org/abs/2405.19570v1","category":"cs.MA"}
{"created":"2024-05-29 23:14:10","title":"Unbending strategies shepherd cooperation and suppress extortion in spatial populations","abstract":"Evolutionary game dynamics on networks typically consider the competition among simple strategies such as cooperation and defection in the Prisoner's Dilemma and summarize the effect of population structure as network reciprocity. However, it remains largely unknown regarding the evolutionary dynamics involving multiple powerful strategies typically considered in repeated games, such as the zero-determinant (ZD) strategies that are able to enforce a linear payoff relationship between them and their co-players. Here, we consider the evolutionary dynamics of always cooperate (AllC), extortionate ZD (extortioners), and unbending players in lattice populations based on the commonly used death-birth updating. Out of the class of unbending strategies, we consider a particular candidate, PSO Gambler, a machine-learning-optimized memory-one strategy, which can foster reciprocal cooperation and fairness among extortionate players. We derive analytical results under weak selection and rare mutations, including pairwise fixation probabilities and long-term frequencies of strategies. In the absence of the third unbending type, extortioners can achieve a half-half split in equilibrium with unconditional cooperators for sufficiently large extortion factors. However, the presence of unbending players fundamentally changes the dynamics and tilts the system to favor unbending cooperation. Most surprisingly, extortioners cannot dominate at all regardless of how large their extortion factor is, and the long-term frequency of unbending players is maintained almost as a constant. Our analytical method is applicable to studying the evolutionary dynamics of multiple strategies in structured populations. Our work provides insights into the interplay between network reciprocity and direct reciprocity, revealing the role of unbending strategies in enforcing fairness and suppressing extortion.","sentences":["Evolutionary game dynamics on networks typically consider the competition among simple strategies such as cooperation and defection in the Prisoner's Dilemma and summarize the effect of population structure as network reciprocity.","However, it remains largely unknown regarding the evolutionary dynamics involving multiple powerful strategies typically considered in repeated games, such as the zero-determinant (ZD) strategies that are able to enforce a linear payoff relationship between them and their co-players.","Here, we consider the evolutionary dynamics of always cooperate (AllC), extortionate ZD (extortioners), and unbending players in lattice populations based on the commonly used death-birth updating.","Out of the class of unbending strategies, we consider a particular candidate, PSO Gambler, a machine-learning-optimized memory-one strategy, which can foster reciprocal cooperation and fairness among extortionate players.","We derive analytical results under weak selection and rare mutations, including pairwise fixation probabilities and long-term frequencies of strategies.","In the absence of the third unbending type, extortioners can achieve a half-half split in equilibrium with unconditional cooperators for sufficiently large extortion factors.","However, the presence of unbending players fundamentally changes the dynamics and tilts the system to favor unbending cooperation.","Most surprisingly, extortioners cannot dominate at all regardless of how large their extortion factor is, and the long-term frequency of unbending players is maintained almost as a constant.","Our analytical method is applicable to studying the evolutionary dynamics of multiple strategies in structured populations.","Our work provides insights into the interplay between network reciprocity and direct reciprocity, revealing the role of unbending strategies in enforcing fairness and suppressing extortion."],"url":"http://arxiv.org/abs/2405.19565v1","category":"physics.soc-ph"}
{"created":"2024-05-29 22:28:33","title":"Tropical Gradient Descent","abstract":"We propose a gradient descent method for solving optimisation problems arising in settings of tropical geometry - a variant of algebraic geometry that has become increasingly studied in applications such as computational biology, economics, and computer science. Our approach takes advantage of the polyhedral and combinatorial structures arising in tropical geometry to propose a versatile approach for approximating local minima in tropical statistical optimisation problems - a rapidly growing body of work in recent years. Theoretical results establish global solvability for 1-sample problems and a convergence rate of $O(1/\\sqrt{k})$. Numerical experiments demonstrate the method's superior performance over classical descent for tropical optimisation problems which exhibit tropical convexity but not classical convexity. Notably, tropical descent seamlessly integrates into advanced optimisation methods, such as Adam, offering improved overall performance.","sentences":["We propose a gradient descent method for solving optimisation problems arising in settings of tropical geometry - a variant of algebraic geometry that has become increasingly studied in applications such as computational biology, economics, and computer science.","Our approach takes advantage of the polyhedral and combinatorial structures arising in tropical geometry to propose a versatile approach for approximating local minima in tropical statistical optimisation problems - a rapidly growing body of work in recent years.","Theoretical results establish global solvability for 1-sample problems and a convergence rate of $O(1/\\sqrt{k})$. Numerical experiments demonstrate the method's superior performance over classical descent for tropical optimisation problems which exhibit tropical convexity but not classical convexity.","Notably, tropical descent seamlessly integrates into advanced optimisation methods, such as Adam, offering improved overall performance."],"url":"http://arxiv.org/abs/2405.19551v1","category":"math.OC"}
{"created":"2024-05-29 22:23:20","title":"RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning","abstract":"Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks. However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation. This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner. Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress. To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward algorithms. Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL. The source code for RLeXplore is available at https://github.com/RLE-Foundation/RLeXplore.","sentences":["Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks.","However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation.","This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner.","Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress.","To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward algorithms.","Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL.","The source code for RLeXplore is available at https://github.com/RLE-Foundation/RLeXplore."],"url":"http://arxiv.org/abs/2405.19548v1","category":"cs.LG"}
{"created":"2024-05-29 22:19:39","title":"Convex Optimization of Initial Perturbations toward Quantitative Weather Control","abstract":"We propose a convex optimization approach to determine perturbations in the initial conditions of a weather phenomenon as control inputs for quantitative weather control. We first construct a sensitivity matrix of outputs, such as accumulated precipitation, to the initial conditions, such as temperature and humidity, through sensitivity analysis of a numerical weather prediction model. We then solve a convex optimization problem to find optimal perturbations in the initial conditions to realize the desired spatial distribution of the targeting outputs. We implement the proposed method in a benchmark of a warm bubble experiment and show that it realizes desired spatial distributions of accumulated precipitation, such as a reference distribution and the reduced maximum value.","sentences":["We propose a convex optimization approach to determine perturbations in the initial conditions of a weather phenomenon as control inputs for quantitative weather control.","We first construct a sensitivity matrix of outputs, such as accumulated precipitation, to the initial conditions, such as temperature and humidity, through sensitivity analysis of a numerical weather prediction model.","We then solve a convex optimization problem to find optimal perturbations in the initial conditions to realize the desired spatial distribution of the targeting outputs.","We implement the proposed method in a benchmark of a warm bubble experiment and show that it realizes desired spatial distributions of accumulated precipitation, such as a reference distribution and the reduced maximum value."],"url":"http://arxiv.org/abs/2405.19546v1","category":"physics.ao-ph"}
{"created":"2024-05-29 21:43:46","title":"Optimal complexity of parameterized quantum circuits","abstract":"Parameterized quantum circuits play a key role for the development of quantum variational algorithms in the realm of the NISQ era. Knowing their actual capability of performing different kinds of tasks is then of the utmost importance. By comparing them with a prototypical class of universal random circuits we have found that their approach to the asymptotic complexity defined by the Haar measure is faster, needing less gates to reach it. Topology has been revealed crucial for this. The majorization criterion has proven as a relevant complementary tool to the expressibility and the mean entanglement.","sentences":["Parameterized quantum circuits play a key role for the development of quantum variational algorithms in the realm of the NISQ era.","Knowing their actual capability of performing different kinds of tasks is then of the utmost importance.","By comparing them with a prototypical class of universal random circuits we have found that their approach to the asymptotic complexity defined by the Haar measure is faster, needing less gates to reach it.","Topology has been revealed crucial for this.","The majorization criterion has proven as a relevant complementary tool to the expressibility and the mean entanglement."],"url":"http://arxiv.org/abs/2405.19537v1","category":"quant-ph"}
{"created":"2024-05-29 21:24:44","title":"Contrasting Multiple Representations with the Multi-Marginal Matching Gap","abstract":"Learning meaningful representations of complex objects that can be seen through multiple ($k\\geq 3$) views or modalities is a core task in machine learning. Existing methods use losses originally intended for paired views, and extend them to $k$ views, either by instantiating $\\tfrac12k(k-1)$ loss-pairs, or by using reduced embeddings, following a \\textit{one vs. average-of-rest} strategy. We propose the multi-marginal matching gap (M3G), a loss that borrows tools from multi-marginal optimal transport (MM-OT) theory to simultaneously incorporate all $k$ views. Given a batch of $n$ points, each seen as a $k$-tuple of views subsequently transformed into $k$ embeddings, our loss contrasts the cost of matching these $n$ ground-truth $k$-tuples with the MM-OT polymatching cost, which seeks $n$ optimally arranged $k$-tuples chosen within these $n\\times k$ vectors. While the exponential complexity $O(n^k$) of the MM-OT problem may seem daunting, we show in experiments that a suitable generalization of the Sinkhorn algorithm for that problem can scale to, e.g., $k=3\\sim 6$ views using mini-batches of size $64~\\sim128$. Our experiments demonstrate improved performance over multiview extensions of pairwise losses, for both self-supervised and multimodal tasks.","sentences":["Learning meaningful representations of complex objects that can be seen through multiple ($k\\geq 3$) views or modalities is a core task in machine learning.","Existing methods use losses originally intended for paired views, and extend them to $k$ views, either by instantiating $\\tfrac12k(k-1)$ loss-pairs, or by using reduced embeddings, following a \\textit{one vs. average-of-rest} strategy.","We propose the multi-marginal matching gap (M3G), a loss that borrows tools from multi-marginal optimal transport (MM-OT) theory to simultaneously incorporate all $k$ views.","Given a batch of $n$ points, each seen as a $k$-tuple of views subsequently transformed into $k$ embeddings, our loss contrasts the cost of matching these $n$ ground-truth $k$-tuples with the MM-OT polymatching cost, which seeks $n$ optimally arranged $k$-tuples chosen within these $n\\times k$ vectors.","While the exponential complexity $O(n^k$) of the MM-OT problem may seem daunting, we show in experiments that a suitable generalization of the Sinkhorn algorithm for that problem can scale to, e.g., $k=3\\sim 6$ views using mini-batches of size $64~\\sim128$. Our experiments demonstrate improved performance over multiview extensions of pairwise losses, for both self-supervised and multimodal tasks."],"url":"http://arxiv.org/abs/2405.19532v1","category":"cs.LG"}
{"created":"2024-05-29 21:13:07","title":"Predicting Long-Term Human Behaviors in Discrete Representations via Physics-Guided Diffusion","abstract":"Long-term human trajectory prediction is a challenging yet critical task in robotics and autonomous systems. Prior work that studied how to predict accurate short-term human trajectories with only unimodal features often failed in long-term prediction. Reinforcement learning provides a good solution for learning human long-term behaviors but can suffer from challenges in data efficiency and optimization. In this work, we propose a long-term human trajectory forecasting framework that leverages a guided diffusion model to generate diverse long-term human behaviors in a high-level latent action space, obtained via a hierarchical action quantization scheme using a VQ-VAE to discretize continuous trajectories and the available context. The latent actions are predicted by our guided diffusion model, which uses physics-inspired guidance at test time to constrain generated multimodal action distributions. Specifically, we use reachability analysis during the reverse denoising process to guide the diffusion steps toward physically feasible latent actions. We evaluate our framework on two publicly available human trajectory forecasting datasets: SFU-Store-Nav and JRDB, and extensive experimental results show that our framework achieves superior performance in long-term human trajectory forecasting.","sentences":["Long-term human trajectory prediction is a challenging yet critical task in robotics and autonomous systems.","Prior work that studied how to predict accurate short-term human trajectories with only unimodal features often failed in long-term prediction.","Reinforcement learning provides a good solution for learning human long-term behaviors but can suffer from challenges in data efficiency and optimization.","In this work, we propose a long-term human trajectory forecasting framework that leverages a guided diffusion model to generate diverse long-term human behaviors in a high-level latent action space, obtained via a hierarchical action quantization scheme using a VQ-VAE to discretize continuous trajectories and the available context.","The latent actions are predicted by our guided diffusion model, which uses physics-inspired guidance at test time to constrain generated multimodal action distributions.","Specifically, we use reachability analysis during the reverse denoising process to guide the diffusion steps toward physically feasible latent actions.","We evaluate our framework on two publicly available human trajectory forecasting datasets: SFU-Store-Nav and JRDB, and extensive experimental results show that our framework achieves superior performance in long-term human trajectory forecasting."],"url":"http://arxiv.org/abs/2405.19528v1","category":"cs.RO"}
{"created":"2024-05-29 20:52:44","title":"Space charge dominated momentum spread and compensation strategies in the post-linac section of Proton Improvement Plan-II at Fermilab","abstract":"The upcoming Proton Improvement Plan-II (PIP-II), designated for enhancements to the Fermilab accelerator complex, features a new 800 MeV superconducting linac and a Beam Transfer Line (BTL) to transport the beam to the existing Booster synchrotron. To mitigate the space charge tune shift associated with a high intensity accumulated beam, the low emittance linac beam is used to paint the ring phase space both transversely and longitudinally. To prevent losses caused by particles injected outside the rf separatrix while painting longitudinal phase space, the momentum spread of the incoming beam should not exceed 2.1 x 10^-4. Detailed simulations showed that due to space charge, the rms momentum spread increases to 4 x 10^-4 while it is transported in the BTL --about twice the allowable limit. In this paper, we outline a mitigation strategy involving a debuncher cavity. We discuss location, operating frequency, and gap voltage under both nominal and perturbed beam conditions, specifically accounting for momentum jitter. The impact of cavity misalignments is also assessed. The paper concludes by recommending an optimized configuration.","sentences":["The upcoming Proton Improvement Plan-II (PIP-II), designated for enhancements to the Fermilab accelerator complex, features a new 800 MeV superconducting linac and a Beam Transfer Line (BTL) to transport the beam to the existing Booster synchrotron.","To mitigate the space charge tune shift associated with a high intensity accumulated beam, the low emittance linac beam is used to paint the ring phase space both transversely and longitudinally.","To prevent losses caused by particles injected outside the rf separatrix while painting longitudinal phase space, the momentum spread of the incoming beam should not exceed 2.1 x 10^-4.","Detailed simulations showed that due to space charge, the rms momentum spread increases to 4 x 10^-4 while it is transported in the BTL --about twice the allowable limit.","In this paper, we outline a mitigation strategy involving a debuncher cavity.","We discuss location, operating frequency, and gap voltage under both nominal and perturbed beam conditions, specifically accounting for momentum jitter.","The impact of cavity misalignments is also assessed.","The paper concludes by recommending an optimized configuration."],"url":"http://arxiv.org/abs/2405.19515v1","category":"physics.acc-ph"}
{"created":"2024-05-29 20:51:38","title":"Decentralized Optimization in Time-Varying Networks with Arbitrary Delays","abstract":"We consider a decentralized optimization problem for networks affected by communication delays. Examples of such networks include collaborative machine learning, sensor networks, and multi-agent systems. To mimic communication delays, we add virtual non-computing nodes to the network, resulting in directed graphs. This motivates investigating decentralized optimization solutions on directed graphs. Existing solutions assume nodes know their out-degrees, resulting in limited applicability. To overcome this limitation, we introduce a novel gossip-based algorithm, called DT-GO, that does not need to know the out-degrees. The algorithm is applicable in general directed networks, for example networks with delays or limited acknowledgment capabilities. We derive convergence rates for both convex and non-convex objectives, showing that our algorithm achieves the same complexity order as centralized Stochastic Gradient Descent. In other words, the effects of the graph topology and delays are confined to higher-order terms. Additionally, we extend our analysis to accommodate time-varying network topologies. Numerical simulations are provided to support our theoretical findings.","sentences":["We consider a decentralized optimization problem for networks affected by communication delays.","Examples of such networks include collaborative machine learning, sensor networks, and multi-agent systems.","To mimic communication delays, we add virtual non-computing nodes to the network, resulting in directed graphs.","This motivates investigating decentralized optimization solutions on directed graphs.","Existing solutions assume nodes know their out-degrees, resulting in limited applicability.","To overcome this limitation, we introduce a novel gossip-based algorithm, called DT-GO, that does not need to know the out-degrees.","The algorithm is applicable in general directed networks, for example networks with delays or limited acknowledgment capabilities.","We derive convergence rates for both convex and non-convex objectives, showing that our algorithm achieves the same complexity order as centralized Stochastic Gradient Descent.","In other words, the effects of the graph topology and delays are confined to higher-order terms.","Additionally, we extend our analysis to accommodate time-varying network topologies.","Numerical simulations are provided to support our theoretical findings."],"url":"http://arxiv.org/abs/2405.19513v1","category":"cs.LG"}
{"created":"2024-05-29 20:45:57","title":"Leveraging partial stragglers within gradient coding","abstract":"Within distributed learning, workers typically compute gradients on their assigned dataset chunks and send them to the parameter server (PS), which aggregates them to compute either an exact or approximate version of $\\nabla L$ (gradient of the loss function $L$). However, in large-scale clusters, many workers are slower than their promised speed or even failure-prone. A gradient coding solution introduces redundancy within the assignment of chunks to the workers and uses coding theoretic ideas to allow the PS to recover $\\nabla L$ (exactly or approximately), even in the presence of stragglers. Unfortunately, most existing gradient coding protocols are inefficient from a computation perspective as they coarsely classify workers as operational or failed; the potentially valuable work performed by slow workers (partial stragglers) is ignored. In this work, we present novel gradient coding protocols that judiciously leverage the work performed by partial stragglers. Our protocols are efficient from a computation and communication perspective and numerically stable. For an important class of chunk assignments, we present efficient algorithms for optimizing the relative ordering of chunks within the workers; this ordering affects the overall execution time. For exact gradient reconstruction, our protocol is around $2\\times$ faster than the original class of protocols and for approximate gradient reconstruction, the mean-squared-error of our reconstructed gradient is several orders of magnitude better.","sentences":["Within distributed learning, workers typically compute gradients on their assigned dataset chunks and send them to the parameter server (PS), which aggregates them to compute either an exact or approximate version of $\\nabla L$ (gradient of the loss function $L$).","However, in large-scale clusters, many workers are slower than their promised speed or even failure-prone.","A gradient coding solution introduces redundancy within the assignment of chunks to the workers and uses coding theoretic ideas to allow the PS to recover $\\nabla L$ (exactly or approximately), even in the presence of stragglers.","Unfortunately, most existing gradient coding protocols are inefficient from a computation perspective as they coarsely classify workers as operational or failed; the potentially valuable work performed by slow workers (partial stragglers) is ignored.","In this work, we present novel gradient coding protocols that judiciously leverage the work performed by partial stragglers.","Our protocols are efficient from a computation and communication perspective and numerically stable.","For an important class of chunk assignments, we present efficient algorithms for optimizing the relative ordering of chunks within the workers; this ordering affects the overall execution time.","For exact gradient reconstruction, our protocol is around $2\\times$ faster than the original class of protocols and for approximate gradient reconstruction, the mean-squared-error of our reconstructed gradient is several orders of magnitude better."],"url":"http://arxiv.org/abs/2405.19509v1","category":"cs.IT"}
{"created":"2024-05-29 20:41:26","title":"A Sixteen Multiple-Amplifier-Sensing CCD and Characterization Techniques Targeting The Next Generation of Astronomical Instruments","abstract":"This work presents a candidate sensor for future spectroscopic applications, such as a Stage-5 Spectroscopic Survey Experiment or the Habitable Worlds Observatory. This new type of CCD sensor features multiple in-line amplifiers at its output stage allowing multiple measurements of the same charge packet, either in each amplifier and/or in the different amplifiers. Recently, the operation of an 8-amplifier sensor has been experimentally demonstrated, and the operation of a 16-amplifier sensor is presented in this work. This new sensor enables a noise level of approximately $1^e_{\\rm rms}$ with a single sample per amplifier. Additionally, it is shown that sub-electron noise can be achieved using multiple samples per amplifier. In addition to demonstrating the performance of the 16-amplifier sensor, this work aims to create a framework for future analysis and performance optimization of this type of detectors. New models and techniques are presented to characterize specific parameters, which are absent in conventional CCDs and Skipper-CCDs: charge transfer between amplifiers and independent and common noise in the amplifiers, and their processing.","sentences":["This work presents a candidate sensor for future spectroscopic applications, such as a Stage-5 Spectroscopic Survey Experiment or the Habitable Worlds Observatory.","This new type of CCD sensor features multiple in-line amplifiers at its output stage allowing multiple measurements of the same charge packet, either in each amplifier and/or in the different amplifiers.","Recently, the operation of an 8-amplifier sensor has been experimentally demonstrated, and the operation of a 16-amplifier sensor is presented in this work.","This new sensor enables a noise level of approximately $1^e_{\\rm rms}$ with a single sample per amplifier.","Additionally, it is shown that sub-electron noise can be achieved using multiple samples per amplifier.","In addition to demonstrating the performance of the 16-amplifier sensor, this work aims to create a framework for future analysis and performance optimization of this type of detectors.","New models and techniques are presented to characterize specific parameters, which are absent in conventional CCDs and Skipper-CCDs: charge transfer between amplifiers and independent and common noise in the amplifiers, and their processing."],"url":"http://arxiv.org/abs/2405.19505v1","category":"astro-ph.IM"}
{"created":"2024-05-29 20:24:42","title":"Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments","abstract":"We explore a Federated Reinforcement Learning (FRL) problem where $N$ agents collaboratively learn a common policy without sharing their trajectory data. To date, existing FRL work has primarily focused on agents operating in the same or ``similar\" environments. In contrast, our problem setup allows for arbitrarily large levels of environment heterogeneity. To obtain the optimal policy which maximizes the average performance across all potentially completely different environments, we propose two algorithms: FedSVRPG-M and FedHAPG-M. In contrast to existing results, we demonstrate that both FedSVRPG-M and FedHAPG-M, both of which leverage momentum mechanisms, can exactly converge to a stationary point of the average performance function, regardless of the magnitude of environment heterogeneity. Furthermore, by incorporating the benefits of variance-reduction techniques or Hessian approximation, both algorithms achieve state-of-the-art convergence results, characterized by a sample complexity of $\\mathcal{O}\\left(\\epsilon^{-\\frac{3}{2}}/N\\right)$. Notably, our algorithms enjoy linear convergence speedups with respect to the number of agents, highlighting the benefit of collaboration among agents in finding a common policy.","sentences":["We explore a Federated Reinforcement Learning (FRL) problem where $N$ agents collaboratively learn a common policy without sharing their trajectory data.","To date, existing FRL work has primarily focused on agents operating in the same or ``similar\" environments.","In contrast, our problem setup allows for arbitrarily large levels of environment heterogeneity.","To obtain the optimal policy which maximizes the average performance across all potentially completely different environments, we propose two algorithms: FedSVRPG-M and FedHAPG-M.","In contrast to existing results, we demonstrate that both FedSVRPG-M and FedHAPG-M, both of which leverage momentum mechanisms, can exactly converge to a stationary point of the average performance function, regardless of the magnitude of environment heterogeneity.","Furthermore, by incorporating the benefits of variance-reduction techniques or Hessian approximation, both algorithms achieve state-of-the-art convergence results, characterized by a sample complexity of $\\mathcal{O}\\left(\\epsilon^{-\\frac{3}{2}}/N\\right)$. Notably, our algorithms enjoy linear convergence speedups with respect to the number of agents, highlighting the benefit of collaboration among agents in finding a common policy."],"url":"http://arxiv.org/abs/2405.19499v1","category":"cs.LG"}
{"created":"2024-05-29 20:23:01","title":"Gaussian Flow Bridges for Audio Domain Transfer with Unpaired Data","abstract":"Audio domain transfer is the process of modifying audio signals to match characteristics of a different domain, while retaining the original content. This paper investigates the potential of Gaussian Flow Bridges, an emerging approach in generative modeling, for this problem. The presented framework addresses the transport problem across different distributions of audio signals through the implementation of a series of two deterministic probability flows. The proposed framework facilitates manipulation of the target distribution properties through a continuous control variable, which defines a certain aspect of the target domain. Notably, this approach does not rely on paired examples for training. To address identified challenges on maintaining the speech content consistent, we recommend a training strategy that incorporates chunk-based minibatch Optimal Transport couplings of data samples and noise. Comparing our unsupervised method with established baselines, we find competitive performance in tasks of reverberation and distortion manipulation. Despite encoutering limitations, the intriguing results obtained in this study underscore potential for further exploration.","sentences":["Audio domain transfer is the process of modifying audio signals to match characteristics of a different domain, while retaining the original content.","This paper investigates the potential of Gaussian Flow Bridges, an emerging approach in generative modeling, for this problem.","The presented framework addresses the transport problem across different distributions of audio signals through the implementation of a series of two deterministic probability flows.","The proposed framework facilitates manipulation of the target distribution properties through a continuous control variable, which defines a certain aspect of the target domain.","Notably, this approach does not rely on paired examples for training.","To address identified challenges on maintaining the speech content consistent, we recommend a training strategy that incorporates chunk-based minibatch Optimal Transport couplings of data samples and noise.","Comparing our unsupervised method with established baselines, we find competitive performance in tasks of reverberation and distortion manipulation.","Despite encoutering limitations, the intriguing results obtained in this study underscore potential for further exploration."],"url":"http://arxiv.org/abs/2405.19497v1","category":"eess.AS"}
{"created":"2024-05-29 20:16:16","title":"Fast Gaussian Distributed Pseudorandom Number Generation in Java via the Ziggurat Algorithm","abstract":"We report on experiments with the ziggurat algorithm for generating Gaussian distributed random numbers. The study utilizes our open source Java implementation that was introduced originally for Java 11 at a time when the Java API only provided the much slower polar method. Our Java implementation of the ziggurat algorithm is a port of the GNU Scientific Library's C implementation. Java 17 introduced a significant overhaul of pseudorandom number generation, including several modern pseudorandom number generators (PRNGs) as well as additional functionality, among which includes switching from the polar method to a modified ziggurat algorithm. In the experiments of this paper, we explore whether there is still a need for our implementation for Java 17+ applications. Our results show that Java 17's modified ziggurat is faster than our implementation for the PRNGs that support it. However, Java 17+ continues to use the polar method for the legacy PRNGs Random, SecureRandom, and ThreadLocalRandom. The linear congruential method of Java's Random class lacks the statistical properties required by Java's modified ziggurat implementation; and SecureRandom and ThreadLocalRandom unfortunately use the polar method as a side-effect of extending Random. Our implementation of the original ziggurat algorithm does not require the same statistical properties of the underlying PRNG as Java 17's optimized version, and can be used with any of these PRNGs, and is especially relevant where pre-Java 17 support is required.","sentences":["We report on experiments with the ziggurat algorithm for generating Gaussian distributed random numbers.","The study utilizes our open source Java implementation that was introduced originally for Java 11 at a time when the Java API only provided the much slower polar method.","Our Java implementation of the ziggurat algorithm is a port of the GNU Scientific Library's C implementation.","Java 17 introduced a significant overhaul of pseudorandom number generation, including several modern pseudorandom number generators (PRNGs) as well as additional functionality, among which includes switching from the polar method to a modified ziggurat algorithm.","In the experiments of this paper, we explore whether there is still a need for our implementation for Java 17+ applications.","Our results show that Java 17's modified ziggurat is faster than our implementation for the PRNGs that support it.","However, Java 17+ continues to use the polar method for the legacy PRNGs Random, SecureRandom, and ThreadLocalRandom.","The linear congruential method of Java's Random class lacks the statistical properties required by Java's modified ziggurat implementation; and SecureRandom and ThreadLocalRandom unfortunately use the polar method as a side-effect of extending Random.","Our implementation of the original ziggurat algorithm does not require the same statistical properties of the underlying PRNG as Java 17's optimized version, and can be used with any of these PRNGs, and is especially relevant where pre-Java 17 support is required."],"url":"http://arxiv.org/abs/2405.19493v1","category":"cs.DS"}
{"created":"2024-05-29 20:09:06","title":"Calibration and Validation of a Phase-Field Model of Brittle Fracture within the Damage Mechanics Challenge","abstract":"In the context of the Damage Mechanics Challenge, we adopt a phase-field model of brittle fracture to blindly predict the behavior up to failure of a notched three-point-bending specimen loaded under mixed-mode conditions. The beam is additively manufactured using a geo-architected gypsum based on the combination of bassanite and a water-based binder. The calibration of the material parameters involved in the model is based on a set of available independent experimental tests and on a two-stage procedure. In the first stage an estimate of most of the elastic parameters is obtained, whereas the remaining parameters are optimized in the second stage so as to minimize the discrepancy between the numerical predictions and a set of experimental results on notched three-point-bending beams. The good agreement between numerical predictions and experimental results in terms of load-displacement curves and crack paths demonstrates the predictive ability of the model and the reliability of the calibration procedure.","sentences":["In the context of the Damage Mechanics Challenge, we adopt a phase-field model of brittle fracture to blindly predict the behavior up to failure of a notched three-point-bending specimen loaded under mixed-mode conditions.","The beam is additively manufactured using a geo-architected gypsum based on the combination of bassanite and a water-based binder.","The calibration of the material parameters involved in the model is based on a set of available independent experimental tests and on a two-stage procedure.","In the first stage an estimate of most of the elastic parameters is obtained, whereas the remaining parameters are optimized in the second stage so as to minimize the discrepancy between the numerical predictions and a set of experimental results on notched three-point-bending beams.","The good agreement between numerical predictions and experimental results in terms of load-displacement curves and crack paths demonstrates the predictive ability of the model and the reliability of the calibration procedure."],"url":"http://arxiv.org/abs/2405.19491v1","category":"cs.CE"}
{"created":"2024-05-29 20:08:31","title":"Bayesian optimization scheme for the design of a nanofibrous high power target","abstract":"High Power Targetry (HPT) R&D is critical in the context of increasing beam intensity and energy for next generation accelerators. Many target concepts and novel materials are being developed and tested for their ability to withstand extreme beam environments; the HPT R&D Group at Fermilab is developing an electrospun nanofiber material for this purpose. The performance of these nanofiber targets is sensitive to their construction parameters, such as the packing density of the fibers. Lowering the density improves the survival of the target, but reduces the secondary particle yield. Optimizing the lifetime and production efficiency of the target poses an interesting design problem, and in this paper we study the applicability of Bayesian optimization to its solution. We first describe how to encode the nanofiber target design problem as the optimization of an objective function, and how to evaluate that function with computer simulations. We then explain the optimization loop setup. Thereafter, we present the optimal design parameters suggested by the algorithm, and close with discussions of limitations and future refinements.","sentences":["High Power Targetry (HPT) R&D is critical in the context of increasing beam intensity and energy for next generation accelerators.","Many target concepts and novel materials are being developed and tested for their ability to withstand extreme beam environments; the HPT R&D Group at Fermilab is developing an electrospun nanofiber material for this purpose.","The performance of these nanofiber targets is sensitive to their construction parameters, such as the packing density of the fibers.","Lowering the density improves the survival of the target, but reduces the secondary particle yield.","Optimizing the lifetime and production efficiency of the target poses an interesting design problem, and in this paper we study the applicability of Bayesian optimization to its solution.","We first describe how to encode the nanofiber target design problem as the optimization of an objective function, and how to evaluate that function with computer simulations.","We then explain the optimization loop setup.","Thereafter, we present the optimal design parameters suggested by the algorithm, and close with discussions of limitations and future refinements."],"url":"http://arxiv.org/abs/2405.19490v1","category":"physics.acc-ph"}
{"created":"2024-05-29 19:54:05","title":"Integrated Communication and Imaging: Design, Analysis, and Performances of COSMIC Waveforms","abstract":"This paper proposes a novel waveform design method named COSMIC (Connectivity-Oriented Sensing Method for Imaging and Communication). These waveforms are engineered to convey communication symbols while adhering to an extended orthogonality condition, enabling their use in generating radio images of the environment. A Multiple-Input Multiple-Output (MIMO) Radar-Communication (RadCom) device transmits COSMIC waveforms from each antenna simultaneously within the same time window and frequency band, indicating that orthogonality is not achieved by space, time, or frequency multiplexing. Indeed, orthogonality among the waveforms is achieved by leveraging the degrees of freedom provided by the assumption that the field of view is limited or significantly smaller than the transmitted signals' length. The RadCom device receives and processes the echoes from an infinite number of infinitesimal scatterers within its field of view, constructing an electromagnetic image of the environment. Concurrently, these waveforms can also carry information to other connected network entities. This work provides the algebraic concepts used to generate COSMIC waveforms. Moreover, an opportunistic optimization of the imaging and communication efficiency is discussed. Simulation results demonstrate that COSMIC waveforms enable accurate environmental imaging while maintaining acceptable communication performances.","sentences":["This paper proposes a novel waveform design method named COSMIC (Connectivity-Oriented Sensing Method for Imaging and Communication).","These waveforms are engineered to convey communication symbols while adhering to an extended orthogonality condition, enabling their use in generating radio images of the environment.","A Multiple-Input Multiple-Output (MIMO) Radar-Communication (RadCom) device transmits COSMIC waveforms from each antenna simultaneously within the same time window and frequency band, indicating that orthogonality is not achieved by space, time, or frequency multiplexing.","Indeed, orthogonality among the waveforms is achieved by leveraging the degrees of freedom provided by the assumption that the field of view is limited or significantly smaller than the transmitted signals' length.","The RadCom device receives and processes the echoes from an infinite number of infinitesimal scatterers within its field of view, constructing an electromagnetic image of the environment.","Concurrently, these waveforms can also carry information to other connected network entities.","This work provides the algebraic concepts used to generate COSMIC waveforms.","Moreover, an opportunistic optimization of the imaging and communication efficiency is discussed.","Simulation results demonstrate that COSMIC waveforms enable accurate environmental imaging while maintaining acceptable communication performances."],"url":"http://arxiv.org/abs/2405.19481v1","category":"eess.SP"}
{"created":"2024-05-29 19:53:45","title":"RANFusion: A Comprehensive Tool for Simulating Handover In Next-G RAN","abstract":"The rapid advancement of 5G networks and the upcoming transition to 6G necessitate the use of the Open Radio Access Network (O-RAN) architecture to enable greater flexibility, interoperability, and innovation. This shift towards 6G and O-RAN requires the development of advanced simulation tools for testing, analyzing, and optimizing Radio Access Network (RAN) operations. This need becomes critical due to the complex dynamics of mobility management inherent in the 6G vision and next-generation networks. These networks anticipate advanced handover methods for mobile users, UAVs, IoT devices, and beyond. Addressing this gap, this paper introduces RANFusion: a robust RAN simulator specifically created to explore a variety of handover scenarios and to test and balance resources between users. This tool enables precise simulations for refining handover strategies within RAN and O-RAN environments, thereby ensuring optimal performance and reliability in these advanced network infrastructures.","sentences":["The rapid advancement of 5G networks and the upcoming transition to 6G necessitate the use of the Open Radio Access Network (O-RAN) architecture to enable greater flexibility, interoperability, and innovation.","This shift towards 6G and O-RAN requires the development of advanced simulation tools for testing, analyzing, and optimizing Radio Access Network (RAN) operations.","This need becomes critical due to the complex dynamics of mobility management inherent in the 6G vision and next-generation networks.","These networks anticipate advanced handover methods for mobile users, UAVs, IoT devices, and beyond.","Addressing this gap, this paper introduces RANFusion: a robust RAN simulator specifically created to explore a variety of handover scenarios and to test and balance resources between users.","This tool enables precise simulations for refining handover strategies within RAN and O-RAN environments, thereby ensuring optimal performance and reliability in these advanced network infrastructures."],"url":"http://arxiv.org/abs/2405.19480v1","category":"cs.NI"}
{"created":"2024-05-29 19:51:20","title":"Development of Cryogenic Scintillation Detectors for the Search of New Physics","abstract":"CryoCsI, the proposed prototype, is a cryogenic undoped CsI scintillating detector, which has a much lower energy threshold potentially down to 0.5 keV$_{nr}$ compared to the doped CsI. This enhanced sensitivity of CryoCsI allows for the observation of more Coherent elastic neutrino-nucleus scattering events. Precise measurements of CEvNS can not only validate the predictions of the Standard Model but also explore new physics. In conjunction with other COHERENT detectors, CryoCsI has the potential to achieve world-leading sensitivities in a broad range of physics topics within and beyond the SM. The sensitivities of CryoCsI to hidden-sector dark matter, non-standard neutrino interactions, and neutron radius are explored.   This thesis delves into the construction of CryoCsI and efforts to enhance its light yield from 20 to $50 \\pm 2$ photoelectrons per keV electron-equivalent (keV$_{ee}$). It will address challenges with cryogenic SiPMs, including inferior energy resolution, optical cross-talk, and potential limitations on detecting rare events. Understanding the light yield of scintillating detectors for nuclear recoils is crucial, as explored through alpha-particle and neutron quenching factor measurements. A QF of approximately 15\\% was measured using a neutron beam at the Triangle Universities Nuclear Lab. Proposed solutions to challenges like the overshoot effect observed in PMTs will be discussed. Additionally, the thesis will explore design considerations for minimizing background noise and optimizing the CsI crystal's shape through optical simulations.","sentences":["CryoCsI, the proposed prototype, is a cryogenic undoped CsI scintillating detector, which has a much lower energy threshold potentially down to 0.5 keV$_{nr}$ compared to the doped CsI. This enhanced sensitivity of CryoCsI allows for the observation of more Coherent elastic neutrino-nucleus scattering events.","Precise measurements of CEvNS can not only validate the predictions of the Standard Model but also explore new physics.","In conjunction with other COHERENT detectors, CryoCsI has the potential to achieve world-leading sensitivities in a broad range of physics topics within and beyond the SM.","The sensitivities of CryoCsI to hidden-sector dark matter, non-standard neutrino interactions, and neutron radius are explored.   ","This thesis delves into the construction of CryoCsI and efforts to enhance its light yield from 20 to $50 \\pm 2$ photoelectrons per keV electron-equivalent (keV$_{ee}$).","It will address challenges with cryogenic SiPMs, including inferior energy resolution, optical cross-talk, and potential limitations on detecting rare events.","Understanding the light yield of scintillating detectors for nuclear recoils is crucial, as explored through alpha-particle and neutron quenching factor measurements.","A QF of approximately 15\\% was measured using a neutron beam at the Triangle Universities Nuclear Lab.","Proposed solutions to challenges like the overshoot effect observed in PMTs will be discussed.","Additionally, the thesis will explore design considerations for minimizing background noise and optimizing the CsI crystal's shape through optical simulations."],"url":"http://arxiv.org/abs/2405.19476v1","category":"physics.ins-det"}
{"created":"2024-05-29 19:21:55","title":"Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data","abstract":"We develop and analyze algorithms for instrumental variable regression by viewing the problem as a conditional stochastic optimization problem. In the context of least-squares instrumental variable regression, our algorithms neither require matrix inversions nor mini-batches and provides a fully online approach for performing instrumental variable regression with streaming data. When the true model is linear, we derive rates of convergence in expectation, that are of order $\\mathcal{O}(\\log T/T)$ and $\\mathcal{O}(1/T^{1-\\iota})$ for any $\\iota>0$, respectively under the availability of two-sample and one-sample oracles, respectively, where $T$ is the number of iterations. Importantly, under the availability of the two-sample oracle, our procedure avoids explicitly modeling and estimating the relationship between confounder and the instrumental variables, demonstrating the benefit of the proposed approach over recent works based on reformulating the problem as minimax optimization problems. Numerical experiments are provided to corroborate the theoretical results.","sentences":["We develop and analyze algorithms for instrumental variable regression by viewing the problem as a conditional stochastic optimization problem.","In the context of least-squares instrumental variable regression, our algorithms neither require matrix inversions nor mini-batches and provides a fully online approach for performing instrumental variable regression with streaming data.","When the true model is linear, we derive rates of convergence in expectation, that are of order $\\mathcal{O}(\\log T/T)$ and $\\mathcal{O}(1/T^{1-\\iota})$ for any $\\iota>0$, respectively under the availability of two-sample and one-sample oracles, respectively, where $T$ is the number of iterations.","Importantly, under the availability of the two-sample oracle, our procedure avoids explicitly modeling and estimating the relationship between confounder and the instrumental variables, demonstrating the benefit of the proposed approach over recent works based on reformulating the problem as minimax optimization problems.","Numerical experiments are provided to corroborate the theoretical results."],"url":"http://arxiv.org/abs/2405.19463v1","category":"stat.ML"}
{"created":"2024-05-29 19:06:04","title":"Development of the Low Frequency Telescope focal plane detector arrays for LiteBIRD","abstract":"LiteBIRD, a forthcoming JAXA mission, aims to accurately study the microwave sky within the 40-400 GHz frequency range divided into 15 distinct nominal bands. The primary objective is to constrain the CMB inflationary signal, specifically the primordial B-modes. LiteBIRD targets the CMB B-mode signal on large angular scales, where the primordial inflationary signal is expected to dominate, with the goal of reaching a tensor-to-scalar ratio sensitivity of $\\sigma_r\\sim0.001$. LiteBIRD frequency bands will be split among three telescopes, with some overlap between telescopes for better control of systematic effects. Here we report on the development status of the detector arrays for the Low Frequency Telescope (LFT), which spans the 34-161 GHz range, with 12 bands subdivided between four types of trichroic pixels consisting of lenslet-coupled sinuous antennas. The signal from the antenna is bandpass filtered and sensed by AlMn Transition-Edge Sensors (TES). We provide an update on the status of the design and development of LiteBIRD's LFT LF1 (40-60-78 GHz), LF2 (50-68-89 GHz) pixels. We discuss design choices motivated by LiteBIRD scientific goals. In particular we focus on the details of the optimization of the design parameters of the sinuous antenna, on-chip bandpass filters, cross-under and impedance transformers and all the RF components that define the LF1 and LF2 pixel detection chain. We present this work in the context of the technical challenges and physical constraints imposed by the finite size of the instrument.","sentences":["LiteBIRD, a forthcoming JAXA mission, aims to accurately study the microwave sky within the 40-400 GHz frequency range divided into 15 distinct nominal bands.","The primary objective is to constrain the CMB inflationary signal, specifically the primordial B-modes.","LiteBIRD targets the CMB B-mode signal on large angular scales, where the primordial inflationary signal is expected to dominate, with the goal of reaching a tensor-to-scalar ratio sensitivity of $\\sigma_r\\sim0.001$. LiteBIRD frequency bands will be split among three telescopes, with some overlap between telescopes for better control of systematic effects.","Here we report on the development status of the detector arrays for the Low Frequency Telescope (LFT), which spans the 34-161 GHz range, with 12 bands subdivided between four types of trichroic pixels consisting of lenslet-coupled sinuous antennas.","The signal from the antenna is bandpass filtered and sensed by AlMn Transition-Edge Sensors (TES).","We provide an update on the status of the design and development of LiteBIRD's LFT LF1 (40-60-78 GHz), LF2 (50-68-89 GHz) pixels.","We discuss design choices motivated by LiteBIRD scientific goals.","In particular we focus on the details of the optimization of the design parameters of the sinuous antenna, on-chip bandpass filters, cross-under and impedance transformers and all the RF components that define the LF1 and LF2 pixel detection chain.","We present this work in the context of the technical challenges and physical constraints imposed by the finite size of the instrument."],"url":"http://arxiv.org/abs/2405.19455v1","category":"astro-ph.IM"}
{"created":"2024-05-29 19:02:57","title":"Gaitor: Learning a Unified Representation Across Gaits for Real-World Quadruped Locomotion","abstract":"The current state-of-the-art in quadruped locomotion is able to produce robust motion for terrain traversal but requires the segmentation of a desired robot trajectory into a discrete set of locomotion skills such as trot and crawl. In contrast, in this work we demonstrate the feasibility of learning a single, unified representation for quadruped locomotion enabling continuous blending between gait types and characteristics. We present Gaitor, which learns a disentangled representation of locomotion skills, thereby sharing information common to all gait types seen during training. The structure emerging in the learnt representation is interpretable in that it is found to encode phase correlations between the different gait types. These can be leveraged to produce continuous gait transitions. In addition, foot swing characteristics are disentangled and directly addressable. Together with a rudimentary terrain encoding and a learned planner operating in this structured latent representation, Gaitor is able to take motion commands including desired gait type and characteristics from a user while reacting to uneven terrain. We evaluate Gaitor in both simulated and real-world settings on the ANYmal C platform. To the best of our knowledge, this is the first work learning such a unified and interpretable latent representation for multiple gaits, resulting in on-demand continuous blending between different locomotion modes on a real quadruped robot.","sentences":["The current state-of-the-art in quadruped locomotion is able to produce robust motion for terrain traversal but requires the segmentation of a desired robot trajectory into a discrete set of locomotion skills such as trot and crawl.","In contrast, in this work we demonstrate the feasibility of learning a single, unified representation for quadruped locomotion enabling continuous blending between gait types and characteristics.","We present Gaitor, which learns a disentangled representation of locomotion skills, thereby sharing information common to all gait types seen during training.","The structure emerging in the learnt representation is interpretable in that it is found to encode phase correlations between the different gait types.","These can be leveraged to produce continuous gait transitions.","In addition, foot swing characteristics are disentangled and directly addressable.","Together with a rudimentary terrain encoding and a learned planner operating in this structured latent representation, Gaitor is able to take motion commands including desired gait type and characteristics from a user while reacting to uneven terrain.","We evaluate Gaitor in both simulated and real-world settings on the ANYmal C platform.","To the best of our knowledge, this is the first work learning such a unified and interpretable latent representation for multiple gaits, resulting in on-demand continuous blending between different locomotion modes on a real quadruped robot."],"url":"http://arxiv.org/abs/2405.19452v1","category":"cs.RO"}
{"created":"2024-05-29 18:43:25","title":"Nanometer scale difference in myofilament lattice structure of muscle alter muscle function in a spatially explicit model","abstract":"Crossbridge binding and force in active muscle is dependent on the radial spacing between the thick and the thin filaments. This radial lattice spacing has been shown through spatially explicit modeling and experimental efforts to greatly affect isometric, force production in muscle. It has recently been suggested that this radial spacing might also be able to drive differences in mechanical function, or net work, under dynamic oscillations like those which occur in muscles in vivo. However, previous spatially explicit models either had no radial spacing dependence, meaning the lattice spacing could not be investigated, or did include radial spacing dependence but could not reproduce in vivo net work during dynamic oscillations and only investigated isometric contractions. Here we show the first spatially explicit model to include radial crossbridge dependence which can produce mechanical function similar to real muscle. Using this spatially explicit model of a half sarcomere, we show that when oscillated at strain amplitudes and frequencies like those in the hawk moth Manduca sexta, net work does depend on the lattice spacing. In addition, we can prescribe a trajectory of lattice spacing changes in the spatially explicit half sarcomere model and investigate the extent to which the time course of lattice spacing changes can affect mechanical function. We simulated a half sarcomere undergoing dynamic oscillations and prescribed the Poisson's ratio of the lattice to be either 0 (constant lattice spacing) or 0.5 (isovolumetric lattice spacing changes). We also simulated net work using lattice spacing data taken from Manduca sexta which has a variable Poisson's ratio. Our simulation indicates that the lattice spacing can change the mechanical function of muscle, and that in some cases a 1 nm difference can switch the net work of the half sarcomere model from motor like to brake like.","sentences":["Crossbridge binding and force in active muscle is dependent on the radial spacing between the thick and the thin filaments.","This radial lattice spacing has been shown through spatially explicit modeling and experimental efforts to greatly affect isometric, force production in muscle.","It has recently been suggested that this radial spacing might also be able to drive differences in mechanical function, or net work, under dynamic oscillations like those which occur in muscles in vivo.","However, previous spatially explicit models either had no radial spacing dependence, meaning the lattice spacing could not be investigated, or did include radial spacing dependence but could not reproduce in vivo net work during dynamic oscillations and only investigated isometric contractions.","Here we show the first spatially explicit model to include radial crossbridge dependence which can produce mechanical function similar to real muscle.","Using this spatially explicit model of a half sarcomere, we show that when oscillated at strain amplitudes and frequencies like those in the hawk moth Manduca sexta, net work does depend on the lattice spacing.","In addition, we can prescribe a trajectory of lattice spacing changes in the spatially explicit half sarcomere model and investigate the extent to which the time course of lattice spacing changes can affect mechanical function.","We simulated a half sarcomere undergoing dynamic oscillations and prescribed the Poisson's ratio of the lattice to be either 0","(constant lattice spacing) or 0.5 (isovolumetric lattice spacing changes).","We also simulated net work using lattice spacing data taken from Manduca sexta which has a variable Poisson's ratio.","Our simulation indicates that the lattice spacing can change the mechanical function of muscle, and that in some cases a 1 nm difference can switch the net work of the half sarcomere model from motor like to brake like."],"url":"http://arxiv.org/abs/2405.19443v1","category":"physics.bio-ph"}
{"created":"2024-05-29 18:40:11","title":"Large-scale DSM registration via motion averaging","abstract":"Generating wide-area digital surface models (DSMs) requires registering a large number of individual, and partially overlapped DSMs. This presents a challenging problem for a typical registration algorithm, since when a large number of observations from these multiple DSMs are considered, it may easily cause memory overflow. Sequential registration algorithms, although can significantly reduce the computation, are especially vulnerable for small overlapped pairs, leading to a large error accumulation. In this work, we propose a novel solution that builds the DSM registration task as a motion averaging problem: pair-wise DSMs are registered to build a scene graph, with edges representing relative poses between DSMs. Specifically, based on the grid structure of the large DSM, the pair-wise registration is performed using a novel nearest neighbor search method. We show that the scene graph can be optimized via an extremely fast motion average algorithm with O(N) complexity (N refers to the number of images). Evaluation of high-resolution satellite-derived DSM demonstrates significant improvement in computation and accuracy.","sentences":["Generating wide-area digital surface models (DSMs) requires registering a large number of individual, and partially overlapped DSMs.","This presents a challenging problem for a typical registration algorithm, since when a large number of observations from these multiple DSMs are considered, it may easily cause memory overflow.","Sequential registration algorithms, although can significantly reduce the computation, are especially vulnerable for small overlapped pairs, leading to a large error accumulation.","In this work, we propose a novel solution that builds the DSM registration task as a motion averaging problem: pair-wise DSMs are registered to build a scene graph, with edges representing relative poses between DSMs.","Specifically, based on the grid structure of the large DSM, the pair-wise registration is performed using a novel nearest neighbor search method.","We show that the scene graph can be optimized via an extremely fast motion average algorithm with O(N) complexity (N refers to the number of images).","Evaluation of high-resolution satellite-derived DSM demonstrates significant improvement in computation and accuracy."],"url":"http://arxiv.org/abs/2405.19442v1","category":"cs.CV"}
{"created":"2024-05-29 18:32:52","title":"Towards an Autonomous Minimally Invasive Spinal Fixation Surgery Using a Concentric Tube Steerable Drilling Robot","abstract":"Towards performing a realistic autonomous minimally invasive spinal fixation procedure, in this paper, we introduce a unique robotic drilling system utilizing a concentric tube steerable drilling robot (CT-SDR) integrated with a seven degree-of-freedom robotic manipulator. The CT-SDR in integration with the robotic arm enables creating precise J-shape trajectories enabling access to the areas within the vertebral body that currently are not accessible utilizing existing rigid instruments. To ensure safety and accuracy of the autonomous drilling procedure, we also performed required calibration procedures. The performance of the proposed robotic system and the calibration steps were thoroughly evaluated by performing various drilling experiments on simulated Sawbone samples.","sentences":["Towards performing a realistic autonomous minimally invasive spinal fixation procedure, in this paper, we introduce a unique robotic drilling system utilizing a concentric tube steerable drilling robot (CT-SDR) integrated with a seven degree-of-freedom robotic manipulator.","The CT-SDR in integration with the robotic arm enables creating precise J-shape trajectories enabling access to the areas within the vertebral body that currently are not accessible utilizing existing rigid instruments.","To ensure safety and accuracy of the autonomous drilling procedure, we also performed required calibration procedures.","The performance of the proposed robotic system and the calibration steps were thoroughly evaluated by performing various drilling experiments on simulated Sawbone samples."],"url":"http://arxiv.org/abs/2405.19438v1","category":"cs.RO"}
{"created":"2024-05-29 18:21:49","title":"Traffic Modeling and Forecast based on Stochastic Cell-Automata and Distributed Fiber-Optic Sensing -- A Numerical Experiment","abstract":"This paper demonstrates accurate traffic modeling and forecast using stochastic cell-automata (CA) and distributed fiber-optic sensing (DFOS). Traffic congestion is a dominant issue in highways. To reduce congestion, real-time traffic control by short-term forecast is necessary. For achieving this, data assimilation using a stochastic CA model and DFOS is promising. Data assimilation with a CA enables us to model real-time traffic flow with simple processes even when rare or sudden events occur, which is challenging for usual machine learning-based methods. DFOS overcomes issues of conventional point sensors that have dead zones of observation. By estimating optimal model parameters that reproduce observed traffic flow in the simulation, future traffic flow is forecasted from the simulation. We propose an optimal model parameter estimation method using mean velocity as an extracted feature and the particle filter. In addition, an estimation methodology for the microscopic traffic situation is developed to set the initial condition of simulation for forecast in accordance with observation. The proposed methods are verified by simulation-based traffic flow. The simulation adopts the stochastic Nishinari-Fukui-Schadschneider model. The optimal model parameters are successfully derived from posterior probability distributions (PPDs) estimated from DFOS data. In contrast, those estimated from point sensors fail. The PPDs of model parameters also indicate that each parameter has different sensitivities to traffic flow. A traffic forecast up to 60 minutes later is carried out. Using optimal model parameters estimated from DFOS, the forecast error of mean velocity is approximately $\\pm$10 km/h (percentage error is 18%). The error attains half of it when conventional point sensors are used. We conclude that DFOS is a powerful technique for traffic modeling and short-term forecast.","sentences":["This paper demonstrates accurate traffic modeling and forecast using stochastic cell-automata (CA) and distributed fiber-optic sensing (DFOS).","Traffic congestion is a dominant issue in highways.","To reduce congestion, real-time traffic control by short-term forecast is necessary.","For achieving this, data assimilation using a stochastic CA model and DFOS is promising.","Data assimilation with a CA enables us to model real-time traffic flow with simple processes even when rare or sudden events occur, which is challenging for usual machine learning-based methods.","DFOS overcomes issues of conventional point sensors that have dead zones of observation.","By estimating optimal model parameters that reproduce observed traffic flow in the simulation, future traffic flow is forecasted from the simulation.","We propose an optimal model parameter estimation method using mean velocity as an extracted feature and the particle filter.","In addition, an estimation methodology for the microscopic traffic situation is developed to set the initial condition of simulation for forecast in accordance with observation.","The proposed methods are verified by simulation-based traffic flow.","The simulation adopts the stochastic Nishinari-Fukui-Schadschneider model.","The optimal model parameters are successfully derived from posterior probability distributions (PPDs) estimated from DFOS data.","In contrast, those estimated from point sensors fail.","The PPDs of model parameters also indicate that each parameter has different sensitivities to traffic flow.","A traffic forecast up to 60 minutes later is carried out.","Using optimal model parameters estimated from DFOS, the forecast error of mean velocity is approximately $\\pm$10 km/h (percentage error is 18%).","The error attains half of it when conventional point sensors are used.","We conclude that DFOS is a powerful technique for traffic modeling and short-term forecast."],"url":"http://arxiv.org/abs/2405.19436v1","category":"nlin.CG"}
{"created":"2024-05-29 18:00:21","title":"Safety through Permissibility: Shield Construction for Fast and Safe Reinforcement Learning","abstract":"Designing Reinforcement Learning (RL) solutions for real-life problems remains a significant challenge. A major area of concern is safety. \"Shielding\" is a popular technique to enforce safety in RL by turning user-defined safety specifications into safe agent behavior. However, these methods either suffer from extreme learning delays, demand extensive human effort in designing models and safe domains in the problem, or require pre-computation. In this paper, we propose a new permissibility-based framework to deal with safety and shield construction. Permissibility was originally designed for eliminating (non-permissible) actions that will not lead to an optimal solution to improve RL training efficiency. This paper shows that safety can be naturally incorporated into this framework, i.e. extending permissibility to include safety, and thereby we can achieve both safety and improved efficiency. Experimental evaluation using three standard RL applications shows the effectiveness of the approach.","sentences":["Designing Reinforcement Learning (RL) solutions for real-life problems remains a significant challenge.","A major area of concern is safety.","\"Shielding\" is a popular technique to enforce safety in RL by turning user-defined safety specifications into safe agent behavior.","However, these methods either suffer from extreme learning delays, demand extensive human effort in designing models and safe domains in the problem, or require pre-computation.","In this paper, we propose a new permissibility-based framework to deal with safety and shield construction.","Permissibility was originally designed for eliminating (non-permissible) actions that will not lead to an optimal solution to improve RL training efficiency.","This paper shows that safety can be naturally incorporated into this framework, i.e. extending permissibility to include safety, and thereby we can achieve both safety and improved efficiency.","Experimental evaluation using three standard RL applications shows the effectiveness of the approach."],"url":"http://arxiv.org/abs/2405.19414v1","category":"cs.LG"}
{"created":"2024-05-29 18:00:11","title":"A T-Duality of Non-Supersymmetric Heterotic Strings and an implication for Topological Modular Forms","abstract":"Motivated by recent developments connecting non-supersymmetric heterotic string theory to the theory of Topological Modular Forms (TMF), we show that the worldsheet theory with central charge $(17,\\frac{3}{2})$ obtained by fibering the $(E_8)_1 \\times (E_8)_1$ current algebra over the $\\mathcal{N}=(0,1)$ sigma model on $S^{1}$ with antiperiodic spin structure (such that the two $E_8$ factors are exchanged as we go around the circle), is continuously connected to the $(E_8)_2$ theory in the Gaiotto$-$Johnson-Freyd$-$Witten sense of going \"up and down the RG trajectories\". Combined with the work of Tachikawa and Yamashita, this furnishes a physical derivation of the fact that the $(E_8)_2$ theory corresponds to the unique nontrivial torsion element $[(E_8)_2]$ of $\\mathsf{TMF}^{31}$ with zero mod-2 elliptic genus.","sentences":["Motivated by recent developments connecting non-supersymmetric heterotic string theory to the theory of Topological Modular Forms (TMF), we show that the worldsheet theory with central charge $(17,\\frac{3}{2})$ obtained by fibering the $(E_8)_1 \\times (E_8)_1$ current algebra over the $\\mathcal{N}=(0,1)$ sigma model on $S^{1}$ with antiperiodic spin structure (such that the two $E_8$ factors are exchanged as we go around the circle), is continuously connected to the $(E_8)_2$ theory in the Gaiotto$-$Johnson-Freyd$-$Witten sense of going \"up and down the RG trajectories\".","Combined with the work of Tachikawa and Yamashita, this furnishes a physical derivation of the fact that the $(E_8)_2$ theory corresponds to the unique nontrivial torsion element $","[(E_8)_2]$ of $\\mathsf{TMF}^{31}$ with zero mod-2 elliptic genus."],"url":"http://arxiv.org/abs/2405.19409v1","category":"hep-th"}
{"created":"2024-05-29 18:00:04","title":"Parity-dependent state transfer for direct entanglement generation","abstract":"As quantum information technologies advance they face challenges in scaling and connectivity. In particular, two necessities remain independent of the technological implementation: the need for connectivity between distant qubits and the need for efficient generation of entanglement. Perfect State Transfer is a technique which realises the time optimal transfer of a quantum state between distant nodes of qubit lattices with only nearest-neighbour couplings, hence providing an important tool to improve device connectivity. Crucially, the transfer protocol results in effective parity-dependent non-local interactions, extending its utility to the efficient generation of entangled states. Here, we experimentally demonstrate Perfect State Transfer and the generation of multi-qubit entanglement on a chain of superconducting qubits. The system consists of six fixed-frequency transmon qubits connected by tunable couplers, where the couplings are controlled via parametric drives. By simultaneously activating all couplings and engineering their individual amplitudes and frequencies, we implement Perfect State Transfer on up to six qubits and observe the respective single-excitation dynamics for different initial states. We then apply the protocol in the presence of multiple excitations and verify its parity-dependent property, where the number of excitations within the chain controls the phase of the transferred state. Finally, we utilise this property to prepare a multi-qubit Greenberger-Horne-Zeilinger state using only a single transfer operation, demonstrating its application for efficient entanglement generation.","sentences":["As quantum information technologies advance they face challenges in scaling and connectivity.","In particular, two necessities remain independent of the technological implementation: the need for connectivity between distant qubits and the need for efficient generation of entanglement.","Perfect State Transfer is a technique which realises the time optimal transfer of a quantum state between distant nodes of qubit lattices with only nearest-neighbour couplings, hence providing an important tool to improve device connectivity.","Crucially, the transfer protocol results in effective parity-dependent non-local interactions, extending its utility to the efficient generation of entangled states.","Here, we experimentally demonstrate Perfect State Transfer and the generation of multi-qubit entanglement on a chain of superconducting qubits.","The system consists of six fixed-frequency transmon qubits connected by tunable couplers, where the couplings are controlled via parametric drives.","By simultaneously activating all couplings and engineering their individual amplitudes and frequencies, we implement Perfect State Transfer on up to six qubits and observe the respective single-excitation dynamics for different initial states.","We then apply the protocol in the presence of multiple excitations and verify its parity-dependent property, where the number of excitations within the chain controls the phase of the transferred state.","Finally, we utilise this property to prepare a multi-qubit Greenberger-Horne-Zeilinger state using only a single transfer operation, demonstrating its application for efficient entanglement generation."],"url":"http://arxiv.org/abs/2405.19408v1","category":"quant-ph"}
{"created":"2024-05-29 18:00:02","title":"Tempered Multifidelity Importance Sampling for Gravitational Wave Parameter Estimation","abstract":"Estimating the parameters of compact binaries which coalesce and produce gravitational waves is a challenging Bayesian inverse problem. Gravitational-wave parameter estimation lies within the class of multifidelity problems, where a variety of models with differing assumptions, levels of fidelity, and computational cost are available for use in inference. In an effort to accelerate the solution of a Bayesian inverse problem, cheaper surrogates for the best models may be used to reduce the cost of likelihood evaluations when sampling the posterior. Importance sampling can then be used to reweight these samples to represent the true target posterior, incurring a reduction in the effective sample size. In cases when the problem is high dimensional, or when the surrogate model produces a poor approximation of the true posterior, this reduction in effective samples can be dramatic and render multifidelity importance sampling ineffective. We propose a novel method of tempered multifidelity importance sampling in order to remedy this issue. With this method the biasing distribution produced by the low-fidelity model is tempered, allowing for potentially better overlap with the target distribution. There is an optimal temperature which maximizes the efficiency in this setting, and we propose a low-cost strategy for approximating this optimal temperature using samples from the untempered distribution. In this paper, we motivate this method by applying it to Gaussian target and biasing distributions. Finally, we apply it to a series of problems in gravitational wave parameter estimation and demonstrate improved efficiencies when applying the method to real gravitational wave detections.","sentences":["Estimating the parameters of compact binaries which coalesce and produce gravitational waves is a challenging Bayesian inverse problem.","Gravitational-wave parameter estimation lies within the class of multifidelity problems, where a variety of models with differing assumptions, levels of fidelity, and computational cost are available for use in inference.","In an effort to accelerate the solution of a Bayesian inverse problem, cheaper surrogates for the best models may be used to reduce the cost of likelihood evaluations when sampling the posterior.","Importance sampling can then be used to reweight these samples to represent the true target posterior, incurring a reduction in the effective sample size.","In cases when the problem is high dimensional, or when the surrogate model produces a poor approximation of the true posterior, this reduction in effective samples can be dramatic and render multifidelity importance sampling ineffective.","We propose a novel method of tempered multifidelity importance sampling in order to remedy this issue.","With this method the biasing distribution produced by the low-fidelity model is tempered, allowing for potentially better overlap with the target distribution.","There is an optimal temperature which maximizes the efficiency in this setting, and we propose a low-cost strategy for approximating this optimal temperature using samples from the untempered distribution.","In this paper, we motivate this method by applying it to Gaussian target and biasing distributions.","Finally, we apply it to a series of problems in gravitational wave parameter estimation and demonstrate improved efficiencies when applying the method to real gravitational wave detections."],"url":"http://arxiv.org/abs/2405.19407v1","category":"gr-qc"}
{"created":"2024-05-30 17:58:28","title":"Did Binary Neutron Star Merger GW170817 Leave Behind A Long-lived Neutron Star?","abstract":"We consider the observational implications of the binary neutron star (BNS) merger GW170817 leaving behind a rapidly rotating massive neutron star that launches a relativistic, equatorial outflow as well as a jet. We show that if the equatorial outflow (ring) is highly beamed in the equatorial plane, its luminosity can be \"hidden\" from view until late times, even if carrying a significant fraction of the spin-down energy of the merger remnant. This hidden ring reveals itself as a re-brightening in the light curve once it slows down enough for Earth to be within the ring's relativistic beaming solid angle. We compute semi-analytic light curves using this model and find they are in agreement with the observations thus far, and we provide predictions for the ensuing afterglow.","sentences":["We consider the observational implications of the binary neutron star (BNS) merger GW170817 leaving behind a rapidly rotating massive neutron star that launches a relativistic, equatorial outflow as well as a jet.","We show that if the equatorial outflow (ring) is highly beamed in the equatorial plane, its luminosity can be \"hidden\" from view until late times, even if carrying a significant fraction of the spin-down energy of the merger remnant.","This hidden ring reveals itself as a re-brightening in the light curve once it slows down enough for Earth to be within the ring's relativistic beaming solid angle.","We compute semi-analytic light curves using this model and find they are in agreement with the observations thus far, and we provide predictions for the ensuing afterglow."],"url":"http://arxiv.org/abs/2405.20329v1","category":"astro-ph.HE"}
{"created":"2024-05-30 17:57:30","title":"MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion","abstract":"Despite impressive advancements in diffusion-based video editing models in altering video attributes, there has been limited exploration into modifying motion information while preserving the original protagonist's appearance and background. In this paper, we propose MotionFollower, a lightweight score-guided diffusion model for video motion editing. To introduce conditional controls to the denoising process, MotionFollower leverages two of our proposed lightweight signal controllers, one for poses and the other for appearances, both of which consist of convolution blocks without involving heavy attention calculations. Further, we design a score guidance principle based on a two-branch architecture, including the reconstruction and editing branches, which significantly enhance the modeling capability of texture details and complicated backgrounds. Concretely, we enforce several consistency regularizers and losses during the score estimation. The resulting gradients thus inject appropriate guidance to the intermediate latents, forcing the model to preserve the original background details and protagonists' appearances without interfering with the motion modification. Experiments demonstrate the competitive motion editing ability of MotionFollower qualitatively and quantitatively. Compared with MotionEditor, the most advanced motion editing model, MotionFollower achieves an approximately 80% reduction in GPU memory while delivering superior motion editing performance and exclusively supporting large camera movements and actions.","sentences":["Despite impressive advancements in diffusion-based video editing models in altering video attributes, there has been limited exploration into modifying motion information while preserving the original protagonist's appearance and background.","In this paper, we propose MotionFollower, a lightweight score-guided diffusion model for video motion editing.","To introduce conditional controls to the denoising process, MotionFollower leverages two of our proposed lightweight signal controllers, one for poses and the other for appearances, both of which consist of convolution blocks without involving heavy attention calculations.","Further, we design a score guidance principle based on a two-branch architecture, including the reconstruction and editing branches, which significantly enhance the modeling capability of texture details and complicated backgrounds.","Concretely, we enforce several consistency regularizers and losses during the score estimation.","The resulting gradients thus inject appropriate guidance to the intermediate latents, forcing the model to preserve the original background details and protagonists' appearances without interfering with the motion modification.","Experiments demonstrate the competitive motion editing ability of MotionFollower qualitatively and quantitatively.","Compared with MotionEditor, the most advanced motion editing model, MotionFollower achieves an approximately 80% reduction in GPU memory while delivering superior motion editing performance and exclusively supporting large camera movements and actions."],"url":"http://arxiv.org/abs/2405.20325v1","category":"cs.CV"}
{"created":"2024-05-30 17:50:10","title":"A consistency test of the cosmological model at the epoch of recombination using DESI BAO and Planck measurements","abstract":"The value of the Hubble constant determined from CMB and BAO measurements is directly dependent on the sound horizon at the photon-baryon decoupling. There has been significant interest in the possibility of new physics at the epoch around recombination that could reduce the sound horizon and increase the inferred value of $H_0$, thus helping to relieve the Hubble tension. One way to determine if new physics is required would be to measure $H_0$ from BAO and CMB without assuming any model for computing the sound horizon. In this study, we use the recently released DESI Year 1 BAO data combined with the CMB acoustic scale $\\theta_\\star$ and the Planck $\\Lambda$CDM prior on $\\Omega_{\\rm m} h^2$ to determine $H_0$ while treating the sound horizon at baryon decoupling $r_{\\rm d}$ as a free parameter. We find $H_0=69.88 \\pm 0.93$ km/s/Mpc, which is $\\sim2\\sigma$ larger than $H_0 = 67.44 \\pm 0.47$ km/s/Mpc in the Planck-best-fit $\\Lambda$CDM where $r_{\\rm d}$ is derived using the standard recombination model. For comparison, we perform the same analysis using the pre-DESI BAO data with $\\theta_\\star$ and the same prior on $\\Omega_{\\rm m} h^2$, finding $H_0= 67.37 \\pm 0.96$ km/s/Mpc. This difference derives from the notably larger value of the product $r_{\\rm d}H_0$ measured by DESI. Future BAO data from DESI will help determine if the cosmological model at the epoch of recombination model requires a modification.","sentences":["The value of the Hubble constant determined from CMB and BAO measurements is directly dependent on the sound horizon at the photon-baryon decoupling.","There has been significant interest in the possibility of new physics at the epoch around recombination that could reduce the sound horizon and increase the inferred value of $H_0$, thus helping to relieve the Hubble tension.","One way to determine if new physics is required would be to measure $H_0$ from BAO and CMB without assuming any model for computing the sound horizon.","In this study, we use the recently released DESI Year 1 BAO data combined with the CMB acoustic scale $\\theta_\\star$ and the Planck $\\Lambda$CDM prior on $\\Omega_{\\rm m} h^2$ to determine $H_0$ while treating the sound horizon at baryon decoupling $r_{\\rm d}$ as a free parameter.","We find $H_0=69.88 \\pm 0.93$ km/s/Mpc, which is $\\sim2\\sigma$ larger than $H_0 = 67.44 \\pm 0.47$ km/s/Mpc in the Planck-best-fit $\\Lambda$CDM where $r_{\\rm d}$ is derived using the standard recombination model.","For comparison, we perform the same analysis using the pre-DESI BAO data with $\\theta_\\star$ and the same prior on $\\Omega_{\\rm m} h^2$, finding $H_0= 67.37 \\pm 0.96$ km/s/Mpc.","This difference derives from the notably larger value of the product $r_{\\rm d}H_0$ measured by DESI.","Future BAO data from DESI will help determine if the cosmological model at the epoch of recombination model requires a modification."],"url":"http://arxiv.org/abs/2405.20306v1","category":"astro-ph.CO"}
{"created":"2024-05-30 17:42:17","title":"All-Loop Geometry for Four-Point Correlation Function","abstract":"In this letter, we consider a positive geometry conjectured to encode the loop integrand of four-point stress-energy correlators in planar $\\mathcal{N}=4$ super Yang-Mills. Beginning with four lines in twistor space, we characterize a positive subspace to which an $\\ell$-loop geometry is attached. The loop geometry then consists of $\\ell$ lines in twistor space satisfying positivity conditions among themselves and with respect to the base. Consequently, the \\textit{loop geometry} can be viewed as fibration over a \\textit{tree geometry}. The fibration naturally dissects the base into chambers, in which the degree-$4 \\ell$ loop form is unique and distinct for each chamber. Interestingly, up to three loops, the chambers are simply organized by the six ordering of $x^2_{12}x^2_{34}$, $x^2_{14}x^2_{23}$ and $x^2_{13}x^2_{24}$. We explicitly verify our conjecture by computing the loop-forms in terms of a basis of planar conformal integrals up to $\\ell=3$, which indeed yield correct loop integrands for the four-point correlator.","sentences":["In this letter, we consider a positive geometry conjectured to encode the loop integrand of four-point stress-energy correlators in planar $\\mathcal{N}=4$ super Yang-Mills.","Beginning with four lines in twistor space, we characterize a positive subspace to which an $\\ell$-loop geometry is attached.","The loop geometry then consists of $\\ell$ lines in twistor space satisfying positivity conditions among themselves and with respect to the base.","Consequently, the \\textit{loop geometry} can be viewed as fibration over a \\textit{tree geometry}.","The fibration naturally dissects the base into chambers, in which the degree-$4 \\ell$ loop form is unique and distinct for each chamber.","Interestingly, up to three loops, the chambers are simply organized by the six ordering of $x^2_{12}x^2_{34}$, $x^2_{14}x^2_{23}$ and $x^2_{13}x^2_{24}$.","We explicitly verify our conjecture by computing the loop-forms in terms of a basis of planar conformal integrals up to $\\ell=3$, which indeed yield correct loop integrands for the four-point correlator."],"url":"http://arxiv.org/abs/2405.20292v1","category":"hep-th"}
{"created":"2024-05-30 17:40:54","title":"Fermi-Level Pinning of Yu-Shiba-Rusinov States in Weak Magnetic Field","abstract":"As is well known, magnetic impurities adsorbed on superconductors, e.g. of the s-wave type, can introduce a bound gap-state (Yu-Shiba-Rusinov resonance). We here investigate within a minimal model how the impurity moment arranges with respect to a weak homogeneous external magnetic field employing a fully self-consistent mean-field treatment. Our investigation reveals a critical window of impurity-to-substrate coupling constants, $J$. The width of the critical region, $\\delta J$, scales like $\\delta J \\sim B/\\Delta$, where $B$ is the magnitude of the external field and $\\Delta$ is the bulk order parameter. While tuning $J$ through the window, the energy of the Yu-Shiba-Rusinov (YSR) resonance is pinned to the Fermi energy $\\varepsilon_\\text{F}$ and the impurity moment rotates in a continuous fashion. We emphasize the pivotal role of self-consistency: In treatments ignoring self-consistency, the critical window adopts zero width, $\\delta J=0$; consequently, there is no pinning of the YSR-resonance to $\\varepsilon_\\text{F}$, the impurity orientation jumps and therefore this orientation cannot be controlled continuously by fine-tuning the coupling $J$. In this sense, our study highlights the significance of self-consistency for understanding intricate magnetic interactions between superconductive materials and Shiba chains.","sentences":["As is well known, magnetic impurities adsorbed on superconductors, e.g. of the s-wave type, can introduce a bound gap-state (Yu-Shiba-Rusinov resonance).","We here investigate within a minimal model how the impurity moment arranges with respect to a weak homogeneous external magnetic field employing a fully self-consistent mean-field treatment.","Our investigation reveals a critical window of impurity-to-substrate coupling constants, $J$. The width of the critical region, $\\delta J$, scales like $\\delta J \\sim B/\\Delta$, where $B$ is the magnitude of the external field and $\\Delta$ is the bulk order parameter.","While tuning $J$ through the window, the energy of the Yu-Shiba-Rusinov (YSR) resonance is pinned to the Fermi energy $\\varepsilon_\\text{F}$ and the impurity moment rotates in a continuous fashion.","We emphasize the pivotal role of self-consistency: In treatments ignoring self-consistency, the critical window adopts zero width, $\\delta J=0$; consequently, there is no pinning of the YSR-resonance to $\\varepsilon_\\text{F}$, the impurity orientation jumps and therefore this orientation cannot be controlled continuously by fine-tuning the coupling $J$. In this sense, our study highlights the significance of self-consistency for understanding intricate magnetic interactions between superconductive materials and Shiba chains."],"url":"http://arxiv.org/abs/2405.20290v1","category":"cond-mat.supr-con"}
{"created":"2024-05-30 17:29:30","title":"Enhanced Duality in 4D Supergravity","abstract":"U-duality imposes strong constraints on the structure of divergences in supergravity. But Gaillard-Zumino Sp$(2n_v, \\mathbb{R})$ duality in 4D, where $n_v$ is the number of vectors, has more symmetries than U-duality. For example, in maximal supergravity, dimension of Sp(56) is 1596, whereas its U-duality subgroup $E_{7(7)}$, has dimension 133. In comparison, D $> 4$ maximal dualities are U-dualities. We argue that the extra dualities in 4D, enhancing U-duality, determine the properties of perturbative quantum supergravity. The presence/absence of enhanced dualities suggests a possible explanation of known amplitude loop computations in D-dimensional supergravities.","sentences":["U-duality imposes strong constraints on the structure of divergences in supergravity.","But Gaillard-Zumino Sp$(2n_v, \\mathbb{R})$ duality in 4D, where $n_v$ is the number of vectors, has more symmetries than U-duality.","For example, in maximal supergravity, dimension of Sp(56) is 1596, whereas its U-duality subgroup $E_{7(7)}$, has dimension 133.","In comparison, D $> 4$ maximal dualities are U-dualities.","We argue that the extra dualities in 4D, enhancing U-duality, determine the properties of perturbative quantum supergravity.","The presence/absence of enhanced dualities suggests a possible explanation of known amplitude loop computations in D-dimensional supergravities."],"url":"http://arxiv.org/abs/2405.20275v1","category":"hep-th"}
{"created":"2024-05-30 16:13:49","title":"The energy shear of protohaloes","abstract":"As it collapses to form a halo, the shape of a protohalo patch is deformed by the initial shear field. This deformation is often modeled using the \"deformation\" tensor, constructed from second derivatives of the gravitational potential, whose trace gives the initial overdensity. However, especially for lower mass protohalos, this matrix is not always positive definite: one of its eigenvalues has a different sign from the others. We show that the evolution of a patch is better described by the \"energy shear\" tensor, which is positive definite and plays a direct role in the evolution. This positive-definiteness simplifies models of halo abundances, assembly and of the cosmic web.","sentences":["As it collapses to form a halo, the shape of a protohalo patch is deformed by the initial shear field.","This deformation is often modeled using the \"deformation\" tensor, constructed from second derivatives of the gravitational potential, whose trace gives the initial overdensity.","However, especially for lower mass protohalos, this matrix is not always positive definite: one of its eigenvalues has a different sign from the others.","We show that the evolution of a patch is better described by the \"energy shear\" tensor, which is positive definite and plays a direct role in the evolution.","This positive-definiteness simplifies models of halo abundances, assembly and of the cosmic web."],"url":"http://arxiv.org/abs/2405.20207v1","category":"astro-ph.CO"}
{"created":"2024-05-30 15:23:58","title":"A theoretical study of the dissociative recombination of SH$^+$ with electrons through the $^2\u03a0$ states of SH","abstract":"A quantitative theoretical study of the dissociative recombination of SH$^+$ with electrons has been carried out. Multireference, configuration interaction calculations were used to determine accurate potential energy curves for SH$^+$ and SH. The block diagonalization method was used to disentangle strongly interacting SH valence and Rydberg states and to construct a diabatic Hamiltonian whose diagonal matrix elements provide the diabatic potential energy curves. The off-diagonal elements are related to the electronic valence-Rydberg couplings. Cross sections and rate coefficients for the dissociative recombination reaction were calculated with a step-wise version of the multichannel quantum defect theory, using the molecular data provided by the block diagonalization method. The calculated rates are compared with the most recent measurements performed on the TSR ion storage ring in Heidelberg, Germany.","sentences":["A quantitative theoretical study of the dissociative recombination of SH$^+$ with electrons has been carried out.","Multireference, configuration interaction calculations were used to determine accurate potential energy curves for SH$^+$ and SH.","The block diagonalization method was used to disentangle strongly interacting SH valence and Rydberg states and to construct a diabatic Hamiltonian whose diagonal matrix elements provide the diabatic potential energy curves.","The off-diagonal elements are related to the electronic valence-Rydberg couplings.","Cross sections and rate coefficients for the dissociative recombination reaction were calculated with a step-wise version of the multichannel quantum defect theory, using the molecular data provided by the block diagonalization method.","The calculated rates are compared with the most recent measurements performed on the TSR ion storage ring in Heidelberg, Germany."],"url":"http://arxiv.org/abs/2405.20147v1","category":"astro-ph.IM"}
{"created":"2024-05-30 15:23:44","title":"Supermassive black holes and very high-energy neutrinos: the case of NGC 1068","abstract":"We present a comprehensive multi-messenger study of NGC 1068, the prototype Seyfert II galaxy recently associated with high-energy IceCube neutrinos. Various aspects of the source, including its nuclear activity, jet, outflow, and starburst region, are analyzed in detail using a multi-wavelength approach and relevant luminosities are derived. We then explore its gamma-ray and neutrino emissions and investigate potential mechanisms underlying these phenomena and their relations with the different astrophysical components to try to understand which one is responsible for the IceCube neutrinos. By first using simple order-of-magnitude arguments and then applying specific theoretical models, we infer that only the region close to the accretion disc around the supermassive black hole has both the right density of X-ray photons needed to provide the targets for protons to sustain neutrino production and of optical/infrared photons required to absorb the associated but unobserved gamma rays. We conclude by highlighting ongoing efforts to constrain a possible broad connection between neutrinos and active galactic nuclei, as well as future synergies between astronomical and neutrino facilities.","sentences":["We present a comprehensive multi-messenger study of NGC 1068, the prototype Seyfert II galaxy recently associated with high-energy IceCube neutrinos.","Various aspects of the source, including its nuclear activity, jet, outflow, and starburst region, are analyzed in detail using a multi-wavelength approach and relevant luminosities are derived.","We then explore its gamma-ray and neutrino emissions and investigate potential mechanisms underlying these phenomena and their relations with the different astrophysical components to try to understand which one is responsible for the IceCube neutrinos.","By first using simple order-of-magnitude arguments and then applying specific theoretical models, we infer that only the region close to the accretion disc around the supermassive black hole has both the right density of X-ray photons needed to provide the targets for protons to sustain neutrino production and of optical/infrared photons required to absorb the associated but unobserved gamma rays.","We conclude by highlighting ongoing efforts to constrain a possible broad connection between neutrinos and active galactic nuclei, as well as future synergies between astronomical and neutrino facilities."],"url":"http://arxiv.org/abs/2405.20146v1","category":"astro-ph.HE"}
{"created":"2024-05-30 14:52:00","title":"Monogamy of nonlocality from multipartite information causality","abstract":"The monogamy of nonlocality is one the most intriguing and cryptographically significant predictions of quantum theory. The physical principle of information causality offers a promising means to understand and restrict the extent of nonlocality without invoking the abstract mathematical formalism of quantum theory. In this article, we demonstrate that the original bipartite formulation of information causality cannot imply non-trivial monogamy relations, thereby refuting the previous claims. Nevertheless, we show that the recently proposed multipartite formulation of information causality implies stronger-than-no-signaling monogamy relations. We use these monogamy relations to enhance the security of device-independent quantum key distribution against a no-signaling eavesdropper constrained by information causality.","sentences":["The monogamy of nonlocality is one the most intriguing and cryptographically significant predictions of quantum theory.","The physical principle of information causality offers a promising means to understand and restrict the extent of nonlocality without invoking the abstract mathematical formalism of quantum theory.","In this article, we demonstrate that the original bipartite formulation of information causality cannot imply non-trivial monogamy relations, thereby refuting the previous claims.","Nevertheless, we show that the recently proposed multipartite formulation of information causality implies stronger-than-no-signaling monogamy relations.","We use these monogamy relations to enhance the security of device-independent quantum key distribution against a no-signaling eavesdropper constrained by information causality."],"url":"http://arxiv.org/abs/2405.20115v1","category":"quant-ph"}
{"created":"2024-05-30 13:24:42","title":"Lepton flavor violation with tau leptons","abstract":"We review the status and importance of lepton flavor violation with tauons, focusing on overlooked flavor-breaking patterns as well as tau-flavor violation in nucleon decays.","sentences":["We review the status and importance of lepton flavor violation with tauons, focusing on overlooked flavor-breaking patterns as well as tau-flavor violation in nucleon decays."],"url":"http://arxiv.org/abs/2405.20043v1","category":"hep-ph"}
{"created":"2024-05-30 12:46:02","title":"Primary and secondary source of energy in the superluminous supernova 2018ibb","abstract":"We examine the pair-instability origin of superluminous supernova 2018ibb. As the base model, we use a non-rotating stellar model with an initial mass of 250 Msun at about 1/15 solar metallicity. We consider three versions of the model as input for radiative transfer simulations done with the STELLA and ARTIS codes: with 25 Msun of 56Ni, 34 Msun of 56Ni, and a chemically mixed case with 34 Msun of 56Ni. We present light curves and spectra in comparison to the observed data of SN 2018ibb, and conclude that the pair-instability supernova model with 34 Msun of 56Ni explains broad-band light curves reasonably well between -100 and 250 days around the peak. Our synthetic spectra have many similarities with the observed spectra. The luminosity excess in the light curves and the blue-flux excess in the spectra can be explained by an additional energy source, which may be interaction of the SN ejecta with circumstellar matter. We discuss possible mechanisms of the origin of the circumstellar matter being ejected in the decades before the pair-instability explosion.","sentences":["We examine the pair-instability origin of superluminous supernova 2018ibb.","As the base model, we use a non-rotating stellar model with an initial mass of 250 Msun at about 1/15 solar metallicity.","We consider three versions of the model as input for radiative transfer simulations done with the STELLA and ARTIS codes: with 25 Msun of 56Ni, 34 Msun of 56Ni, and a chemically mixed case with 34 Msun of 56Ni.","We present light curves and spectra in comparison to the observed data of SN 2018ibb, and conclude that the pair-instability supernova model with 34 Msun of 56Ni explains broad-band light curves reasonably well between -100 and 250 days around the peak.","Our synthetic spectra have many similarities with the observed spectra.","The luminosity excess in the light curves and the blue-flux excess in the spectra can be explained by an additional energy source, which may be interaction of the SN ejecta with circumstellar matter.","We discuss possible mechanisms of the origin of the circumstellar matter being ejected in the decades before the pair-instability explosion."],"url":"http://arxiv.org/abs/2405.20009v1","category":"astro-ph.HE"}
{"created":"2024-05-30 11:57:18","title":"Atomistic compositional details and their importance for spin qubits in isotope-purified silicon-germanium quantum wells","abstract":"Understanding crystal characteristics down to the atomistic level increasingly emerges as a crucial insight for creating solid state platforms for qubits with reproducible and homogeneous properties. Here, isotope composition depth profiles in a SiGe/$^{28}$Si/SiGe heterostructure are analyzed with atom probe tomography (APT) and time-of-flight secondary-ion mass spectrometry. Spin-echo dephasing times $T_2^{echo}=128 \\mu s$ and valley energy splittings around $200 \\mu eV$ have been observed for single spin qubits in this quantum well (QW) heterostructure, pointing towards the suppression of qubit decoherence through hyperfine interaction or via scattering between valley states. The concentration of nuclear spin-carrying $^{29}$Si is 50 ppm in the $^{28}$Si QW. APT allows to uncover that both the top SiGe/$^{28}$Si and the bottom $^{28}$Si/SiGe interfaces of the QW are shaped by epitaxial growth front segregation signatures on a few monolayer scale. A subsequent thermal treatment broadens the top interface by about two monolayers, while the width of the bottom interface remains unchanged. Using a tight-binding model including SiGe alloy disorder, these experimental results suggest that the combination of the slightly thermally broadened top interface and of a minimal Ge concentration of $0.3 \\%$ in the QW, resulting from segregation, is instrumental for the observed large valley splitting. Minimal Ge additions $< 1 \\%$, which get more likely in thin QWs, will hence support high valley splitting without compromising coherence times. At the same time, taking thermal treatments during device processing as well as the occurrence of crystal growth characteristics into account seems important for the design of reproducible qubit properties.","sentences":["Understanding crystal characteristics down to the atomistic level increasingly emerges as a crucial insight for creating solid state platforms for qubits with reproducible and homogeneous properties.","Here, isotope composition depth profiles in a SiGe/$^{28}$Si/SiGe heterostructure are analyzed with atom probe tomography (APT) and time-of-flight secondary-ion mass spectrometry.","Spin-echo dephasing times $T_2^{echo}=128 \\mu s$ and valley energy splittings around $200 \\mu eV$ have been observed for single spin qubits in this quantum well (QW) heterostructure, pointing towards the suppression of qubit decoherence through hyperfine interaction or via scattering between valley states.","The concentration of nuclear spin-carrying $^{29}$Si is 50 ppm in the $^{28}$Si QW.","APT allows to uncover that both the top SiGe/$^{28}$Si and the bottom $^{28}$Si/SiGe interfaces of the QW are shaped by epitaxial growth front segregation signatures on a few monolayer scale.","A subsequent thermal treatment broadens the top interface by about two monolayers, while the width of the bottom interface remains unchanged.","Using a tight-binding model including SiGe alloy disorder, these experimental results suggest that the combination of the slightly thermally broadened top interface and of a minimal Ge concentration of $0.3 \\%$ in the QW, resulting from segregation, is instrumental for the observed large valley splitting.","Minimal Ge additions $< 1 \\%$, which get more likely in thin QWs, will hence support high valley splitting without compromising coherence times.","At the same time, taking thermal treatments during device processing as well as the occurrence of crystal growth characteristics into account seems important for the design of reproducible qubit properties."],"url":"http://arxiv.org/abs/2405.19974v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-30 11:32:45","title":"Scanning tunneling spectroscopy of magnetic quantum impurities in two-dimensional magnets","abstract":"We study scanning tunneling spectroscopy (STS) of magnetic quantum impurities in (partially) polarized phases of two-dimensional (2D) van der Waals magnets. Focusing on the Kitaev-Heisenberg honeycomb model for transition metal trihalides, e.g., CrI$_3$ or $\\alpha$-RuCl$_3$, we consider adatom and substitutional impurity positions. Unlike the non-magnetic case, magnetic impurities can induce bound states below the magnon gap, whose energy one can measured by STS at finite bias voltage. Whenever the bound state energy vanishes as function of the magnetic field, we predict discontinuous local spin transitions which are expected to be ubiquitous in 2D magnets. They can be detected by STS in the zero-bias limit.","sentences":["We study scanning tunneling spectroscopy (STS) of magnetic quantum impurities in (partially) polarized phases of two-dimensional (2D) van der Waals magnets.","Focusing on the Kitaev-Heisenberg honeycomb model for transition metal trihalides, e.g., CrI$_3$ or $\\alpha$-RuCl$_3$, we consider adatom and substitutional impurity positions.","Unlike the non-magnetic case, magnetic impurities can induce bound states below the magnon gap, whose energy one can measured by STS at finite bias voltage.","Whenever the bound state energy vanishes as function of the magnetic field, we predict discontinuous local spin transitions which are expected to be ubiquitous in 2D magnets.","They can be detected by STS in the zero-bias limit."],"url":"http://arxiv.org/abs/2405.19962v1","category":"cond-mat.str-el"}
{"created":"2024-05-30 11:22:45","title":"Tau Physics Opportunities at the Super Tau-Charm Facility","abstract":"The super tau-charm facility will provide excellent conditions to perform a high-precision investigation of the tau-lepton properties: very high statistics, controllable systematics and low backgrounds. An overview of the broad physics program that could be addressed at this facility is presented.","sentences":["The super tau-charm facility will provide excellent conditions to perform a high-precision investigation of the tau-lepton properties: very high statistics, controllable systematics and low backgrounds.","An overview of the broad physics program that could be addressed at this facility is presented."],"url":"http://arxiv.org/abs/2405.19955v1","category":"hep-ph"}
{"created":"2024-05-30 10:44:32","title":"Adsorption of Mo and O at S-vacancy on ReS2 surface of ReS2/MoTe2 vdW heterointerface","abstract":"Applications like high density information storage, neuromorphic computing, nanophotonics, etc. require ultra-thin electronic devices which can be controlled with applied electric field. Of late, atomically thin two-dimensional (2D) materials and van der Waals (vdW) heterointerface of those have emerged as suitable candidates for such ultra-low power nanoelectric devices. In this work, employing density functional theory (DFT), the monolayer ReS2 / monolayer MoTe2 vdW heterostructure with Sulphur vacancy is studied to examine various ground state electronic properties. Changes in effective band gap owing to defect-induced states and modulation of the energy gap value with Molybdenum (Mo) and Oxygen (O) adsorption at the defect site are examined. Since two-dimensional (2D) material based nanoscaled devices exhibit promising switching between non-conducting and conducting states, determining the role of defect-induced states and the adsorption of atoms/molecules on surfaces is crucial. Here, a detailed theoretical study to determine surface properties and relative energetic stability of the vdW heterostructures is carried out. The charge re-distribution between the constituent layers is also analyzed by obtaining Electron Difference Density (EDD) for different heterointerfaces. Nonetheless, the efficacy of switching between non-conducting and conducting states is assessed based on adsorption energy of adatoms binding at the defect site.","sentences":["Applications like high density information storage, neuromorphic computing, nanophotonics, etc. require ultra-thin electronic devices which can be controlled with applied electric field.","Of late, atomically thin two-dimensional (2D) materials and van der Waals (vdW) heterointerface of those have emerged as suitable candidates for such ultra-low power nanoelectric devices.","In this work, employing density functional theory (DFT), the monolayer ReS2 / monolayer MoTe2 vdW heterostructure with Sulphur vacancy is studied to examine various ground state electronic properties.","Changes in effective band gap owing to defect-induced states and modulation of the energy gap value with Molybdenum (Mo) and Oxygen (O) adsorption at the defect site are examined.","Since two-dimensional (2D) material based nanoscaled devices exhibit promising switching between non-conducting and conducting states, determining the role of defect-induced states and the adsorption of atoms/molecules on surfaces is crucial.","Here, a detailed theoretical study to determine surface properties and relative energetic stability of the vdW heterostructures is carried out.","The charge re-distribution between the constituent layers is also analyzed by obtaining Electron Difference Density (EDD) for different heterointerfaces.","Nonetheless, the efficacy of switching between non-conducting and conducting states is assessed based on adsorption energy of adatoms binding at the defect site."],"url":"http://arxiv.org/abs/2405.19927v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 10:37:14","title":"Multi-line study of the radial extent of SiO, CS, and SiS in AGB envelopes","abstract":"The spatial distribution of molecules in AGB circumstellar envelopes is regulated by different processes. In the outer layers all molecules are destroyed due to the interaction with interstellar ultraviolet photons. Here we aim to characterize in a coherent and uniform way the radial extent of three molecules (SiO, CS, and SiS) in envelopes around AGB stars of O- and C-rich character, and to study their dependence with mass loss rate. To that purpose, we used the Yebes 40m and IRAM 30m telescopes to observe 7 M-type and 7 C-type AGB envelopes covering a wide range of mass loss rates (1e-7 - 1e-5 Msun/yr) in lines of SiO, CS, and SiS spanning a range of upper level energies of 2-130 K. We carried out excitation and radiative transfer calculations over a wide parameter space to characterize the molecular abundance and radial extent. A chi2 analysis indicates that the abundance is well constrained while the radial extent is more difficult to constrain. The radial extent increases with increasing envelope density, in agreement with previous observational findings. At high envelope densities, Mdot/vexp > 1e-6 (Msun/yr)/(km/s), the radial extent of SiO, CS, and SiS are similar, while at low envelope densities, Mdot/vexp < 1e-7 (Msun/yr)/(km/s), the radial extent differ among the three molecules, in agreement with theoretical expectations based on destruction due to photodissociation. At low envelope densities we find a sequence of increasing radial extent, SiS -> CS -> SiO. We also find a tentative dependence of the radial extent with the chemical type (O- or C-rich) of the star for SiO and CS. Interferometric observations and further investigation of the photodissociation of SiO, CS, and SiS should allow to clarify the situation on the relative photodissociation radius of SiO, CS, and SiS in AGB envelopes and the dependence with envelope density and C/O ratio.","sentences":["The spatial distribution of molecules in AGB circumstellar envelopes is regulated by different processes.","In the outer layers all molecules are destroyed due to the interaction with interstellar ultraviolet photons.","Here we aim to characterize in a coherent and uniform way the radial extent of three molecules (SiO, CS, and SiS) in envelopes around AGB stars of O- and C-rich character, and to study their dependence with mass loss rate.","To that purpose, we used the Yebes 40m and IRAM 30m telescopes to observe 7 M-type and 7 C-type AGB envelopes covering a wide range of mass loss rates (1e-7 - 1e-5 Msun/yr) in lines of SiO, CS, and SiS spanning a range of upper level energies of 2-130 K. We carried out excitation and radiative transfer calculations over a wide parameter space to characterize the molecular abundance and radial extent.","A chi2 analysis indicates that the abundance is well constrained while the radial extent is more difficult to constrain.","The radial extent increases with increasing envelope density, in agreement with previous observational findings.","At high envelope densities, Mdot/vexp > 1e-6 (Msun/yr)/(km/s), the radial extent of SiO, CS, and SiS are similar, while at low envelope densities, Mdot/vexp <","1e-7 (Msun/yr)/(km/s)",", the radial extent differ among the three molecules, in agreement with theoretical expectations based on destruction due to photodissociation.","At low envelope densities we find a sequence of increasing radial extent, SiS -> CS -> SiO.","We also find a tentative dependence of the radial extent with the chemical type (O- or C-rich) of the star for SiO and CS.","Interferometric observations and further investigation of the photodissociation of SiO, CS, and SiS should allow to clarify the situation on the relative photodissociation radius of SiO, CS, and SiS in AGB envelopes and the dependence with envelope density and C/O ratio."],"url":"http://arxiv.org/abs/2405.19922v1","category":"astro-ph.SR"}
{"created":"2024-05-30 10:32:59","title":"The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions","abstract":"We present the ARR2 prior, a joint prior over the auto-regressive components in Bayesian time-series models and their induced $R^2$. Compared to other priors designed for times-series models, the ARR2 prior allows for flexible and intuitive shrinkage. We derive the prior for pure auto-regressive models, and extend it to auto-regressive models with exogenous inputs, and state-space models. Through both simulations and real-world modelling exercises, we demonstrate the efficacy of the ARR2 prior in improving sparse and reliable inference, while showing greater inference quality and predictive performance than other shrinkage priors. An open-source implementation of the prior is provided.","sentences":["We present the ARR2 prior, a joint prior over the auto-regressive components in Bayesian time-series models and their induced $R^2$. Compared to other priors designed for times-series models, the ARR2 prior allows for flexible and intuitive shrinkage.","We derive the prior for pure auto-regressive models, and extend it to auto-regressive models with exogenous inputs, and state-space models.","Through both simulations and real-world modelling exercises, we demonstrate the efficacy of the ARR2 prior in improving sparse and reliable inference, while showing greater inference quality and predictive performance than other shrinkage priors.","An open-source implementation of the prior is provided."],"url":"http://arxiv.org/abs/2405.19920v1","category":"stat.CO"}
{"created":"2024-05-30 09:39:29","title":"Refractive index in the JUNO liquid scintillator","abstract":"In the field of rare event physics, it is common to have huge masses of organic liquid scintillator as detection medium. In particular, they are widely used to study neutrino properties or astrophysical neutrinos. Thanks to its safety properties (such as low toxicity and high flash point) and easy scalability, linear alkyl benzene is the most common solvent used to produce liquid scintillators for large mass experiments. The knowledge of the refractive index is a pivotal point to understand the detector response, as this quantity (and its wavelength dependence) affects the Cherenkov radiation and photon propagation in the medium. In this paper, we report the measurement of the refractive index of the JUNO liquid scintillator between 260-1064 nm performed with two different methods (an ellipsometer and a refractometer), with a sub percent level precision. In addition, we used an interferometer to measure the group velocity in the JUNO liquid scintillator and verify the expected value derived from the refractive index measurements.","sentences":["In the field of rare event physics, it is common to have huge masses of organic liquid scintillator as detection medium.","In particular, they are widely used to study neutrino properties or astrophysical neutrinos.","Thanks to its safety properties (such as low toxicity and high flash point) and easy scalability, linear alkyl benzene is the most common solvent used to produce liquid scintillators for large mass experiments.","The knowledge of the refractive index is a pivotal point to understand the detector response, as this quantity (and its wavelength dependence) affects the Cherenkov radiation and photon propagation in the medium.","In this paper, we report the measurement of the refractive index of the JUNO liquid scintillator between 260-1064 nm performed with two different methods (an ellipsometer and a refractometer), with a sub percent level precision.","In addition, we used an interferometer to measure the group velocity in the JUNO liquid scintillator and verify the expected value derived from the refractive index measurements."],"url":"http://arxiv.org/abs/2405.19879v1","category":"physics.ins-det"}
{"created":"2024-05-30 09:21:59","title":"Discontinuities of banana integrals in dispersion relation representation","abstract":"We derive the discontinuities of banana integrals using the dispersion relation iteratively. We find a series of identities between the parameterized discontinuities of banana integrals (p-DOBIs). Similar to elliptic integrals, these identities enable the reduction of various p-DOBIs to be a linear combination of some fundamental ones. We present a practical application of p-DOBIs for the massive three-loop case and establish the expression of generalized dispersion relation, which enables us to obtain the dispersion relation representation of arbitrary banana integrals. Moreover, we propose a hypothesis for generalized dispersion relation and p-DOBIs, which provides a simple way to calculate the discontinuities and transform dispersion relation representation to p-DOBIs.","sentences":["We derive the discontinuities of banana integrals using the dispersion relation iteratively.","We find a series of identities between the parameterized discontinuities of banana integrals (p-DOBIs).","Similar to elliptic integrals, these identities enable the reduction of various p-DOBIs to be a linear combination of some fundamental ones.","We present a practical application of p-DOBIs for the massive three-loop case and establish the expression of generalized dispersion relation, which enables us to obtain the dispersion relation representation of arbitrary banana integrals.","Moreover, we propose a hypothesis for generalized dispersion relation and p-DOBIs, which provides a simple way to calculate the discontinuities and transform dispersion relation representation to p-DOBIs."],"url":"http://arxiv.org/abs/2405.19868v1","category":"hep-ph"}
{"created":"2024-05-30 09:12:25","title":"Search for pair-produced vector-like quarks coupling to light quarks in the lepton plus jets final state using 13 TeV $pp$ collisions with the ATLAS detector","abstract":"A search is presented for the pair-production of heavy vector-like quarks (VLQs) that each decay into a $W$ boson and a light quark. This study focuses on events where one $W$ boson decays into leptons and the other into hadrons. The search analyzed 140 fb$^{-1}$ of $pp$ collision data with $\\sqrt{s} = 13$ TeV, recorded by the ATLAS detector from 2015 to 2018 during Run 2 of the Large Hadron Collider. The final state is characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum, multiple small-radius jets, and a single large-radius jet identified as originating from the hadronic decay of a boosted $W$ boson. With higher center-of-mass energy and integrated luminosity than in the Run 1 search, and improved analysis tools, this analysis excludes VLQs ($Q$) with masses below 1530 GeV at 95% CL for the branching ratio ${{\\cal B}(Q \\to Wq)} = 1$, an improvement of 840 GeV on the previous ATLAS limit.","sentences":["A search is presented for the pair-production of heavy vector-like quarks (VLQs) that each decay into a $W$ boson and a light quark.","This study focuses on events where one $W$ boson decays into leptons and the other into hadrons.","The search analyzed 140 fb$^{-1}$ of $pp$ collision data with $\\sqrt{s} = 13$ TeV, recorded by the ATLAS detector from 2015 to 2018 during Run 2 of the Large Hadron Collider.","The final state is characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum, multiple small-radius jets, and a single large-radius jet identified as originating from the hadronic decay of a boosted $W$ boson.","With higher center-of-mass energy and integrated luminosity than in the Run 1 search, and improved analysis tools, this analysis excludes VLQs ($Q$) with masses below 1530 GeV at 95% CL for the branching ratio ${{\\cal B}(Q \\to Wq)} = 1$, an improvement of 840 GeV on the previous ATLAS limit."],"url":"http://arxiv.org/abs/2405.19862v1","category":"hep-ex"}
{"created":"2024-05-30 09:10:21","title":"Quasi-isodynamic stellarators with low turbulence as fusion reactor candidates","abstract":"The stellarator is a type of fusion energy device that - if properly designed - could provide clean, safe, and abundant energy to the grid. To generate this energy, a stellarator must keep a hot mixture of charged particles (known as a plasma) sufficiently confined by using a fully shaped magnetic field. If this is achieved, the heat from fusion reactions within the plasma can be harvested as energy. We present a novel method for designing reactor-relevant stellarator magnetic fields, which combine several key physical properties. These include plasma stability, excellent confinement of the fast moving particles generated by fusion reactions, and reduction of the turbulence that is known to limit the performance of the most advanced stellarator experiment in the world, Wendelstein 7-X.","sentences":["The stellarator is a type of fusion energy device that - if properly designed - could provide clean, safe, and abundant energy to the grid.","To generate this energy, a stellarator must keep a hot mixture of charged particles (known as a plasma) sufficiently confined by using a fully shaped magnetic field.","If this is achieved, the heat from fusion reactions within the plasma can be harvested as energy.","We present a novel method for designing reactor-relevant stellarator magnetic fields, which combine several key physical properties.","These include plasma stability, excellent confinement of the fast moving particles generated by fusion reactions, and reduction of the turbulence that is known to limit the performance of the most advanced stellarator experiment in the world, Wendelstein 7-X."],"url":"http://arxiv.org/abs/2405.19860v1","category":"physics.plasm-ph"}
{"created":"2024-05-30 09:03:27","title":"Investigating $\u039b$ baryon production in p-Pb collisions in jets and underlying event using angular correlations","abstract":"First measurements of hadron(h)$-\\Lambda$ azimuthal angular correlations in p$-$Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV using the ALICE detector at the LHC are presented. These correlations are used to separate the production of associated $\\Lambda$ baryons into three different kinematic regions, namely those produced in the direction of the trigger particle (near-side), those produced in the opposite direction (away-side), and those whose production is uncorrelated with the jet-axis (underlying event). The per-trigger associated $\\Lambda$ yields in these regions are extracted, along with the near- and away-side azimuthal peak widths, and the results are studied as a function of associated particle $p_{\\rm T}$ and event multiplicity. Comparisons with the DPMJET event generator and previous measurements of the $\\phi(1020)$ meson are also made. The final results indicate that strangeness production in the highest multiplicity p$-$Pb collisions is enhanced relative to low multiplicity collisions in the jet-like regions, as well as the underlying event. The production of $\\Lambda$ relative to charged hadrons is also enhanced in the underlying event when compared to the jet-like regions. Additionally, the results hint that strange quark production in the away-side of the jet is modified by soft interactions with the underlying event.","sentences":["First measurements of hadron(h)$-\\Lambda$ azimuthal angular correlations in p$-$Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV using the ALICE detector at the LHC are presented.","These correlations are used to separate the production of associated $\\Lambda$ baryons into three different kinematic regions, namely those produced in the direction of the trigger particle (near-side), those produced in the opposite direction (away-side), and those whose production is uncorrelated with the jet-axis (underlying event).","The per-trigger associated $\\Lambda$ yields in these regions are extracted, along with the near- and away-side azimuthal peak widths, and the results are studied as a function of associated particle $p_{\\rm T}$ and event multiplicity.","Comparisons with the DPMJET event generator and previous measurements of the $\\phi(1020)$ meson are also made.","The final results indicate that strangeness production in the highest multiplicity p$-$Pb collisions is enhanced relative to low multiplicity collisions in the jet-like regions, as well as the underlying event.","The production of $\\Lambda$ relative to charged hadrons is also enhanced in the underlying event when compared to the jet-like regions.","Additionally, the results hint that strange quark production in the away-side of the jet is modified by soft interactions with the underlying event."],"url":"http://arxiv.org/abs/2405.19855v1","category":"nucl-ex"}
{"created":"2024-05-30 08:54:14","title":"Investigation of a high-entropy oxide photocatalyst for hydrogen generation by first-principles calculations coupled with experiments: Significance of electronegativity","abstract":"High-entropy oxides (HEOs), containing at least five principal cations, have recently emerged as promising photocatalysts for hydrogen production via water splitting. Despite their high potential, the impact of the cation mixtures on photocatalytic activity remains poorly understood. This study investigates the high-entropy photocatalyst TiZrHfNbTaO11 using first-principles calculations combined with experimental methods to elucidate the effects of various elements on electronic structure and water splitting performance. The results indicate that the HEO exhibits a bandgap comparable to TiO2 polymorphs rutile, brookite and anatase. Cations with lower electronegativity, such as hafnium and zirconium, provide the strongest water adsorption energy, serving as active sites for water adsorption. Additionally, the co-presence of highly electronegative cations like niobium and tantalum adjacent to hafnium and zirconium enhances charge transfer to water molecules, improving splitting efficiency. These findings suggest novel strategies for designing high-entropy photocatalysts by synergistic incorporating cations with different electronegativities.","sentences":["High-entropy oxides (HEOs), containing at least five principal cations, have recently emerged as promising photocatalysts for hydrogen production via water splitting.","Despite their high potential, the impact of the cation mixtures on photocatalytic activity remains poorly understood.","This study investigates the high-entropy photocatalyst TiZrHfNbTaO11 using first-principles calculations combined with experimental methods to elucidate the effects of various elements on electronic structure and water splitting performance.","The results indicate that the HEO exhibits a bandgap comparable to TiO2 polymorphs rutile, brookite and anatase.","Cations with lower electronegativity, such as hafnium and zirconium, provide the strongest water adsorption energy, serving as active sites for water adsorption.","Additionally, the co-presence of highly electronegative cations like niobium and tantalum adjacent to hafnium and zirconium enhances charge transfer to water molecules, improving splitting efficiency.","These findings suggest novel strategies for designing high-entropy photocatalysts by synergistic incorporating cations with different electronegativities."],"url":"http://arxiv.org/abs/2405.19847v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-30 08:19:53","title":"On minimal presentations of numerical monoids","abstract":"We consider the classical problem of determining the largest possible cardinality of a minimal presentation of a numerical monoid with given embedding dimension and multiplicity. Very few values of this cardinality are known. In addressing this problem, we apply tools from Hilbert functions and free resolutions of artinian standard graded algebras. This approach allows us to solve the problem in many cases and, at the same time, identify subtle difficulties in the remaining cases. As a by-product of our analysis, we deduce results for the corresponding problem for the type of a numerical monoid.","sentences":["We consider the classical problem of determining the largest possible cardinality of a minimal presentation of a numerical monoid with given embedding dimension and multiplicity.","Very few values of this cardinality are known.","In addressing this problem, we apply tools from Hilbert functions and free resolutions of artinian standard graded algebras.","This approach allows us to solve the problem in many cases and, at the same time, identify subtle difficulties in the remaining cases.","As a by-product of our analysis, we deduce results for the corresponding problem for the type of a numerical monoid."],"url":"http://arxiv.org/abs/2405.19810v1","category":"math.CO"}
{"created":"2024-05-30 08:10:51","title":"Lipschitz-free spaces over strongly countable-dimensional spaces and approximation properties","abstract":"Let $T$ be a compact, metrisable and strongly countable-dimensional topological space. Let $\\mathcal{M}^T$ be the set of all metrics $d$ on $T$ compatible with its topology, and equip $\\mathcal{M}^T$ with the topology of uniform convergence, where the metrics are regarded as functions on $T^2$. We prove that the set $\\mathcal{A}^{T,1}$ of metrics $d\\in\\mathcal{M}^T$ for which the Lipschitz-free space $\\mathcal{F}(T,d)$ has the metric approximation property is residual in $\\mathcal{M}^T$.","sentences":["Let $T$ be a compact, metrisable and strongly countable-dimensional topological space.","Let $\\mathcal{M}^T$ be the set of all metrics $d$ on $T$ compatible with its topology, and equip $\\mathcal{M}^T$ with the topology of uniform convergence, where the metrics are regarded as functions on $T^2$. We prove that the set $\\mathcal{A}^{T,1}$ of metrics $d\\in\\mathcal{M}^T$ for which the Lipschitz-free space $\\mathcal{F}(T,d)$ has the metric approximation property is residual in $\\mathcal{M}^T$."],"url":"http://arxiv.org/abs/2405.19800v1","category":"math.FA"}
{"created":"2024-05-30 08:01:18","title":"Low-energy collisions between electrons and BeD$^+$","abstract":"Multichannel quantum defect theory is applied in the treatment of the dissociative recombination and vibrational excitation processes for the BeD$^+$ ion in the twenty four vibrational levels of its ground electronic state ($\\textrm{X}\\,{^{1}\\Sigma^{+}},v_{i}^{+}=0\\ldots 23$). Three electronic symmetries of BeD$^{**}$ states (\\ensuremath{^{2}\\Pi}, \\ensuremath{^{2}\\Sigma^{+}}, and \\ensuremath{^{2}\\Delta}), are considered in the calculation of cross sections and the corresponding rate coefficients. The incident electron energy range is $10^{-5}$--2.7 eV and the electron temperature range is 100--5000~K. The vibrational dependence of these collisional processes is highlighted. The resulting data are useful in magnetic confinement fusion edge plasma modelling and spectroscopy, in devices with beryllium based main chamber materials, such as ITER and JET, and operating with the deuterium-tritium fuel mix. An extensive rate coefficients database is presented in graphical form and also by analytic fit functions whose parameters are tabulated in the supplementary material.","sentences":["Multichannel quantum defect theory is applied in the treatment of the dissociative recombination and vibrational excitation processes for the BeD$^+$ ion in the twenty four vibrational levels of its ground electronic state ($\\textrm{X}\\,{^{1}\\Sigma^{+}},v_{i}^{+}=0\\ldots 23$).","Three electronic symmetries of BeD$^{**}$ states (\\ensuremath{^{2}\\Pi}, \\ensuremath{^{2}\\Sigma^{+}}, and \\ensuremath{^{2}\\Delta}), are considered in the calculation of cross sections and the corresponding rate coefficients.","The incident electron energy range is $10^{-5}$--2.7 eV and the electron temperature range is 100--5000~","K. The vibrational dependence of these collisional processes is highlighted.","The resulting data are useful in magnetic confinement fusion edge plasma modelling and spectroscopy, in devices with beryllium based main chamber materials, such as ITER and JET, and operating with the deuterium-tritium fuel mix.","An extensive rate coefficients database is presented in graphical form and also by analytic fit functions whose parameters are tabulated in the supplementary material."],"url":"http://arxiv.org/abs/2405.19792v1","category":"physics.plasm-ph"}
{"created":"2024-05-30 07:39:50","title":"CPT, Majorana fermions, and particle physics beyond the Standard Model","abstract":"After reviewing charge conjugation and the CPT theorem, we define Majorana fermions and clarify the relationship of Majorana, Weyl, and Dirac fields. Appearance of Majorana fermions in various scenarios of physics beyond the Standard Model is discussed, including neutrino masses, baryon asymmetry of the universe, grand unified theories, and supersymmetry.","sentences":["After reviewing charge conjugation and the CPT theorem, we define Majorana fermions and clarify the relationship of Majorana, Weyl, and Dirac fields.","Appearance of Majorana fermions in various scenarios of physics beyond the Standard Model is discussed, including neutrino masses, baryon asymmetry of the universe, grand unified theories, and supersymmetry."],"url":"http://arxiv.org/abs/2405.19774v1","category":"hep-ph"}
{"created":"2024-05-30 07:31:43","title":"Measurement-Induced Phase Transition in Free Bosons","abstract":"The competition between quantum many-particle dynamics and continuous monitoring can lead to measurement-induced phase transitions (MIPTs). So far, MIPTs have been much explored in fermionic or spin systems. To examine the possibility of a MIPT in bosonic systems, we study the entanglement structure in continuously monitored free bosons with long-range couplings. When the measurement is local, we find that no MIPTs occur because the substantial entanglement generated by the long-range coupling overcome the entanglement destruction due to the measurement. In contrast, we show that the nonlocal measurement can efficiently suppress the entanglement generation, leading to a MIPT where the bipartite entanglement entropy exhibits the subvolume-to-area law transition as the measurement strength is increased. Possible experimental relevance to levitated nanoparticle arrays is also briefly discussed.","sentences":["The competition between quantum many-particle dynamics and continuous monitoring can lead to measurement-induced phase transitions (MIPTs).","So far, MIPTs have been much explored in fermionic or spin systems.","To examine the possibility of a MIPT in bosonic systems, we study the entanglement structure in continuously monitored free bosons with long-range couplings.","When the measurement is local, we find that no MIPTs occur because the substantial entanglement generated by the long-range coupling overcome the entanglement destruction due to the measurement.","In contrast, we show that the nonlocal measurement can efficiently suppress the entanglement generation, leading to a MIPT where the bipartite entanglement entropy exhibits the subvolume-to-area law transition as the measurement strength is increased.","Possible experimental relevance to levitated nanoparticle arrays is also briefly discussed."],"url":"http://arxiv.org/abs/2405.19768v1","category":"quant-ph"}
{"created":"2024-05-30 07:17:57","title":"The Kosmosis Use-Case of Crypto Rug Pull Detection and Prevention","abstract":"Current methods to prevent crypto asset fraud are based on the analysis of transaction graphs within blockchain networks. While effective for identifying transaction patterns indicative of fraud, it does not capture the semantics of transactions and is constrained to blockchain data. Consequently, preventive methods based on transaction graphs are inherently limited. In response to these limitations, we propose the Kosmosis approach, which aims to incrementally construct a knowledge graph as new blockchain and social media data become available. During construction, it aims to extract the semantics of transactions and connect blockchain addresses to their real-world entities by fusing blockchain and social media data in a knowledge graph. This enables novel preventive methods against rug pulls as a form of crypto asset fraud. To demonstrate the effectiveness and practical applicability of the Kosmosis approach, we examine a series of real-world rug pulls from 2021. Through this case, we illustrate how Kosmosis can aid in identifying and preventing such fraudulent activities by leveraging the insights from the constructed knowledge graph.","sentences":["Current methods to prevent crypto asset fraud are based on the analysis of transaction graphs within blockchain networks.","While effective for identifying transaction patterns indicative of fraud, it does not capture the semantics of transactions and is constrained to blockchain data.","Consequently, preventive methods based on transaction graphs are inherently limited.","In response to these limitations, we propose the Kosmosis approach, which aims to incrementally construct a knowledge graph as new blockchain and social media data become available.","During construction, it aims to extract the semantics of transactions and connect blockchain addresses to their real-world entities by fusing blockchain and social media data in a knowledge graph.","This enables novel preventive methods against rug pulls as a form of crypto asset fraud.","To demonstrate the effectiveness and practical applicability of the Kosmosis approach, we examine a series of real-world rug pulls from 2021.","Through this case, we illustrate how Kosmosis can aid in identifying and preventing such fraudulent activities by leveraging the insights from the constructed knowledge graph."],"url":"http://arxiv.org/abs/2405.19762v1","category":"cs.CR"}
{"created":"2024-05-30 06:51:21","title":"Recipes for forming a carbon-rich giant planet","abstract":"The exploration of carbon-to-oxygen ratios has yielded intriguing insights into the composition of close-in giant exoplanets, giving rise to a distinct classification: carbon-rich planets, characterized by a carbon-to-oxygen ratio $\\ge$ 1 in their atmospheres, as opposed to giant planets exhibiting carbon-to-oxygen ratios close to the protosolar value. In contrast, despite numerous space missions dispatched to the outer solar system and the proximity of Jupiter, Saturn, Uranus, and Neptune, our understanding of the carbon-to-oxygen ratio in these giants remains notably deficient. Determining this ratio is crucial as it serves as a marker linking a planet's volatile composition directly to its formation region within the disk. This article provides an overview of the current understanding of the carbon-to-oxygen ratio in the four gas giants of our solar system and explores why there is yet no definitive dismissal of the possibility that Jupiter, Saturn, Uranus, or Neptune could be considered carbon-rich planets. Additionally, we delve into the three primary formation scenarios proposed in existing literature to account for a bulk carbon-to-oxygen ratio $\\ge$ 1 in a giant planet. A significant challenge lies in accurately inferring the bulk carbon-to-oxygen ratio of our solar system's gas giants. Retrieval methods involve integrating in situ measurements from entry probes equipped with mass spectrometers and remote sensing observations conducted at microwave wavelengths by orbiters. However, these methods fall short of fully discerning the deep carbon-to-oxygen abundance in the gas giants due to their limited probing depth, typically within the 10-100 bar range.","sentences":["The exploration of carbon-to-oxygen ratios has yielded intriguing insights into the composition of close-in giant exoplanets, giving rise to a distinct classification: carbon-rich planets, characterized by a carbon-to-oxygen ratio $\\ge$ 1 in their atmospheres, as opposed to giant planets exhibiting carbon-to-oxygen ratios close to the protosolar value.","In contrast, despite numerous space missions dispatched to the outer solar system and the proximity of Jupiter, Saturn, Uranus, and Neptune, our understanding of the carbon-to-oxygen ratio in these giants remains notably deficient.","Determining this ratio is crucial as it serves as a marker linking a planet's volatile composition directly to its formation region within the disk.","This article provides an overview of the current understanding of the carbon-to-oxygen ratio in the four gas giants of our solar system and explores why there is yet no definitive dismissal of the possibility that Jupiter, Saturn, Uranus, or Neptune could be considered carbon-rich planets.","Additionally, we delve into the three primary formation scenarios proposed in existing literature to account for a bulk carbon-to-oxygen ratio $\\ge$ 1 in a giant planet.","A significant challenge lies in accurately inferring the bulk carbon-to-oxygen ratio of our solar system's gas giants.","Retrieval methods involve integrating in situ measurements from entry probes equipped with mass spectrometers and remote sensing observations conducted at microwave wavelengths by orbiters.","However, these methods fall short of fully discerning the deep carbon-to-oxygen abundance in the gas giants due to their limited probing depth, typically within the 10-100 bar range."],"url":"http://arxiv.org/abs/2405.19748v1","category":"astro-ph.EP"}
{"created":"2024-05-30 06:16:33","title":"Streaming Video Diffusion: Online Video Editing with Diffusion Models","abstract":"We present a novel task called online video editing, which is designed to edit \\textbf{streaming} frames while maintaining temporal consistency. Unlike existing offline video editing assuming all frames are pre-established and accessible, online video editing is tailored to real-life applications such as live streaming and online chat, requiring (1) fast continual step inference, (2) long-term temporal modeling, and (3) zero-shot video editing capability. To solve these issues, we propose Streaming Video Diffusion (SVDiff), which incorporates the compact spatial-aware temporal recurrence into off-the-shelf Stable Diffusion and is trained with the segment-level scheme on large-scale long videos. This simple yet effective setup allows us to obtain a single model that is capable of executing a broad range of videos and editing each streaming frame with temporal coherence. Our experiments indicate that our model can edit long, high-quality videos with remarkable results, achieving a real-time inference speed of 15.2 FPS at a resolution of 512x512.","sentences":["We present a novel task called online video editing, which is designed to edit \\textbf{streaming} frames while maintaining temporal consistency.","Unlike existing offline video editing assuming all frames are pre-established and accessible, online video editing is tailored to real-life applications such as live streaming and online chat, requiring (1) fast continual step inference, (2) long-term temporal modeling, and (3) zero-shot video editing capability.","To solve these issues, we propose Streaming Video Diffusion (SVDiff), which incorporates the compact spatial-aware temporal recurrence into off-the-shelf Stable Diffusion and is trained with the segment-level scheme on large-scale long videos.","This simple yet effective setup allows us to obtain a single model that is capable of executing a broad range of videos and editing each streaming frame with temporal coherence.","Our experiments indicate that our model can edit long, high-quality videos with remarkable results, achieving a real-time inference speed of 15.2 FPS at a resolution of 512x512."],"url":"http://arxiv.org/abs/2405.19726v1","category":"cs.CV"}
