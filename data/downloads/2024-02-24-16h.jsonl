{"created":"2024-02-22 18:59:58","title":"PALO: A Polyglot Large Multimodal Model for 5B People","abstract":"In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \\textsc{Palo}. \\textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Code: https://github.com/mbzuai-oryx/PALO.","sentences":["In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \\textsc{Palo}.","\\textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world population).","Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort.","The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu.","The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines.","We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages.","Code: https://github.com/mbzuai-oryx/PALO."],"url":"http://arxiv.org/abs/2402.14818v1","category":"cs.CL"}
{"created":"2024-02-22 18:59:56","title":"Cameras as Rays: Pose Estimation via Ray Diffusion","abstract":"Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures.","sentences":["Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10).","In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays.","This representation allows for a tight coupling with spatial image features improving pose precision.","We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays.","To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance.","Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures."],"url":"http://arxiv.org/abs/2402.14817v1","category":"cs.CV"}
{"created":"2024-02-22 18:59:53","title":"Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging","abstract":"Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black female patients. Such demographic biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its significant encoding of demographic information. Deploying AI systems with these biases in medical imaging can intensify pre-existing care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical application.","sentences":["Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications.","Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations.","However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients.","The manifestation of such biases could systematically delay essential medical care for certain patient subgroups.","In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets.","Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black female patients.","Such demographic biases present over a wide range of pathologies and demographic attributes.","Further analysis of the model embedding uncovers its significant encoding of demographic information.","Deploying AI systems with these biases in medical imaging can intensify pre-existing care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical application."],"url":"http://arxiv.org/abs/2402.14815v1","category":"cs.CY"}
{"created":"2024-02-22 18:59:25","title":"SymTh for non-finite symmetries","abstract":"Symmetry topological field theory (SymTFT) is a very convenient tool to study finite generalized symmetries of a given quantum field theory (QFT). In particular, SymTFTs encode all the symmetry structures and properties including anomalies. Recently this tool has been applied for non-finite symmetries as well. In this paper, we take a different route, which consists of considering a free theory rather than a topological field theory in the bulk. We call it Symmetry Theory (SymTh). We study its topological operators together with the free boundary conditions. We also propose a procedure that is analogous to the sandwich construction of symmetry TFTs and it allows us to obtain the physical QFT. We apply this to many examples, ranging from abelian $p$-form symmetries to 2-groups, and the (solvable) case of group-like symmetries in quantum mechanics. Finally, we provide a derivation for the SymTh of $\\mathbb Q/ \\mathbb Z$ non-invertible symmetries from the dimensional reduction of IIB supergravity on the conifold. In addition, we give an ultraviolet (UV) interpretation of the quantum Hall states dressing the non-invertible $\\mathbb Q/ \\mathbb Z$ topological operators, in terms of branes in the IIB supergravity background.","sentences":["Symmetry topological field theory (SymTFT) is a very convenient tool to study finite generalized symmetries of a given quantum field theory (QFT).","In particular, SymTFTs encode all the symmetry structures and properties including anomalies.","Recently this tool has been applied for non-finite symmetries as well.","In this paper, we take a different route, which consists of considering a free theory rather than a topological field theory in the bulk.","We call it Symmetry Theory (SymTh).","We study its topological operators together with the free boundary conditions.","We also propose a procedure that is analogous to the sandwich construction of symmetry TFTs and it allows us to obtain the physical QFT.","We apply this to many examples, ranging from abelian $p$-form symmetries to 2-groups, and the (solvable) case of group-like symmetries in quantum mechanics.","Finally, we provide a derivation for the SymTh of $\\mathbb Q/","\\mathbb Z$ non-invertible symmetries from the dimensional reduction of IIB supergravity on the conifold.","In addition, we give an ultraviolet (UV) interpretation of the quantum Hall states dressing the non-invertible $\\mathbb Q/","\\mathbb Z$ topological operators, in terms of branes in the IIB supergravity background."],"url":"http://arxiv.org/abs/2402.14813v1","category":"hep-th"}
{"created":"2024-02-22 18:59:24","title":"Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking","abstract":"Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality: Entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned versions. (iii) Performance boost in the fine-tuned models is primarily attributed to its improved ability to handle the augmented positional information. To uncover these findings, we employ: Patch Patching, DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model.","sentences":["Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks.","Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive.","We study how fine-tuning affects the internal mechanisms implemented in language models.","As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains.","We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking.","In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model.","(ii) The circuits of all the models implement roughly the same functionality: Entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned versions.","(iii) Performance boost in the fine-tuned models is primarily attributed to its improved ability to handle the augmented positional information.","To uncover these findings, we employ: Patch Patching, DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms.","Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model."],"url":"http://arxiv.org/abs/2402.14811v1","category":"cs.CL"}
{"created":"2024-02-22 18:59:24","title":"WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition","abstract":"Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.","sentences":["Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem.","It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling.","This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM).","WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization.","It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation.","Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively.","The code is available at \\url{https://github.com/hustvl/WeakSAM}."],"url":"http://arxiv.org/abs/2402.14812v1","category":"cs.CV"}
{"created":"2024-02-22 18:59:21","title":"GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion","abstract":"In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a \"denoising via diffusion\" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. Project website: https://meowuu7.github.io/GeneOH-Diffusion/.","sentences":["In this work, we tackle the challenging problem of denoising hand-object interactions (HOI).","Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence.","This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns.","We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme.","The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios.","The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a \"denoising via diffusion\" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser.","Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method.","GeneOH Diffusion also shows promise for various downstream applications.","Project website:","https://meowuu7.github.io/GeneOH-Diffusion/."],"url":"http://arxiv.org/abs/2402.14810v1","category":"cs.CV"}
{"created":"2024-02-22 18:59:02","title":"CriticBench: Benchmarking LLMs for Critique-Correct Reasoning","abstract":"The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.","sentences":["The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement.","This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks.","CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic.","It compiles 15 datasets and incorporates responses from three LLM families.","Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning.","Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique.","We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement."],"url":"http://arxiv.org/abs/2402.14809v1","category":"cs.CL"}
{"created":"2024-02-22 18:58:28","title":"RelayAttention for Efficient Large Language Model Serving with Long System Prompts","abstract":"Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention.","sentences":["Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests.","However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t.","the sequence length.","This paper aims to improve the efficiency of LLM services that involve long system prompts.","Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms.","Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request.","To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens.","RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention."],"url":"http://arxiv.org/abs/2402.14808v1","category":"cs.CL"}
{"created":"2024-02-22 18:58:27","title":"A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health","abstract":"Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose code reward functions for a multi-agent RL environment for RMABs, and (3) iterate on the generated reward using feedback from RMAB simulations to effectively adapt policy outcomes. In collaboration with ARMMAN, an India-based public health organization promoting preventative care for pregnant mothers, we conduct a simulation study, showing DLM can dynamically shape policy outcomes using only human language commands as input.","sentences":["Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations.","These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities.","While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities.","Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation.","In this paper, we propose DLM: a Decision Language Model for RMABs.","To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose code reward functions for a multi-agent RL environment for RMABs, and (3) iterate on the generated reward using feedback from RMAB simulations to effectively adapt policy outcomes.","In collaboration with ARMMAN, an India-based public health organization promoting preventative care for pregnant mothers, we conduct a simulation study, showing DLM can dynamically shape policy outcomes using only human language commands as input."],"url":"http://arxiv.org/abs/2402.14807v1","category":"cs.MA"}
{"created":"2024-02-22 18:57:20","title":"Identifying Multiple Personalities in Large Language Models with External Evaluation","abstract":"As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their personalities. Many recent studies quantify LLMs' personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the-art models as the tool to analyze LLMs' responses. Then, we prompt the LLMs with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles. Using the external personality evaluation method, we identify that the obtained personality types for LLMs are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations. This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans. With our work, we call for a re-evaluation of personality definition and measurement in LLMs.","sentences":["As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs.","One of the ways to comprehend LLMs' behavior is to analyze their personalities.","Many recent studies quantify LLMs' personalities using self-assessment tests that are created for humans.","Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs.","In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model.","We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the-art models as the tool to analyze LLMs' responses.","Then, we prompt the LLMs with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles.","Using the external personality evaluation method, we identify that the obtained personality types for LLMs are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations.","This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans.","With our work, we call for a re-evaluation of personality definition and measurement in LLMs."],"url":"http://arxiv.org/abs/2402.14805v1","category":"cs.CL"}
{"created":"2024-02-22 18:56:38","title":"Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset","abstract":"Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development. The project is available at https://mathvision-cuhk.github.io","sentences":["Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista.","However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks.","To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions.","Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs.","Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs.","Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development.","The project is available at https://mathvision-cuhk.github.io"],"url":"http://arxiv.org/abs/2402.14804v1","category":"cs.CV"}
{"created":"2024-02-22 18:56:19","title":"Mochi: Fast \\& Exact Collision Detection","abstract":"Collision Detection (CD) has several applications across the domains such as robotics, visual graphics, and fluid mechanics.   Finding exact collisions between the objects in the scene is quite computationally intensive.   To quickly filter the object pairs that do not result in a collision, bounding boxes are built on the objects, indexed using a Bounding Volume Hierarchy(BVH), and tested for intersection before performing the expensive object-object intersection tests.   In state-of-the-art CD libraries, accelerators such as GPUs are used to accelerate BVH traversal by building specialized data structures.   The recent addition of ray tracing architecture to GPU hardware is designed to do the same but in the context of implementing a Ray Tracing algorithm to render a graphical scene in real-time.   We present Mochi, a fast and exact collision detection engine that accelerates both the broad and narrow phases by taking advantage of the capabilities of Ray Tracing cores.   We introduce multiple new reductions to perform generic CD to support three types of objects for CD: simple spherical particles, objects describable by mathematical equations, and complex objects composed of a triangle mesh.   By implementing our reductions, Mochi achieves several orders of magnitude speedups on synthetic datasets and 5x-28x speedups on real-world triangle mesh datasets.   We further evaluate our reductions thoroughly and provide several architectural insights on the ray tracing cores that are otherwise unknown due to their proprietorship.","sentences":["Collision Detection (CD) has several applications across the domains such as robotics, visual graphics, and fluid mechanics.   ","Finding exact collisions between the objects in the scene is quite computationally intensive.   ","To quickly filter the object pairs that do not result in a collision, bounding boxes are built on the objects, indexed using a Bounding Volume Hierarchy(BVH), and tested for intersection before performing the expensive object-object intersection tests.   ","In state-of-the-art CD libraries, accelerators such as GPUs are used to accelerate BVH traversal by building specialized data structures.   ","The recent addition of ray tracing architecture to GPU hardware is designed to do the same but in the context of implementing a Ray Tracing algorithm to render a graphical scene in real-time.   ","We present Mochi, a fast and exact collision detection engine that accelerates both the broad and narrow phases by taking advantage of the capabilities of Ray Tracing cores.   ","We introduce multiple new reductions to perform generic CD to support three types of objects for CD: simple spherical particles, objects describable by mathematical equations, and complex objects composed of a triangle mesh.   ","By implementing our reductions, Mochi achieves several orders of magnitude speedups on synthetic datasets and 5x-28x speedups on real-world triangle mesh datasets.   ","We further evaluate our reductions thoroughly and provide several architectural insights on the ray tracing cores that are otherwise unknown due to their proprietorship."],"url":"http://arxiv.org/abs/2402.14801v1","category":"cs.GR"}
{"created":"2024-02-22 18:56:07","title":"Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models","abstract":"A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity.","sentences":["A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs.","Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes.","Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques.","Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks.","Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance.","Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity."],"url":"http://arxiv.org/abs/2402.14800v1","category":"cs.CL"}
{"created":"2024-02-22 18:55:17","title":"Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic","abstract":"Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in a modern neuro-symbolic reasoning engine significantly improves results (both accuracy and proof quality) over other entailment classifier baselines, illustrating the practical benefit of this advance for textual inference.","sentences":["Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic.","However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is.","This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines.","To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference.","We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment.","We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in a modern neuro-symbolic reasoning engine significantly improves results (both accuracy and proof quality) over other entailment classifier baselines, illustrating the practical benefit of this advance for textual inference."],"url":"http://arxiv.org/abs/2402.14798v1","category":"cs.CL"}
{"created":"2024-02-22 18:55:08","title":"Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis","abstract":"Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-video model with billions of parameters for the first time, reach state-of-the-art results on a number of benchmarks, and generate videos with substantially higher quality, temporal consistency, and motion complexity. The user studies showed that our model was favored by a large margin over the most recent methods. See our website at https://snap-research.github.io/snapvideo/.","sentences":["Contemporary models for generating images show remarkable quality and versatility.","Swayed by these advantages, the research community repurposes them to generate videos.","Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability.","In this work, we build Snap Video, a video-first model that systematically addresses these challenges.","To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation.","Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead.","Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference).","This allows us to efficiently train a text-to-video model with billions of parameters for the first time, reach state-of-the-art results on a number of benchmarks, and generate videos with substantially higher quality, temporal consistency, and motion complexity.","The user studies showed that our model was favored by a large margin over the most recent methods.","See our website at https://snap-research.github.io/snapvideo/."],"url":"http://arxiv.org/abs/2402.14797v1","category":"cs.CV"}
{"created":"2024-02-22 18:54:04","title":"Disjointness of inertial KMS states and the role of Lorentz symmetry in thermalization","abstract":"For any local, translation-covariant quantum field theory on Minkowski spacetime we prove that two distinct primary states that are invariant under the inertial time evolutions in different inertial reference frames and satisfy a timelike cluster property called the mixing property are disjoint, i.e. each state is not a perturbation of the other. These conditions are fulfilled by the inertial KMS states of the free scalar field, thus showing that a state satisfying the KMS condition relative to one reference frame is far from thermal equilibrium relative to other frames. We review the property of return to equilibrium in open quantum systems theory and discuss the implications of disjointness on the asymptotic behavior of detector systems coupled to states of a free massless scalar field. We argue that a coupled system consisting of an Unruh-DeWitt detector moving with constant velocity relative to the field in a thermal state, or an excitation thereof, cannot approach a KMS state at late times under generic conditions. This leads to an illustration of the physical differences between heat baths in inertial systems and the apparent \"heat bath\" of the Unruh effect from the viewpoint of moving detectors. The article also reviews, from a quantum field theoretical perspective, the quantum dynamical system of an Unruh-DeWitt detector coupled to a massless scalar field in a KMS state relative to the rest frame of the detector.","sentences":["For any local, translation-covariant quantum field theory on Minkowski spacetime we prove that two distinct primary states that are invariant under the inertial time evolutions in different inertial reference frames and satisfy a timelike cluster property called the mixing property are disjoint, i.e. each state is not a perturbation of the other.","These conditions are fulfilled by the inertial KMS states of the free scalar field, thus showing that a state satisfying the KMS condition relative to one reference frame is far from thermal equilibrium relative to other frames.","We review the property of return to equilibrium in open quantum systems theory and discuss the implications of disjointness on the asymptotic behavior of detector systems coupled to states of a free massless scalar field.","We argue that a coupled system consisting of an Unruh-DeWitt detector moving with constant velocity relative to the field in a thermal state, or an excitation thereof, cannot approach a KMS state at late times under generic conditions.","This leads to an illustration of the physical differences between heat baths in inertial systems and the apparent \"heat bath\" of the Unruh effect from the viewpoint of moving detectors.","The article also reviews, from a quantum field theoretical perspective, the quantum dynamical system of an Unruh-DeWitt detector coupled to a massless scalar field in a KMS state relative to the rest frame of the detector."],"url":"http://arxiv.org/abs/2402.14794v1","category":"gr-qc"}
{"created":"2024-02-22 18:51:10","title":"Adaptive hybrid density functionals","abstract":"Exact exchange contributions are known to crucially affect electronic states, which in turn govern covalent bond formation and breaking in chemical species. Empirically averaging the exact exchange admixture over configurational and compositional degrees of freedom, hybrid density functional approximations (DFAs) have been widely successful, yet have fallen short to reach explicitly correlated high level quantum chemistry accuracy in general. Using quantum machine learning, we have adaptified hybrid DFAs by generating optimal exact exchange admixture ratios \"on the fly\" and specifically for any molecule. Adaptifying the PBE0 hybrid DFA, our predictions of atomization energies are sufficiently accurate to effectively cure the infamous spin gap problem in DFT, and also lead to improved electron densities, and HOMO-LUMO gaps. Using adaptive PBE0, we revised the entire QM9 data set presenting more accurate quantum properties that include, on average, stronger covalent binding, larger band-gaps, more localized electron densities, and slightly larger dipole-moments.","sentences":["Exact exchange contributions are known to crucially affect electronic states, which in turn govern covalent bond formation and breaking in chemical species.","Empirically averaging the exact exchange admixture over configurational and compositional degrees of freedom, hybrid density functional approximations (DFAs) have been widely successful, yet have fallen short to reach explicitly correlated high level quantum chemistry accuracy in general.","Using quantum machine learning, we have adaptified hybrid DFAs by generating optimal exact exchange admixture ratios \"on the fly\" and specifically for any molecule.","Adaptifying the PBE0 hybrid DFA, our predictions of atomization energies are sufficiently accurate to effectively cure the infamous spin gap problem in DFT, and also lead to improved electron densities, and HOMO-LUMO gaps.","Using adaptive PBE0, we revised the entire QM9 data set presenting more accurate quantum properties that include, on average, stronger covalent binding, larger band-gaps, more localized electron densities, and slightly larger dipole-moments."],"url":"http://arxiv.org/abs/2402.14793v1","category":"physics.chem-ph"}
{"created":"2024-02-22 18:50:18","title":"Consolidating Attention Features for Multi-view Image Editing","abstract":"Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry.","sentences":["Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls.","However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results.","In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views.","We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure.","Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries.","To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images.","Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency.","We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps.","We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene.","These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry."],"url":"http://arxiv.org/abs/2402.14792v1","category":"cs.CV"}
{"created":"2024-02-22 18:46:22","title":"Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning","abstract":"Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.","sentences":["Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities.","Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task.","While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific.","We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method.","SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions.","We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics.","We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks."],"url":"http://arxiv.org/abs/2402.14789v1","category":"cs.LG"}
{"created":"2024-02-22 18:40:29","title":"Automated chemical reaction network generation and its application to exoplanet atmospheres","abstract":"With the advent of JWST and spectroscopic characterization of exoplanet atmospheres with unprecedented detail, there is a demand for a more complete picture of chemical and photochemical reactions and their impact on atmospheric composition. Traditionally, building reaction networks for (exo)planetary atmospheres involves manually tracking relevant species and reactions, a time-consuming and error-prone process. This approach's applicability is also often limited to specific conditions, making it less versatile for different planetary types. (i.e., photochemical networks for Jupiters may not be directly applicable to water-rich exoplanets). We introduce an automated approach using a computer-aided chemical reaction network generator, combined with a one-dimensional photochemical kinetic-transport model, offering significant advantages. This approach automatically selects reaction rates through a rate-based iterative algorithm and refinement steps, enhancing model reliability. Also, this approach allows for the efficient simulation of diverse chemical environments, from hydrogen to water, carbon dioxide, and nitrogen-dominated atmospheres. Using WASP-39b and WASP-80b as examples, we demonstrate our approach's effectiveness. Our WASP-39b model aligns with prior studies and JWST observations, capturing photochemically produced sulfur dioxide. The WASP-80b model reveals an atmosphere influenced by deep interior thermochemistry and vertical mixing, consistent with JWST NIRCam observations. Furthermore, our model identifies a novel initial step for the N2-NH3-HCN pathway that enhances the conversion efficiency in high-temperature/pressure environments. This automated chemical network generation offers a novel, efficient, and precise framework for studying exoplanetary atmospheres, marking a significant advancement over traditional modeling techniques.","sentences":["With the advent of JWST and spectroscopic characterization of exoplanet atmospheres with unprecedented detail, there is a demand for a more complete picture of chemical and photochemical reactions and their impact on atmospheric composition.","Traditionally, building reaction networks for (exo)planetary atmospheres involves manually tracking relevant species and reactions, a time-consuming and error-prone process.","This approach's applicability is also often limited to specific conditions, making it less versatile for different planetary types.","(i.e., photochemical networks for Jupiters may not be directly applicable to water-rich exoplanets).","We introduce an automated approach using a computer-aided chemical reaction network generator, combined with a one-dimensional photochemical kinetic-transport model, offering significant advantages.","This approach automatically selects reaction rates through a rate-based iterative algorithm and refinement steps, enhancing model reliability.","Also, this approach allows for the efficient simulation of diverse chemical environments, from hydrogen to water, carbon dioxide, and nitrogen-dominated atmospheres.","Using WASP-39b and WASP-80b as examples, we demonstrate our approach's effectiveness.","Our WASP-39b model aligns with prior studies and JWST observations, capturing photochemically produced sulfur dioxide.","The WASP-80b model reveals an atmosphere influenced by deep interior thermochemistry and vertical mixing, consistent with JWST NIRCam observations.","Furthermore, our model identifies a novel initial step for the N2-NH3-HCN pathway that enhances the conversion efficiency in high-temperature/pressure environments.","This automated chemical network generation offers a novel, efficient, and precise framework for studying exoplanetary atmospheres, marking a significant advancement over traditional modeling techniques."],"url":"http://arxiv.org/abs/2402.14784v1","category":"astro-ph.EP"}
{"created":"2024-02-22 18:40:13","title":"Solar Power Generation Profile Estimation for Lunar Surface Solar PV Systems","abstract":"As NASA prepares to carry out its Artemis lunar missions, the design and planning of robust power systems tailored to the lunar environment become necessary and urgent. Solar photovoltaic (PV) systems are among the most suitable power generators for lunar applications given the abundant solar irradiance the lunar surface receives as a result of the lack of an atmosphere. However, the vastly different environmental conditions of the moon compared to those on Earth call for special reconsiderations to the traditional PV power output models used for terrestrial applications. The substantially wider range of temperatures on the moon combined with the absence of atmosphere implies the PV will operate at very different thermal conditions. Therefore, this paper proposes a PV power output model that determines PV cell temperature on the lunar surface based on lunar ambient temperature as well as solar irradiance, while also capturing these special lunar conditions. The model is based on thermal exchange via the three main heat transfer paths: heat radiation, absorption, and convection. Moreover, four different array configurations are presented and compared to determine the most suitable choice based on energy generation and complexity for different locations around the moon.","sentences":["As NASA prepares to carry out its Artemis lunar missions, the design and planning of robust power systems tailored to the lunar environment become necessary and urgent.","Solar photovoltaic (PV) systems are among the most suitable power generators for lunar applications given the abundant solar irradiance the lunar surface receives as a result of the lack of an atmosphere.","However, the vastly different environmental conditions of the moon compared to those on Earth call for special reconsiderations to the traditional PV power output models used for terrestrial applications.","The substantially wider range of temperatures on the moon combined with the absence of atmosphere implies the PV will operate at very different thermal conditions.","Therefore, this paper proposes a PV power output model that determines PV cell temperature on the lunar surface based on lunar ambient temperature as well as solar irradiance, while also capturing these special lunar conditions.","The model is based on thermal exchange via the three main heat transfer paths: heat radiation, absorption, and convection.","Moreover, four different array configurations are presented and compared to determine the most suitable choice based on energy generation and complexity for different locations around the moon."],"url":"http://arxiv.org/abs/2402.14783v1","category":"physics.space-ph"}
{"created":"2024-02-22 18:39:24","title":"Rao-Blackwellising Bayesian Causal Inference","abstract":"Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature. In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based graph learning into an effective Bayesian causal inference framework. Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for which we learn a distribution via gradient-based optimisation. The combination of Rao-Blackwellization with our sequential inference procedure for causal orders yields state-of-the-art on linear and non-linear additive noise benchmarks with scale-free and Erdos-Renyi graph structures.","sentences":["Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature.","In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based graph learning into an effective Bayesian causal inference framework.","Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable.","When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time.","We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation.","This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for which we learn a distribution via gradient-based optimisation.","The combination of Rao-Blackwellization with our sequential inference procedure for causal orders yields state-of-the-art on linear and non-linear additive noise benchmarks with scale-free and Erdos-Renyi graph structures."],"url":"http://arxiv.org/abs/2402.14781v1","category":"cs.LG"}
{"created":"2024-02-22 18:38:48","title":"Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models","abstract":"Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapting it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling from the reference videos. To disentangle the spatial and temporal information during the training pipeline, we introduce a novel concept of appearance absorbers that detach the original appearance from the single reference video prior to motion learning. Our proposed method can be easily extended to various downstream tasks, including custom video generation and editing, video appearance customization, and multiple motion combination, in a plug-and-play fashion. Our project page can be found at https://anonymous-314.github.io.","sentences":["Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications.","With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated.","To address the challenge of one-shot motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapting it to new subjects and scenes with both spatial and temporal varieties.","It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling from the reference videos.","To disentangle the spatial and temporal information during the training pipeline, we introduce a novel concept of appearance absorbers that detach the original appearance from the single reference video prior to motion learning.","Our proposed method can be easily extended to various downstream tasks, including custom video generation and editing, video appearance customization, and multiple motion combination, in a plug-and-play fashion.","Our project page can be found at https://anonymous-314.github.io."],"url":"http://arxiv.org/abs/2402.14780v1","category":"cs.CV"}
{"created":"2024-02-22 18:37:53","title":"Measure contraction property and curvature-dimension condition on sub-Finsler Heisenberg groups","abstract":"In this paper, we investigate the validity of synthetic curvature-dimension bounds in the sub-Finsler Heisenberg group, equipped with a positive smooth measure. Firstly, we study the measure contraction property, in short $\\mathsf{MCP}$, proving that its validity depends on the norm generating the sub-Finsler structure. Indeed, we show that, if it is neither $C^1$ nor strongly convex, the associated Heisenberg group does not satisfy $\\mathsf{MCP}(K,N)$ for any pair of parameters $K \\in \\mathbb{R}$ and $N \\in (1,\\infty)$. On the contrary, we prove that the sub-Finsler Heisenberg group, equipped with a $C^{1,1}$ and strongly convex norm, and with the Lebesgue measure, satisfies $\\mathsf{MCP}(0,N)$ for some $N \\in (1,\\infty)$. Additionally, we provide a lower bound on the optimal dimensional parameter, and we also study the case of $C^1$ and strongly convex norms. Secondly, we address the validity of the curvature-dimension condition pioneered by Sturm and Lott-Villani, in short $\\mathsf{CD}(K,N)$. We show that the sub-Finsler Heisenberg group, equipped with a $C^1$ and strongly convex norm, and with a positive smooth measure, does not satisfy the $\\mathsf{MCP}(K,N)$ condition for any pair of parameters $K \\in \\mathbb{R}$ and $N \\in (1,\\infty)$. Combining this result with our findings regarding the measure contraction property, we conclude the failure of the $\\mathsf{CD}$ condition in the Heisenberg group for every sub-Finsler structure.","sentences":["In this paper, we investigate the validity of synthetic curvature-dimension bounds in the sub-Finsler Heisenberg group, equipped with a positive smooth measure.","Firstly, we study the measure contraction property, in short $\\mathsf{MCP}$, proving that its validity depends on the norm generating the sub-Finsler structure.","Indeed, we show that, if it is neither $C^1$ nor strongly convex, the associated Heisenberg group does not satisfy $\\mathsf{MCP}(K,N)$ for any pair of parameters $K \\in \\mathbb{R}$ and $N \\in (1,\\infty)$. On the contrary, we prove that the sub-Finsler Heisenberg group, equipped with a $C^{1,1}$ and strongly convex norm, and with the Lebesgue measure, satisfies $\\mathsf{MCP}(0,N)$ for some $N \\in (1,\\infty)$. Additionally, we provide a lower bound on the optimal dimensional parameter, and we also study the case of $C^1$ and strongly convex norms.","Secondly, we address the validity of the curvature-dimension condition pioneered by Sturm and Lott-Villani, in short $\\mathsf{CD}(K,N)$. We show that the sub-Finsler Heisenberg group, equipped with a $C^1$ and strongly convex norm, and with a positive smooth measure, does not satisfy the $\\mathsf{MCP}(K,N)$ condition for any pair of parameters $K \\in \\mathbb{R}$ and $N \\in (1,\\infty)$. Combining this result with our findings regarding the measure contraction property, we conclude the failure of the $\\mathsf{CD}$ condition in the Heisenberg group for every sub-Finsler structure."],"url":"http://arxiv.org/abs/2402.14779v1","category":"math.MG"}
{"created":"2024-02-22 18:37:33","title":"Zero-shot cross-lingual transfer in instruction tuning of large language model","abstract":"Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.","sentences":["Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings.","In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages.","We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following.","We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data.","English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors."],"url":"http://arxiv.org/abs/2402.14778v1","category":"cs.CL"}
{"created":"2024-02-22 18:29:28","title":"Generalized eikonal identities for charged currents","abstract":"We discuss QED radiative corrections to contact operators coupling two heavy fields and one light field. New eikonal identities are derived in the static limit that demonstrate the equivalence of a class of ladder graphs to an equivalent theory with a single heavy-light vertex and a background Coulomb field which communicates exclusively with the light field. We apply these new identities to nuclear beta decays and demonstrates that the \"independent particle model\" used by Jaus, Rasche, Sirlin \\& Zucchini is closely related, though not identical, to a model independent EFT calculation.","sentences":["We discuss QED radiative corrections to contact operators coupling two heavy fields and one light field.","New eikonal identities are derived in the static limit that demonstrate the equivalence of a class of ladder graphs to an equivalent theory with a single heavy-light vertex and a background Coulomb field which communicates exclusively with the light field.","We apply these new identities to nuclear beta decays and demonstrates that the \"independent particle model\" used by Jaus, Rasche, Sirlin \\& Zucchini is closely related, though not identical, to a model independent EFT calculation."],"url":"http://arxiv.org/abs/2402.14769v1","category":"hep-ph"}
{"created":"2024-02-22 18:22:35","title":"Functional Spatial Autoregressive Models","abstract":"This study introduces a novel spatial autoregressive model in which the dependent variable is a function that may exhibit functional autocorrelation with the outcome functions of nearby units. This model can be characterized as a simultaneous integral equation system, which, in general, does not necessarily have a unique solution. For this issue, we provide a simple condition on the magnitude of the spatial interaction to ensure the uniqueness in data realization. For estimation, to account for the endogeneity caused by the spatial interaction, we propose a regularized two-stage least squares estimator based on a basis approximation for the functional parameter. The asymptotic properties of the estimator including the consistency and asymptotic normality are investigated under certain conditions. Additionally, we propose a simple Wald-type test for detecting the presence of spatial effects. As an empirical illustration, we apply the proposed model and method to analyze age distributions in Japanese cities.","sentences":["This study introduces a novel spatial autoregressive model in which the dependent variable is a function that may exhibit functional autocorrelation with the outcome functions of nearby units.","This model can be characterized as a simultaneous integral equation system, which, in general, does not necessarily have a unique solution.","For this issue, we provide a simple condition on the magnitude of the spatial interaction to ensure the uniqueness in data realization.","For estimation, to account for the endogeneity caused by the spatial interaction, we propose a regularized two-stage least squares estimator based on a basis approximation for the functional parameter.","The asymptotic properties of the estimator including the consistency and asymptotic normality are investigated under certain conditions.","Additionally, we propose a simple Wald-type test for detecting the presence of spatial effects.","As an empirical illustration, we apply the proposed model and method to analyze age distributions in Japanese cities."],"url":"http://arxiv.org/abs/2402.14763v1","category":"econ.EM"}
{"created":"2024-02-22 18:21:59","title":"MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues","abstract":"The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities.","sentences":["The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems.","However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge.","Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues.","To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues.","By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks.","We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks.","Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs.","Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities."],"url":"http://arxiv.org/abs/2402.14762v1","category":"cs.CL"}
{"created":"2024-02-22 18:20:33","title":"Generalizing Reward Modeling for Out-of-Distribution Preference Learning","abstract":"Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model for PL. We theoretically demonstrate the convergence rate of the bilevel optimization algorithm under reasonable assumptions. Additionally, we conduct experiments on two text generation tasks across 20 held-out domains and outperform a variety of strong baselines across various evaluation metrics.","sentences":["Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences.","Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL.","However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging.","Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback.","This work addresses OOD PL by optimizing a general reward model through a meta-learning approach.","During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions.","When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model for PL.","We theoretically demonstrate the convergence rate of the bilevel optimization algorithm under reasonable assumptions.","Additionally, we conduct experiments on two text generation tasks across 20 held-out domains and outperform a variety of strong baselines across various evaluation metrics."],"url":"http://arxiv.org/abs/2402.14760v1","category":"cs.LG"}
{"created":"2024-02-22 18:20:25","title":"Generalising realisability in statistical learning theory under epistemic uncertainty","abstract":"The purpose of this paper is to look into how central notions in statistical learning theory, such as realisability, generalise under the assumption that train and test distribution are issued from the same credal set, i.e., a convex set of probability distributions. This can be considered as a first step towards a more general treatment of statistical learning under epistemic uncertainty.","sentences":["The purpose of this paper is to look into how central notions in statistical learning theory, such as realisability, generalise under the assumption that train and test distribution are issued from the same credal set, i.e., a convex set of probability distributions.","This can be considered as a first step towards a more general treatment of statistical learning under epistemic uncertainty."],"url":"http://arxiv.org/abs/2402.14759v1","category":"cs.LG"}
{"created":"2024-02-22 18:20:22","title":"Batch and match: black-box variational inference with a score-based divergence","abstract":"Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization.","sentences":["Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO).","But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates.","In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence.","Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices.","We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance.","We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models.","In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization."],"url":"http://arxiv.org/abs/2402.14758v1","category":"stat.ML"}
{"created":"2024-02-22 18:19:45","title":"SHM-Traffic: DRL and Transfer learning based UAV Control for Structural Health Monitoring of Bridges with Traffic","abstract":"This work focuses on using advanced techniques for structural health monitoring (SHM) for bridges with Traffic. We propose an approach using deep reinforcement learning (DRL)-based control for Unmanned Aerial Vehicle (UAV). Our approach conducts a concrete bridge deck survey while traffic is ongoing and detects cracks. The UAV performs the crack detection, and the location of cracks is initially unknown. We use two edge detection techniques. First, we use canny edge detection for crack detection. We also use a Convolutional Neural Network (CNN) for crack detection and compare it with canny edge detection. Transfer learning is applied using CNN with pre-trained weights obtained from a crack image dataset. This enables the model to adapt and improve its performance in identifying and localizing cracks. Proximal Policy Optimization (PPO) is applied for UAV control and bridge surveys. The experimentation across various scenarios is performed to evaluate the performance of the proposed methodology. Key metrics such as task completion time and reward convergence are observed to gauge the effectiveness of the approach. We observe that the Canny edge detector offers up to 40\\% lower task completion time, while the CNN excels in up to 12\\% better damage detection and 1.8 times better rewards.","sentences":["This work focuses on using advanced techniques for structural health monitoring (SHM) for bridges with Traffic.","We propose an approach using deep reinforcement learning (DRL)-based control for Unmanned Aerial Vehicle (UAV).","Our approach conducts a concrete bridge deck survey while traffic is ongoing and detects cracks.","The UAV performs the crack detection, and the location of cracks is initially unknown.","We use two edge detection techniques.","First, we use canny edge detection for crack detection.","We also use a Convolutional Neural Network (CNN) for crack detection and compare it with canny edge detection.","Transfer learning is applied using CNN with pre-trained weights obtained from a crack image dataset.","This enables the model to adapt and improve its performance in identifying and localizing cracks.","Proximal Policy Optimization (PPO) is applied for UAV control and bridge surveys.","The experimentation across various scenarios is performed to evaluate the performance of the proposed methodology.","Key metrics such as task completion time and reward convergence are observed to gauge the effectiveness of the approach.","We observe that the Canny edge detector offers up to 40\\% lower task completion time, while the CNN excels in up to 12\\% better damage detection and 1.8 times better rewards."],"url":"http://arxiv.org/abs/2402.14757v1","category":"cs.AI"}
{"created":"2024-02-22 18:18:15","title":"Towards homological projective duality for $\\mathrm{Gr}(2, 2n)$","abstract":"Consider a Grassmannian $\\mathrm{Gr}(2, V)$ for an even-dimensional vector space $V$. Its derived category of coherent sheaves has a Lefschetz exceptional collection with respect to the Pl\\\"ucker embedding. We consider a variety $X_1$ of pairs consisting of a degenerate $2$-form on $V$ and a line in its kernel. Note that $X_1$ is generically a $\\mathbb{P}^1$-fibration over the Pfaffian variety of degenerate $2$-forms on $V$. We construct an exceptional collection of coherent sheaves on $X_1$ such that the subcategory of $D^b_{\\mathrm{coh}}(X_1)$ generated by that collection is conjecturally equivalent to the homologically projectively dual category of the Grassmannian.","sentences":["Consider a Grassmannian $\\mathrm{Gr}(2, V)$ for an even-dimensional vector space $V$. Its derived category of coherent sheaves has a Lefschetz exceptional collection with respect to the Pl\\\"ucker embedding.","We consider a variety $X_1$ of pairs consisting of a degenerate $2$-form on $V$ and a line in its kernel.","Note that $X_1$ is generically a $\\mathbb{P}^1$-fibration over the Pfaffian variety of degenerate $2$-forms on $V$. We construct an exceptional collection of coherent sheaves on $X_1$ such that the subcategory of $D^b_{\\mathrm{coh}}(X_1)$ generated by that collection is conjecturally equivalent to the homologically projectively dual category of the Grassmannian."],"url":"http://arxiv.org/abs/2402.14754v1","category":"math.AG"}
{"created":"2024-02-22 18:12:48","title":"Prompting a Pretrained Transformer Can Be a Universal Approximator","abstract":"Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it. Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length. Beyond these density-type results, we also offer Jackson-type bounds on the length of the prefix needed to approximate a function to a desired precision.","sentences":["Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited.","A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it.","Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions.","This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed.","In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function.","Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length.","Beyond these density-type results, we also offer Jackson-type bounds on the length of the prefix needed to approximate a function to a desired precision."],"url":"http://arxiv.org/abs/2402.14753v1","category":"cs.LG"}
{"created":"2024-02-22 18:11:19","title":"On the communication complexity of finding a king in a tournament","abstract":"A tournament is a complete directed graph. A king in a tournament is a vertex v such that every other vertex is reachable from v via a path of length at most 2. It is well known that every tournament has at least one king, one of which is a maximum out-degree vertex. The tasks of finding a king, a maximum out-degree vertex and a source in a tournament has been relatively well studied in the context of query complexity. We study the communication complexity of these tasks, where the edges are partitioned between two players. The following are our main results for n-vertex tournaments:   1) The deterministic communication complexity of finding whether a source exists is tilde{Theta}(log^2 n).   2) The deterministic and randomized communication complexities of finding a king are Theta(n). The quantum communication complexity is tilde{Theta}(sqrt{n}).   3) The deterministic, randomized and quantum communication complexities of finding a maximum out-degree vertex are Theta(n log n), tilde{Theta}(n) and tilde{Theta}(sqrt{n}), respectively.   Our upper bounds hold for all partitions of edges, and the lower bounds for a specific partition of the edges. To show the first bullet above, we show, perhaps surprisingly, that finding a source in a tournament is equivalent to the well-studied Clique vs. Independent Set (CIS) problem on undirected graphs. Our bounds for finding a source then follow from known bounds on the complexity of the CIS problem. In view of this equivalence, we can view the task of finding a king in a tournament to be a natural generalization of CIS.   One of our lower bounds uses a fooling-set based argument, and all our other lower bounds follow from carefully-constructed reductions from Set-Disjointness.","sentences":["A tournament is a complete directed graph.","A king in a tournament is a vertex v such that every other vertex is reachable from v via a path of length at most 2.","It is well known that every tournament has at least one king, one of which is a maximum out-degree vertex.","The tasks of finding a king, a maximum out-degree vertex and a source in a tournament has been relatively well studied in the context of query complexity.","We study the communication complexity of these tasks, where the edges are partitioned between two players.","The following are our main results for n-vertex tournaments:   1) The deterministic communication complexity of finding whether a source exists is tilde{Theta}(log^2 n).   ","2)","The deterministic and randomized communication complexities of finding a king are Theta(n).","The quantum communication complexity is tilde{Theta}(sqrt{n}).   ","3) The deterministic, randomized and quantum communication complexities of finding a maximum out-degree vertex are Theta(n log n), tilde{Theta}(n) and tilde{Theta}(sqrt{n}), respectively.   ","Our upper bounds hold for all partitions of edges, and the lower bounds for a specific partition of the edges.","To show the first bullet above, we show, perhaps surprisingly, that finding a source in a tournament is equivalent to the well-studied Clique vs. Independent Set (CIS) problem on undirected graphs.","Our bounds for finding a source then follow from known bounds on the complexity of the CIS problem.","In view of this equivalence, we can view the task of finding a king in a tournament to be a natural generalization of CIS.   ","One of our lower bounds uses a fooling-set based argument, and all our other lower bounds follow from carefully-constructed reductions from Set-Disjointness."],"url":"http://arxiv.org/abs/2402.14751v1","category":"cs.CC"}
{"created":"2024-02-22 18:09:17","title":"Testing Spacecraft Formation Flying with Crazyflie Drones as Satellite Surrogates","abstract":"As the space domain becomes increasingly congested, autonomy is proposed as one approach to enable small numbers of human ground operators to manage large constellations of satellites and tackle more complex missions such as on-orbit or in-space servicing, assembly, and manufacturing. One of the biggest challenges in developing novel spacecraft autonomy is mechanisms to test and evaluate their performance. Testing spacecraft autonomy on-orbit can be high risk and prohibitively expensive. An alternative method is to test autonomy terrestrially using satellite surrogates such as attitude test beds on air bearings or drones for translational motion visualization. Against this background, this work develops an approach to evaluate autonomous spacecraft behavior using a surrogate platform, namely a micro-quadcopter drone developed by the Bitcraze team, the Crazyflie 2.1. The Crazyflie drones are increasingly becoming ubiquitous in flight testing labs because they are affordable, open source, readily available, and include expansion decks which allow for features such as positioning systems, distance and/or motion sensors, wireless charging, and AI capabilities. In this paper, models of Crazyflie drones are used to simulate the relative motion dynamics of spacecraft under linearized Clohessy-Wiltshire dynamics in elliptical natural motion trajectories, in pre-generated docking trajectories, and via trajectories output by neural network control systems.","sentences":["As the space domain becomes increasingly congested, autonomy is proposed as one approach to enable small numbers of human ground operators to manage large constellations of satellites and tackle more complex missions such as on-orbit or in-space servicing, assembly, and manufacturing.","One of the biggest challenges in developing novel spacecraft autonomy is mechanisms to test and evaluate their performance.","Testing spacecraft autonomy on-orbit can be high risk and prohibitively expensive.","An alternative method is to test autonomy terrestrially using satellite surrogates such as attitude test beds on air bearings or drones for translational motion visualization.","Against this background, this work develops an approach to evaluate autonomous spacecraft behavior using a surrogate platform, namely a micro-quadcopter drone developed by the Bitcraze team, the Crazyflie 2.1.","The Crazyflie drones are increasingly becoming ubiquitous in flight testing labs because they are affordable, open source, readily available, and include expansion decks which allow for features such as positioning systems, distance and/or motion sensors, wireless charging, and AI capabilities.","In this paper, models of Crazyflie drones are used to simulate the relative motion dynamics of spacecraft under linearized Clohessy-Wiltshire dynamics in elliptical natural motion trajectories, in pre-generated docking trajectories, and via trajectories output by neural network control systems."],"url":"http://arxiv.org/abs/2402.14750v1","category":"cs.RO"}
{"created":"2024-02-22 18:07:45","title":"Catani's generalization of collinear factorization breaking","abstract":"We consider the most general form of soft and collinear factorization for hard-scattering amplitudes to all orders in perturbative Quantum Chromodynamics. Specifically, we present the generalization of collinear factorization to configurations with several collinear directions, where the most singular behaviour is encoded by generalized collinear splitting amplitudes that manifestly embed the breaking of strict collinear factorization in space-like collinear configurations. We also extend the analysis to the simultaneous soft-collinear factorization with multiple collinear directions and show how na\\\"{\\i}ve multiplicative factorization do not hold.","sentences":["We consider the most general form of soft and collinear factorization for hard-scattering amplitudes to all orders in perturbative Quantum Chromodynamics.","Specifically, we present the generalization of collinear factorization to configurations with several collinear directions, where the most singular behaviour is encoded by generalized collinear splitting amplitudes that manifestly embed the breaking of strict collinear factorization in space-like collinear configurations.","We also extend the analysis to the simultaneous soft-collinear factorization with multiple collinear directions and show how na\\\"{\\i}ve multiplicative factorization do not hold."],"url":"http://arxiv.org/abs/2402.14749v1","category":"hep-ph"}
{"created":"2024-02-22 18:04:27","title":"Epimorphisms between finitely generated algebras","abstract":"A quasivariety has the weak ES property when the epimorphisms between its finitely generated members are surjective. A characterization of quasivarieties with the weak ES property is obtained and a method for detecting failures of this property in quasivarieties with a near unanimity term and in congruence permutable varieties is given. It is also shown that under reasonable assumptions the weak ES property implies arithmeticity. In particular, every filtral variety with the weak ES property is a discriminator variety.","sentences":["A quasivariety has the weak ES property when the epimorphisms between its finitely generated members are surjective.","A characterization of quasivarieties with the weak ES property is obtained and a method for detecting failures of this property in quasivarieties with a near unanimity term and in congruence permutable varieties is given.","It is also shown that under reasonable assumptions the weak ES property implies arithmeticity.","In particular, every filtral variety with the weak ES property is a discriminator variety."],"url":"http://arxiv.org/abs/2402.14745v1","category":"math.LO"}
{"created":"2024-02-22 18:03:14","title":"Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation","abstract":"This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This research marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.","sentences":["This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation.","LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks.","Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility.","The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation.","In experimental studies, comprehensive validation is performed using real-world data.","This research marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis."],"url":"http://arxiv.org/abs/2402.14744v1","category":"cs.AI"}
{"created":"2024-02-22 17:55:18","title":"Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning","abstract":"Tuberculosis (TB) remains a significant global health challenge, with pediatric cases posing a major concern. The World Health Organization (WHO) advocates for chest X-rays (CXRs) for TB screening. However, visual interpretation by radiologists can be subjective, time-consuming and prone to error, especially in pediatric TB. Artificial intelligence (AI)-driven computer-aided detection (CAD) tools, especially those utilizing deep learning, show promise in enhancing lung disease detection. However, challenges include data scarcity and lack of generalizability. In this context, we propose a novel self-supervised paradigm leveraging Vision Transformers (ViT) for improved TB detection in CXR, enabling zero-shot pediatric TB detection. We demonstrate improvements in TB detection performance ($\\sim$12.7% and $\\sim$13.4% top AUC/AUPR gains in adults and children, respectively) when conducting self-supervised pre-training when compared to fully-supervised (i.e., non pre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPR in adult TB detection, and 0.697 AUC and 0.607 AUPR in zero-shot pediatric TB detection. As a result, this work demonstrates that self-supervised learning on adult CXRs effectively extends to challenging downstream tasks such as pediatric TB detection, where data are scarce.","sentences":["Tuberculosis (TB) remains a significant global health challenge, with pediatric cases posing a major concern.","The World Health Organization (WHO) advocates for chest X-rays (CXRs) for TB screening.","However, visual interpretation by radiologists can be subjective, time-consuming and prone to error, especially in pediatric TB.","Artificial intelligence (AI)-driven computer-aided detection (CAD) tools, especially those utilizing deep learning, show promise in enhancing lung disease detection.","However, challenges include data scarcity and lack of generalizability.","In this context, we propose a novel self-supervised paradigm leveraging Vision Transformers (ViT) for improved TB detection in CXR, enabling zero-shot pediatric TB detection.","We demonstrate improvements in TB detection performance ($\\sim$12.7% and $\\sim$13.4% top AUC/AUPR gains in adults and children, respectively) when conducting self-supervised pre-training when compared to fully-supervised (i.e., non pre-trained) ViT models, achieving top performances of 0.959 AUC and 0.962 AUPR in adult TB detection, and 0.697 AUC and 0.607 AUPR in zero-shot pediatric TB detection.","As a result, this work demonstrates that self-supervised learning on adult CXRs effectively extends to challenging downstream tasks such as pediatric TB detection, where data are scarce."],"url":"http://arxiv.org/abs/2402.14741v1","category":"eess.IV"}
{"created":"2024-02-22 17:49:32","title":"Eavesdropping with Intelligent Reflective Surfaces: Near-Optimal Configuration Cycling","abstract":"Intelligent reflecting surfaces (IRSs) have several prominent advantages, including improving the level of wireless communication security and privacy. In this work, we focus on the latter aspect and introduce a strategy to counteract the presence of passive eavesdroppers overhearing transmissions from a base station towards legitimate users that are facilitated by the presence of IRSs. Specifically, we envision a transmission scheme that cycles across a number of IRS-to-user assignments, and we select them in a near-optimal fashion, thus guaranteeing both a high data rate and a good secrecy rate. Unlike most of the existing works addressing passive eavesdropping, the strategy we envision has low complexity and is suitable for scenarios where nodes are equipped with a limited number of antennas. Through our performance evaluation, we highlight the trade-off between the legitimate users' data rate and secrecy rate, and how the system parameters affect such a trade-off.","sentences":["Intelligent reflecting surfaces (IRSs) have several prominent advantages, including improving the level of wireless communication security and privacy.","In this work, we focus on the latter aspect and introduce a strategy to counteract the presence of passive eavesdroppers overhearing transmissions from a base station towards legitimate users that are facilitated by the presence of IRSs.","Specifically, we envision a transmission scheme that cycles across a number of IRS-to-user assignments, and we select them in a near-optimal fashion, thus guaranteeing both a high data rate and a good secrecy rate.","Unlike most of the existing works addressing passive eavesdropping, the strategy we envision has low complexity and is suitable for scenarios where nodes are equipped with a limited number of antennas.","Through our performance evaluation, we highlight the trade-off between the legitimate users' data rate and secrecy rate, and how the system parameters affect such a trade-off."],"url":"http://arxiv.org/abs/2402.14737v1","category":"cs.NI"}
{"created":"2024-02-22 17:48:14","title":"Grouped approximate control variate estimators","abstract":"This paper analyzes the approximate control variate (ACV) approach to multifidelity uncertainty quantification in the case where weighted estimators are combined to form the components of the ACV. The weighted estimators enable one to precisely group models that share input samples to achieve improved variance reduction. We demonstrate that this viewpoint yields a generalized linear estimator that can assign any weight to any sample. This generalization shows that other linear estimators in the literature, particularly the multilevel best linear unbiased estimator (ML-BLUE) of Schaden and Ullman in 2020, becomes a specific version of the ACV estimator of Gorodetsky, Geraci, Jakeman, and Eldred, 2020. Moreover, this connection enables numerous extensions and insights. For example, we empirically show that having non-independent groups can yield better variance reduction compared to the independent groups used by ML-BLUE. Furthermore, we show that such grouped estimators can use arbitrary weighted estimators, not just the simple Monte Carlo estimators used in ML-BLUE. Furthermore, the analysis enables the derivation of ML-BLUE directly from a variance reduction perspective, rather than a regression perspective.","sentences":["This paper analyzes the approximate control variate (ACV) approach to multifidelity uncertainty quantification in the case where weighted estimators are combined to form the components of the ACV.","The weighted estimators enable one to precisely group models that share input samples to achieve improved variance reduction.","We demonstrate that this viewpoint yields a generalized linear estimator that can assign any weight to any sample.","This generalization shows that other linear estimators in the literature, particularly the multilevel best linear unbiased estimator (ML-BLUE) of Schaden and Ullman in 2020, becomes a specific version of the ACV estimator of Gorodetsky, Geraci, Jakeman, and Eldred, 2020.","Moreover, this connection enables numerous extensions and insights.","For example, we empirically show that having non-independent groups can yield better variance reduction compared to the independent groups used by ML-BLUE.","Furthermore, we show that such grouped estimators can use arbitrary weighted estimators, not just the simple Monte Carlo estimators used in ML-BLUE.","Furthermore, the analysis enables the derivation of ML-BLUE directly from a variance reduction perspective, rather than a regression perspective."],"url":"http://arxiv.org/abs/2402.14736v1","category":"stat.CO"}
{"created":"2024-02-22 17:47:03","title":"How Transformers Learn Causal Structure with Gradient Descent","abstract":"The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.","sentences":["The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence.","Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling.","However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood.","To better understand this process, we introduce an in-context learning task that requires learning latent causal structure.","We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer.","The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens.","As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph.","As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022).","We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures."],"url":"http://arxiv.org/abs/2402.14735v1","category":"cs.LG"}
{"created":"2024-02-22 17:44:44","title":"Constraining spherically symmetric metrics by the gap between photon rings","abstract":"Gravitational lensing of luminous matter that surrounds a black hole or some other sufficiently compact object produces an infinite sequence of images. Besides the direct (or primary) image, it comprises demagnified and deformed replicas of the original known as photon rings which are progressively nearing the boundary of the socalled shadow. In the present paper, we present analytical approximation formulas for higher-order photon rings for an asymptotically flat, static, spherically symmetric spacetime that admits a photon sphere. We consider a geometrically thin disk of light sources in the equatorial plane and an observer at arbitrary inclination far away from the center. Fixing the emission radius and leveraging the strong deflection limit, which provides an analytical logarithmic approximation for the deflection angle, we find the deformed shape of higher-order photon rings in the form of a polar equation on the observer's screen. It has been suggested by other authors to use the relative size of photon rings for characterizing the underlying spacetime. In particular, the relative separation between two neighboring photon rings, which we call \"gap parameter\", was considered. We analytically calculate the gap parameter of higher-order photon rings for metrics of the considered class that may depend on multiple parameters. The advantage of using this quantity is in the fact that, to within the assumed approximations, it is independent of the mass of the central object (or of some other characteristic parameter if the mass is zero) and of the distance of the observer. Measurements of the gap parameter, which may become possible in the near future, will restrict the spacetime models that are in agreement with the observations. Even without knowing the inner and outer radii of the shining disk, it will conclusively rule out some metrics. Some examples are provided.","sentences":["Gravitational lensing of luminous matter that surrounds a black hole or some other sufficiently compact object produces an infinite sequence of images.","Besides the direct (or primary) image, it comprises demagnified and deformed replicas of the original known as photon rings which are progressively nearing the boundary of the socalled shadow.","In the present paper, we present analytical approximation formulas for higher-order photon rings for an asymptotically flat, static, spherically symmetric spacetime that admits a photon sphere.","We consider a geometrically thin disk of light sources in the equatorial plane and an observer at arbitrary inclination far away from the center.","Fixing the emission radius and leveraging the strong deflection limit, which provides an analytical logarithmic approximation for the deflection angle, we find the deformed shape of higher-order photon rings in the form of a polar equation on the observer's screen.","It has been suggested by other authors to use the relative size of photon rings for characterizing the underlying spacetime.","In particular, the relative separation between two neighboring photon rings, which we call \"gap parameter\", was considered.","We analytically calculate the gap parameter of higher-order photon rings for metrics of the considered class that may depend on multiple parameters.","The advantage of using this quantity is in the fact that, to within the assumed approximations, it is independent of the mass of the central object (or of some other characteristic parameter if the mass is zero) and of the distance of the observer.","Measurements of the gap parameter, which may become possible in the near future, will restrict the spacetime models that are in agreement with the observations.","Even without knowing the inner and outer radii of the shining disk, it will conclusively rule out some metrics.","Some examples are provided."],"url":"http://arxiv.org/abs/2402.14733v1","category":"gr-qc"}
{"created":"2024-02-22 17:42:57","title":"Lefschetz operators on convex valuations","abstract":"We investigate the action of Alesker's Lefschetz operators on translation invariant valuations on convex bodies. For scalar valued valuations, we describe this action on the level of Klain-Schneider functions by a Radon type transform, generalizing a result by Schuster and Wannerer. In the case of rotationally equivariant Minkowski valuations, the Lefschetz operators act on the generating function as a convolution transform. We show that the convolution kernel satisfies a Legendre type differential equation, and thus, is a strictly positive function that is smooth up to one point.","sentences":["We investigate the action of Alesker's Lefschetz operators on translation invariant valuations on convex bodies.","For scalar valued valuations, we describe this action on the level of Klain-Schneider functions by a Radon type transform, generalizing a result by Schuster and Wannerer.","In the case of rotationally equivariant Minkowski valuations, the Lefschetz operators act on the generating function as a convolution transform.","We show that the convolution kernel satisfies a Legendre type differential equation, and thus, is a strictly positive function that is smooth up to one point."],"url":"http://arxiv.org/abs/2402.14731v1","category":"math.MG"}
{"created":"2024-02-22 17:42:15","title":"Clifford-Steerable Convolutional Neural Networks","abstract":"We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\\mathbb{R}^{p,q}$. They cover, for instance, $\\mathrm{E}(3)$-equivariance on $\\mathbb{R}^3$ and Poincar\\'e-equivariance on Minkowski spacetime $\\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.","sentences":["We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\\mathrm{E}(p, q)$-equivariant CNNs.","CS-CNNs process multivector fields on pseudo-Euclidean spaces $\\mathbb{R}^{p,q}$. They cover, for instance, $\\mathrm{E}(3)$-equivariance on $\\mathbb{R}^3$ and Poincar\\'e-equivariance on Minkowski spacetime $\\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks.","We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks."],"url":"http://arxiv.org/abs/2402.14730v1","category":"cs.LG"}
{"created":"2024-02-22 17:35:29","title":"The European Commitment to Human-Centered Technology: The Integral Role of HCI in the EU AI Act's Success","abstract":"The evolution of AI is set to profoundly reshape the future. The European Union, recognizing this impending prominence, has enacted the AI Act, regulating market access for AI-based systems. A salient feature of the Act is to guard democratic and humanistic values by focusing regulation on transparency, explainability, and the human ability to understand and control AI systems. Hereby, the EU AI Act does not merely specify technological requirements for AI systems. The EU issues a democratic call for human-centered AI systems and, in turn, an interdisciplinary research agenda for human-centered innovation in AI development. Without robust methods to assess AI systems and their effect on individuals and society, the EU AI Act may lead to repeating the mistakes of the General Data Protection Regulation of the EU and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more confusion than lending guidance. Moreover, determined research activities in Human-AI interaction will be pivotal for both regulatory compliance and the advancement of AI in a manner that is both ethical and effective. Such an approach will ensure that AI development aligns with human values and needs, fostering a technology landscape that is innovative, responsible, and an integral part of our society.","sentences":["The evolution of AI is set to profoundly reshape the future.","The European Union, recognizing this impending prominence, has enacted the AI Act, regulating market access for AI-based systems.","A salient feature of the Act is to guard democratic and humanistic values by focusing regulation on transparency, explainability, and the human ability to understand and control AI systems.","Hereby, the EU AI Act does not merely specify technological requirements for AI systems.","The EU issues a democratic call for human-centered AI systems and, in turn, an interdisciplinary research agenda for human-centered innovation in AI development.","Without robust methods to assess AI systems and their effect on individuals and society, the EU AI Act may lead to repeating the mistakes of the General Data Protection Regulation of the EU and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more confusion than lending guidance.","Moreover, determined research activities in Human-AI interaction will be pivotal for both regulatory compliance and the advancement of AI in a manner that is both ethical and effective.","Such an approach will ensure that AI development aligns with human values and needs, fostering a technology landscape that is innovative, responsible, and an integral part of our society."],"url":"http://arxiv.org/abs/2402.14728v1","category":"cs.HC"}
{"created":"2024-02-22 17:33:49","title":"Incorporating Expert Rules into Neural Networks in the Framework of Concept-Based Learning","abstract":"A problem of incorporating the expert rules into machine learning models for extending the concept-based learning is formulated in the paper. It is proposed how to combine logical rules and neural networks predicting the concept probabilities. The first idea behind the combination is to form constraints for a joint probability distribution over all combinations of concept values to satisfy the expert rules. The second idea is to represent a feasible set of probability distributions in the form of a convex polytope and to use its vertices or faces. We provide several approaches for solving the stated problem and for training neural networks which guarantee that the output probabilities of concepts would not violate the expert rules. The solution of the problem can be viewed as a way for combining the inductive and deductive learning. Expert rules are used in a broader sense when any logical function that connects concepts and class labels or just concepts with each other can be regarded as a rule. This feature significantly expands the class of the proposed results. Numerical examples illustrate the approaches. The code of proposed algorithms is publicly available.","sentences":["A problem of incorporating the expert rules into machine learning models for extending the concept-based learning is formulated in the paper.","It is proposed how to combine logical rules and neural networks predicting the concept probabilities.","The first idea behind the combination is to form constraints for a joint probability distribution over all combinations of concept values to satisfy the expert rules.","The second idea is to represent a feasible set of probability distributions in the form of a convex polytope and to use its vertices or faces.","We provide several approaches for solving the stated problem and for training neural networks which guarantee that the output probabilities of concepts would not violate the expert rules.","The solution of the problem can be viewed as a way for combining the inductive and deductive learning.","Expert rules are used in a broader sense when any logical function that connects concepts and class labels or just concepts with each other can be regarded as a rule.","This feature significantly expands the class of the proposed results.","Numerical examples illustrate the approaches.","The code of proposed algorithms is publicly available."],"url":"http://arxiv.org/abs/2402.14726v1","category":"cs.LG"}
{"created":"2024-02-22 17:31:24","title":"Run Time Assurance for Simultaneous Constraint Satisfaction During Spacecraft Attitude Maneuvering","abstract":"A fundamental capability for On-orbit Servicing, Assembly, and Manufacturing (OSAM) is inspection of the vehicle to be serviced, or the structure being assembled. This research assumes autonomous slewing to maintain situational awareness of multiple vehicles operating in close proximity where several safety constraints must be satisfied. A variety of techniques may be used as the primary controller. The focus of this research is developing Run Time Assurance (RTA) filters that monitor system behavior and the output of the primary controller to enforce safety constraint satisfaction. Specifically, this research explores combining a subset of the constraints into an Active Set Invariance Filter (ASIF) RTA defined using control barrier functions. This method is minimally invasive to the primary control by minimizing deviation from the desired control output of the primary controller, while simultaneously enforcing all safety constraints. The RTA is designed to ensure the spacecraft maintains attitude requirements for communication and data transfer with a ground station during scheduled communication windows, adheres to conical attitude keep out zones, limits thermally unfavorable attitude duration, maintains attitude requirements for sufficient power generation, ensures maneuvers are below threshold to cause structural damage, ensures maximum angular velocity is below limits to maintain ability to respond quickly to new slewing commands, and conserves actuator use to prevent wear when possible. Slack variables are introduced into the ASIF controller to prioritize safety constraints when a solution to all safety constraints is infeasible. Monte Carlo simulation results as well as plots of example cases are shown and evaluated for a three degree of freedom spacecraft with reaction wheel attitude control.","sentences":["A fundamental capability for On-orbit Servicing, Assembly, and Manufacturing (OSAM) is inspection of the vehicle to be serviced, or the structure being assembled.","This research assumes autonomous slewing to maintain situational awareness of multiple vehicles operating in close proximity where several safety constraints must be satisfied.","A variety of techniques may be used as the primary controller.","The focus of this research is developing Run Time Assurance (RTA) filters that monitor system behavior and the output of the primary controller to enforce safety constraint satisfaction.","Specifically, this research explores combining a subset of the constraints into an Active Set Invariance Filter (ASIF) RTA defined using control barrier functions.","This method is minimally invasive to the primary control by minimizing deviation from the desired control output of the primary controller, while simultaneously enforcing all safety constraints.","The RTA is designed to ensure the spacecraft maintains attitude requirements for communication and data transfer with a ground station during scheduled communication windows, adheres to conical attitude keep out zones, limits thermally unfavorable attitude duration, maintains attitude requirements for sufficient power generation, ensures maneuvers are below threshold to cause structural damage, ensures maximum angular velocity is below limits to maintain ability to respond quickly to new slewing commands, and conserves actuator use to prevent wear when possible.","Slack variables are introduced into the ASIF controller to prioritize safety constraints when a solution to all safety constraints is infeasible.","Monte Carlo simulation results as well as plots of example cases are shown and evaluated for a three degree of freedom spacecraft with reaction wheel attitude control."],"url":"http://arxiv.org/abs/2402.14723v1","category":"eess.SY"}
{"created":"2024-02-22 17:30:04","title":"A method for describing the maximal ideal in universal affine vertex algebras at non-admissible levels","abstract":"The problem of determining maximal ideals in universal affine vertex algebras is difficult for levels beyond admissible, since there are no simple character formulas which can be applied. Here we investigate when certain quotient $\\mathcal V$ of universal affine vertex algebra $V^k(\\mathfrak{g})$ is simple. We present a new method for proving simplicity of quotients of universal affine vertex algebras in the case of affine vertex algebra $L_{k_n}(\\mathfrak{sl}_{2n})$ at level $k_n:=-\\frac{2n+1}{2}$. In that way we describe the maximal ideal in $V^{k_n}(\\mathfrak{sl}_{2n})$. For that purpose, we use the representation theory of minimal affine $W$-algebra $W^{min}_{k_{n+1}}(\\mathfrak{sl}_{2n+2})$ developed in [2]. In particular, we use the embedding $L_{k_n}(\\mathfrak{sl}_{2n}) \\subset W^{min}_{k_{n+1}}(\\mathfrak{sl}_{2n+2})$ and fusion rules for $L_{k_n}(\\mathfrak{sl}_{2n})$--modules. We apply this result in the cases $n=3,4$ and prove that a maximal ideal is generated by one singular vector of conformal weight $4$. As a byproduct, we classify irreducible modules in the category $\\mathcal{O}$ for the simple affine vertex algebra $L_{-7/2}(\\mathfrak{sl}_{6})$.","sentences":["The problem of determining maximal ideals in universal affine vertex algebras is difficult for levels beyond admissible, since there are no simple character formulas which can be applied.","Here we investigate when certain quotient $\\mathcal V$ of universal affine vertex algebra $V^k(\\mathfrak{g})$ is simple.","We present a new method for proving simplicity of quotients of universal affine vertex algebras in the case of affine vertex algebra $L_{k_n}(\\mathfrak{sl}_{2n})$ at level $k_n:=-\\frac{2n+1}{2}$.","In that way we describe the maximal ideal in $V^{k_n}(\\mathfrak{sl}_{2n})$. For that purpose, we use the representation theory of minimal affine $W$-algebra $W^{min}_{k_{n+1}}(\\mathfrak{sl}_{2n+2})$ developed in [2].","In particular, we use the embedding $L_{k_n}(\\mathfrak{sl}_{2n})","\\subset W^{min}_{k_{n+1}}(\\mathfrak{sl}_{2n+2})$ and fusion rules for $L_{k_n}(\\mathfrak{sl}_{2n})$--modules.","We apply this result in the cases $n=3,4$ and prove that a maximal ideal is generated by one singular vector of conformal weight $4$. As a byproduct, we classify irreducible modules in the category $\\mathcal{O}$ for the simple affine vertex algebra $L_{-7/2}(\\mathfrak{sl}_{6})$."],"url":"http://arxiv.org/abs/2402.14722v1","category":"math.QA"}
{"created":"2024-02-22 17:27:30","title":"Anomalous Giant Superradiance in Molecular Aggregates Coupled to Polaritons","abstract":"In this study, we unveil an eccentric superradiance phenomenon in molecular aggregates coupled to surface plasmon polaritons. Through the quantization of electromagnetic fields in media, we demonstrate that superradiance can be significantly enhanced by polaritons and its behavior distinguishably surpasses the Dick's $N$ scaling law. To understand the mechanism of this anomalous phenomenon, we derive an analytical expression of the superradiance rate, which is general for molecular aggregates in arbitrary dispersive and absorbing media. Furthermore, we show the importance of intermolecular distance for this extraordinary superradiance.","sentences":["In this study, we unveil an eccentric superradiance phenomenon in molecular aggregates coupled to surface plasmon polaritons.","Through the quantization of electromagnetic fields in media, we demonstrate that superradiance can be significantly enhanced by polaritons and its behavior distinguishably surpasses the Dick's $N$ scaling law.","To understand the mechanism of this anomalous phenomenon, we derive an analytical expression of the superradiance rate, which is general for molecular aggregates in arbitrary dispersive and absorbing media.","Furthermore, we show the importance of intermolecular distance for this extraordinary superradiance."],"url":"http://arxiv.org/abs/2402.14721v1","category":"physics.chem-ph"}
{"created":"2024-02-22 17:20:42","title":"The cosmological constant and scale hierarchies with emergent gauge symmetries","abstract":"Motivated by the stability of the electroweak Higgs vacuum we consider the possibility that the Standard Model might work up to large scales between about $10^{10}$ GeV and close to the Planck scale. A plausible scenario is an emergent Standard Model with gauge symmetries originating in some topological like phase transition deep in the ultraviolet. In this case the cosmological constant scale and neutrino masses should be of similar size, suppressed by factor of the large scale of emergence. The key physics involves a subtle interplay of Poincar\\'e invariance, mass generation and renormalisation group invariance. The Higgs mass would be environmentally selected in connection with vacuum stability. Consequences for dark matter scenarios are discussed.","sentences":["Motivated by the stability of the electroweak Higgs vacuum we consider the possibility that the Standard Model might work up to large scales between about $10^{10}$ GeV and close to the Planck scale.","A plausible scenario is an emergent Standard Model with gauge symmetries originating in some topological like phase transition deep in the ultraviolet.","In this case the cosmological constant scale and neutrino masses should be of similar size, suppressed by factor of the large scale of emergence.","The key physics involves a subtle interplay of Poincar\\'e invariance, mass generation and renormalisation group invariance.","The Higgs mass would be environmentally selected in connection with vacuum stability.","Consequences for dark matter scenarios are discussed."],"url":"http://arxiv.org/abs/2402.14719v1","category":"hep-ph"}
{"created":"2024-02-22 17:12:39","title":"Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models","abstract":"This report introduces \\texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \\texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We open-source our models on Huggingface to empower the open research community in various languages.","sentences":["This report introduces \\texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding.","Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization.","In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens.","Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \\texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard.","We open-source our models on Huggingface to empower the open research community in various languages."],"url":"http://arxiv.org/abs/2402.14714v1","category":"cs.CL"}
{"created":"2024-02-22 17:12:03","title":"Deep Learning Models for Conditioning Extremely Noisy Signals","abstract":"This paper presents a comparison of several Convolutional Neural Network (CNN) models for extracting target signals in highly noisy measurement conditions. Four CNN architectures were investigated. The first comprises six consecutive convolutional blocks while the second employs a U-Net structure. The third architecture introduces a new model inspired by the principles of wavelet transform. It consists of three CNN blocks with varied kernel sizes branching from the input layer before merging into consecutive concatenation and dense layers. The fourth is a Multilevel Wavelet Convolutional Neural Network (MWCNN), resembling U-net but the upsampling and downsampling are replaced by Discrete Wavelet transform and its inverse respectively. To evaluate these architectures, synthetic data were generated using pulse trains corrupted with various degrees of Gaussian noise to simulate measurement conditions with signal-to-noise ratios (SNR) as low as -20 dB. The methodology encompassed the generation and processing of signals with varied parameters: period (5-25 ms), duty cycle (0.1-0.5), and amplitude (1-150 mV). Subsequently, the machine learning models were trained, validated and tested. Finally, the output is processed to recover the signal amplitude before standardisation. The modified MWCNN model demonstrated superior performance achieving a median of 25.9 dB with a mean Root Mean Square Error (RMSE) that decreases with the amplitude of the signal reaching an average RMSE of 0.000128 mV for 1 mV signals. However, for this architecture, the output SNR drops by a factor of 1.23 when the input SNR decreases by 1 dB. All of the architectures exhibited consistency when testing output SNR for different signal amplitudes, periods and duty cycles. These findings indicate that CNN architectures can be used to denoise signals with SNR as low as -20 dB[...]","sentences":["This paper presents a comparison of several Convolutional Neural Network (CNN) models for extracting target signals in highly noisy measurement conditions.","Four CNN architectures were investigated.","The first comprises six consecutive convolutional blocks while the second employs a U-Net structure.","The third architecture introduces a new model inspired by the principles of wavelet transform.","It consists of three CNN blocks with varied kernel sizes branching from the input layer before merging into consecutive concatenation and dense layers.","The fourth is a Multilevel Wavelet Convolutional Neural Network (MWCNN), resembling U-net but the upsampling and downsampling are replaced by Discrete Wavelet transform and its inverse respectively.","To evaluate these architectures, synthetic data were generated using pulse trains corrupted with various degrees of Gaussian noise to simulate measurement conditions with signal-to-noise ratios (SNR) as low as -20 dB. The methodology encompassed the generation and processing of signals with varied parameters: period (5-25 ms), duty cycle (0.1-0.5), and amplitude (1-150 mV).","Subsequently, the machine learning models were trained, validated and tested.","Finally, the output is processed to recover the signal amplitude before standardisation.","The modified MWCNN model demonstrated superior performance achieving a median of 25.9 dB with a mean Root Mean Square Error (RMSE) that decreases with the amplitude of the signal reaching an average RMSE of 0.000128 mV for 1 mV signals.","However, for this architecture, the output SNR drops by a factor of 1.23 when the input SNR decreases by 1 dB. All of the architectures exhibited consistency when testing output SNR for different signal amplitudes, periods and duty cycles.","These findings indicate that CNN architectures can be used to denoise signals with SNR as low as -20 dB[...]"],"url":"http://arxiv.org/abs/2402.14713v1","category":"eess.SP"}
{"created":"2024-02-22 17:11:41","title":"Observability for Nonlinear Systems: Connecting Variational Dynamics, Lyapunov Exponents, and Empirical Gramians","abstract":"Observability is a key problem in dynamic network sciences. While it has been thoroughly studied for linear systems, observability for nonlinear networks is less intuitive and more cumbersome. One common approach to quantify observability for nonlinear systems is via the Empirical Gramian (Empr-Gram) -- a generalized form of the Gramian of linear systems. In this technical note, we produce three new results. First, we establish that a variational form of nonlinear systems (computed via perturbing initial conditions) yields a so-called Variational Gramian (Var-Gram) that is equivalent to the classic Empr-Gram; the former being easier to compute than the latter. Via Lyapunov exponents derived from Lyapunov's direct method, the technical note's second result derives connections between vintage observability measures and Var-Gram. The third result demonstrates the applicability of these new notions for sensor selection/placement in nonlinear systems. Numerical case studies demonstrate these three developments and their merits.","sentences":["Observability is a key problem in dynamic network sciences.","While it has been thoroughly studied for linear systems, observability for nonlinear networks is less intuitive and more cumbersome.","One common approach to quantify observability for nonlinear systems is via the Empirical Gramian (Empr-Gram) -- a generalized form of the Gramian of linear systems.","In this technical note, we produce three new results.","First, we establish that a variational form of nonlinear systems (computed via perturbing initial conditions) yields a so-called Variational Gramian (Var-Gram) that is equivalent to the classic Empr-Gram; the former being easier to compute than the latter.","Via","Lyapunov exponents derived from Lyapunov's direct method, the technical note's second result derives connections between vintage observability measures and Var-Gram.","The third result demonstrates the applicability of these new notions for sensor selection/placement in nonlinear systems.","Numerical case studies demonstrate these three developments and their merits."],"url":"http://arxiv.org/abs/2402.14711v1","category":"eess.SY"}
{"created":"2024-02-22 17:11:38","title":"IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus","abstract":"Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.","sentences":["Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE).","Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema.","To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens.","We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus.","Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization.","We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community."],"url":"http://arxiv.org/abs/2402.14710v1","category":"cs.CL"}
{"created":"2024-02-22 17:08:09","title":"CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks","abstract":"Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters. Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes. Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods. Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions.","sentences":["Credit card fraud poses a significant threat to the economy.","While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions.","This paper introduces a novel method for credit card fraud detection, the \\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal \\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data.","By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability.","CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener.","The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environment nodes without introducing additional parameters.","Subsequently, the Causal-Intervener performs a causal mixup enhancement on environment nodes based on the set of nodes.","Evaluated on three datasets, including a private financial dataset and two public datasets, CaT-GNN demonstrates superior performance over existing state-of-the-art methods.","Our findings highlight the potential of integrating causal reasoning with graph neural networks to improve fraud detection capabilities in financial transactions."],"url":"http://arxiv.org/abs/2402.14708v1","category":"cs.LG"}
{"created":"2024-02-22 17:06:47","title":"Two-stage Cytopathological Image Synthesis for Augmenting Cervical Abnormality Screening","abstract":"Automatic thin-prep cytologic test (TCT) screening can assist pathologists in finding cervical abnormality towards accurate and efficient cervical cancer diagnosis. Current automatic TCT screening systems mostly involve abnormal cervical cell detection, which generally requires large-scale and diverse training data with high-quality annotations to achieve promising performance. Pathological image synthesis is naturally raised to minimize the efforts in data collection and annotation. However, it is challenging to generate realistic large-size cytopathological images while simultaneously synthesizing visually plausible appearances for small-size abnormal cervical cells. In this paper, we propose a two-stage image synthesis framework to create synthetic data for augmenting cervical abnormality screening. In the first Global Image Generation stage, a Normal Image Generator is designed to generate cytopathological images full of normal cervical cells. In the second Local Cell Editing stage, normal cells are randomly selected from the generated images and then are converted to different types of abnormal cells using the proposed Abnormal Cell Synthesizer. Both Normal Image Generator and Abnormal Cell Synthesizer are built upon the pre-trained Stable Diffusion via parameter-efficient fine-tuning methods for customizing cytopathological image contents and extending spatial layout controllability, respectively. Our experiments demonstrate the synthetic image quality, diversity, and controllability of the proposed synthesis framework, and validate its data augmentation effectiveness in enhancing the performance of abnormal cervical cell detection.","sentences":["Automatic thin-prep cytologic test (TCT) screening can assist pathologists in finding cervical abnormality towards accurate and efficient cervical cancer diagnosis.","Current automatic TCT screening systems mostly involve abnormal cervical cell detection, which generally requires large-scale and diverse training data with high-quality annotations to achieve promising performance.","Pathological image synthesis is naturally raised to minimize the efforts in data collection and annotation.","However, it is challenging to generate realistic large-size cytopathological images while simultaneously synthesizing visually plausible appearances for small-size abnormal cervical cells.","In this paper, we propose a two-stage image synthesis framework to create synthetic data for augmenting cervical abnormality screening.","In the first Global Image Generation stage, a Normal Image Generator is designed to generate cytopathological images full of normal cervical cells.","In the second Local Cell Editing stage, normal cells are randomly selected from the generated images and then are converted to different types of abnormal cells using the proposed Abnormal Cell Synthesizer.","Both Normal Image Generator and Abnormal Cell Synthesizer are built upon the pre-trained Stable Diffusion via parameter-efficient fine-tuning methods for customizing cytopathological image contents and extending spatial layout controllability, respectively.","Our experiments demonstrate the synthetic image quality, diversity, and controllability of the proposed synthesis framework, and validate its data augmentation effectiveness in enhancing the performance of abnormal cervical cell detection."],"url":"http://arxiv.org/abs/2402.14707v1","category":"cs.CV"}
{"created":"2024-02-22 17:05:54","title":"Engineering and Revealing Dirac Strings in Spinor Condensates","abstract":"Artificial monopoles have been engineered in various systems, yet there has been no systematic study on the singular vector potentials associated with the monopole field. We show that the Dirac string, the line singularity of the vector potential, can be engineered, manipulated, and made manifest in a spinor atomic condensate. We elucidate the connection among spin, orbital degrees of freedom, and the artificial gauge, and reveal that there exists a mapping between the vortex filament and the Dirac string. We also devise a proposal where preparing initial spin states with relevant symmetries can result in different vortex patterns, revealing an underlying correspondence between the internal spin states and the spherical vortex structures. Such a mapping also leads to a new way of constructing monopole harmonics. Our observation provides significant insights in quantum matter possessing internal symmetries in curved spaces.","sentences":["Artificial monopoles have been engineered in various systems, yet there has been no systematic study on the singular vector potentials associated with the monopole field.","We show that the Dirac string, the line singularity of the vector potential, can be engineered, manipulated, and made manifest in a spinor atomic condensate.","We elucidate the connection among spin, orbital degrees of freedom, and the artificial gauge, and reveal that there exists a mapping between the vortex filament and the Dirac string.","We also devise a proposal where preparing initial spin states with relevant symmetries can result in different vortex patterns, revealing an underlying correspondence between the internal spin states and the spherical vortex structures.","Such a mapping also leads to a new way of constructing monopole harmonics.","Our observation provides significant insights in quantum matter possessing internal symmetries in curved spaces."],"url":"http://arxiv.org/abs/2402.14705v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-22 17:00:50","title":"On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation","abstract":"We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such as outcome coverage and belief coverage. These assumptions not only enable polynomial bounds on the aforementioned quantities, but also lead to the discovery of new algorithms with complementary properties.","sentences":["We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon.","While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object.","Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space.","However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method.","In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such as outcome coverage and belief coverage.","These assumptions not only enable polynomial bounds on the aforementioned quantities, but also lead to the discovery of new algorithms with complementary properties."],"url":"http://arxiv.org/abs/2402.14703v1","category":"cs.LG"}
{"created":"2024-02-22 16:56:44","title":"COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling","abstract":"The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic modeling techniques in combination with generative language prompting, we analyze the topical characteristics of different psychiatric conditions and incorporate temporal modeling to capture the evolution of topics at a turn-level resolution. This combined framework enhances the understanding of therapeutic interactions, enabling timely feedback for therapists regarding conversation quality and providing interpretable insights to improve the effectiveness of psychotherapy.","sentences":["The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment.","Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients.","In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions.","Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory.","Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated.","By employing various neural topic modeling techniques in combination with generative language prompting, we analyze the topical characteristics of different psychiatric conditions and incorporate temporal modeling to capture the evolution of topics at a turn-level resolution.","This combined framework enhances the understanding of therapeutic interactions, enabling timely feedback for therapists regarding conversation quality and providing interpretable insights to improve the effectiveness of psychotherapy."],"url":"http://arxiv.org/abs/2402.14701v1","category":"cs.CL"}
{"created":"2024-02-22 16:56:13","title":"Unveiling Linguistic Regions in Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions exist for different monolingual families, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common occurrence observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence.","sentences":["Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability.","Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities.","However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment.","From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs.","We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters.","Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages.","Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence.","Moreover, we discover that distinct regions exist for different monolingual families, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages.","Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common occurrence observed during further pre-training of LLMs.","Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence."],"url":"http://arxiv.org/abs/2402.14700v1","category":"cs.CL"}
{"created":"2024-02-22 16:50:32","title":"Big data analytics to classify earthwork-related locations: A Chengdu study","abstract":"Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called Alpha MAPS in Chengdu to provide decision support for urban pollution control.","sentences":["Air pollution has significantly intensified, leading to severe health consequences worldwide.","Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution.","The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution.","To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data.","We compared several prediction models and investigated the relationship between features and dust pollution sources using real data.","The results demonstrate that high-accuracy classification can be achieved with a limited number of features.","This method was successfully implemented in the system called Alpha MAPS in Chengdu to provide decision support for urban pollution control."],"url":"http://arxiv.org/abs/2402.14698v1","category":"cs.LG"}
{"created":"2024-02-22 16:49:59","title":"On Schr\u00f6dingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms","abstract":"We analyze the Schr\\\"odingerisation method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr\\\"odingerisation technique, introduced in \\cite{JLY22a,JLY23}, transforms any linear ordinary and partial differential equations with non-unitary dynamics into a system under unitary dynamics via a warped phase transition that maps the equations into a higher dimension, making them suitable for quantum simulation. This technique can also be applied to these equations with inhomogeneous terms modeling source or forcing terms or boundary and interface conditions, and discrete dynamical systems such as iterative methods in numerical linear algebra, through extra equations in the system. Difficulty airses with the presense of inhomogeneous terms since it can change the stability of the original system.   In this paper, we systematically study--both theoretically and numerically--the important issue of recovering the original variables from the Schr\\\"odingerized equations, even when the evolution operator contains unstable modes. We show that even with unstable modes, one can still construct a stable scheme, yet to recover the original variable one needs to use suitable data in the extended space. We analyze and compare both the discrete and continuous Fourier transforms used in the extended dimension, and derive corresponding error estimates, which allows one to use the more appropriate transform for specific equations. We also provide a smoother initialization for the Schrod\\\"odingerized system to gain higher order accuracy in the extended space. We homogenize the inhomogeneous terms with a stretch transformation, making it easier to recover the original variable. Our recovering technique also provides a simple and generic framework to solve general ill-posed problems in a computationally stable way.","sentences":["We analyze the Schr\\\"odingerisation method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms.","The Schr\\\"odingerisation technique, introduced in \\cite{JLY22a,JLY23}, transforms any linear ordinary and partial differential equations with non-unitary dynamics into a system under unitary dynamics via a warped phase transition that maps the equations into a higher dimension, making them suitable for quantum simulation.","This technique can also be applied to these equations with inhomogeneous terms modeling source or forcing terms or boundary and interface conditions, and discrete dynamical systems such as iterative methods in numerical linear algebra, through extra equations in the system.","Difficulty airses with the presense of inhomogeneous terms since it can change the stability of the original system.   ","In this paper, we systematically study--both theoretically and numerically--the important issue of recovering the original variables from the Schr\\\"odingerized equations, even when the evolution operator contains unstable modes.","We show that even with unstable modes, one can still construct a stable scheme, yet to recover the original variable one needs to use suitable data in the extended space.","We analyze and compare both the discrete and continuous Fourier transforms used in the extended dimension, and derive corresponding error estimates, which allows one to use the more appropriate transform for specific equations.","We also provide a smoother initialization for the Schrod\\\"odingerized system to gain higher order accuracy in the extended space.","We homogenize the inhomogeneous terms with a stretch transformation, making it easier to recover the original variable.","Our recovering technique also provides a simple and generic framework to solve general ill-posed problems in a computationally stable way."],"url":"http://arxiv.org/abs/2402.14696v1","category":"math.NA"}
{"created":"2024-02-22 16:48:17","title":"A Quick Introduction to Quantum Machine Learning for Non-Practitioners","abstract":"This paper provides an introduction to quantum machine learning, exploring the potential benefits of using quantum computing principles and algorithms that may improve upon classical machine learning approaches. Quantum computing utilizes particles governed by quantum mechanics for computational purposes, leveraging properties like superposition and entanglement for information representation and manipulation. Quantum machine learning applies these principles to enhance classical machine learning models, potentially reducing network size and training time on quantum hardware. The paper covers basic quantum mechanics principles, including superposition, phase space, and entanglement, and introduces the concept of quantum gates that exploit these properties. It also reviews classical deep learning concepts, such as artificial neural networks, gradient descent, and backpropagation, before delving into trainable quantum circuits as neural networks. An example problem demonstrates the potential advantages of quantum neural networks, and the appendices provide detailed derivations. The paper aims to help researchers new to quantum mechanics and machine learning develop their expertise more efficiently.","sentences":["This paper provides an introduction to quantum machine learning, exploring the potential benefits of using quantum computing principles and algorithms that may improve upon classical machine learning approaches.","Quantum computing utilizes particles governed by quantum mechanics for computational purposes, leveraging properties like superposition and entanglement for information representation and manipulation.","Quantum machine learning applies these principles to enhance classical machine learning models, potentially reducing network size and training time on quantum hardware.","The paper covers basic quantum mechanics principles, including superposition, phase space, and entanglement, and introduces the concept of quantum gates that exploit these properties.","It also reviews classical deep learning concepts, such as artificial neural networks, gradient descent, and backpropagation, before delving into trainable quantum circuits as neural networks.","An example problem demonstrates the potential advantages of quantum neural networks, and the appendices provide detailed derivations.","The paper aims to help researchers new to quantum mechanics and machine learning develop their expertise more efficiently."],"url":"http://arxiv.org/abs/2402.14694v1","category":"quant-ph"}
{"created":"2024-02-22 16:47:15","title":"PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model","abstract":"This paper presents a neural vocoder based on a denoising diffusion probabilistic model (DDPM) incorporating explicit periodic signals as auxiliary conditioning signals. Recently, DDPM-based neural vocoders have gained prominence as non-autoregressive models that can generate high-quality waveforms. The neural vocoders based on DDPM have the advantage of training with a simple time-domain loss. In practical applications, such as singing voice synthesis, there is a demand for neural vocoders to generate high-fidelity speech waveforms with flexible pitch control. However, conventional DDPM-based neural vocoders struggle to generate speech waveforms under such conditions. Our proposed model aims to accurately capture the periodic structure of speech waveforms by incorporating explicit periodic signals. Experimental results show that our model improves sound quality and provides better pitch control than conventional DDPM-based neural vocoders.","sentences":["This paper presents a neural vocoder based on a denoising diffusion probabilistic model (DDPM) incorporating explicit periodic signals as auxiliary conditioning signals.","Recently, DDPM-based neural vocoders have gained prominence as non-autoregressive models that can generate high-quality waveforms.","The neural vocoders based on DDPM have the advantage of training with a simple time-domain loss.","In practical applications, such as singing voice synthesis, there is a demand for neural vocoders to generate high-fidelity speech waveforms with flexible pitch control.","However, conventional DDPM-based neural vocoders struggle to generate speech waveforms under such conditions.","Our proposed model aims to accurately capture the periodic structure of speech waveforms by incorporating explicit periodic signals.","Experimental results show that our model improves sound quality and provides better pitch control than conventional DDPM-based neural vocoders."],"url":"http://arxiv.org/abs/2402.14692v1","category":"eess.AS"}
{"created":"2024-02-22 16:45:32","title":"UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models","abstract":"Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or \\textit{hallucination}. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose \\texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most QA tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented QA tasks. In news fact generation tasks, search engine results and LLM knowledge are essential. Our dataset and code are available at \\url{https://github.com/WaldenRUC/UFO}.","sentences":["Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or \\textit{hallucination}.","Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source.","However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored.","To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets.","Then, we propose \\texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources.","We implement five evaluation scenarios based on this framework.","Experimental results show that for most QA tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented QA tasks.","In news fact generation tasks, search engine results and LLM knowledge are essential.","Our dataset and code are available at \\url{https://github.com/WaldenRUC/UFO}."],"url":"http://arxiv.org/abs/2402.14690v1","category":"cs.CL"}
{"created":"2024-02-22 16:45:04","title":"SVD, joint-MVD, Berry phase, and generic loss of rank for a matrix valued function of 2 parameters","abstract":"In this work we consider generic losses of rank for complex valued matrix functions depending on two parameters. We give theoretical results that characterize parameter regions where these losses of rank occur. Our main results consist in showing how following an appropriate smooth SVD along a closed loop it is possible to monitor the Berry phases accrued by the singular vectors to decide if -- inside the loop -- there are parameter values where a loss of rank takes place. It will be needed to use a new construction of a smooth SVD, which we call the \"joint-MVD\" (minimum variation decomposition).","sentences":["In this work we consider generic losses of rank for complex valued matrix functions depending on two parameters.","We give theoretical results that characterize parameter regions where these losses of rank occur.","Our main results consist in showing how following an appropriate smooth SVD along a closed loop it is possible to monitor the Berry phases accrued by the singular vectors to decide if -- inside the loop -- there are parameter values where a loss of rank takes place.","It will be needed to use a new construction of a smooth SVD, which we call the \"joint-MVD\" (minimum variation decomposition)."],"url":"http://arxiv.org/abs/2402.14689v1","category":"math.RA"}
{"created":"2024-02-22 16:43:16","title":"Q-Probe: A Lightweight Approach to Reward Maximization for Language Models","abstract":"We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be trained on top of an API since it only assumes access to sampling and embeddings. Code: https://github.com/likenneth/q_probe .","sentences":["We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function.","At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either.","The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions.","We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases.","To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients.","With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes.","Moreover, a Q-probe can be trained on top of an API since it only assumes access to sampling and embeddings.","Code: https://github.com/likenneth/q_probe ."],"url":"http://arxiv.org/abs/2402.14688v1","category":"cs.LG"}
{"created":"2024-02-22 16:41:00","title":"Interferometry of Atomic Matter Waves in the Cold Atom Lab onboard the International Space Station","abstract":"Ultracold atomic gases hold unique promise for space science by capitalizing on quantum advantages and extended freefall, afforded in a microgravity environment, to enable next-generation precision sensors. Atom interferometers are a class of quantum sensors which can use freely falling gases of atoms cooled to sub-photon-recoil temperatures to provide unprecedented sensitivities to accelerations, rotations, and gravitational forces, and are currently being developed for space-based applications in gravitational, earth, and planetary sciences, as well as to search for subtle forces that could signify physics beyond General Relativity and the Standard Model. NASA's Cold Atom Lab (CAL) operates onboard the International Space Station as a multi-user facility for studies of ultracold atoms and to mature quantum technologies, including atom interferometry, in persistent microgravity. In this paper, we report on path-finding experiments utilizing ultracold $^{87}$Rb atoms in the CAL atom interferometer, which was enabled by an on-orbit upgrade of the CAL science module: A three-pulse Mach-Zehnder interferometer was studied to understand limitations from the influence of ISS vibrations. Additionally, Ramsey shear-wave interferometry was used to manifest interference patterns in a single run that were observable for over 150 ms free-expansion time. Finally, the CAL atom interferometer was used to remotely measure the photon recoil from the atom interferometer laser as a demonstration of the first quantum sensor using matter-wave interferometry in space.","sentences":["Ultracold atomic gases hold unique promise for space science by capitalizing on quantum advantages and extended freefall, afforded in a microgravity environment, to enable next-generation precision sensors.","Atom interferometers are a class of quantum sensors which can use freely falling gases of atoms cooled to sub-photon-recoil temperatures to provide unprecedented sensitivities to accelerations, rotations, and gravitational forces, and are currently being developed for space-based applications in gravitational, earth, and planetary sciences, as well as to search for subtle forces that could signify physics beyond General Relativity and the Standard Model.","NASA's Cold Atom Lab (CAL) operates onboard the International Space Station as a multi-user facility for studies of ultracold atoms and to mature quantum technologies, including atom interferometry, in persistent microgravity.","In this paper, we report on path-finding experiments utilizing ultracold $^{87}$Rb atoms in the CAL atom interferometer, which was enabled by an on-orbit upgrade of the CAL science module: A three-pulse Mach-Zehnder interferometer was studied to understand limitations from the influence of ISS vibrations.","Additionally, Ramsey shear-wave interferometry was used to manifest interference patterns in a single run that were observable for over 150 ms free-expansion time.","Finally, the CAL atom interferometer was used to remotely measure the photon recoil from the atom interferometer laser as a demonstration of the first quantum sensor using matter-wave interferometry in space."],"url":"http://arxiv.org/abs/2402.14685v1","category":"physics.atom-ph"}
{"created":"2024-02-22 16:40:33","title":"Visual Hallucinations of Multi-modal Large Language Models","abstract":"Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.","sentences":["Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering.","Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances.","In this work, we propose a tool called VHTest to generate a diverse set of VH instances.","Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions.","We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest.","We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark.","Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks.","Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest."],"url":"http://arxiv.org/abs/2402.14683v1","category":"cs.CV"}
{"created":"2024-02-22 16:33:48","title":"Neutron-nucleus dynamics simulations for quantum computers","abstract":"With a view toward addressing the explosive growth in the computational demands of nuclear structure and reactions modeling, we develop a novel quantum algorithm for neutron-nucleus simulations with general potentials, which provides acceptable bound-state energies even in the presence of noise, through the noise-resilient training method. In particular, the algorithm can now solve for any band-diagonal to full Hamiltonian matrices, as needed to accommodate a general central potential. This includes exponential Gaussian-like potentials and ab initio inter-cluster potentials (optical potentials). The approach can also accommodate the complete form of the chiral effective-field-theory nucleon-nucleon potentials used in ab initio nuclear calculations. We make this potential available for three different qubit encodings, including the one-hot (OHE), binary (BE), and Gray encodings (GE), and we provide a comprehensive analysis of the number of Pauli terms and commuting sets involved. We find that the GE allows for an efficient scaling of the model-space size $N$ (or number of basis states used) and is more resource efficient not only for tridiagonal Hamiltonians, but also for band-diagonal Hamiltonians having bandwidth up to $N$. We introduce a new commutativity scheme called distance-grouped commutativity (DGC) and compare its performance with the well-known qubit-commutativity (QC) scheme. We lay out the explicit grouping of Pauli strings and the diagonalizing unitary under the DGC scheme, and we find that it outperforms the QC scheme, at the cost of a more complex diagonalizing unitary. Lastly, we provide first solutions of the neutron-alpha dynamics from quantum simulations suitable for NISQ processors, using an optical potential rooted in first principles, and a study of the bound-state physics in neutron-Carbon systems, along with a comparison of the efficacy of the OHE and GE.","sentences":["With a view toward addressing the explosive growth in the computational demands of nuclear structure and reactions modeling, we develop a novel quantum algorithm for neutron-nucleus simulations with general potentials, which provides acceptable bound-state energies even in the presence of noise, through the noise-resilient training method.","In particular, the algorithm can now solve for any band-diagonal to full Hamiltonian matrices, as needed to accommodate a general central potential.","This includes exponential Gaussian-like potentials and ab initio inter-cluster potentials (optical potentials).","The approach can also accommodate the complete form of the chiral effective-field-theory nucleon-nucleon potentials used in ab initio nuclear calculations.","We make this potential available for three different qubit encodings, including the one-hot (OHE), binary (BE), and Gray encodings (GE), and we provide a comprehensive analysis of the number of Pauli terms and commuting sets involved.","We find that the GE allows for an efficient scaling of the model-space size $N$ (or number of basis states used) and is more resource efficient not only for tridiagonal Hamiltonians, but also for band-diagonal Hamiltonians having bandwidth up to $N$. We introduce a new commutativity scheme called distance-grouped commutativity (DGC) and compare its performance with the well-known qubit-commutativity (QC) scheme.","We lay out the explicit grouping of Pauli strings and the diagonalizing unitary under the DGC scheme, and we find that it outperforms the QC scheme, at the cost of a more complex diagonalizing unitary.","Lastly, we provide first solutions of the neutron-alpha dynamics from quantum simulations suitable for NISQ processors, using an optical potential rooted in first principles, and a study of the bound-state physics in neutron-Carbon systems, along with a comparison of the efficacy of the OHE and GE."],"url":"http://arxiv.org/abs/2402.14680v1","category":"quant-ph"}
{"created":"2024-02-22 16:29:38","title":"Constant $Q$-curvature metrics on a product Riemannian manifold","abstract":"Let $(M,g)$ be an analytic Riemannian manifold of dimension $n \\geq 5$. In this paper, we consider the so-called constant $Q$-curvature equation \\[ \\varepsilon^4\\Delta_{g}^2 u -\\varepsilon^2 b \\Delta_{g} u +a u = u^{p} , \\qquad \\text{in } M, \\quad u>0, \\quad u\\in H^2_g(M) \\] where $a,b$ are positive constants such that $b^2-4 a>0$, $p$ is a sub-critical exponent $1<p<2^\\#-1=\\frac{n+4}{n-4}$, $\\Delta_g$ denotes the Laplace-Beltrami operator and $\\Delta_g^2:=\\Delta_{g}(\\Delta_{g})$ is the bilaplacian operator on $M$.   We show that, if $\\varepsilon>0$ is small enough, then positive solutions to the above constant $Q$-curvature equation are generated by a maximum or minimum point of the function $\\tau_g$, given by \\[   \\tau_g(\\xi):= \\sum_{i, j=1}^{n} \\frac{\\partial^{2} g_{\\xi}^{i i}}{\\partial z_{j}^{2}}(0),   \\] where $g_{\\xi}^{i j}$ denotes the components of the inverse of the metric $g$ in geodesic normal coordinates. This result shows that the geometry of $M$ plays a crucial role in finding solutions to the equation above and provides a metric of constant $Q$-curvature on a product manifold of the form $(M\\times X, g+\\varepsilon^2 h)$ where $(M,g)$ is flat and closed, and $(X,h)$ any $m$-dimensional Einstein Riemannian manifold, $m\\geq 3$.","sentences":["Let $(M,g)$ be an analytic Riemannian manifold of dimension $n \\geq 5$.","In this paper, we consider the so-called constant $Q$-curvature equation \\[ \\varepsilon^4\\Delta_{g}^2 u -\\varepsilon^2 b \\Delta_{g} u +a u = u^{p} , \\qquad \\text{in } M, \\quad u>0, \\quad u\\in H^2_g(M) \\] where $a,b$ are positive constants such that $b^2-4 a>0$, $p$ is a sub-critical exponent $1<p<2^\\#-1=\\frac{n+4}{n-4}$, $\\Delta_g$ denotes the Laplace-Beltrami operator and $\\Delta_g^2:=\\Delta_{g}(\\Delta_{g})$ is the bilaplacian operator on $M$.   We show that, if $\\varepsilon>0$ is small enough, then positive solutions to the above constant $Q$-curvature equation are generated by a maximum or minimum point of the function $\\tau_g$, given by \\[   \\tau_g(\\xi):= \\sum_{i, j=1}^{n} \\frac{\\partial^{2} g_{\\xi}^{i i}}{\\partial z_{j}^{2}}(0),   \\] where $g_{\\xi}^{i j}$ denotes the components of the inverse of the metric $g$ in geodesic normal coordinates.","This result shows that the geometry of $M$ plays a crucial role in finding solutions to the equation above and provides a metric of constant $Q$-curvature on a product manifold of the form $(M\\times X, g+\\varepsilon^2 h)$ where $(M,g)$ is flat and closed, and $(X,h)$ any $m$-dimensional Einstein Riemannian manifold, $m\\geq 3$."],"url":"http://arxiv.org/abs/2402.14675v1","category":"math.DG"}
{"created":"2024-02-22 16:29:31","title":"Doing AI: Algorithmic decision support as a human activity","abstract":"Algorithmic decision support (ADS), using Machine-Learning-based AI, is becoming a major part of many processes. Organizations introduce ADS to improve decision-making and make optimal use of data, thereby possibly avoiding deviations from the normative \"homo economicus\" and the biases that characterize human decision-making. A closer look at the development process of ADS systems reveals that ADS itself results from a series of largely unspecified human decisions. They begin with deliberations for which decisions to use ADS, continue with choices while developing the ADS, and end with using the ADS output for decisions. Finally, conclusions are implemented in organizational settings, often without analyzing the implications of the decision support. The paper explores some issues in developing and using ADS, pointing to behavioral aspects that should be considered when implementing ADS in organizational settings. It points out directions for further research, which is essential for gaining an informed understanding of the processes and their vulnerabilities.","sentences":["Algorithmic decision support (ADS), using Machine-Learning-based AI, is becoming a major part of many processes.","Organizations introduce ADS to improve decision-making and make optimal use of data, thereby possibly avoiding deviations from the normative \"homo economicus\" and the biases that characterize human decision-making.","A closer look at the development process of ADS systems reveals that ADS itself results from a series of largely unspecified human decisions.","They begin with deliberations for which decisions to use ADS, continue with choices while developing the ADS, and end with using the ADS output for decisions.","Finally, conclusions are implemented in organizational settings, often without analyzing the implications of the decision support.","The paper explores some issues in developing and using ADS, pointing to behavioral aspects that should be considered when implementing ADS in organizational settings.","It points out directions for further research, which is essential for gaining an informed understanding of the processes and their vulnerabilities."],"url":"http://arxiv.org/abs/2402.14674v1","category":"cs.HC"}
{"created":"2024-02-22 16:18:07","title":"Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments","abstract":"The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with these tools, GPT-4 achieves 2.8X the performance of the best baseline in tasks requiring access to database content and 2.2X in KB tasks. Our findings illuminate the path for advancing language agents in complex real-world applications.","sentences":["The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments.","These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory.","Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity.","To this end, we design customized tools to aid in the proactive exploration within these massive environments.","Such tools can serve as a middleware layer shielding the LLM from environmental complexity.","In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments.","Notably, equipped with these tools, GPT-4 achieves 2.8X the performance of the best baseline in tasks requiring access to database content and 2.2X in KB tasks.","Our findings illuminate the path for advancing language agents in complex real-world applications."],"url":"http://arxiv.org/abs/2402.14672v1","category":"cs.CL"}
{"created":"2024-02-22 16:11:07","title":"Differential equations for the series of hypermaps with control on their full degree profile","abstract":"We consider the generating series of oriented and non-oriented hypermaps with controlled degrees of vertices, hyperedges and faces. It is well known that these series have natural expansions in terms of Schur and Zonal symmetric functions, and with some particular specializations, they satisfy the celebrated KP and BKP equations.   We prove that the full generating series of hypermaps satisfy a family of differential equations. We give a first proof which works for an $\\alpha$ deformation of these series related to Jack polynomials. This proof is based on a recent construction formula for Jack characters using differential operators. We also provide a combinatorial proof for the orientable case.   Our approach also applies to the series of $k$-constellations with control of the degrees of vertices of all colors. In other words, we obtain an equation for the generating function of Hurwitz numbers (and their $\\alpha$-deformations) with control of full ramification profiles above an arbitrary number of points. Such equations are new even in the orientable case.","sentences":["We consider the generating series of oriented and non-oriented hypermaps with controlled degrees of vertices, hyperedges and faces.","It is well known that these series have natural expansions in terms of Schur and Zonal symmetric functions, and with some particular specializations, they satisfy the celebrated KP and BKP equations.   ","We prove that the full generating series of hypermaps satisfy a family of differential equations.","We give a first proof which works for an $\\alpha$ deformation of these series related to Jack polynomials.","This proof is based on a recent construction formula for Jack characters using differential operators.","We also provide a combinatorial proof for the orientable case.   ","Our approach also applies to the series of $k$-constellations with control of the degrees of vertices of all colors.","In other words, we obtain an equation for the generating function of Hurwitz numbers (and their $\\alpha$-deformations) with control of full ramification profiles above an arbitrary number of points.","Such equations are new even in the orientable case."],"url":"http://arxiv.org/abs/2402.14668v1","category":"math.CO"}
{"created":"2024-02-22 16:10:42","title":"Stability of P2P Networks Under Greedy Peering (Full Version)","abstract":"Major cryptocurrency networks have relied on random peering choice rules for making connections in their peer-to-peer networks. Generally, these choices have good properties, particularly for open, permissionless networks. Random peering choices however do not take into account that some actors may choose to optimize who they connect to such that they are quicker to hear about information being propagated in the network. In this paper, we explore the dynamics of such greedy strategies. We study a model in which nodes select peers with the objective of minimizing their average distance to a designated subset of nodes in the network, and consider the impact of several factors including the peer selection process, degree constraints, and the size of the designated subset. The latter is particularly interesting in the context of blockchain networks as generally only a subset of nodes are the propagation source for content.   We first analyze an idealized version of the game where each node has full knowledge of the current network and aims to select the $d$ best connections, and prove the existence of equilibria under various model assumptions. Since in reality nodes only have local knowledge based on their peers' behavior, we also study a greedy protocol which runs in rounds, with each node replacing its worst-performing edge with a new random edge. We exactly characterize stability properties of networks that evolve with this peering rule and derive regimes where stability is possible and even inevitable. We also run extensive simulations with this peering rule examining both how the network evolves and how different network parameters affect the stability properties of the network. Our findings generally show that the only stable networks that arise from greedy peering choices are low-diameter and result in disparate performance for nodes in the network.","sentences":["Major cryptocurrency networks have relied on random peering choice rules for making connections in their peer-to-peer networks.","Generally, these choices have good properties, particularly for open, permissionless networks.","Random peering choices however do not take into account that some actors may choose to optimize who they connect to such that they are quicker to hear about information being propagated in the network.","In this paper, we explore the dynamics of such greedy strategies.","We study a model in which nodes select peers with the objective of minimizing their average distance to a designated subset of nodes in the network, and consider the impact of several factors including the peer selection process, degree constraints, and the size of the designated subset.","The latter is particularly interesting in the context of blockchain networks as generally only a subset of nodes are the propagation source for content.   ","We first analyze an idealized version of the game where each node has full knowledge of the current network and aims to select the $d$ best connections, and prove the existence of equilibria under various model assumptions.","Since in reality nodes only have local knowledge based on their peers' behavior, we also study a greedy protocol which runs in rounds, with each node replacing its worst-performing edge with a new random edge.","We exactly characterize stability properties of networks that evolve with this peering rule and derive regimes where stability is possible and even inevitable.","We also run extensive simulations with this peering rule examining both how the network evolves and how different network parameters affect the stability properties of the network.","Our findings generally show that the only stable networks that arise from greedy peering choices are low-diameter and result in disparate performance for nodes in the network."],"url":"http://arxiv.org/abs/2402.14666v1","category":"cs.GT"}
{"created":"2024-02-22 16:09:45","title":"Bayesian Off-Policy Evaluation and Learning for Large Action Spaces","abstract":"In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.","sentences":["In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces.","We introduce a unified Bayesian framework to capture these correlations through structured and informative priors.","In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations.","Notably, sDM leverages action correlations without compromising computational efficiency.","Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments.","We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations.","Empirical evidence showcases the strong performance of sDM."],"url":"http://arxiv.org/abs/2402.14664v1","category":"cs.LG"}
{"created":"2024-02-22 16:08:25","title":"An upper limit to differential magnification effects in strongly gravitationally lensed galaxies","abstract":"Differential magnification is now well-known to distort the spectral energy distributions of strongly gravitationally lensed galaxies. However, that does not mean that any distortions are possible. Here I prove an analytic upper bound to differential magnification effects. For example, a thermal or sub-thermal CO ladder cannot be made to appear super-thermal just from gravitational lensing, and the Balmer decrement emission line ratio H$\\alpha$:H$\\beta$ cannot reduce below the case B prediction just from differential magnification. In general, if a physical model of a galaxy predicts upper and/or lower bounds to an emission line ratio, then those bounds also apply to the differentially magnified strongly gravitationally lensed case. This applies not just for velocity-integrated emission lines, but also for the line emission in any rest-frame velocity interval.","sentences":["Differential magnification is now well-known to distort the spectral energy distributions of strongly gravitationally lensed galaxies.","However, that does not mean that any distortions are possible.","Here I prove an analytic upper bound to differential magnification effects.","For example, a thermal or sub-thermal CO ladder cannot be made to appear super-thermal just from gravitational lensing, and the Balmer decrement emission line ratio H$\\alpha$:H$\\beta$ cannot reduce below the case B prediction just from differential magnification.","In general, if a physical model of a galaxy predicts upper and/or lower bounds to an emission line ratio, then those bounds also apply to the differentially magnified strongly gravitationally lensed case.","This applies not just for velocity-integrated emission lines, but also for the line emission in any rest-frame velocity interval."],"url":"http://arxiv.org/abs/2402.14663v1","category":"astro-ph.GA"}
{"created":"2024-02-22 16:07:52","title":"Varieties of quantitative algebras as categories","abstract":"Varieties of quantitative algebras of Mardare, Panangaden and Plotkin are characterized as certain categories enriched over metric spaces. We introduce the concept of a procongruence (a metric-enriched analogue of a congruence) and the corresponding proexact categories. Varieties of quantitative algebras are precisely the metric-enriched proexact categories with a provarietal generator. This means an abstractly finite, strong generator whose hom-functor preserves proregular epimorphisms.","sentences":["Varieties of quantitative algebras of Mardare, Panangaden and Plotkin are characterized as certain categories enriched over metric spaces.","We introduce the concept of a procongruence (a metric-enriched analogue of a congruence) and the corresponding proexact categories.","Varieties of quantitative algebras are precisely the metric-enriched proexact categories with a provarietal generator.","This means an abstractly finite, strong generator whose hom-functor preserves proregular epimorphisms."],"url":"http://arxiv.org/abs/2402.14662v1","category":"math.CT"}
{"created":"2024-02-22 16:06:49","title":"ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models","abstract":"This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models.","sentences":["This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs).","Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies.","Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones.","Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs.","Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models."],"url":"http://arxiv.org/abs/2402.14660v1","category":"cs.CL"}
{"created":"2024-02-22 16:06:23","title":"OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement","abstract":"The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.","sentences":["The introduction of large language models has significantly advanced code generation.","However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter.","To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code.","Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement.","Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance.","Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4.","OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter."],"url":"http://arxiv.org/abs/2402.14658v1","category":"cs.SE"}
{"created":"2024-02-22 15:59:09","title":"Quantum Markov Decision Processes Part I: General Theory, Approximations, and Classes of Policies","abstract":"In this two part article, the aim is to develop a quantum counterpart to classical Markov decision processes (MDPs). In Part I, we provide a very general formulation of quantum MDPs with state and action spaces in the quantum domain, quantum transitions, and cost functions. Once we formulate the quantum MDP (q-MDP), our focus shifts to establishing the verification theorem that proves the sufficiency of Markovian quantum control policies and provides a dynamic programming principle. Subsequently, a comparison is drawn between our q-MDP model and previously established quantum MDP models (referred to as QOMDPs) found in the literature. Furthermore, approximations of q-MDPs are obtained via finite-action models, which can be formulated as QOMDPs. Finally, classes of open-loop and closed-loop policies for q-MDPs are introduced, along with structural results for these policies. In summary, we present a novel quantum MDP model aiming to introduce a new framework, algorithms, and future research avenues. We believe that our approach will pave the way for a new research direction in discrete-time quantum control.","sentences":["In this two part article, the aim is to develop a quantum counterpart to classical Markov decision processes (MDPs).","In Part I, we provide a very general formulation of quantum MDPs with state and action spaces in the quantum domain, quantum transitions, and cost functions.","Once we formulate the quantum MDP (q-MDP), our focus shifts to establishing the verification theorem that proves the sufficiency of Markovian quantum control policies and provides a dynamic programming principle.","Subsequently, a comparison is drawn between our q-MDP model and previously established quantum MDP models (referred to as QOMDPs) found in the literature.","Furthermore, approximations of q-MDPs are obtained via finite-action models, which can be formulated as QOMDPs.","Finally, classes of open-loop and closed-loop policies for q-MDPs are introduced, along with structural results for these policies.","In summary, we present a novel quantum MDP model aiming to introduce a new framework, algorithms, and future research avenues.","We believe that our approach will pave the way for a new research direction in discrete-time quantum control."],"url":"http://arxiv.org/abs/2402.14649v1","category":"quant-ph"}
{"created":"2024-02-22 15:53:46","title":"Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off","abstract":"Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a \"gradient conflict\" between invariance loss and classification objectives, indicating the existence of \"collapsing solutions,\" and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid \"collapsing solutions,\" inspired by a recent non-contrastive self-supervised learning approach, and a split-BatchNorm (BN) structure to resolve the mixture distribution problem. Our method significantly improves the robustness-accuracy trade-off by learning adversarially invariant representations without sacrificing discriminative power. Furthermore, we discuss the relevance of our findings to knowledge-distillation-based defense methods, contributing to a deeper understanding of their relative successes.","sentences":["Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off.","In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off.","We empirically identify two key issues hindering invariance regularization: (1) a \"gradient conflict\" between invariance loss and classification objectives, indicating the existence of \"collapsing solutions,\" and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs.","To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid \"collapsing solutions,\" inspired by a recent non-contrastive self-supervised learning approach, and a split-BatchNorm (BN) structure to resolve the mixture distribution problem.","Our method significantly improves the robustness-accuracy trade-off by learning adversarially invariant representations without sacrificing discriminative power.","Furthermore, we discuss the relevance of our findings to knowledge-distillation-based defense methods, contributing to a deeper understanding of their relative successes."],"url":"http://arxiv.org/abs/2402.14648v1","category":"cs.LG"}
{"created":"2024-02-22 15:45:27","title":"Sparse Linear Regression and Lattice Problems","abstract":"Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\\in\\mathbb{R}^{m\\times n}$ and a response vector $y=X\\theta^*+w$ for a $k$-sparse vector $\\theta^*$ (that is, $\\|\\theta^*\\|_0\\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\\widehat{\\theta} \\in \\mathbb{R}^n$ that minimizes the mean squared prediction error $\\frac{1}{m}\\|X\\widehat{\\theta}-X\\theta^*\\|^2_2$. While $\\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bounded distance decoding (BDD) problem on lattices to SLR, where the condition number of the lattice basis that defines the BDD instance is directly related to the restricted eigenvalue condition of the design matrix, which characterizes some of the classical statistical-computational gaps for sparse linear regression. Also, by appealing to worst-case to average-case reductions from the world of lattices, this shows hardness for a distribution of SLR instances; while the design matrices are ill-conditioned, the resulting SLR instances are in the identifiable regime.   Furthermore, for well-conditioned (essentially) isotropic Gaussian design matrices, where Lasso is known to behave well in the identifiable regime, we show hardness of outputting any good solution in the unidentifiable regime where there are many solutions, assuming the worst-case hardness of standard and well-studied lattice problems.","sentences":["Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\\in\\mathbb{R}^{m\\times n}$ and a response vector $y=X\\theta^*+w$ for a $k$-sparse vector $\\theta^*$ (that is, $\\|\\theta^*\\|_0\\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\\widehat{\\theta} \\in \\mathbb{R}^n$ that minimizes the mean squared prediction error $\\frac{1}{m}\\|X\\widehat{\\theta}-X\\theta^*\\|^2_2$. While $\\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   ","We give evidence of average-case hardness of SLR w.r.t.","all efficient algorithms assuming the worst-case hardness of lattice problems.","Specifically, we give an instance-by-instance reduction from a variant of the bounded distance decoding (BDD) problem on lattices to SLR, where the condition number of the lattice basis that defines the BDD instance is directly related to the restricted eigenvalue condition of the design matrix, which characterizes some of the classical statistical-computational gaps for sparse linear regression.","Also, by appealing to worst-case to average-case reductions from the world of lattices, this shows hardness for a distribution of SLR instances; while the design matrices are ill-conditioned, the resulting SLR instances are in the identifiable regime.   ","Furthermore, for well-conditioned (essentially) isotropic Gaussian design matrices, where Lasso is known to behave well in the identifiable regime, we show hardness of outputting any good solution in the unidentifiable regime where there are many solutions, assuming the worst-case hardness of standard and well-studied lattice problems."],"url":"http://arxiv.org/abs/2402.14645v1","category":"cs.LG"}
{"created":"2024-02-22 15:40:15","title":"The Spatial Whitham Equation","abstract":"The Whitham equation is a nonlocal, nonlinear partial differential equation that models the temporal evolution of spatial profiles of surface displacement of water waves. However, many laboratory and field measurements record time series at fixed spatial locations. In order to directly model data of this type, it is desirable to have equations that model the spatial evolution of time series. The spatial Whitham equation, proposed as the spatial generalization of the Whitham equation, fills this need. In this paper, we study this equation and apply it to water-wave experiments on shallow and deep water.   We compute periodic traveling-wave solutions to the spatial Whitham equation and examine their properties, including their stability. Results for small-amplitude solutions align with known results for the Whitham equation. This suggests that the systems are consistent in the weakly nonlinear regime. At larger amplitudes, there are some discrepancies. Notably, the spatial Whitham equation does not appear to admit cusped solutions of maximal wave height. In the second part, we compare predictions from the temporal and spatial Korteweg-deVries and Whitham equations with measurements from laboratory experiments. We show that the spatial Whitham equation accurately models measurements of tsunami-like waves of depression and solitary waves on shallow water. Its predictions also compare favorably with experimental measurements of waves of depression and elevation on deep water. Accuracy is increased by adding a phenomenological damping term. Finally, we show that neither the spatial nor the temporal Whitham equation accurately models the evolution of wave packets on deep water.","sentences":["The Whitham equation is a nonlocal, nonlinear partial differential equation that models the temporal evolution of spatial profiles of surface displacement of water waves.","However, many laboratory and field measurements record time series at fixed spatial locations.","In order to directly model data of this type, it is desirable to have equations that model the spatial evolution of time series.","The spatial Whitham equation, proposed as the spatial generalization of the Whitham equation, fills this need.","In this paper, we study this equation and apply it to water-wave experiments on shallow and deep water.   ","We compute periodic traveling-wave solutions to the spatial Whitham equation and examine their properties, including their stability.","Results for small-amplitude solutions align with known results for the Whitham equation.","This suggests that the systems are consistent in the weakly nonlinear regime.","At larger amplitudes, there are some discrepancies.","Notably, the spatial Whitham equation does not appear to admit cusped solutions of maximal wave height.","In the second part, we compare predictions from the temporal and spatial Korteweg-deVries and Whitham equations with measurements from laboratory experiments.","We show that the spatial Whitham equation accurately models measurements of tsunami-like waves of depression and solitary waves on shallow water.","Its predictions also compare favorably with experimental measurements of waves of depression and elevation on deep water.","Accuracy is increased by adding a phenomenological damping term.","Finally, we show that neither the spatial nor the temporal Whitham equation accurately models the evolution of wave packets on deep water."],"url":"http://arxiv.org/abs/2402.14643v1","category":"physics.flu-dyn"}
{"created":"2024-02-22 15:26:00","title":"Random Polynomials Associated with Non-orthonormal Bases","abstract":"This article addresses an equidistribution problem concerning the zeros of random polynomials on $\\mathbb{C}^{m},$ which are represented using a general basis that may not necessarily be orthonormal. We prove the equidistribution results which are more general than the previously acquired ones for non-discrete probability measures. More precisely, our result demonstrates that the equidistribution holds true even when the random coefficients in the basis representation are not independent and identically distributed (i.i.d.), and moreover, they are not constrained to any particular probability distribution function. Main tools are borrowed from the capacity theory.   In the context of random polynomial mappings from $\\mathbb{C}^{m}$ to $\\mathbb{C}^k$, we prove an equidistribution theorem for various codimensions $1 \\leq k \\leq m$ by applying the results from the capacity theory along with the codimension $1$ analysis. Based on our current information, this result is general enough to encompass many previous ones in the literature regarding the asymptotic distribution of zeros of random polynomial mappings. We touch upon some well-known probability in this direction.   Finally, by extending the concept of a sequence of asymptotically Bernstein-Markov measures to the setting of holomorphic line bundles over compact K\\\"{a}hler manifolds, we establish a global equidistribution theorem related to the zeros of systems of random holomorphic sections for large tensor powers of a fixed line bundle for any codimension $k$.","sentences":["This article addresses an equidistribution problem concerning the zeros of random polynomials on $\\mathbb{C}^{m},$ which are represented using a general basis that may not necessarily be orthonormal.","We prove the equidistribution results which are more general than the previously acquired ones for non-discrete probability measures.","More precisely, our result demonstrates that the equidistribution holds true even when the random coefficients in the basis representation are not independent and identically distributed (i.i.d.), and moreover, they are not constrained to any particular probability distribution function.","Main tools are borrowed from the capacity theory.   ","In the context of random polynomial mappings from $\\mathbb{C}^{m}$ to $\\mathbb{C}^k$, we prove an equidistribution theorem for various codimensions $1 \\leq k \\leq m$ by applying the results from the capacity theory along with the codimension $1$ analysis.","Based on our current information, this result is general enough to encompass many previous ones in the literature regarding the asymptotic distribution of zeros of random polynomial mappings.","We touch upon some well-known probability in this direction.   ","Finally, by extending the concept of a sequence of asymptotically Bernstein-Markov measures to the setting of holomorphic line bundles over compact K\\\"{a}hler manifolds, we establish a global equidistribution theorem related to the zeros of systems of random holomorphic sections for large tensor powers of a fixed line bundle for any codimension $k$."],"url":"http://arxiv.org/abs/2402.14631v1","category":"math.CV"}
{"created":"2024-02-22 15:12:50","title":"A bivariate spatial extreme mixture model for unreplicated heavy metal soil contamination","abstract":"Geostatistical models for multivariate applications such as heavy metal soil contamination work under Gaussian assumptions and may result in underestimated extreme values and misleading risk assessments (Marchant et al, 2011). A more suitable framework to analyse extreme values is extreme value theory (EVT). However, EVT relies on replications in time, which are generally not available in geochemical datasets. Therefore, using EVT to map soil contamination requires adaptation to be used in the usual single-replicate data framework of soil surveys. We propose a bivariate spatial extreme mixture model to model the body and tail of contaminant pairs, where the tails are described using a stationary generalised Pareto distribution. We demonstrate the performance of our model using a simulation study and through modelling bivariate soil contamination in the Glasgow conurbation. Model results are given as maps of predicted marginal concentrations and probabilities of joint exceedance of soil guideline values. Marginal concentration maps show areas of elevated lead levels along the Clyde River and elevated levels of chromium around the south and southeast villages such as East Kilbride and Wishaw. The joint probability maps show higher probabilities of joint exceedance to the south and southeast of the city centre, following known legacy contamination regions in the Clyde River basin.","sentences":["Geostatistical models for multivariate applications such as heavy metal soil contamination work under Gaussian assumptions and may result in underestimated extreme values and misleading risk assessments (Marchant et al, 2011).","A more suitable framework to analyse extreme values is extreme value theory (EVT).","However, EVT relies on replications in time, which are generally not available in geochemical datasets.","Therefore, using EVT to map soil contamination requires adaptation to be used in the usual single-replicate data framework of soil surveys.","We propose a bivariate spatial extreme mixture model to model the body and tail of contaminant pairs, where the tails are described using a stationary generalised Pareto distribution.","We demonstrate the performance of our model using a simulation study and through modelling bivariate soil contamination in the Glasgow conurbation.","Model results are given as maps of predicted marginal concentrations and probabilities of joint exceedance of soil guideline values.","Marginal concentration maps show areas of elevated lead levels along the Clyde River and elevated levels of chromium around the south and southeast villages such as East Kilbride and Wishaw.","The joint probability maps show higher probabilities of joint exceedance to the south and southeast of the city centre, following known legacy contamination regions in the Clyde River basin."],"url":"http://arxiv.org/abs/2402.14624v1","category":"stat.AP"}
{"created":"2024-02-22 15:12:00","title":"RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation","abstract":"Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \\textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and simulation validation with Gazebo. We demonstrate the adaptability of our code generation framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers. Additionally, our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions. Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: code generation, perception, motion planning, and even object geometric properties, impact the overall performance of the system.","sentences":["Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI.","However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control.","To bridge this ``ideal-to-real'' gap, this paper presents \\textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language.","The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and simulation validation with Gazebo.","We demonstrate the adaptability of our code generation framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers.","Additionally, our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions.","Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: code generation, perception, motion planning, and even object geometric properties, impact the overall performance of the system."],"url":"http://arxiv.org/abs/2402.14623v1","category":"cs.RO"}
{"created":"2024-02-22 15:10:45","title":"From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access","abstract":"This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.","sentences":["This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications.","The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach.","This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords.","The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates."],"url":"http://arxiv.org/abs/2402.14622v1","category":"cs.IR"}
{"created":"2024-02-22 15:03:38","title":"An Entropy-Stable Discontinuous Galerkin Discretization of the Ideal Multi-Ion Magnetohydrodynamics System","abstract":"In this paper, we present an entropy-stable (ES) discretization using a nodal discontinuous Galerkin (DG) method for the ideal multi-ion magneto-hydrodynamics (MHD) equations.   We start by performing a continuous entropy analysis of the ideal multi-ion MHD system, described by, e.g., [Toth (2010) Multi-Ion Magnetohydrodynamics] \\cite{Toth2010}, which describes the motion of multi-ion plasmas with independent momentum and energy equations for each ion species. Following the continuous entropy analysis, we propose an algebraic manipulation to the multi-ion MHD system, such that entropy consistency can be transferred from the continuous analysis to its discrete approximation. Moreover, we augment the system of equations with a generalized Lagrange multiplier (GLM) technique to have an additional cleaning mechanism of the magnetic field divergence error.   We first derive robust entropy-conservative (EC) fluxes for the alternative formulation of the multi-ion GLM-MHD system that satisfy a Tadmor-type condition and are consistent with existing EC fluxes for single-fluid GLM-MHD equations. Using these numerical two-point fluxes, we construct high-order EC and ES DG discretizations of the ideal multi-ion MHD system using collocated Legendre--Gauss--Lobatto summation-by-parts (SBP) operators. The resulting nodal DG schemes satisfy the second-law of thermodynamics at the semi-discrete level, while maintaining high-order convergence and local node-wise conservation properties.   We demonstrate the high-order convergence, and the EC and ES properties of our scheme with numerical validation experiments. Moreover, we demonstrate the importance of the GLM divergence technique and the ES discretization to improve the robustness properties of a DG discretization of the multi-ion MHD system by solving a challenging magnetized Kelvin-Helmholtz instability problem that exhibits MHD turbulence.","sentences":["In this paper, we present an entropy-stable (ES) discretization using a nodal discontinuous Galerkin (DG) method for the ideal multi-ion magneto-hydrodynamics (MHD) equations.   ","We start by performing a continuous entropy analysis of the ideal multi-ion MHD system, described by, e.g., [Toth (2010) Multi-Ion Magnetohydrodynamics] \\cite{Toth2010}, which describes the motion of multi-ion plasmas with independent momentum and energy equations for each ion species.","Following the continuous entropy analysis, we propose an algebraic manipulation to the multi-ion MHD system, such that entropy consistency can be transferred from the continuous analysis to its discrete approximation.","Moreover, we augment the system of equations with a generalized Lagrange multiplier (GLM) technique to have an additional cleaning mechanism of the magnetic field divergence error.   ","We first derive robust entropy-conservative (EC) fluxes for the alternative formulation of the multi-ion GLM-MHD system that satisfy a Tadmor-type condition and are consistent with existing EC fluxes for single-fluid GLM-MHD equations.","Using these numerical two-point fluxes, we construct high-order EC and ES DG discretizations of the ideal multi-ion MHD system using collocated Legendre--Gauss--Lobatto summation-by-parts (SBP) operators.","The resulting nodal DG schemes satisfy the second-law of thermodynamics at the semi-discrete level, while maintaining high-order convergence and local node-wise conservation properties.   ","We demonstrate the high-order convergence, and the EC and ES properties of our scheme with numerical validation experiments.","Moreover, we demonstrate the importance of the GLM divergence technique and the ES discretization to improve the robustness properties of a DG discretization of the multi-ion MHD system by solving a challenging magnetized Kelvin-Helmholtz instability problem that exhibits MHD turbulence."],"url":"http://arxiv.org/abs/2402.14615v1","category":"math.NA"}
{"created":"2024-02-22 15:02:46","title":"Factorization and irreducibility of composed products","abstract":"Brawley and Carlitz introduced diamond products of elements of finite fields and associated composed products of polynomials in 1987. Composed products yield a method to construct irreducible polynomials of large composite degrees from irreducible polynomials of lower degrees. We show that the composed product of two irreducible polynomials of degrees $m$ and $n$ is again irreducible if and only if $m$ and $n$ are coprime and the involved diamond product satisfies a special cancellation property, the so-called conjugate cancellation. This completes the characterization of irreducible composed products, considered in several previous papers. More generally, we give precise criteria when a diamond product satisfies conjugate cancellation. For diamond products defined via bivariate polynomials, we prove simple criteria that characterize when conjugate cancellation holds. We also provide efficient algorithms to check these criteria. We achieve stronger results as well as more efficient algorithms in the case that the polynomials are bilinear. Lastly, we consider possible constructions of normal elements using composed products and the methods we developed.","sentences":["Brawley and Carlitz introduced diamond products of elements of finite fields and associated composed products of polynomials in 1987.","Composed products yield a method to construct irreducible polynomials of large composite degrees from irreducible polynomials of lower degrees.","We show that the composed product of two irreducible polynomials of degrees $m$ and $n$ is again irreducible if and only if $m$ and $n$ are coprime and the involved diamond product satisfies a special cancellation property, the so-called conjugate cancellation.","This completes the characterization of irreducible composed products, considered in several previous papers.","More generally, we give precise criteria when a diamond product satisfies conjugate cancellation.","For diamond products defined via bivariate polynomials, we prove simple criteria that characterize when conjugate cancellation holds.","We also provide efficient algorithms to check these criteria.","We achieve stronger results as well as more efficient algorithms in the case that the polynomials are bilinear.","Lastly, we consider possible constructions of normal elements using composed products and the methods we developed."],"url":"http://arxiv.org/abs/2402.14613v1","category":"math.NT"}
{"created":"2024-02-22 14:57:44","title":"Federated Complex Qeury Answering","abstract":"Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge cross graphs. Fortunately, federated learning is utilized in knowledge graphs to collaboratively learn representations with privacy preserved. Federated knowledge graph embeddings enrich the relations in knowledge graphs to improve the representation quality. However, these methods only focus on one-hop relations and cannot perform complex reasoning tasks. In this paper, we apply federated learning to complex query-answering tasks to reason over multi-source knowledge graphs while preserving privacy. We propose a Federated Complex Query Answering framework (FedCQA), to reason over multi-source KGs avoiding sensitive raw data transmission to protect privacy. We conduct extensive experiments on three real-world datasets and evaluate retrieval performance on various types of complex queries.","sentences":["Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied.","The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines.","Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs.","However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs.","In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers.","Thus, it remains unknown how to answer queries on multi-source KGs.","An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge cross graphs.","Fortunately, federated learning is utilized in knowledge graphs to collaboratively learn representations with privacy preserved.","Federated knowledge graph embeddings enrich the relations in knowledge graphs to improve the representation quality.","However, these methods only focus on one-hop relations and cannot perform complex reasoning tasks.","In this paper, we apply federated learning to complex query-answering tasks to reason over multi-source knowledge graphs while preserving privacy.","We propose a Federated Complex Query Answering framework (FedCQA), to reason over multi-source KGs avoiding sensitive raw data transmission to protect privacy.","We conduct extensive experiments on three real-world datasets and evaluate retrieval performance on various types of complex queries."],"url":"http://arxiv.org/abs/2402.14609v1","category":"cs.LG"}
{"created":"2024-02-22 14:54:28","title":"Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations","abstract":"Imitation learning with human data has demonstrated remarkable success in teaching robots in a wide range of skills. However, the inherent diversity in human behavior leads to the emergence of multi-modal data distributions, thereby presenting a formidable challenge for existing imitation learning algorithms. Quantifying a model's capacity to capture and replicate this diversity effectively is still an open problem. In this work, we introduce simulation benchmark environments and the corresponding Datasets with Diverse human Demonstrations for Imitation Learning (D3IL), designed explicitly to evaluate a model's ability to learn multi-modal behavior. Our environments are designed to involve multiple sub-tasks that need to be solved, consider manipulation of multiple objects which increases the diversity of the behavior and can only be solved by policies that rely on closed loop sensory feedback. Other available datasets are missing at least one of these challenging properties. To address the challenge of diversity quantification, we introduce tractable metrics that provide valuable insights into a model's ability to acquire and reproduce diverse behaviors. These metrics offer a practical means to assess the robustness and versatility of imitation learning algorithms. Furthermore, we conduct a thorough evaluation of state-of-the-art methods on the proposed task suite. This evaluation serves as a benchmark for assessing their capability to learn diverse behaviors. Our findings shed light on the effectiveness of these methods in tackling the intricate problem of capturing and generalizing multi-modal human behaviors, offering a valuable reference for the design of future imitation learning algorithms.","sentences":["Imitation learning with human data has demonstrated remarkable success in teaching robots in a wide range of skills.","However, the inherent diversity in human behavior leads to the emergence of multi-modal data distributions, thereby presenting a formidable challenge for existing imitation learning algorithms.","Quantifying a model's capacity to capture and replicate this diversity effectively is still an open problem.","In this work, we introduce simulation benchmark environments and the corresponding Datasets with Diverse human Demonstrations for Imitation Learning (D3IL), designed explicitly to evaluate a model's ability to learn multi-modal behavior.","Our environments are designed to involve multiple sub-tasks that need to be solved, consider manipulation of multiple objects which increases the diversity of the behavior and can only be solved by policies that rely on closed loop sensory feedback.","Other available datasets are missing at least one of these challenging properties.","To address the challenge of diversity quantification, we introduce tractable metrics that provide valuable insights into a model's ability to acquire and reproduce diverse behaviors.","These metrics offer a practical means to assess the robustness and versatility of imitation learning algorithms.","Furthermore, we conduct a thorough evaluation of state-of-the-art methods on the proposed task suite.","This evaluation serves as a benchmark for assessing their capability to learn diverse behaviors.","Our findings shed light on the effectiveness of these methods in tackling the intricate problem of capturing and generalizing multi-modal human behaviors, offering a valuable reference for the design of future imitation learning algorithms."],"url":"http://arxiv.org/abs/2402.14606v1","category":"cs.RO"}
{"created":"2024-02-22 14:51:17","title":"Don't mention it: An approach to assess challenges to using software mentions for citation and discoverability research","abstract":"Datasets collecting software mentions from scholarly publications can potentially be used for research into the software that has been used in the published research, as well as into the practice of software citation. Recently, new software mention datasets with different characteristics have been published. We present an approach to assess the usability of such datasets for research on research software. Our approach includes sampling and data preparation, manual annotation for quality and mention characteristics, and annotation analysis. We applied it to two software mention datasets for evaluation based on qualitative observation. Doing this, we were able to find challenges to working with the selected datasets to do research. Main issues refer to the structure of the dataset, the quality of the extracted mentions (54% and 23% of mentions respectively are not to software), and software accessibility. While one dataset does not provide links to mentioned software at all, the other does so in a way that can impede quantitative research endeavors: (1) Links may come from different sources and each point to different software for the same mention. (2) The quality of the automatically retrieved links is generally poor (in our sample, 65.4% link the wrong software). (3) Links exist only for a small subset (in our sample, 20.5%) of mentions, which may lead to skewed or disproportionate samples. However, the greatest challenge and underlying issue in working with software mention datasets is the still suboptimal practice of software citation: Software should not be mentioned, it should be cited following the software citation principles.","sentences":["Datasets collecting software mentions from scholarly publications can potentially be used for research into the software that has been used in the published research, as well as into the practice of software citation.","Recently, new software mention datasets with different characteristics have been published.","We present an approach to assess the usability of such datasets for research on research software.","Our approach includes sampling and data preparation, manual annotation for quality and mention characteristics, and annotation analysis.","We applied it to two software mention datasets for evaluation based on qualitative observation.","Doing this, we were able to find challenges to working with the selected datasets to do research.","Main issues refer to the structure of the dataset, the quality of the extracted mentions (54% and 23% of mentions respectively are not to software), and software accessibility.","While one dataset does not provide links to mentioned software at all, the other does so in a way that can impede quantitative research endeavors: (1) Links may come from different sources and each point to different software for the same mention.","(2) The quality of the automatically retrieved links is generally poor (in our sample, 65.4% link the wrong software).","(3) Links exist only for a small subset (in our sample, 20.5%) of mentions, which may lead to skewed or disproportionate samples.","However, the greatest challenge and underlying issue in working with software mention datasets is the still suboptimal practice of software citation: Software should not be mentioned, it should be cited following the software citation principles."],"url":"http://arxiv.org/abs/2402.14602v1","category":"cs.SE"}
{"created":"2024-02-22 14:44:50","title":"High-Speed Detector For Low-Powered Devices In Aerial Grasping","abstract":"Autonomous aerial harvesting is a highly complex problem because it requires numerous interdisciplinary algorithms to be executed on mini low-powered computing devices. Object detection is one such algorithm that is compute-hungry. In this context, we make the following contributions: (i) Fast Fruit Detector (FFD), a resource-efficient, single-stage, and postprocessing-free object detector based on our novel latent object representation (LOR) module, query assignment, and prediction strategy. FFD achieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded device while co-existing with other time-critical sub-systems such as control, grasping, SLAM, a major achievement of this work. (ii) a method to generate vast amounts of training data without exhaustive manual labelling of fruit images since they consist of a large number of instances, which increases the labelling cost and time. (iii) an open-source fruit detection dataset having plenty of very small-sized instances that are difficult to detect. Our exhaustive evaluations on our and MinneApple dataset show that FFD, being only a single-scale detector, is more accurate than many representative detectors, e.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale Faster-RCNN by 2.3AP, and better than latest single-scale YOLO-v8 by 8AP and multi-scale YOLO-v8 by 0.3 while being considerably faster.","sentences":["Autonomous aerial harvesting is a highly complex problem because it requires numerous interdisciplinary algorithms to be executed on mini low-powered computing devices.","Object detection is one such algorithm that is compute-hungry.","In this context, we make the following contributions: (i) Fast Fruit Detector (FFD), a resource-efficient, single-stage, and postprocessing-free object detector based on our novel latent object representation (LOR) module, query assignment, and prediction strategy.","FFD achieves 100FPS@FP32 precision on the latest 10W NVIDIA Jetson-NX embedded device while co-existing with other time-critical sub-systems such as control, grasping, SLAM, a major achievement of this work.","(ii) a method to generate vast amounts of training data without exhaustive manual labelling of fruit images since they consist of a large number of instances, which increases the labelling cost and time.","(iii) an open-source fruit detection dataset having plenty of very small-sized instances that are difficult to detect.","Our exhaustive evaluations on our and MinneApple dataset show that FFD, being only a single-scale detector, is more accurate than many representative detectors, e.g. FFD is better than single-scale Faster-RCNN by 10.7AP, multi-scale Faster-RCNN by 2.3AP, and better than latest single-scale YOLO-v8 by 8AP and multi-scale YOLO-v8 by 0.3 while being considerably faster."],"url":"http://arxiv.org/abs/2402.14591v1","category":"cs.CV"}
{"created":"2024-02-22 14:42:28","title":"Small electron polarons bound to interstitial tantalum defects in lithium tantalate","abstract":"The absorption features of optically generated, short-lived small bound electron polarons are inspected in congruent lithium tantalate, LiTaO$_3$ (LT), in order to address the question whether it is possible to localize electrons at interstitial Ta$_{\\rm V}$:V$_{\\rm Li}$ defect pairs by strong, short-range electron-phonon coupling. Solid-state photoabsorption spectroscopy under light exposure and density functional theory are used for an experimental and theoretical access to the spectral features of small bound polaron states and to calculate the binding energies of the small bound Ta$_{\\rm Li}^{4+}$ (antisite) and Ta$_{\\rm V}^{4+}$:V$_{\\rm Li}$ (interstitial site) electron polarons. As a result, two energetically well separated ($\\Delta E \\approx 0.5$ eV) absorption features with a distinct dependence on the probe light polarization and peaking at 1.6 eV and 2.1 eV are discovered. We contrast our results to the interpretation of a single small bound Ta$_{\\rm Li}^{4+}$ electron state with strong anisotropy of the lattice distortion and discuss the optical generation of interstitial Ta$_{\\rm V}^{4+}$:V$_{\\rm Li}$ small polarons in the framework of optical gating of Ta$_{\\rm V}^{4+}$:Ta$_{\\rm Ta}^{4+}$ bipolarons. We can conclude that the appearance of carrier localization at Ta$_{\\rm V}$:V$_{\\rm Li}$ must be considered as additional intermediate state for the 3D hopping transport mechanisms at room temperature in addition to Ta$_{\\rm Li}$, as well, and, thus, impacts a variety of optical, photoelectrical and electrical applications of LT in nonlinear photonics. Furthermore, it is envisaged that LT represents a promising model system for the further examination of the small-polaron based photogalvanic effect in polar oxides with the unique feature of two, energetically well separated small polaron states.","sentences":["The absorption features of optically generated, short-lived small bound electron polarons are inspected in congruent lithium tantalate, LiTaO$_3$ (LT), in order to address the question whether it is possible to localize electrons at interstitial Ta$_{\\rm","V}$:V$_{\\rm Li}$ defect pairs by strong, short-range electron-phonon coupling.","Solid-state photoabsorption spectroscopy under light exposure and density functional theory are used for an experimental and theoretical access to the spectral features of small bound polaron states and to calculate the binding energies of the small bound Ta$_{\\rm Li}^{4+}$ (antisite) and Ta$_{\\rm V}^{4+}$:V$_{\\rm Li}$ (interstitial site) electron polarons.","As a result, two energetically well separated ($\\Delta E \\approx 0.5$ eV) absorption features with a distinct dependence on the probe light polarization and peaking at 1.6 eV and 2.1 eV are discovered.","We contrast our results to the interpretation of a single small bound Ta$_{\\rm Li}^{4+}$ electron state with strong anisotropy of the lattice distortion and discuss the optical generation of","interstitial Ta$_{\\rm","V}^{4+}$:V$_{\\rm Li}$ small polarons in the framework of optical gating of Ta$_{\\rm V}^{4+}$:Ta$_{\\rm Ta}^{4+}$ bipolarons.","We can conclude that the appearance of carrier localization at Ta$_{\\rm V}$:V$_{\\rm Li}$ must be considered as additional intermediate state for the 3D hopping transport mechanisms at room temperature in addition to Ta$_{\\rm Li}$, as well, and, thus, impacts a variety of optical, photoelectrical and electrical applications of LT in nonlinear photonics.","Furthermore, it is envisaged that LT represents a promising model system for the further examination of the small-polaron based photogalvanic effect in polar oxides with the unique feature of two, energetically well separated small polaron states."],"url":"http://arxiv.org/abs/2402.14587v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 14:41:02","title":"FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View Synthesis","abstract":"We present a novel framework, called FrameNeRF, designed to apply off-the-shelf fast high-fidelity NeRF models with fast training speed and high rendering quality for few-shot novel view synthesis tasks. The training stability of fast high-fidelity models is typically constrained to dense views, making them unsuitable for few-shot novel view synthesis tasks. To address this limitation, we utilize a regularization model as a data generator to produce dense views from sparse inputs, facilitating subsequent training of fast high-fidelity models. Since these dense views are pseudo ground truth generated by the regularization model, original sparse images are then used to fine-tune the fast high-fidelity model. This process helps the model learn realistic details and correct artifacts introduced in earlier stages. By leveraging an off-the-shelf regularization model and a fast high-fidelity model, our approach achieves state-of-the-art performance across various benchmark datasets.","sentences":["We present a novel framework, called FrameNeRF, designed to apply off-the-shelf fast high-fidelity NeRF models with fast training speed and high rendering quality for few-shot novel view synthesis tasks.","The training stability of fast high-fidelity models is typically constrained to dense views, making them unsuitable for few-shot novel view synthesis tasks.","To address this limitation, we utilize a regularization model as a data generator to produce dense views from sparse inputs, facilitating subsequent training of fast high-fidelity models.","Since these dense views are pseudo ground truth generated by the regularization model, original sparse images are then used to fine-tune the fast high-fidelity model.","This process helps the model learn realistic details and correct artifacts introduced in earlier stages.","By leveraging an off-the-shelf regularization model and a fast high-fidelity model, our approach achieves state-of-the-art performance across various benchmark datasets."],"url":"http://arxiv.org/abs/2402.14586v1","category":"cs.CV"}
{"created":"2024-02-22 14:38:52","title":"Bandits with Abstention under Expert Advice","abstract":"We study the classic problem of prediction with expert advice under bandit feedback. Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. We propose the CBA algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. We can view our problem as the aggregation of confidence-rated predictors when the learner has the option of abstention from play. Importantly, we are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action). As an example application, we discuss learning unions of balls in a finite metric space. In this contextual setting, we devise an efficient implementation of CBA, reducing the runtime from quadratic to almost linear in the number of contexts. Preliminary experiments show that CBA improves over existing bandit algorithms.","sentences":["We study the classic problem of prediction with expert advice under bandit feedback.","Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial.","We propose the CBA algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm.","We can view our problem as the aggregation of confidence-rated predictors when the learner has the option of abstention from play.","Importantly, we are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors.","In the special case of specialists we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action).","As an example application, we discuss learning unions of balls in a finite metric space.","In this contextual setting, we devise an efficient implementation of CBA, reducing the runtime from quadratic to almost linear in the number of contexts.","Preliminary experiments show that CBA improves over existing bandit algorithms."],"url":"http://arxiv.org/abs/2402.14585v1","category":"cs.LG"}
{"created":"2024-02-22 14:38:49","title":"Estimating angular momenta of fission fragments from isomeric yield ratios","abstract":"Purpose: To deduce the angular momenta of fission fragments based on the observed isomeric yield ratios (IYR) in 25-MeV proton-induced fission of 238U and to compare these using Wilson's model. Method: A surrogate model of GEF has been developed to generate properties of primary fission fragments. Based on the excitation energy and angular momentum of fission fragments from GEF, an energy versus angular momentum matrix is reconstructed using a set of parameters. With such matrices as input, TALYS is used to calculate the de-excitation of the fission fragments, from which the IYRs are obtained. By varying one of the parameters, the root-mean-square angular momentum (Jrms), which determines the angular momentum distribution of the matrix, Jrms-dependent IYRs are obtained. Considering all primary fission fragments contributing to the IYR for a given fission product, the average angular momentum of those fragments is estimated. Results: Data of 31 IYRs in proton-induced fission of 238U were analysed. As a result, the average Jrms, equivalent to average angular momentum Jav, with uncertainties of 24 fission products, are presented. Considering the neutron emissions of the primary fission fragments, the Jav as a function of the primary fission fragment is presented. Conclusion: A mass dependency of Jav is observed in the proton-induced fission of 238U. Moreover, the Jav for A larger than 131 could be described by the parameterisation proposed by J. Wilson. In general, higher Jav are observed in the present work compared to those from Wilson et al. This is likely due to the higher excitation energy of the fissioning nuclei in this work compared to Wilson's. Furthermore, systematic measurements of the Jav of fission products in the symmetric mass region are presented for the first time. A decreasing trend with mass numbers is observed, which can not be explained by Wilson's proposal.","sentences":["Purpose: To deduce the angular momenta of fission fragments based on the observed isomeric yield ratios (IYR) in 25-MeV proton-induced fission of 238U and to compare these using Wilson's model.","Method: A surrogate model of GEF has been developed to generate properties of primary fission fragments.","Based on the excitation energy and angular momentum of fission fragments from GEF, an energy versus angular momentum matrix is reconstructed using a set of parameters.","With such matrices as input, TALYS is used to calculate the de-excitation of the fission fragments, from which the IYRs are obtained.","By varying one of the parameters, the root-mean-square angular momentum (Jrms), which determines the angular momentum distribution of the matrix, Jrms-dependent IYRs are obtained.","Considering all primary fission fragments contributing to the IYR for a given fission product, the average angular momentum of those fragments is estimated.","Results: Data of 31 IYRs in proton-induced fission of 238U were analysed.","As a result, the average Jrms, equivalent to average angular momentum Jav, with uncertainties of 24 fission products, are presented.","Considering the neutron emissions of the primary fission fragments, the Jav as a function of the primary fission fragment is presented.","Conclusion: A mass dependency of Jav is observed in the proton-induced fission of 238U. Moreover, the Jav for A larger than 131 could be described by the parameterisation proposed by J. Wilson.","In general, higher Jav are observed in the present work compared to those from Wilson et al.","This is likely due to the higher excitation energy of the fissioning nuclei in this work compared to Wilson's.","Furthermore, systematic measurements of the Jav of fission products in the symmetric mass region are presented for the first time.","A decreasing trend with mass numbers is observed, which can not be explained by Wilson's proposal."],"url":"http://arxiv.org/abs/2402.14584v1","category":"nucl-th"}
{"created":"2024-02-22 14:36:36","title":"Semantic Communication-assisted Physical Layer Security over Fading Wiretap Channels","abstract":"A novel semantic communication (SC)-assisted secrecy transmission framework is proposed. In particular, the legitimate transmitter (Tx) sends the superimposed semantic and bit stream to the legitimate receiver (Rx), where the information may be eavesdropped by the malicious node (EVE). As the EVE merely has the conventional bit-oriented communication structure, the semantic signal acts as the type of beneficial information-bearing artificial noise (AN), which not only keeps strictly confidential to the EVE but also interferes with the EVE. The ergodic (equivalent) secrecy rate over fading wiretap channels is maximized by jointly optimizing the transmit power, semantic-bit power splitting ratio, and the successive interference cancellation decoding order at the Tx, subject to both the instantaneous peak and long-term average power constraints. To address this non-convex problem, both the optimal and suboptimal algorithms are developed by employing the Lagrangian dual method and the successive convex approximation method, respectively. Numerical results show that the proposed SC-assisted secrecy transmission scheme can significantly enhance the physical layer security compared to the baselines using the conventional bit-oriented communication and no-information-bearing AN. It also shows that the proposed suboptimal algorithm can achieve a near-optimal performance.","sentences":["A novel semantic communication (SC)-assisted secrecy transmission framework is proposed.","In particular, the legitimate transmitter (Tx) sends the superimposed semantic and bit stream to the legitimate receiver (Rx), where the information may be eavesdropped by the malicious node (EVE).","As the EVE merely has the conventional bit-oriented communication structure, the semantic signal acts as the type of beneficial information-bearing artificial noise (AN), which not only keeps strictly confidential to the EVE but also interferes with the EVE.","The ergodic (equivalent) secrecy rate over fading wiretap channels is maximized by jointly optimizing the transmit power, semantic-bit power splitting ratio, and the successive interference cancellation decoding order at the Tx, subject to both the instantaneous peak and long-term average power constraints.","To address this non-convex problem, both the optimal and suboptimal algorithms are developed by employing the Lagrangian dual method and the successive convex approximation method, respectively.","Numerical results show that the proposed SC-assisted secrecy transmission scheme can significantly enhance the physical layer security compared to the baselines using the conventional bit-oriented communication and no-information-bearing AN.","It also shows that the proposed suboptimal algorithm can achieve a near-optimal performance."],"url":"http://arxiv.org/abs/2402.14581v1","category":"cs.IT"}
{"created":"2024-02-22 14:33:23","title":"Debiasing Text-to-Image Diffusion Models","abstract":"Learning-based Text-to-Image (TTI) models like Stable Diffusion have revolutionized the way visual content is generated in various domains. However, recent research has shown that nonnegligible social bias exists in current state-of-the-art TTI systems, which raises important concerns. In this work, we target resolving the social bias in TTI diffusion models. We begin by formalizing the problem setting and use the text descriptions of bias groups to establish an unsafe direction for guiding the diffusion process. Next, we simplify the problem into a weight optimization problem and attempt a Reinforcement solver, Policy Gradient, which shows sub-optimal performance with slow convergence. Further, to overcome limitations, we propose an iterative distribution alignment (IDA) method. Despite its simplicity, we show that IDA shows efficiency and fast convergence in resolving the social bias in TTI diffusion models. Our code will be released.","sentences":["Learning-based Text-to-Image (TTI) models like Stable Diffusion have revolutionized the way visual content is generated in various domains.","However, recent research has shown that nonnegligible social bias exists in current state-of-the-art TTI systems, which raises important concerns.","In this work, we target resolving the social bias in TTI diffusion models.","We begin by formalizing the problem setting and use the text descriptions of bias groups to establish an unsafe direction for guiding the diffusion process.","Next, we simplify the problem into a weight optimization problem and attempt a Reinforcement solver, Policy Gradient, which shows sub-optimal performance with slow convergence.","Further, to overcome limitations, we propose an iterative distribution alignment (IDA) method.","Despite its simplicity, we show that IDA shows efficiency and fast convergence in resolving the social bias in TTI diffusion models.","Our code will be released."],"url":"http://arxiv.org/abs/2402.14577v1","category":"cs.CV"}
{"created":"2024-02-22 14:20:07","title":"Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning","abstract":"Robot navigation has transitioned from prioritizing obstacle avoidance to adopting socially aware navigation strategies that accommodate human presence. As a result, the recognition of socially aware navigation within dynamic human-centric environments has gained prominence in the field of robotics. Although reinforcement learning technique has fostered the advancement of socially aware navigation, defining appropriate reward functions, especially in congested environments, has posed a significant challenge. These rewards, crucial in guiding robot actions, demand intricate human-crafted design due to their complex nature and inability to be automatically set. The multitude of manually designed rewards poses issues with hyperparameter redundancy, imbalance, and inadequate representation of unique object characteristics. To address these challenges, we introduce a transformable gaussian reward function (TGRF). The TGRF significantly reduces the burden of hyperparameter tuning, displays adaptability across various reward functions, and demonstrates accelerated learning rates, particularly excelling in crowded environments utilizing deep reinforcement learning (DRL). We introduce and validate TGRF through sections highlighting its conceptual background, characteristics, experiments, and real-world application, paving the way for a more effective and adaptable approach in robotics.The complete source code is available on https://github.com/JinnnK/TGRF","sentences":["Robot navigation has transitioned from prioritizing obstacle avoidance to adopting socially aware navigation strategies that accommodate human presence.","As a result, the recognition of socially aware navigation within dynamic human-centric environments has gained prominence in the field of robotics.","Although reinforcement learning technique has fostered the advancement of socially aware navigation, defining appropriate reward functions, especially in congested environments, has posed a significant challenge.","These rewards, crucial in guiding robot actions, demand intricate human-crafted design due to their complex nature and inability to be automatically set.","The multitude of manually designed rewards poses issues with hyperparameter redundancy, imbalance, and inadequate representation of unique object characteristics.","To address these challenges, we introduce a transformable gaussian reward function (TGRF).","The TGRF significantly reduces the burden of hyperparameter tuning, displays adaptability across various reward functions, and demonstrates accelerated learning rates, particularly excelling in crowded environments utilizing deep reinforcement learning (DRL).","We introduce and validate TGRF through sections highlighting its conceptual background, characteristics, experiments, and real-world application, paving the way for a more effective and adaptable approach in robotics.","The complete source code is available on https://github.com/JinnnK/TGRF"],"url":"http://arxiv.org/abs/2402.14569v1","category":"cs.RO"}
{"created":"2024-02-22 14:19:56","title":"LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition","abstract":"Despite the impressive capabilities of large language models (LLMs), their performance on information extraction tasks is still not entirely satisfactory. However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. In this paper, we propose $LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot NER task. To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels. Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. Extensive experiments demonstrate the effectiveness of our approach in enhancing NER model performance with limited data. Furthermore, additional analyses provide further evidence supporting the assertion that the quality of the data we generate surpasses that of other existing methods.","sentences":["Despite the impressive capabilities of large language models (LLMs), their performance on information extraction tasks is still not entirely satisfactory.","However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks.","In this paper, we propose $LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot NER task.","To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels.","Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness.","Extensive experiments demonstrate the effectiveness of our approach in enhancing NER model performance with limited data.","Furthermore, additional analyses provide further evidence supporting the assertion that the quality of the data we generate surpasses that of other existing methods."],"url":"http://arxiv.org/abs/2402.14568v1","category":"cs.CL"}
{"created":"2024-02-22 14:03:18","title":"The Fundamental Theorem of Calculus in higher dimensions","abstract":"We generalise the Fundamental Theorem of Calculus to higher dimensions. Our generalisation is based on the observation that the antiderivative of a function of $n$-variables is a solution of a partial differential equation of order $n$ generalising the classical case. The generalised Fundamental Theorem of Calculus then states that the $n$-dimensional integrals over $n$-dimensional axis-parallel rectangular hypercuboids is given by a combinatorial formula evaluating the antiderivative on the vertices of the hypercuboid.","sentences":["We generalise the Fundamental Theorem of Calculus to higher dimensions.","Our generalisation is based on the observation that the antiderivative of a function of $n$-variables is a solution of a partial differential equation of order $n$ generalising the classical case.","The generalised Fundamental Theorem of Calculus then states that the $n$-dimensional integrals over $n$-dimensional axis-parallel rectangular hypercuboids is given by a combinatorial formula evaluating the antiderivative on the vertices of the hypercuboid."],"url":"http://arxiv.org/abs/2402.14564v1","category":"math.GM"}
{"created":"2024-02-22 13:59:47","title":"Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools","abstract":"Wizard of OZ (WOZ) is a well-established method for simulating the functionality and user experience of future systems. Using a human wizard to mimic certain operations of a potential system is particularly useful in situations where extensive engineering effort would otherwise be needed to explore the design possibilities offered by such operations. The WOZ method has been widely used in connection with speech and language technologies, but advances in sensor technology and pattern recognition as well as new application areas such as human-robot interaction have made it increasingly relevant to the design of a wider range of interactive systems. In such cases achieving acceptable performance at the user interface level often hinges on resource intensive improvements such as domain tuning, which are better done once the overall design is relatively stable. While WOZ is recognised as a valuable prototyping technique, surprisingly little effort has been put into exploring it from a methodological point of view. Starting from a survey of the literature, this paper presents a systematic investigation and analysis of the design space for WOZ for language technology applications, and proposes a generic architecture for tool support that supports the integration of components for speech recognition and synthesis as well as for machine translation. This architecture is instantiated in WebWOZ - a new web-based open-source WOZ prototyping platform. The viability of generic support is explored empirically through a series of evaluations. Researchers from a variety of backgrounds were able to create experiments, independent of their previous experience with WOZ. The approach was further validated through a number of real experiments, which also helped to identify a number of possibilities for additional support, and flagged potential issues relating to consistency in Wizard performance.","sentences":["Wizard of OZ (WOZ) is a well-established method for simulating the functionality and user experience of future systems.","Using a human wizard to mimic certain operations of a potential system is particularly useful in situations where extensive engineering effort would otherwise be needed to explore the design possibilities offered by such operations.","The WOZ method has been widely used in connection with speech and language technologies, but advances in sensor technology and pattern recognition as well as new application areas such as human-robot interaction have made it increasingly relevant to the design of a wider range of interactive systems.","In such cases achieving acceptable performance at the user interface level often hinges on resource intensive improvements such as domain tuning, which are better done once the overall design is relatively stable.","While WOZ is recognised as a valuable prototyping technique, surprisingly little effort has been put into exploring it from a methodological point of view.","Starting from a survey of the literature, this paper presents a systematic investigation and analysis of the design space for WOZ for language technology applications, and proposes a generic architecture for tool support that supports the integration of components for speech recognition and synthesis as well as for machine translation.","This architecture is instantiated in WebWOZ - a new web-based open-source WOZ prototyping platform.","The viability of generic support is explored empirically through a series of evaluations.","Researchers from a variety of backgrounds were able to create experiments, independent of their previous experience with WOZ.","The approach was further validated through a number of real experiments, which also helped to identify a number of possibilities for additional support, and flagged potential issues relating to consistency in Wizard performance."],"url":"http://arxiv.org/abs/2402.14563v1","category":"cs.HC"}
{"created":"2024-02-22 13:59:25","title":"Recoverability of Causal Effects in a Longitudinal Study under Presence of Missing Data","abstract":"Missing data in multiple variables is a common issue. We investigate the applicability of the framework of graphical models for handling missing data to a complex longitudinal pharmacological study of HIV-positive children treated with an efavirenz-based regimen as part of the CHAPAS-3 trial. Specifically, we examine whether the causal effects of interest, defined through static interventions on multiple continuous variables, can be recovered (estimated consistently) from the available data only. So far, no general algorithms are available to decide on recoverability, and decisions have to be made on a case-by-case basis. We emphasize sensitivity of recoverability to even the smallest changes in the graph structure, and present recoverability results for three plausible missingness directed acyclic graphs (m-DAGs) in the CHAPAS-3 study, informed by clinical knowledge. Furthermore, we propose the concept of ''closed missingness mechanisms'' and show that under these mechanisms an available case analysis is admissible for consistent estimation for any type of statistical and causal query, even if the underlying missingness mechanism is of missing not at random (MNAR) type. Both simulations and theoretical considerations demonstrate how, in the assumed MNAR setting of our study, a complete or available case analysis can be superior to multiple imputation, and estimation results vary depending on the assumed missingness DAG. Our analyses are possibly the first to show the applicability of missingness DAGs (m-DAGs) to complex longitudinal real-world data, while highlighting the sensitivity with respect to the assumed causal model.","sentences":["Missing data in multiple variables is a common issue.","We investigate the applicability of the framework of graphical models for handling missing data to a complex longitudinal pharmacological study of HIV-positive children treated with an efavirenz-based regimen as part of the CHAPAS-3 trial.","Specifically, we examine whether the causal effects of interest, defined through static interventions on multiple continuous variables, can be recovered (estimated consistently) from the available data only.","So far, no general algorithms are available to decide on recoverability, and decisions have to be made on a case-by-case basis.","We emphasize sensitivity of recoverability to even the smallest changes in the graph structure, and present recoverability results for three plausible missingness directed acyclic graphs (m-DAGs) in the CHAPAS-3 study, informed by clinical knowledge.","Furthermore, we propose the concept of ''closed missingness mechanisms'' and show that under these mechanisms an available case analysis is admissible for consistent estimation for any type of statistical and causal query, even if the underlying missingness mechanism is of missing not at random (MNAR) type.","Both simulations and theoretical considerations demonstrate how, in the assumed MNAR setting of our study, a complete or available case analysis can be superior to multiple imputation, and estimation results vary depending on the assumed missingness DAG.","Our analyses are possibly the first to show the applicability of missingness DAGs (m-DAGs) to complex longitudinal real-world data, while highlighting the sensitivity with respect to the assumed causal model."],"url":"http://arxiv.org/abs/2402.14562v1","category":"stat.ME"}
{"created":"2024-02-22 13:57:59","title":"Closed compact forms of LQU and LQFI for general qubit-qutrit axially symmetric states","abstract":"We derive the closed compact forms of local quantum uncertainty (LQU) and local quantum Fisher information (LQFI) for hybrid qubit-qutrit axially symmetric states. As an application of the derived formulas, we study the behavior of these two quantum correlation measures at thermal equilibrium. New features are observed in their behavior that are important for quantum information processing. Interestingly, our analytical expressions for LQU and LQFI can also be useful in other scenarios and problems.","sentences":["We derive the closed compact forms of local quantum uncertainty (LQU) and local quantum Fisher information (LQFI) for hybrid qubit-qutrit axially symmetric states.","As an application of the derived formulas, we study the behavior of these two quantum correlation measures at thermal equilibrium.","New features are observed in their behavior that are important for quantum information processing.","Interestingly, our analytical expressions for LQU and LQFI can also be useful in other scenarios and problems."],"url":"http://arxiv.org/abs/2402.14560v1","category":"quant-ph"}
{"created":"2024-02-22 13:52:02","title":"LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey","abstract":"Large language models (LLMs) have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks. From natural language processing and sentiment analysis to content generation and personalized recommendations, their unparalleled adaptability has facilitated widespread adoption across industries. This transformative shift driven by LLMs underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization. In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context. To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions.","sentences":["Large language models (LLMs) have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks.","From natural language processing and sentiment analysis to content generation and personalized recommendations, their unparalleled adaptability has facilitated widespread adoption across industries.","This transformative shift driven by LLMs underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization.","In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context.","To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions."],"url":"http://arxiv.org/abs/2402.14558v1","category":"cs.CL"}
{"created":"2024-02-22 13:51:02","title":"Categories which are varieties of classical or ordered algebras","abstract":"Following ideas of Lawvere and Linton we prove that classical varieties are precisely the exact categories with a varietal generator. This means a strong generator which is abstractly finite and regularly projective. An analogous characterization of varieties of ordered algebras is also presented. We work with order-enriched categories, and introduce the concept of subexact category and subregular projective (corresponding naturally to the ordinary case). Varieties of ordered algebras are precisely the subexact categories with a subvarietal generator. This means a strong generator which is abstractly finite and subregularly projective.","sentences":["Following ideas of Lawvere and Linton we prove that classical varieties are precisely the exact categories with a varietal generator.","This means a strong generator which is abstractly finite and regularly projective.","An analogous characterization of varieties of ordered algebras is also presented.","We work with order-enriched categories, and introduce the concept of subexact category and subregular projective (corresponding naturally to the ordinary case).","Varieties of ordered algebras are precisely the subexact categories with a subvarietal generator.","This means a strong generator which is abstractly finite and subregularly projective."],"url":"http://arxiv.org/abs/2402.14557v1","category":"math.CT"}
{"created":"2024-02-22 13:50:39","title":"The Riccati Tontine: How to Satisfy Regulators on Average","abstract":"This paper presents a new type of modern accumulation-based tontine, called the Riccati tontine, named after two Italians: mathematician Jacobo Riccati (b. 1676, d. 1754) and financier Lorenzo di Tonti (b. 1602, d. 1684). The Riccati tontine is yet another way of pooling and sharing longevity risk, but is different from competing designs in two key ways. The first is that in the Riccati tontine, the representative investor is expected -- although not guaranteed -- to receive their money back if they die, or when the tontine lapses. The second is that the underlying funds within the tontine are deliberately {\\em not} indexed to the stock market. Instead, the risky assets or underlying investments are selected so that return shocks are negatively correlated with stochastic mortality, which will maximize the expected payout to survivors. This means that during a pandemic, for example, the Riccati tontine fund's performance will be impaired relative to the market index, but will not be expected to lose money for participants. In addition to describing and explaining the rationale for this non-traditional asset allocation, the paper provides a mathematical proof that the recovery schedule that generates this financial outcome satisfies a first-order ODE that is quadratic in the unknown function, which (yes) is known as a Riccati equation.","sentences":["This paper presents a new type of modern accumulation-based tontine, called the Riccati tontine, named after two Italians: mathematician Jacobo Riccati (b. 1676, d. 1754) and financier Lorenzo di Tonti (b. 1602, d. 1684).","The Riccati tontine is yet another way of pooling and sharing longevity risk, but is different from competing designs in two key ways.","The first is that in the Riccati tontine, the representative investor is expected -- although not guaranteed -- to receive their money back if they die, or when the tontine lapses.","The second is that the underlying funds within the tontine are deliberately {\\em not} indexed to the stock market.","Instead, the risky assets or underlying investments are selected so that return shocks are negatively correlated with stochastic mortality, which will maximize the expected payout to survivors.","This means that during a pandemic, for example, the Riccati tontine fund's performance will be impaired relative to the market index, but will not be expected to lose money for participants.","In addition to describing and explaining the rationale for this non-traditional asset allocation, the paper provides a mathematical proof that the recovery schedule that generates this financial outcome satisfies a first-order ODE that is quadratic in the unknown function, which (yes) is known as a Riccati equation."],"url":"http://arxiv.org/abs/2402.14555v1","category":"q-fin.MF"}
{"created":"2024-02-22 13:48:20","title":"The Stepanov theorem for Q-valued functions","abstract":"In this work we prove the Stepanov differentiation theorem for multiple-valued functions. This theorem is proved in the wide generality of metric-space-multiple-valued functions without relying on a Lipschitz extension result. General definitions of differentiability and approximate differentiability for functions between suitable metric spaces are also introduced.","sentences":["In this work we prove the Stepanov differentiation theorem for multiple-valued functions.","This theorem is proved in the wide generality of metric-space-multiple-valued functions without relying on a Lipschitz extension result.","General definitions of differentiability and approximate differentiability for functions between suitable metric spaces are also introduced."],"url":"http://arxiv.org/abs/2402.14554v1","category":"math.MG"}
{"created":"2024-02-22 13:45:01","title":"CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion","abstract":"State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks, achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in transfer learning settings with the BEiT-3 model. Importantly, our proposed CLCE approach effectively mitigates the dependency of contrastive learning on large batch sizes such as 4096 samples per batch, a limitation that has previously constrained the application of contrastive learning in budget-limited hardware environments.","sentences":["State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE).","However, it has been demonstrated that CE can compromise model generalization and stability.","While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches.","To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE.","Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance.","Experimental results demonstrate that CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks, achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in transfer learning settings with the BEiT-3 model.","Importantly, our proposed CLCE approach effectively mitigates the dependency of contrastive learning on large batch sizes such as 4096 samples per batch, a limitation that has previously constrained the application of contrastive learning in budget-limited hardware environments."],"url":"http://arxiv.org/abs/2402.14551v1","category":"cs.CV"}
{"created":"2024-02-22 13:39:02","title":"Three-loop renormalization of the quantum action for a four-dimensional scalar model with quartic interaction with the usage of the background field method and a cutoff regularization","abstract":"The paper studies the quantum action for the four-dimensional real $\\phi^4$-theory in the case of a general formulation using the background field method. The three-loop renormalization is performed with the usage of a cutoff regularization in the coordinate representation. The absence of non-local singular contributions and the correctness of the renormalization $\\mathcal{R}$-operation on the example of separate three-loop diagrams are also discussed. The explicit form of the first three coefficients for the renormalization constants and for the $\\beta$-function is presented. Consistency with previously known results is shown.","sentences":["The paper studies the quantum action for the four-dimensional real $\\phi^4$-theory in the case of a general formulation using the background field method.","The three-loop renormalization is performed with the usage of a cutoff regularization in the coordinate representation.","The absence of non-local singular contributions and the correctness of the renormalization $\\mathcal{R}$-operation on the example of separate three-loop diagrams are also discussed.","The explicit form of the first three coefficients for the renormalization constants and for the $\\beta$-function is presented.","Consistency with previously known results is shown."],"url":"http://arxiv.org/abs/2402.14549v1","category":"hep-th"}
{"created":"2024-02-22 13:36:53","title":"OmniPred: Language Models as Universal Regressors","abstract":"Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.","sentences":["Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task.","In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments.","Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models."],"url":"http://arxiv.org/abs/2402.14547v1","category":"cs.LG"}
{"created":"2024-02-22 13:33:13","title":"Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective","abstract":"Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge.","sentences":["Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs.","In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits.","By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image.","This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs.","To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations.","Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge."],"url":"http://arxiv.org/abs/2402.14545v1","category":"cs.CL"}
{"created":"2024-02-22 13:32:33","title":"{A New Hope}: Contextual Privacy Policies for Mobile Applications and An Approach Toward Automated Generation","abstract":"Privacy policies have emerged as the predominant approach to conveying privacy notices to mobile application users. In an effort to enhance both readability and user engagement, the concept of contextual privacy policies (CPPs) has been proposed by researchers. The aim of CPPs is to fragment privacy policies into concise snippets, displaying them only within the corresponding contexts within the application's graphical user interfaces (GUIs). In this paper, we first formulate CPP in mobile application scenario, and then present a novel multimodal framework, named SeePrivacy, specifically designed to automatically generate CPPs for mobile applications. This method uniquely integrates vision-based GUI understanding with privacy policy analysis, achieving 0.88 precision and 0.90 recall to detect contexts, as well as 0.98 precision and 0.96 recall in extracting corresponding policy segments. A human evaluation shows that 77% of the extracted privacy policy segments were perceived as well-aligned with the detected contexts. These findings suggest that SeePrivacy could serve as a significant tool for bolstering user interaction with, and understanding of, privacy policies. Furthermore, our solution has the potential to make privacy notices more accessible and inclusive, thus appealing to a broader demographic. A demonstration of our work can be accessed at https://cpp4app.github.io/SeePrivacy/","sentences":["Privacy policies have emerged as the predominant approach to conveying privacy notices to mobile application users.","In an effort to enhance both readability and user engagement, the concept of contextual privacy policies (CPPs) has been proposed by researchers.","The aim of CPPs is to fragment privacy policies into concise snippets, displaying them only within the corresponding contexts within the application's graphical user interfaces (GUIs).","In this paper, we first formulate CPP in mobile application scenario, and then present a novel multimodal framework, named SeePrivacy, specifically designed to automatically generate CPPs for mobile applications.","This method uniquely integrates vision-based GUI understanding with privacy policy analysis, achieving 0.88 precision and 0.90 recall to detect contexts, as well as 0.98 precision and 0.96 recall in extracting corresponding policy segments.","A human evaluation shows that 77% of the extracted privacy policy segments were perceived as well-aligned with the detected contexts.","These findings suggest that SeePrivacy could serve as a significant tool for bolstering user interaction with, and understanding of, privacy policies.","Furthermore, our solution has the potential to make privacy notices more accessible and inclusive, thus appealing to a broader demographic.","A demonstration of our work can be accessed at https://cpp4app.github.io/SeePrivacy/"],"url":"http://arxiv.org/abs/2402.14544v1","category":"cs.CR"}
{"created":"2024-02-22 13:26:56","title":"Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis","abstract":"Domain adaption has been widely adapted for cross-domain sentiment analysis to transfer knowledge from the source domain to the target domain. Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice. In this paper, we focus on the problem of domain generalization for cross-domain sentiment analysis. Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift. First, we rethink the cross-domain sentiment analysis task in a causal view to model the causal-and-effect relationships among different variables. Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment. A series of experiments over many homologous and diverse datasets show the great performance and robustness of our model by comparing it with the state-of-the-art domain generalization baselines.","sentences":["Domain adaption has been widely adapted for cross-domain sentiment analysis to transfer knowledge from the source domain to the target domain.","Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice.","In this paper, we focus on the problem of domain generalization for cross-domain sentiment analysis.","Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift.","First, we rethink the cross-domain sentiment analysis task in a causal view to model the causal-and-effect relationships among different variables.","Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment.","A series of experiments over many homologous and diverse datasets show the great performance and robustness of our model by comparing it with the state-of-the-art domain generalization baselines."],"url":"http://arxiv.org/abs/2402.14536v1","category":"cs.CL"}
{"created":"2024-02-22 13:25:17","title":"Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard","abstract":"Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality. However, it is unclear whether LLMs tend to exhibit distinctive linguistic styles akin to how human authors do. Through a comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse inputs. The results point to significant linguistic variations which, in turn, enable us to attribute a given text to its LLM origin with a favorable 88\\% accuracy using a simple off-the-shelf classification model. Theoretical and practical implications of this intriguing finding are discussed.","sentences":["Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality.","However, it is unclear whether LLMs tend to exhibit distinctive linguistic styles akin to how human authors do.","Through a comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse inputs.","The results point to significant linguistic variations which, in turn, enable us to attribute a given text to its LLM origin with a favorable 88\\% accuracy using a simple off-the-shelf classification model.","Theoretical and practical implications of this intriguing finding are discussed."],"url":"http://arxiv.org/abs/2402.14533v1","category":"cs.CL"}
{"created":"2024-02-22 13:24:03","title":"Dynamical quantum maps for single-qubit gates under non-Markovian phase noise","abstract":"Noise is both ubiquitous and generally deleterious in settings where precision is required. This is especially true in the quantum technology sector where system utility typically decays rapidly under its influence. Understanding the noise in quantum devices is thus a prerequisite for efficient strategies to mitigate or even eliminate its harmful effects. However, this requires resources that are often prohibitive, such that the typically-used noise models rely on simplifications that sometimes depart from experimental reality. Here we derive a compact microscopic error model for single-qubit gates that only requires a single experimental input - the noise power spectral density. Our model goes beyond standard depolarizing or Pauli-twirled noise models, explicitly including non-Clifford and non-Markovian contributions to the dynamical error map. We gauge our predictions for experimentally relevant metrics against established characterization techniques run on a trapped-ion quantum computer. In particular, we find that experimental estimates of average gate errors measured through randomized benchmarking and reconstructed via quantum process tomography are tightly lower-bounded by our analytical estimates, while the depolarizing model overestimates the gate error. Our noise modeling including non-Markovian contributions can be readily applied to established frameworks such as dynamical decoupling and dynamically-corrected gates, or to provide more realistic thresholds for quantum error correction.","sentences":["Noise is both ubiquitous and generally deleterious in settings where precision is required.","This is especially true in the quantum technology sector where system utility typically decays rapidly under its influence.","Understanding the noise in quantum devices is thus a prerequisite for efficient strategies to mitigate or even eliminate its harmful effects.","However, this requires resources that are often prohibitive, such that the typically-used noise models rely on simplifications that sometimes depart from experimental reality.","Here we derive a compact microscopic error model for single-qubit gates that only requires a single experimental input - the noise power spectral density.","Our model goes beyond standard depolarizing or Pauli-twirled noise models, explicitly including non-Clifford and non-Markovian contributions to the dynamical error map.","We gauge our predictions for experimentally relevant metrics against established characterization techniques run on a trapped-ion quantum computer.","In particular, we find that experimental estimates of average gate errors measured through randomized benchmarking and reconstructed via quantum process tomography are tightly lower-bounded by our analytical estimates, while the depolarizing model overestimates the gate error.","Our noise modeling including non-Markovian contributions can be readily applied to established frameworks such as dynamical decoupling and dynamically-corrected gates, or to provide more realistic thresholds for quantum error correction."],"url":"http://arxiv.org/abs/2402.14530v1","category":"quant-ph"}
{"created":"2024-02-22 13:22:06","title":"ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization","abstract":"The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. Benchmark results and videos are available at https://ace-rl.github.io/.","sentences":["The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms.","Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training.","We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration.","Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method.","Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach.","Benchmark results and videos are available at https://ace-rl.github.io/."],"url":"http://arxiv.org/abs/2402.14528v1","category":"cs.LG"}
{"created":"2024-02-22 13:20:53","title":"Balanced Data Sampling for Language Model Training with Clustering","abstract":"Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models.","sentences":["Data plays a fundamental role in the training of Large Language Models (LLMs).","While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question.","Most LLMs are trained with a simple strategy, random sampling.","However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal.","In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training.","Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results.","A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters.","Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models."],"url":"http://arxiv.org/abs/2402.14526v1","category":"cs.CL"}
{"created":"2024-02-22 13:19:02","title":"Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers","abstract":"Bimanual handovers are crucial for transferring large, deformable or delicate objects. This paper proposes a framework for generating kinematically constrained human-like bimanual robot motions to ensure seamless and natural robot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to reactively generate suitable response trajectories for a robot based on the observed human partner's motion. The trajectories are adapted with task space constraints to ensure accurate handovers. Results from a pilot study show that our approach is perceived as more human--like compared to a baseline Inverse Kinematics approach.","sentences":["Bimanual handovers are crucial for transferring large, deformable or delicate objects.","This paper proposes a framework for generating kinematically constrained human-like bimanual robot motions to ensure seamless and natural robot-to-human object handovers.","We use a Hidden Semi-Markov Model (HSMM) to reactively generate suitable response trajectories for a robot based on the observed human partner's motion.","The trajectories are adapted with task space constraints to ensure accurate handovers.","Results from a pilot study show that our approach is perceived as more human--like compared to a baseline Inverse Kinematics approach."],"url":"http://arxiv.org/abs/2402.14525v1","category":"cs.RO"}
{"created":"2024-02-22 13:17:34","title":"Simulations and Performance Studies of a MAPS in 65 nm CMOS Imaging Technology","abstract":"Monolithic active pixel sensors (MAPS) produced in a 65 nm CMOS imaging technology are being investigated for applications in particle physics. The MAPS design has a small collection electrode characterized by an input capacitance of ~fF, granting a high signal-to-noise ratio and low power consumption. Additionally, the 65 nm CMOS imaging technology brings a reduction in material budget and improved logic density of the readout circuitry, compared to previously studied technologies. Given these features, this technology was chosen by the TANGERINE project to develop the next generation of silicon pixel sensors. The sensor design targets temporal and spatial resolutions compatible with the requirements for a vertex detector at future lepton colliders. Simulations and test-beam characterization of technology demonstrators have been carried out in close collaboration with the CERN EP R&D program and the ALICE ITS3 upgrade. TCAD device simulations using generic doping profiles and Monte Carlo simulations have been used to build an understanding of the technology and predict the performance parameters of the sensor. Technology demonstrators of a 65 nm CMOS MAPS with a small collection electrode have been characterized in laboratory and test-beam facilities by studying performance parameters such as cluster size, charge collection, and efficiency. This work compares simulation results to test-beam data. The experimental results establish this technology as a promising candidate for a vertex detector at future lepton colliders and give valuable information for improving the simulation approach.","sentences":["Monolithic active pixel sensors (MAPS) produced in a 65 nm CMOS imaging technology are being investigated for applications in particle physics.","The MAPS design has a small collection electrode characterized by an input capacitance of ~fF, granting a high signal-to-noise ratio and low power consumption.","Additionally, the 65 nm CMOS imaging technology brings a reduction in material budget and improved logic density of the readout circuitry, compared to previously studied technologies.","Given these features, this technology was chosen by the TANGERINE project to develop the next generation of silicon pixel sensors.","The sensor design targets temporal and spatial resolutions compatible with the requirements for a vertex detector at future lepton colliders.","Simulations and test-beam characterization of technology demonstrators have been carried out in close collaboration with the CERN EP R&D program and the ALICE ITS3 upgrade.","TCAD device simulations using generic doping profiles and Monte Carlo simulations have been used to build an understanding of the technology and predict the performance parameters of the sensor.","Technology demonstrators of a 65 nm CMOS MAPS with a small collection electrode have been characterized in laboratory and test-beam facilities by studying performance parameters such as cluster size, charge collection, and efficiency.","This work compares simulation results to test-beam data.","The experimental results establish this technology as a promising candidate for a vertex detector at future lepton colliders and give valuable information for improving the simulation approach."],"url":"http://arxiv.org/abs/2402.14524v1","category":"physics.ins-det"}
{"created":"2024-02-22 13:08:23","title":"Bulk Boundary Paradox in the Surface Reconstructed Magnetic Weyl Semimetal NdAlSi","abstract":"The bulk boundary correspondence in the context of Weyl semimetals is a fundamental topological principle that establishes a connection between the bulk properties of the material and the emergence of specific surface states. In Weyl semimetals, the bulk boundary correspondence is manifested by the presence of surface Fermi arcs connecting pairs of Weyl nodes with opposite chirality. Here we demonstrate that this bulk boundary correspondence is challenged in the case of the surface selectively reconstructed noncentrosymmetric magnetic Weyl semimetal NdAlSi. By comparing angle-resolved photoemission spectroscopy measurements with surface projected density functional theory calculations and scanning tunneling microscope measurements, the existence of surface selective spontaneous reconstruction is demonstrated. The surface reconstruction in NdAlSi not only leads to the reconstruction of the surface Fermi arcs, but also generates new surface Fermi arcs that do not connect corresponding Weyl nodes. This observation challenges the conventional view of the bulk boundary correspondence in Weyl semimetals.","sentences":["The bulk boundary correspondence in the context of Weyl semimetals is a fundamental topological principle that establishes a connection between the bulk properties of the material and the emergence of specific surface states.","In Weyl semimetals, the bulk boundary correspondence is manifested by the presence of surface Fermi arcs connecting pairs of Weyl nodes with opposite chirality.","Here we demonstrate that this bulk boundary correspondence is challenged in the case of the surface selectively reconstructed noncentrosymmetric magnetic Weyl semimetal NdAlSi.","By comparing angle-resolved photoemission spectroscopy measurements with surface projected density functional theory calculations and scanning tunneling microscope measurements, the existence of surface selective spontaneous reconstruction is demonstrated.","The surface reconstruction in NdAlSi not only leads to the reconstruction of the surface Fermi arcs, but also generates new surface Fermi arcs that do not connect corresponding Weyl nodes.","This observation challenges the conventional view of the bulk boundary correspondence in Weyl semimetals."],"url":"http://arxiv.org/abs/2402.14518v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 13:06:36","title":"Upper bounds on the genus of Albanese fibrations","abstract":"Let $S$ be a minimal irregular surface of general type, whose Albanese map induces a hyperelliptic fibration $f:\\,S \\to B$ of genus $g$.We prove a quadratic upper bound on the genus $g$, i.e., $g\\leq h\\big(\\chi(\\mathcal{O}_S)\\big)$, where $h$ is a quadratic function. We also construct examples showing that the quadratic upper bounds can not be improved to the linear ones. In the special case when $p_g(S)=q(S)=1$, we show that $g\\leq 14$.","sentences":["Let $S$ be a minimal irregular surface of general type, whose Albanese map induces a hyperelliptic fibration $f:\\,S \\to B$ of genus $g$.We prove a quadratic upper bound on the genus $g$, i.e., $g\\leq h\\big(\\chi(\\mathcal{O}_S)\\big)$, where $h$ is a quadratic function.","We also construct examples showing that the quadratic upper bounds can not be improved to the linear ones.","In the special case when $p_g(S)=q(S)=1$, we show that $g\\leq 14$."],"url":"http://arxiv.org/abs/2402.14516v1","category":"math.AG"}
{"created":"2024-02-22 13:04:50","title":"Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks","abstract":"Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum. We analyse this frequency spectrum and prove, for a large class of models, various maximality results. Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depends only on the area $A = RL$ and not on the individual values of $R$ and $L$. Moreover, we extend existing results and specify the maximum possible frequency spectrum of a QNN with arbitrarily many layers as a function of the spectrum of its generators. If the generators of the QNN can be further decomposed into 2-dimensional sub-generators, then this specification follows from elementary number-theoretical considerations. In the case of arbitrary dimensional generators, we extend existing results based on the so-called Golomb ruler and introduce a second novel approach based on a variation of the turnpike problem, which we call the relaxed turnpike problem.","sentences":["Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices.","A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum.","We analyse this frequency spectrum and prove, for a large class of models, various maximality results.","Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations.","With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depends only on the area","$A = RL$ and not on the individual values of $R$ and $L$.","Moreover, we extend existing results and specify the maximum possible frequency spectrum of a QNN with arbitrarily many layers as a function of the spectrum of its generators.","If the generators of the QNN can be further decomposed into 2-dimensional sub-generators, then this specification follows from elementary number-theoretical considerations.","In the case of arbitrary dimensional generators, we extend existing results based on the so-called Golomb ruler and introduce a second novel approach based on a variation of the turnpike problem, which we call the relaxed turnpike problem."],"url":"http://arxiv.org/abs/2402.14515v1","category":"quant-ph"}
{"created":"2024-02-22 13:03:26","title":"Chern-Simons gravity and PT Symmetry","abstract":"This paper considers a string inspired effective axion anomalously coupled to Abelian gauge fields and gravity. By considering the renormalisation group flows in the flat space limit it is observed that a Hermitian parity symmetric phase of the theory can flow into a non-Hermitian parity-time symmetric phase. This behavior has implications for Chern-Simons gravity. The repulsive nature of gravity, usually attributed to the existence of a positive cosmological constant, is reinterpreted at large scales as a flow from a Hermitian (attractive) gravitational theory, to a PT-symmetric (repulsive) gravity in the infrared. The discussion here is presented in the context of a Chern-Simons gravitational theory but it may be valid more generally in gravity with torsion. The validity of such a scenario in realistic theories might alleviate the need for de Sitter phases in the current epoch of the cosmological evolution, thus avoiding their associated conceptual and technical complications.","sentences":["This paper considers a string inspired effective axion anomalously coupled to Abelian gauge fields and gravity.","By considering the renormalisation group flows in the flat space limit it is observed that a Hermitian parity symmetric phase of the theory can flow into a non-Hermitian parity-time symmetric phase.","This behavior has implications for Chern-Simons gravity.","The repulsive nature of gravity, usually attributed to the existence of a positive cosmological constant, is reinterpreted at large scales as a flow from a Hermitian (attractive) gravitational theory, to a PT-symmetric (repulsive) gravity in the infrared.","The discussion here is presented in the context of a Chern-Simons gravitational theory but it may be valid more generally in gravity with torsion.","The validity of such a scenario in realistic theories might alleviate the need for de Sitter phases in the current epoch of the cosmological evolution, thus avoiding their associated conceptual and technical complications."],"url":"http://arxiv.org/abs/2402.14513v1","category":"hep-th"}
{"created":"2024-02-22 13:00:19","title":"Voltage tunable sign inversion of magnetoresistance in van der Waals Fe3GeTe2/MoSe2/Fe3GeTe2 tunnel junctions","abstract":"The magnetic tunnel junctions (MTJ) based on van der Waals (vdW) materials possess atomically smooth interfaces with minimal element intermixing. This characteristic ensures that spin polarization is well maintained during transport, leading to the emergence of richer magnetoresistance behaviors. Here, using all 2D vdW MTJs based on magnetic metal Fe3GeTe2 and non-magnetic semiconductor MoSe2, we demonstrate that the magnitude and even sign of the magnetoresistance can be tuned by the applied voltage. The sign inversion of the magnetoresistance is observed in a wide temperature range below the Curie temperature. This tunable magnetoresistance sign may be attributed to the spin polarizations of the tunneling carriers and the band structure of the two ferromagnetic electrodes. Such robust electrical tunability of magnetoresistance extends the functionalities of low-dimensional spintronics and makes it more appealing for next-generation spintronics with all-vdW MTJs.","sentences":["The magnetic tunnel junctions (MTJ) based on van der Waals (vdW) materials possess atomically smooth interfaces with minimal element intermixing.","This characteristic ensures that spin polarization is well maintained during transport, leading to the emergence of richer magnetoresistance behaviors.","Here, using all 2D vdW MTJs based on magnetic metal Fe3GeTe2 and non-magnetic semiconductor MoSe2, we demonstrate that the magnitude and even sign of the magnetoresistance can be tuned by the applied voltage.","The sign inversion of the magnetoresistance is observed in a wide temperature range below the Curie temperature.","This tunable magnetoresistance sign may be attributed to the spin polarizations of the tunneling carriers and the band structure of the two ferromagnetic electrodes.","Such robust electrical tunability of magnetoresistance extends the functionalities of low-dimensional spintronics and makes it more appealing for next-generation spintronics with all-vdW MTJs."],"url":"http://arxiv.org/abs/2402.14510v1","category":"physics.app-ph"}
{"created":"2024-02-22 12:55:57","title":"Shifts on the lamplighter group","abstract":"We prove that the lamplighter group admits strongly aperiodic SFTs, has undecidable tiling problem, and the entropies of its SFTs are exactly the upper semicomputable nonnegative real numbers, and some other results. These results follow from two relatively general simulation theorems, which show that for a large class of effective subshifts on the sea-level subgroup, their induction to the lamplighter group is sofic; and the pullback of every effective Cantor system on the integers admits an SFT cover. We exhibit a concrete strongly aperiodic set with $1488$ tetrahedra. We show that metabelian Baumslag-Solitar groups are intersimulable with lamplighter groups, and thus we obtain the same characterization for their entropies.","sentences":["We prove that the lamplighter group admits strongly aperiodic SFTs, has undecidable tiling problem, and the entropies of its SFTs are exactly the upper semicomputable nonnegative real numbers, and some other results.","These results follow from two relatively general simulation theorems, which show that for a large class of effective subshifts on the sea-level subgroup, their induction to the lamplighter group is sofic; and the pullback of every effective Cantor system on the integers admits an SFT cover.","We exhibit a concrete strongly aperiodic set with $1488$ tetrahedra.","We show that metabelian Baumslag-Solitar groups are intersimulable with lamplighter groups, and thus we obtain the same characterization for their entropies."],"url":"http://arxiv.org/abs/2402.14508v1","category":"math.DS"}
{"created":"2024-02-22 12:55:01","title":"Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition","abstract":"Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the time of submission). The code is released at https://github.com/Lu-Feng/SelaVPR.","sentences":["Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems.","However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR).","Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address.","To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR.","Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model.","Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking.","Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification.","It ranks 1st on the MSLS challenge leaderboard (at the time of submission).","The code is released at https://github.com/Lu-Feng/SelaVPR."],"url":"http://arxiv.org/abs/2402.14505v1","category":"cs.CV"}
{"created":"2024-02-22 12:51:10","title":"Understanding Human-AI Collaboration in Music Therapy Through Co-Design with Therapists","abstract":"The rapid development of musical AI technologies has expanded the creative potential of various musical activities, ranging from music style transformation to music generation. However, little research has investigated how musical AIs can support music therapists, who urgently need new technology support. This study used a mixed method, including semi-structured interviews and a participatory design approach. By collaborating with music therapists, we explored design opportunities for musical AIs in music therapy. We presented the co-design outcomes involving the integration of musical AIs into a music therapy process, which was developed from a theoretical framework rooted in emotion-focused therapy. After that, we concluded the benefits and concerns surrounding music AIs from the perspective of music therapists. Based on our findings, we discussed the opportunities and design implications for applying musical AIs to music therapy. Our work offers valuable insights for developing human-AI collaborative music systems in therapy involving complex procedures and specific requirements.","sentences":["The rapid development of musical AI technologies has expanded the creative potential of various musical activities, ranging from music style transformation to music generation.","However, little research has investigated how musical AIs can support music therapists, who urgently need new technology support.","This study used a mixed method, including semi-structured interviews and a participatory design approach.","By collaborating with music therapists, we explored design opportunities for musical AIs in music therapy.","We presented the co-design outcomes involving the integration of musical AIs into a music therapy process, which was developed from a theoretical framework rooted in emotion-focused therapy.","After that, we concluded the benefits and concerns surrounding music AIs from the perspective of music therapists.","Based on our findings, we discussed the opportunities and design implications for applying musical AIs to music therapy.","Our work offers valuable insights for developing human-AI collaborative music systems in therapy involving complex procedures and specific requirements."],"url":"http://arxiv.org/abs/2402.14503v1","category":"cs.HC"}
{"created":"2024-02-22 12:47:33","title":"\"My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models","abstract":"The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with \"Sure\" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output, too and ii) caution against relying solely on first-token evaluation.","sentences":["The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging.","One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space.","The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction.","However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with \"Sure\" or refusing to answer.","Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users.","But by how much?","We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation.","Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%.","Models heavily fine-tuned on conversational or safety data are especially impacted.","Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template.","Our findings i) underscore the importance of inspecting the text output, too and ii) caution against relying solely on first-token evaluation."],"url":"http://arxiv.org/abs/2402.14499v1","category":"cs.CL"}
{"created":"2024-02-22 12:47:04","title":"A Collision-Aware Cable Grasping Method in Cluttered Environment","abstract":"We introduce a Cable Grasping-Convolutional Neural Network designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. Supplementary materials can be found at https://leizhang-public.github.io/cg-cnn/ .","sentences":["We introduce a Cable Grasping-Convolutional Neural Network designed to facilitate robust cable grasping in cluttered environments.","Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers.","We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts.","The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques.","Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot controller for execution.","Grasping efficacy is assessed across both synthetic and real-world settings.","Given our model implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches.","Supplementary materials can be found at https://leizhang-public.github.io/cg-cnn/ ."],"url":"http://arxiv.org/abs/2402.14498v1","category":"cs.RO"}
{"created":"2024-02-22 12:40:08","title":"Fundamental bounds for parameter estimation with few measurements","abstract":"Bounding the optimal precision in parameter estimation tasks is of central importance for technological applications. In the regime of a small number of measurements, or that of low signal-to-noise ratios, the meaning of common frequentist bounds such as the Cram\\'er-Rao bound (CRB) become questionable. Here, we discuss different linear (Barankin-like) conditions that can be imposed on estimators and analyze when these conditions admit an optimal estimator with finite variance, for any number of measurement repetitions. We show that, if the number of imposed conditions is larger than the number of measurement outcomes, there generally does not exist a corresponding estimator with finite variance. We analyze this result from different viewpoints and examples and elaborate on connections to the shot-noise limit and the Kitaev phase estimation algorithm. We then derive an extended Cram\\'er-Rao bound that is compatible with a finite variance in situations where the Barankin bound is undefined. Finally, we show an exemplary numerical confrontation between frequentist and Bayesian approaches to parameter estimation.","sentences":["Bounding the optimal precision in parameter estimation tasks is of central importance for technological applications.","In the regime of a small number of measurements, or that of low signal-to-noise ratios, the meaning of common frequentist bounds such as the Cram\\'er-Rao bound (CRB) become questionable.","Here, we discuss different linear (Barankin-like) conditions that can be imposed on estimators and analyze when these conditions admit an optimal estimator with finite variance, for any number of measurement repetitions.","We show that, if the number of imposed conditions is larger than the number of measurement outcomes, there generally does not exist a corresponding estimator with finite variance.","We analyze this result from different viewpoints and examples and elaborate on connections to the shot-noise limit and the Kitaev phase estimation algorithm.","We then derive an extended Cram\\'er-Rao bound that is compatible with a finite variance in situations where the Barankin bound is undefined.","Finally, we show an exemplary numerical confrontation between frequentist and Bayesian approaches to parameter estimation."],"url":"http://arxiv.org/abs/2402.14495v1","category":"quant-ph"}
{"created":"2024-02-22 12:39:50","title":"Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task","abstract":"In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to improve the model's robustness. Experimental results demonstrate the superiority of our proposed approach over state-of-the-art models, and further analysis confirms its effectiveness and generalization ability.","sentences":["In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task.","Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances.","In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training.","Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution.","During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels.","Additionally, we introduce an adversarial attack training strategy to improve the model's robustness.","Experimental results demonstrate the superiority of our proposed approach over state-of-the-art models, and further analysis confirms its effectiveness and generalization ability."],"url":"http://arxiv.org/abs/2402.14494v1","category":"cs.CL"}
{"created":"2024-02-22 12:35:50","title":"INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning","abstract":"Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks. It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times. Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training data multiple times.","sentences":["Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks.","Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data.","In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks.","It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times.","Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training data multiple times."],"url":"http://arxiv.org/abs/2402.14492v1","category":"cs.CL"}
{"created":"2024-02-22 12:27:35","title":"A Class of Topological Pseudodistances for Fast Comparison of Persistence Diagrams","abstract":"Persistence diagrams (PD)s play a central role in topological data analysis, and are used in an ever increasing variety of applications. The comparison of PD data requires computing comparison metrics among large sets of PDs, with metrics which are accurate, theoretically sound, and fast to compute. Especially for denser multi-dimensional PDs, such comparison metrics are lacking. While on the one hand, Wasserstein-type distances have high accuracy and theoretical guarantees, they incur high computational cost. On the other hand, distances between vectorizations such as Persistence Statistics (PS)s have lower computational cost, but lack the accuracy guarantees and in general they are not guaranteed to distinguish PDs (i.e. the two PS vectors of different PDs may be equal). In this work we introduce a class of pseudodistances called Extended Topological Pseudodistances (ETD)s, which have tunable complexity, and can approximate Sliced and classical Wasserstein distances at the high-complexity extreme, while being computationally lighter and close to Persistence Statistics at the lower complexity extreme, and thus allow users to interpolate between the two metrics. We build theoretical comparisons to show how to fit our new distances at an intermediate level between persistence vectorizations and Wasserstein distances. We also experimentally verify that ETDs outperform PSs in terms of accuracy and outperform Wasserstein and Sliced Wasserstein distances in terms of computational complexity.","sentences":["Persistence diagrams (PD)s play a central role in topological data analysis, and are used in an ever increasing variety of applications.","The comparison of PD data requires computing comparison metrics among large sets of PDs, with metrics which are accurate, theoretically sound, and fast to compute.","Especially for denser multi-dimensional PDs, such comparison metrics are lacking.","While on the one hand, Wasserstein-type distances have high accuracy and theoretical guarantees, they incur high computational cost.","On the other hand, distances between vectorizations such as Persistence Statistics (PS)s have lower computational cost, but lack the accuracy guarantees and in general they are not guaranteed to distinguish PDs (i.e. the two PS vectors of different PDs may be equal).","In this work we introduce a class of pseudodistances called Extended Topological Pseudodistances (ETD)s, which have tunable complexity, and can approximate Sliced and classical Wasserstein distances at the high-complexity extreme, while being computationally lighter and close to Persistence Statistics at the lower complexity extreme, and thus allow users to interpolate between the two metrics.","We build theoretical comparisons to show how to fit our new distances at an intermediate level between persistence vectorizations and Wasserstein distances.","We also experimentally verify that ETDs outperform PSs in terms of accuracy and outperform Wasserstein and Sliced Wasserstein distances in terms of computational complexity."],"url":"http://arxiv.org/abs/2402.14489v1","category":"cs.CG"}
{"created":"2024-02-22 12:26:07","title":"Does the Generator Mind its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer","abstract":"The present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation. However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations. To gain deeper insights into the underlying causes of this phenomenon, we conduct a series of experiments that verify the critical role played by context in hallucination, both during training and testing, from various perspectives.","sentences":["The present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context.","Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation.","However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge.","Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence.","In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations.","Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations.","To gain deeper insights into the underlying causes of this phenomenon, we conduct a series of experiments that verify the critical role played by context in hallucination, both during training and testing, from various perspectives."],"url":"http://arxiv.org/abs/2402.14488v1","category":"cs.CL"}
{"created":"2024-02-22 12:19:19","title":"Are Bounded Contracts Learnable and Approximately Optimal?","abstract":"This paper considers the hidden-action model of the principal-agent problem, in which a principal incentivizes an agent to work on a project using a contract. We investigate whether contracts with bounded payments are learnable and approximately optimal. Our main results are two learning algorithms that can find a nearly optimal bounded contract using a polynomial number of queries, under two standard assumptions in the literature: a costlier action for the agent leads to a better outcome distribution for the principal, and the agent's cost/effort has diminishing returns. Our polynomial query complexity upper bound shows that standard assumptions are sufficient for achieving an exponential improvement upon the known lower bound for general instances. Unlike the existing algorithms, which relied on discretizing the contract space, our algorithms directly learn the underlying outcome distributions. As for the approximate optimality of bounded contracts, we find that they could be far from optimal in terms of multiplicative or additive approximation, but satisfy a notion of mixed approximation.","sentences":["This paper considers the hidden-action model of the principal-agent problem, in which a principal incentivizes an agent to work on a project using a contract.","We investigate whether contracts with bounded payments are learnable and approximately optimal.","Our main results are two learning algorithms that can find a nearly optimal bounded contract using a polynomial number of queries, under two standard assumptions in the literature: a costlier action for the agent leads to a better outcome distribution for the principal, and the agent's cost/effort has diminishing returns.","Our polynomial query complexity upper bound shows that standard assumptions are sufficient for achieving an exponential improvement upon the known lower bound for general instances.","Unlike the existing algorithms, which relied on discretizing the contract space, our algorithms directly learn the underlying outcome distributions.","As for the approximate optimality of bounded contracts, we find that they could be far from optimal in terms of multiplicative or additive approximation, but satisfy a notion of mixed approximation."],"url":"http://arxiv.org/abs/2402.14486v1","category":"cs.GT"}
{"created":"2024-02-22 12:19:04","title":"Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis","abstract":"Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency to falsely recognize non-causal sequences as causal sequences. These issues become even more pronounced with advanced versions of the model, such as GPT-4. In addition, we highlight the constraints of ChatGPT in handling complex causality types, including both intra/inter-sentential and implicit causality. The model also faces challenges with effectively leveraging in-context learning and domain adaptation. Our code is available on \\url{https://github.com/retarfi/gemcausal}","sentences":["Causality is fundamental in human cognition and has drawn attention in diverse research fields.","With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns.","This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities.","Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets.","We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches.","Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining.","Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets.","However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance.","Additionally, ChatGPT suffers from the tendency to falsely recognize non-causal sequences as causal sequences.","These issues become even more pronounced with advanced versions of the model, such as GPT-4.","In addition, we highlight the constraints of ChatGPT in handling complex causality types, including both intra/inter-sentential and implicit causality.","The model also faces challenges with effectively leveraging in-context learning and domain adaptation.","Our code is available on \\url{https://github.com/retarfi/gemcausal}"],"url":"http://arxiv.org/abs/2402.14484v1","category":"cs.CL"}
{"created":"2024-02-22 12:15:05","title":"SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects","abstract":"The use of deep learning models in computational biology has increased massively in recent years, and is expected to do so further with the current advances in fields like Natural Language Processing. These models, although able to draw complex relations between input and target, are also largely inclined to learn noisy deviations from the pool of data used during their development. In order to assess their performance on unseen data (their capacity to generalize), it is common to randomly split the available data in development (train/validation) and test sets. This procedure, although standard, has lately been shown to produce dubious assessments of generalization due to the existing similarity between samples in the databases used. In this work, we present SpanSeq, a database partition method for machine learning that can scale to most biological sequences (genes, proteins and genomes) in order to avoid data leakage between sets. We also explore the effect of not restraining similarity between sets by reproducing the development of the state-of-the-art model DeepLoc, not only confirming the consequences of randomly splitting databases on the model assessment, but expanding those repercussions to the model development. SpanSeq is available for downloading and installing at https://github.com/genomicepidemiology/SpanSeq.","sentences":["The use of deep learning models in computational biology has increased massively in recent years, and is expected to do so further with the current advances in fields like Natural Language Processing.","These models, although able to draw complex relations between input and target, are also largely inclined to learn noisy deviations from the pool of data used during their development.","In order to assess their performance on unseen data (their capacity to generalize), it is common to randomly split the available data in development (train/validation) and test sets.","This procedure, although standard, has lately been shown to produce dubious assessments of generalization due to the existing similarity between samples in the databases used.","In this work, we present SpanSeq, a database partition method for machine learning that can scale to most biological sequences (genes, proteins and genomes) in order to avoid data leakage between sets.","We also explore the effect of not restraining similarity between sets by reproducing the development of the state-of-the-art model DeepLoc, not only confirming the consequences of randomly splitting databases on the model assessment, but expanding those repercussions to the model development.","SpanSeq is available for downloading and installing at https://github.com/genomicepidemiology/SpanSeq."],"url":"http://arxiv.org/abs/2402.14482v1","category":"cs.LG"}
{"created":"2024-02-22 12:13:58","title":"Towards Automated Causal Discovery: a case study on 5G telecommunication data","abstract":"We introduce the concept of Automated Causal Discovery (AutoCD), defined as any system that aims to fully automate the application of causal discovery and causal reasoning methods. AutoCD's goal is to deliver all causal information that an expert human analyst would and answer a user's causal queries. We describe the architecture of such a platform, and illustrate its performance on synthetic data sets. As a case study, we apply it on temporal telecommunication data. The system is general and can be applied to a plethora of causal discovery problems.","sentences":["We introduce the concept of Automated Causal Discovery (AutoCD), defined as any system that aims to fully automate the application of causal discovery and causal reasoning methods.","AutoCD's goal is to deliver all causal information that an expert human analyst would and answer a user's causal queries.","We describe the architecture of such a platform, and illustrate its performance on synthetic data sets.","As a case study, we apply it on temporal telecommunication data.","The system is general and can be applied to a plethora of causal discovery problems."],"url":"http://arxiv.org/abs/2402.14481v1","category":"cs.LG"}
{"created":"2024-02-22 12:13:35","title":"MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation","abstract":"Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation. This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM-augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world LLM scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51\\% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM-augmented applications.","sentences":["Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information.","However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching.","False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses.","Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation.","This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM-augmented generation systems.","We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissimilar ones should not.","MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world LLM scenarios.","Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies.","The results, showing a maximum accuracy of only 41.51\\% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM-augmented applications."],"url":"http://arxiv.org/abs/2402.14480v1","category":"cs.SE"}
{"created":"2024-02-22 12:09:52","title":"DynGMA: a robust approach for learning stochastic differential equations from data","abstract":"Learning unknown stochastic differential equations (SDEs) from observed data is a significant and challenging task with applications in various fields. Current approaches often use neural networks to represent drift and diffusion functions, and construct likelihood-based loss by approximating the transition density to train these networks. However, these methods often rely on one-step stochastic numerical schemes, necessitating data with sufficiently high time resolution. In this paper, we introduce novel approximations to the transition density of the parameterized SDE: a Gaussian density approximation inspired by the random perturbation theory of dynamical systems, and its extension, the dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invariant distribution from trajectory data. And it is capable of handling trajectory data with low time resolution and variable, even uncontrollable, time step sizes, such as data generated from Gillespie's stochastic simulations. We then conduct several experiments across various scenarios to verify the advantages and robustness of the proposed method.","sentences":["Learning unknown stochastic differential equations (SDEs) from observed data is a significant and challenging task with applications in various fields.","Current approaches often use neural networks to represent drift and diffusion functions, and construct likelihood-based loss by approximating the transition density to train these networks.","However, these methods often rely on one-step stochastic numerical schemes, necessitating data with sufficiently high time resolution.","In this paper, we introduce novel approximations to the transition density of the parameterized SDE: a Gaussian density approximation inspired by the random perturbation theory of dynamical systems, and its extension, the dynamical Gaussian mixture approximation (DynGMA).","Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invariant distribution from trajectory data.","And it is capable of handling trajectory data with low time resolution and variable, even uncontrollable, time step sizes, such as data generated from Gillespie's stochastic simulations.","We then conduct several experiments across various scenarios to verify the advantages and robustness of the proposed method."],"url":"http://arxiv.org/abs/2402.14475v1","category":"cs.LG"}
{"created":"2024-02-22 12:04:15","title":"Data Science with LLMs and Interpretable Models","abstract":"Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans. In this work, we show that large language models (LLMs) are remarkably good at working with interpretable models, too. In particular, we show that LLMs can describe, interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique. LLMs can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon. We release \\url{https://github.com/interpretml/TalkToEBM} as an open-source LLM-GAM interface.","sentences":["Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans.","In this work, we show that large language models (LLMs) are remarkably good at working with interpretable models, too.","In particular, we show that LLMs can describe, interpret, and debug Generalized Additive Models (GAMs).","Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique.","LLMs can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon.","We release \\url{https://github.com/interpretml/TalkToEBM} as an open-source LLM-GAM interface."],"url":"http://arxiv.org/abs/2402.14474v1","category":"cs.LG"}
{"created":"2024-02-22 12:03:21","title":"Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation","abstract":"Sequential Recommendation (SR) captures users' dynamic preferences by modeling how users transit among items. However, SR models that utilize only single type of behavior interaction data encounter performance degradation when the sequences are short. To tackle this problem, we focus on Multi-Behavior Sequential Recommendation (MBSR) in this paper, which aims to leverage time-evolving heterogeneous behavioral dependencies for better exploring users' potential intents on the target behavior. Solving MBSR is challenging. On the one hand, users exhibit diverse multi-behavior patterns due to personal characteristics. On the other hand, there exists comprehensive co-influence between behavior correlations and item collaborations, the intensity of which is deeply affected by temporal factors. To tackle these challenges, we propose a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem, which models personalized patterns and multifaceted sequential collaborations in a novel way to boost recommendation performance. First, PBAT develops a personalized behavior pattern generator in the representation layer, which extracts dynamic and discriminative behavior patterns for sequential learning. Second, PBAT reforms the self-attention layer with a behavior-aware collaboration extractor, which introduces a fused behavior-aware attention mechanism for incorporating both behavioral and temporal impacts into collaborative transitions. We conduct experiments on three benchmark datasets and the results demonstrate the effectiveness and interpretability of our framework. Our implementation code is released at https://github.com/TiliaceaeSU/PBAT.","sentences":["Sequential Recommendation (SR) captures users' dynamic preferences by modeling how users transit among items.","However, SR models that utilize only single type of behavior interaction data encounter performance degradation when the sequences are short.","To tackle this problem, we focus on Multi-Behavior Sequential Recommendation (MBSR) in this paper, which aims to leverage time-evolving heterogeneous behavioral dependencies for better exploring users' potential intents on the target behavior.","Solving MBSR is challenging.","On the one hand, users exhibit diverse multi-behavior patterns due to personal characteristics.","On the other hand, there exists comprehensive co-influence between behavior correlations and item collaborations, the intensity of which is deeply affected by temporal factors.","To tackle these challenges, we propose a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem, which models personalized patterns and multifaceted sequential collaborations in a novel way to boost recommendation performance.","First, PBAT develops a personalized behavior pattern generator in the representation layer, which extracts dynamic and discriminative behavior patterns for sequential learning.","Second, PBAT reforms the self-attention layer with a behavior-aware collaboration extractor, which introduces a fused behavior-aware attention mechanism for incorporating both behavioral and temporal impacts into collaborative transitions.","We conduct experiments on three benchmark datasets and the results demonstrate the effectiveness and interpretability of our framework.","Our implementation code is released at https://github.com/TiliaceaeSU/PBAT."],"url":"http://arxiv.org/abs/2402.14473v1","category":"cs.IR"}
{"created":"2024-02-22 11:59:43","title":"BUGFIX: towards a common language and framework for the AutomaticProgram Repair community","abstract":"Techniques of Automatic Program Repair (APR) have the potential of thoroughly facilitating the task of producing quality software. After a promising start, however, progress in making APR practical has been hindered by the lack of a common framework to support the multiplicity of APR ideas and tools, and of target programming languages and environments.   In this position paper we outline a general framework to enable the APR community to benefit from each other\\'s advances, in particular through a standard language for describing bugs and their fixes. Such a common framework (which is also applicable to work on fault seeding) could be a tremendous benefit to researchers and developers of Interactive Development Environments (IDEs) who are working to make APR an effective part of the practical experience of software developers.","sentences":["Techniques of Automatic Program Repair (APR) have the potential of thoroughly facilitating the task of producing quality software.","After a promising start, however, progress in making APR practical has been hindered by the lack of a common framework to support the multiplicity of APR ideas and tools, and of target programming languages and environments.   ","In this position paper we outline a general framework to enable the APR community to benefit from each other\\'s advances, in particular through a standard language for describing bugs and their fixes.","Such a common framework (which is also applicable to work on fault seeding) could be a tremendous benefit to researchers and developers of Interactive Development Environments (IDEs) who are working to make APR an effective part of the practical experience of software developers."],"url":"http://arxiv.org/abs/2402.14471v1","category":"cs.SE"}
{"created":"2024-02-22 11:57:14","title":"The limit law of certain discrete multivariate distributions","abstract":"Let $X_1,\\,X_2,\\,\\ldots,\\,X_N$, $N\\in\\mathbb{N}$ be independent but not necessarily identically distributed discrete and integer-valued random variables. Assume that $X_1\\geqslant m_1$, $X_2\\geqslant m_2$, $\\ldots$, $X_N\\geqslant m_N$ almost surely, where $m_1,\\,m_2,\\ldots,\\,m_N$ are some integer numbers such that $m_1+m_2+\\ldots+m_N<0$, and $X_k$ is identically distributed as $X_{k+N}$, for all $k\\in\\mathbb{N}$ in the sequence $X_1,\\,X_2,\\,\\ldots$ In this communication, we make use of some of the known results to provide the closed-form expression of the limit multivariate distribution function $\\mathbb{P}(X_1\\leqslant x,\\,X_1+X_2\\leqslant x,\\,\\ldots)$, $x\\in\\mathbb{Z}$ via: (1) inclusion-exclusion principle based product of the roots of $G_{N}(s)=1$, where $G_{N}(s)$ is the probability generating function of $S_N=X_1+X_2+\\ldots+X_N$, (2) the probability mass function of $S_N$, and (3) the expectation $\\mathbb{E}S_N$.","sentences":["Let $X_1,\\,X_2,\\,\\ldots,\\,X_N$, $N\\in\\mathbb{N}$ be independent but not necessarily identically distributed discrete and integer-valued random variables.","Assume that $X_1\\geqslant m_1$, $X_2\\geqslant m_2$, $\\ldots$, $X_N\\geqslant m_N$ almost surely, where $m_1,\\,m_2,\\ldots,\\,m_N$ are some integer numbers such that $m_1+m_2+\\ldots+m_N<0$, and $X_k$ is identically distributed as $X_{k+N}$, for all $k\\in\\mathbb{N}$ in the sequence $X_1,\\,X_2,\\,\\ldots$ In this communication, we make use of some of the known results to provide the closed-form expression of the limit multivariate distribution function $\\mathbb{P}(X_1\\leqslant x,\\,X_1+X_2\\leqslant x,\\,\\ldots)$, $x\\in\\mathbb{Z}$ via: (1) inclusion-exclusion principle based product of the roots of $G_{N}(s)=1$, where $G_{N}(s)$ is the probability generating function of $S_N=X_1+X_2+\\ldots+X_N$, (2) the probability mass function of $S_N$, and (3) the expectation $\\mathbb{E}S_N$."],"url":"http://arxiv.org/abs/2402.14470v1","category":"math.PR"}
{"created":"2024-02-22 11:56:44","title":"Reimagining Anomalies: What If Anomalies Were Normal?","abstract":"Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness. A counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector. The method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore \"what-if scenarios.\" Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors.","sentences":["Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous.","We introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness.","A counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector.","The method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore \"what-if scenarios.\"","Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors."],"url":"http://arxiv.org/abs/2402.14469v1","category":"cs.CV"}
{"created":"2024-02-22 11:40:49","title":"S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR","abstract":"Scene graph generation (SGG) of surgical procedures is crucial in enhancing holistically cognitive intelligence in the operating room (OR). However, previous works have primarily relied on the multi-stage learning that generates semantic scene graphs dependent on intermediate processes with pose estimation and object detection, which may compromise model efficiency and efficacy, also impose extra annotation burden. In this study, we introduce a novel single-stage bimodal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D point clouds for SGG in an end-to-end manner. Concretely, our model embraces a View-Sync Transfusion scheme to encourage multi-view visual information interaction. Concurrently, a Geometry-Visual Cohesion operation is designed to integrate the synergic 2D semantic features into 3D point cloud features. Moreover, based on the augmented feature, we propose a novel relation-sensitive transformer decoder that embeds dynamic entity-pair queries and relational trait priors, which enables the direct prediction of entity-pair relations for graph generation without intermediate steps. Extensive experiments have validated the superior SGG performance and lower computational cost of S^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3% Precision increase and 24.2M reduction in model parameters. We further compared our method with generic single-stage SGG methods with broader metrics for a comprehensive evaluation, with consistently better performance achieved. The code will be made available.","sentences":["Scene graph generation (SGG) of surgical procedures is crucial in enhancing holistically cognitive intelligence in the operating room (OR).","However, previous works have primarily relied on the multi-stage learning that generates semantic scene graphs dependent on intermediate processes with pose estimation and object detection, which may compromise model efficiency and efficacy, also impose extra annotation burden.","In this study, we introduce a novel single-stage bimodal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3D point clouds for SGG in an end-to-end manner.","Concretely, our model embraces a View-Sync Transfusion scheme to encourage multi-view visual information interaction.","Concurrently, a Geometry-Visual Cohesion operation is designed to integrate the synergic 2D semantic features into 3D point cloud features.","Moreover, based on the augmented feature, we propose a novel relation-sensitive transformer decoder that embeds dynamic entity-pair queries and relational trait priors, which enables the direct prediction of entity-pair relations for graph generation without intermediate steps.","Extensive experiments have validated the superior SGG performance and lower computational cost of S^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3% Precision increase and 24.2M reduction in model parameters.","We further compared our method with generic single-stage SGG methods with broader metrics for a comprehensive evaluation, with consistently better performance achieved.","The code will be made available."],"url":"http://arxiv.org/abs/2402.14461v1","category":"cs.CV"}
{"created":"2024-02-22 11:38:43","title":"Reframing the Expected Free Energy: Four Formulations and a Unification","abstract":"Active inference is a leading theory of perception, learning and decision making, which can be applied to neuroscience, robotics, psychology, and machine learning. Active inference is based on the expected free energy, which is mostly justified by the intuitive plausibility of its formulations, e.g., the risk plus ambiguity and information gain / pragmatic value formulations. This paper seek to formalize the problem of deriving these formulations from a single root expected free energy definition, i.e., the unification problem. Then, we study two settings, each one having its own root expected free energy definition. In the first setting, no justification for the expected free energy has been proposed to date, but all the formulations can be recovered from it. However, in this setting, the agent cannot have arbitrary prior preferences over observations. Indeed, only a limited class of prior preferences over observations is compatible with the likelihood mapping of the generative model. In the second setting, a justification of the root expected free energy definition is known, but this setting only accounts for two formulations, i.e., the risk over states plus ambiguity and entropy plus expected energy formulations.","sentences":["Active inference is a leading theory of perception, learning and decision making, which can be applied to neuroscience, robotics, psychology, and machine learning.","Active inference is based on the expected free energy, which is mostly justified by the intuitive plausibility of its formulations, e.g., the risk plus ambiguity and information gain / pragmatic value formulations.","This paper seek to formalize the problem of deriving these formulations from a single root expected free energy definition, i.e., the unification problem.","Then, we study two settings, each one having its own root expected free energy definition.","In the first setting, no justification for the expected free energy has been proposed to date, but all the formulations can be recovered from it.","However, in this setting, the agent cannot have arbitrary prior preferences over observations.","Indeed, only a limited class of prior preferences over observations is compatible with the likelihood mapping of the generative model.","In the second setting, a justification of the root expected free energy definition is known, but this setting only accounts for two formulations, i.e., the risk over states plus ambiguity and entropy plus expected energy formulations."],"url":"http://arxiv.org/abs/2402.14460v1","category":"cs.AI"}
{"created":"2024-02-22 11:31:50","title":"NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes","abstract":"Some of the major limitations identified in the areas of argument mining, argument generation, and natural language argument analysis are related to the complexity of annotating argumentatively rich data, the limited size of these corpora, and the constraints that represent the different languages and domains in which these data is annotated. To address these limitations, in this paper we present the following contributions: (i) an effective methodology for the automatic generation of natural language arguments in different topics and languages, (ii) the largest publicly available corpus of natural language argumentation schemes, and (iii) a set of solid baselines and fine-tuned models for the automatic identification of argumentation schemes.","sentences":["Some of the major limitations identified in the areas of argument mining, argument generation, and natural language argument analysis are related to the complexity of annotating argumentatively rich data, the limited size of these corpora, and the constraints that represent the different languages and domains in which these data is annotated.","To address these limitations, in this paper we present the following contributions: (i) an effective methodology for the automatic generation of natural language arguments in different topics and languages, (ii) the largest publicly available corpus of natural language argumentation schemes, and (iii) a set of solid baselines and fine-tuned models for the automatic identification of argumentation schemes."],"url":"http://arxiv.org/abs/2402.14458v1","category":"cs.CL"}
{"created":"2024-02-22 11:21:54","title":"VLPose: Bridging the Domain Gap in Pose Estimation with Language-Vision Tuning","abstract":"Thanks to advances in deep learning techniques, Human Pose Estimation (HPE) has achieved significant progress in natural scenarios. However, these models perform poorly in artificial scenarios such as painting and sculpture due to the domain gap, constraining the development of virtual reality and augmented reality. With the growth of model size, retraining the whole model on both natural and artificial data is computationally expensive and inefficient. Our research aims to bridge the domain gap between natural and artificial scenarios with efficient tuning strategies. Leveraging the potential of language models, we enhance the adaptability of traditional pose estimation models across diverse scenarios with a novel framework called VLPose. VLPose leverages the synergy between language and vision to extend the generalization and robustness of pose estimation models beyond the traditional domains. Our approach has demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO, respectively, compared to state-of-the-art tuning strategies.","sentences":["Thanks to advances in deep learning techniques, Human Pose Estimation (HPE) has achieved significant progress in natural scenarios.","However, these models perform poorly in artificial scenarios such as painting and sculpture due to the domain gap, constraining the development of virtual reality and augmented reality.","With the growth of model size, retraining the whole model on both natural and artificial data is computationally expensive and inefficient.","Our research aims to bridge the domain gap between natural and artificial scenarios with efficient tuning strategies.","Leveraging the potential of language models, we enhance the adaptability of traditional pose estimation models across diverse scenarios with a novel framework called VLPose.","VLPose leverages the synergy between language and vision to extend the generalization and robustness of pose estimation models beyond the traditional domains.","Our approach has demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO, respectively, compared to state-of-the-art tuning strategies."],"url":"http://arxiv.org/abs/2402.14456v1","category":"cs.CV"}
{"created":"2024-02-22 11:16:34","title":"CCPA: Long-term Person Re-Identification via Contrastive Clothing and Pose Augmentation","abstract":"Long-term Person Re-Identification (LRe-ID) aims at matching an individual across cameras after a long period of time, presenting variations in clothing, pose, and viewpoint. In this work, we propose CCPA: Contrastive Clothing and Pose Augmentation framework for LRe-ID. Beyond appearance, CCPA captures body shape information which is cloth-invariant using a Relation Graph Attention Network. Training a robust LRe-ID model requires a wide range of clothing variations and expensive cloth labeling, which is lacked in current LRe-ID datasets. To address this, we perform clothing and pose transfer across identities to generate images of more clothing variations and of different persons wearing similar clothing. The augmented batch of images serve as inputs to our proposed Fine-grained Contrastive Losses, which not only supervise the Re-ID model to learn discriminative person embeddings under long-term scenarios but also ensure in-distribution data generation. Results on LRe-ID datasets demonstrate the effectiveness of our CCPA framework.","sentences":["Long-term Person Re-Identification (LRe-ID) aims at matching an individual across cameras after a long period of time, presenting variations in clothing, pose, and viewpoint.","In this work, we propose CCPA: Contrastive Clothing and Pose Augmentation framework for LRe-ID.","Beyond appearance, CCPA captures body shape information which is cloth-invariant using a Relation Graph Attention Network.","Training a robust LRe-ID model requires a wide range of clothing variations and expensive cloth labeling, which is lacked in current LRe-ID datasets.","To address this, we perform clothing and pose transfer across identities to generate images of more clothing variations and of different persons wearing similar clothing.","The augmented batch of images serve as inputs to our proposed Fine-grained Contrastive Losses, which not only supervise the Re-ID model to learn discriminative person embeddings under long-term scenarios but also ensure in-distribution data generation.","Results on LRe-ID datasets demonstrate the effectiveness of our CCPA framework."],"url":"http://arxiv.org/abs/2402.14454v1","category":"cs.CV"}
{"created":"2024-02-22 11:16:23","title":"Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?","abstract":"Education that suits the individual learning level is necessary to improve students' understanding. The first step in achieving this purpose by using large language models (LLMs) is to adjust the textual difficulty of the response to students. This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text. To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of question-answering-based conversation. Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response. We also observed that some LLMs can surpass humans in handling text difficulty and the importance of instruction-tuning.","sentences":["Education that suits the individual learning level is necessary to improve students' understanding.","The first step in achieving this purpose by using large language models (LLMs) is to adjust the textual difficulty of the response to students.","This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text.","To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of question-answering-based conversation.","Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response.","We also observed that some LLMs can surpass humans in handling text difficulty and the importance of instruction-tuning."],"url":"http://arxiv.org/abs/2402.14453v1","category":"cs.CL"}
{"created":"2024-02-22 11:14:27","title":"Rough statistical convergence of sequences in a partial metric space","abstract":"In this paper, using the concept of natural density, we have introduced the notion of rough statistical convergence which is an extension of the notion of rough convergence in a partial metric space. We have defined the set of rough statistical limit points of a sequence in a partial metric space and proved that this set is closed and bounded. Finally, we have found out the relationship between the set of statistical cluster points and the set of rough statistical limit points of sequences in a partial metric space.","sentences":["In this paper, using the concept of natural density, we have introduced the notion of rough statistical convergence which is an extension of the notion of rough convergence in a partial metric space.","We have defined the set of rough statistical limit points of a sequence in a partial metric space and proved that this set is closed and bounded.","Finally, we have found out the relationship between the set of statistical cluster points and the set of rough statistical limit points of sequences in a partial metric space."],"url":"http://arxiv.org/abs/2402.14452v1","category":"math.GN"}
{"created":"2024-02-22 11:13:18","title":"Bayesian games with nested information","abstract":"We prove that any Bayesian game (\\'a la Aumann) with a general state space, compact metric action spaces, and nested information admits a Harsanyi $\\varepsilon$-equilibrium for every $\\varepsilon> 0$. When, in addition, the action spaces and the payoffs are discrete, there is a Bayesian $\\varepsilon$-equilibrium. To this end, we develop a new finite approximation of information structures, which has independent interest. We also put forth several open problems, including the existence of a $0$-equilibrium (Harsanyi or Bayesian) in Bayesian games with nested information, the existence of a Harsanyi $\\varepsilon$-equilibrium in multi-stage Bayesian games with nested information, and the canonical structure of the universal belief space when the information structure is nested.","sentences":["We prove that any Bayesian game (\\'a la Aumann) with a general state space, compact metric action spaces, and nested information admits a Harsanyi $\\varepsilon$-equilibrium for every $\\varepsilon> 0$.","When, in addition, the action spaces and the payoffs are discrete, there is a Bayesian $\\varepsilon$-equilibrium.","To this end, we develop a new finite approximation of information structures, which has independent interest.","We also put forth several open problems, including the existence of a $0$-equilibrium (Harsanyi or Bayesian) in Bayesian games with nested information, the existence of a Harsanyi $\\varepsilon$-equilibrium in multi-stage Bayesian games with nested information, and the canonical structure of the universal belief space when the information structure is nested."],"url":"http://arxiv.org/abs/2402.14450v1","category":"math.PR"}
{"created":"2024-02-22 11:12:18","title":"On decentralized computation of the leader's strategy in bi-level games","abstract":"Motivated by the omnipresence of hierarchical structures in many real-world applications, this study delves into the intricate realm of bi-level games, with a specific focus on exploring local Stackelberg equilibria as a solution concept. While existing literature offers various methods tailored to specific game structures featuring one leader and multiple followers, a comprehensive framework providing formal convergence guarantees to a local Stackelberg equilibrium appears to be lacking. Drawing inspiration from sensitivity results for nonlinear programs and guided by the imperative to maintain scalability and preserve agent privacy, we propose a decentralized approach based on the projected gradient descent with the Armijo stepsize rule. The main challenge here lies in assuring the existence and well-posedness of Jacobians that describe the leader's decision's influence on the achieved equilibrium of the followers. By meticulous tracking of the Implicit Function Theorem requirements at each iteration, we establish formal convergence guarantees to a local Stackelberg equilibrium for a broad class of bi-level games. Building on our prior work on quadratic aggregative Stackelberg games, we also introduce a decentralized warm-start procedure based on the consensus alternating direction method of multipliers addressing the previously reported initialization issues. Finally, we provide empirical validation through two case studies in smart mobility, showcasing the effectiveness of our general method in handling general convex constraints, and the effectiveness of its extension in tackling initialization issues.","sentences":["Motivated by the omnipresence of hierarchical structures in many real-world applications, this study delves into the intricate realm of bi-level games, with a specific focus on exploring local Stackelberg equilibria as a solution concept.","While existing literature offers various methods tailored to specific game structures featuring one leader and multiple followers, a comprehensive framework providing formal convergence guarantees to a local Stackelberg equilibrium appears to be lacking.","Drawing inspiration from sensitivity results for nonlinear programs and guided by the imperative to maintain scalability and preserve agent privacy, we propose a decentralized approach based on the projected gradient descent with the Armijo stepsize rule.","The main challenge here lies in assuring the existence and well-posedness of Jacobians that describe the leader's decision's influence on the achieved equilibrium of the followers.","By meticulous tracking of the Implicit Function Theorem requirements at each iteration, we establish formal convergence guarantees to a local Stackelberg equilibrium for a broad class of bi-level games.","Building on our prior work on quadratic aggregative Stackelberg games, we also introduce a decentralized warm-start procedure based on the consensus alternating direction method of multipliers addressing the previously reported initialization issues.","Finally, we provide empirical validation through two case studies in smart mobility, showcasing the effectiveness of our general method in handling general convex constraints, and the effectiveness of its extension in tackling initialization issues."],"url":"http://arxiv.org/abs/2402.14449v1","category":"eess.SY"}
{"created":"2024-02-22 10:42:29","title":"Leptogenesis consequences of TM1 mixing and \u03bc-\u03c4reflection symmetry in minimal seesaw model with pseudo-Dirac right-handed neutrinos","abstract":"In this paper we have studied the realizations of the popular TM1 neutrino mixing and neutrino \\mu-\\tau reflection symmetry (which are well motivated from the neutrino oscillation data and lead to interesting phenomenological consequences) in the minimal seesaw model with a Dirac pair of right-handed neutrinos (with equal masses but opposite parities), and their consequences for leptogenesis. In order to realize the low-scale resonant leptogenesis scenario, we have considered two possible ways of generating the tiny mass splitting between the two right-handed neutrinos: one way is to modify their Majorana mass matrix to a form as shown in Eq.(25); the other way is to consider the renormalization-group corrections for their masses. For the \\mu-\\tau reflection symmetry, in order for leptogenesis to work, we have further considered the flavor-dependent conversion efficiencies from the lepton asymmetry to the baryon asymmetry during the sphaleron processes, and its breaking via the renormalization-group corrections.","sentences":["In this paper we have studied the realizations of the popular TM1 neutrino mixing and neutrino \\mu-\\tau reflection symmetry (which are well motivated from the neutrino oscillation data and lead to interesting phenomenological consequences) in the minimal seesaw model with a Dirac pair of right-handed neutrinos (with equal masses but opposite parities), and their consequences for leptogenesis.","In order to realize the low-scale resonant leptogenesis scenario, we have considered two possible ways of generating the tiny mass splitting between the two right-handed neutrinos: one way is to modify their Majorana mass matrix to a form as shown in Eq.(25); the other way is to consider the renormalization-group corrections for their masses.","For the \\mu-\\tau reflection symmetry, in order for leptogenesis to work, we have further considered the flavor-dependent conversion efficiencies from the lepton asymmetry to the baryon asymmetry during the sphaleron processes, and its breaking via the renormalization-group corrections."],"url":"http://arxiv.org/abs/2402.14441v1","category":"hep-ph"}
{"created":"2024-02-22 10:36:29","title":"Efficiency-improved doubly robust estimation with non-confounding predictive covariates","abstract":"In observational studies, covariates with substantial missing data are often omitted, despite their strong predictive capabilities. These excluded covariates are generally believed not to simultaneously affect both treatment and outcome, indicating that they are not genuine confounders and do not impact the identification of the average treatment effect (ATE). In this paper, we introduce an alternative doubly robust (DR) estimator that fully leverages non-confounding predictive covariates to enhance efficiency, while also allowing missing values in such covariates. Beyond the double robustness property, our proposed estimator is designed to be more efficient than the standard DR estimator. Specifically, when the propensity score model is correctly specified, it achieves the smallest asymptotic variance among the class of DR estimators, and brings additional efficiency gains by further integrating predictive covariates. Simulation studies demonstrate the notable performance of the proposed estimator over current popular methods. An illustrative example is provided to assess the effectiveness of right heart catheterization (RHC) for critically ill patients.","sentences":["In observational studies, covariates with substantial missing data are often omitted, despite their strong predictive capabilities.","These excluded covariates are generally believed not to simultaneously affect both treatment and outcome, indicating that they are not genuine confounders and do not impact the identification of the average treatment effect (ATE).","In this paper, we introduce an alternative doubly robust (DR) estimator that fully leverages non-confounding predictive covariates to enhance efficiency, while also allowing missing values in such covariates.","Beyond the double robustness property, our proposed estimator is designed to be more efficient than the standard DR estimator.","Specifically, when the propensity score model is correctly specified, it achieves the smallest asymptotic variance among the class of DR estimators, and brings additional efficiency gains by further integrating predictive covariates.","Simulation studies demonstrate the notable performance of the proposed estimator over current popular methods.","An illustrative example is provided to assess the effectiveness of right heart catheterization (RHC) for critically ill patients."],"url":"http://arxiv.org/abs/2402.14438v1","category":"stat.ME"}
{"created":"2024-02-22 10:29:53","title":"Random time horizon BSDEs with stochastic monotonicity and general growth generators and related PDEs","abstract":"This paper is devoted to solving a multidimensional backward stochastic differential equation (BSDE) with a general random terminal time, which may take values in [0,+infinity]. The generator g satisfies a stochastic monotonicity condition in the first unknown variable y and a stochastic Lipschitz continuity condition in the second unknown variable z, and it can have a more general growth with respect to y than the classical one stated in (H5) of Briand et al. [2003]. Without imposing any restriction of finite moment on the stochastic coefficients, we establish a general existence and uniqueness result for the adapted solution of the previous BSDE in a proper weighted L2-space. This result is proved via some innovative ideas and delicate analytical techniques, and it unifies and strengthens many existing works on BSDEs with stochastic monotonicity generators, BSDEs with stochastic Lipschitz generators, and BSDEs with deterministic Lipschitz/monotonicity generators. Then, a continuous dependence property and a stability theorem for the weighted L2-solutions are given. We also derive the nonlinear Feynman-Kac formula for both parabolic and elliptic PDEs in this context.","sentences":["This paper is devoted to solving a multidimensional backward stochastic differential equation (BSDE) with a general random terminal time, which may take values in [0,+infinity].","The generator g satisfies a stochastic monotonicity condition in the first unknown variable y and a stochastic Lipschitz continuity condition in the second unknown variable z, and it can have a more general growth with respect to y than the classical one stated in (H5) of Briand et al.","[2003].","Without imposing any restriction of finite moment on the stochastic coefficients, we establish a general existence and uniqueness result for the adapted solution of the previous BSDE in a proper weighted L2-space.","This result is proved via some innovative ideas and delicate analytical techniques, and it unifies and strengthens many existing works on BSDEs with stochastic monotonicity generators, BSDEs with stochastic Lipschitz generators, and BSDEs with deterministic Lipschitz/monotonicity generators.","Then, a continuous dependence property and a stability theorem for the weighted L2-solutions are given.","We also derive the nonlinear Feynman-Kac formula for both parabolic and elliptic PDEs in this context."],"url":"http://arxiv.org/abs/2402.14435v1","category":"math.PR"}
{"created":"2024-02-22 10:25:14","title":"A Language Model's Guide Through Latent Space","abstract":"Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive tuning to work, or even experience confusion. Moreover, we find that probes with optimal detection accuracies do not necessarily make for the optimal guides, contradicting previous observations for truthfulness. Our work warrants a deeper investigation into the interplay between detectability, guidability, and the nature of the concept, and we hope that our rich experimental test-bed for guidance research inspires stronger follow-up approaches.","sentences":["Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time.","While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings.","To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model.","Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive tuning to work, or even experience confusion.","Moreover, we find that probes with optimal detection accuracies do not necessarily make for the optimal guides, contradicting previous observations for truthfulness.","Our work warrants a deeper investigation into the interplay between detectability, guidability, and the nature of the concept, and we hope that our rich experimental test-bed for guidance research inspires stronger follow-up approaches."],"url":"http://arxiv.org/abs/2402.14433v1","category":"cs.CL"}
{"created":"2024-02-22 10:17:57","title":"KoCoSa: Korean Context-aware Sarcasm Detection Dataset","abstract":"Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on the dataset show that our baseline system outperforms strong baselines like large language models, such as GPT-3.5, in the Korean sarcasm detection task. We show that the sarcasm detection task relies deeply on the existence of sufficient context. We will release the dataset at https://anonymous.4open.science/r/KoCoSa-2372.","sentences":["Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea.","It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history).","In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response.","To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task.","We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset.","Experimental results on the dataset show that our baseline system outperforms strong baselines like large language models, such as GPT-3.5, in the Korean sarcasm detection task.","We show that the sarcasm detection task relies deeply on the existence of sufficient context.","We will release the dataset at https://anonymous.4open.science/r/KoCoSa-2372."],"url":"http://arxiv.org/abs/2402.14428v1","category":"cs.CL"}
{"created":"2024-02-22 10:14:59","title":"Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR","abstract":"In human activity recognition (HAR), the availability of substantial ground truth is necessary for training efficient models. However, acquiring ground pressure data through physical sensors itself can be cost-prohibitive, time-consuming. To address this critical need, we introduce Text-to-Pressure (T2P), a framework designed to generate extensive ground pressure sequences from textual descriptions of human activities using deep learning techniques. We show that the combination of vector quantization of sensor data along with simple text conditioned auto regressive strategy allows us to obtain high-quality generated pressure sequences from textual descriptions with the help of discrete latent correlation between text and pressure maps. We achieved comparable performance on the consistency between text and generated motion with an R squared value of 0.722, Masked R squared value of 0.892, and FID score of 1.83. Additionally, we trained a HAR model with the the synthesized data and evaluated it on pressure dynamics collected by a real pressure sensor which is on par with a model trained on only real data. Combining both real and synthesized training data increases the overall macro F1 score by 5.9 percent.","sentences":["In human activity recognition (HAR), the availability of substantial ground truth is necessary for training efficient models.","However, acquiring ground pressure data through physical sensors itself can be cost-prohibitive, time-consuming.","To address this critical need, we introduce Text-to-Pressure (T2P), a framework designed to generate extensive ground pressure sequences from textual descriptions of human activities using deep learning techniques.","We show that the combination of vector quantization of sensor data along with simple text conditioned auto regressive strategy allows us to obtain high-quality generated pressure sequences from textual descriptions with the help of discrete latent correlation between text and pressure maps.","We achieved comparable performance on the consistency between text and generated motion with an R squared value of 0.722, Masked R squared value of 0.892, and FID score of 1.83.","Additionally, we trained a HAR model with the the synthesized data and evaluated it on pressure dynamics collected by a real pressure sensor which is on par with a model trained on only real data.","Combining both real and synthesized training data increases the overall macro F1 score by 5.9 percent."],"url":"http://arxiv.org/abs/2402.14427v1","category":"cs.LG"}
{"created":"2024-02-22 10:13:27","title":"Square-free Word-representation of Word-representable Graphs","abstract":"A graph $G = (V, E)$ is word-representable, if there exists a word w over the alphabet V such that for letters ${x, y} \\in V$ , $x$ and $y$ alternate in $w$ if and only if $xy \\in E$. In this paper, we prove that any non-empty word-representable graph can be represented by a word containing no non-trivial squares. This result provides a positive answer to the open problem present in the book Words and graphs written by Sergey Kitaev, and Vadim Lozin. Also, we prove that for a word-representable graph $G$, if the representation number of $G$ is $k$, then every $k$-uniform word representing the graph $G$ is also square-free. Moreover, we prove that every minimal-length word representing a graph is square-free. Then, we count the number of possible square-free word-representations of a complete graph. At last, using the infinite square-free string generated from the Thue-Morse sequence, we prove that infinitely many square-free words represent a non-complete connected word-representable graph.","sentences":["A graph $G = (V, E)$ is word-representable, if there exists a word w over the alphabet V such that for letters ${x, y} \\in V$ , $x$ and $y$ alternate in $w$ if and only if $xy \\in E$.","In this paper, we prove that any non-empty word-representable graph can be represented by a word containing no non-trivial squares.","This result provides a positive answer to the open problem present in the book Words and graphs written by Sergey Kitaev, and Vadim Lozin.","Also, we prove that for a word-representable graph $G$, if the representation number of $G$ is $k$, then every $k$-uniform word representing the graph $G$ is also square-free.","Moreover, we prove that every minimal-length word representing a graph is square-free.","Then, we count the number of possible square-free word-representations of a complete graph.","At last, using the infinite square-free string generated from the Thue-Morse sequence, we prove that infinitely many square-free words represent a non-complete connected word-representable graph."],"url":"http://arxiv.org/abs/2402.14426v1","category":"math.CO"}
{"created":"2024-02-22 10:12:16","title":"Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph","abstract":"Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p<0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal knowledge graphs can revolutionize automated discovery in psychology, extracting novel insights from the extensive literature. This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research.","sentences":["Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology.","We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs.","This analysis produced a specialized causal graph for psychology.","Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM.","Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p<0.001, respectively).","This alignment was further corroborated using deep semantic analysis.","Our results show that combining LLM with machine learning techniques such as causal knowledge graphs can revolutionize automated discovery in psychology, extracting novel insights from the extensive literature.","This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research."],"url":"http://arxiv.org/abs/2402.14424v1","category":"cs.AI"}
{"created":"2024-02-22 10:11:51","title":"The Universe as a Learning System","abstract":"At its microscopic level, the universe follows the laws of quantum mechanics. Focusing on the quantum trajectories of particles as followed from the hydrodynamical formulation of quantum mechanics, we propose that under general requirements, quantum systems follow a disrupted version of the gradient descent model, a basic machine learning algorithm, where the learning is distorted due to the self-organizing process of the quantum system. Such a learning process is possible only when we assume dissipation, i.e., that the quantum system is open. The learning parameter is the time increment of the process over the mass of the quantum particle, and a friction parameter determines the nonlinearity of the quantum system. We then provide an empirical demonstration of the proposed model.","sentences":["At its microscopic level, the universe follows the laws of quantum mechanics.","Focusing on the quantum trajectories of particles as followed from the hydrodynamical formulation of quantum mechanics, we propose that under general requirements, quantum systems follow a disrupted version of the gradient descent model, a basic machine learning algorithm, where the learning is distorted due to the self-organizing process of the quantum system.","Such a learning process is possible only when we assume dissipation, i.e., that the quantum system is open.","The learning parameter is the time increment of the process over the mass of the quantum particle, and a friction parameter determines the nonlinearity of the quantum system.","We then provide an empirical demonstration of the proposed model."],"url":"http://arxiv.org/abs/2402.14423v1","category":"quant-ph"}
{"created":"2024-02-22 10:04:17","title":"Uncertainty-Aware Evaluation for Vision-Language Models","abstract":"Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs.   Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities.   Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.","sentences":["Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks.","Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs.","Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs.   ","Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task.","We examine models on 5 datasets that evaluate various vision-language capabilities.   ","Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy.","Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs.","Our empirical findings also reveal a correlation between model uncertainty and its language model part."],"url":"http://arxiv.org/abs/2402.14418v1","category":"cs.CV"}
{"created":"2024-02-22 10:03:01","title":"TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via Direct Taylor-based Grid Optimization","abstract":"Coordinate-based neural implicit representation or implicit fields have been widely studied for 3D geometry representation or novel view synthesis. Recently, a series of efforts have been devoted to accelerating the speed and improving the quality of the coordinate-based implicit field learning. Instead of learning heavy MLPs to predict the neural implicit values for the query coordinates, neural voxels or grids combined with shallow MLPs have been proposed to achieve high-quality implicit field learning with reduced optimization time. On the other hand, lightweight field representations such as linear grid have been proposed to further improve the learning speed. In this paper, we aim for both fast and high-quality implicit field learning, and propose TaylorGrid, a novel implicit field representation which can be efficiently computed via direct Taylor expansion optimization on 2D or 3D grids. As a general representation, TaylorGrid can be adapted to different implicit fields learning tasks such as SDF learning or NeRF. From extensive quantitative and qualitative comparisons, TaylorGrid achieves a balance between the linear grid and neural voxels, showing its superiority in fast and high-quality implicit field learning.","sentences":["Coordinate-based neural implicit representation or implicit fields have been widely studied for 3D geometry representation or novel view synthesis.","Recently, a series of efforts have been devoted to accelerating the speed and improving the quality of the coordinate-based implicit field learning.","Instead of learning heavy MLPs to predict the neural implicit values for the query coordinates, neural voxels or grids combined with shallow MLPs have been proposed to achieve high-quality implicit field learning with reduced optimization time.","On the other hand, lightweight field representations such as linear grid have been proposed to further improve the learning speed.","In this paper, we aim for both fast and high-quality implicit field learning, and propose TaylorGrid, a novel implicit field representation which can be efficiently computed via direct Taylor expansion optimization on 2D or 3D grids.","As a general representation, TaylorGrid can be adapted to different implicit fields learning tasks such as SDF learning or NeRF.","From extensive quantitative and qualitative comparisons, TaylorGrid achieves a balance between the linear grid and neural voxels, showing its superiority in fast and high-quality implicit field learning."],"url":"http://arxiv.org/abs/2402.14415v1","category":"cs.CV"}
{"created":"2024-02-22 09:57:07","title":"Atomic clock interferometry using optical tweezers","abstract":"Clock interferometry refers to the coherent splitting of a clock into two different paths and recombining in a way that reveals the proper time difference between them. In contrast to comparing two separate clocks, this type of measurement can test quantum gravity theories. Atomic clocks are currently the most accurate time keeping devices. Here we propose using optical tweezers to implement clock interferometry. Our proposed clock interferometer employs an alkaline-earth-like atom held in an optical trap at the magic wavelength. Through a combination of adiabatic, tweezer-based, splitting and recombining schemes and a modified Ramsey sequence on the clock states, we achieve a linear sensitivity to the gravitational time dilation. Moreover, the measurement of the time dilation is insensitive to relative fluctuations in the intensity of the tweezer beams. We analyze the tweezer clock interferometer and show that it is feasible with current technological capabilities. The proposed interferometer could test the effect of gravitational redshift on quantum coherence, and implement the quantum twin paradox.","sentences":["Clock interferometry refers to the coherent splitting of a clock into two different paths and recombining in a way that reveals the proper time difference between them.","In contrast to comparing two separate clocks, this type of measurement can test quantum gravity theories.","Atomic clocks are currently the most accurate time keeping devices.","Here we propose using optical tweezers to implement clock interferometry.","Our proposed clock interferometer employs an alkaline-earth-like atom held in an optical trap at the magic wavelength.","Through a combination of adiabatic, tweezer-based, splitting and recombining schemes and a modified Ramsey sequence on the clock states, we achieve a linear sensitivity to the gravitational time dilation.","Moreover, the measurement of the time dilation is insensitive to relative fluctuations in the intensity of the tweezer beams.","We analyze the tweezer clock interferometer and show that it is feasible with current technological capabilities.","The proposed interferometer could test the effect of gravitational redshift on quantum coherence, and implement the quantum twin paradox."],"url":"http://arxiv.org/abs/2402.14412v1","category":"quant-ph"}
{"created":"2024-02-22 09:54:41","title":"Human-machine social systems","abstract":"From fake accounts on social media and generative-AI bots such as ChatGPT to high-frequency trading algorithms on financial markets and self-driving vehicles on the streets, robots, bots, and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions, and transportation arteries. Networks of multiple interdependent and interacting humans and autonomous machines constitute complex adaptive social systems where the collective outcomes cannot be simply deduced from either human or machine behavior alone. Under this paradigm, we review recent experimental, theoretical, and observational research from across a range of disciplines - robotics, human-computer interaction, web science, complexity science, computational social science, finance, economics, political science, social psychology, and sociology. We identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion, and collective decision-making, and contextualize them in four prominent existing human-machine communities: high-frequency trading markets, the social media platform formerly known as Twitter, the open-collaboration encyclopedia Wikipedia, and the news aggregation and discussion community Reddit. We conclude with suggestions for the research, design, and governance of human-machine social systems, which are necessary to reduce misinformation, prevent financial crashes, improve road safety, overcome labor market disruptions, and enable a better human future.","sentences":["From fake accounts on social media and generative-AI bots such as ChatGPT to high-frequency trading algorithms on financial markets and self-driving vehicles on the streets, robots, bots, and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions, and transportation arteries.","Networks of multiple interdependent and interacting humans and autonomous machines constitute complex adaptive social systems where the collective outcomes cannot be simply deduced from either human or machine behavior alone.","Under this paradigm, we review recent experimental, theoretical, and observational research from across a range of disciplines - robotics, human-computer interaction, web science, complexity science, computational social science, finance, economics, political science, social psychology, and sociology.","We identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion, and collective decision-making, and contextualize them in four prominent existing human-machine communities: high-frequency trading markets, the social media platform formerly known as Twitter, the open-collaboration encyclopedia Wikipedia, and the news aggregation and discussion community Reddit.","We conclude with suggestions for the research, design, and governance of human-machine social systems, which are necessary to reduce misinformation, prevent financial crashes, improve road safety, overcome labor market disruptions, and enable a better human future."],"url":"http://arxiv.org/abs/2402.14410v1","category":"cs.SI"}
{"created":"2024-02-22 09:51:08","title":"Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models","abstract":"Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability bias towards common knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence: We reveal that RALMs follow the principle of majority rule, leaning towards placing trust in evidence that appears more frequently. Moreover, we find that RALMs exhibit confirmation bias, and are more willing to choose evidence that is consistent with their internal memory. To solve the challenge of knowledge conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding (CD2) to better calibrate the model's confidence. Experimental results demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.","sentences":["Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources.","However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources.","Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability.","In this paper, we focus on exploring and resolving knowledge conflicts in RALMs.","First, we present an evaluation framework for assessing knowledge conflicts across various dimensions.","Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided.","Besides, RALMs exhibit an availability bias towards common knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence: We reveal that RALMs follow the principle of majority rule, leaning towards placing trust in evidence that appears more frequently.","Moreover, we find that RALMs exhibit confirmation bias, and are more willing to choose evidence that is consistent with their internal memory.","To solve the challenge of knowledge conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding (CD2) to better calibrate the model's confidence.","Experimental results demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs."],"url":"http://arxiv.org/abs/2402.14409v1","category":"cs.CL"}
{"created":"2024-02-22 09:48:47","title":"Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning","abstract":"Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior generalization ability. Our project website is available at https://video-diff.github.io/.","sentences":["Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets.","In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world.","Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations.","In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos.","We start by compressing both human and robot videos into unified video tokens.","In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space.","In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data.","Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior generalization ability.","Our project website is available at https://video-diff.github.io/."],"url":"http://arxiv.org/abs/2402.14407v1","category":"cs.LG"}
{"created":"2024-02-22 09:47:20","title":"On zero-cycles of varieties over Laurent fields","abstract":"We generalize a recent result of Pavic--Schreieder regarding the surjectivity of the obstruction morphism defined in [PS23]. As a consequence of this result, we show that geometrically (retract) rational varieties over a Laurent field of characteristic 0, which admit a strictly semi-stable model, have trivial Chow group of zero-cycles. Our key new ingredient comes from toric geometry.","sentences":["We generalize a recent result of Pavic--Schreieder regarding the surjectivity of the obstruction morphism defined in [PS23].","As a consequence of this result, we show that geometrically (retract) rational varieties over a Laurent field of characteristic 0, which admit a strictly semi-stable model, have trivial Chow group of zero-cycles.","Our key new ingredient comes from toric geometry."],"url":"http://arxiv.org/abs/2402.14406v1","category":"math.AG"}
{"created":"2024-02-22 09:46:41","title":"Genericity of homeomorphisms with full mean Hausdorff dimension","abstract":"It is well known that the presence of horseshoes leads to positive entropy. If our goal is to construct a continuous map with infinite entropy, we can consider an infinite sequence of horseshoes, ensuring an unbounded number of legs.   Estimating the exact values of both the metric mean dimension and mean Hausdorff dimension for a homeomorphism is a challenging task. We need to establish a precise relationship between the sizes of the horseshoes and the number of appropriated legs to control both quantities.   Let $N$ be an $n$-dimensional compact Riemannian manifold, where $n \\geq 2$, and $\\alpha \\in [0, n]$. In this paper, we construct a homeomorphism $\\phi: N \\rightarrow N$ with mean Hausdorff dimension equal to $\\alpha$. Furthermore, we prove that the set of homeomorphisms on $N$ with both lower and upper mean Hausdorff dimensions equal to $\\alpha$ is dense in $\\text{Hom}(N)$. Additionally, we establish that the set of homeomorphisms with upper mean Hausdorff dimension equal to $n$ contains a residual subset of $\\text{Hom}(N).$","sentences":["It is well known that the presence of horseshoes leads to positive entropy.","If our goal is to construct a continuous map with infinite entropy, we can consider an infinite sequence of horseshoes, ensuring an unbounded number of legs.   ","Estimating the exact values of both the metric mean dimension and mean Hausdorff dimension for a homeomorphism is a challenging task.","We need to establish a precise relationship between the sizes of the horseshoes and the number of appropriated legs to control both quantities.   ","Let $N$ be an $n$-dimensional compact Riemannian manifold, where $n \\geq 2$, and $\\alpha \\in [0, n]$. In this paper, we construct a homeomorphism $\\phi: N \\rightarrow N$ with mean Hausdorff dimension equal to $\\alpha$. Furthermore, we prove that the set of homeomorphisms on $N$ with both lower and upper mean Hausdorff dimensions equal to $\\alpha$ is dense in $\\text{Hom}(N)$. Additionally, we establish that the set of homeomorphisms with upper mean Hausdorff dimension equal to $n$ contains a residual subset of $\\text{Hom}(N).$"],"url":"http://arxiv.org/abs/2402.14405v1","category":"math.DS"}
{"created":"2024-02-22 09:45:26","title":"On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe","abstract":"Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commonsense reasoning problems.","sentences":["Probing and enhancing large language models' reasoning capacity remains a crucial open question.","Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference.","We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description.","Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features.","Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models.","Explorative analyses suggest that prompting LLMs with description$\\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commonsense reasoning problems."],"url":"http://arxiv.org/abs/2402.14404v1","category":"cs.CL"}
{"created":"2024-02-22 09:39:46","title":"Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment","abstract":"Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual information. Secondly, two visual evaluation branches are designed to comprehensively analyze the obtained high-level feature information. These include the visual compensation guidance branch, grounded in the transformer architecture and noise embedding strategy, and the visual difference analysis branch, built on the ResNet architecture and the residual transposed attention block. Extensive experiments are conducted on seven public NR-IQA datasets, and the results demonstrate that the proposed model outperforms SOTA methods for NR-IQA.","sentences":["Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge.","As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features.","In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA.","Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual information.","Secondly, two visual evaluation branches are designed to comprehensively analyze the obtained high-level feature information.","These include the visual compensation guidance branch, grounded in the transformer architecture and noise embedding strategy, and the visual difference analysis branch, built on the ResNet architecture and the residual transposed attention block.","Extensive experiments are conducted on seven public NR-IQA datasets, and the results demonstrate that the proposed model outperforms SOTA methods for NR-IQA."],"url":"http://arxiv.org/abs/2402.14401v1","category":"cs.CV"}
{"created":"2024-02-22 09:32:34","title":"Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation","abstract":"Live streaming recommender system is specifically designed to recommend real-time live streaming of interest to users. Due to the dynamic changes of live content, improving the timeliness of the live streaming recommender system is a critical problem. Intuitively, the timeliness of the data determines the upper bound of the timeliness that models can learn. However, none of the previous works addresses the timeliness problem of the live streaming recommender system from the perspective of data stream design. Employing the conventional fixed window data stream paradigm introduces a trade-off dilemma between labeling accuracy and timeliness. In this paper, we propose a new data stream design paradigm, dubbed Sliver, that addresses the timeliness and accuracy problem of labels by reducing the window size and implementing a sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco strategy reducing the latency between request and impression to improve the timeliness of the recommendation service and features by periodically requesting the recommendation service. To demonstrate the effectiveness of our approach, we conduct offline experiments on a multi-task live streaming dataset with labeling timestamps collected from the Kuaishou live streaming platform. Experimental results demonstrate that Sliver outperforms two fixed-window data streams with varying window sizes across all targets in four typical multi-task recommendation models. Furthermore, we deployed Sliver on the Kuaishou live streaming platform. Results of the online A/B test show a significant improvement in click-through rate (CTR), and new follow number (NFN), further validating the effectiveness of Sliver.","sentences":["Live streaming recommender system is specifically designed to recommend real-time live streaming of interest to users.","Due to the dynamic changes of live content, improving the timeliness of the live streaming recommender system is a critical problem.","Intuitively, the timeliness of the data determines the upper bound of the timeliness that models can learn.","However, none of the previous works addresses the timeliness problem of the live streaming recommender system from the perspective of data stream design.","Employing the conventional fixed window data stream paradigm introduces a trade-off dilemma between labeling accuracy and timeliness.","In this paper, we propose a new data stream design paradigm, dubbed Sliver, that addresses the timeliness and accuracy problem of labels by reducing the window size and implementing a sliding window correspondingly.","Meanwhile, we propose a time-sensitive re-reco strategy reducing the latency between request and impression to improve the timeliness of the recommendation service and features by periodically requesting the recommendation service.","To demonstrate the effectiveness of our approach, we conduct offline experiments on a multi-task live streaming dataset with labeling timestamps collected from the Kuaishou live streaming platform.","Experimental results demonstrate that Sliver outperforms two fixed-window data streams with varying window sizes across all targets in four typical multi-task recommendation models.","Furthermore, we deployed Sliver on the Kuaishou live streaming platform.","Results of the online A/B test show a significant improvement in click-through rate (CTR), and new follow number (NFN), further validating the effectiveness of Sliver."],"url":"http://arxiv.org/abs/2402.14399v1","category":"cs.IR"}
{"created":"2024-02-22 09:28:47","title":"Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing","abstract":"GAN-based image attribute editing firstly leverages GAN Inversion to project real images into the latent space of GAN and then manipulates corresponding latent codes. Recent inversion methods mainly utilize additional high-bit features to improve image details preservation, as low-bit codes cannot faithfully reconstruct source images, leading to the loss of details. However, during editing, existing works fail to accurately complement the lost details and suffer from poor editability. The main reason is they inject all the lost details indiscriminately at one time, which inherently induces the position and quantity of details to overfit source images, resulting in inconsistent content and artifacts in edited images. This work argues that details should be gradually injected into both the reconstruction and editing process in a multi-stage coarse-to-fine manner for better detail preservation and high editability. Therefore, a novel dual-stream framework is proposed to accurately complement details at each stage. The Reconstruction Stream is employed to embed coarse-to-fine lost details into residual features and then adaptively add them to the GAN generator. In the Editing Stream, residual features are accurately aligned by our Selective Attention mechanism and then injected into the editing process in a multi-stage manner. Extensive experiments have shown the superiority of our framework in both reconstruction accuracy and editing quality compared with existing methods.","sentences":["GAN-based image attribute editing firstly leverages GAN Inversion to project real images into the latent space of GAN and then manipulates corresponding latent codes.","Recent inversion methods mainly utilize additional high-bit features to improve image details preservation, as low-bit codes cannot faithfully reconstruct source images, leading to the loss of details.","However, during editing, existing works fail to accurately complement the lost details and suffer from poor editability.","The main reason is they inject all the lost details indiscriminately at one time, which inherently induces the position and quantity of details to overfit source images, resulting in inconsistent content and artifacts in edited images.","This work argues that details should be gradually injected into both the reconstruction and editing process in a multi-stage coarse-to-fine manner for better detail preservation and high editability.","Therefore, a novel dual-stream framework is proposed to accurately complement details at each stage.","The Reconstruction Stream is employed to embed coarse-to-fine lost details into residual features and then adaptively add them to the GAN generator.","In the Editing Stream, residual features are accurately aligned by our Selective Attention mechanism and then injected into the editing process in a multi-stage manner.","Extensive experiments have shown the superiority of our framework in both reconstruction accuracy and editing quality compared with existing methods."],"url":"http://arxiv.org/abs/2402.14398v1","category":"cs.CV"}
{"created":"2024-02-22 09:10:28","title":"Semantic Image Synthesis with Unconditional Generator","abstract":"Semantic image synthesis (SIS) aims to generate realistic images that match given semantic masks. Despite recent advances allowing high-quality results and precise spatial control, they require a massive semantic segmentation dataset for training the models. Instead, we propose to employ a pre-trained unconditional generator and rearrange its feature maps according to proxy masks. The proxy masks are prepared from the feature maps of random samples in the generator by simple clustering. The feature rearranger learns to rearrange original feature maps to match the shape of the proxy masks that are either from the original sample itself or from random samples. Then we introduce a semantic mapper that produces the proxy masks from various input conditions including semantic masks. Our method is versatile across various applications such as free-form spatial editing of real images, sketch-to-photo, and even scribble-to-photo. Experiments validate advantages of our method on a range of datasets: human faces, animal faces, and buildings.","sentences":["Semantic image synthesis (SIS) aims to generate realistic images that match given semantic masks.","Despite recent advances allowing high-quality results and precise spatial control, they require a massive semantic segmentation dataset for training the models.","Instead, we propose to employ a pre-trained unconditional generator and rearrange its feature maps according to proxy masks.","The proxy masks are prepared from the feature maps of random samples in the generator by simple clustering.","The feature rearranger learns to rearrange original feature maps to match the shape of the proxy masks that are either from the original sample itself or from random samples.","Then we introduce a semantic mapper that produces the proxy masks from various input conditions including semantic masks.","Our method is versatile across various applications such as free-form spatial editing of real images, sketch-to-photo, and even scribble-to-photo.","Experiments validate advantages of our method on a range of datasets: human faces, animal faces, and buildings."],"url":"http://arxiv.org/abs/2402.14395v1","category":"cs.CV"}
{"created":"2024-02-22 09:08:43","title":"Probing excitons with time-resolved momentum microscopy","abstract":"Excitons -- two-particle correlated electron-hole pairs -- are the dominant low-energy optical excitation in the broad class of semiconductor materials, which range from classical silicon to perovskites, and from two-dimensional to organic materials. Recently, the study of excitons has been brought on a new level of detail by the application of photoemission momentum microscopy -- a technique that has dramatically extended the experimental capabilities of time- and angle-resolved photoemission spectroscopy (trARPES). Here, we review how the energy- and momentum-resolved photoelectron detection scheme enables direct access to the energy landscape of bright and dark excitons, and, more generally, to the momentum-coordinate of the exciton that is fundamental to its wavefunction. Focusing on two-dimensional materials and organic semiconductors as two tuneable platforms for exciton physics, we first discuss the typical photoemission fingerprint of excitons in momentum microscopy and highlight that is is possible to obtain information not only on the electron- but also hole-component of the former exciton. Second, we focus on the recent application of photoemission orbital tomography to such excitons, and discuss how this provides a unique access to the real-space properties of the exciton wavefunction. Throughout the review, we detail how studies performed on two-dimensional transition metal dichalcogenides and organic semiconductors lead to very similar conclusions, and, in this manner, highlight the strength of time-resolved momentum microscopy for the study of optical excitations in semiconductors.","sentences":["Excitons -- two-particle correlated electron-hole pairs -- are the dominant low-energy optical excitation in the broad class of semiconductor materials, which range from classical silicon to perovskites, and from two-dimensional to organic materials.","Recently, the study of excitons has been brought on a new level of detail by the application of photoemission momentum microscopy -- a technique that has dramatically extended the experimental capabilities of time- and angle-resolved photoemission spectroscopy (trARPES).","Here, we review how the energy- and momentum-resolved photoelectron detection scheme enables direct access to the energy landscape of bright and dark excitons, and, more generally, to the momentum-coordinate of the exciton that is fundamental to its wavefunction.","Focusing on two-dimensional materials and organic semiconductors as two tuneable platforms for exciton physics, we first discuss the typical photoemission fingerprint of excitons in momentum microscopy and highlight that is is possible to obtain information not only on the electron- but also hole-component of the former exciton.","Second, we focus on the recent application of photoemission orbital tomography to such excitons, and discuss how this provides a unique access to the real-space properties of the exciton wavefunction.","Throughout the review, we detail how studies performed on two-dimensional transition metal dichalcogenides and organic semiconductors lead to very similar conclusions, and, in this manner, highlight the strength of time-resolved momentum microscopy for the study of optical excitations in semiconductors."],"url":"http://arxiv.org/abs/2402.14394v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 09:08:36","title":"Graph Parsing Networks","abstract":"Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering. Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph. In this work, inspired by bottom-up grammar induction, we propose an efficient graph parsing algorithm to infer the pooling structure, which then drives graph pooling. The resulting Graph Parsing Network (GPN) adaptively learns personalized pooling structure for each individual graph. GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node information intact. Experimental results on standard benchmarks demonstrate that GPN outperforms state-of-the-art graph pooling methods in graph classification tasks while being able to achieve competitive performance in node classification tasks. We also conduct a graph reconstruction task to show GPN's ability to preserve node information and measure both memory and time efficiency through relevant tests.","sentences":["Graph pooling compresses graph information into a compact representation.","State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step.","These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering.","Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph.","In this work, inspired by bottom-up grammar induction, we propose an efficient graph parsing algorithm to infer the pooling structure, which then drives graph pooling.","The resulting Graph Parsing Network (GPN) adaptively learns personalized pooling structure for each individual graph.","GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node information intact.","Experimental results on standard benchmarks demonstrate that GPN outperforms state-of-the-art graph pooling methods in graph classification tasks while being able to achieve competitive performance in node classification tasks.","We also conduct a graph reconstruction task to show GPN's ability to preserve node information and measure both memory and time efficiency through relevant tests."],"url":"http://arxiv.org/abs/2402.14393v1","category":"cs.LG"}
{"created":"2024-02-22 09:03:51","title":"Composite likelihood inference for the Poisson log-normal model","abstract":"Inferring parameters of a latent variable model can be a daunting task when the conditional distribution of the latent variables given the observed ones is intractable. Variational approaches prove to be computationally efficient but, possibly, lack theoretical guarantees on the estimates, while sampling based solutions are quite the opposite. Starting from already available variational approximations, we define a first Monte Carlo EM algorithm to obtain maximum likelihood estimators, focusing on the Poisson log-normal model which provides a generic framework for the analysis of multivariate count data. We then extend this algorithm to the case of a composite likelihood in order to be able to handle higher dimensional count data.","sentences":["Inferring parameters of a latent variable model can be a daunting task when the conditional distribution of the latent variables given the observed ones is intractable.","Variational approaches prove to be computationally efficient but, possibly, lack theoretical guarantees on the estimates, while sampling based solutions are quite the opposite.","Starting from already available variational approximations, we define a first Monte Carlo EM algorithm to obtain maximum likelihood estimators, focusing on the Poisson log-normal model which provides a generic framework for the analysis of multivariate count data.","We then extend this algorithm to the case of a composite likelihood in order to be able to handle higher dimensional count data."],"url":"http://arxiv.org/abs/2402.14390v1","category":"stat.CO"}
{"created":"2024-02-22 09:01:42","title":"Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search","abstract":"Financial institutions and businesses face an ongoing challenge from fraudulent transactions, prompting the need for effective detection methods. Detecting credit card fraud is crucial for identifying and preventing unauthorized transactions.Timely detection of fraud enables investigators to take swift actions to mitigate further losses. However, the investigation process is often time-consuming, limiting the number of alerts that can be thoroughly examined each day. Therefore, the primary objective of a fraud detection model is to provide accurate alerts while minimizing false alarms and missed fraud cases. In this paper, we introduce a state-of-the-art hybrid ensemble (ENS) dependable Machine learning (ML) model that intelligently combines multiple algorithms with proper weighted optimization using Grid search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor (KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To address the data imbalance issue, we employ the Instant Hardness Threshold (IHT) technique in conjunction with Logistic Regression (LR), surpassing conventional approaches. Our experiments are conducted on a publicly available credit card dataset comprising 284,807 transactions. The proposed model achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid ensemble model outperforms existing works, establishing a new benchmark for detecting fraudulent transactions in high-frequency scenarios. The results highlight the effectiveness and reliability of our approach, demonstrating superior performance metrics and showcasing its exceptional potential for real-world fraud detection applications.","sentences":["Financial institutions and businesses face an ongoing challenge from fraudulent transactions, prompting the need for effective detection methods.","Detecting credit card fraud is crucial for identifying and preventing unauthorized transactions.","Timely detection of fraud enables investigators to take swift actions to mitigate further losses.","However, the investigation process is often time-consuming, limiting the number of alerts that can be thoroughly examined each day.","Therefore, the primary objective of a fraud detection model is to provide accurate alerts while minimizing false alarms and missed fraud cases.","In this paper, we introduce a state-of-the-art hybrid ensemble (ENS) dependable Machine learning (ML) model that intelligently combines multiple algorithms with proper weighted optimization using Grid search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor (KNN), and Multilayer Perceptron (MLP), to enhance fraud identification.","To address the data imbalance issue, we employ the Instant Hardness Threshold (IHT) technique in conjunction with Logistic Regression (LR), surpassing conventional approaches.","Our experiments are conducted on a publicly available credit card dataset comprising 284,807 transactions.","The proposed model achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively.","The hybrid ensemble model outperforms existing works, establishing a new benchmark for detecting fraudulent transactions in high-frequency scenarios.","The results highlight the effectiveness and reliability of our approach, demonstrating superior performance metrics and showcasing its exceptional potential for real-world fraud detection applications."],"url":"http://arxiv.org/abs/2402.14389v1","category":"cs.LG"}
{"created":"2024-02-22 08:59:12","title":"Saturating linear sets in PG$(2,q^4)$","abstract":"Bonini, Borello and Byrne started the study of saturating linear sets in Desarguesian projective spaces, in connection with the covering problem in the rank metric. In this paper we study \\emph{$1$-saturating} linear sets in PG$(2,q^4)$, that is $\\mathbb{F}_q$-linear sets in PG$(2,q^4)$ with the property that their secant lines cover the entire plane. By making use of a characterization of generalized Gabidulin codes, we prove that the rank of such a linear set is at least $5$. This answers to a recent question posed by Bartoli, Borello and Marino.","sentences":["Bonini, Borello and Byrne started the study of saturating linear sets in Desarguesian projective spaces, in connection with the covering problem in the rank metric.","In this paper we study \\emph{$1$-saturating} linear sets in PG$(2,q^4)$, that is $\\mathbb{F}_q$-linear sets in PG$(2,q^4)$ with the property that their secant lines cover the entire plane.","By making use of a characterization of generalized Gabidulin codes, we prove that the rank of such a linear set is at least $5$. This answers to a recent question posed by Bartoli, Borello and Marino."],"url":"http://arxiv.org/abs/2402.14387v1","category":"math.CO"}
{"created":"2024-02-22 08:57:53","title":"Workspace Analysis for Laparoscopic Rectal Surgery : A Preliminary Study","abstract":"The integration of medical imaging, computational analysis, and robotic technology has brought about a significant transformation in minimally invasive surgical procedures, particularly in the realm of laparoscopic rectal surgery (LRS). This specialized surgical technique, aimed at addressing rectal cancer, requires an in-depth comprehension of the spatial dynamics within the narrow space of the pelvis. Leveraging Magnetic Resonance Imaging (MRI) scans as a foundational dataset, this study incorporates them into Computer-Aided Design (CAD) software to generate precise three-dimensional (3D) reconstructions of the patient's anatomy. At the core of this research is the analysis of the surgical workspace, a critical aspect in the optimization of robotic interventions. Sophisticated computational algorithms process MRI data within the CAD environment, meticulously calculating the dimensions and contours of the pelvic internal regions. The outcome is a nuanced understanding of both viable and restricted zones during LRS, taking into account factors such as curvature, diameter variations, and potential obstacles. This paper delves deeply into the complexities of workspace analysis for robotic LRS, illustrating the seamless collaboration between medical imaging, CAD software, and surgical robotics. Through this interdisciplinary approach, the study aims to surpass traditional surgical methodologies, offering novel insights for a paradigm shift in optimizing robotic interventions within the complex environment of the pelvis.","sentences":["The integration of medical imaging, computational analysis, and robotic technology has brought about a significant transformation in minimally invasive surgical procedures, particularly in the realm of laparoscopic rectal surgery (LRS).","This specialized surgical technique, aimed at addressing rectal cancer, requires an in-depth comprehension of the spatial dynamics within the narrow space of the pelvis.","Leveraging Magnetic Resonance Imaging (MRI) scans as a foundational dataset, this study incorporates them into Computer-Aided Design (CAD) software to generate precise three-dimensional (3D) reconstructions of the patient's anatomy.","At the core of this research is the analysis of the surgical workspace, a critical aspect in the optimization of robotic interventions.","Sophisticated computational algorithms process MRI data within the CAD environment, meticulously calculating the dimensions and contours of the pelvic internal regions.","The outcome is a nuanced understanding of both viable and restricted zones during LRS, taking into account factors such as curvature, diameter variations, and potential obstacles.","This paper delves deeply into the complexities of workspace analysis for robotic LRS, illustrating the seamless collaboration between medical imaging, CAD software, and surgical robotics.","Through this interdisciplinary approach, the study aims to surpass traditional surgical methodologies, offering novel insights for a paradigm shift in optimizing robotic interventions within the complex environment of the pelvis."],"url":"http://arxiv.org/abs/2402.14386v1","category":"cs.RO"}
{"created":"2024-02-22 08:54:57","title":"Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection","abstract":"In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data. Anomaly detection involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network. Soft-DTW is used as a differentiable alternative for the reconstruction loss and is found to be superior to Euclidean distance. Combining reconstruction loss and the latent space's prior probability distribution serves as the anomaly score. Our novel method accelerates detection by parallel computation of reconstruction of multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings.","sentences":["In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data.","Anomaly detection involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network.","Soft-DTW is used as a differentiable alternative for the reconstruction loss and is found to be superior to Euclidean distance.","Combining reconstruction loss and the latent space's prior probability distribution serves as the anomaly score.","Our novel method accelerates detection by parallel computation of reconstruction of multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings."],"url":"http://arxiv.org/abs/2402.14384v1","category":"cs.LG"}
{"created":"2024-02-22 08:52:25","title":"Typical dynamics of Newton's method","abstract":"This article consists of two papers: $\\textit{Typical dynamics of Newton's method}$ by Steele and $\\textit{Erratum to \"Typical dynamics of Newton's method\"}$ by Dud\\'ak and Steele.   Let $C^1(M)$ be the space of continuously differentiable real-valued functions defined on $[-M,M]$. We show that for the typical element $f$ in $C^1(M)$, there exists a set $S \\subseteq [-M,M]$, both residual and of full measure in $[-M,M]$, such that for any $x \\in S$, the trajectory generated by Newton's method using $f$ and $x$ either diverges, converges to a root of $f$, or generates a Cantor set as its attractor. Whenever the Cantor set is the attractor, the dynamics on the attractor are described by a single type of adding machine, so that the dynamics on all of these attracting Cantor sets are topologically equivalent.","sentences":["This article consists of two papers: $\\textit{Typical dynamics of Newton's method}$ by Steele and $\\textit{Erratum to \"Typical dynamics of Newton's method\"}$ by Dud\\'ak and Steele.   ","Let $C^1(M)$ be the space of continuously differentiable real-valued functions defined on $[-M,M]$.","We show that for the typical element $f$ in $C^1(M)$, there exists a set $S","\\subseteq","[-M,M]$, both residual and of full measure in $[-M,M]$, such that for any $x \\in S$, the trajectory generated by Newton's method using $f$ and $x$ either diverges, converges to a root of $f$, or generates a Cantor set as its attractor.","Whenever the Cantor set is the attractor, the dynamics on the attractor are described by a single type of adding machine, so that the dynamics on all of these attracting Cantor sets are topologically equivalent."],"url":"http://arxiv.org/abs/2402.14383v1","category":"math.DS"}
{"created":"2024-02-22 08:48:21","title":"Novi jezi\u010dki modeli za srpski jezik","abstract":"The paper will briefly present the development history of transformer-based language models for the Serbian language. Several new models for text generation and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented. Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks. Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language.","sentences":["The paper will briefly present the development history of transformer-based language models for the Serbian language.","Several new models for text generation and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented.","Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks.","Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language."],"url":"http://arxiv.org/abs/2402.14379v1","category":"cs.CL"}
{"created":"2024-02-22 08:15:08","title":"Parsimonious Generative Machine Learning for Non-Gaussian Tail Modeling and Risk-Neutral Distribution Extraction","abstract":"In financial modeling problems, non-Gaussian tails exist widely in many circumstances. Among them, the accurate estimation of risk-neutral distribution (RND) from option prices is of great importance for researchers and practitioners. A precise RND can provide valuable information regarding the market's expectations, and can further help empirical asset pricing studies. This paper presents a parsimonious parametric approach to extract RNDs of underlying asset returns by using a generative machine learning model. The model incorporates the asymmetric heavy tails property of returns with a clever design. To calibrate the model, we design a Monte Carlo algorithm that has good capability with the assistance of modern machine learning computing tools. Numerically, the model fits Heston option prices well and captures the main shapes of implied volatility curves. Empirically, using S\\&P 500 index option prices, we demonstrate that the model outperforms some popular parametric density methods under mean absolute error. Furthermore, the skewness and kurtosis of RNDs extracted by our model are consistent with intuitive expectations.","sentences":["In financial modeling problems, non-Gaussian tails exist widely in many circumstances.","Among them, the accurate estimation of risk-neutral distribution (RND) from option prices is of great importance for researchers and practitioners.","A precise RND can provide valuable information regarding the market's expectations, and can further help empirical asset pricing studies.","This paper presents a parsimonious parametric approach to extract RNDs of underlying asset returns by using a generative machine learning model.","The model incorporates the asymmetric heavy tails property of returns with a clever design.","To calibrate the model, we design a Monte Carlo algorithm that has good capability with the assistance of modern machine learning computing tools.","Numerically, the model fits Heston option prices well and captures the main shapes of implied volatility curves.","Empirically, using S\\&P 500 index option prices, we demonstrate that the model outperforms some popular parametric density methods under mean absolute error.","Furthermore, the skewness and kurtosis of RNDs extracted by our model are consistent with intuitive expectations."],"url":"http://arxiv.org/abs/2402.14368v1","category":"stat.AP"}
{"created":"2024-02-22 08:09:01","title":"Understanding and Detecting Annotation-Induced Faults of Static Analyzers","abstract":"Static analyzers can reason about the properties and behaviors of programs and detect various issues without executing them. Hence, they should extract the necessary information to understand the analyzed program well. Annotation has been a widely used feature for different purposes in Java since the introduction of Java 5. Annotations can change program structures and convey semantics information without awareness of static analyzers, consequently leading to imprecise analysis results. This paper presents the first comprehensive study of annotation-induced faults (AIF) by analyzing 246 issues in six open-source and popular static analyzers (i.e., PMD, SpotBugs, CheckStyle, Infer, SonarQube, and Soot). We analyzed the issues' root causes, symptoms, and fix strategies and derived ten findings and some practical guidelines for detecting and repairing annotation-induced faults. Moreover, we developed an automated testing framework called AnnaTester based on three metamorphic relations originating from the findings. AnnaTester generated new tests based on the official test suites of static analyzers and unveiled 43 new faults, 20 of which have been fixed. The results confirm the value of our study and its findings.","sentences":["Static analyzers can reason about the properties and behaviors of programs and detect various issues without executing them.","Hence, they should extract the necessary information to understand the analyzed program well.","Annotation has been a widely used feature for different purposes in Java since the introduction of Java 5.","Annotations can change program structures and convey semantics information without awareness of static analyzers, consequently leading to imprecise analysis results.","This paper presents the first comprehensive study of annotation-induced faults (AIF) by analyzing 246 issues in six open-source and popular static analyzers (i.e., PMD, SpotBugs, CheckStyle, Infer, SonarQube, and Soot).","We analyzed the issues' root causes, symptoms, and fix strategies and derived ten findings and some practical guidelines for detecting and repairing annotation-induced faults.","Moreover, we developed an automated testing framework called AnnaTester based on three metamorphic relations originating from the findings.","AnnaTester generated new tests based on the official test suites of static analyzers and unveiled 43 new faults, 20 of which have been fixed.","The results confirm the value of our study and its findings."],"url":"http://arxiv.org/abs/2402.14366v1","category":"cs.SE"}
{"created":"2024-02-22 08:01:01","title":"OpenTab: Advancing Large Language Models as Open-domain Table Reasoners","abstract":"Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.","sentences":["Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously.","One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope.","However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes.","In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs.","Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently.","Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response.","Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy.","We further run ablation studies to validate the efficacy of our proposed designs of the system."],"url":"http://arxiv.org/abs/2402.14361v1","category":"cs.LG"}
{"created":"2024-02-22 07:58:29","title":"Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark","abstract":"The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. This suggests an area for future enhancement of LLMs.","sentences":["The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed.","This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content.","Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects.","This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.","Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations.","Our findings confirm that FM offers a more logical approach to evaluating scientific summaries.","In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains.","This suggests an area for future enhancement of LLMs."],"url":"http://arxiv.org/abs/2402.14359v1","category":"cs.CL"}
{"created":"2024-02-22 07:55:26","title":"Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?","abstract":"Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.","sentences":["Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning.","In contrast, humans convey and pass down commonsense implicitly through stories.","This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling.","We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs.","Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy.","Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions.","This aligns with the reporting bias of commonsense in text corpora.","We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning.","These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities."],"url":"http://arxiv.org/abs/2402.14355v1","category":"cs.CL"}
{"created":"2024-02-22 07:31:41","title":"Quadratic Spinor Polynomials with Infinitely Many Factorizations","abstract":"Spinor polynomials are polynomials with coefficients in the even sub-algebra of conformal geometric algebra whose norm polynomial is real. They describe rational conformal motions. Factorizations of spinor polynomial corresponds to the decomposition of the rational motion into elementary motions. Generic spinor polynomials allow for a finite number of factorizations. We present two examples of quadratic spinor polynomials that admit infinitely many factorizations. One of them, the circular translation, is well-known. The other one has only been introduced recently but in a different context. We not only compute all factorizations of these conformal motions but also interpret them geometrically.","sentences":["Spinor polynomials are polynomials with coefficients in the even sub-algebra of conformal geometric algebra whose norm polynomial is real.","They describe rational conformal motions.","Factorizations of spinor polynomial corresponds to the decomposition of the rational motion into elementary motions.","Generic spinor polynomials allow for a finite number of factorizations.","We present two examples of quadratic spinor polynomials that admit infinitely many factorizations.","One of them, the circular translation, is well-known.","The other one has only been introduced recently but in a different context.","We not only compute all factorizations of these conformal motions but also interpret them geometrically."],"url":"http://arxiv.org/abs/2402.14347v1","category":"math.RA"}
{"created":"2024-02-22 07:24:26","title":"Dependable Distributed Training of Compressed Machine Learning Models","abstract":"The existing work on the distributed training of machine learning (ML) models has consistently overlooked the distribution of the achieved learning quality, focusing instead on its average value. This leads to a poor dependability}of the resulting ML models, whose performance may be much worse than expected. We fill this gap by proposing DepL, a framework for dependable learning orchestration, able to make high-quality, efficient decisions on (i) the data to leverage for learning, (ii) the models to use and when to switch among them, and (iii) the clusters of nodes, and the resources thereof, to exploit. For concreteness, we consider as possible available models a full DNN and its compressed versions. Unlike previous studies, DepL guarantees that a target learning quality is reached with a target probability, while keeping the training cost at a minimum. We prove that DepL has constant competitive ratio and polynomial complexity, and show that it outperforms the state-of-the-art by over 27% and closely matches the optimum.","sentences":["The existing work on the distributed training of machine learning (ML) models has consistently overlooked the distribution of the achieved learning quality, focusing instead on its average value.","This leads to a poor dependability}of the resulting ML models, whose performance may be much worse than expected.","We fill this gap by proposing DepL, a framework for dependable learning orchestration, able to make high-quality, efficient decisions on (i) the data to leverage for learning, (ii) the models to use and when to switch among them, and (iii) the clusters of nodes, and the resources thereof, to exploit.","For concreteness, we consider as possible available models a full DNN and its compressed versions.","Unlike previous studies, DepL guarantees that a target learning quality is reached with a target probability, while keeping the training cost at a minimum.","We prove that DepL has constant competitive ratio and polynomial complexity, and show that it outperforms the state-of-the-art by over 27% and closely matches the optimum."],"url":"http://arxiv.org/abs/2402.14346v1","category":"cs.LG"}
{"created":"2024-02-22 07:20:53","title":"The expansion of half-integral polytopes","abstract":"The expansion of a polytope is an important parameter for the analysis of the random walks on its graph. A conjecture of Mihai and Vazirani states that all $0/1$-polytopes have expansion at least 1. We show that the generalization to half-integral polytopes does not hold by constructing $d$-dimensional half-integral polytopes whose expansion decreases exponentially fast with $d$. We also prove that the expansion of half-integral zonotopes is uniformly bounded away from $0$. As an intermediate result, we show that half-integral zonotopes are always graphical.","sentences":["The expansion of a polytope is an important parameter for the analysis of the random walks on its graph.","A conjecture of Mihai and Vazirani states that all $0/1$-polytopes have expansion at least 1.","We show that the generalization to half-integral polytopes does not hold by constructing $d$-dimensional half-integral polytopes whose expansion decreases exponentially fast with $d$. We also prove that the expansion of half-integral zonotopes is uniformly bounded away from $0$. As an intermediate result, we show that half-integral zonotopes are always graphical."],"url":"http://arxiv.org/abs/2402.14343v1","category":"math.CO"}
{"created":"2024-02-22 07:13:35","title":"Coherently excited superresolution using intensity product of phase-controlled quantum erasers via polarization-basis projection measurements","abstract":"Recently, the delayed-choice quantum eraser has been applied for coherently excited superresolution using phase-controlled projection measurements of laser light to overcome the diffraction limit in classical physics as well as to solve the limited photon number of the N00N state in quantum physics. Unlike other methods of phase-controlled superresolution in a noninterferometric system, the proposed method is for the intensity products between phase-controlled quantum erasers, resulting in superresolution compatible with the most conventional sensing metrologies. Here, both the general scheme and solution of the phase-controlled quantum eraser-based superresolution are derived for arbitrary Nth-order intensity correlation, where the superresolution shows the photonic de Broglie wave-like quantum feature. Furthermore, the phase quantization of the superresolution is discussed to better understand quantum mechanics.","sentences":["Recently, the delayed-choice quantum eraser has been applied for coherently excited superresolution using phase-controlled projection measurements of laser light to overcome the diffraction limit in classical physics as well as to solve the limited photon number of the N00N state in quantum physics.","Unlike other methods of phase-controlled superresolution in a noninterferometric system, the proposed method is for the intensity products between phase-controlled quantum erasers, resulting in superresolution compatible with the most conventional sensing metrologies.","Here, both the general scheme and solution of the phase-controlled quantum eraser-based superresolution are derived for arbitrary Nth-order intensity correlation, where the superresolution shows the photonic de Broglie wave-like quantum feature.","Furthermore, the phase quantization of the superresolution is discussed to better understand quantum mechanics."],"url":"http://arxiv.org/abs/2402.14338v1","category":"quant-ph"}
{"created":"2024-02-22 18:56:31","title":"Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach","abstract":"In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification performance under heterophily, thanks to the adoption of physics biases in the message-passing. Drawing inspiration from these findings, we advocate that the methodology employed by GRAFF can improve link prediction performance as well. To further explore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link prediction. We evaluate its efficacy within a recent collection of heterophilic graphs, establishing a new benchmark for link prediction under heterophily. Our approach surpasses previous methods, in most of the datasets, showcasing a strong flexibility in different contexts, and achieving relative AUROC improvements of up to 26.7%.","sentences":["In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs.","However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels.","Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks.","This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems.","For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance.","Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification performance under heterophily, thanks to the adoption of physics biases in the message-passing.","Drawing inspiration from these findings, we advocate that the methodology employed by GRAFF can improve link prediction performance as well.","To further explore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link prediction.","We evaluate its efficacy within a recent collection of heterophilic graphs, establishing a new benchmark for link prediction under heterophily.","Our approach surpasses previous methods, in most of the datasets, showcasing a strong flexibility in different contexts, and achieving relative AUROC improvements of up to 26.7%."],"url":"http://arxiv.org/abs/2402.14802v1","category":"cs.LG"}
{"created":"2024-02-22 18:54:32","title":"CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation","abstract":"We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks. By incorporating extensive data augmentation in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions. Regardless of its affordability and convenience in data collection, CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects. For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves. Our research demonstrates the significant potential of simulated human demonstrations for real-world dexterous manipulation tasks. More details can be found at https://cyber-demo.github.io","sentences":["We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks.","By incorporating extensive data augmentation in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions.","Regardless of its affordability and convenience in data collection, CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects.","For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves.","Our research demonstrates the significant potential of simulated human demonstrations for real-world dexterous manipulation tasks.","More details can be found at https://cyber-demo.github.io"],"url":"http://arxiv.org/abs/2402.14795v1","category":"cs.RO"}
{"created":"2024-02-22 17:47:00","title":"The Spectrum of He$^+$ as a Proving Ground for Bohr's Model of the Atom: A Legacy of Williamina Fleming's Astrophysical Discovery","abstract":"In 1896, Edward Charles Pickering (1846-1919), Director of the Harvard College Observatory (HCO), reported in a trio of publications the observation of \"peculiar spectra\" of the southern star $\\zeta$ Puppis, which he attributed to an \"element not yet found in other stars or on earth.\" Supported by laboratory spectra obtained by Alfred Fowler (1868-1940), Niels Bohr (1885-1962) showed in 1913 that this \"element\" was in fact ionized helium, He$^+$. Its spectrum has become known as the Pickering Series, even though Pickering credited Williamina Fleming (1857-1911) for the discovery. Fleming was one of HCO's \"computers\" and the future Curator of the Astronomical Photographic Glass Plate Collection. The series of spectral lines associated with Pickering's name played a unique role on the path to quantum mechanics by serving as a proving ground for Bohr's model of the atom. Our examination of the discovery of the Pickering series relied on the records held at the Center for Astrophysics $\\vert$ Harvard \\& Smithsonian (the successor institution to HCO), especially the Notebooks and Diaries of Williamina Fleming and others as well as on the Center's Glass Plate Collection. Glimpses of the \"peculiar sociology\" of a research institution, half of whose staff were women employed on grossly unequal terms with men, are given in the course of the narrative.","sentences":["In 1896, Edward Charles Pickering (1846-1919), Director of the Harvard College Observatory (HCO), reported in a trio of publications the observation of \"peculiar spectra\" of the southern star $\\zeta$ Puppis, which he attributed to an \"element not yet found in other stars or on earth.\"","Supported by laboratory spectra obtained by Alfred Fowler (1868-1940), Niels Bohr (1885-1962) showed in 1913 that this \"element\" was in fact ionized helium,","He$^+$.","Its spectrum has become known as the Pickering Series, even though Pickering credited Williamina Fleming (1857-1911) for the discovery.","Fleming was one of HCO's \"computers\" and the future Curator of the Astronomical Photographic Glass Plate Collection.","The series of spectral lines associated with Pickering's name played a unique role on the path to quantum mechanics by serving as a proving ground for Bohr's model of the atom.","Our examination of the discovery of the Pickering series relied on the records held at the Center for Astrophysics $\\vert$ Harvard \\& Smithsonian (the successor institution to HCO), especially the Notebooks and Diaries of Williamina Fleming and others as well as on the Center's Glass Plate Collection.","Glimpses of the \"peculiar sociology\" of a research institution, half of whose staff were women employed on grossly unequal terms with men, are given in the course of the narrative."],"url":"http://arxiv.org/abs/2402.14734v1","category":"physics.atom-ph"}
{"created":"2024-02-22 16:04:03","title":"Cleaner Pretraining Corpus Curation with Neural Web Scraping","abstract":"The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.","sentences":["The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans.","Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining.","However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate.","This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages.","Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining.","All of the code is available at https://github.com/OpenMatch/NeuScraper."],"url":"http://arxiv.org/abs/2402.14652v1","category":"cs.CL"}
{"created":"2024-02-22 14:21:27","title":"Sorting of mesoporous silica derivatives by random optical fields","abstract":"Mesoporous silica particles are promising candidates for drug delivery applications. In this paper, we first synthesize mesoporous silica MCM-41 and its derivative MCM-41GA with anchored glutaraldehyde bridges, and characterize them using a variety of techniques, including nitrogen adsorption/desorption, X-ray diffraction, NMR spectroscopy, scanning electron microscopy, and thermogravimetric analysis. Then, we employ random optical fields to sort mesoporous silica particles. Random optical fields by containing local intensity gradients throughout a wide range of field of view provide an elegant, easy-to-implement, and low-cost variant of multiple optical tweezers, which is known as speckle tweezers (ST). ST, similar to multiple optical tweezers, for manipulation tasks, such as trapping, sorting, and guiding of collection of micro and sub-micro objects in several disciplines including statistical physics, chemistry, microfluidics and material science. We show that ST can restrict, sieve, and sort MCM-41 and MCM-41GA particles. The different interaction of mesoporous silica variations with the applied ST may be attributed to the pre-applied modification and the differences in the porosity structure and distribution. Therefore, the results provide insight into the textural and chemical characteristics of mesoporous materials, contributing to a deeper understanding of their potential applications.","sentences":["Mesoporous silica particles are promising candidates for drug delivery applications.","In this paper, we first synthesize mesoporous silica MCM-41 and its derivative MCM-41GA with anchored glutaraldehyde bridges, and characterize them using a variety of techniques, including nitrogen adsorption/desorption, X-ray diffraction, NMR spectroscopy, scanning electron microscopy, and thermogravimetric analysis.","Then, we employ random optical fields to sort mesoporous silica particles.","Random optical fields by containing local intensity gradients throughout a wide range of field of view provide an elegant, easy-to-implement, and low-cost variant of multiple optical tweezers, which is known as speckle tweezers (ST).","ST, similar to multiple optical tweezers, for manipulation tasks, such as trapping, sorting, and guiding of collection of micro and sub-micro objects in several disciplines including statistical physics, chemistry, microfluidics and material science.","We show that ST can restrict, sieve, and sort MCM-41 and MCM-41GA particles.","The different interaction of mesoporous silica variations with the applied ST may be attributed to the pre-applied modification and the differences in the porosity structure and distribution.","Therefore, the results provide insight into the textural and chemical characteristics of mesoporous materials, contributing to a deeper understanding of their potential applications."],"url":"http://arxiv.org/abs/2402.14571v1","category":"physics.optics"}
{"created":"2024-02-22 14:04:13","title":"Non-Contact Acquisition of PPG Signal using Chest Movement-Modulated Radio Signals","abstract":"We present for the first time a novel method that utilizes the chest movement-modulated radio signals for non-contact acquisition of the photoplethysmography (PPG) signal. Under the proposed method, a software-defined radio (SDR) exposes the chest of a subject sitting nearby to an orthogonal frequency division multiplexing signal with 64 sub-carriers at a center frequency 5.24 GHz, while another SDR in the close vicinity collects the modulated radio signal reflected off the chest. This way, we construct a custom dataset by collecting 160 minutes of labeled data (both raw radio data as well as the reference PPG signal) from 16 healthy young subjects. With this, we first utilize principal component analysis for dimensionality reduction of the radio data. Next, we denoise the radio signal and reference PPG signal using wavelet technique, followed by segmentation and Z-score normalization. We then synchronize the radio and PPG segments using cross-correlation method. Finally, we proceed to the waveform translation (regression) task, whereby we first convert the radio and PPG segments into frequency domain using discrete cosine transform (DCT), and then learn the non-linear regression between them. Eventually, we reconstruct the synthetic PPG signal by taking inverse DCT of the output of regression block, with a mean absolute error of 8.1294. The synthetic PPG waveform has a great clinical significance as it could be used for non-contact performance assessment of cardiovascular and respiratory systems of patients suffering from infectious diseases, e.g., covid19.","sentences":["We present for the first time a novel method that utilizes the chest movement-modulated radio signals for non-contact acquisition of the photoplethysmography (PPG) signal.","Under the proposed method, a software-defined radio (SDR) exposes the chest of a subject sitting nearby to an orthogonal frequency division multiplexing signal with 64 sub-carriers at a center frequency 5.24 GHz, while another SDR in the close vicinity collects the modulated radio signal reflected off the chest.","This way, we construct a custom dataset by collecting 160 minutes of labeled data (both raw radio data as well as the reference PPG signal) from 16 healthy young subjects.","With this, we first utilize principal component analysis for dimensionality reduction of the radio data.","Next, we denoise the radio signal and reference PPG signal using wavelet technique, followed by segmentation and Z-score normalization.","We then synchronize the radio and PPG segments using cross-correlation method.","Finally, we proceed to the waveform translation (regression) task, whereby we first convert the radio and PPG segments into frequency domain using discrete cosine transform (DCT), and then learn the non-linear regression between them.","Eventually, we reconstruct the synthetic PPG signal by taking inverse DCT of the output of regression block, with a mean absolute error of 8.1294.","The synthetic PPG waveform has a great clinical significance as it could be used for non-contact performance assessment of cardiovascular and respiratory systems of patients suffering from infectious diseases, e.g., covid19."],"url":"http://arxiv.org/abs/2402.14565v1","category":"eess.SP"}
{"created":"2024-02-22 12:29:19","title":"Search for long-lived particles decaying to final states with a pair of muons in proton-proton collisions at $\\sqrt{s}$ = 13.6 TeV","abstract":"An inclusive search for long-lived exotic particles (LLPs) decaying to final states with a pair of muons is presented. The search uses data corresponding to an integrated luminosity of 36.6 fb$^{-1}$ collected by the CMS experiment from the proton-proton collisions at $\\sqrt{s}$ = 13.6 TeV in 2022, the first year of Run 3 of the CERN LHC. The experimental signature is a pair of oppositely charged muons originating from a common vertex spatially separated from the proton-proton interaction point by distances ranging from several hundred $\\mu$m to several meters. The sensitivity of the search benefits from new triggers for displaced dimuons developed for Run 3. The results are interpreted in the framework of the hidden Abelian Higgs model, in which the Higgs boson decays to a pair of long-lived dark photons, and of an $R$-parity violating supersymmetry model, in which long-lived neutralinos decay to a pair of muons and a neutrino. The limits set on these models are the most stringent to date in wide regions of lifetimes for LLPs with masses larger than 10 GeV.","sentences":["An inclusive search for long-lived exotic particles (LLPs) decaying to final states with a pair of muons is presented.","The search uses data corresponding to an integrated luminosity of 36.6 fb$^{-1}$ collected by the CMS experiment from the proton-proton collisions at $\\sqrt{s}$ = 13.6 TeV in 2022, the first year of Run 3 of the CERN LHC.","The experimental signature is a pair of oppositely charged muons originating from a common vertex spatially separated from the proton-proton interaction point by distances ranging from several hundred $\\mu$m to several meters.","The sensitivity of the search benefits from new triggers for displaced dimuons developed for Run 3.","The results are interpreted in the framework of the hidden Abelian Higgs model, in which the Higgs boson decays to a pair of long-lived dark photons, and of an $R$-parity violating supersymmetry model, in which long-lived neutralinos decay to a pair of muons and a neutrino.","The limits set on these models are the most stringent to date in wide regions of lifetimes for LLPs with masses larger than 10 GeV."],"url":"http://arxiv.org/abs/2402.14491v1","category":"hep-ex"}
{"created":"2024-02-22 11:48:06","title":"NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection","abstract":"NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at https://github.com/mrsempress/NeRF-Detplusplus.","sentences":["NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning.","Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision.","To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement.","We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors.","2) Perspective-aware Sampling.","Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues.","3)Ordinal Residual Depth Supervision.","As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process.","The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets.","Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$.","The code will be publicly at https://github.com/mrsempress/NeRF-Detplusplus."],"url":"http://arxiv.org/abs/2402.14464v1","category":"cs.CV"}
{"created":"2024-02-22 11:35:52","title":"Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on Mediterranean Sea Water","abstract":"Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that fosters biodiversity, stores carbon, releases oxygen, and provides habitat to numerous sea organisms. Leveraging augmented research, we collected a comprehensive dataset of 174 features compiled from diverse data sources. Through machine learning analysis, we discovered the existence of a robust correlation between the exact location of P. oceanica and water biogeochemical properties. The model's feature importance, showed that carbon-related variables as net biomass production and downward surface mass flux of carbon dioxide have their values altered in the areas with P. oceanica, which in turn can be used for indirect location of P. oceanica meadows. The study provides the evidence of the plant's ability to exert a global impact on the environment and underscores the crucial role of this plant in sea ecosystems, emphasizing the need for its conservation and management.","sentences":["Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that fosters biodiversity, stores carbon, releases oxygen, and provides habitat to numerous sea organisms.","Leveraging augmented research, we collected a comprehensive dataset of 174 features compiled from diverse data sources.","Through machine learning analysis, we discovered the existence of a robust correlation between the exact location of P. oceanica and water biogeochemical properties.","The model's feature importance, showed that carbon-related variables as net biomass production and downward surface mass flux of carbon dioxide have their values altered in the areas with P. oceanica, which in turn can be used for indirect location of P. oceanica meadows.","The study provides the evidence of the plant's ability to exert a global impact on the environment and underscores the crucial role of this plant in sea ecosystems, emphasizing the need for its conservation and management."],"url":"http://arxiv.org/abs/2402.14459v1","category":"physics.ao-ph"}
{"created":"2024-02-22 07:07:16","title":"HyperFast: Instant Classification for Tabular Data","abstract":"Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines. HyperFast shows highly competitive results, while being significantly faster. Additionally, our approach demonstrates robust adaptability across a variety of classification tasks with little to no fine-tuning, positioning HyperFast as a strong solution for numerous applications and rapid model deployment. HyperFast introduces a promising paradigm for fast classification, with the potential to substantially decrease the computational burden of deep learning. Our code, which offers a scikit-learn-like interface, along with the trained HyperFast model, can be found at https://github.com/AI-sandbox/HyperFast.","sentences":["Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming.","Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings.","In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass.","HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model.","We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines.","HyperFast shows highly competitive results, while being significantly faster.","Additionally, our approach demonstrates robust adaptability across a variety of classification tasks with little to no fine-tuning, positioning HyperFast as a strong solution for numerous applications and rapid model deployment.","HyperFast introduces a promising paradigm for fast classification, with the potential to substantially decrease the computational burden of deep learning.","Our code, which offers a scikit-learn-like interface, along with the trained HyperFast model, can be found at https://github.com/AI-sandbox/HyperFast."],"url":"http://arxiv.org/abs/2402.14335v1","category":"cs.LG"}
{"created":"2024-02-22 06:34:50","title":"REPOFUSE: Repository-Level Code Completion with Fused Dual Context","abstract":"The success of language models in code assistance has spurred the proposal of repository-level code completion as a means to enhance prediction accuracy, utilizing the context from the entire codebase. However, this amplified context can inadvertently increase inference latency, potentially undermining the developer experience and deterring tool adoption-a challenge we termed the Context-Latency Conundrum. This paper introduces RepoGenix, a pioneering solution designed to enhance repository-level code completion without the latency trade-off. RepoGenix uniquely fuses two types of contexts: the analogy context, rooted in code analogies, and the rationale context, which encompasses in-depth semantic relationships. We propose a novel rank truncated generation (RTG) technique that efficiently condenses these contexts into prompts with restricted size. This enables RepoGenix to deliver precise code completions while maintaining inference efficiency. Through testing with the CrossCodeEval suite, RepoGenix has demonstrated a significant leap over existing models, achieving a 40.90% to 59.75% increase in exact match (EM) accuracy for code completions and a 26.8% enhancement in inference speed. Beyond experimental validation, RepoGenix has been integrated into the workflow of a large enterprise, where it actively supports various coding tasks.","sentences":["The success of language models in code assistance has spurred the proposal of repository-level code completion as a means to enhance prediction accuracy, utilizing the context from the entire codebase.","However, this amplified context can inadvertently increase inference latency, potentially undermining the developer experience and deterring tool adoption-a challenge we termed the Context-Latency Conundrum.","This paper introduces RepoGenix, a pioneering solution designed to enhance repository-level code completion without the latency trade-off.","RepoGenix uniquely fuses two types of contexts: the analogy context, rooted in code analogies, and the rationale context, which encompasses in-depth semantic relationships.","We propose a novel rank truncated generation (RTG) technique that efficiently condenses these contexts into prompts with restricted size.","This enables RepoGenix to deliver precise code completions while maintaining inference efficiency.","Through testing with the CrossCodeEval suite, RepoGenix has demonstrated a significant leap over existing models, achieving a 40.90% to 59.75% increase in exact match (EM) accuracy for code completions and a 26.8% enhancement in inference speed.","Beyond experimental validation, RepoGenix has been integrated into the workflow of a large enterprise, where it actively supports various coding tasks."],"url":"http://arxiv.org/abs/2402.14323v1","category":"cs.SE"}
{"created":"2024-02-22 06:23:37","title":"Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering","abstract":"Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.","sentences":["Recent progress with LLM-based agents has shown promising results across various tasks.","However, their use in answering questions from knowledge bases remains largely unexplored.","Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures.","In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks.","The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge.","Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles.","We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively."],"url":"http://arxiv.org/abs/2402.14320v1","category":"cs.CL"}
{"created":"2024-02-22 05:53:04","title":"Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases","abstract":"We introduce Ground-Fusion, a low-cost sensor fusion simultaneous localization and mapping (SLAM) system for ground vehicles. Our system features efficient initialization, effective sensor anomaly detection and handling, real-time dense color mapping, and robust localization in diverse environments. We tightly integrate RGB-D images, inertial measurements, wheel odometer and GNSS signals within a factor graph to achieve accurate and reliable localization both indoors and outdoors. To ensure successful initialization, we propose an efficient strategy that comprises three different methods: stationary, visual, and dynamic, tailored to handle diverse cases. Furthermore, we develop mechanisms to detect sensor anomalies and degradation, handling them adeptly to maintain system accuracy. Our experimental results on both public and self-collected datasets demonstrate that Ground-Fusion outperforms existing low-cost SLAM systems in corner cases. We release the code and datasets at https://github.com/SJTU-ViSYS/Ground-Fusion.","sentences":["We introduce Ground-Fusion, a low-cost sensor fusion simultaneous localization and mapping (SLAM) system for ground vehicles.","Our system features efficient initialization, effective sensor anomaly detection and handling, real-time dense color mapping, and robust localization in diverse environments.","We tightly integrate RGB-D images, inertial measurements, wheel odometer and GNSS signals within a factor graph to achieve accurate and reliable localization both indoors and outdoors.","To ensure successful initialization, we propose an efficient strategy that comprises three different methods: stationary, visual, and dynamic, tailored to handle diverse cases.","Furthermore, we develop mechanisms to detect sensor anomalies and degradation, handling them adeptly to maintain system accuracy.","Our experimental results on both public and self-collected datasets demonstrate that Ground-Fusion outperforms existing low-cost SLAM systems in corner cases.","We release the code and datasets at https://github.com/SJTU-ViSYS/Ground-Fusion."],"url":"http://arxiv.org/abs/2402.14308v1","category":"cs.RO"}
{"created":"2024-02-22 05:45:17","title":"Vision-Language Navigation with Embodied Intelligence: A Survey","abstract":"As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges. This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence. After a detailed summary of its system architecture and research based on methods and commonly used benchmark datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers.","sentences":["As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment.","Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation.","VLN integrates artificial intelligence, natural language processing, computer vision, and robotics.","This field faces technical challenges but shows potential for application such as human-computer interaction.","However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges.","This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence.","After a detailed summary of its system architecture and research based on methods and commonly used benchmark datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers."],"url":"http://arxiv.org/abs/2402.14304v1","category":"cs.RO"}
{"created":"2024-02-22 05:32:27","title":"We Choose to Go to Space: Agent-driven Human and Multi-Robot Collaboration in Microgravity","abstract":"We present SpaceAgents-1, a system for learning human and multi-robot collaboration (HMRC) strategies under microgravity conditions. Future space exploration requires humans to work together with robots. However, acquiring proficient robot skills and adept collaboration under microgravity conditions poses significant challenges within ground laboratories. To address this issue, we develop a microgravity simulation environment and present three typical configurations of intra-cabin robots. We propose a hierarchical heterogeneous multi-agent collaboration architecture: guided by foundation models, a Decision-Making Agent serves as a task planner for human-robot collaboration, while individual Skill-Expert Agents manage the embodied control of robots. This mechanism empowers the SpaceAgents-1 system to execute a range of intricate long-horizon HMRC tasks.","sentences":["We present SpaceAgents-1, a system for learning human and multi-robot collaboration (HMRC) strategies under microgravity conditions.","Future space exploration requires humans to work together with robots.","However, acquiring proficient robot skills and adept collaboration under microgravity conditions poses significant challenges within ground laboratories.","To address this issue, we develop a microgravity simulation environment and present three typical configurations of intra-cabin robots.","We propose a hierarchical heterogeneous multi-agent collaboration architecture: guided by foundation models, a Decision-Making Agent serves as a task planner for human-robot collaboration, while individual Skill-Expert Agents manage the embodied control of robots.","This mechanism empowers the SpaceAgents-1 system to execute a range of intricate long-horizon HMRC tasks."],"url":"http://arxiv.org/abs/2402.14299v1","category":"cs.RO"}
{"created":"2024-02-22 05:15:25","title":"Saharaline: A Collective Social Support Intervention for Teachers in Low-Income Indian Schools","abstract":"This paper presents Saharaline, an intervention designed to provide collective social support for teachers in low-income schools. Implemented as a WhatsApp-based helpline, Saharaline enables teachers to reach out for personalized, long-term assistance with a wide range of problems and stressors, including pedagogical, emotional, and technological challenges. Depending on the support needed, teachers' requests are routed to appropriate domain experts -- staff employed by educational non-profit organizations who understand teachers' on-the-ground realities -- who offer localized and contextualized assistance. Via a three-month exploratory deployment with 28 teachers in India, we show how Saharaline's design enabled a collective of diverse education experts to craft and deliver localized solutions that teachers could incorporate into their practice. We conclude by reflecting on the efficacy of our intervention in low-resource work contexts and provide recommendations to enhance collective social support interventions similar to Saharaline.","sentences":["This paper presents Saharaline, an intervention designed to provide collective social support for teachers in low-income schools.","Implemented as a WhatsApp-based helpline, Saharaline enables teachers to reach out for personalized, long-term assistance with a wide range of problems and stressors, including pedagogical, emotional, and technological challenges.","Depending on the support needed, teachers' requests are routed to appropriate domain experts -- staff employed by educational non-profit organizations who understand teachers' on-the-ground realities -- who offer localized and contextualized assistance.","Via a three-month exploratory deployment with 28 teachers in India, we show how Saharaline's design enabled a collective of diverse education experts to craft and deliver localized solutions that teachers could incorporate into their practice.","We conclude by reflecting on the efficacy of our intervention in low-resource work contexts and provide recommendations to enhance collective social support interventions similar to Saharaline."],"url":"http://arxiv.org/abs/2402.14292v1","category":"cs.HC"}
{"created":"2024-02-22 04:47:22","title":"Extention of Bagging MARS with Group LASSO for Heterogeneous Treatment Effect Estimation","abstract":"Recent years, large scale clinical data like patient surveys and medical record data are playing an increasing role in medical data science. These large-scale clinical data, collectively referred to as \"real-world data (RWD)\". It is expected to be widely used in large-scale observational studies of specific diseases, personal medicine or precise medicine, finding the responder of drugs or treatments. Applying RWD for estimating heterogeneous treat ment effect (HTE) has already been a trending topic. HTE has the potential to considerably impact the development of precision medicine by helping doctors make more informed precise treatment decisions and provide more personalized medical care. The statistical models used to estimate HTE is called treatment effect models. Powers et al. proposed a some treatment effect models for observational study, where they pointed out that the bagging causal MARS (BCM) performs outstanding compared to other models. While BCM has excellent performance, it still has room for improvement. In this paper, we proposed a new treatment effect model called shrinkage causal bagging MARS method to improve their shared basis conditional mean regression framework based on the following points: first, we estimated basis functions using transformed outcome, then applied the group LASSO method to optimize the model and estimate parameters. Besides, we are focusing on pursing better interpretability of model to improve the ethical acceptance. We designed simulations to verify the performance of our proposed method and our proposed method superior in mean square error and bias in most simulation settings. Also we applied it to real data set ACTG 175 to verify its usability, where our results are supported by previous studies.","sentences":["Recent years, large scale clinical data like patient surveys and medical record data are playing an increasing role in medical data science.","These large-scale clinical data, collectively referred to as \"real-world data (RWD)\".","It is expected to be widely used in large-scale observational studies of specific diseases, personal medicine or precise medicine, finding the responder of drugs or treatments.","Applying RWD for estimating heterogeneous treat ment effect (HTE) has already been a trending topic.","HTE has the potential to considerably impact the development of precision medicine by helping doctors make more informed precise treatment decisions and provide more personalized medical care.","The statistical models used to estimate HTE is called treatment effect models.","Powers et al. proposed a some treatment effect models for observational study, where they pointed out that the bagging causal MARS (BCM) performs outstanding compared to other models.","While BCM has excellent performance, it still has room for improvement.","In this paper, we proposed a new treatment effect model called shrinkage causal bagging MARS method to improve their shared basis conditional mean regression framework based on the following points: first, we estimated basis functions using transformed outcome, then applied the group LASSO method to optimize the model and estimate parameters.","Besides, we are focusing on pursing better interpretability of model to improve the ethical acceptance.","We designed simulations to verify the performance of our proposed method and our proposed method superior in mean square error and bias in most simulation settings.","Also we applied it to real data set ACTG 175 to verify its usability, where our results are supported by previous studies."],"url":"http://arxiv.org/abs/2402.14282v1","category":"stat.ME"}
{"created":"2024-02-22 04:43:20","title":"A Landmark-Aware Visual Navigation Dataset","abstract":"Map representation learned by expert demonstrations has shown promising research value. However, recent advancements in the visual navigation field face challenges due to the lack of human datasets in the real world for efficient supervised representation learning of the environments. We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised learning of human-centric exploration policies and map building. We collect RGB observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space. The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or graph building and localization. These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments. Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors. Dataset is available at DOI: 10.5281/zenodo.10608067.","sentences":["Map representation learned by expert demonstrations has shown promising research value.","However, recent advancements in the visual navigation field face challenges due to the lack of human datasets in the real world for efficient supervised representation learning of the environments.","We present a Landmark-Aware Visual Navigation (LAVN) dataset to allow for supervised learning of human-centric exploration policies and map building.","We collect RGB observation and human point-click pairs as a human annotator explores virtual and real-world environments with the goal of full coverage exploration of the space.","The human annotators also provide distinct landmark examples along each trajectory, which we intuit will simplify the task of map or graph building and localization.","These human point-clicks serve as direct supervision for waypoint prediction when learning to explore in environments.","Our dataset covers a wide spectrum of scenes, including rooms in indoor environments, as well as walkways outdoors.","Dataset is available at DOI: 10.5281/zenodo.10608067."],"url":"http://arxiv.org/abs/2402.14281v1","category":"cs.CV"}
{"created":"2024-02-22 04:41:52","title":"Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding","abstract":"Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.","sentences":["Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages.","We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords).","We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap."],"url":"http://arxiv.org/abs/2402.14279v1","category":"cs.CL"}
{"created":"2024-02-22 04:36:14","title":"GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages","abstract":"Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with GPT-4 and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.","sentences":["Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern.","Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies.","To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English.","Each translation is accompanied by feminine, masculine, and neutral variants.","The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena.","Additionally, we present a translation gender rewriting solution built with GPT-4 and use GATE X-E to evaluate it.","We open source our contributions to encourage further research on gender debiasing."],"url":"http://arxiv.org/abs/2402.14277v1","category":"cs.CL"}
{"created":"2024-02-22 04:08:31","title":"Optimal Mechanism in a Dynamic Stochastic Knapsack Environment","abstract":"This study introduces an optimal mechanism in a dynamic stochastic knapsack environment. The model features a single seller who has a fixed quantity of a perfectly divisible item. Impatient buyers with a piece-wise linear utility function arrive randomly and they report the two-dimensional private information: marginal value and demanded quantity. We derive a revenue-maximizing dynamic mechanism in a finite discrete time framework that satisfies incentive compatibility, individual rationality, and feasibility conditions. It is achieved by characterizing buyers' utility and deriving the Bellman equation. Moreover, we propose the essential penalty scheme for incentive compatibility, as well as the allocation and payment policies. Lastly, we propose algorithms to approximate the optimal policy, based on the Monte Carlo simulation-based regression method and reinforcement learning.","sentences":["This study introduces an optimal mechanism in a dynamic stochastic knapsack environment.","The model features a single seller who has a fixed quantity of a perfectly divisible item.","Impatient buyers with a piece-wise linear utility function arrive randomly and they report the two-dimensional private information: marginal value and demanded quantity.","We derive a revenue-maximizing dynamic mechanism in a finite discrete time framework that satisfies incentive compatibility, individual rationality, and feasibility conditions.","It is achieved by characterizing buyers' utility and deriving the Bellman equation.","Moreover, we propose the essential penalty scheme for incentive compatibility, as well as the allocation and payment policies.","Lastly, we propose algorithms to approximate the optimal policy, based on the Monte Carlo simulation-based regression method and reinforcement learning."],"url":"http://arxiv.org/abs/2402.14269v1","category":"cs.GT"}
{"created":"2024-02-22 04:07:00","title":"Can Large Language Models Detect Misinformation in Scientific News Reporting?","abstract":"Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database. Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles. Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation. We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press. For each of these architectures, we use several prompt engineering strategies including zero-shot, few-shot, and chain-of-thought prompting. We also test these architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B, Llama2-13B.","sentences":["Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic.","Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence.","Most research on the validity of scientific reporting treats this problem as a claim verification challenge.","In doing so, significant expert human effort is required to generate appropriate claims.","Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available.","The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting.","To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database.","Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles.","Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation.","We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press.","For each of these architectures, we use several prompt engineering strategies including zero-shot, few-shot, and chain-of-thought prompting.","We also test these architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B, Llama2-13B."],"url":"http://arxiv.org/abs/2402.14268v1","category":"cs.CL"}
{"created":"2024-02-22 03:51:34","title":"Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming","abstract":"The integration of Large Language Models (LLMs) into Development Environments (IDEs) has become a focal point in modern software development. LLMs such as OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. However, utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance. In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, including code generation from natural language (generate), documentation generation from code (doc), test case generation (test), bug-fixing (fix), and workspace understanding and query resolution (workspace). These success metrics are designed to evaluate the performance of LLMs within a given IDE and its respective parameter space. Our learnings from evaluating three common LLMs using these metrics can inform the development and validation of future scenarios in LLM guided IDEs.","sentences":["The integration of Large Language Models (LLMs) into Development Environments (IDEs) has become a focal point in modern software development.","LLMs such as OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants.","However, utilizing LLMs out of the box is unlikely to be optimal for any given scenario.","Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance.","In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages.","We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems.","We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, including code generation from natural language (generate), documentation generation from code (doc), test case generation (test), bug-fixing (fix), and workspace understanding and query resolution (workspace).","These success metrics are designed to evaluate the performance of LLMs within a given IDE and its respective parameter space.","Our learnings from evaluating three common LLMs using these metrics can inform the development and validation of future scenarios in LLM guided IDEs."],"url":"http://arxiv.org/abs/2402.14261v1","category":"cs.SE"}
{"created":"2024-02-22 03:46:08","title":"Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond","abstract":"Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 \"off-the-shelf\" large language models (LLMs), and show that WSE exhibits superior performance on accurate uncertainty measurement under two standard criteria for correctness evaluation (e.g., WSE outperforms existing state-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, in terms of the potential for real-world medical QA applications, we achieve a significant enhancement in the performance of LLMs when employing sequences with lower uncertainty, identified by WSE, as final answers (e.g., +6.36% accuracy improvement on the COVID-QA dataset), without requiring any additional task-specific fine-tuning or architectural modifications.","sentences":["Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain.","However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality.","In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification.","We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 \"off-the-shelf\" large language models (LLMs), and show that WSE exhibits superior performance on accurate uncertainty measurement under two standard criteria for correctness evaluation (e.g., WSE outperforms existing state-of-the-art method by 3.23% AUROC on the MedQA dataset).","Additionally, in terms of the potential for real-world medical QA applications, we achieve a significant enhancement in the performance of LLMs when employing sequences with lower uncertainty, identified by WSE, as final answers (e.g., +6.36% accuracy improvement on the COVID-QA dataset), without requiring any additional task-specific fine-tuning or architectural modifications."],"url":"http://arxiv.org/abs/2402.14259v1","category":"cs.CL"}
{"created":"2024-02-22 03:39:48","title":"MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion","abstract":"As a promising 3D generation technique, multiview diffusion (MVD) has received a lot of attention due to its advantages in terms of generalizability, quality, and efficiency. By finetuning pretrained large image diffusion models with 3D data, the MVD methods first generate multiple views of a 3D object based on an image or text prompt and then reconstruct 3D shapes with multiview 3D reconstruction. However, the sparse views and inconsistent details in the generated images make 3D reconstruction challenging. We present MVD$^2$, an efficient 3D reconstruction method for multiview diffusion (MVD) images. MVD$^2$ aggregates image features into a 3D feature volume by projection and convolution and then decodes volumetric features into a 3D mesh. We train MVD$^2$ with 3D shape collections and MVD images prompted by rendered views of 3D shapes. To address the discrepancy between the generated multiview images and ground-truth views of the 3D shapes, we design a simple-yet-efficient view-dependent training scheme. MVD$^2$ improves the 3D generation quality of MVD and is fast and robust to various MVD methods. After training, it can efficiently decode 3D meshes from multiview images within one second. We train MVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its superior performance in generating 3D models from multiview images generated by different MVD methods, using both synthetic and real images as prompts.","sentences":["As a promising 3D generation technique, multiview diffusion (MVD) has received a lot of attention due to its advantages in terms of generalizability, quality, and efficiency.","By finetuning pretrained large image diffusion models with 3D data, the MVD methods first generate multiple views of a 3D object based on an image or text prompt and then reconstruct 3D shapes with multiview 3D reconstruction.","However, the sparse views and inconsistent details in the generated images make 3D reconstruction challenging.","We present MVD$^2$, an efficient 3D reconstruction method for multiview diffusion (MVD) images.","MVD$^2$ aggregates image features into a 3D feature volume by projection and convolution and then decodes volumetric features into a 3D mesh.","We train MVD$^2$ with 3D shape collections and MVD images prompted by rendered views of 3D shapes.","To address the discrepancy between the generated multiview images and ground-truth views of the 3D shapes, we design a simple-yet-efficient view-dependent training scheme.","MVD$^2$ improves the 3D generation quality of MVD and is fast and robust to various MVD methods.","After training, it can efficiently decode 3D meshes from multiview images within one second.","We train MVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its superior performance in generating 3D models from multiview images generated by different MVD methods, using both synthetic and real images as prompts."],"url":"http://arxiv.org/abs/2402.14253v1","category":"cs.CV"}
{"created":"2024-02-22 03:14:03","title":"Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models","abstract":"Recently, there has been considerable attention towards leveraging large language models (LLMs) to enhance decision-making processes. However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details. To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of multimodal LLMs to provide automated preference feedback solely from image inputs to guide decision-making. In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback. Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective. Experimental evaluation of the algorithm's preference accuracy demonstrates its effective generalization ability to new tasks. Furthermore, performance on Meta-World tasks reveals that CriticGPT's reward model efficiently guides policy learning, surpassing rewards based on state-of-the-art pre-trained representation models.","sentences":["Recently, there has been considerable attention towards leveraging large language models (LLMs) to enhance decision-making processes.","However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details.","To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of multimodal LLMs to provide automated preference feedback solely from image inputs to guide decision-making.","In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback.","Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective.","Experimental evaluation of the algorithm's preference accuracy demonstrates its effective generalization ability to new tasks.","Furthermore, performance on Meta-World tasks reveals that CriticGPT's reward model efficiently guides policy learning, surpassing rewards based on state-of-the-art pre-trained representation models."],"url":"http://arxiv.org/abs/2402.14245v1","category":"cs.RO"}
{"created":"2024-02-22 03:11:09","title":"MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint","abstract":"Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a \"mentor\", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into subgoals to guide the right learning direction, subgoals that are too difficult or too easy can still hinder downstream learning efficiency. We propose the Dynamic Distance Constraint (DDC) mechanism dynamically adjusting the space of optional subgoals. Thus MENTOR can generate subgoals matching the low-level policy learning process from easy to hard. Extensive experiments demonstrate that MENTOR uses a small amount of human feedback to achieve significant improvement in complex tasks with sparse rewards.","sentences":["Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially.","However, current methods struggle to find suitable subgoals for ensuring a stable learning process.","Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space.","To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR).","MENTOR acts as a \"mentor\", incorporating human feedback into high-level policy learning, to find better subgoals.","As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training.","Furthermore, although humans can simply break down tasks into subgoals to guide the right learning direction, subgoals that are too difficult or too easy can still hinder downstream learning efficiency.","We propose the Dynamic Distance Constraint (DDC) mechanism dynamically adjusting the space of optional subgoals.","Thus MENTOR can generate subgoals matching the low-level policy learning process from easy to hard.","Extensive experiments demonstrate that MENTOR uses a small amount of human feedback to achieve significant improvement in complex tasks with sparse rewards."],"url":"http://arxiv.org/abs/2402.14244v1","category":"cs.AI"}
{"created":"2024-02-22 02:54:43","title":"A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets","abstract":"In environments where RGB images are inadequate, pressure maps is a viable alternative, garnering scholarly attention. This study introduces a novel self-supervised pressure map keypoint detection (SPMKD) method, addressing the current gap in specialized designs for human keypoint extraction from pressure maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model, which is a robust framework that integrates a lightweight encoder for precise human keypoint detection, a fuser for efficient gradient propagation, and a decoder that transforms human keypoints into reconstructed pressure maps. This structure is further enhanced by the Classification-to-Regression Weight Transfer (CRWT) method, which fine-tunes accuracy through initial classification task training. This innovation not only enhances human keypoint generalization without manual annotations but also showcases remarkable efficiency and generalization, evidenced by a reduction to only $5.96\\%$ in FLOPs and $1.11\\%$ in parameter count compared to the baseline methods.","sentences":["In environments where RGB images are inadequate, pressure maps is a viable alternative, garnering scholarly attention.","This study introduces a novel self-supervised pressure map keypoint detection (SPMKD) method, addressing the current gap in specialized designs for human keypoint extraction from pressure maps.","Central to our contribution is the Encoder-Fuser-Decoder (EFD) model, which is a robust framework that integrates a lightweight encoder for precise human keypoint detection, a fuser for efficient gradient propagation, and a decoder that transforms human keypoints into reconstructed pressure maps.","This structure is further enhanced by the Classification-to-Regression Weight Transfer (CRWT) method, which fine-tunes accuracy through initial classification task training.","This innovation not only enhances human keypoint generalization without manual annotations but also showcases remarkable efficiency and generalization, evidenced by a reduction to only $5.96\\%$ in FLOPs and $1.11\\%$ in parameter count compared to the baseline methods."],"url":"http://arxiv.org/abs/2402.14241v1","category":"cs.CV"}
{"created":"2024-02-22 02:36:14","title":"Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning","abstract":"Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techniques, the proposed method demonstrates superior design efficiency, highlighting the substantial potential of RL in circuit design automation.","sentences":["Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers.","However, manual design methods tend to have exceedingly low-efficiency.","This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs.","The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers.","Thus, it significantly reduces the subjectivity and constraints associated with circuit design.","The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods.","In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs.","Furthermore, compared to existing circuit automation design techniques, the proposed method demonstrates superior design efficiency, highlighting the substantial potential of RL in circuit design automation."],"url":"http://arxiv.org/abs/2402.14236v1","category":"cs.LG"}
{"created":"2024-02-22 02:31:57","title":"Stand-Up Indulgent Gathering on Rings","abstract":"We consider a collection of $k \\geq 2$ robots that evolve in a ring-shaped network without common orientation, and address a variant of the crash-tolerant gathering problem called the \\emph{Stand-Up Indulgent Gathering} (SUIG): given a collection of robots, if no robot crashes, robots have to meet at the same arbitrary location, not known beforehand, in finite time; if one robot or more robots crash on the same location, the remaining correct robots gather at the location of the crashed robots. We aim at characterizing the solvability of the SUIG problem without multiplicity detection capability.","sentences":["We consider a collection of $k \\geq 2$ robots that evolve in a ring-shaped network without common orientation, and address a variant of the crash-tolerant gathering problem called the \\emph{Stand-Up Indulgent Gathering} (SUIG): given a collection of robots, if no robot crashes, robots have to meet at the same arbitrary location, not known beforehand, in finite time; if one robot or more robots crash on the same location, the remaining correct robots gather at the location of the crashed robots.","We aim at characterizing the solvability of the SUIG problem without multiplicity detection capability."],"url":"http://arxiv.org/abs/2402.14233v1","category":"cs.DC"}
{"created":"2024-02-22 02:21:59","title":"MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems","abstract":"In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement. The rise of Consumer-to-Consumer (C2C) recommendation systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend. However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C recommendation datasets that lack in item attributes, user diversity, and scale. The intricacy of C2C recommendation systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs. Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C recommendations, sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023. MerRec not only includes standard features such as user_id, item_id, and session_id, but also unique elements like timestamped action types, product taxonomy, and textual product attributes, offering a comprehensive dataset for research. This dataset, extensively evaluated across six recommendation tasks, establishes a new benchmark for the development of advanced recommendation algorithms in real-world scenarios, bridging the gap between academia and industry and propelling the study of C2C recommendations.","sentences":["In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement.","The rise of Consumer-to-Consumer (C2C) recommendation systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend.","However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C recommendation datasets that lack in item attributes, user diversity, and scale.","The intricacy of C2C recommendation systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs.","Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C recommendations, sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023.","MerRec not only includes standard features such as user_id, item_id, and session_id, but also unique elements like timestamped action types, product taxonomy, and textual product attributes, offering a comprehensive dataset for research.","This dataset, extensively evaluated across six recommendation tasks, establishes a new benchmark for the development of advanced recommendation algorithms in real-world scenarios, bridging the gap between academia and industry and propelling the study of C2C recommendations."],"url":"http://arxiv.org/abs/2402.14230v1","category":"cs.IR"}
{"created":"2024-02-22 02:20:08","title":"COPR: Continual Human Preference Learning via Optimal Policy Regularization","abstract":"Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF and avoids over-emphasizing unbalanced objectives. We also provide formal proof for the learnability of COPR. The experimental results show that COPR outperforms strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4 evaluations and human assessment. Furthermore, we validate the robustness of COPR under various CL settings, including different backbones, replay memory sizes, and learning orders.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences.","Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment.","Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process.","Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs.","To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory.","COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL.","It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF and avoids over-emphasizing unbalanced objectives.","We also provide formal proof for the learnability of COPR.","The experimental results show that COPR outperforms strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4 evaluations and human assessment.","Furthermore, we validate the robustness of COPR under various CL settings, including different backbones, replay memory sizes, and learning orders."],"url":"http://arxiv.org/abs/2402.14228v1","category":"cs.LG"}
{"created":"2024-02-22 02:08:35","title":"SICRN: Advancing Speech Enhancement through State Space Model and Inplace Convolution Techniques","abstract":"Speech enhancement aims to improve speech quality and intelligibility, especially in noisy environments where background noise degrades speech signals. Currently, deep learning methods achieve great success in speech enhancement, e.g. the representative convolutional recurrent neural network (CRN) and its variants. However, CRN typically employs consecutive downsampling and upsampling convolution for frequency modeling, which destroys the inherent structure of the signal over frequency. Additionally, convolutional layers lacks of temporal modelling abilities. To address these issues, we propose an innovative module combing a State space model and Inplace Convolution (SIC), and to replace the conventional convolution in CRN, called SICRN. Specifically, a dual-path multidimensional State space model captures the global frequencies dependency and long-term temporal dependencies. Meanwhile, the 2D-inplace convolution is used to capture the local structure, which abandons the downsampling and upsampling. Systematic evaluations on the public INTERSPEECH 2020 DNS challenge dataset demonstrate SICRN's efficacy. Compared to strong baselines, SICRN achieves performance close to state-of-the-art while having advantages in model parameters, computations, and algorithmic delay. The proposed SICRN shows great promise for improved speech enhancement.","sentences":["Speech enhancement aims to improve speech quality and intelligibility, especially in noisy environments where background noise degrades speech signals.","Currently, deep learning methods achieve great success in speech enhancement, e.g. the representative convolutional recurrent neural network (CRN) and its variants.","However, CRN typically employs consecutive downsampling and upsampling convolution for frequency modeling, which destroys the inherent structure of the signal over frequency.","Additionally, convolutional layers lacks of temporal modelling abilities.","To address these issues, we propose an innovative module combing a State space model and Inplace Convolution (SIC), and to replace the conventional convolution in CRN, called SICRN.","Specifically, a dual-path multidimensional State space model captures the global frequencies dependency and long-term temporal dependencies.","Meanwhile, the 2D-inplace convolution is used to capture the local structure, which abandons the downsampling and upsampling.","Systematic evaluations on the public INTERSPEECH 2020 DNS challenge dataset demonstrate SICRN's efficacy.","Compared to strong baselines, SICRN achieves performance close to state-of-the-art while having advantages in model parameters, computations, and algorithmic delay.","The proposed SICRN shows great promise for improved speech enhancement."],"url":"http://arxiv.org/abs/2402.14225v1","category":"eess.AS"}
{"created":"2024-02-22 01:33:31","title":"Moonwalk: Inverse-Forward Differentiation","abstract":"Backpropagation, while effective for gradient computation, falls short in addressing memory consumption, limiting scalability. This work explores forward-mode gradient computation as an alternative in invertible networks, showing its potential to reduce the memory footprint without substantial drawbacks. We introduce a novel technique based on a vector-inverse-Jacobian product that accelerates the computation of forward gradients while retaining the advantages of memory reduction and preserving the fidelity of true gradients. Our method, Moonwalk, has a time complexity linear in the depth of the network, unlike the quadratic time complexity of na\\\"ive forward, and empirically reduces computation time by several orders of magnitude without allocating more memory. We further accelerate Moonwalk by combining it with reverse-mode differentiation to achieve time complexity comparable with backpropagation while maintaining a much smaller memory footprint. Finally, we showcase the robustness of our method across several architecture choices. Moonwalk is the first forward-based method to compute true gradients in invertible networks in computation time comparable to backpropagation and using significantly less memory.","sentences":["Backpropagation, while effective for gradient computation, falls short in addressing memory consumption, limiting scalability.","This work explores forward-mode gradient computation as an alternative in invertible networks, showing its potential to reduce the memory footprint without substantial drawbacks.","We introduce a novel technique based on a vector-inverse-Jacobian product that accelerates the computation of forward gradients while retaining the advantages of memory reduction and preserving the fidelity of true gradients.","Our method, Moonwalk, has a time complexity linear in the depth of the network, unlike the quadratic time complexity of na\\\"ive forward, and empirically reduces computation time by several orders of magnitude without allocating more memory.","We further accelerate Moonwalk by combining it with reverse-mode differentiation to achieve time complexity comparable with backpropagation while maintaining a much smaller memory footprint.","Finally, we showcase the robustness of our method across several architecture choices.","Moonwalk is the first forward-based method to compute true gradients in invertible networks in computation time comparable to backpropagation and using significantly less memory."],"url":"http://arxiv.org/abs/2402.14212v1","category":"cs.LG"}
{"created":"2024-02-22 01:20:51","title":"Content Conditional Debiasing for Fair Text Embedding","abstract":"Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embeddings, representing a pioneering effort in achieving conditional independence for fair text embeddings.","sentences":["Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP).","Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications.","In this paper, we propose a novel method for learning fair text embeddings.","We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content.","Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text.","Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups.","Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embeddings, representing a pioneering effort in achieving conditional independence for fair text embeddings."],"url":"http://arxiv.org/abs/2402.14208v1","category":"cs.CL"}
{"created":"2024-02-22 01:20:17","title":"Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models","abstract":"We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.","sentences":["We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.","This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing.","We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking.","STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   ","For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage.","We further gather feedback from experienced Wikipedia editors.","Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%).","The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts."],"url":"http://arxiv.org/abs/2402.14207v1","category":"cs.CL"}
{"created":"2024-02-22 00:24:44","title":"From Adoption to Adaption: Tracing the Diffusion of New Emojis on Twitter","abstract":"In the rapidly evolving landscape of social media, the introduction of new emojis in Unicode release versions presents a structured opportunity to explore digital language evolution. Analyzing a large dataset of sampled English tweets, we examine how newly released emojis gain traction and evolve in meaning. We find that community size of early adopters and emoji semantics are crucial in determining their popularity. Certain emojis experienced notable shifts in the meanings and sentiment associations during the diffusion process. Additionally, we propose a novel framework utilizing language models to extract words and pre-existing emojis with semantically similar contexts, which enhances interpretation of new emojis. The framework demonstrates its effectiveness in improving sentiment classification performance by substituting unknown new emojis with familiar ones. This study offers a new perspective in understanding how new language units are adopted, adapted, and integrated into the fabric of online communication.","sentences":["In the rapidly evolving landscape of social media, the introduction of new emojis in Unicode release versions presents a structured opportunity to explore digital language evolution.","Analyzing a large dataset of sampled English tweets, we examine how newly released emojis gain traction and evolve in meaning.","We find that community size of early adopters and emoji semantics are crucial in determining their popularity.","Certain emojis experienced notable shifts in the meanings and sentiment associations during the diffusion process.","Additionally, we propose a novel framework utilizing language models to extract words and pre-existing emojis with semantically similar contexts, which enhances interpretation of new emojis.","The framework demonstrates its effectiveness in improving sentiment classification performance by substituting unknown new emojis with familiar ones.","This study offers a new perspective in understanding how new language units are adopted, adapted, and integrated into the fabric of online communication."],"url":"http://arxiv.org/abs/2402.14187v1","category":"cs.CY"}
{"created":"2024-02-22 00:24:03","title":"On Organizational Principles of Neural Systems","abstract":"How do we understand natural intelligence? Inspired by classical embodied cognition and the emerging multimodal interaction, we study the organizational principles of neural systems at three levels (device/implementation, circuit/algorithm, and system/computational) in this survey paper. Our main contributions consist of 1) Device/Implementation level: reproducibility of macroscopic states in polychronization neural group (PNG) serves as the physical basis of associative memory; 2) Circuit/Algorithm level: canonical microcircuits implement a universal predictive coding algorithm underlying all high-level cognitive functions; 3) System/Computational level: modeling sensorimotor interaction for embodied cognition plays a fundamental role in understanding natural intelligence. At each level, we use mathematical models as our abstractions and study their organizational principles (e.g., entropy reduction, predictive coding, and coordinate transformation). The unifying theme is that natural intelligence evolves by recycling a simple navigation trick in different coordinates (reference frames) hundreds of thousands of times. By representing space as a latent sequence, the neocortex solves the embodied cognition problem by building parallel cognitive maps at a large scale. We hope this survey article can inspire new research at the intersection of neuroscience and learning systems, helping bridge the gap between natural and artificial intelligence.","sentences":["How do we understand natural intelligence?","Inspired by classical embodied cognition and the emerging multimodal interaction, we study the organizational principles of neural systems at three levels (device/implementation, circuit/algorithm, and system/computational) in this survey paper.","Our main contributions consist of 1) Device/Implementation level: reproducibility of macroscopic states in polychronization neural group (PNG) serves as the physical basis of associative memory; 2) Circuit/Algorithm level: canonical microcircuits implement a universal predictive coding algorithm underlying all high-level cognitive functions; 3) System/Computational level: modeling sensorimotor interaction for embodied cognition plays a fundamental role in understanding natural intelligence.","At each level, we use mathematical models as our abstractions and study their organizational principles (e.g., entropy reduction, predictive coding, and coordinate transformation).","The unifying theme is that natural intelligence evolves by recycling a simple navigation trick in different coordinates (reference frames) hundreds of thousands of times.","By representing space as a latent sequence, the neocortex solves the embodied cognition problem by building parallel cognitive maps at a large scale.","We hope this survey article can inspire new research at the intersection of neuroscience and learning systems, helping bridge the gap between natural and artificial intelligence."],"url":"http://arxiv.org/abs/2402.14186v1","category":"q-bio.NC"}
{"created":"2024-02-22 00:03:45","title":"Parking of Connected Automated Vehicles: Vehicle Control, Parking Assignment, and Multi-agent Simulation","abstract":"This paper introduces a novel approach to optimize the parking efficiency for fleets of Connected and Automated Vehicles (CAVs). We present a novel multi-vehicle parking simulator, equipped with hierarchical path planning and collision avoidance capabilities for individual CAVs. The simulator is designed to capture the key decision-making processes in parking, from low-level vehicle control to high-level parking assignment, and it enables the effective assessment of parking strategies for large fleets of ground vehicles. We formulate and compare different strategic parking spot assignments to minimize a collective cost. While the proposed framework is designed to optimize various objective functions, we choose the total parking time for the experiment, as it is closely related to the reduction of vehicles' energy consumption and greenhouse gas emissions. We validate the effectiveness of the proposed strategies through empirical evaluation against a dataset of real-world parking lot dynamics, realizing a substantial reduction in parking time by up to 43.8%. This improvement is attributed to the synergistic benefits of driving automation, the utilization of shared infrastructure state data, the exclusion of pedestrian traffic, and the real-time computation of optimal parking spot allocation.","sentences":["This paper introduces a novel approach to optimize the parking efficiency for fleets of Connected and Automated Vehicles (CAVs).","We present a novel multi-vehicle parking simulator, equipped with hierarchical path planning and collision avoidance capabilities for individual CAVs.","The simulator is designed to capture the key decision-making processes in parking, from low-level vehicle control to high-level parking assignment, and it enables the effective assessment of parking strategies for large fleets of ground vehicles.","We formulate and compare different strategic parking spot assignments to minimize a collective cost.","While the proposed framework is designed to optimize various objective functions, we choose the total parking time for the experiment, as it is closely related to the reduction of vehicles' energy consumption and greenhouse gas emissions.","We validate the effectiveness of the proposed strategies through empirical evaluation against a dataset of real-world parking lot dynamics, realizing a substantial reduction in parking time by up to 43.8%.","This improvement is attributed to the synergistic benefits of driving automation, the utilization of shared infrastructure state data, the exclusion of pedestrian traffic, and the real-time computation of optimal parking spot allocation."],"url":"http://arxiv.org/abs/2402.14183v1","category":"eess.SY"}
{"created":"2024-02-22 00:01:02","title":"Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization","abstract":"Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability. Informally, we lack a formulaic or intuitive understanding of what and how models learn from code. Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers. In this paper, we report negative results from our investigation of explainability of language models in code summarization through the lens of human comprehension. We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code summarization tasks. To approximate language model focus, we employ a state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP (SHapley Additive exPlanations), to identify which code tokens influence that generation of summaries. Using these settings, we find no statistically significant relationship between language models' focus and human programmers' attention. Furthermore, alignment between model and human foci in this setting does not seem to dictate the quality of the LLM-generated summaries. Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general, including the training mechanisms of language models for code, whether there is an alignment between human and model attention on code, whether human attention can improve the development of language models, and what other model focus measures are appropriate for improving explainability.","sentences":["Recent language models have demonstrated proficiency in summarizing source code.","However, as in many other domains of machine learning, language models of code lack sufficient explainability.","Informally, we lack a formulaic or intuitive understanding of what and how models learn from code.","Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers.","In this paper, we report negative results from our investigation of explainability of language models in code summarization through the lens of human comprehension.","We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code summarization tasks.","To approximate language model focus, we employ a state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP (SHapley Additive exPlanations), to identify which code tokens influence that generation of summaries.","Using these settings, we find no statistically significant relationship between language models' focus and human programmers' attention.","Furthermore, alignment between model and human foci in this setting does not seem to dictate the quality of the LLM-generated summaries.","Our study highlights an inability to align human focus with SHAP-based model focus measures.","This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general, including the training mechanisms of language models for code, whether there is an alignment between human and model attention on code, whether human attention can improve the development of language models, and what other model focus measures are appropriate for improving explainability."],"url":"http://arxiv.org/abs/2402.14182v1","category":"cs.SE"}
{"created":"2024-02-21 23:43:04","title":"Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media","abstract":"Ethnic media, which caters to diaspora communities in host nations, serves as a vital platform for these communities to both produce content and access information. Rather than utilizing the language of the host nation, ethnic media delivers news in the language of the immigrant community. For instance, in the USA, Bangla ethnic media presents news in Bangla rather than English. This research delves into the prospective integration of large language models (LLM) and multi-lingual machine translations (MMT) within the ethnic media industry. It centers on the transformative potential of using LLM in MMT in various facets of news translation, searching, and categorization. The paper outlines a theoretical framework elucidating the integration of LLM and MMT into the news searching and translation processes for ethnic media. Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT in news translation procedures.","sentences":["Ethnic media, which caters to diaspora communities in host nations, serves as a vital platform for these communities to both produce content and access information.","Rather than utilizing the language of the host nation, ethnic media delivers news in the language of the immigrant community.","For instance, in the USA, Bangla ethnic media presents news in Bangla rather than English.","This research delves into the prospective integration of large language models (LLM) and multi-lingual machine translations (MMT) within the ethnic media industry.","It centers on the transformative potential of using LLM in MMT in various facets of news translation, searching, and categorization.","The paper outlines a theoretical framework elucidating the integration of LLM and MMT into the news searching and translation processes for ethnic media.","Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT in news translation procedures."],"url":"http://arxiv.org/abs/2402.14179v1","category":"cs.CL"}
{"created":"2024-02-21 23:22:32","title":"Blending Data-Driven Priors in Dynamic Games","abstract":"As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that permits modulation between task-driven and data-driven behaviors. We propose an efficient algorithm for computing multimodal approximate feedback Nash equilibrium strategies of KLGame in real time. Through a series of simulated and real-world autonomous driving scenarios, we demonstrate that KLGame policies can more effectively incorporate guidance from the reference policy and account for noisily-rational human behaviors versus non-regularized baselines.","sentences":["As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question.","Existing dynamic game formulations assume all agents are task-driven and behave optimally.","However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm.","In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy.","We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy.","Our method incorporates, for each decision maker, a tunable parameter that permits modulation between task-driven and data-driven behaviors.","We propose an efficient algorithm for computing multimodal approximate feedback Nash equilibrium strategies of KLGame in real time.","Through a series of simulated and real-world autonomous driving scenarios, we demonstrate that KLGame policies can more effectively incorporate guidance from the reference policy and account for noisily-rational human behaviors versus non-regularized baselines."],"url":"http://arxiv.org/abs/2402.14174v1","category":"cs.RO"}
{"created":"2024-02-21 23:06:39","title":"Fragmentation of the Giant Pairing Vibration in 14C induced by many-body processes","abstract":"We present a theoretical framework for treating the full excitation spectrum of J{\\pi} = 0+ pair addition modes, including the well-known low-lying and bound Pairing Vibration on par with the predicted Giant Pairing Vibration lying in the continuum. Our formalism includes the coupling to low-energy collective quadrupole modes of the core, in such a way that both single-particle self-energy effects and the pairing interaction induced by phonon exchange are accounted for. The theory is applied to the case of the excitation spectrum of 14C, recently populated by two-neutron transfer reactions.","sentences":["We present a theoretical framework for treating the full excitation spectrum of J{\\pi} = 0+ pair addition modes, including the well-known low-lying and bound Pairing Vibration on par with the predicted Giant Pairing Vibration lying in the continuum.","Our formalism includes the coupling to low-energy collective quadrupole modes of the core, in such a way that both single-particle self-energy effects and the pairing interaction induced by phonon exchange are accounted for.","The theory is applied to the case of the excitation spectrum of 14C, recently populated by two-neutron transfer reactions."],"url":"http://arxiv.org/abs/2402.14166v1","category":"nucl-th"}
{"created":"2024-02-21 23:01:38","title":"On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study","abstract":"Recently, large language models (LLMs) have taken the spotlight in natural language processing. Further, integrating LLMs with vision enables the users to explore emergent abilities with multimodal data. Visual language models (VLMs), such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on various visio-linguistic tasks. Consequently, there are enormous applications of large models that could be potentially used in the biomedical imaging field. Along that direction, there is a lack of related work to show the ability of large models to diagnose the diseases. In this work, we study the zero-shot and few-shot robustness of VLMs on the medical imaging analysis tasks. Our comprehensive experiments demonstrate the effectiveness of VLMs in analyzing biomedical images such as brain MRIs, microscopic images of blood cells, and chest X-rays.","sentences":["Recently, large language models (LLMs) have taken the spotlight in natural language processing.","Further, integrating LLMs with vision enables the users to explore emergent abilities with multimodal data.","Visual language models (VLMs), such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on various visio-linguistic tasks.","Consequently, there are enormous applications of large models that could be potentially used in the biomedical imaging field.","Along that direction, there is a lack of related work to show the ability of large models to diagnose the diseases.","In this work, we study the zero-shot and few-shot robustness of VLMs on the medical imaging analysis tasks.","Our comprehensive experiments demonstrate the effectiveness of VLMs in analyzing biomedical images such as brain MRIs, microscopic images of blood cells, and chest X-rays."],"url":"http://arxiv.org/abs/2402.14162v1","category":"cs.CV"}
{"created":"2024-02-21 22:57:49","title":"Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement","abstract":"Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree. During RSD's drafting, the tree is built by either Gumbel-Top-$k$ trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of LLM. We empirically evaluate RSD with Llama 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at LLM.","sentences":["Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel.","Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding.","However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability.","Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods.","None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices.","We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree.","During RSD's drafting, the tree is built by either Gumbel-Top-$k$ trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of LLM.","We empirically evaluate RSD with Llama 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at LLM."],"url":"http://arxiv.org/abs/2402.14160v1","category":"cs.LG"}
{"created":"2024-02-21 22:44:46","title":"Mapping the Landscape of Independent Food Delivery Platforms in the United States","abstract":"Beyond the well-known giants like Uber Eats and DoorDash, there are hundreds of independent food delivery platforms in the United States. However, little is known about the sociotechnical landscape of these ``indie'' platforms. In this paper, we analyzed these platforms to understand why they were created, how they operate, and what technologies they use. We collected data on 495 indie platforms and detailed survey responses from 29 platforms. We found that personalized, timely service is a central value of indie platforms, as is a sense of responsibility to the local community they serve. Indie platforms are motivated to provide fair rates for restaurants and couriers. These alternative business practices differentiate them from mainstream platforms. Though indie platforms have plans to expand, a lack of customizability in off-the-shelf software prevents independent platforms from personalizing services for their local communities. We show that these platforms are a widespread and longstanding fixture of the food delivery market. We illustrate the diversity of motivations and values to explain why a one-size-fits-all support is insufficient, and we discuss the siloing of technology that inhibits platforms' growth. Through these insights, we aim to promote future HCI research into the potential development of public-interest technologies for local food delivery.","sentences":["Beyond the well-known giants like Uber Eats and DoorDash, there are hundreds of independent food delivery platforms in the United States.","However, little is known about the sociotechnical landscape of these ``indie'' platforms.","In this paper, we analyzed these platforms to understand why they were created, how they operate, and what technologies they use.","We collected data on 495 indie platforms and detailed survey responses from 29 platforms.","We found that personalized, timely service is a central value of indie platforms, as is a sense of responsibility to the local community they serve.","Indie platforms are motivated to provide fair rates for restaurants and couriers.","These alternative business practices differentiate them from mainstream platforms.","Though indie platforms have plans to expand, a lack of customizability in off-the-shelf software prevents independent platforms from personalizing services for their local communities.","We show that these platforms are a widespread and longstanding fixture of the food delivery market.","We illustrate the diversity of motivations and values to explain why a one-size-fits-all support is insufficient, and we discuss the siloing of technology that inhibits platforms' growth.","Through these insights, we aim to promote future HCI research into the potential development of public-interest technologies for local food delivery."],"url":"http://arxiv.org/abs/2402.14159v1","category":"cs.HC"}
{"created":"2024-02-21 22:36:37","title":"Beyond Diagonal RIS: Key to Next-Generation Integrated Sensing and Communications?","abstract":"Reconfigurable intelligent surface (RIS) is a promising technology that has the potential to revolutionize wireless systems by introducing unprecedented flexibility and adaptability in the creation of smart wireless channels. Recent research on integrated sensing and communication (ISAC) systems has demonstrated that RIS platforms enable enhanced signal quality, coverage, and link capacity. In this paper, we explore the application of fully-connected beyond diagonal RIS (BD-RIS) to ISAC systems. BD-RIS introduces additional degrees of freedom by allowing non-zero off-diagonal elements for the scattering matrix, potentially enabling further functionalities and performance enhancements. In particular, we consider the joint design objective of maximizing the weighted sum of the signal-to-noise ratio (SNR) at the radar receiver and communications users by leveraging the extra degrees-of-freedom offered in the BD-RIS setting. These degrees-of-freedom are unleashed by formulating an alternating optimization process over known and auxiliary (latent) variables of such systems. Our numerical results reveal the advantages of deploying BD-RIS in the context of ISAC and the effectiveness of the proposed algorithm by improving the SNR values for both radar and communication users by several orders of magnitude.","sentences":["Reconfigurable intelligent surface (RIS) is a promising technology that has the potential to revolutionize wireless systems by introducing unprecedented flexibility and adaptability in the creation of smart wireless channels.","Recent research on integrated sensing and communication (ISAC) systems has demonstrated that RIS platforms enable enhanced signal quality, coverage, and link capacity.","In this paper, we explore the application of fully-connected beyond diagonal RIS (BD-RIS) to ISAC systems.","BD-RIS introduces additional degrees of freedom by allowing non-zero off-diagonal elements for the scattering matrix, potentially enabling further functionalities and performance enhancements.","In particular, we consider the joint design objective of maximizing the weighted sum of the signal-to-noise ratio (SNR) at the radar receiver and communications users by leveraging the extra degrees-of-freedom offered in the BD-RIS setting.","These degrees-of-freedom are unleashed by formulating an alternating optimization process over known and auxiliary (latent) variables of such systems.","Our numerical results reveal the advantages of deploying BD-RIS in the context of ISAC and the effectiveness of the proposed algorithm by improving the SNR values for both radar and communication users by several orders of magnitude."],"url":"http://arxiv.org/abs/2402.14157v1","category":"eess.SP"}
{"created":"2024-02-21 22:30:57","title":"Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?","abstract":"Task-oriented dialogue systems are expected to handle a constantly expanding set of intents and domains even after they have been deployed to support more and more functionalities. To live up to this expectation, it becomes critical to mitigate the catastrophic forgetting problem (CF) that occurs in continual learning (CL) settings for a task such as intent recognition. While existing dialogue systems research has explored replay-based and regularization-based methods to this end, the effect of domain ordering on the CL performance of intent recognition models remains unexplored. If understood well, domain ordering has the potential to be an orthogonal technique that can be leveraged alongside existing techniques such as experience replay. Our work fills this gap by comparing the impact of three domain-ordering strategies (min-sum path, max-sum path, random) on the CL performance of a generative intent recognition model. Our findings reveal that the min-sum path strategy outperforms the others in reducing catastrophic forgetting when training on the 220M T5-Base model. However, this advantage diminishes with the larger 770M T5-Large model. These results underscores the potential of domain ordering as a complementary strategy for mitigating catastrophic forgetting in continually learning intent recognition models, particularly in resource-constrained scenarios.","sentences":["Task-oriented dialogue systems are expected to handle a constantly expanding set of intents and domains even after they have been deployed to support more and more functionalities.","To live up to this expectation, it becomes critical to mitigate the catastrophic forgetting problem (CF) that occurs in continual learning (CL) settings for a task such as intent recognition.","While existing dialogue systems research has explored replay-based and regularization-based methods to this end, the effect of domain ordering on the CL performance of intent recognition models remains unexplored.","If understood well, domain ordering has the potential to be an orthogonal technique that can be leveraged alongside existing techniques such as experience replay.","Our work fills this gap by comparing the impact of three domain-ordering strategies (min-sum path, max-sum path, random) on the CL performance of a generative intent recognition model.","Our findings reveal that the min-sum path strategy outperforms the others in reducing catastrophic forgetting when training on the 220M T5-Base model.","However, this advantage diminishes with the larger 770M T5-Large model.","These results underscores the potential of domain ordering as a complementary strategy for mitigating catastrophic forgetting in continually learning intent recognition models, particularly in resource-constrained scenarios."],"url":"http://arxiv.org/abs/2402.14155v1","category":"cs.CL"}
{"created":"2024-02-21 22:22:30","title":"BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives","abstract":"We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.","sentences":["We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO).","BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives.","The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems.","We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives.","No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs."],"url":"http://arxiv.org/abs/2402.14151v1","category":"cs.IR"}
{"created":"2024-02-21 22:10:21","title":"Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia","abstract":"AI tools are increasingly deployed in community contexts. However, datasets used to evaluate AI are typically created by developers and annotators outside a given community, which can yield misleading conclusions about AI performance. How might we empower communities to drive the intentional design and curation of evaluation datasets for AI that impacts them? We investigate this question on Wikipedia, an online community with multiple AI-based content moderation tools deployed. We introduce Wikibench, a system that enables communities to collaboratively curate AI evaluation datasets, while navigating ambiguities and differences in perspective through discussion. A field study on Wikipedia shows that datasets curated using Wikibench can effectively capture community consensus, disagreement, and uncertainty. Furthermore, study participants used Wikibench to shape the overall data curation process, including refining label definitions, determining data inclusion criteria, and authoring data statements. Based on our findings, we propose future directions for systems that support community-driven data curation.","sentences":["AI tools are increasingly deployed in community contexts.","However, datasets used to evaluate AI are typically created by developers and annotators outside a given community, which can yield misleading conclusions about AI performance.","How might we empower communities to drive the intentional design and curation of evaluation datasets for AI that impacts them?","We investigate this question on Wikipedia, an online community with multiple AI-based content moderation tools deployed.","We introduce Wikibench, a system that enables communities to collaboratively curate AI evaluation datasets, while navigating ambiguities and differences in perspective through discussion.","A field study on Wikipedia shows that datasets curated using Wikibench can effectively capture community consensus, disagreement, and uncertainty.","Furthermore, study participants used Wikibench to shape the overall data curation process, including refining label definitions, determining data inclusion criteria, and authoring data statements.","Based on our findings, we propose future directions for systems that support community-driven data curation."],"url":"http://arxiv.org/abs/2402.14147v1","category":"cs.HC"}
{"created":"2024-02-21 21:55:29","title":"SecurePose: Automated Face Blurring and Human Movement Kinematics Extraction from Videos Recorded in Clinical Settings","abstract":"Movement disorders are typically diagnosed by consensus-based expert evaluation of clinically acquired patient videos. However, such broad sharing of patient videos poses risks to patient privacy. Face blurring can be used to de-identify videos, but this process is often manual and time-consuming. Available automated face blurring techniques are subject to either excessive, inconsistent, or insufficient facial blurring - all of which can be disastrous for video assessment and patient privacy. Furthermore, assessing movement disorders in these videos is often subjective. The extraction of quantifiable kinematic features can help inform movement disorder assessment in these videos, but existing methods to do this are prone to errors if using pre-blurred videos. We have developed an open-source software called SecurePose that can both achieve reliable face blurring and automated kinematic extraction in patient videos recorded in a clinic setting using an iPad. SecurePose, extracts kinematics using a pose estimation method (OpenPose), tracks and uniquely identifies all individuals in the video, identifies the patient, and performs face blurring. The software was validated on gait videos recorded in outpatient clinic visits of 116 children with cerebral palsy. The validation involved assessing intermediate steps of kinematics extraction and face blurring with manual blurring (ground truth). Moreover, when SecurePose was compared with six selected existing methods, it outperformed other methods in automated face detection and achieved ceiling accuracy in 91.08% less time than a robust manual face blurring method. Furthermore, ten experienced researchers found SecurePose easy to learn and use, as evidenced by the System Usability Scale. The results of this work validated the performance and usability of SecurePose on clinically recorded gait videos for face blurring and kinematics extraction.","sentences":["Movement disorders are typically diagnosed by consensus-based expert evaluation of clinically acquired patient videos.","However, such broad sharing of patient videos poses risks to patient privacy.","Face blurring can be used to de-identify videos, but this process is often manual and time-consuming.","Available automated face blurring techniques are subject to either excessive, inconsistent, or insufficient facial blurring - all of which can be disastrous for video assessment and patient privacy.","Furthermore, assessing movement disorders in these videos is often subjective.","The extraction of quantifiable kinematic features can help inform movement disorder assessment in these videos, but existing methods to do this are prone to errors if using pre-blurred videos.","We have developed an open-source software called SecurePose that can both achieve reliable face blurring and automated kinematic extraction in patient videos recorded in a clinic setting using an iPad.","SecurePose, extracts kinematics using a pose estimation method (OpenPose), tracks and uniquely identifies all individuals in the video, identifies the patient, and performs face blurring.","The software was validated on gait videos recorded in outpatient clinic visits of 116 children with cerebral palsy.","The validation involved assessing intermediate steps of kinematics extraction and face blurring with manual blurring (ground truth).","Moreover, when SecurePose was compared with six selected existing methods, it outperformed other methods in automated face detection and achieved ceiling accuracy in 91.08% less time than a robust manual face blurring method.","Furthermore, ten experienced researchers found SecurePose easy to learn and use, as evidenced by the System Usability Scale.","The results of this work validated the performance and usability of SecurePose on clinically recorded gait videos for face blurring and kinematics extraction."],"url":"http://arxiv.org/abs/2402.14143v1","category":"cs.CV"}
{"created":"2024-02-21 21:53:13","title":"Adiabatic Light Guide with S-shape Strips","abstract":"A light guide is an essential part of many scintillator counters and light collection systems. There is large interest in an adiabatic light guide which has high light transmission while converting the area of the light source to the shape of the photo-detector. We propose a variation of the adiabatic light guide which avoids a 90o twist of the strips, reduces the length of the light pipe, and significantly cuts the cost of production.","sentences":["A light guide is an essential part of many scintillator counters and light collection systems.","There is large interest in an adiabatic light guide which has high light transmission while converting the area of the light source to the shape of the photo-detector.","We propose a variation of the adiabatic light guide which avoids a 90o twist of the strips, reduces the length of the light pipe, and significantly cuts the cost of production."],"url":"http://arxiv.org/abs/2402.14142v1","category":"physics.ins-det"}
{"created":"2024-02-21 21:34:06","title":"QuantTM: Business-Centric Threat Quantification for Risk Management and Cyber Resilience","abstract":"Threat modeling has emerged as a key process for understanding relevant threats within businesses. However, understanding the importance of threat events is rarely driven by the business incorporating the system. Furthermore, prioritization of threat events often occurs based on abstract and qualitative scoring. While such scores enable prioritization, they do not allow the results to be easily interpreted by decision-makers. This can hinder downstream activities, such as discussing security investments and a security control's economic applicability. This article introduces QuantTM, an approach that incorporates views from operational and strategic business representatives to collect threat information during the threat modeling process to measure potential financial loss incurred by a specific threat event. It empowers the analysis of threats' impacts and the applicability of security controls, thus supporting the threat analysis and prioritization from an economic perspective. QuantTM comprises an overarching process for data collection and aggregation and a method for business impact analysis. The performance and feasibility of the QuantTM approach are demonstrated in a real-world case study conducted in a Swiss SME to analyze the impacts of threats and economic benefits of security controls. Secondly, it is shown that employing business impact analysis is feasible and that the supporting prototype exhibits great usability.","sentences":["Threat modeling has emerged as a key process for understanding relevant threats within businesses.","However, understanding the importance of threat events is rarely driven by the business incorporating the system.","Furthermore, prioritization of threat events often occurs based on abstract and qualitative scoring.","While such scores enable prioritization, they do not allow the results to be easily interpreted by decision-makers.","This can hinder downstream activities, such as discussing security investments and a security control's economic applicability.","This article introduces QuantTM, an approach that incorporates views from operational and strategic business representatives to collect threat information during the threat modeling process to measure potential financial loss incurred by a specific threat event.","It empowers the analysis of threats' impacts and the applicability of security controls, thus supporting the threat analysis and prioritization from an economic perspective.","QuantTM comprises an overarching process for data collection and aggregation and a method for business impact analysis.","The performance and feasibility of the QuantTM approach are demonstrated in a real-world case study conducted in a Swiss SME to analyze the impacts of threats and economic benefits of security controls.","Secondly, it is shown that employing business impact analysis is feasible and that the supporting prototype exhibits great usability."],"url":"http://arxiv.org/abs/2402.14140v1","category":"cs.CR"}
{"created":"2024-02-21 20:52:05","title":"An expert system for diagnosing and treating heart disease","abstract":"Timely detection of illnesses is vital to prevent severe infections and ensure effective treatment, as it's always better to prevent diseases than to cure them. Sadly, many patients remain undiagnosed until their conditions worsen, resulting in high death rates. Expert systems offer a solution by automating early-stage diagnoses using a fuzzy rule-based approach. Our study gathered data from various sources, including hospitals, to develop an expert system aimed at identifying early signs of diseases, particularly heart conditions. The diagnostic process involves collecting and processing test results using the expert system, which categorizes disease risks and aids physicians in treatment decisions. By incorporating expert systems into clinical practice, we can improve the accuracy of disease detection and address challenges in patient management, particularly in areas with limited medical resources.","sentences":["Timely detection of illnesses is vital to prevent severe infections and ensure effective treatment, as it's always better to prevent diseases than to cure them.","Sadly, many patients remain undiagnosed until their conditions worsen, resulting in high death rates.","Expert systems offer a solution by automating early-stage diagnoses using a fuzzy rule-based approach.","Our study gathered data from various sources, including hospitals, to develop an expert system aimed at identifying early signs of diseases, particularly heart conditions.","The diagnostic process involves collecting and processing test results using the expert system, which categorizes disease risks and aids physicians in treatment decisions.","By incorporating expert systems into clinical practice, we can improve the accuracy of disease detection and address challenges in patient management, particularly in areas with limited medical resources."],"url":"http://arxiv.org/abs/2402.14128v1","category":"cs.HC"}
{"created":"2024-02-21 20:43:49","title":"DeiSAM: Segment Anything with Deictic Prompting","abstract":"Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as \"The object that is on the desk and behind the cup.\". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions. As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual prompts. Our empirical results demonstrate that DeiSAM is a substantial improvement over purely data-driven baselines for deictic promptable segmentation.","sentences":["Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation.","To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as \"The object that is on the desk and behind the cup.\".","However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios.","To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation.","Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs.","Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions.","As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual prompts.","Our empirical results demonstrate that DeiSAM is a substantial improvement over purely data-driven baselines for deictic promptable segmentation."],"url":"http://arxiv.org/abs/2402.14123v1","category":"cs.LG"}
{"created":"2024-02-21 20:36:08","title":"Masked Matrix Multiplication for Emergent Sparsity","abstract":"Artificial intelligence workloads, especially transformer models, exhibit emergent sparsity in which computations perform selective sparse access to dense data. The workloads are inefficient on hardware designed for dense computations and do not map well onto sparse data representations. We build a vectorized and parallel matrix-multiplication system A X B = C that eliminates unnecessary computations and avoids branches based on a runtime evaluation of sparsity. We use a combination of dynamic code lookup to adapt to the specific sparsity encoded in the B matrix and preprocessing of sparsity maps of the A and B matrices to compute conditional branches once for the whole computation. For a wide range of sparsity, from 60% to 95% zeros, our implementation performs fewer instructions and increases performance when compared with Intel MKL's dense or sparse matrix multiply routines. Benefits can be as large as 2 times speedup and 4 times fewer instructions.","sentences":["Artificial intelligence workloads, especially transformer models, exhibit emergent sparsity in which computations perform selective sparse access to dense data.","The workloads are inefficient on hardware designed for dense computations and do not map well onto sparse data representations.","We build a vectorized and parallel matrix-multiplication system A X B = C that eliminates unnecessary computations and avoids branches based on a runtime evaluation of sparsity.","We use a combination of dynamic code lookup to adapt to the specific sparsity encoded in the B matrix and preprocessing of sparsity maps of the A and B matrices to compute conditional branches once for the whole computation.","For a wide range of sparsity, from 60% to 95% zeros, our implementation performs fewer instructions and increases performance when compared with Intel MKL's dense or sparse matrix multiply routines.","Benefits can be as large as 2 times speedup and 4 times fewer instructions."],"url":"http://arxiv.org/abs/2402.14118v1","category":"cs.DS"}
{"created":"2024-02-21 20:30:45","title":"FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models","abstract":"One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com","sentences":["One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities.","However, there exist few resources to evaluate this type of question-answering capability among large language models.","To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base.","We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context.","We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com"],"url":"http://arxiv.org/abs/2402.14116v1","category":"cs.CL"}
{"created":"2024-02-21 20:28:37","title":"Saturation of $k$-chains in the Boolean lattice","abstract":"Given a set $X$, a collection $\\mathcal{F} \\subset \\mathcal{P}(X)$ is said to be $k$-Sperner if it does not contain a chain of length $k+1$ under set inclusion and it is saturated if it is maximal with respect to this probability. Gerbner et al. proved that the smallest saturated $k$-Sperner system contains at least $2^{k/2-1}$ elements, and later, Morrison, Noel, and Scott showed that the smallest such set contains no more than $2^{0.976723k}$ elements. We improve both the upper and lower bounds, showing that the size of the smallest saturated $k$-Sperner system lies between $\\sqrt{k}2^{k/2}$ and $2^{0.950978k}$.","sentences":["Given a set $X$, a collection $\\mathcal{F} \\subset \\mathcal{P}(X)$ is said to be $k$-Sperner if it does not contain a chain of length $k+1$ under set inclusion and it is saturated if it is maximal with respect to this probability.","Gerbner et al. proved that the smallest saturated $k$-Sperner system contains at least $2^{k/2-1}$ elements, and later, Morrison, Noel, and Scott showed that the smallest such set contains no more than $2^{0.976723k}$ elements.","We improve both the upper and lower bounds, showing that the size of the smallest saturated $k$-Sperner system lies between $\\sqrt{k}2^{k/2}$ and $2^{0.950978k}$."],"url":"http://arxiv.org/abs/2402.14113v1","category":"math.CO"}
{"created":"2024-02-21 19:53:36","title":"Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation","abstract":"In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for moral sentiment. We demonstrate that our framework surpasses the previous SOTA in capturing the annotators' individual perspectives with as little as 25% of the original annotation budget on two datasets. Furthermore, our framework results in more equitable models, reducing the performance disparity among annotators.","sentences":["In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations.","In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling.","We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator.","Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator.","To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for moral sentiment.","We demonstrate that our framework surpasses the previous SOTA in capturing the annotators' individual perspectives with as little as 25% of the original annotation budget on two datasets.","Furthermore, our framework results in more equitable models, reducing the performance disparity among annotators."],"url":"http://arxiv.org/abs/2402.14101v1","category":"cs.CL"}
{"created":"2024-02-21 19:45:06","title":"EyeTrans: Merging Human and Machine Attention for Neural Code Summarization","abstract":"Neural code summarization leverages deep learning models to automatically generate brief natural language summaries of code snippets. The development of Transformer models has led to extensive use of attention during model design. While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention, that is, where programmers focus while examining and comprehending code. In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code summarization. To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention with machine attention in the Transformer architecture, and (3) we conduct comprehensive experiments on two code summarization tasks to demonstrate the effectiveness of incorporating human attention into Transformers. Integrating human attention leads to an improvement of up to 29.91% in Functional Summarization and up to 6.39% in General Code Summarization performance, demonstrating the substantial benefits of this combination. We further explore performance in terms of robustness and efficiency by creating challenging summarization scenarios in which EyeTrans exhibits interesting properties. We also visualize the attention map to depict the simplifying effect of machine attention in the Transformer by incorporating human attention. This work has the potential to propel AI research in software engineering by introducing more human-centered approaches and data.","sentences":["Neural code summarization leverages deep learning models to automatically generate brief natural language summaries of code snippets.","The development of Transformer models has led to extensive use of attention during model design.","While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention, that is, where programmers focus while examining and comprehending code.","In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code summarization.","To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention with machine attention in the Transformer architecture, and (3) we conduct comprehensive experiments on two code summarization tasks to demonstrate the effectiveness of incorporating human attention into Transformers.","Integrating human attention leads to an improvement of up to 29.91% in Functional Summarization and up to 6.39% in General Code Summarization performance, demonstrating the substantial benefits of this combination.","We further explore performance in terms of robustness and efficiency by creating challenging summarization scenarios in which EyeTrans exhibits interesting properties.","We also visualize the attention map to depict the simplifying effect of machine attention in the Transformer by incorporating human attention.","This work has the potential to propel AI research in software engineering by introducing more human-centered approaches and data."],"url":"http://arxiv.org/abs/2402.14096v1","category":"cs.SE"}
{"created":"2024-02-21 19:45:05","title":"Zero-shot generalization across architectures for visual classification","abstract":"Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.","sentences":["Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear.","Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures.","Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth.","Code is available at https://github.com/dyballa/zero-shot-generalization."],"url":"http://arxiv.org/abs/2402.14095v1","category":"cs.CV"}
{"created":"2024-02-21 19:29:14","title":"Social Environment Design","abstract":"Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.","sentences":["Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making.","This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities.","The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation.","We highlight key open problems for future research in AI-based policy-making.","By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making."],"url":"http://arxiv.org/abs/2402.14090v1","category":"cs.AI"}
{"created":"2024-02-21 19:20:06","title":"LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons","abstract":"Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classification tasks respectively. We show that conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen is also practical -- it only needs a single GPU to generate data at scale. It works well with open-access LLMs, and its cost is one-fifth of the cost of GPT4-based multilingual data generation.","sentences":["Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons.","However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization.","We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale.","Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation.","Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classification tasks respectively.","We show that conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen is also practical -- it only needs a single GPU to generate data at scale.","It works well with open-access LLMs, and its cost is one-fifth of the cost of GPT4-based multilingual data generation."],"url":"http://arxiv.org/abs/2402.14086v1","category":"cs.CL"}
{"created":"2024-02-21 19:17:28","title":"Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping","abstract":"While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than standard $A^*$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of $A^*$. This model is then fine-tuned via expert iterations to perform fewer search steps than $A^*$ search while still generating an optimal plan. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\\times$ smaller model size and a 10$\\times$ smaller training dataset. We also demonstrate how Searchformer scales to larger and more complex decision making tasks like Sokoban with improved percentage of solved tasks and shortened search dynamics.","sentences":["While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks.","In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than standard $A^*$ search.","Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of $A^*$. This model is then fine-tuned via expert iterations to perform fewer search steps than $A^*$ search while still generating an optimal plan.","In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning.","In our ablation studies on maze navigation, we find that Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\\times$ smaller model size and a 10$\\times$ smaller training dataset.","We also demonstrate how Searchformer scales to larger and more complex decision making tasks like Sokoban with improved percentage of solved tasks and shortened search dynamics."],"url":"http://arxiv.org/abs/2402.14083v1","category":"cs.AI"}
{"created":"2024-02-21 19:10:08","title":"Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes","abstract":"While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corresponding time series using the concept of the most informative timestamps. Our unmixing classification approach involves maximizing the likelihood across all the mixed noisy time series sequences of varying lengths. This stochastic approach allows us to learn not only within a single type of noisy time series data but also across many underlying stochastic processes, giving us a way to learn multiple dynamical models in an integrated and robust manner. The different learned latent stochastic models allow us to generate specific sub-type forecasting. We provide several quantitative comparisons demonstrating the performance of our approach.","sentences":["While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging.","Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process.","For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging.","Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels.","More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code.","Then, conditioned on each assigned motion code, we infer a sparse approximation of the corresponding time series using the concept of the most informative timestamps.","Our unmixing classification approach involves maximizing the likelihood across all the mixed noisy time series sequences of varying lengths.","This stochastic approach allows us to learn not only within a single type of noisy time series data but also across many underlying stochastic processes, giving us a way to learn multiple dynamical models in an integrated and robust manner.","The different learned latent stochastic models allow us to generate specific sub-type forecasting.","We provide several quantitative comparisons demonstrating the performance of our approach."],"url":"http://arxiv.org/abs/2402.14081v1","category":"cs.LG"}
{"created":"2024-02-21 19:09:53","title":"Efficient Normalized Conformal Prediction and Uncertainty Quantification for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests","abstract":"Deep learning models are being adopted and applied on various critical decision-making tasks, yet they are trained to provide point predictions without providing degrees of confidence. The trustworthiness of deep learning models can be increased if paired with uncertainty estimations. Conformal Prediction has emerged as a promising method to pair machine learning models with prediction intervals, allowing for a view of the model's uncertainty. However, popular uncertainty estimation methods for conformal prediction fail to provide heteroskedastic intervals that are equally accurate for all samples. In this paper, we propose a method to estimate the uncertainty of each sample by calculating the variance obtained from a Deep Regression Forest. We show that the deep regression forest variance improves the efficiency and coverage of normalized inductive conformal prediction on a drug response prediction task.","sentences":["Deep learning models are being adopted and applied on various critical decision-making tasks, yet they are trained to provide point predictions without providing degrees of confidence.","The trustworthiness of deep learning models can be increased if paired with uncertainty estimations.","Conformal Prediction has emerged as a promising method to pair machine learning models with prediction intervals, allowing for a view of the model's uncertainty.","However, popular uncertainty estimation methods for conformal prediction fail to provide heteroskedastic intervals that are equally accurate for all samples.","In this paper, we propose a method to estimate the uncertainty of each sample by calculating the variance obtained from a Deep Regression Forest.","We show that the deep regression forest variance improves the efficiency and coverage of normalized inductive conformal prediction on a drug response prediction task."],"url":"http://arxiv.org/abs/2402.14080v1","category":"cs.LG"}
{"created":"2024-02-21 19:06:33","title":"Strong Haken via Thin Position","abstract":"We use thin position of Heegaard splittings to give a new proof of Haken's Lemma that a Heegaard surface of a reducible manifold is reducible and of Scharlemann's ``Strong Haken Theorem'', that a Heegaard surface for a 3-manifold may be isotoped to intersect a given collection of essential spheres and discs in a single loop each. This article could also serve as an introduction to the theory of generalized Heegaard surfaces and it includes a careful study of their behaviour under amalgamation.","sentences":["We use thin position of Heegaard splittings to give a new proof of Haken's Lemma that a Heegaard surface of a reducible manifold is reducible and of Scharlemann's ``Strong Haken Theorem'', that a Heegaard surface for a 3-manifold may be isotoped to intersect a given collection of essential spheres and discs in a single loop each.","This article could also serve as an introduction to the theory of generalized Heegaard surfaces and it includes a careful study of their behaviour under amalgamation."],"url":"http://arxiv.org/abs/2402.14077v1","category":"math.GT"}
{"created":"2024-02-21 18:25:04","title":"Generative Adversarial Models for Extreme Downscaling of Climate Datasets","abstract":"Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty inherent to the downscaling process that tends to be ignored in existing methods. Given an input, the method can produce a multitude of plausible high-resolution samples instead of one single deterministic result. These samples allow for an empirical exploration and inferences of model uncertainty and robustness. With a case study of gridded climate datasets (wind velocity and solar irradiance), we demonstrate the performances of the framework in downscaling tasks with very high scaling factors (up to $64\\times$) and highlight the advantages of the framework with a comprehensive comparison with commonly used downscaling methods, including area-to-point (ATP) kriging, deep image prior (DIP), enhanced deep super-resolution network (EDSR), enhanced super-resolution generative adversarial networks (ESRGAN), and physics-informed resolution-enhancing GAN (PhIRE GAN).","sentences":["Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables.","However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand.","Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets.","In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets.","Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs.","More importantly, the method explicitly considers the uncertainty inherent to the downscaling process that tends to be ignored in existing methods.","Given an input, the method can produce a multitude of plausible high-resolution samples instead of one single deterministic result.","These samples allow for an empirical exploration and inferences of model uncertainty and robustness.","With a case study of gridded climate datasets (wind velocity and solar irradiance), we demonstrate the performances of the framework in downscaling tasks with very high scaling factors (up to $64\\times$) and highlight the advantages of the framework with a comprehensive comparison with commonly used downscaling methods, including area-to-point (ATP) kriging, deep image prior (DIP), enhanced deep super-resolution network (EDSR), enhanced super-resolution generative adversarial networks (ESRGAN), and physics-informed resolution-enhancing GAN (PhIRE GAN)."],"url":"http://arxiv.org/abs/2402.14049v1","category":"cs.LG"}
{"created":"2024-02-21 16:38:14","title":"PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization","abstract":"Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems. In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows PolyNet to find better solutions than approaches the explicitly enforce diverse solution generation.","sentences":["Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms.","To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process.","Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems.","In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies.","In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules.","We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows PolyNet to find better solutions than approaches the explicitly enforce diverse solution generation."],"url":"http://arxiv.org/abs/2402.14048v1","category":"cs.LG"}
{"created":"2024-02-22 18:55:05","title":"Explicit formulae for linear characters of $\u0393_0(N)$","abstract":"We give explicit formulae for a class of complex linear unitary characters of the congruence subgroups $\\Gamma_0(N)$ which involve a variant of Rademacher's $\\Psi$ function. We then prove that these characters cover all characters of $\\Gamma_0(N)$ precisely when $N=1,2,3,4,5,6,7,8,10,12,13$.","sentences":["We give explicit formulae for a class of complex linear unitary characters of the congruence subgroups $\\Gamma_0(N)$ which involve a variant of Rademacher's $\\Psi$ function.","We then prove that these characters cover all characters of $\\Gamma_0(N)$ precisely when $N=1,2,3,4,5,6,7,8,10,12,13$."],"url":"http://arxiv.org/abs/2402.14796v1","category":"math.NT"}
{"created":"2024-02-22 18:50:10","title":"Amplified Amplitude Estimation: Exploiting Prior Knowledge to Improve Estimates of Expectation Values","abstract":"We provide a method for estimating the expectation value of an operator that can utilize prior knowledge to accelerate the learning process on a quantum computer. Specifically, suppose we have an operator that can be expressed as a concise sum of projectors whose expectation values we know a priori to be $O(\\epsilon)$. In that case, we can estimate the expectation value of the entire operator within error $\\epsilon$ using a number of quantum operations that scales as $O(1/\\sqrt{\\epsilon})$. We then show how this can be used to reduce the cost of learning a potential energy surface in quantum chemistry applications by exploiting information gained from the energy at nearby points. Furthermore, we show, using Newton-Cotes methods, how these ideas can be exploited to learn the energy via integration of derivatives that we can estimate using a priori knowledge. This allows us to reduce the cost of energy estimation if the block-encodings of directional derivative operators have a smaller normalization constant than the Hamiltonian of the system.","sentences":["We provide a method for estimating the expectation value of an operator that can utilize prior knowledge to accelerate the learning process on a quantum computer.","Specifically, suppose we have an operator that can be expressed as a concise sum of projectors whose expectation values we know a priori to be $O(\\epsilon)$. In that case, we can estimate the expectation value of the entire operator within error $\\epsilon$ using a number of quantum operations that scales as $O(1/\\sqrt{\\epsilon})$. We then show how this can be used to reduce the cost of learning a potential energy surface in quantum chemistry applications by exploiting information gained from the energy at nearby points.","Furthermore, we show, using Newton-Cotes methods, how these ideas can be exploited to learn the energy via integration of derivatives that we can estimate using a priori knowledge.","This allows us to reduce the cost of energy estimation if the block-encodings of directional derivative operators have a smaller normalization constant than the Hamiltonian of the system."],"url":"http://arxiv.org/abs/2402.14791v1","category":"quant-ph"}
{"created":"2024-02-22 18:37:33","title":"Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models","abstract":"We consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. As a running example, we consider predicting how different drugs affect cells from different cell types. We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initial state of the system. We show that, under a linearity assumption, this setup induces a latent factor model over the matrix of outcomes, with an additional fixed effect term. To perform causal prediction based on this model class, we introduce simple extension to the Synthetic Interventions estimator (Agarwal et al., 2020). We evaluate several matrix completion approaches on the PRISM drug repurposing dataset, showing that our method outperforms all other considered matrix completion approaches.","sentences":["We consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts.","As a running example, we consider predicting how different drugs affect cells from different cell types.","We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values.","Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied.","Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes.","We introduce a novel SCM-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initial state of the system.","We show that, under a linearity assumption, this setup induces a latent factor model over the matrix of outcomes, with an additional fixed effect term.","To perform causal prediction based on this model class, we introduce simple extension to the Synthetic Interventions estimator (Agarwal et al., 2020).","We evaluate several matrix completion approaches on the PRISM drug repurposing dataset, showing that our method outperforms all other considered matrix completion approaches."],"url":"http://arxiv.org/abs/2402.14777v1","category":"stat.ML"}
{"created":"2024-02-22 18:32:53","title":"Dominant 1/3-filling Correlated Insulator States and Orbital Geometric Frustration in Twisted Bilayer Graphene","abstract":"Geometric frustration is a phenomenon in a lattice system where not all interactions can be satisfied, the simplest example being antiferromagnetically coupled spins on a triangular lattice. Frustrated systems are characterized by their many nearly degenerate ground states, leading to non-trivial phases such as spin ice and spin liquids. To date most studies are on geometric frustration of spins; much less explored is orbital geometric frustration. For electrons in twisted bilayer graphene (tBLG) at denominator 3 fractional filling, Coulomb interactions and the Wannier orbital shapes are predicted to strongly constrain spatial charge ordering, leading to geometrically frustrated ground states that produce a new class of correlated insulators (CIs). Here we report the observation of dominant denominator 3 fractional filling insulating states in large angle tBLG; these states persist in magnetic fields and display magnetic ordering signatures and tripled unit cell reconstruction. These results are in agreement with a strong-coupling theory of symmetry-breaking of geometrically frustrated fractional states.","sentences":["Geometric frustration is a phenomenon in a lattice system where not all interactions can be satisfied, the simplest example being antiferromagnetically coupled spins on a triangular lattice.","Frustrated systems are characterized by their many nearly degenerate ground states, leading to non-trivial phases such as spin ice and spin liquids.","To date most studies are on geometric frustration of spins; much less explored is orbital geometric frustration.","For electrons in twisted bilayer graphene (tBLG) at denominator 3 fractional filling, Coulomb interactions and the Wannier orbital shapes are predicted to strongly constrain spatial charge ordering, leading to geometrically frustrated ground states that produce a new class of correlated insulators (CIs).","Here we report the observation of dominant denominator 3 fractional filling insulating states in large angle tBLG; these states persist in magnetic fields and display magnetic ordering signatures and tripled unit cell reconstruction.","These results are in agreement with a strong-coupling theory of symmetry-breaking of geometrically frustrated fractional states."],"url":"http://arxiv.org/abs/2402.14774v1","category":"cond-mat.str-el"}
{"created":"2024-02-22 18:30:28","title":"A numerical study of rigidity of hyperbolic splittings in simple two-dimensional maps","abstract":"Chaotic hyperbolic dynamical systems enjoy a surprising degree of rigidity, a fact which is well known in the mathematics community but perhaps less so in theoretical physics circles. Low-dimensional hyperbolic systems are either conjugate to linear automorphisms, that is, dynamically equivalent to the Arnold cat map and its variants, or their hyperbolic structure is not smooth. We illustrate this dichotomy using a family of analytic maps, for which we show by means of numerical simulations that the corresponding hyperbolic structure is not smooth, thereby providing an example for a global mechanism which produces non-smooth phase space structures in an otherwise smooth dynamical system.","sentences":["Chaotic hyperbolic dynamical systems enjoy a surprising degree of rigidity, a fact which is well known in the mathematics community but perhaps less so in theoretical physics circles.","Low-dimensional hyperbolic systems are either conjugate to linear automorphisms, that is, dynamically equivalent to the Arnold cat map and its variants, or their hyperbolic structure is not smooth.","We illustrate this dichotomy using a family of analytic maps, for which we show by means of numerical simulations that the corresponding hyperbolic structure is not smooth, thereby providing an example for a global mechanism which produces non-smooth phase space structures in an otherwise smooth dynamical system."],"url":"http://arxiv.org/abs/2402.14770v1","category":"math.DS"}
{"created":"2024-02-22 18:26:31","title":"Using Hybrid System Dynamics and Discrete Event Simulations to Identify High Leverage Targets for Process Improvement in a Skill-based Organizational Structure","abstract":"This paper is based on a case study of an IT organization in a large, US-based healthcare provider, and develops simluation models to identify areas for performance improvement. These organizations are often grouped into departments by technical skill and support both operational work (tickets) and project work (tasks) of various priorities. From a practical standpoint, resource managers and staff regularly manage all work as queued and assign / complete it based on the priorities of the day. Using project and operational metrics from the case study organization, the hybrid model using both system dynamics and discrete event simulation developed through this research depicts the flow of work through a skill-based team as well as many of the key factors that influence that workflow, both positive and negative. Experience indicates that the interaction between project and operational work -- as well as between teams with differing skills -- entangles work queues and wait times within those queues in a way that rapidly scales in complexity as the number of interacting individuals and teams increases. Results from model simulation bear out this intuition. Scaling the models to accommodate multiple teams is a topic of future research.","sentences":["This paper is based on a case study of an IT organization in a large, US-based healthcare provider, and develops simluation models to identify areas for performance improvement.","These organizations are often grouped into departments by technical skill and support both operational work (tickets) and project work (tasks) of various priorities.","From a practical standpoint, resource managers and staff regularly manage all work as queued and assign / complete it based on the priorities of the day.","Using project and operational metrics from the case study organization, the hybrid model using both system dynamics and discrete event simulation developed through this research depicts the flow of work through a skill-based team as well as many of the key factors that influence that workflow, both positive and negative.","Experience indicates that the interaction between project and operational work -- as well as between teams with differing skills -- entangles work queues and wait times within those queues in a way that rapidly scales in complexity as the number of interacting individuals and teams increases.","Results from model simulation bear out this intuition.","Scaling the models to accommodate multiple teams is a topic of future research."],"url":"http://arxiv.org/abs/2402.14768v1","category":"eess.SY"}
{"created":"2024-02-22 18:24:30","title":"Environment Semantic Communication: Enabling Distributed Sensing Aided Networks","abstract":"Millimeter-wave (mmWave) and terahertz (THz) communication systems require large antenna arrays and use narrow directive beams to ensure sufficient receive signal power. However, selecting the optimal beams for these large antenna arrays incurs a significant beam training overhead, making it challenging to support applications involving high mobility. In recent years, machine learning (ML) solutions have shown promising results in reducing the beam training overhead by utilizing various sensing modalities such as GPS position and RGB images. However, the existing approaches are mainly limited to scenarios with only a single object of interest present in the wireless environment and focus only on co-located sensing, where all the sensors are installed at the communication terminal. This brings key challenges such as the limited sensing coverage compared to the coverage of the communication system and the difficulty in handling non-line-of-sight scenarios. To overcome these limitations, our paper proposes the deployment of multiple distributed sensing nodes, each equipped with an RGB camera. These nodes focus on extracting environmental semantics from the captured RGB images. The semantic data, rather than the raw images, are then transmitted to the basestation. This strategy significantly alleviates the overhead associated with the data storage and transmission of the raw images. Furthermore, semantic communication enhances the system's adaptability and responsiveness to dynamic environments, allowing for prioritization and transmission of contextually relevant information. Experimental results on the DeepSense 6G dataset demonstrate the effectiveness of the proposed solution in reducing the sensing data transmission overhead while accurately predicting the optimal beams in realistic communication environments.","sentences":["Millimeter-wave (mmWave) and terahertz (THz) communication systems require large antenna arrays and use narrow directive beams to ensure sufficient receive signal power.","However, selecting the optimal beams for these large antenna arrays incurs a significant beam training overhead, making it challenging to support applications involving high mobility.","In recent years, machine learning (ML) solutions have shown promising results in reducing the beam training overhead by utilizing various sensing modalities such as GPS position and RGB images.","However, the existing approaches are mainly limited to scenarios with only a single object of interest present in the wireless environment and focus only on co-located sensing, where all the sensors are installed at the communication terminal.","This brings key challenges such as the limited sensing coverage compared to the coverage of the communication system and the difficulty in handling non-line-of-sight scenarios.","To overcome these limitations, our paper proposes the deployment of multiple distributed sensing nodes, each equipped with an RGB camera.","These nodes focus on extracting environmental semantics from the captured RGB images.","The semantic data, rather than the raw images, are then transmitted to the basestation.","This strategy significantly alleviates the overhead associated with the data storage and transmission of the raw images.","Furthermore, semantic communication enhances the system's adaptability and responsiveness to dynamic environments, allowing for prioritization and transmission of contextually relevant information.","Experimental results on the DeepSense 6G dataset demonstrate the effectiveness of the proposed solution in reducing the sensing data transmission overhead while accurately predicting the optimal beams in realistic communication environments."],"url":"http://arxiv.org/abs/2402.14766v1","category":"cs.IT"}
{"created":"2024-02-22 18:06:54","title":"Inequality indices for heterogeneous systems: a tool for failure prediction","abstract":"We have numerically studied a mean-field fiber bundle model of fracture at a non-zero temperature and acted by a constant external tensile stress. The individual fibers fail (local damage) due to creep-like dynamics that lead up to a catastrophic breakdown (global failure). We quantify the variations in sizes of the resulting avalanches by calculating the Lorenz function and two inequality indices -- Gini ($g$) and Kolkata ($k$) indices -- derived from the Lorenz function. We show that the two indices cross just prior to the failure point when the dynamics goes through intermittent avalanches. For a continuous failure dynamics (finite numbers of fibers breaking at each time step), the crossing does not happen. However, in that phase, the usual prediction method i.e., linear relation between the time of minimum strain-rate and failure time, holds. The boundary between continuous and intermittent dynamics is very close to the boundary between crossing and non-crossing of the two indices in the temperature-stress phase space.","sentences":["We have numerically studied a mean-field fiber bundle model of fracture at a non-zero temperature and acted by a constant external tensile stress.","The individual fibers fail (local damage) due to creep-like dynamics that lead up to a catastrophic breakdown (global failure).","We quantify the variations in sizes of the resulting avalanches by calculating the Lorenz function and two inequality indices -- Gini ($g$) and Kolkata ($k$) indices -- derived from the Lorenz function.","We show that the two indices cross just prior to the failure point when the dynamics goes through intermittent avalanches.","For a continuous failure dynamics (finite numbers of fibers breaking at each time step), the crossing does not happen.","However, in that phase, the usual prediction method i.e., linear relation between the time of minimum strain-rate and failure time, holds.","The boundary between continuous and intermittent dynamics is very close to the boundary between crossing and non-crossing of the two indices in the temperature-stress phase space."],"url":"http://arxiv.org/abs/2402.14747v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-22 17:50:55","title":"Autonomy Oriented Digital Twins for Real2Sim2Real Autoware Deployment","abstract":"Modeling and simulation of autonomous vehicles plays a crucial role in achieving enterprise-scale realization that aligns with technical, business and regulatory requirements. Contemporary trends in digital lifecycle treatment have proven beneficial to support SBD as well as V&V of these complex systems. Although, the development of appropriate fidelity simulation models capable of capturing the intricate real-world physics and graphics (real2sim), while enabling real-time interactivity for decision-making, has remained a challenge. Nevertheless, recent advances in AI-based tools and workflows, such as online deep-learning algorithms leveraging live-streaming data sources, offer the tantalizing potential for real-time system-identification and adaptive modeling to simulate vehicles, environments, as well as their interactions. This transition from virtual prototypes to digital twins not only improves simulation fidelity and real-time factor, but can also support the development of online adaption/augmentation techniques that can help bridge the gap between simulation and reality (sim2real). In such a milieu, this work focuses on developing autonomy-oriented digital twins of vehicles across different scales and configurations to help support the streamlined development and deployment of Autoware stack, using a unified real2sim2real toolchain. Particularly, the core deliverable for this project was to integrate the Autoware stack with AutoDRIVE Ecosystem to demonstrate end-to-end task of map-based autonomous navigation. This work discusses the development of vehicle and environment digital twins using AutoDRIVE Ecosystem, along with various APIs and HMIs to connect with the same, followed by a detailed section on AutoDRIVE-Autoware integration. Furthermore, this study describes the first-ever off-road deployment of the Autoware stack, expanding the ODD beyond on-road autonomous navigation.","sentences":["Modeling and simulation of autonomous vehicles plays a crucial role in achieving enterprise-scale realization that aligns with technical, business and regulatory requirements.","Contemporary trends in digital lifecycle treatment have proven beneficial to support SBD as well as V&V of these complex systems.","Although, the development of appropriate fidelity simulation models capable of capturing the intricate real-world physics and graphics (real2sim), while enabling real-time interactivity for decision-making, has remained a challenge.","Nevertheless, recent advances in AI-based tools and workflows, such as online deep-learning algorithms leveraging live-streaming data sources, offer the tantalizing potential for real-time system-identification and adaptive modeling to simulate vehicles, environments, as well as their interactions.","This transition from virtual prototypes to digital twins not only improves simulation fidelity and real-time factor, but can also support the development of online adaption/augmentation techniques that can help bridge the gap between simulation and reality (sim2real).","In such a milieu, this work focuses on developing autonomy-oriented digital twins of vehicles across different scales and configurations to help support the streamlined development and deployment of Autoware stack, using a unified real2sim2real toolchain.","Particularly, the core deliverable for this project was to integrate the Autoware stack with AutoDRIVE Ecosystem to demonstrate end-to-end task of map-based autonomous navigation.","This work discusses the development of vehicle and environment digital twins using AutoDRIVE Ecosystem, along with various APIs and HMIs to connect with the same, followed by a detailed section on AutoDRIVE-Autoware integration.","Furthermore, this study describes the first-ever off-road deployment of the Autoware stack, expanding the ODD beyond on-road autonomous navigation."],"url":"http://arxiv.org/abs/2402.14739v1","category":"cs.RO"}
{"created":"2024-02-22 17:50:44","title":"Signals of Detailed Balance Violation in Nonequilibrium Stationary States: Subtle, Manifest, and Extraordinary","abstract":"The evolution of physical systems are often modeled by simple Markovian processes. When settled into stationary states, the probability distributions of such systems are time independent, by definition. However, they do not necessarily fall within the framework of equilibrium statistical mechanics. Instead, they may be non-equilibrium steady states (NESS). One distinguished feature of NESS is the presence of time reversal asymmetry (TRA) and persistent probability current loops. These loops lead naturally to the notion of probability angular momenta, which play a role on the same footing as the noise covariance in stochastic processes. Illustrating with simulations of simple models and physical data, we present ways to detect these signals of TRA, from the subtle to the prominent.","sentences":["The evolution of physical systems are often modeled by simple Markovian processes.","When settled into stationary states, the probability distributions of such systems are time independent, by definition.","However, they do not necessarily fall within the framework of equilibrium statistical mechanics.","Instead, they may be non-equilibrium steady states (NESS).","One distinguished feature of NESS is the presence of time reversal asymmetry (TRA) and persistent probability current loops.","These loops lead naturally to the notion of probability angular momenta, which play a role on the same footing as the noise covariance in stochastic processes.","Illustrating with simulations of simple models and physical data, we present ways to detect these signals of TRA, from the subtle to the prominent."],"url":"http://arxiv.org/abs/2402.14738v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-22 17:25:01","title":"A Transformer Model for Boundary Detection in Continuous Sign Language","abstract":"Sign Language Recognition (SLR) has garnered significant attention from researchers in recent years, particularly the intricate domain of Continuous Sign Language Recognition (CSLR), which presents heightened complexity compared to Isolated Sign Language Recognition (ISLR). One of the prominent challenges in CSLR pertains to accurately detecting the boundaries of isolated signs within a continuous video stream. Additionally, the reliance on handcrafted features in existing models poses a challenge to achieving optimal accuracy. To surmount these challenges, we propose a novel approach utilizing a Transformer-based model. Unlike traditional models, our approach focuses on enhancing accuracy while eliminating the need for handcrafted features. The Transformer model is employed for both ISLR and CSLR. The training process involves using isolated sign videos, where hand keypoint features extracted from the input video are enriched using the Transformer model. Subsequently, these enriched features are forwarded to the final classification layer. The trained model, coupled with a post-processing method, is then applied to detect isolated sign boundaries within continuous sign videos. The evaluation of our model is conducted on two distinct datasets, including both continuous signs and their corresponding isolated signs, demonstrates promising results.","sentences":["Sign Language Recognition (SLR) has garnered significant attention from researchers in recent years, particularly the intricate domain of Continuous Sign Language Recognition (CSLR), which presents heightened complexity compared to Isolated Sign Language Recognition (ISLR).","One of the prominent challenges in CSLR pertains to accurately detecting the boundaries of isolated signs within a continuous video stream.","Additionally, the reliance on handcrafted features in existing models poses a challenge to achieving optimal accuracy.","To surmount these challenges, we propose a novel approach utilizing a Transformer-based model.","Unlike traditional models, our approach focuses on enhancing accuracy while eliminating the need for handcrafted features.","The Transformer model is employed for both ISLR and CSLR.","The training process involves using isolated sign videos, where hand keypoint features extracted from the input video are enriched using the Transformer model.","Subsequently, these enriched features are forwarded to the final classification layer.","The trained model, coupled with a post-processing method, is then applied to detect isolated sign boundaries within continuous sign videos.","The evaluation of our model is conducted on two distinct datasets, including both continuous signs and their corresponding isolated signs, demonstrates promising results."],"url":"http://arxiv.org/abs/2402.14720v1","category":"cs.CV"}
{"created":"2024-02-22 17:18:29","title":"Optimal schedules for annealing algorithms","abstract":"Annealing algorithms such as simulated annealing and population annealing are widely used both for sampling the Gibbs distribution and solving optimization problems (i.e. finding ground states). For both statistical mechanics and optimization, additional parameters beyond temperature are often needed such as chemical potentials, external fields or Lagrange multipliers enforcing constraints. In this paper we derive a formalism for optimal annealing schedules in multidimensional parameter spaces using methods from non-equilibrium statistical mechanics. The results are closely related to work on optimal control of thermodynamic systems [Sivak and Crooks, PRL 108, 190602 (2012)]. Within the formalism, we compare the efficiency of population annealing and multiple weighted runs of simulated annealing (\"annealed importance sampling\") and discuss the effects of non-ergodicity on both algorithms. Theoretical results are supported by numerical simulations of spin glasses.","sentences":["Annealing algorithms such as simulated annealing and population annealing are widely used both for sampling the Gibbs distribution and solving optimization problems (i.e. finding ground states).","For both statistical mechanics and optimization, additional parameters beyond temperature are often needed such as chemical potentials, external fields or Lagrange multipliers enforcing constraints.","In this paper we derive a formalism for optimal annealing schedules in multidimensional parameter spaces using methods from non-equilibrium statistical mechanics.","The results are closely related to work on optimal control of thermodynamic systems","[Sivak and Crooks, PRL 108, 190602 (2012)].","Within the formalism, we compare the efficiency of population annealing and multiple weighted runs of simulated annealing (\"annealed importance sampling\") and discuss the effects of non-ergodicity on both algorithms.","Theoretical results are supported by numerical simulations of spin glasses."],"url":"http://arxiv.org/abs/2402.14717v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-22 17:17:12","title":"Balanced Truncation of Descriptor Systems with a Quadratic Output","abstract":"This work discusses model reduction for differential-algebraic systems with quadratic output equations. Under mild conditions, these systems can be transformed into a Weierstra{\\ss} canonical form and, thus, be decoupled into differential equations and algebraic equations. The corresponding decoupled states are referred to as proper and improper states. Due to the quadratic function of the state as an output, the proper and improper states are coupled in the output equation, which imposes a challenge from a model reduction viewpoint. Keeping the coupling in mind, our goal in this work is to find important subspaces of the proper and improper states and to reduce the system accordingly. To that end, we first propose the system's matrices, the so-called Gramians, to characterize the system's dominant subspaces. We pay particular attention to the computation of the observability Gramians that take into account the nonlinear coupling between the proper and the improper states. We furthermore show that the proposed Gramians are related to certain kernel functions, which are used to identify important subspaces. This allows us to propose a reduction algorithm to obtain reduced-order systems by removing the subspaces that are difficult to reach, as well as, difficult to observe. Moreover, we quantify the error between the full-order and reduced-order models and demonstrate the proposed methodology using three numerical experiments.","sentences":["This work discusses model reduction for differential-algebraic systems with quadratic output equations.","Under mild conditions, these systems can be transformed into a Weierstra{\\ss} canonical form and, thus, be decoupled into differential equations and algebraic equations.","The corresponding decoupled states are referred to as proper and improper states.","Due to the quadratic function of the state as an output, the proper and improper states are coupled in the output equation, which imposes a challenge from a model reduction viewpoint.","Keeping the coupling in mind, our goal in this work is to find important subspaces of the proper and improper states and to reduce the system accordingly.","To that end, we first propose the system's matrices, the so-called Gramians, to characterize the system's dominant subspaces.","We pay particular attention to the computation of the observability Gramians that take into account the nonlinear coupling between the proper and the improper states.","We furthermore show that the proposed Gramians are related to certain kernel functions, which are used to identify important subspaces.","This allows us to propose a reduction algorithm to obtain reduced-order systems by removing the subspaces that are difficult to reach, as well as, difficult to observe.","Moreover, we quantify the error between the full-order and reduced-order models and demonstrate the proposed methodology using three numerical experiments."],"url":"http://arxiv.org/abs/2402.14716v1","category":"math.DS"}
{"created":"2024-02-22 17:16:55","title":"Towards the quark mass dependence of $T_{cc}^+$ from lattice QCD","abstract":"The $DD^*$ scattering phase shifts in the $T_{cc}^+=cc\\bar{u}\\bar{d}$ channel are extracted from lattice QCD for five different charm quark masses and a fixed light-quark mass corresponding to $m_\\pi\\simeq 280$ MeV. The phase shifts are analysed employing two approaches: effective range expansion and Lippmann-Schwinger equation derived in the effective field theory. In the latter case, the results imply an attraction at short range parametrised by contact terms and a slight repulsion at long range mediated by one-pion exchange with $m_\\pi >m_{D^*}-m_D$. The poles in the amplitude across the complex energy plane are extracted and their trajectories are discussed as the charm quark mass is varied. Two complex conjugate poles corresponding to a resonance below threshold are found for $m_c$ close to the physical value. They turn into a pair of virtual states at the largest $m_c$ studied. With further increasing $m_c$, one virtual pole representing $T_{cc}^+$ is expected to move towards the two-body threshold and turn into a bound state. The light-quark mass dependence of the $T_{cc}^+$ pole is briefly discussed using the data on $DD^*$ scattering from other lattice collaborations.","sentences":["The $DD^*$ scattering phase shifts in the $T_{cc}^+=cc\\bar{u}\\bar{d}$ channel are extracted from lattice QCD for five different charm quark masses and a fixed light-quark mass corresponding to $m_\\pi\\simeq 280$ MeV. The phase shifts are analysed employing two approaches: effective range expansion and Lippmann-Schwinger equation derived in the effective field theory.","In the latter case, the results imply an attraction at short range parametrised by contact terms and a slight repulsion at long range mediated by one-pion exchange with $m_\\pi >m_{D^*}-m_D$. The poles in the amplitude across the complex energy plane are extracted and their trajectories are discussed as the charm quark mass is varied.","Two complex conjugate poles corresponding to a resonance below threshold are found for $m_c$ close to the physical value.","They turn into a pair of virtual states at the largest $m_c$ studied.","With further increasing $m_c$, one virtual pole representing $T_{cc}^+$ is expected to move towards the two-body threshold and turn into a bound state.","The light-quark mass dependence of the $T_{cc}^+$ pole is briefly discussed using the data on $DD^*$ scattering from other lattice collaborations."],"url":"http://arxiv.org/abs/2402.14715v1","category":"hep-lat"}
{"created":"2024-02-22 17:04:30","title":"An LLM-Enhanced Adversarial Editing System for Lexical Simplification","abstract":"Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.","sentences":["Lexical Simplification (LS) aims to simplify text at the lexical level.","Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios.","In this paper, we propose a novel LS method without parallel corpora.","This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences.","Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system.","From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words.","At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2402.14704v1","category":"cs.CL"}
{"created":"2024-02-22 16:47:43","title":"Joint AP-UE Association and Power Factor Optimization for Distributed Massive MIMO","abstract":"The uplink sum-throughput of distributed massive multiple-input-multiple-output (mMIMO) networks depends majorly on Access point (AP)-User Equipment (UE) association and power control. The AP-UE association and power control both are important problems in their own right in distributed mMIMO networks to improve scalability and reduce front-haul load of the network, and to enhance the system performance by mitigating the interference and boosting the desired signals, respectively. Unlike previous studies, which focused primarily on addressing the AP-UE association or power control problems separately, this work addresses the uplink sum-throughput maximization problem in distributed mMIMO networks by solving the joint AP-UE association and power control problem, while maintaining Quality-of-Service (QoS) requirements for each UE. To improve scalability, we present an l1-penalty function that delicately balances the trade-off between spectral efficiency (SE) and front-haul signaling load. Our proposed methodology leverages fractional programming, Lagrangian dual formation, and penalty functions to provide an elegant and effective iterative solution with guaranteed convergence while meeting strict QoS criteria. Extensive numerical simulations validate the efficacy of the proposed technique for maximizing sum-throughput while considering the joint AP-UE association and power control problem, demonstrating its superiority over approaches that address these problems individually. Furthermore, the results show that the introduced penalty function can help us effectively control the maximum front-haul load for uplink distributed mMIMO systems.","sentences":["The uplink sum-throughput of distributed massive multiple-input-multiple-output (mMIMO) networks depends majorly on Access point (AP)-User Equipment (UE) association and power control.","The AP-UE association and power control both are important problems in their own right in distributed mMIMO networks to improve scalability and reduce front-haul load of the network, and to enhance the system performance by mitigating the interference and boosting the desired signals, respectively.","Unlike previous studies, which focused primarily on addressing the AP-UE association or power control problems separately, this work addresses the uplink sum-throughput maximization problem in distributed mMIMO networks by solving the joint AP-UE association and power control problem, while maintaining Quality-of-Service (QoS) requirements for each UE.","To improve scalability, we present an l1-penalty function that delicately balances the trade-off between spectral efficiency (SE) and front-haul signaling load.","Our proposed methodology leverages fractional programming, Lagrangian dual formation, and penalty functions to provide an elegant and effective iterative solution with guaranteed convergence while meeting strict QoS criteria.","Extensive numerical simulations validate the efficacy of the proposed technique for maximizing sum-throughput while considering the joint AP-UE association and power control problem, demonstrating its superiority over approaches that address these problems individually.","Furthermore, the results show that the introduced penalty function can help us effectively control the maximum front-haul load for uplink distributed mMIMO systems."],"url":"http://arxiv.org/abs/2402.14693v1","category":"cs.NI"}
{"created":"2024-02-22 16:40:55","title":"Adaptive time series forecasting with markovian variance switching","abstract":"Adaptive time series forecasting is essential for prediction under regime changes. Several classical methods assume linear Gaussian state space model (LGSSM) with variances constant in time. However, there are many real-world processes that cannot be captured by such models. We consider a state-space model with Markov switching variances. Such dynamical systems are usually intractable because of their computational complexity increasing exponentially with time; Variational Bayes (VB) techniques have been applied to this problem. In this paper, we propose a new way of estimating variances based on online learning theory; we adapt expert aggregation methods to learn the variances over time. We apply the proposed method to synthetic data and to the problem of electricity load forecasting. We show that this method is robust to misspecification and outperforms traditional expert aggregation.","sentences":["Adaptive time series forecasting is essential for prediction under regime changes.","Several classical methods assume linear Gaussian state space model (LGSSM) with variances constant in time.","However, there are many real-world processes that cannot be captured by such models.","We consider a state-space model with Markov switching variances.","Such dynamical systems are usually intractable because of their computational complexity increasing exponentially with time; Variational Bayes (VB) techniques have been applied to this problem.","In this paper, we propose a new way of estimating variances based on online learning theory; we adapt expert aggregation methods to learn the variances over time.","We apply the proposed method to synthetic data and to the problem of electricity load forecasting.","We show that this method is robust to misspecification and outperforms traditional expert aggregation."],"url":"http://arxiv.org/abs/2402.14684v1","category":"stat.ML"}
{"created":"2024-02-22 16:35:41","title":"Decomposition of P\u0142onka sums into direct systems","abstract":"The P{\\l}onka sum is an algebra determined using a structure called direct system. By a direct system, we mean an indexed family of algebras with disjoint universes whose indexes form a join-semilattice s.t. if two indexes are in a partial order relation, then there is a homomorphism from the algebra of the first index to the algebra of the second index. The sum of the sets of the direct system determines the universe of P{\\l}onka sum. Therefore, to speak about a P{\\l}onka sum, there must be a direct system on which this algebra is based.   Up to now, P{\\l}onka sums have been analyzed only by referring to their direct systems. In our paper, we will decompose the P{\\l}onka sum without knowing any of direct systems that determine it. The decomposition we describe will be based on the method of determining all direct systems of a given P{\\l}onka sum. Our analysis also allows us to determine, in the case of any algebra that meets the assumptions made in the article, whether it is a P{\\l}onka sum. The proposed method is based on two concepts introduced in the paper: isolated algebra and P{\\l}onka homomorphism.","sentences":["The P{\\l}onka sum is an algebra determined using a structure called direct system.","By a direct system, we mean an indexed family of algebras with disjoint universes whose indexes form a join-semilattice s.t.","if two indexes are in a partial order relation, then there is a homomorphism from the algebra of the first index to the algebra of the second index.","The sum of the sets of the direct system determines the universe of P{\\l}onka sum.","Therefore, to speak about a P{\\l}onka sum, there must be a direct system on which this algebra is based.   ","Up to now, P{\\l}onka sums have been analyzed only by referring to their direct systems.","In our paper, we will decompose the P{\\l}onka sum without knowing any of direct systems that determine it.","The decomposition we describe will be based on the method of determining all direct systems of a given P{\\l}onka sum.","Our analysis also allows us to determine, in the case of any algebra that meets the assumptions made in the article, whether it is a P{\\l}onka sum.","The proposed method is based on two concepts introduced in the paper: isolated algebra and P{\\l}onka homomorphism."],"url":"http://arxiv.org/abs/2402.14681v1","category":"math.LO"}
{"created":"2024-02-22 16:31:00","title":"Zeptonewton and Attotesla per Centimeter Metrology With Coupled Oscillators","abstract":"We present the coupled oscillator: a new mechanism for signal amplification with widespread application in metrology. We introduce the mechanical theory of this framework, and support it by way of simulations. We present a particular implementation of coupled oscillators: a microelectromechanical system (MEMS) that uses one large (~100mm) N52 magnet coupled magnetically to a small (~0.25mm), oscillating N52 magnet, providing a force resolution of 200zN measured over 1s in a noiseless environment. We show that the same system is able to resolve magnetic gradients of 130aT/cm at a single point (within 500um). This technology therefore has the potential to revolutionize force and magnetic gradient sensing, including high-impact areas such cardiac and brain imaging.","sentences":["We present the coupled oscillator: a new mechanism for signal amplification with widespread application in metrology.","We introduce the mechanical theory of this framework, and support it by way of simulations.","We present a particular implementation of coupled oscillators: a microelectromechanical system (MEMS) that uses one large (~100mm) N52 magnet coupled magnetically to a small (~0.25mm), oscillating N52 magnet, providing a force resolution of 200zN measured over 1s in a noiseless environment.","We show that the same system is able to resolve magnetic gradients of 130aT/cm at a single point (within 500um).","This technology therefore has the potential to revolutionize force and magnetic gradient sensing, including high-impact areas such cardiac and brain imaging."],"url":"http://arxiv.org/abs/2402.14678v1","category":"physics.app-ph"}
{"created":"2024-02-22 16:30:34","title":"Influence of thermal effects on atomic Bloch oscillation","abstract":"Advancements in the experimental toolbox of cold atoms have enabled the meticulous control of atomic Bloch oscillation within optical lattices, thereby enhancing the capabilities of gravity interferometers. This work delves into the impact of thermal effects on Bloch oscillation in 1D accelerated optical lattices aligned with gravity by varying the system's initial temperature. Through the application of Raman cooling, we effectively reduce the longitudinal thermal effect, stabilizing the longitudinal coherence length over the timescale of its lifetime. The atomic losses over multiple Bloch oscillation is measured, which are primarily attributed to transverse excitation. Furthermore, we identify two distinct inverse scaling behaviors in the oscillation lifetime scaled by the corresponding density with respect to temperatures, implying diverse equilibrium processes within or outside the Bose-Einstein condensate regime. The competition between the system's coherence and atomic density leads to a relatively smooth variation in the actual lifetime versus temperature. Our findings provide valuable insights into the interaction between thermal effects and Bloch oscillation, offering avenues for the refinement of quantum measurement technologies.","sentences":["Advancements in the experimental toolbox of cold atoms have enabled the meticulous control of atomic Bloch oscillation within optical lattices, thereby enhancing the capabilities of gravity interferometers.","This work delves into the impact of thermal effects on Bloch oscillation in 1D accelerated optical lattices aligned with gravity by varying the system's initial temperature.","Through the application of Raman cooling, we effectively reduce the longitudinal thermal effect, stabilizing the longitudinal coherence length over the timescale of its lifetime.","The atomic losses over multiple Bloch oscillation is measured, which are primarily attributed to transverse excitation.","Furthermore, we identify two distinct inverse scaling behaviors in the oscillation lifetime scaled by the corresponding density with respect to temperatures, implying diverse equilibrium processes within or outside the Bose-Einstein condensate regime.","The competition between the system's coherence and atomic density leads to a relatively smooth variation in the actual lifetime versus temperature.","Our findings provide valuable insights into the interaction between thermal effects and Bloch oscillation, offering avenues for the refinement of quantum measurement technologies."],"url":"http://arxiv.org/abs/2402.14677v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-22 16:16:19","title":"Linear isometries of Hol(D)","abstract":"A complete characterisation is given of all the linear isometries of the Fr\\'echet space of all holomorphic functions on the unit disc, when it is given one of the two standard metrics: these turn out to be weighted composition operators of a particular form. Operators similar to an isometry are also classified. Further, the larger class of operators isometric when restricted to one of the defining seminorms is identified. Finally, the spectra of such operators are studied.","sentences":["A complete characterisation is given of all the linear isometries of the Fr\\'echet space of all holomorphic functions on the unit disc, when it is given one of the two standard metrics: these turn out to be weighted composition operators of a particular form.","Operators similar to an isometry are also classified.","Further, the larger class of operators isometric when restricted to one of the defining seminorms is identified.","Finally, the spectra of such operators are studied."],"url":"http://arxiv.org/abs/2402.14671v1","category":"math.CV"}
{"created":"2024-02-22 16:14:51","title":"Looking Forward: A High-Throughput Track Following Algorithm for Parallel Architectures","abstract":"Real-time data processing is a central aspect of particle physics experiments with high requirements on computing resources. The LHCb experiment must cope with the 30 million proton-proton bunches collision per second rate of the Large Hadron Collider (LHC), producing $10^9$ particles/s. The large input data rate of 32 Tb/s needs to be processed in real time by the LHCb trigger system, which includes both reconstruction and selection algorithms to reduce the number of saved events. The trigger system is implemented in two stages and deployed in a custom data centre.   We present Looking Forward, a high-throughput track following algorithm designed for the first stage of the LHCb trigger and optimised for GPUs. The algorithm focuses on the reconstruction of particles traversing the whole LHCb detector and is developed to obtain the best physics performance while respecting the throughput limitations of the trigger. The physics and computing performances are discussed and validated with simulated samples.","sentences":["Real-time data processing is a central aspect of particle physics experiments with high requirements on computing resources.","The LHCb experiment must cope with the 30 million proton-proton bunches collision per second rate of the Large Hadron Collider (LHC), producing $10^9$ particles/s.","The large input data rate of 32 Tb/s needs to be processed in real time by the LHCb trigger system, which includes both reconstruction and selection algorithms to reduce the number of saved events.","The trigger system is implemented in two stages and deployed in a custom data centre.   ","We present Looking Forward, a high-throughput track following algorithm designed for the first stage of the LHCb trigger and optimised for GPUs.","The algorithm focuses on the reconstruction of particles traversing the whole LHCb detector and is developed to obtain the best physics performance while respecting the throughput limitations of the trigger.","The physics and computing performances are discussed and validated with simulated samples."],"url":"http://arxiv.org/abs/2402.14670v1","category":"hep-ex"}
{"created":"2024-02-22 16:10:39","title":"Quadruplet Loss For Improving the Robustness to Face Morphing Attacks","abstract":"Recent advancements in deep learning have revolutionized technology and security measures, necessitating robust identification methods. Biometric approaches, leveraging personalized characteristics, offer a promising solution. However, Face Recognition Systems are vulnerable to sophisticated attacks, notably face morphing techniques, enabling the creation of fraudulent documents. In this study, we introduce a novel quadruplet loss function for increasing the robustness of face recognition systems against morphing attacks. Our approach involves specific sampling of face image quadruplets, combined with face morphs, for network training. Experimental results demonstrate the efficiency of our strategy in improving the robustness of face recognition networks against morphing attacks.","sentences":["Recent advancements in deep learning have revolutionized technology and security measures, necessitating robust identification methods.","Biometric approaches, leveraging personalized characteristics, offer a promising solution.","However, Face Recognition Systems are vulnerable to sophisticated attacks, notably face morphing techniques, enabling the creation of fraudulent documents.","In this study, we introduce a novel quadruplet loss function for increasing the robustness of face recognition systems against morphing attacks.","Our approach involves specific sampling of face image quadruplets, combined with face morphs, for network training.","Experimental results demonstrate the efficiency of our strategy in improving the robustness of face recognition networks against morphing attacks."],"url":"http://arxiv.org/abs/2402.14665v1","category":"cs.CV"}
{"created":"2024-02-22 16:05:58","title":"Stabilization of a matrix via a low rank-adaptive ODE","abstract":"Let $A$ be a square matrix with a given structure (e.g. real matrix, sparsity pattern, Toeplitz structure, etc.) and assume that it is unstable, i.e. at least one of its eigenvalues lies in the complex right half-plane. The problem of stabilizing $A$ consists in the computation of a matrix $B$, whose eigenvalues have negative real part and such that the perturbation $\\Delta=B-A$ has minimal norm. The structured stabilization further requires that the perturbation preserves the structural pattern of $A$. We solve this non-convex problem by a two-level procedure which involves the computation of the stationary points of a matrix ODE. We exploit the low rank underlying features of the problem by using an adaptive-rank integrator that follows slavishly the rank of the solution. We show the benefits derived from the low rank setting in several numerical examples, which also allow to deal with high dimensional problems.","sentences":["Let $A$ be a square matrix with a given structure (e.g. real matrix, sparsity pattern, Toeplitz structure, etc.) and assume that it is unstable, i.e. at least one of its eigenvalues lies in the complex right half-plane.","The problem of stabilizing $A$ consists in the computation of a matrix $B$, whose eigenvalues have negative real part and such that the perturbation $\\Delta=B-A$ has minimal norm.","The structured stabilization further requires that the perturbation preserves the structural pattern of $A$.","We solve this non-convex problem by a two-level procedure which involves the computation of the stationary points of a matrix ODE.","We exploit the low rank underlying features of the problem by using an adaptive-rank integrator that follows slavishly the rank of the solution.","We show the benefits derived from the low rank setting in several numerical examples, which also allow to deal with high dimensional problems."],"url":"http://arxiv.org/abs/2402.14657v1","category":"math.NA"}
{"created":"2024-02-22 16:05:13","title":"Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot","abstract":"We present Multi-HMR, a strong single-shot model for multi-person 3D human mesh recovery from a single RGB image. Predictions encompass the whole body, i.e, including hands and facial expressions, using the SMPL-X parametric model and spatial location in the camera coordinate system. Our model detects people by predicting coarse 2D heatmaps of person centers, using features produced by a standard Vision Transformer (ViT) backbone. It then predicts their whole-body pose, shape and spatial location using a new cross-attention module called the Human Prediction Head (HPH), with one query per detected center token, attending to the entire set of features. As direct prediction of SMPL-X parameters yields suboptimal results, we introduce CUFFS; the Close-Up Frames of Full-Body Subjects dataset, containing humans close to the camera with diverse hand poses. We show that incorporating this dataset into training further enhances predictions, particularly for hands, enabling us to achieve state-of-the-art performance. Multi-HMR also optionally accounts for camera intrinsics, if available, by encoding camera ray directions for each image token. This simple design achieves strong performance on whole-body and body-only benchmarks simultaneously. We train models with various backbone sizes and input resolutions. In particular, using a ViT-S backbone and $448\\times448$ input images already yields a fast and competitive model with respect to state-of-the-art methods, while considering larger models and higher resolutions further improve performance.","sentences":["We present Multi-HMR, a strong single-shot model for multi-person 3D human mesh recovery from a single RGB image.","Predictions encompass the whole body, i.e, including hands and facial expressions, using the SMPL-X parametric model and spatial location in the camera coordinate system.","Our model detects people by predicting coarse 2D heatmaps of person centers, using features produced by a standard Vision Transformer (ViT) backbone.","It then predicts their whole-body pose, shape and spatial location using a new cross-attention module called the Human Prediction Head (HPH), with one query per detected center token, attending to the entire set of features.","As direct prediction of SMPL-X parameters yields suboptimal results, we introduce CUFFS; the Close-Up Frames of Full-Body Subjects dataset, containing humans close to the camera with diverse hand poses.","We show that incorporating this dataset into training further enhances predictions, particularly for hands, enabling us to achieve state-of-the-art performance.","Multi-HMR also optionally accounts for camera intrinsics, if available, by encoding camera ray directions for each image token.","This simple design achieves strong performance on whole-body and body-only benchmarks simultaneously.","We train models with various backbone sizes and input resolutions.","In particular, using a ViT-S backbone and $448\\times448$ input images already yields a fast and competitive model with respect to state-of-the-art methods, while considering larger models and higher resolutions further improve performance."],"url":"http://arxiv.org/abs/2402.14654v1","category":"cs.CV"}
{"created":"2024-02-22 16:00:21","title":"Quantum Markov Decision Processes Part II: Optimal Solutions and Algorithms","abstract":"This two-part article aims to introduce a quantum analogue to classical Markov decision processes (MDPs). In Part II, building on the formulation of q-MDPs presented in Part I, our focus shifts to the development of algorithms for computing optimal policies and value functions of both open-loop and closed-loop policies. First, by using the duality between the dynamic programming and the semi-definite programming formulations of any q-MDP with open-loop policies, we establish an algorithm that enables us to efficiently compute optimal open-loop quantum policies and value functions. Then, dynamic programming and semi-definite programming formulations for closed-loop policies is established, where duality of these two formulations similarly enables the efficient computation of optimal closed-loop policies and value functions. Finally, given that any q-MDP can be approximated by q-MDPs with classical policies--potentially with higher-dimensional underlying Hilbert spaces than the original model--and since any classical policy is an element of the set of closed-loop policies, we conclude that any q-MDP can be approximated by q-MDPs with closed-loop policies having higher-dimensional Hilbert spaces.","sentences":["This two-part article aims to introduce a quantum analogue to classical Markov decision processes (MDPs).","In Part II, building on the formulation of q-MDPs presented in Part I, our focus shifts to the development of algorithms for computing optimal policies and value functions of both open-loop and closed-loop policies.","First, by using the duality between the dynamic programming and the semi-definite programming formulations of any q-MDP with open-loop policies, we establish an algorithm that enables us to efficiently compute optimal open-loop quantum policies and value functions.","Then, dynamic programming and semi-definite programming formulations for closed-loop policies is established, where duality of these two formulations similarly enables the efficient computation of optimal closed-loop policies and value functions.","Finally, given that any q-MDP can be approximated by q-MDPs with classical policies--potentially with higher-dimensional underlying Hilbert spaces than the original model--and since any classical policy is an element of the set of closed-loop policies, we conclude that any q-MDP can be approximated by q-MDPs with closed-loop policies having higher-dimensional Hilbert spaces."],"url":"http://arxiv.org/abs/2402.14651v1","category":"quant-ph"}
{"created":"2024-02-22 15:36:39","title":"Cool and Data-Driven: An Exploration of Optical Cool Dwarf Chemistry with Both Data-Driven and Physical Models","abstract":"Detailed chemical studies of F/G/K -- or Solar-type -- stars have long been routine in stellar astrophysics, enabling studies in both Galactic chemodynamics, and exoplanet demographics. However, similar understanding of the chemistry of M and late-K dwarfs -- the most common stars in the Galaxy -- has been greatly hampered both observationally and theoretically by the complex molecular chemistry of their atmospheres. Here we present a new implementation of the data-driven \\textit{Cannon} model, modelling $T_{\\rm eff}$, $\\log g$, [Fe/H], and [Ti/Fe] trained on low-medium resolution optical spectra ($4\\,000-7\\,000\\,$\\SI{}{\\angstrom}) from 103 cool dwarf benchmarks. Alongside this, we also investigate the sensitivity of optical wavelengths to various atomic and molecular species using both data-driven and theoretical means via a custom grid of MARCS synthetic spectra, and make recommendations for where MARCS struggles to reproduce cool dwarf fluxes. Under leave-one-out cross-validation, our \\textit{Cannon} model is capable of recovering $T_{\\rm eff}$, $\\log g$, [Fe/H], and [Ti/Fe] with precisions of 1.4\\%, $\\pm0.04\\,$dex, $\\pm0.10\\,$dex, and $\\pm0.06\\,$dex respectively, with the recovery of [Ti/Fe] pointing to the as-yet mostly untapped potential of exploiting the abundant -- but complex -- chemical information within optical spectra of cool stars.","sentences":["Detailed chemical studies of F/G/K -- or Solar-type -- stars have long been routine in stellar astrophysics, enabling studies in both Galactic chemodynamics, and exoplanet demographics.","However, similar understanding of the chemistry of M and late-K dwarfs -- the most common stars in the Galaxy -- has been greatly hampered both observationally and theoretically by the complex molecular chemistry of their atmospheres.","Here we present a new implementation of the data-driven \\textit{Cannon} model, modelling $T_{\\rm eff}$, $\\log g$, [Fe/H], and [Ti/Fe] trained on low-medium resolution optical spectra ($4\\,000-7\\,000\\,$\\SI{}{\\angstrom}) from 103 cool dwarf benchmarks.","Alongside this, we also investigate the sensitivity of optical wavelengths to various atomic and molecular species using both data-driven and theoretical means via a custom grid of MARCS synthetic spectra, and make recommendations for where MARCS struggles to reproduce cool dwarf fluxes.","Under leave-one-out cross-validation, our \\textit{Cannon} model is capable of recovering $T_{\\rm eff}$, $\\log g$, [Fe/H], and [Ti/Fe] with precisions of 1.4\\%, $\\pm0.04\\,$dex, $\\pm0.10\\,$dex, and $\\pm0.06\\,$dex respectively, with the recovery of [Ti/Fe] pointing to the as-yet mostly untapped potential of exploiting the abundant -- but complex -- chemical information within optical spectra of cool stars."],"url":"http://arxiv.org/abs/2402.14639v1","category":"astro-ph.SR"}
{"created":"2024-02-22 15:29:37","title":"Chemically reactive thin films: dynamics and stability","abstract":"Catalyst particles or complexes suspended in liquid films can trigger chemical reactions leading to inhomogeneous concentrations of reactants and products in the film. We demonstrate that the sensitivity of the liquid film's gas-liquid surface tension to these inhomogeneous concentrations strongly impacts the film stability. Using linear stability analysis, we identify novel scenarios in which the film can be either stabilized or destabilized by the reactions. Furthermore, we find so far unrevealed rupture mechanisms which are absent in the chemically inactive case. The linear stability predictions are confirmed by numerical simulations, which also demonstrate that the shape of chemically active droplets can depart from the spherical cap and that unsteady states such as traveling and standing waves might appear. Finally, we critically discuss the relevance of our predictions by showing that the range of our selected parameters is well accessible by typical experiments.","sentences":["Catalyst particles or complexes suspended in liquid films can trigger chemical reactions leading to inhomogeneous concentrations of reactants and products in the film.","We demonstrate that the sensitivity of the liquid film's gas-liquid surface tension to these inhomogeneous concentrations strongly impacts the film stability.","Using linear stability analysis, we identify novel scenarios in which the film can be either stabilized or destabilized by the reactions.","Furthermore, we find so far unrevealed rupture mechanisms which are absent in the chemically inactive case.","The linear stability predictions are confirmed by numerical simulations, which also demonstrate that the shape of chemically active droplets can depart from the spherical cap and that unsteady states such as traveling and standing waves might appear.","Finally, we critically discuss the relevance of our predictions by showing that the range of our selected parameters is well accessible by typical experiments."],"url":"http://arxiv.org/abs/2402.14635v1","category":"cond-mat.soft"}
{"created":"2024-02-22 15:28:26","title":"GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame","abstract":"In this paper, we present GazeTrak, the first acoustic-based eye tracking system on glasses. Our system only needs one speaker and four microphones attached to each side of the glasses. These acoustic sensors capture the formations of the eyeballs and the surrounding areas by emitting encoded inaudible sound towards eyeballs and receiving the reflected signals. These reflected signals are further processed to calculate the echo profiles, which are fed to a customized deep learning pipeline to continuously infer the gaze position. In a user study with 20 participants, GazeTrak achieves an accuracy of 3.6{\\deg} within the same remounting session and 4.9{\\deg} across different sessions with a refreshing rate of 83.3 Hz and a power signature of 287.9 mW. Furthermore, we report the performance of our gaze tracking system fully implemented on an MCU with a low-power CNN accelerator (MAX78002). In this configuration, the system runs at up to 83.3 Hz and has a total power signature of 95.4 mW with a 30 Hz FPS.","sentences":["In this paper, we present GazeTrak, the first acoustic-based eye tracking system on glasses.","Our system only needs one speaker and four microphones attached to each side of the glasses.","These acoustic sensors capture the formations of the eyeballs and the surrounding areas by emitting encoded inaudible sound towards eyeballs and receiving the reflected signals.","These reflected signals are further processed to calculate the echo profiles, which are fed to a customized deep learning pipeline to continuously infer the gaze position.","In a user study with 20 participants, GazeTrak achieves an accuracy of 3.6{\\deg} within the same remounting session and 4.9{\\deg} across different sessions with a refreshing rate of 83.3 Hz and a power signature of 287.9 mW.","Furthermore, we report the performance of our gaze tracking system fully implemented on an MCU with a low-power CNN accelerator (MAX78002).","In this configuration, the system runs at up to 83.3 Hz and has a total power signature of 95.4 mW with a 30 Hz FPS."],"url":"http://arxiv.org/abs/2402.14634v1","category":"cs.HC"}
{"created":"2024-02-22 15:27:58","title":"Time Efficient Implementation for Online $k$-server Problem on Trees","abstract":"We consider online algorithms for the $k$-server problem on trees of size $n$. Chrobak and Larmore proposed a $k$-competitive algorithm for this problem that has the optimal competitive ratio. However, the existing implementations have $O\\left(k^2 + k\\cdot \\log n\\right)$ or $O\\left(k(\\log n)^2\\right)$ time complexity for processing a query, where $n$ is the number of nodes. We propose a new time-efficient implementation of this algorithm that has $O(n)$ time complexity for preprocessing and $O\\left(k\\log k\\right)$ time for processing a query. The new algorithm is faster than both existing algorithms and the time complexity for query processing does not depend on the tree size.","sentences":["We consider online algorithms for the $k$-server problem on trees of size $n$. Chrobak and Larmore proposed a $k$-competitive algorithm for this problem that has the optimal competitive ratio.","However, the existing implementations have $O\\left(k^2 + k\\cdot \\log n\\right)$ or $O\\left(k(\\log n)^2\\right)$ time complexity for processing a query, where $n$ is the number of nodes.","We propose a new time-efficient implementation of this algorithm that has $O(n)$ time complexity for preprocessing and $O\\left(k\\log k\\right)$ time for processing a query.","The new algorithm is faster than both existing algorithms and the time complexity for query processing does not depend on the tree size."],"url":"http://arxiv.org/abs/2402.14633v1","category":"cs.DS"}
{"created":"2024-02-22 15:21:27","title":"FlexibleSUSY extended to automatically compute physical quantities in any Beyond the Standard Model theory: Charged Lepton Flavor Violation processes, Higgs decays, and user-defined observables","abstract":"FlexibleSUSY is a framework for the automated computation of physical quantities (observables) in models beyond the Standard Model (BSM). This paper describes an extension of FlexibleSUSY which allows to define and add new observables that can be enabled and computed in applicable user-defined BSM models. The extension has already been used to include Charged Lepton Flavor Violation (CLFV) observables, but further observables can now be added straightforwardly. The paper is split into two parts. The first part is non-technical and describes from the user's perspective how to enable the calculation of predefined observables, in particular CLFV observables. The second part of the paper explains how to define new observables such that their automatic computation in any applicable BSM model becomes possible. A key ingredient is the new NPointFunctions extension which allows to use tree-level and loop calculations in the model-independent setup of observables. Three examples of increasing complexity are fully worked out. This illustrates the features and provides code snippets that may be used as a starting point for implementation of further observables.","sentences":["FlexibleSUSY is a framework for the automated computation of physical quantities (observables) in models beyond the Standard Model (BSM).","This paper describes an extension of FlexibleSUSY which allows to define and add new observables that can be enabled and computed in applicable user-defined BSM models.","The extension has already been used to include Charged Lepton Flavor Violation (CLFV) observables, but further observables can now be added straightforwardly.","The paper is split into two parts.","The first part is non-technical and describes from the user's perspective how to enable the calculation of predefined observables, in particular CLFV observables.","The second part of the paper explains how to define new observables such that their automatic computation in any applicable BSM model becomes possible.","A key ingredient is the new NPointFunctions extension which allows to use tree-level and loop calculations in the model-independent setup of observables.","Three examples of increasing complexity are fully worked out.","This illustrates the features and provides code snippets that may be used as a starting point for implementation of further observables."],"url":"http://arxiv.org/abs/2402.14630v1","category":"hep-ph"}
{"created":"2024-02-22 15:19:49","title":"Temporal Talbot interferometer of strongly interacting molecular Bose-Einstein condensate","abstract":"Talbot interferometer, as a periodic reproduction of momentum distribution in the time domain, finds significant applications in multiple research. The inter-particle interactions during the diffraction and interference process introduce numerous many-body physics problems, leading to unconventional interference characteristics. This work investigates both experimentally and theoretically the influence of interaction in a Talbot interferometer with a $^{6}\\rm Li_2$ molecular Bose-Einstein condensate in a one-dimensional optical lattice, with interaction strength directly tunable via magnetic Feshbach resonance. A clear dependence of the period and amplitude of signal revivals on the interaction strength can be observed. While interactions increase the decay rate of the signal and advance the revivals, we find that over a wide range of interactions, the Talbot interferometer remains highly effective over a certain evolutionary timescale, including the case of fractional Talbot interference. This work provides insight into the interplay between interaction and the coherence properties of a temporal Talbot interference in optical lattices, paving the way for research into quantum interference in strongly interacting systems.","sentences":["Talbot interferometer, as a periodic reproduction of momentum distribution in the time domain, finds significant applications in multiple research.","The inter-particle interactions during the diffraction and interference process introduce numerous many-body physics problems, leading to unconventional interference characteristics.","This work investigates both experimentally and theoretically the influence of interaction in a Talbot interferometer with a $^{6}\\rm Li_2$ molecular Bose-Einstein condensate in a one-dimensional optical lattice, with interaction strength directly tunable via magnetic Feshbach resonance.","A clear dependence of the period and amplitude of signal revivals on the interaction strength can be observed.","While interactions increase the decay rate of the signal and advance the revivals, we find that over a wide range of interactions, the Talbot interferometer remains highly effective over a certain evolutionary timescale, including the case of fractional Talbot interference.","This work provides insight into the interplay between interaction and the coherence properties of a temporal Talbot interference in optical lattices, paving the way for research into quantum interference in strongly interacting systems."],"url":"http://arxiv.org/abs/2402.14629v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-22 15:16:52","title":"Thermal-Aware Floorplanner for 3D IC, including TSVs, Liquid Microchannels and Thermal Domains Optimization","abstract":"3D stacked technology has emerged as an effective mechanism to overcome physical limits and communication delays found in 2D integration. However, 3D technology also presents several drawbacks that prevent its smooth application. Two of the major concerns are heat reduction and power density distribution. In our work, we propose a novel 3D thermal-aware floorplanner that includes: (1) an effective thermal-aware process with 3 different evolutionary algorithms that aim to solve the soft computing problem of optimizing the placement of functional units and through silicon vias, as well as the smooth inclusion of active cooling systems and new design strategies,(2) an approximated thermal model inside the optimization loop, (3) an optimizer for active cooling (liquid channels), and (4) a novel technique based on air channel placement designed to isolate thermal domains have been also proposed. The experimental work is conducted for a realistic many-core single-chip architecture based on the Niagara design. Results show promising improvements of the thermal and reliability metrics, and also show optimal scaling capabilities to target future-trend many-core systems.","sentences":["3D stacked technology has emerged as an effective mechanism to overcome physical limits and communication delays found in 2D integration.","However, 3D technology also presents several drawbacks that prevent its smooth application.","Two of the major concerns are heat reduction and power density distribution.","In our work, we propose a novel 3D thermal-aware floorplanner that includes: (1) an effective thermal-aware process with 3 different evolutionary algorithms that aim to solve the soft computing problem of optimizing the placement of functional units and through silicon vias, as well as the smooth inclusion of active cooling systems and new design strategies,(2) an approximated thermal model inside the optimization loop, (3) an optimizer for active cooling (liquid channels), and (4) a novel technique based on air channel placement designed to isolate thermal domains have been also proposed.","The experimental work is conducted for a realistic many-core single-chip architecture based on the Niagara design.","Results show promising improvements of the thermal and reliability metrics, and also show optimal scaling capabilities to target future-trend many-core systems."],"url":"http://arxiv.org/abs/2402.14627v1","category":"cs.AR"}
{"created":"2024-02-22 15:06:45","title":"Seer: Proactive Revenue-Aware Scheduling for Live Streaming Services in Crowdsourced Cloud-Edge Platforms","abstract":"As live streaming services skyrocket, Crowdsourced Cloud-edge service Platforms (CCPs) have surfaced as pivotal intermediaries catering to the mounting demand. Despite the role of stream scheduling to CCPs' Quality of Service (QoS) and throughput, conventional optimization strategies struggle to enhancing CCPs' revenue, primarily due to the intricate relationship between resource utilization and revenue. Additionally, the substantial scale of CCPs magnifies the difficulties of time-intensive scheduling. To tackle these challenges, we propose Seer, a proactive revenue-aware scheduling system for live streaming services in CCPs. The design of Seer is motivated by meticulous measurements of real-world CCPs environments, which allows us to achieve accurate revenue modeling and overcome three key obstacles that hinder the integration of prediction and optimal scheduling. Utilizing an innovative Pre-schedule-Execute-Re-schedule paradigm and flexible scheduling modes, Seer achieves efficient revenue-optimized scheduling in CCPs. Extensive evaluations demonstrate Seer's superiority over competitors in terms of revenue, utilization, and anomaly penalty mitigation, boosting CCPs revenue by 147% and expediting scheduling $3.4 \\times$ faster.","sentences":["As live streaming services skyrocket, Crowdsourced Cloud-edge service Platforms (CCPs) have surfaced as pivotal intermediaries catering to the mounting demand.","Despite the role of stream scheduling to CCPs' Quality of Service (QoS) and throughput, conventional optimization strategies struggle to enhancing CCPs' revenue, primarily due to the intricate relationship between resource utilization and revenue.","Additionally, the substantial scale of CCPs magnifies the difficulties of time-intensive scheduling.","To tackle these challenges, we propose Seer, a proactive revenue-aware scheduling system for live streaming services in CCPs.","The design of Seer is motivated by meticulous measurements of real-world CCPs environments, which allows us to achieve accurate revenue modeling and overcome three key obstacles that hinder the integration of prediction and optimal scheduling.","Utilizing an innovative Pre-schedule-Execute-Re-schedule paradigm and flexible scheduling modes, Seer achieves efficient revenue-optimized scheduling in CCPs.","Extensive evaluations demonstrate Seer's superiority over competitors in terms of revenue, utilization, and anomaly penalty mitigation, boosting CCPs revenue by 147% and expediting scheduling $3.4 \\times$ faster."],"url":"http://arxiv.org/abs/2402.14619v1","category":"cs.DC"}
{"created":"2024-02-22 15:05:53","title":"On the origin of the above-room-temperature magnetism in the 2D van der Waals ferromagnet Fe$_3$GaTe$_2$","abstract":"Recent advancements in 2D magnetic materials have attracted a growing interest driven by their unique properties and potential applications in spintronic devices. However, the scarcity of systems that exhibit magnetism at room-temperature has limited their practical implementation into functional devices. In this work we focus on the recently synthetised van der Waals (vdW) ferromagnet Fe$_3$GaTe$_2$, which exhibits above-room-temperature magnetism (T$_{\\mathrm{c}}$ = 350-380 K) and strong perpendicular magnetic anisotropy. Through first-principles calculations, we examine the magnetic properties of Fe$_3$GaTe$_2$ and compare them with the widely known Fe$_3$GeTe$_2$ ferromagnet. Our calculations unveil the complex microscopic mechanisms governing their magnetic behaviour, emphasizing the pivotal role of the ferromagnetic in-plane exchange interactions in the stabilization of the elevated T$_{\\mathrm{c}}$ in Fe$_3$GaTe$_2$. Additionally, we predict the stability, strong perpendicular anisotropy and high T$_{\\mathrm{c}}$ of single-layer Fe$_3$GaTe$_2$. We also demonstrate the potential of strain engineering and electrostatic doping to modulate its magnetic exchange interactions and anisotropy. Our results incentivise the isolation of the monolayer and pave the way for the future optimization of Fe$_3$GaTe$_2$ in magnetic and spintronic nanodevices.","sentences":["Recent advancements in 2D magnetic materials have attracted a growing interest driven by their unique properties and potential applications in spintronic devices.","However, the scarcity of systems that exhibit magnetism at room-temperature has limited their practical implementation into functional devices.","In this work we focus on the recently synthetised van der Waals (vdW) ferromagnet Fe$_3$GaTe$_2$, which exhibits above-room-temperature magnetism (T$_{\\mathrm{c}}$ = 350-380 K) and strong perpendicular magnetic anisotropy.","Through first-principles calculations, we examine the magnetic properties of Fe$_3$GaTe$_2$ and compare them with the widely known Fe$_3$GeTe$_2$ ferromagnet.","Our calculations unveil the complex microscopic mechanisms governing their magnetic behaviour, emphasizing the pivotal role of the ferromagnetic in-plane exchange interactions in the stabilization of the elevated T$_{\\mathrm{c}}$ in Fe$_3$GaTe$_2$. Additionally, we predict the stability, strong perpendicular anisotropy and high T$_{\\mathrm{c}}$ of single-layer Fe$_3$GaTe$_2$. We also demonstrate the potential of strain engineering and electrostatic doping to modulate its magnetic exchange interactions and anisotropy.","Our results incentivise the isolation of the monolayer and pave the way for the future optimization of Fe$_3$GaTe$_2$ in magnetic and spintronic nanodevices."],"url":"http://arxiv.org/abs/2402.14618v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 15:02:30","title":"Fast and Efficient Sequential Radar Parameter Estimation in MIMO-OTFS Systems","abstract":"We consider the estimation of three-dimensional (3D) radar parameters, namely, bearing or angle-of-arrival (AoA), delay or range, and Doppler shift velocity, under a mono-static multiple-input multiple-output (MIMO) joint communications and radar (JCR) system based on Orthogonal Time Frequency Space (OTFS) signals. In particular, we propose a novel two-step algorithm to estimate the three radar parameters sequentially, where the AoA is obtained first, followed by the estimation of range and velocity via a reduced two-dimensional (2D) grid maximum likelihood (ML) search in the delay-Doppler (DD) domain. Besides the resulting lower complexity, the decoupling of AoA and DD estimation enables the incorporation of an linear minimum mean square error (LMMSE) procedure in the ML estimation of range and velocity, which are found to significantly outperform State-of-the-Art (SotA) alternatives and approach the fundamental limits of the Cram`er-Rao lower bound (CRLB) and search grid resolution.","sentences":["We consider the estimation of three-dimensional (3D) radar parameters, namely, bearing or angle-of-arrival (AoA), delay or range, and Doppler shift velocity, under a mono-static multiple-input multiple-output (MIMO) joint communications and radar (JCR) system based on Orthogonal Time Frequency Space (OTFS) signals.","In particular, we propose a novel two-step algorithm to estimate the three radar parameters sequentially, where the AoA is obtained first, followed by the estimation of range and velocity via a reduced two-dimensional (2D) grid maximum likelihood (ML) search in the delay-Doppler (DD) domain.","Besides the resulting lower complexity, the decoupling of AoA and DD estimation enables the incorporation of an linear minimum mean square error (LMMSE) procedure in the ML estimation of range and velocity, which are found to significantly outperform State-of-the-Art (SotA) alternatives and approach the fundamental limits of the Cram`er-Rao lower bound (CRLB) and search grid resolution."],"url":"http://arxiv.org/abs/2402.14612v1","category":"eess.SP"}
{"created":"2024-02-22 14:59:33","title":"Toward Scalable Docker-Based Emulations of Blockchain Networks for Research and Development","abstract":"Blockchain, like any other complex technology, needs a strong testing methodology to support its evolution in both research and development contexts. Setting up meaningful tests for permissionless blockchain technology is a notoriously complex task for several reasons: software is complex, large number of nodes are involved, network is non ideal, etc. Developers usually adopt small virtual laboratories or costly real devnets, based on real software. Researchers usually prefer simulations of a large number of nodes, based on simplified models. In this paper, we aim to obtain the advantages of both approaches, i.e., performing large, realistic, inexpensive, and flexible experiments, using real blockchain software within a virtual environment. To do that, we tackle the challenge of running large blockchain networks in a single physical machine, leveraging Linux and Docker. We analyze a number of problems that arise when large blockchain networks are emulated and we provide technical solutions for all of them. Finally, we describe two experiences of emulating fairly large blockchain networks on a single machine, adopting both research oriented and production oriented software, and involving up to more than 3000 containers.","sentences":["Blockchain, like any other complex technology, needs a strong testing methodology to support its evolution in both research and development contexts.","Setting up meaningful tests for permissionless blockchain technology is a notoriously complex task for several reasons: software is complex, large number of nodes are involved, network is non ideal, etc.","Developers usually adopt small virtual laboratories or costly real devnets, based on real software.","Researchers usually prefer simulations of a large number of nodes, based on simplified models.","In this paper, we aim to obtain the advantages of both approaches, i.e., performing large, realistic, inexpensive, and flexible experiments, using real blockchain software within a virtual environment.","To do that, we tackle the challenge of running large blockchain networks in a single physical machine, leveraging Linux and Docker.","We analyze a number of problems that arise when large blockchain networks are emulated and we provide technical solutions for all of them.","Finally, we describe two experiences of emulating fairly large blockchain networks on a single machine, adopting both research oriented and production oriented software, and involving up to more than 3000 containers."],"url":"http://arxiv.org/abs/2402.14610v1","category":"cs.DC"}
{"created":"2024-02-22 14:54:04","title":"Observation of the antiferromagnetic phase transition in the fermionic Hubbard model","abstract":"The fermionic Hubbard model (FHM)[1], despite its simple form, captures essential features of strongly correlated electron physics. Ultracold fermions in optical lattices[2, 3] provide a clean and well-controlled platform for simulating FHM. Doping its antiferromagnetic ground state at half filling, various exotic phases are expected to arise in the FHM simulator, including stripe order[4], pseudogap[5], and d-wave superconductors[6], offering valuable insights into high-temperature superconductivity[7{9]. Although notable progress, such as the observation of antiferromagnetic correlations over short[10] and extended distances[11], has been obtained, the antiferromagnetic phase has yet to be realized due to the significant challenges of achieving low temperatures in a large and uniform quantum simulator. Here, we report the observation of the antiferromagnetic phase transition in a three-dimensional fermionic Hubbard system comprising lithium-6 atoms in a uniform optical lattice with approximately 800,000 sites. When the interaction strength, temperature, and doping concentration are finely tuned to approach their respective critical values, sharp increases in the spin structure factor (SSF) are observed. These observations can be well described by a power-law divergence, with a critical exponent of 1.396 from the Heisenberg universality class[12]. At half filling and with optimal interaction strength, the measured SSF reaches 123(8), signifying the establishment of an antiferromagnetic phase. Our results set the stage for exploring the low-temperature phase diagram of FHM.","sentences":["The fermionic Hubbard model (FHM)[1], despite its simple form, captures essential features of strongly correlated electron physics.","Ultracold fermions in optical lattices[2, 3] provide a clean and well-controlled platform for simulating FHM.","Doping its antiferromagnetic ground state at half filling, various exotic phases are expected to arise in the FHM simulator, including stripe order[4], pseudogap[5], and d-wave superconductors[6], offering valuable insights into high-temperature superconductivity[7{9].","Although notable progress, such as the observation of antiferromagnetic correlations over short[10] and extended distances[11], has been obtained, the antiferromagnetic phase has yet to be realized due to the significant challenges of achieving low temperatures in a large and uniform quantum simulator.","Here, we report the observation of the antiferromagnetic phase transition in a three-dimensional fermionic Hubbard system comprising lithium-6 atoms in a uniform optical lattice with approximately 800,000 sites.","When the interaction strength, temperature, and doping concentration are finely tuned to approach their respective critical values, sharp increases in the spin structure factor (SSF) are observed.","These observations can be well described by a power-law divergence, with a critical exponent of 1.396 from the Heisenberg universality class[12].","At half filling and with optimal interaction strength, the measured SSF reaches 123(8), signifying the establishment of an antiferromagnetic phase.","Our results set the stage for exploring the low-temperature phase diagram of FHM."],"url":"http://arxiv.org/abs/2402.14605v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-22 14:47:42","title":"Enhancing SCADA Security: Developing a Host-Based Intrusion Detection System to Safeguard Against Cyberattacks","abstract":"With the increasing reliance of smart grids on correctly functioning SCADA systems and their vulnerability to cyberattacks, there is a pressing need for effective security measures. SCADA systems are prone to cyberattacks, posing risks to critical infrastructure. As there is a lack of host-based intrusion detection systems specifically designed for the stable nature of SCADA systems, the objective of this work is to propose a host-based intrusion detection system tailored for SCADA systems in smart grids. The proposed system utilizes USB device identification, flagging, and process memory scanning to monitor and detect anomalies in SCADA systems, providing enhanced security measures. Evaluation in three different scenarios demonstrates the tool's effectiveness in detecting and disabling malware. The proposed approach effectively identifies potential threats and enhances the security of SCADA systems in smart grids, providing a promising solution to protect against cyberattacks.","sentences":["With the increasing reliance of smart grids on correctly functioning SCADA systems and their vulnerability to cyberattacks, there is a pressing need for effective security measures.","SCADA systems are prone to cyberattacks, posing risks to critical infrastructure.","As there is a lack of host-based intrusion detection systems specifically designed for the stable nature of SCADA systems, the objective of this work is to propose a host-based intrusion detection system tailored for SCADA systems in smart grids.","The proposed system utilizes USB device identification, flagging, and process memory scanning to monitor and detect anomalies in SCADA systems, providing enhanced security measures.","Evaluation in three different scenarios demonstrates the tool's effectiveness in detecting and disabling malware.","The proposed approach effectively identifies potential threats and enhances the security of SCADA systems in smart grids, providing a promising solution to protect against cyberattacks."],"url":"http://arxiv.org/abs/2402.14599v1","category":"cs.CR"}
{"created":"2024-02-22 14:45:22","title":"The role of gap junctions and clustered connectivity in emergent synchronisation patterns of inhibitory neuronal networks","abstract":"Inhibitory interneurons, ubiquitous in the central nervous system, form networks connected through both chemical synapses and gap junctions. These networks are essential for regulating the activity of principal neurons, especially by inducing temporally patterned dynamic states. We aim to understand the dynamic mechanisms for synchronisation in networks of electrically and chemically coupled interneurons. We use the exact mean-field reduction to derive a neural mass model for both homogeneous and clustered networks. We first analyse a single population of neurons to understand how the two couplings interact with one another. We demonstrate that the network transitions from an asynchronous to a synchronous regime either by increasing the strength of the gap junction connectivity or the strength of the background input current. Conversely, the strength of inhibitory synapses affects the population firing rate, suggesting that electrical and chemical coupling strengths act as complementary mechanisms by which networks can tune synchronous oscillatory behavior. In line with previous work, we confirm that the depolarizing spikelet is crucial for the emergence of synchrony. Furthermore, find that the fast frequency component of the spikelet ensures robustness to heterogeneity. Next, inspired by the existence of multiple interconnected interneuron subtypes in the cerebellum, we analyse networks consisting of two clusters of cell types defined by differing chemical versus electrical coupling strengths. We show that breaking the electrical and chemical coupling symmetry between these clusters induces bistability, so that a transient external input can switch the network between synchronous and asynchronous firing. Together, our results shows the variety of cell-intrinsic and network properties that contribute to synchronisation of interneuronal networks with multiple types of coupling.","sentences":["Inhibitory interneurons, ubiquitous in the central nervous system, form networks connected through both chemical synapses and gap junctions.","These networks are essential for regulating the activity of principal neurons, especially by inducing temporally patterned dynamic states.","We aim to understand the dynamic mechanisms for synchronisation in networks of electrically and chemically coupled interneurons.","We use the exact mean-field reduction to derive a neural mass model for both homogeneous and clustered networks.","We first analyse a single population of neurons to understand how the two couplings interact with one another.","We demonstrate that the network transitions from an asynchronous to a synchronous regime either by increasing the strength of the gap junction connectivity or the strength of the background input current.","Conversely, the strength of inhibitory synapses affects the population firing rate, suggesting that electrical and chemical coupling strengths act as complementary mechanisms by which networks can tune synchronous oscillatory behavior.","In line with previous work, we confirm that the depolarizing spikelet is crucial for the emergence of synchrony.","Furthermore, find that the fast frequency component of the spikelet ensures robustness to heterogeneity.","Next, inspired by the existence of multiple interconnected interneuron subtypes in the cerebellum, we analyse networks consisting of two clusters of cell types defined by differing chemical versus electrical coupling strengths.","We show that breaking the electrical and chemical coupling symmetry between these clusters induces bistability, so that a transient external input can switch the network between synchronous and asynchronous firing.","Together, our results shows the variety of cell-intrinsic and network properties that contribute to synchronisation of interneuronal networks with multiple types of coupling."],"url":"http://arxiv.org/abs/2402.14592v1","category":"q-bio.NC"}
{"created":"2024-02-22 14:43:49","title":"How do digital threats change requirements for the software industry?","abstract":"Digital systems are, by definition, the core of digital transformation. This has led many to think that the system being considered in digital transformation is solely software. I argue that this approach is a fatal mistake, and it has induced a great number of already realized problems and even a greater number of concerns about the future. These problems and concerns have become evident along with rising requirements for sustainability and responsibility. In this paper, I call for a better understanding of the digital society in its entirety. By digital society I mean the societal system that is affected by the digital systems and the ongoing societal trans-formations. When shifting the focus to the effects of digital systems on societies, we are forced to consider all the anticipated outcomes, both desirable and undesirable ones. Unfortunately, the mainstream research has ignored, to a large extent, the potential threats, and unwanted outcomes, of digitalization, which makes the efforts to change software businesses to be more sustainable, difficult to succeed. In my paper, I will provide an overall picture of current and future challenges of digital societies and discuss what these challenges mean to the software industry in future. The easiest way to start with, is to learn from earlier experiences, especially from the unsuccessful stories.","sentences":["Digital systems are, by definition, the core of digital transformation.","This has led many to think that the system being considered in digital transformation is solely software.","I argue that this approach is a fatal mistake, and it has induced a great number of already realized problems and even a greater number of concerns about the future.","These problems and concerns have become evident along with rising requirements for sustainability and responsibility.","In this paper, I call for a better understanding of the digital society in its entirety.","By digital society I mean the societal system that is affected by the digital systems and the ongoing societal trans-formations.","When shifting the focus to the effects of digital systems on societies, we are forced to consider all the anticipated outcomes, both desirable and undesirable ones.","Unfortunately, the mainstream research has ignored, to a large extent, the potential threats, and unwanted outcomes, of digitalization, which makes the efforts to change software businesses to be more sustainable, difficult to succeed.","In my paper, I will provide an overall picture of current and future challenges of digital societies and discuss what these challenges mean to the software industry in future.","The easiest way to start with, is to learn from earlier experiences, especially from the unsuccessful stories."],"url":"http://arxiv.org/abs/2402.14588v1","category":"cs.CY"}
{"created":"2024-02-22 13:50:55","title":"Quantum computing in civil engineering: Limitations","abstract":"Quantum computing is a new computational paradigm with the potential to solve certain computationally challenging problems much faster than traditional approaches. Civil engineering encompasses many computationally challenging problems, which leads to the question of how well quantum computing is suitable for solving civil engineering problems and how much impact and implications to the field of civil engineering can be expected when deploying quantum computing for solving these problems. To address these questions, we will, in this paper, first introduce the fundamentals of quantum computing. Thereupon, we will analyze the problem classes to elucidate where quantum computing holds the potential to outperform traditional computers and, focusing on the limitations, where quantum computing is not considered the most suitable solution. Finally, we will review common complex computation use cases in civil engineering and evaluate the potential and the limitations of being improved by quantum computing.","sentences":["Quantum computing is a new computational paradigm with the potential to solve certain computationally challenging problems much faster than traditional approaches.","Civil engineering encompasses many computationally challenging problems, which leads to the question of how well quantum computing is suitable for solving civil engineering problems and how much impact and implications to the field of civil engineering can be expected when deploying quantum computing for solving these problems.","To address these questions, we will, in this paper, first introduce the fundamentals of quantum computing.","Thereupon, we will analyze the problem classes to elucidate where quantum computing holds the potential to outperform traditional computers and, focusing on the limitations, where quantum computing is not considered the most suitable solution.","Finally, we will review common complex computation use cases in civil engineering and evaluate the potential and the limitations of being improved by quantum computing."],"url":"http://arxiv.org/abs/2402.14556v1","category":"cs.ET"}
{"created":"2024-02-22 13:48:03","title":"Unraveling the origin of antiferromagnetic coupling at YIG/permalloy interface","abstract":"We investigate the structural and electronic origin of antiferromagnetic coupling in the Yttrium iron garnet (YIG) and permalloy (Py) bilayer system at the atomic level. Ferromagnetic Resonance (FMR) reveal unique hybrid modes in samples prepared with surface ion milling, indicative of antiferromagnetic exchange coupling at the YIG/Py interface. Using scanning transmission electron microscopy (STEM), we highlight significant interfacial differences introduced by ion-milling. The observations suggests that the antiferromagnetic coupling in YIG/Py bilayers is predominantly driven by an oxygen-mediated super-exchange coupling mechanism on the tetrahedral Fe terminated YIG surface, which is supported by density functional theory (DFT) calculations. This research provides critical insight into the fundamental mechanisms governing the efficiency of coupling in magnetic bilayers and underscores the pivotal role of oxide surface termination in modulating magnetic interfacial dynamics.","sentences":["We investigate the structural and electronic origin of antiferromagnetic coupling in the Yttrium iron garnet (YIG) and permalloy (Py) bilayer system at the atomic level.","Ferromagnetic Resonance (FMR) reveal unique hybrid modes in samples prepared with surface ion milling, indicative of antiferromagnetic exchange coupling at the YIG/Py interface.","Using scanning transmission electron microscopy (STEM), we highlight significant interfacial differences introduced by ion-milling.","The observations suggests that the antiferromagnetic coupling in YIG/Py bilayers is predominantly driven by an oxygen-mediated super-exchange coupling mechanism on the tetrahedral Fe terminated YIG surface, which is supported by density functional theory (DFT) calculations.","This research provides critical insight into the fundamental mechanisms governing the efficiency of coupling in magnetic bilayers and underscores the pivotal role of oxide surface termination in modulating magnetic interfacial dynamics."],"url":"http://arxiv.org/abs/2402.14553v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 13:42:47","title":"Approximate Circular Pattern Matching under Edit Distance","abstract":"In the $k$-Edit Circular Pattern Matching ($k$-Edit CPM) problem, we are given a length-$n$ text $T$, a length-$m$ pattern $P$, and a positive integer threshold $k$, and we are to report all starting positions of the substrings of $T$ that are at edit distance at most $k$ from some cyclic rotation of $P$. In the decision version of the problem, we are to check if any such substring exists. Very recently, Charalampopoulos et al. [ESA 2022] presented $O(nk^2)$-time and $O(nk \\log^3 k)$-time solutions for the reporting and decision versions of $k$-Edit CPM, respectively. Here, we show that the reporting and decision versions of $k$-Edit CPM can be solved in $O(n+(n/m) k^6)$ time and $O(n+(n/m) k^5 \\log^3 k)$ time, respectively, thus obtaining the first algorithms with a complexity of the type $O(n+(n/m) \\mathrm{poly}(k))$ for this problem. Notably, our algorithms run in $O(n)$ time when $m=\\Omega(k^6)$ and are superior to the previous respective solutions when $m=\\omega(k^4)$. We provide a meta-algorithm that yields efficient algorithms in several other interesting settings, such as when the strings are given in a compressed form (as straight-line programs), when the strings are dynamic, or when we have a quantum computer.   We obtain our solutions by exploiting the structure of approximate circular occurrences of $P$ in $T$, when $T$ is relatively short w.r.t. $P$. Roughly speaking, either the starting positions of approximate occurrences of rotations of $P$ form $O(k^4)$ intervals that can be computed efficiently, or some rotation of $P$ is almost periodic (is at a small edit distance from a string with small period). Dealing with the almost periodic case is the most technically demanding part of this work; we tackle it using properties of locked fragments (originating from [Cole and Hariharan, SICOMP 2002]).","sentences":["In the $k$-Edit Circular Pattern Matching ($k$-Edit CPM) problem, we are given a length-$n$ text $T$, a length-$m$ pattern $P$, and a positive integer threshold $k$, and we are to report all starting positions of the substrings of $T$ that are at edit distance at most $k$ from some cyclic rotation of $P$.","In the decision version of the problem, we are to check if any such substring exists.","Very recently, Charalampopoulos et al.","[ESA 2022] presented $O(nk^2)$-time and $O(nk \\log^3 k)$-time solutions for the reporting and decision versions of $k$-Edit CPM, respectively.","Here, we show that the reporting and decision versions of $k$-Edit CPM can be solved in $O(n+(n/m) k^6)$","time and $O(n+(n/m) k^5 \\log^3 k)$ time, respectively, thus obtaining the first algorithms with a complexity of the type $O(n+(n/m) \\mathrm{poly}(k))$ for this problem.","Notably, our algorithms run in $O(n)$ time when $m=\\Omega(k^6)$ and are superior to the previous respective solutions when $m=\\omega(k^4)$. We provide a meta-algorithm that yields efficient algorithms in several other interesting settings, such as when the strings are given in a compressed form (as straight-line programs), when the strings are dynamic, or when we have a quantum computer.   ","We obtain our solutions by exploiting the structure of approximate circular occurrences of $P$ in $T$, when $T$ is relatively short w.r.t.","$P$. Roughly speaking, either the starting positions of approximate occurrences of rotations of $P$ form $O(k^4)$ intervals that can be computed efficiently, or some rotation of $P$ is almost periodic (is at a small edit distance from a string with small period).","Dealing with the almost periodic case is the most technically demanding part of this work; we tackle it using properties of locked fragments (originating from [Cole and Hariharan, SICOMP 2002])."],"url":"http://arxiv.org/abs/2402.14550v1","category":"cs.DS"}
{"created":"2024-02-22 13:35:26","title":"Algebraic description of complex conjugation on cohomology of a smooth projective hypersurface","abstract":"We describe complex conjugation on the primitive middle-dimensional algebraic de Rham cohomology of a smooth projective hypersurface defined over a number field that admits a real embedding. We use Griffiths' description of the cohomology in terms of a Jacobian ring. The resulting description is algebraic up to transcendental factors explicitly given by certain periods.","sentences":["We describe complex conjugation on the primitive middle-dimensional algebraic de Rham cohomology of a smooth projective hypersurface defined over a number field that admits a real embedding.","We use Griffiths' description of the cohomology in terms of a Jacobian ring.","The resulting description is algebraic up to transcendental factors explicitly given by certain periods."],"url":"http://arxiv.org/abs/2402.14546v1","category":"math.AG"}
{"created":"2024-02-22 13:32:07","title":"Low-frequency Resonances in Grid-Forming Converters: Causes and Damping Control","abstract":"Grid-forming voltage-source converter (GFM-VSC) may experience low-frequency resonances, such as synchronous resonance (SR) and sub-synchronous resonance (SSR), in the output power. This paper offers a comprehensive study on the root causes of low-frequency resonances with GFM-VSC systems and the damping control methods. The typical GFM control structures are introduced first, along with a mapping between the resonances and control loops. Then, the causes of SR and SSR are discussed, highlighting the impacts of control interactions on the resonances. Further, the recent advancements in stabilizing control methods for SR and SSR are critically reviewed with experimental tests of a GFM-VSC under different grid conditions.","sentences":["Grid-forming voltage-source converter (GFM-VSC) may experience low-frequency resonances, such as synchronous resonance (SR) and sub-synchronous resonance (SSR), in the output power.","This paper offers a comprehensive study on the root causes of low-frequency resonances with GFM-VSC systems and the damping control methods.","The typical GFM control structures are introduced first, along with a mapping between the resonances and control loops.","Then, the causes of SR and SSR are discussed, highlighting the impacts of control interactions on the resonances.","Further, the recent advancements in stabilizing control methods for SR and SSR are critically reviewed with experimental tests of a GFM-VSC under different grid conditions."],"url":"http://arxiv.org/abs/2402.14543v1","category":"eess.SY"}
{"created":"2024-02-22 13:30:52","title":"Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models","abstract":"Pandemics, with their profound societal and economic impacts, pose significant threats to global health, mortality rates, economic stability, and political landscapes. In response to the persistent challenges posed by emerging and reemerging pandemics, numerous studies have employed spatio-temporal models to enhance our understanding and management of these complex phenomena. These spatio-temporal models can be roughly divided into two main spatial categories: norm-based and graph-based trade-offering between accuracy, computational burden, and representational feasibility. In this study, we explore the ability to transform from norm-based to graph-based spatial representation for these models. We introduce a novel framework for this task together with twelve possible implementations using a wide range of heuristic optimization approaches. Our findings show that by leveraging agent-based simulations and heuristic algorithms for the graph node's location and population's spatial walk dynamics approximation one can use graph-based spatial representation without losing much of the model's accuracy and expressiveness. For three real-world cases, the best-performing algorithmic configuration archives 94\\% accuracy presence, on average. Moreover, an analysis of synthetic cases shows the proposed framework is relatively robust, as fluctuation in both spatial and temporal dynamics is not badly reflected by the framework's performance.","sentences":["Pandemics, with their profound societal and economic impacts, pose significant threats to global health, mortality rates, economic stability, and political landscapes.","In response to the persistent challenges posed by emerging and reemerging pandemics, numerous studies have employed spatio-temporal models to enhance our understanding and management of these complex phenomena.","These spatio-temporal models can be roughly divided into two main spatial categories: norm-based and graph-based trade-offering between accuracy, computational burden, and representational feasibility.","In this study, we explore the ability to transform from norm-based to graph-based spatial representation for these models.","We introduce a novel framework for this task together with twelve possible implementations using a wide range of heuristic optimization approaches.","Our findings show that by leveraging agent-based simulations and heuristic algorithms for the graph node's location and population's spatial walk dynamics approximation one can use graph-based spatial representation without losing much of the model's accuracy and expressiveness.","For three real-world cases, the best-performing algorithmic configuration archives 94\\% accuracy presence, on average.","Moreover, an analysis of synthetic cases shows the proposed framework is relatively robust, as fluctuation in both spatial and temporal dynamics is not badly reflected by the framework's performance."],"url":"http://arxiv.org/abs/2402.14539v1","category":"cs.IR"}
{"created":"2024-02-22 13:25:36","title":"Shubnikov-de Haas oscillations of biaxial-strain-tuned superconductors in pulsed magnetic field up to 60 T","abstract":"Two-dimensional (2D) materials have gained increasing prominence not only in fundamental research but also in daily applications. However, to fully harness their potential, it is crucial to optimize their properties with an external parameter and track the electronic structure simultaneously. Magnetotransport over a wide magnetic field range is a powerful method to probe the electronic structure and, for metallic 2D materials, quantum oscillations superimposed on the transport signals encode Fermi surface parameters. In this manuscript, we utilize biaxial strain as an external tuning parameter and investigate the effects of strain on the electronic properties of two quasi-2D superconductors, MoTe$_2$ and RbV$_3$Sb$_5$, by measuring their magnetoresistance in pulsed magnetic fields up to 60 T. With a careful selection of insulating substrates, we demonstrate the possibility of both the compressive and tensile biaxial strain, imposed on MoTe$_2$ and RbV$_3$Sb$_5$, respectively. For both systems, the applied strain has led to superconducting critical temperature enhancement compared to their free-standing counterparts, proving the effectiveness of this biaxial strain method at cryogenic temperatures. Clear quantum oscillations in the magnetoresistance -- the Shubnikov-de Haas (SdH) effect -- are obtained in both samples. In strained MoTe$_2$, the magnetoresistance exhibits a nearly quadratic dependence on the magnetic field and remains non-saturating even at the highest field. Whereas in strained RbV$_3$Sb$_5$, two SdH frequencies showed a substantial enhancement in effective mass values, hinting at a possible enhancement of charge fluctuations. Our results demonstrate that combining biaxial strain and pulsed magnetic field paves the way for studying 2D materials under unprecedented conditions.","sentences":["Two-dimensional (2D) materials have gained increasing prominence not only in fundamental research but also in daily applications.","However, to fully harness their potential, it is crucial to optimize their properties with an external parameter and track the electronic structure simultaneously.","Magnetotransport over a wide magnetic field range is a powerful method to probe the electronic structure and, for metallic 2D materials, quantum oscillations superimposed on the transport signals encode Fermi surface parameters.","In this manuscript, we utilize biaxial strain as an external tuning parameter and investigate the effects of strain on the electronic properties of two quasi-2D superconductors, MoTe$_2$ and RbV$_3$Sb$_5$, by measuring their magnetoresistance in pulsed magnetic fields up to 60 T. With a careful selection of insulating substrates, we demonstrate the possibility of both the compressive and tensile biaxial strain, imposed on MoTe$_2$ and RbV$_3$Sb$_5$, respectively.","For both systems, the applied strain has led to superconducting critical temperature enhancement compared to their free-standing counterparts, proving the effectiveness of this biaxial strain method at cryogenic temperatures.","Clear quantum oscillations in the magnetoresistance -- the Shubnikov-de Haas (SdH) effect -- are obtained in both samples.","In strained MoTe$_2$, the magnetoresistance exhibits a nearly quadratic dependence on the magnetic field and remains non-saturating even at the highest field.","Whereas in strained RbV$_3$Sb$_5$, two SdH frequencies showed a substantial enhancement in effective mass values, hinting at a possible enhancement of charge fluctuations.","Our results demonstrate that combining biaxial strain and pulsed magnetic field paves the way for studying 2D materials under unprecedented conditions."],"url":"http://arxiv.org/abs/2402.14534v1","category":"cond-mat.supr-con"}
{"created":"2024-02-22 13:10:17","title":"Strong-ARM Dynamic Latch Comparators: Design and Analyses on CAD Platform","abstract":"Strong-ARM Dynamic Latch Comparators are widely used in high-speed analog-to-digital converters (ADCs), sense amplifiers in memory, RFID applications, and data receivers. This paper presents different methods to improve the performance of Strong-Arm latch-based comparators. The comparator's significant features such as power dissipation, propagation delay, offset voltage, clock feedthrough, area, and kickback noises are discussed and compared with state-of-the-art candidate topologies. Simulation results show that the new comparator topologies of Strong-ARM Dynamic Latch proposed by these authors gave the best results. The proposed designs are tested. The simulations are carried out using UMC 180nm double metal, double poly standard CMOS process technology, for a 100 MHz clock, at 1.8V supply-rail on the Cadence Virtuoso EDA platform.","sentences":["Strong-ARM Dynamic Latch Comparators are widely used in high-speed analog-to-digital converters (ADCs), sense amplifiers in memory, RFID applications, and data receivers.","This paper presents different methods to improve the performance of Strong-Arm latch-based comparators.","The comparator's significant features such as power dissipation, propagation delay, offset voltage, clock feedthrough, area, and kickback noises are discussed and compared with state-of-the-art candidate topologies.","Simulation results show that the new comparator topologies of Strong-ARM Dynamic Latch proposed by these authors gave the best results.","The proposed designs are tested.","The simulations are carried out using UMC 180nm double metal, double poly standard CMOS process technology, for a 100 MHz clock, at 1.8V supply-rail on the Cadence Virtuoso EDA platform."],"url":"http://arxiv.org/abs/2402.14519v1","category":"eess.SY"}
{"created":"2024-02-22 13:07:11","title":"The elliptical invariant tori of nearly integrable Hamiltonian system through symplectic algorithms","abstract":"In this paper we apply symplectic algorithms to nearly integrable Hamiltonian system, and prove it can maintain lots of elliptic lower dimensional invariant tori.   We are committed to consider the elliptic lower dimensional invariant tori for symplectic mapping with a small twist under the R\\\"{u}ssmann's non-degenerate condition, and focus on its measure estimation. And then apply it to the nearly integrable Hamiltonian system to obtain lots of elliptic lower dimensional invariant tori.","sentences":["In this paper we apply symplectic algorithms to nearly integrable Hamiltonian system, and prove it can maintain lots of elliptic lower dimensional invariant tori.   ","We are committed to consider the elliptic lower dimensional invariant tori for symplectic mapping with a small twist under the R\\\"{u}ssmann's non-degenerate condition, and focus on its measure estimation.","And then apply it to the nearly integrable Hamiltonian system to obtain lots of elliptic lower dimensional invariant tori."],"url":"http://arxiv.org/abs/2402.14517v1","category":"math.DS"}
{"created":"2024-02-22 12:55:24","title":"Enhancing Rolling Horizon Production Planning Through Stochastic Optimization Evaluated by Means of Simulation","abstract":"Production planning must account for uncertainty in a production system, arising from fluctuating demand forecasts. Therefore, this article focuses on the integration of updated customer demand into the rolling horizon planning cycle. We use scenario-based stochastic programming to solve capacitated lot sizing problems under stochastic demand in a rolling horizon environment. This environment is replicated using a discrete event simulation-optimization framework, where the optimization problem is periodically solved, leveraging the latest demand information to continually adjust the production plan. We evaluate the stochastic optimization approach and compare its performance to solving a deterministic lot sizing model, using expected demand figures as input, as well as to standard Material Requirements Planning (MRP). In the simulation study, we analyze three different customer behaviors related to forecasting, along with four levels of shop load, within a multi-item and multi-stage production system. We test a range of significant parameter values for the three planning methods and compute the overall costs to benchmark them. The results show that the production plans obtained by MRP are outperformed by deterministic and stochastic optimization. Particularly, when facing tight resource restrictions and rising uncertainty in customer demand, the use of stochastic optimization becomes preferable compared to deterministic optimization.","sentences":["Production planning must account for uncertainty in a production system, arising from fluctuating demand forecasts.","Therefore, this article focuses on the integration of updated customer demand into the rolling horizon planning cycle.","We use scenario-based stochastic programming to solve capacitated lot sizing problems under stochastic demand in a rolling horizon environment.","This environment is replicated using a discrete event simulation-optimization framework, where the optimization problem is periodically solved, leveraging the latest demand information to continually adjust the production plan.","We evaluate the stochastic optimization approach and compare its performance to solving a deterministic lot sizing model, using expected demand figures as input, as well as to standard Material Requirements Planning (MRP).","In the simulation study, we analyze three different customer behaviors related to forecasting, along with four levels of shop load, within a multi-item and multi-stage production system.","We test a range of significant parameter values for the three planning methods and compute the overall costs to benchmark them.","The results show that the production plans obtained by MRP are outperformed by deterministic and stochastic optimization.","Particularly, when facing tight resource restrictions and rising uncertainty in customer demand, the use of stochastic optimization becomes preferable compared to deterministic optimization."],"url":"http://arxiv.org/abs/2402.14506v1","category":"econ.EM"}
{"created":"2024-02-22 12:51:03","title":"Ferroelectric texture of individual barium titanate nanocrystals","abstract":"Ferroelectric materials display exotic polarization textures at the nanoscale that could be used to improve the energetic efficiency of electronic components. The vast majority of studies were conducted in two dimensions on thin films, that can be further nanostructured, but very few studies address the situation of individual isolated nanocrystals synthesized in solution, while such structures could open other field of applications. In this work, we experimentally and theoretically studied the polarization texture of ferroelectric barium titanate (BaTiO$_3$, BTO) nanocrystals (NC) attached to a conductive substrate and surrounded by air. We synthesized NC of well defined quasi-cubic shape and 160 nm average size, that conserve the tetragonal structure of BTO at room temperature.   We then investigated the inverse piezoelectric properties of such pristine individual NC by piezoresponse force microscopy (PFM), taking particular care of suppressing electrostatic artifacts. In all the NC studied, we could not detect any vertical PFM signal, and the maps of the lateral response all displayed larger displacements on the edges. Using field-phase simulations dedicated to ferroelectric nanostructures, we were able to predict the equilibrium polarization texture. These simulations revealed that the NC core is composed of 180{\\deg} up and down domains defining the polar axis, that rotate by 90{\\deg} in the two facets orthogonal to this axis, eventually lying within these planes forming a layer of about 10 nm thickness mainly composed of 180{\\deg} domains along an edge. From this polarization distribution we predicted the lateral PFM response, that revealed to be in very good qualitative agreement with the experimental observations. This work positions PFM as a relevant tool to evaluate the potential of complex ferroelectric nanostructures to be used as sensors.","sentences":["Ferroelectric materials display exotic polarization textures at the nanoscale that could be used to improve the energetic efficiency of electronic components.","The vast majority of studies were conducted in two dimensions on thin films, that can be further nanostructured, but very few studies address the situation of individual isolated nanocrystals synthesized in solution, while such structures could open other field of applications.","In this work, we experimentally and theoretically studied the polarization texture of ferroelectric barium titanate (BaTiO$_3$, BTO) nanocrystals (NC) attached to a conductive substrate and surrounded by air.","We synthesized NC of well defined quasi-cubic shape and 160 nm average size, that conserve the tetragonal structure of BTO at room temperature.   ","We then investigated the inverse piezoelectric properties of such pristine individual NC by piezoresponse force microscopy (PFM), taking particular care of suppressing electrostatic artifacts.","In all the NC studied, we could not detect any vertical PFM signal, and the maps of the lateral response all displayed larger displacements on the edges.","Using field-phase simulations dedicated to ferroelectric nanostructures, we were able to predict the equilibrium polarization texture.","These simulations revealed that the NC core is composed of 180{\\deg} up and down domains defining the polar axis, that rotate by 90{\\deg} in the two facets orthogonal to this axis, eventually lying within these planes forming a layer of about 10 nm thickness mainly composed of 180{\\deg} domains along an edge.","From this polarization distribution we predicted the lateral PFM response, that revealed to be in very good qualitative agreement with the experimental observations.","This work positions PFM as a relevant tool to evaluate the potential of complex ferroelectric nanostructures to be used as sensors."],"url":"http://arxiv.org/abs/2402.14502v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 12:48:50","title":"Constructions of normal numbers with infinitely many digits","abstract":"Let $L=(L_d)_{d \\in \\mathbb N}$ be any ordered probability sequence, i.e., satisfying $0 < L_{d+1} \\le L_d$ for each $d \\in \\mathbb N$ and $\\sum_{d \\in \\mathbb N} L_d =1$. We construct sequences $A = (a_i)_{i \\in \\mathbb N}$ on the countably infinite alphabet $\\mathbb N$ in which each possible block of digits $\\alpha_1, \\ldots, \\alpha_k \\in \\mathbb N$, $k \\in \\mathbb N$, occurs with frequency $\\prod_{d=1}^k L_{\\alpha_d}$. In other words, we construct $L$-normal sequences. These sequences can then be projected to normal numbers in various affine number systems, such as real numbers $x \\in [0,1]$ that are normal in GLS number systems that correspond to the sequence $L$ or higher dimensional variants. In particular, this construction provides a family of numbers that have a normal L\\\"uroth expansion.","sentences":["Let $L=(L_d)_{d \\in \\mathbb N}$ be any ordered probability sequence, i.e., satisfying $0 < L_{d+1} \\le L_d$ for each $d \\in \\mathbb N$ and $\\sum_{d \\in \\mathbb N} L_d =1$. We construct sequences $A = (a_i)_{i \\in \\mathbb N}$ on the countably infinite alphabet $\\mathbb N$ in which each possible block of digits $\\alpha_1, \\ldots, \\alpha_k \\in \\mathbb N$, $k \\in \\mathbb N$, occurs with frequency $\\prod_{d=1}^k L_{\\alpha_d}$.","In other words, we construct $L$-normal sequences.","These sequences can then be projected to normal numbers in various affine number systems, such as real numbers $x \\in","[0,1]$ that are normal in GLS number systems that correspond to the sequence $L$ or higher dimensional variants.","In particular, this construction provides a family of numbers that have a normal L\\\"uroth expansion."],"url":"http://arxiv.org/abs/2402.14500v1","category":"math.NT"}
{"created":"2024-02-22 12:40:46","title":"First-principle tight-binding approach to angle-resolved photoemission spectroscopy simulations: importance of light-matter gauge and ubiquitous interference effects","abstract":"Angle-resolved photoemission spectroscopy (ARPES) is one of the most powerful techniques to study the electronic structure of materials. To go beyond the paradigm of band mapping and extract aspects of the Bloch wave-functions, the intricate interplay of experimental geometry, crystal structure, and photon polarization needs to be understood. In this work we discuss several model approaches to computing ARPES signals in a unified fashion. While we represent the Bloch wave-functions by first-principle Wannier functions, we introduce different approximations to the final states and discuss the implications for the predictive power. We also introduce various light-matter gauges and explain the role of the inevitable breaking of gauge invariance.Finally, we benchmark the different models for the two-dimensional semiconductor WSe$_2$, known for its strong Berry curvature, orbital angular momentum (OAM), and nontrivial orbital texture. The models are compared based on their ability to simulate photoemission intensity and interpret circular dichroism in ARPES (CD-ARPES). We show that interference effects are crucial to understanding the circular dichroism, and explain their photon-energy dependence. Our in-depth analysis provides insights into the advantages and limitations of various model approaches in clarifying the complex interplay between experimental observables and underlying orbital texture in materials.","sentences":["Angle-resolved photoemission spectroscopy (ARPES) is one of the most powerful techniques to study the electronic structure of materials.","To go beyond the paradigm of band mapping and extract aspects of the Bloch wave-functions, the intricate interplay of experimental geometry, crystal structure, and photon polarization needs to be understood.","In this work we discuss several model approaches to computing ARPES signals in a unified fashion.","While we represent the Bloch wave-functions by first-principle Wannier functions, we introduce different approximations to the final states and discuss the implications for the predictive power.","We also introduce various light-matter gauges and explain the role of the inevitable breaking of gauge invariance.","Finally, we benchmark the different models for the two-dimensional semiconductor WSe$_2$, known for its strong Berry curvature, orbital angular momentum (OAM), and nontrivial orbital texture.","The models are compared based on their ability to simulate photoemission intensity and interpret circular dichroism in ARPES (CD-ARPES).","We show that interference effects are crucial to understanding the circular dichroism, and explain their photon-energy dependence.","Our in-depth analysis provides insights into the advantages and limitations of various model approaches in clarifying the complex interplay between experimental observables and underlying orbital texture in materials."],"url":"http://arxiv.org/abs/2402.14496v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 12:27:38","title":"Imbalanced Data Clustering using Equilibrium K-Means","abstract":"Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various clustering algorithms, including HKM, FKM, maximum-entropy fuzzy clustering, two FKM variations designed for imbalanced data, and the Gaussian mixture model. The results demonstrate that EKM performs competitively on balanced data while significantly outperforming other techniques on imbalanced data. For high-dimensional data clustering, we demonstrate that a more discriminative representation can be obtained by mapping high-dimensional data via deep neural networks into a low-dimensional, EKM-friendly space. Deep clustering with EKM improves clustering accuracy by 35% on an imbalanced dataset derived from MNIST compared to deep clustering based on HKM.","sentences":["Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm).","This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters.","We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method.","EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition.","We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various clustering algorithms, including HKM, FKM, maximum-entropy fuzzy clustering, two FKM variations designed for imbalanced data, and the Gaussian mixture model.","The results demonstrate that EKM performs competitively on balanced data while significantly outperforming other techniques on imbalanced data.","For high-dimensional data clustering, we demonstrate that a more discriminative representation can be obtained by mapping high-dimensional data via deep neural networks into a low-dimensional, EKM-friendly space.","Deep clustering with EKM improves clustering accuracy by 35% on an imbalanced dataset derived from MNIST compared to deep clustering based on HKM."],"url":"http://arxiv.org/abs/2402.14490v1","category":"cs.LG"}
{"created":"2024-02-22 12:16:43","title":"MR-ARL: Model Reference Adaptive Reinforcement Learning for Robustly Stable On-Policy Data-Driven LQR","abstract":"This article introduces a novel framework for data-driven linear quadratic regulator (LQR) design. First, we introduce a reinforcement learning paradigm for on-policy data-driven LQR, where exploration and exploitation are simultaneously performed while guaranteeing robust stability of the whole closed-loop system encompassing the plant and the control/learning dynamics. Then, we propose Model Reference Adaptive Reinforcement Learning (MR-ARL), a control architecture integrating tools from reinforcement learning and model reference adaptive control. The approach stands on a variable reference model containing the currently identified value function. Then, an adaptive stabilizer is used to ensure convergence of the applied policy to the optimal one, convergence of the plant to the optimal reference model, and overall robust closed-loop stability. The proposed framework provides theoretical robustness certificates against real-world perturbations such as measurement noise, plant nonlinearities, or slowly varying parameters. The effectiveness of the proposed architecture is validated via realistic numerical simulations.","sentences":["This article introduces a novel framework for data-driven linear quadratic regulator (LQR) design.","First, we introduce a reinforcement learning paradigm for on-policy data-driven LQR, where exploration and exploitation are simultaneously performed while guaranteeing robust stability of the whole closed-loop system encompassing the plant and the control/learning dynamics.","Then, we propose Model Reference Adaptive Reinforcement Learning (MR-ARL), a control architecture integrating tools from reinforcement learning and model reference adaptive control.","The approach stands on a variable reference model containing the currently identified value function.","Then, an adaptive stabilizer is used to ensure convergence of the applied policy to the optimal one, convergence of the plant to the optimal reference model, and overall robust closed-loop stability.","The proposed framework provides theoretical robustness certificates against real-world perturbations such as measurement noise, plant nonlinearities, or slowly varying parameters.","The effectiveness of the proposed architecture is validated via realistic numerical simulations."],"url":"http://arxiv.org/abs/2402.14483v1","category":"eess.SY"}
{"created":"2024-02-22 12:13:33","title":"Existence and upper semicontinuity of pullback attractors for Kirchhoff wave equations in time-dependent spaces","abstract":"In this paper, we shall investigate the existence and upper semicontinuity of pullback attractors for non-autonomous Kirchhoff wave equations with a strong damping in the time-dependent space $X_t$. After deriving the existence and uniqueness of solutions by the Faedo-Galerkin approximation method, we establish the existence of pullback attractors. Later on, we prove the upper semicontinuity of pullback attractors between the Kirchhoff-type wave equations with $\\delta \\geq 0$ and the conventional wave equations with $\\delta=0$ by a series of complex energy estimates.","sentences":["In this paper, we shall investigate the existence and upper semicontinuity of pullback attractors for non-autonomous Kirchhoff wave equations with a strong damping in the time-dependent space $X_t$. After deriving the existence and uniqueness of solutions by the Faedo-Galerkin approximation method, we establish the existence of pullback attractors.","Later on, we prove the upper semicontinuity of pullback attractors between the Kirchhoff-type wave equations with $\\delta \\geq 0$ and the conventional wave equations with $\\delta=0$ by a series of complex energy estimates."],"url":"http://arxiv.org/abs/2402.14479v1","category":"math.AP"}
{"created":"2024-02-22 12:12:57","title":"A KAM theorem of symplectic algorithms for nearly integrabel Hamiltonian systems","abstract":"In this paper we prove a KAM-like theorem of symplectic algorithms for nearly integrable Hamiltonian systems which generalises the result of \\cite{r1} and \\cite{r6} for the case of integrable systems.","sentences":["In this paper we prove a KAM-like theorem of symplectic algorithms for nearly integrable Hamiltonian systems which generalises the result of \\cite{r1} and \\cite{r6} for the case of integrable systems."],"url":"http://arxiv.org/abs/2402.14478v1","category":"math.DS"}
{"created":"2024-02-22 12:11:41","title":"Pressure tunable magnetic skyrmion phase in Co8Zn8Mn4 single crystals","abstract":"In a magnetic skyrmion phase, magnetic moments form vortex-like topological textures which are of both fundamental and industrial interests. In $\\beta$-Mn-type Co-Zn-Mn alloys, chrial magnetic skyrmions emerge above room temperature, providing a unique system for studying the skrymion physics and exploring spintronics applications. However, the magnetic skyrmion phase is typically confined in a narrow and limited temperature ($T$) and magnetic field ($H$) range. Here, we demonstrate that hydrostatic pressure can expand the skyrmion phase in the $T-H$ phase diagram of single-crystalline Co$_8$Zn$_8$Mn$_4$. At ambient pressure, signatures of skyrmions are seen within $T\\sim302-308$ K and $H\\sim50-100$ Oe. Applying a moderate pressure of 6 kbar extends this range to $T\\sim300-310$ K and $H\\sim50-150$ Oe. However, further escalation of pressure to 10 kbar results in a slight contraction of the skyrmion phase. These findings underscore the sensitivity of the skyrmion phase in Co$_8$Zn$_8$Mn$_4$ to external pressures, and hint at the potential of strain engineering, particularly in $\\beta$-Mn-type Co-Zn-Mn thin films, as a promising avenue to customize the skyrmion phase.","sentences":["In a magnetic skyrmion phase, magnetic moments form vortex-like topological textures which are of both fundamental and industrial interests.","In $\\beta$-Mn-type Co-Zn-Mn alloys, chrial magnetic skyrmions emerge above room temperature, providing a unique system for studying the skrymion physics and exploring spintronics applications.","However, the magnetic skyrmion phase is typically confined in a narrow and limited temperature ($T$) and magnetic field ($H$) range.","Here, we demonstrate that hydrostatic pressure can expand the skyrmion phase in the $T-H$ phase diagram of single-crystalline Co$_8$Zn$_8$Mn$_4$.","At ambient pressure, signatures of skyrmions are seen within $T\\sim302-308$ K and $H\\sim50-100$ Oe.","Applying a moderate pressure of 6 kbar extends this range to $T\\sim300-310$ K and $H\\sim50-150$ Oe.","However, further escalation of pressure to 10 kbar results in a slight contraction of the skyrmion phase.","These findings underscore the sensitivity of the skyrmion phase in Co$_8$Zn$_8$Mn$_4$ to external pressures, and hint at the potential of strain engineering, particularly in $\\beta$-Mn-type Co-Zn-Mn thin films, as a promising avenue to customize the skyrmion phase."],"url":"http://arxiv.org/abs/2402.14477v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 12:10:50","title":"Quantifying neural network uncertainty under volatility clustering","abstract":"Time-series with time-varying variance pose a unique challenge to uncertainty quantification (UQ) methods. Time-varying variance, such as volatility clustering as seen in financial time-series, can lead to large mismatch between predicted uncertainty and forecast error. Building on recent advances in neural network UQ literature, we extend and simplify Deep Evidential Regression and Deep Ensembles into a unified framework to deal with UQ under the presence of volatility clustering. We show that a Scale Mixture Distribution is a simpler alternative to the Normal-Inverse-Gamma prior that provides favorable complexity-accuracy trade-off. To illustrate the performance of our proposed approach, we apply it to two sets of financial time-series exhibiting volatility clustering: cryptocurrencies and U.S. equities.","sentences":["Time-series with time-varying variance pose a unique challenge to uncertainty quantification (UQ) methods.","Time-varying variance, such as volatility clustering as seen in financial time-series, can lead to large mismatch between predicted uncertainty and forecast error.","Building on recent advances in neural network UQ literature, we extend and simplify Deep Evidential Regression and Deep Ensembles into a unified framework to deal with UQ under the presence of volatility clustering.","We show that a Scale Mixture Distribution is a simpler alternative to the Normal-Inverse-Gamma prior that provides favorable complexity-accuracy trade-off.","To illustrate the performance of our proposed approach, we apply it to two sets of financial time-series exhibiting volatility clustering: cryptocurrencies and U.S. equities."],"url":"http://arxiv.org/abs/2402.14476v1","category":"q-fin.ST"}
{"created":"2024-02-22 11:13:41","title":"Robust 1D proximity superconductivity along graphene domain walls in the quantum Hall regime","abstract":"Extensive efforts have been undertaken to combine superconductivity and the quantum Hall effect so that Cooper-pair transport between superconducting electrodes in Josephson junctions is mediated by one-dimensional (1D) edge states. This interest has been motivated by prospects of finding new physics, including topologically-protected quasiparticles, but also extends into metrology and device applications. So far it has proven challenging to achieve detectable supercurrents through quantum Hall conductors. Here we show that domain walls in minimally twisted bilayer graphene support exceptionally robust proximity superconductivity in the quantum Hall regime, allowing Josephson junctions operational in fields close to the upper critical field of superconducting electrodes. The critical current is found to be non-oscillatory, practically unchanging over the entire range of quantizing fields, with its value being limited by the quantum conductance of ballistic strictly-1D electronic channels residing within the domain walls. The described system is unique in its ability to support Andreev bound states in high fields and offers many interesting directions for further exploration.","sentences":["Extensive efforts have been undertaken to combine superconductivity and the quantum Hall effect so that Cooper-pair transport between superconducting electrodes in Josephson junctions is mediated by one-dimensional (1D) edge states.","This interest has been motivated by prospects of finding new physics, including topologically-protected quasiparticles, but also extends into metrology and device applications.","So far it has proven challenging to achieve detectable supercurrents through quantum Hall conductors.","Here we show that domain walls in minimally twisted bilayer graphene support exceptionally robust proximity superconductivity in the quantum Hall regime, allowing Josephson junctions operational in fields close to the upper critical field of superconducting electrodes.","The critical current is found to be non-oscillatory, practically unchanging over the entire range of quantizing fields, with its value being limited by the quantum conductance of ballistic strictly-1D electronic channels residing within the domain walls.","The described system is unique in its ability to support Andreev bound states in high fields and offers many interesting directions for further exploration."],"url":"http://arxiv.org/abs/2402.14451v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-22 11:06:07","title":"Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems","abstract":"Mathematical and computational tools have proven to be reliable in decision-making processes. In recent times, in particular, machine learning-based methods are becoming increasingly popular as advanced support tools. When dealing with control problems, reinforcement learning has been applied to decision-making in several applications, most notably in games. The success of these methods in finding solutions to complex problems motivates the exploration of new areas where they can be employed to overcome current difficulties. In this paper, we explore the use of automatic control strategies to initial boundary value problems in thermal and disease transport. Specifically, in this work, we adapt an existing reinforcement learning algorithm using a stochastic policy gradient method and we introduce two novel reward functions to drive the flow of the transported field. The new model-based framework exploits the interactions between a reaction-diffusion model and the modified agent. The results show that certain controls can be implemented successfully in these applications, although model simplifications had to be assumed.","sentences":["Mathematical and computational tools have proven to be reliable in decision-making processes.","In recent times, in particular, machine learning-based methods are becoming increasingly popular as advanced support tools.","When dealing with control problems, reinforcement learning has been applied to decision-making in several applications, most notably in games.","The success of these methods in finding solutions to complex problems motivates the exploration of new areas where they can be employed to overcome current difficulties.","In this paper, we explore the use of automatic control strategies to initial boundary value problems in thermal and disease transport.","Specifically, in this work, we adapt an existing reinforcement learning algorithm using a stochastic policy gradient method and we introduce two novel reward functions to drive the flow of the transported field.","The new model-based framework exploits the interactions between a reaction-diffusion model and the modified agent.","The results show that certain controls can be implemented successfully in these applications, although model simplifications had to be assumed."],"url":"http://arxiv.org/abs/2402.14446v1","category":"math.OC"}
{"created":"2024-02-22 10:54:12","title":"X-ray observations of Isolated Neutron Stars","abstract":"Pulsars are rapidly spinning neutron stars, that radiate at the expense of their strong magnetic field and their high surface temperature. Five decades of multi-wavelength observations showed a large variety of physical parameters, such as the spin period, the magnetic field and the age, and of observational properties, especially in the radio and X-ray band. Isolated neutron stars have been classified according to the presence of thermal or non-thermal emission, and whether they show a constant flux, rapid flares and bursts or long-standing outbursts. One of the current challenges in the study of such objects is to explain these different manifestations in the context of a unified evolutionary picture. On the other hand, recent findings show that the classes of isolated neutron stars are more connected than previously thought, and that non only magnetars hold a complex magnetic field topology in the crust and above the surface.","sentences":["Pulsars are rapidly spinning neutron stars, that radiate at the expense of their strong magnetic field and their high surface temperature.","Five decades of multi-wavelength observations showed a large variety of physical parameters, such as the spin period, the magnetic field and the age, and of observational properties, especially in the radio and X-ray band.","Isolated neutron stars have been classified according to the presence of thermal or non-thermal emission, and whether they show a constant flux, rapid flares and bursts or long-standing outbursts.","One of the current challenges in the study of such objects is to explain these different manifestations in the context of a unified evolutionary picture.","On the other hand, recent findings show that the classes of isolated neutron stars are more connected than previously thought, and that non only magnetars hold a complex magnetic field topology in the crust and above the surface."],"url":"http://arxiv.org/abs/2402.14442v1","category":"astro-ph.HE"}
{"created":"2024-02-22 10:39:03","title":"Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations","abstract":"Recommender systems have been widely used for various scenarios, such as e-commerce, news, and music, providing online contents to help and enrich users' daily life. Different scenarios hold distinct and unique characteristics, calling for domain-specific investigations and corresponding designed recommender systems. Therefore, in this paper, we focus on food delivery recommendations to unveil unique features in this domain, where users order food online and enjoy their meals shortly after delivery. We first conduct an in-depth analysis on food delivery datasets. The analysis shows that repeat orders are prevalent for both users and stores, and situations' differently influence repeat and exploration consumption in the food delivery recommender systems. Moreover, we revisit the ability of existing situation-aware methods for repeat and exploration recommendations respectively, and find them unable to effectively solve both tasks simultaneously. Based on the analysis and experiments, we have designed two separate recommendation models -- ReRec for repeat orders and ExpRec for exploration orders; both are simple in their design and computation. We conduct experiments on three real-world food delivery datasets, and our proposed models outperform various types of baselines on repeat, exploration, and combined recommendation tasks. This paper emphasizes the importance of dedicated analyses and methods for domain-specific characteristics for the recommender system studies.","sentences":["Recommender systems have been widely used for various scenarios, such as e-commerce, news, and music, providing online contents to help and enrich users' daily life.","Different scenarios hold distinct and unique characteristics, calling for domain-specific investigations and corresponding designed recommender systems.","Therefore, in this paper, we focus on food delivery recommendations to unveil unique features in this domain, where users order food online and enjoy their meals shortly after delivery.","We first conduct an in-depth analysis on food delivery datasets.","The analysis shows that repeat orders are prevalent for both users and stores, and situations' differently influence repeat and exploration consumption in the food delivery recommender systems.","Moreover, we revisit the ability of existing situation-aware methods for repeat and exploration recommendations respectively, and find them unable to effectively solve both tasks simultaneously.","Based on the analysis and experiments, we have designed two separate recommendation models -- ReRec for repeat orders and ExpRec for exploration orders; both are simple in their design and computation.","We conduct experiments on three real-world food delivery datasets, and our proposed models outperform various types of baselines on repeat, exploration, and combined recommendation tasks.","This paper emphasizes the importance of dedicated analyses and methods for domain-specific characteristics for the recommender system studies."],"url":"http://arxiv.org/abs/2402.14440v1","category":"cs.IR"}
{"created":"2024-02-22 10:22:27","title":"Exploring the Influence of Driving Context on Lateral Driving Style Preferences: A Simulator-Based Study","abstract":"Technological advancements focus on developing comfortable and acceptable driving characteristics in autonomous vehicles. Present driving functions predominantly possess predefined parameters, and there is no universally accepted driving style for autonomous vehicles. Although driving may be technically safe, the passenger might still feel insecure due to a mismatch in driving styles between the human and the autonomous system. Incorporating driving style preferences into automated vehicles enhances acceptance, reduces uncertainty, and poses the opportunity to expedite their adoption. Despite the increased research focus on driving styles, there remains a need for comprehensive studies investigating how variations in the driving context impact the assessment of automated driving functions. Therefore, this work evaluates lateral driving style preferences for autonomous vehicles on rural roads, considering different weather and traffic situations. A controlled study (N = 32) was conducted with a variety of German participants utilizing a high-fidelity driving simulator. The subjects experienced four different driving styles, including mimicking of their own driving behavior under two weather conditions. A notable preference for the more passive driving style became evident based on statistical analyses of participants' responses during and after the drives. A low curve-cutting gradient, moderate lateral and longitudinal acceleration constraints, and a pronounced reaction to oncoming traffic characterize this style. This study could not confirm the hypothesis that subjects prefer to be driven by mimicking their own driving behavior. Furthermore, the study illustrated that weather conditions and oncoming traffic substantially influence the perceived comfort during autonomous rides.","sentences":["Technological advancements focus on developing comfortable and acceptable driving characteristics in autonomous vehicles.","Present driving functions predominantly possess predefined parameters, and there is no universally accepted driving style for autonomous vehicles.","Although driving may be technically safe, the passenger might still feel insecure due to a mismatch in driving styles between the human and the autonomous system.","Incorporating driving style preferences into automated vehicles enhances acceptance, reduces uncertainty, and poses the opportunity to expedite their adoption.","Despite the increased research focus on driving styles, there remains a need for comprehensive studies investigating how variations in the driving context impact the assessment of automated driving functions.","Therefore, this work evaluates lateral driving style preferences for autonomous vehicles on rural roads, considering different weather and traffic situations.","A controlled study (N = 32) was conducted with a variety of German participants utilizing a high-fidelity driving simulator.","The subjects experienced four different driving styles, including mimicking of their own driving behavior under two weather conditions.","A notable preference for the more passive driving style became evident based on statistical analyses of participants' responses during and after the drives.","A low curve-cutting gradient, moderate lateral and longitudinal acceleration constraints, and a pronounced reaction to oncoming traffic characterize this style.","This study could not confirm the hypothesis that subjects prefer to be driven by mimicking their own driving behavior.","Furthermore, the study illustrated that weather conditions and oncoming traffic substantially influence the perceived comfort during autonomous rides."],"url":"http://arxiv.org/abs/2402.14432v1","category":"eess.SY"}
{"created":"2024-02-22 10:13:10","title":"Bounds for the zeros of Bicomplex Polynomials using matrix method","abstract":"In this paper we investigate bounds for the zeros of a bicomplex polynomial using matrix method. In particular, we find analogue of Gershgorin disk theorem, Cauchy Theorem, theorem of Fujiwara, Walsh and other theorems concerning to zeros of a polynomial to bicomplex polynomials.","sentences":["In this paper we investigate bounds for the zeros of a bicomplex polynomial using matrix method.","In particular, we find analogue of Gershgorin disk theorem, Cauchy Theorem, theorem of Fujiwara, Walsh and other theorems concerning to zeros of a polynomial to bicomplex polynomials."],"url":"http://arxiv.org/abs/2402.14425v1","category":"math.CV"}
{"created":"2024-02-22 10:10:25","title":"Investigations of optical aberration on quantum diamond microscopy toward high spatial resolution and sensitivity","abstract":"Quantum diamond microscopy (QDM), which employs nitrogen-vacancy (NV) center ensembles, is a promising approach to quantitatively imaging magnetic fields with both high resolution that approaches the diffraction limit and a wide field of view. The commonly adopted setups of QDM capture the photoluminescence through transparent diamonds, which inevitably entail aberrations -- optical errors that degrade the optical resolution and contrast of the obtainable image. In this study, we delve into the impact of optical aberrations, focusing on their dependence on diamond thickness. We first introduce a rigorous model [Richards et al., Braat et al.] of diffraction that incorporates aberrations, producing the NV center optical image. We confirm that this model accurately reproduces the confocal images of single NV centers obtained at various depths in diamonds. Extending this model to a wide-field microscope, we find that the model also accurately reproduces the USAF 1951 resolution test chart obtained through diamonds of various thicknesses. Based on these investigations, we quantitatively assess the consequent resolution constraints and propose thinning the diamond as a viable solution. We present a robust method to quantitatively ascertain resolution in optical systems influenced by aberrations caused by ray transmission through diamonds. For instance, for a typical microscope with an objective lens of NA = 0.7, the diffraction limit is achievable through diamonds that are 30 {\\mu}m thick, and a resolution of 1 {\\mu}m is obtained through diamonds that are 100 {\\mu}m thick. Those results opens up avenues for enhanced performance in QDM.","sentences":["Quantum diamond microscopy (QDM), which employs nitrogen-vacancy (NV) center ensembles, is a promising approach to quantitatively imaging magnetic fields with both high resolution that approaches the diffraction limit and a wide field of view.","The commonly adopted setups of QDM capture the photoluminescence through transparent diamonds, which inevitably entail aberrations -- optical errors that degrade the optical resolution and contrast of the obtainable image.","In this study, we delve into the impact of optical aberrations, focusing on their dependence on diamond thickness.","We first introduce a rigorous model","[Richards et al., Braat et al.] of diffraction that incorporates aberrations, producing the NV center optical image.","We confirm that this model accurately reproduces the confocal images of single NV centers obtained at various depths in diamonds.","Extending this model to a wide-field microscope, we find that the model also accurately reproduces the USAF 1951 resolution test chart obtained through diamonds of various thicknesses.","Based on these investigations, we quantitatively assess the consequent resolution constraints and propose thinning the diamond as a viable solution.","We present a robust method to quantitatively ascertain resolution in optical systems influenced by aberrations caused by ray transmission through diamonds.","For instance, for a typical microscope with an objective lens of NA = 0.7, the diffraction limit is achievable through diamonds that are 30 {\\mu}m thick, and a resolution of 1 {\\mu}m is obtained through diamonds that are 100 {\\mu}m thick.","Those results opens up avenues for enhanced performance in QDM."],"url":"http://arxiv.org/abs/2402.14422v1","category":"physics.optics"}
{"created":"2024-02-22 10:09:36","title":"Thurston obstructions and tropical geometry","abstract":"We describe an application of tropical moduli spaces to complex dynamics. A post-critically finite branched covering $\\varphi$ of $S^2$ induces a pullback map on the Teichm\\\"uller space of complex structures of $S^2$; this descends to an algebraic correspondence on the moduli space of point-configurations of $\\mathbb{C}\\mathbb{P}^1$. We make a case for studying the action of the tropical moduli space correspondence by making explicit the connections between objects that have come up in one guise in tropical geometry and in another guise in complex dynamics. For example, a Thurston obstruction for $\\varphi$ corresponds to a ray that is fixed by the tropical moduli space correspndence, and scaled by a factor $\\ge 1$. This article is intended to be accessible to algebraic and tropical geometers as well as to complex dynamicists.","sentences":["We describe an application of tropical moduli spaces to complex dynamics.","A post-critically finite branched covering $\\varphi$ of $S^2$ induces a pullback map on the Teichm\\\"uller space of complex structures of $S^2$; this descends to an algebraic correspondence on the moduli space of point-configurations of $\\mathbb{C}\\mathbb{P}^1$. We make a case for studying the action of the tropical moduli space correspondence by making explicit the connections between objects that have come up in one guise in tropical geometry and in another guise in complex dynamics.","For example, a Thurston obstruction for $\\varphi$ corresponds to a ray that is fixed by the tropical moduli space correspndence, and scaled by a factor $\\ge 1$.","This article is intended to be accessible to algebraic and tropical geometers as well as to complex dynamicists."],"url":"http://arxiv.org/abs/2402.14421v1","category":"math.DS"}
{"created":"2024-02-22 10:05:55","title":"On nonparametric estimation of the interaction function in particle system models","abstract":"This paper delves into a nonparametric estimation approach for the interaction function within diffusion-type particle system models. We introduce two estimation methods based upon an empirical risk minimization. Our study encompasses an analysis of the stochastic and approximation errors associated with both procedures, along with an examination of certain minimax lower bounds. In particular, we show that there is a natural metric under which the corresponding minimax estimation error of the interaction function converges to zero with parametric rate. This result is rather suprising given complexity of the underlying estimation problem and rather large classes of interaction functions for which the above parametric rate holds.","sentences":["This paper delves into a nonparametric estimation approach for the interaction function within diffusion-type particle system models.","We introduce two estimation methods based upon an empirical risk minimization.","Our study encompasses an analysis of the stochastic and approximation errors associated with both procedures, along with an examination of certain minimax lower bounds.","In particular, we show that there is a natural metric under which the corresponding minimax estimation error of the interaction function converges to zero with parametric rate.","This result is rather suprising given complexity of the underlying estimation problem and rather large classes of interaction functions for which the above parametric rate holds."],"url":"http://arxiv.org/abs/2402.14419v1","category":"math.ST"}
{"created":"2024-02-22 09:07:04","title":"Reading Relevant Feature from Global Representation Memory for Visual Object Tracking","abstract":"Reference features from a template or historical frames are crucial for visual object tracking. Prior works utilize all features from a fixed template or memory for visual object tracking. However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent. Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance. To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features. Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally. Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information. Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS.","sentences":["Reference features from a template or historical frames are crucial for visual object tracking.","Prior works utilize all features from a fixed template or memory for visual object tracking.","However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent.","Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance.","To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features.","Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally.","Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information.","Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS."],"url":"http://arxiv.org/abs/2402.14392v1","category":"cs.CV"}
{"created":"2024-02-22 09:04:41","title":"MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding","abstract":"Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the \"vocabulary\" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment \"vocabulary\" (i.e., codebook). Moreover, we propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input. With the learned microenvironment codebook, we can reuse it as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors.","sentences":["Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities.","The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction.","While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions.","To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features.","In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the \"vocabulary\" is usually extremely small.","This makes it difficult to cover the diversity and complexity of microenvironments.","In this paper, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment \"vocabulary\" (i.e., codebook).","Moreover, we propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input.","With the learned microenvironment codebook, we can reuse it as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction.","Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors."],"url":"http://arxiv.org/abs/2402.14391v1","category":"cs.LG"}
{"created":"2024-02-22 08:55:21","title":"WindDragon: Enhancing wind power forecasting with Automated Deep Learning","abstract":"Achieving net zero carbon emissions by 2050 requires the integration of increasing amounts of wind power into power grids. This energy source poses a challenge to system operators due to its variability and uncertainty. Therefore, accurate forecasting of wind power is critical for grid operation and system balancing. This paper presents an innovative approach to short-term (1 to 6 hour horizon) windpower forecasting at a national level. The method leverages Automated Deep Learning combined with Numerical Weather Predictions wind speed maps to accurately forecast wind power.","sentences":["Achieving net zero carbon emissions by 2050 requires the integration of increasing amounts of wind power into power grids.","This energy source poses a challenge to system operators due to its variability and uncertainty.","Therefore, accurate forecasting of wind power is critical for grid operation and system balancing.","This paper presents an innovative approach to short-term (1 to 6 hour horizon) windpower forecasting at a national level.","The method leverages Automated Deep Learning combined with Numerical Weather Predictions wind speed maps to accurately forecast wind power."],"url":"http://arxiv.org/abs/2402.14385v1","category":"cs.LG"}
{"created":"2024-02-22 08:48:59","title":"RadarMOSEVE: A Spatial-Temporal Transformer Network for Radar-Only Moving Object Segmentation and Ego-Velocity Estimation","abstract":"Moving object segmentation (MOS) and Ego velocity estimation (EVE) are vital capabilities for mobile systems to achieve full autonomy. Several approaches have attempted to achieve MOSEVE using a LiDAR sensor. However, LiDAR sensors are typically expensive and susceptible to adverse weather conditions. Instead, millimeter-wave radar (MWR) has gained popularity in robotics and autonomous driving for real applications due to its cost-effectiveness and resilience to bad weather. Nonetheless, publicly available MOSEVE datasets and approaches using radar data are limited. Some existing methods adopt point convolutional networks from LiDAR-based approaches, ignoring the specific artifacts and the valuable radial velocity information of radar measurements, leading to suboptimal performance. In this paper, we propose a novel transformer network that effectively addresses the sparsity and noise issues and leverages the radial velocity measurements of radar points using our devised radar self- and cross-attention mechanisms. Based on that, our method achieves accurate EVE of the robot and performs MOS using only radar data simultaneously. To thoroughly evaluate the MOSEVE performance of our method, we annotated the radar points in the public View-of-Delft (VoD) dataset and additionally constructed a new radar dataset in various environments. The experimental results demonstrate the superiority of our approach over existing state-of-the-art methods. The code is available at https://github.com/ORCA-Uboat/RadarMOSEVE.","sentences":["Moving object segmentation (MOS) and Ego velocity estimation (EVE) are vital capabilities for mobile systems to achieve full autonomy.","Several approaches have attempted to achieve MOSEVE using a LiDAR sensor.","However, LiDAR sensors are typically expensive and susceptible to adverse weather conditions.","Instead, millimeter-wave radar (MWR) has gained popularity in robotics and autonomous driving for real applications due to its cost-effectiveness and resilience to bad weather.","Nonetheless, publicly available MOSEVE datasets and approaches using radar data are limited.","Some existing methods adopt point convolutional networks from LiDAR-based approaches, ignoring the specific artifacts and the valuable radial velocity information of radar measurements, leading to suboptimal performance.","In this paper, we propose a novel transformer network that effectively addresses the sparsity and noise issues and leverages the radial velocity measurements of radar points using our devised radar self- and cross-attention mechanisms.","Based on that, our method achieves accurate EVE of the robot and performs MOS using only radar data simultaneously.","To thoroughly evaluate the MOSEVE performance of our method, we annotated the radar points in the public View-of-Delft (VoD) dataset and additionally constructed a new radar dataset in various environments.","The experimental results demonstrate the superiority of our approach over existing state-of-the-art methods.","The code is available at https://github.com/ORCA-Uboat/RadarMOSEVE."],"url":"http://arxiv.org/abs/2402.14380v1","category":"cs.RO"}
{"created":"2024-02-22 08:40:03","title":"Parameterized Complexity of Finding Dissimilar Shortest Paths","abstract":"We consider the problem of finding ``dissimilar'' $k$ shortest paths from $s$ to $t$ in an edge-weighted directed graph $D$, where the dissimilarity is measured by the minimum pairwise Hamming distances between these paths. More formally, given an edge-weighted directed graph $D = (V, A)$, two specified vertices $s, t \\in V$, and integers $d, k$, the goal of Dissimilar Shortest Paths is to decide whether $D$ has $k$ shortest paths $P_1, \\dots, P_k$ from $s$ to $t$ such that $|A(P_i) \\mathbin{\\triangle} A(P_j)| \\ge d$ for distinct $P_i$ and $P_j$. We design a deterministic algorithm to solve Dissimilar Shortest Paths with running time $2^{O(3^kdk^2)}n^{O(1)}$, that is, Dissimilar Shortest Paths is fixed-parameter tractable parameterized by $k + d$. To complement this positive result, we show that Dissimilar Shortest Paths is W[1]-hard when parameterized by only $k$ and paraNP-hard parameterized by $d$.","sentences":["We consider the problem of finding ``dissimilar'' $k$ shortest paths from $s$ to $t$ in an edge-weighted directed graph $D$, where the dissimilarity is measured by the minimum pairwise Hamming distances between these paths.","More formally, given an edge-weighted directed graph $D = (V, A)$, two specified vertices $s, t \\in V$, and integers $d, k$, the goal of Dissimilar Shortest Paths is to decide whether $D$ has $k$ shortest paths $P_1, \\dots, P_k$ from $s$ to $t$ such that $|A(P_i) \\mathbin{\\triangle} A(P_j)| \\ge d$ for distinct $P_i$ and $P_j$. We design a deterministic algorithm to solve Dissimilar Shortest Paths with running time $2^{O(3^kdk^2)}n^{O(1)}$, that is, Dissimilar Shortest Paths is fixed-parameter tractable parameterized by $k + d$.","To complement this positive result, we show that Dissimilar Shortest Paths is W[1]-hard when parameterized by only $k$ and paraNP-hard parameterized by $d$."],"url":"http://arxiv.org/abs/2402.14376v1","category":"cs.DS"}
{"created":"2024-02-22 08:29:38","title":"Closed-loop Data-Enabled Predictive Control and its equivalence with Closed-loop Subspace Predictive Control","abstract":"Factors like improved data availability and increasing system complexity have sparked interest in data-driven predictive control (DDPC) methods like Data-enabled Predictive Control (DeePC). However, closed-loop identification bias arises in the presence of noise, which reduces the effectiveness of obtained control policies. In this paper we propose Closed-loop Data-enabled Predictive Control (CL-DeePC), a framework that unifies different approaches to address this challenge. To this end, CL-DeePC incorporates instrumental variables (IVs) to synthesize and sequentially apply consistent single or multi-step-ahead predictors. Furthermore, a computationally efficient CL-DeePC implementation is developed that reveals an equivalence with Closed-loop Subspace Predictive Control (CL-SPC). Compared to DeePC, CL-DeePC simulations demonstrate superior reference tracking, with a sensitivity study finding a 48% lower susceptibility to noise-induced reference tracking performance degradation.","sentences":["Factors like improved data availability and increasing system complexity have sparked interest in data-driven predictive control (DDPC) methods like Data-enabled Predictive Control (DeePC).","However, closed-loop identification bias arises in the presence of noise, which reduces the effectiveness of obtained control policies.","In this paper we propose Closed-loop Data-enabled Predictive Control (CL-DeePC), a framework that unifies different approaches to address this challenge.","To this end, CL-DeePC incorporates instrumental variables (IVs) to synthesize and sequentially apply consistent single or multi-step-ahead predictors.","Furthermore, a computationally efficient CL-DeePC implementation is developed that reveals an equivalence with Closed-loop Subspace Predictive Control (CL-SPC).","Compared to DeePC, CL-DeePC simulations demonstrate superior reference tracking, with a sensitivity study finding a 48% lower susceptibility to noise-induced reference tracking performance degradation."],"url":"http://arxiv.org/abs/2402.14374v1","category":"eess.SY"}
{"created":"2024-02-22 08:21:43","title":"Exact non-Hermitian mobility edges and robust flat bands in two-dimensional Lieb lattices with imaginary quasiperiodic potentials","abstract":"The mobility edge (ME) is a critical energy delineates the boundary between extended and localized states within the energy spectrum, and it plays a crucial role in understanding the metal-insulator transition in disordered or quasiperiodic systems. While there have been extensive studies on MEs in one-dimensional non-Hermitian (NH) quasiperiodic lattices recently, the investigation of exact NH MEs in two-dimensional (2D) cases remains rare. In the present study, we introduce a 2D dissipative Lieb lattice (DLL) model with imaginary quasiperiodic potentials applied solely to the vertices of the Lieb lattice. By mapping this DLL model to the 2D NH Aubry-Andr{\\'e}-Harper (AAH) model, we analytically derive the exact ME and find it associated with the absolute eigenenergies. We find that the eigenvalues of extended states are purely imaginary when the quasiperiodic potential is strong enough. Additionally, we demonstrate that the introduction of imaginary quasiperiodic potentials does not disrupt the flat bands inherent in the system. Finally, we propose a theoretical framework for realizing our model using the Lindblad master equation. Our results pave the way for further investigation of exact NH MEs and flat bands in 2D dissipative quasiperiodic systems.","sentences":["The mobility edge (ME) is a critical energy delineates the boundary between extended and localized states within the energy spectrum, and it plays a crucial role in understanding the metal-insulator transition in disordered or quasiperiodic systems.","While there have been extensive studies on MEs in one-dimensional non-Hermitian (NH) quasiperiodic lattices recently, the investigation of exact NH MEs in two-dimensional (2D) cases remains rare.","In the present study, we introduce a 2D dissipative Lieb lattice (DLL) model with imaginary quasiperiodic potentials applied solely to the vertices of the Lieb lattice.","By mapping this DLL model to the 2D NH Aubry-Andr{\\'e}-Harper (AAH) model, we analytically derive the exact ME and find it associated with the absolute eigenenergies.","We find that the eigenvalues of extended states are purely imaginary when the quasiperiodic potential is strong enough.","Additionally, we demonstrate that the introduction of imaginary quasiperiodic potentials does not disrupt the flat bands inherent in the system.","Finally, we propose a theoretical framework for realizing our model using the Lindblad master equation.","Our results pave the way for further investigation of exact NH MEs and flat bands in 2D dissipative quasiperiodic systems."],"url":"http://arxiv.org/abs/2402.14370v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-22 08:16:43","title":"Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems","abstract":"Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers. However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended. Prior approaches to fairness-aware recommendation optimize a regularized objective to balance user satisfaction and item fairness based on some notion such as exposure fairness. These existing methods have been shown to be effective in controlling fairness, however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations. This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale recommender systems where millions of users and items exist. To enable a computationally efficient exposure control even for such large-scale systems, this work develops a scalable, fast, and fair method called \\emph{\\textbf{ex}posure-aware \\textbf{ADMM} (\\textbf{exADMM})}. exADMM is based on implicit alternating least squares (iALS), a conventional scalable algorithm for collaborative filtering, but optimizes a regularized objective to achieve a flexible control of accuracy-fairness tradeoff. A particular technical challenge in developing exADMM is the fact that the fairness regularizer destroys the separability of optimization subproblems for users and items, which is an essential property to ensure the scalability of iALS. Therefore, we develop a set of optimization tools to enable yet scalable fairness control with provable convergence guarantees as a basis of our algorithm.","sentences":["Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers.","However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended.","Prior approaches to fairness-aware recommendation optimize a regularized objective to balance user satisfaction and item fairness based on some notion such as exposure fairness.","These existing methods have been shown to be effective in controlling fairness, however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations.","This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale recommender systems where millions of users and items exist.","To enable a computationally efficient exposure control even for such large-scale systems, this work develops a scalable, fast, and fair method called \\emph{\\textbf{ex}posure-aware \\textbf{ADMM} (\\textbf{exADMM})}.","exADMM is based on implicit alternating least squares (iALS), a conventional scalable algorithm for collaborative filtering, but optimizes a regularized objective to achieve a flexible control of accuracy-fairness tradeoff.","A particular technical challenge in developing exADMM is the fact that the fairness regularizer destroys the separability of optimization subproblems for users and items, which is an essential property to ensure the scalability of iALS.","Therefore, we develop a set of optimization tools to enable yet scalable fairness control with provable convergence guarantees as a basis of our algorithm."],"url":"http://arxiv.org/abs/2402.14369v1","category":"cs.IR"}
{"created":"2024-02-22 08:07:56","title":"A method to correct the temporal drift of single photon detectors, based on asynchronous quantum ghost imaging","abstract":"Single photon detection and timing gathered increasing interest in the last few years due to both its necessity in the field of quantum sensing and the advantages of single quanta detection in the field of low level light imaging. While simple bucket detectors are mature enough for commercial applications, more complex imaging detectors are still a field of research with mostly prototype level detectors. A major problem in these detectors is the implementation of in-pixel timing circuitry, especially for two-dimensional imagers. One of the most promising approaches is the use of voltage controlled ring resonators in every pixel. Each of those is running independently, based on a voltage supplied by a global reference. However, this yields the problem that across the chip the supply voltage can change, which in turn changes the period of the ring resonator. Due to additional parasitic effects, this problem can worsen with increasing measurement time, leading to a drift of the timing information. We present here a method to identify and correct such temporal drifts of single photon detectors, based on asynchronous quantum ghost imaging. We also show the effect of this correction on a recent QGI measurement from our group.","sentences":["Single photon detection and timing gathered increasing interest in the last few years due to both its necessity in the field of quantum sensing and the advantages of single quanta detection in the field of low level light imaging.","While simple bucket detectors are mature enough for commercial applications, more complex imaging detectors are still a field of research with mostly prototype level detectors.","A major problem in these detectors is the implementation of in-pixel timing circuitry, especially for two-dimensional imagers.","One of the most promising approaches is the use of voltage controlled ring resonators in every pixel.","Each of those is running independently, based on a voltage supplied by a global reference.","However, this yields the problem that across the chip the supply voltage can change, which in turn changes the period of the ring resonator.","Due to additional parasitic effects, this problem can worsen with increasing measurement time, leading to a drift of the timing information.","We present here a method to identify and correct such temporal drifts of single photon detectors, based on asynchronous quantum ghost imaging.","We also show the effect of this correction on a recent QGI measurement from our group."],"url":"http://arxiv.org/abs/2402.14365v1","category":"quant-ph"}
{"created":"2024-02-22 08:05:05","title":"The Architecture of Sponge Choanocyte Chambers Maximizes Mechanical Pumping Efficiency","abstract":"Sponges, the basalmost members of the animal kingdom, exhibit a range of complex architectures in which microfluidic channels connect multitudes of spherical chambers lined with choanocytes, flagellated filter-feeding cells. Choanocyte chambers can possess scores or even hundreds of such cells, which drive complex flows entering through porous walls and exiting into the sponge channels. One of the mysteries of the choanocyte chamber is its spherical shape, as it seems inappropriate for inducing directional transport since many choanocyte flagella beat in opposition to such a flow. Here we combine direct imaging of choanocyte chambers in living sponges with computational studies of many-flagella models to understand the connection between chamber architecture and directional flow. We find that those flagella that beat against the flow play a key role in raising the pressure inside the choanocyte chamber, with the result that the mechanical pumping efficiency, calculated from the pressure rise and flow rate, reaches a maximum at a small outlet opening angle. Comparison between experimental observations and the results of numerical simulations reveal that the chamber diameter, flagellar wave number and the outlet opening angle of the freshwater sponge $E. muelleri$, as well as several other species, are related in a manner that maximizes the mechanical pumping efficiency. These results indicate the subtle balances at play during morphogenesis of choanocyte chambers, and give insights into the physiology and body design of sponges.","sentences":["Sponges, the basalmost members of the animal kingdom, exhibit a range of complex architectures in which microfluidic channels connect multitudes of spherical chambers lined with choanocytes, flagellated filter-feeding cells.","Choanocyte chambers can possess scores or even hundreds of such cells, which drive complex flows entering through porous walls and exiting into the sponge channels.","One of the mysteries of the choanocyte chamber is its spherical shape, as it seems inappropriate for inducing directional transport since many choanocyte flagella beat in opposition to such a flow.","Here we combine direct imaging of choanocyte chambers in living sponges with computational studies of many-flagella models to understand the connection between chamber architecture and directional flow.","We find that those flagella that beat against the flow play a key role in raising the pressure inside the choanocyte chamber, with the result that the mechanical pumping efficiency, calculated from the pressure rise and flow rate, reaches a maximum at a small outlet opening angle.","Comparison between experimental observations and the results of numerical simulations reveal that the chamber diameter, flagellar wave number and the outlet opening angle of the freshwater sponge $E. muelleri$, as well as several other species, are related in a manner that maximizes the mechanical pumping efficiency.","These results indicate the subtle balances at play during morphogenesis of choanocyte chambers, and give insights into the physiology and body design of sponges."],"url":"http://arxiv.org/abs/2402.14364v1","category":"cond-mat.soft"}
{"created":"2024-02-22 08:03:32","title":"Repeated erfc statistics for deformed GinUEs","abstract":"For an additive perturbation of the complex Ginibre ensemble under a deterministic matrix $X_0$, under certain assumption on $X_0$, we observe that there are only two kinds of local statistical patterns at the spectral edge: GinUE statistics and critical statistics, which corresponds to regular or quadratic vanishing spectral points.   As a continuation of our previous study on critical statistics \"Critical edge statistics for deformed GinUEs\"(arXiv: 2311.13227), in this paper we establish the local statistics of GinUE type at the regular spectral edge, which is characterized by a repeated erfc integral found in \"Phase transition of eigenvalues in deformed Ginibre ensembles\"(arXiv: 2204.13171v2).","sentences":["For an additive perturbation of the complex Ginibre ensemble under a deterministic matrix $X_0$, under certain assumption on $X_0$, we observe that there are only two kinds of local statistical patterns at the spectral edge: GinUE statistics and critical statistics, which corresponds to regular or quadratic vanishing spectral points.   ","As a continuation of our previous study on critical statistics \"Critical edge statistics for deformed GinUEs\"(arXiv: 2311.13227), in this paper we establish the local statistics of GinUE type at the regular spectral edge, which is characterized by a repeated erfc integral found in \"Phase transition of eigenvalues in deformed Ginibre ensembles\"(arXiv: 2204.13171v2)."],"url":"http://arxiv.org/abs/2402.14362v1","category":"math.PR"}
{"created":"2024-02-22 07:58:07","title":"Jackson integral representation for Kajihara's $q$-hypergeometric series and an extension of the $q$-Riemann-Papperitz system","abstract":"We give an integral representation for the Kajihara's $q$-hypergeometric series $W^{M,2}$. And we construct a $q$-difference system that corresponds to this integral. This system is an extension of the variant of $q$-hypergeometric equation of degree three, defined by Hatano-Matsunawa-Sato-Takemura. We show that this system includes the $q$-Appell-Lauricella system as a degeneration.","sentences":["We give an integral representation for the Kajihara's $q$-hypergeometric series $W^{M,2}$.","And we construct a $q$-difference system that corresponds to this integral.","This system is an extension of the variant of $q$-hypergeometric equation of degree three, defined by Hatano-Matsunawa-Sato-Takemura.","We show that this system includes the $q$-Appell-Lauricella system as a degeneration."],"url":"http://arxiv.org/abs/2402.14358v1","category":"math.CA"}
{"created":"2024-02-22 07:57:52","title":"Cellular Load Dependent Sleep Control for Energy Efficient HetNets with Non-Uniform User Distributions","abstract":"This study proposes a novel stochastic geometry framework analyzing power control strategies in spatially correlated network topologies. Heterogeneous networks are studied, with users modeled via the superposition of homogeneous and Poisson cluster processes. First, a new expression approaching the distribution of the number of users per base station is provided. This distribution defines the load associated with each Vorono\\\"i cell, capturing non-uniformities in user locations and correlation to BSs positions. The power allocation is adjusted based on this load, allowing BSs to enter sleep mode when their activity falls below a defined threshold. Furthermore, the propagation model features millimeter wave transmission characteristics and directional beamforming. Considering these aspects, revisited definitions of coverage probability, spectral efficiency, and energy efficiency are proposed. Tractable expressions for these metrics are derived and validated using Monte-Carlo simulations. Asymptotic expressions are also proposed, providing further understanding on the influence of the system parameters. Our numerical results finally analyze the impact of the sleep control on the performance and display the optimal strategies in terms of energy efficiency.","sentences":["This study proposes a novel stochastic geometry framework analyzing power control strategies in spatially correlated network topologies.","Heterogeneous networks are studied, with users modeled via the superposition of homogeneous and Poisson cluster processes.","First, a new expression approaching the distribution of the number of users per base station is provided.","This distribution defines the load associated with each Vorono\\\"i cell, capturing non-uniformities in user locations and correlation to BSs positions.","The power allocation is adjusted based on this load, allowing BSs to enter sleep mode when their activity falls below a defined threshold.","Furthermore, the propagation model features millimeter wave transmission characteristics and directional beamforming.","Considering these aspects, revisited definitions of coverage probability, spectral efficiency, and energy efficiency are proposed.","Tractable expressions for these metrics are derived and validated using Monte-Carlo simulations.","Asymptotic expressions are also proposed, providing further understanding on the influence of the system parameters.","Our numerical results finally analyze the impact of the sleep control on the performance and display the optimal strategies in terms of energy efficiency."],"url":"http://arxiv.org/abs/2402.14356v1","category":"eess.SP"}
{"created":"2024-02-22 07:51:23","title":"Heavenly metrics, hyper-Lagrangians and Joyce structures","abstract":"In \\cite{B3}, Bridgeland defined a geometric structure, named a Joyce structure, conjectured to exist on the space $M$ of stability conditions of a $CY_3$ triangulated category. A feature of this structure is a complex hyper-K\\\"ahler metric with homothetic symmetry on the total space $X = TM$ of the holomorphic tangent bundle. \\par Generalising the isomonodromy calculation which leads to the $A_2$ Joyce structure in \\cite{BM}, we obtain an explicit expression for a hyper-K\\\"ahler metric with homothetic symmetry via construction of the isomonodromic flows of a Schr\\\"odinger equation with deformed polynomial oscillator potential of odd degree $2n+1$. The metric is defined on a total space $X$ of complex dimension $4n$ and fibres over a $2n$--dimensional manifold $M$ which can be identified with the unfolding of the $A_{2n}$-singularity. The hyper-K\\\"ahler structure is shown to be compatible with the natural symplectic structure on $M$ in the sense of admitting an \\textit{affine symplectic fibration} as defined in \\cite{BS}. \\par Separately, using the additional conditions imposed by a Joyce structure, we consider reductions of Pleba\\'nski's heavenly equations that govern the hyper-K\\\"ahler condition. We introduce the notion of a \\textit{projectable hyper-Lagrangian} foliation and show that in dimension four such a foliation of $X$ leads to a linearisation of the heavenly equation.","sentences":["In \\cite{B3}, Bridgeland defined a geometric structure, named a Joyce structure, conjectured to exist on the space $M$ of stability conditions of a $CY_3$ triangulated category.","A feature of this structure is a complex hyper-K\\\"ahler metric with homothetic symmetry on the total space $X = TM$ of the holomorphic tangent bundle.","\\par","Generalising the isomonodromy calculation which leads to the $A_2$ Joyce structure in \\cite{BM}, we obtain an explicit expression for a hyper-K\\\"ahler metric with homothetic symmetry via construction of the isomonodromic flows of a Schr\\\"odinger equation with deformed polynomial oscillator potential of odd degree $2n+1$. The metric is defined on a total space $X$ of complex dimension $4n$ and fibres over a $2n$--dimensional manifold $M$ which can be identified with the unfolding of the $A_{2n}$-singularity.","The hyper-K\\\"ahler structure is shown to be compatible with the natural symplectic structure on $M$ in the sense of admitting an \\textit{affine symplectic fibration} as defined in \\cite{BS}.","\\par","Separately, using the additional conditions imposed by a Joyce structure, we consider reductions of Pleba\\'nski's heavenly equations that govern the hyper-K\\\"ahler condition.","We introduce the notion of a \\textit{projectable hyper-Lagrangian} foliation and show that in dimension four such a foliation of $X$ leads to a linearisation of the heavenly equation."],"url":"http://arxiv.org/abs/2402.14352v1","category":"math.DG"}
{"created":"2024-02-22 07:48:39","title":"Non-integrability of the rational Sasano system of type $A^{(2)}_4$","abstract":"In this paper we prove rigorously that for these values of the parameters for which the Sasano system of type $A^{(2)}_4$ has a particular rational solution it is not integrable by meromorphic first integrals which are rational functions in $t$. By an explicit computation we show that the connected component $G^0$ of the unit element of the differential Galois group of the normal variational equations along a simple particular rational solution is a direct product of two groups $SL_2(\\mathbb{C})$. This the Morales-Ramis theory leads to a non-integrable result. Moreover using B\\\"{a}cklund transformations we extend the obtained particular non-integrable result to the entire orbit of the parameters.","sentences":["In this paper we prove rigorously that for these values of the parameters for which the Sasano system of type $A^{(2)}_4$ has a particular rational solution it is not integrable by meromorphic first integrals which are rational functions in $t$. By an explicit computation we show that the connected component $G^0$ of the unit element of the differential Galois group of the normal variational equations along a simple particular rational solution is a direct product of two groups $SL_2(\\mathbb{C})$. This","the Morales-Ramis theory leads to a non-integrable result.","Moreover using B\\\"{a}cklund transformations we extend the obtained particular non-integrable result to the entire orbit of the parameters."],"url":"http://arxiv.org/abs/2402.14351v1","category":"math-ph"}
{"created":"2024-02-22 07:22:45","title":"An Error-Matching Exclusion Method for Accelerating Visual SLAM","abstract":"In Visual SLAM, achieving accurate feature matching consumes a significant amount of time, severely impacting the real-time performance of the system. This paper proposes an accelerated method for Visual SLAM by integrating GMS (Grid-based Motion Statistics) with RANSAC (Random Sample Consensus) for the removal of mismatched features. The approach first utilizes the GMS algorithm to estimate the quantity of matched pairs within the neighborhood and ranks the matches based on their confidence. Subsequently, the Random Sample Consensus (RANSAC) algorithm is employed to further eliminate mismatched features. To address the time-consuming issue of randomly selecting all matched pairs, this method transforms it into the problem of prioritizing sample selection from high-confidence matches. This enables the iterative solution of the optimal model. Experimental results demonstrate that the proposed method achieves a comparable accuracy to the original GMS-RANSAC while reducing the average runtime by 24.13% on the KITTI, TUM desk, and TUM doll datasets.","sentences":["In Visual SLAM, achieving accurate feature matching consumes a significant amount of time, severely impacting the real-time performance of the system.","This paper proposes an accelerated method for Visual SLAM by integrating GMS (Grid-based Motion Statistics) with RANSAC (Random Sample Consensus) for the removal of mismatched features.","The approach first utilizes the GMS algorithm to estimate the quantity of matched pairs within the neighborhood and ranks the matches based on their confidence.","Subsequently, the Random Sample Consensus (RANSAC) algorithm is employed to further eliminate mismatched features.","To address the time-consuming issue of randomly selecting all matched pairs, this method transforms it into the problem of prioritizing sample selection from high-confidence matches.","This enables the iterative solution of the optimal model.","Experimental results demonstrate that the proposed method achieves a comparable accuracy to the original GMS-RANSAC while reducing the average runtime by 24.13% on the KITTI, TUM desk, and TUM doll datasets."],"url":"http://arxiv.org/abs/2402.14345v1","category":"cs.CV"}
{"created":"2024-02-22 07:17:30","title":"TIE-KD: Teacher-Independent and Explainable Knowledge Distillation for Monocular Depth Estimation","abstract":"Monocular depth estimation (MDE) is essential for numerous applications yet is impeded by the substantial computational demands of accurate deep learning models. To mitigate this, we introduce a novel Teacher-Independent Explainable Knowledge Distillation (TIE-KD) framework that streamlines the knowledge transfer from complex teacher models to compact student networks, eliminating the need for architectural similarity. The cornerstone of TIE-KD is the Depth Probability Map (DPM), an explainable feature map that interprets the teacher's output, enabling feature-based knowledge distillation solely from the teacher's response. This approach allows for efficient student learning, leveraging the strengths of feature-based distillation. Extensive evaluation of the KITTI dataset indicates that TIE-KD not only outperforms conventional response-based KD methods but also demonstrates consistent efficacy across diverse teacher and student architectures. The robustness and adaptability of TIE-KD underscore its potential for applications requiring efficient and interpretable models, affirming its practicality for real-world deployment.","sentences":["Monocular depth estimation (MDE) is essential for numerous applications yet is impeded by the substantial computational demands of accurate deep learning models.","To mitigate this, we introduce a novel Teacher-Independent Explainable Knowledge Distillation (TIE-KD) framework that streamlines the knowledge transfer from complex teacher models to compact student networks, eliminating the need for architectural similarity.","The cornerstone of TIE-KD is the Depth Probability Map (DPM), an explainable feature map that interprets the teacher's output, enabling feature-based knowledge distillation solely from the teacher's response.","This approach allows for efficient student learning, leveraging the strengths of feature-based distillation.","Extensive evaluation of the KITTI dataset indicates that TIE-KD not only outperforms conventional response-based KD methods but also demonstrates consistent efficacy across diverse teacher and student architectures.","The robustness and adaptability of TIE-KD underscore its potential for applications requiring efficient and interpretable models, affirming its practicality for real-world deployment."],"url":"http://arxiv.org/abs/2402.14340v1","category":"cs.CV"}
{"created":"2024-02-22 07:12:34","title":"AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales","abstract":"Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationales and low-resource settings.","sentences":["Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks.","However, obtaining impeccable rationales is often impossible.","Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance.","Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of.","In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty.","We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness.","We then guide models to select one of two different reasoning models according to the ambiguity of rationales.","We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationales and low-resource settings."],"url":"http://arxiv.org/abs/2402.14337v1","category":"cs.CL"}
{"created":"2024-02-22 07:09:40","title":"Vibrational lifetimes and viscoelastic properties of ultrastable glasses","abstract":"Amorphous solids are viscoelastic. They dissipate energy when deformed at finite rate and finite temperature. We here use analytic theory and molecular simulations to demonstrate that linear viscoelastic dissipation can be directly related to the static and dynamic properties of the fundamental vibrational excitations of an amorphous system. We study ultrastable glasses that do not age, i.e. that remain in stable minima of the potential energy surface at finite temperature. Our simulations show four types of vibrational modes, which differ in spatial localization, similarity to plane waves and vibrational lifetimes. At frequencies below the Boson peak, the viscoelastic response can be split into contributions from plane-wave and quasilocalized modes. We derive a parameter-free expression for the viscoelastic storage and loss moduli for both of these modes. Our results show that the dynamics of microscopic dissipation, in particular the lifetimes of the modes, determine the viscoelastic response only at high frequency. Quasilocalized modes dominate the linear viscoelastic response at intermediate frequencies below the Boson peak.","sentences":["Amorphous solids are viscoelastic.","They dissipate energy when deformed at finite rate and finite temperature.","We here use analytic theory and molecular simulations to demonstrate that linear viscoelastic dissipation can be directly related to the static and dynamic properties of the fundamental vibrational excitations of an amorphous system.","We study ultrastable glasses that do not age, i.e. that remain in stable minima of the potential energy surface at finite temperature.","Our simulations show four types of vibrational modes, which differ in spatial localization, similarity to plane waves and vibrational lifetimes.","At frequencies below the Boson peak, the viscoelastic response can be split into contributions from plane-wave and quasilocalized modes.","We derive a parameter-free expression for the viscoelastic storage and loss moduli for both of these modes.","Our results show that the dynamics of microscopic dissipation, in particular the lifetimes of the modes, determine the viscoelastic response only at high frequency.","Quasilocalized modes dominate the linear viscoelastic response at intermediate frequencies below the Boson peak."],"url":"http://arxiv.org/abs/2402.14336v1","category":"cond-mat.soft"}
{"created":"2024-02-22 06:52:04","title":"Collisions of light bullets of different circular polarization","abstract":"For a locally isotropic focusing Kerr medium with anomalous chromatic dispersion, collisions of left- and right-polarized spatiotemporal optical solitons have been numerically simulated. Stable propagation of such ``light bullets'' in a moderate nonlinear regime is supported by a transverse parabolic profile of the refraction index in a multimode waveguide. In such systems, the transverse motion of centers of mass of wavepackets occurs on classical trajectories of the harmonic oscillator, while the longitudinal motion is uniform. Therefore tangent collisions of two solitons are possible as well as head-on collisions. As the result, an inelastic collision between two solitons with opposite circular polarizations can produce either two binary light bullets containing the left and the right polarization in some proportion, or more evolved bound structures.","sentences":["For a locally isotropic focusing Kerr medium with anomalous chromatic dispersion, collisions of left- and right-polarized spatiotemporal optical solitons have been numerically simulated.","Stable propagation of such ``light bullets'' in a moderate nonlinear regime is supported by a transverse parabolic profile of the refraction index in a multimode waveguide.","In such systems, the transverse motion of centers of mass of wavepackets occurs on classical trajectories of the harmonic oscillator, while the longitudinal motion is uniform.","Therefore tangent collisions of two solitons are possible as well as head-on collisions.","As the result, an inelastic collision between two solitons with opposite circular polarizations can produce either two binary light bullets containing the left and the right polarization in some proportion, or more evolved bound structures."],"url":"http://arxiv.org/abs/2402.14330v1","category":"physics.optics"}
{"created":"2024-02-22 06:47:44","title":"Subobject-level Image Tokenization","abstract":"Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level tokenization. Codes and models will be open-sourced at https://github.com/ChenDelong1999/subobjects.","sentences":["Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure.","Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models).","To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning.","Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level tokenization.","Codes and models will be open-sourced at https://github.com/ChenDelong1999/subobjects."],"url":"http://arxiv.org/abs/2402.14327v1","category":"cs.CV"}
{"created":"2024-02-22 06:38:25","title":"Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video Semantic Segmentation","abstract":"Offloading computing to edge servers is a promising solution to support growing video understanding applications at resource-constrained IoT devices. Recent efforts have been made to enhance the scalability of such systems by reducing inference costs on edge servers. However, existing research is not directly applicable to pixel-level vision tasks such as video semantic segmentation (VSS), partly due to the fluctuating VSS accuracy and segment bitrate caused by the dynamic video content. In response, we present Penance, a new edge inference cost reduction framework. By exploiting softmax outputs of VSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes model selection and compression settings to minimize the inference cost while meeting the required accuracy within the available bandwidth constraints. We implement Penance in a commercial IoT device with only CPUs. Experimental results show that Penance consumes a negligible 6.8% more computation resources than the optimal strategy while satisfying accuracy and bandwidth constraints with a low failure rate.","sentences":["Offloading computing to edge servers is a promising solution to support growing video understanding applications at resource-constrained IoT devices.","Recent efforts have been made to enhance the scalability of such systems by reducing inference costs on edge servers.","However, existing research is not directly applicable to pixel-level vision tasks such as video semantic segmentation (VSS), partly due to the fluctuating VSS accuracy and segment bitrate caused by the dynamic video content.","In response, we present Penance, a new edge inference cost reduction framework.","By exploiting softmax outputs of VSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes model selection and compression settings to minimize the inference cost while meeting the required accuracy within the available bandwidth constraints.","We implement Penance in a commercial IoT device with only CPUs.","Experimental results show that Penance consumes a negligible 6.8% more computation resources than the optimal strategy while satisfying accuracy and bandwidth constraints with a low failure rate."],"url":"http://arxiv.org/abs/2402.14326v1","category":"cs.MM"}
{"created":"2024-02-22 06:38:09","title":"AuroraMag: Twin Explorer of Asymmetry in Aurora and Solar Wind-Magnetosphere Coupling","abstract":"In the present-day context, small satellites and their constellations consisting of varying sizes (nano, micro, pico satellites) are being favored for remote sensing and in situ probing of the heliosphere and terrestrial magnetosphere-ionosphere system. We introduce a mission concept aimed at concurrently observing Earth's northern and southern auroral ovals while conducting in situ measurements of particles, fields, and temperature. The mission concept consists of two small satellites, each having an identical auroral X-ray imager, an in situ particle detector, a magnetometer pair, and an electron temperature analyzer onboard in an elliptical polar orbit (400X1000 km ). This mission would assist the space weather community in primarily answering important questions about the formation, morphology, and hemispherical asymmetries that we observe in the X-ray aurora, the fluxes of precipitating particles, Solar Energetic Particles, currents, and cusp dynamics. Once realized, this would be the first dedicated twin spacecraft mission of such kind to simultaneously study hemispheric asymmetries of solar-wind magnetosphere coupling. This study reveals the intricacies of the mission concept, encompassing orbital details, potential payloads, and its underlying scientific objectives. By leveraging the capabilities of small satellites, this mission concept is poised to make significant contributions to space weather monitoring and research.","sentences":["In the present-day context, small satellites and their constellations consisting of varying sizes (nano, micro, pico satellites) are being favored for remote sensing and in situ probing of the heliosphere and terrestrial magnetosphere-ionosphere system.","We introduce a mission concept aimed at concurrently observing Earth's northern and southern auroral ovals while conducting in situ measurements of particles, fields, and temperature.","The mission concept consists of two small satellites, each having an identical auroral X-ray imager, an in situ particle detector, a magnetometer pair, and an electron temperature analyzer onboard in an elliptical polar orbit (400X1000 km ).","This mission would assist the space weather community in primarily answering important questions about the formation, morphology, and hemispherical asymmetries that we observe in the X-ray aurora, the fluxes of precipitating particles, Solar Energetic Particles, currents, and cusp dynamics.","Once realized, this would be the first dedicated twin spacecraft mission of such kind to simultaneously study hemispheric asymmetries of solar-wind magnetosphere coupling.","This study reveals the intricacies of the mission concept, encompassing orbital details, potential payloads, and its underlying scientific objectives.","By leveraging the capabilities of small satellites, this mission concept is poised to make significant contributions to space weather monitoring and research."],"url":"http://arxiv.org/abs/2402.14325v1","category":"physics.space-ph"}
{"created":"2024-02-22 06:37:32","title":"Barium hexaferrite-based nanocomposites as Random Magnets for microwave absorption H","abstract":"The present work reports experimental evidence of random magnetic behavior observed in modified barium hexagonal ferrites. A significant transition in the magnetic properties of this system is observed when divalent cations (Ni2+, Cu2+, Mn2+) are introduced in the structure and give rise to a magnetic nanocomposite. Such introduction takes place in a random manner throughout each sample and creates the conditions for such materials to behave as random magnets. We verify the occurrence of such behavior in our samples by fitting the magnetization in approaching saturation to the corresponding theoretical model. We therefore analyze the microwave absorption capacities of random magnets in the GHz range and predict large and broad absorption signals under certain conditions. The findings presented here postulate, for the first time, ceramic materials as promising random magnets and underline their potential as microwave absorbers, in good agreement with recent theoretical models.","sentences":["The present work reports experimental evidence of random magnetic behavior observed in modified barium hexagonal ferrites.","A significant transition in the magnetic properties of this system is observed when divalent cations (Ni2+, Cu2+, Mn2+) are introduced in the structure and give rise to a magnetic nanocomposite.","Such introduction takes place in a random manner throughout each sample and creates the conditions for such materials to behave as random magnets.","We verify the occurrence of such behavior in our samples by fitting the magnetization in approaching saturation to the corresponding theoretical model.","We therefore analyze the microwave absorption capacities of random magnets in the GHz range and predict large and broad absorption signals under certain conditions.","The findings presented here postulate, for the first time, ceramic materials as promising random magnets and underline their potential as microwave absorbers, in good agreement with recent theoretical models."],"url":"http://arxiv.org/abs/2402.14324v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 06:19:41","title":"Oscillations between Grid-Forming Converters in Weakly Connected Offshore WPPs","abstract":"This paper studies control interactions between grid-forming (GFM) converters exhibited by power and frequency oscillations in a weakly connected offshore wind power plant (WPP). Two GFM controls are considered, namely virtual synchronous machine (VSM) and virtual admittance (VAdm) based GFM. The GFM control methods are implemented in wind turbine generators (WTGs) of a verified aggregated model of a WPP and the control interaction between these GFM WTGs is studied for several cases: cases with the same GFM control methods, and cases with different GFM control methods. A sensitivity analysis is performed for the observed oscillations to understand which system parameter affects the oscillations the most. Several solution methods are proposed and the inapplicability of some of the conventional solution methods are elaborated in this paper.","sentences":["This paper studies control interactions between grid-forming (GFM) converters exhibited by power and frequency oscillations in a weakly connected offshore wind power plant (WPP).","Two GFM controls are considered, namely virtual synchronous machine (VSM) and virtual admittance (VAdm) based GFM.","The GFM control methods are implemented in wind turbine generators (WTGs) of a verified aggregated model of a WPP and the control interaction between these GFM WTGs is studied for several cases: cases with the same GFM control methods, and cases with different GFM control methods.","A sensitivity analysis is performed for the observed oscillations to understand which system parameter affects the oscillations the most.","Several solution methods are proposed and the inapplicability of some of the conventional solution methods are elaborated in this paper."],"url":"http://arxiv.org/abs/2402.14317v1","category":"eess.SY"}
{"created":"2024-02-22 06:19:22","title":"Place Anything into Any Video","abstract":"Controllable video editing has demonstrated remarkable potential across diverse applications, particularly in scenarios where capturing or re-capturing real-world videos is either impractical or costly. This paper introduces a novel and efficient system named Place-Anything, which facilitates the insertion of any object into any video solely based on a picture or text description of the target object or element. The system comprises three modules: 3D generation, video reconstruction, and 3D target insertion. This integrated approach offers an efficient and effective solution for producing and editing high-quality videos by seamlessly inserting realistic objects. Through a user study, we demonstrate that our system can effortlessly place any object into any video using just a photograph of the object. Our demo video can be found at https://youtu.be/afXqgLLRnTE. Please also visit our project page https://place-anything.github.io to get access.","sentences":["Controllable video editing has demonstrated remarkable potential across diverse applications, particularly in scenarios where capturing or re-capturing real-world videos is either impractical or costly.","This paper introduces a novel and efficient system named Place-Anything, which facilitates the insertion of any object into any video solely based on a picture or text description of the target object or element.","The system comprises three modules: 3D generation, video reconstruction, and 3D target insertion.","This integrated approach offers an efficient and effective solution for producing and editing high-quality videos by seamlessly inserting realistic objects.","Through a user study, we demonstrate that our system can effortlessly place any object into any video using just a photograph of the object.","Our demo video can be found at https://youtu.be/afXqgLLRnTE.","Please also visit our project page https://place-anything.github.io to get access."],"url":"http://arxiv.org/abs/2402.14316v1","category":"cs.CV"}
{"created":"2024-02-22 06:17:11","title":"Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling","abstract":"Structure-based drug design aims at generating high affinity ligands with prior knowledge of 3D target structures. Existing methods either use conditional generative model to learn the distribution of 3D ligands given target binding sites, or iteratively modify molecules to optimize a structure-based activity estimator. The former is highly constrained by data quantity and quality, which leaves optimization-based approaches more promising in practical scenario. However, existing optimization-based approaches choose to edit molecules in 2D space, and use molecular docking to estimate the activity using docking predicted 3D target-ligand complexes. The misalignment between the action space and the objective hinders the performance of these models, especially for those employ deep learning for acceleration. In this work, we propose MolEdit3D to combine 3D molecular generation with optimization frameworks. We develop a novel 3D graph editing model to generate molecules using fragments, and pre-train this model on abundant 3D ligands for learning target-independent properties. Then we employ a target-guided self-learning strategy to improve target-related properties using self-sampled molecules. MolEdit3D achieves state-of-the-art performance on majority of the evaluation metrics, and demonstrate strong capability of capturing both target-dependent and -independent properties.","sentences":["Structure-based drug design aims at generating high affinity ligands with prior knowledge of 3D target structures.","Existing methods either use conditional generative model to learn the distribution of 3D ligands given target binding sites, or iteratively modify molecules to optimize a structure-based activity estimator.","The former is highly constrained by data quantity and quality, which leaves optimization-based approaches more promising in practical scenario.","However, existing optimization-based approaches choose to edit molecules in 2D space, and use molecular docking to estimate the activity using docking predicted 3D target-ligand complexes.","The misalignment between the action space and the objective hinders the performance of these models, especially for those employ deep learning for acceleration.","In this work, we propose MolEdit3D to combine 3D molecular generation with optimization frameworks.","We develop a novel 3D graph editing model to generate molecules using fragments, and pre-train this model on abundant 3D ligands for learning target-independent properties.","Then we employ a target-guided self-learning strategy to improve target-related properties using self-sampled molecules.","MolEdit3D achieves state-of-the-art performance on majority of the evaluation metrics, and demonstrate strong capability of capturing both target-dependent and -independent properties."],"url":"http://arxiv.org/abs/2402.14315v1","category":"q-bio.BM"}
{"created":"2024-02-22 06:15:51","title":"Typographic Text Generation with Off-the-Shelf Diffusion Model","abstract":"Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed system is a novel combination of two off-the-shelf methods for diffusion models, ControlNet and Blended Latent Diffusion. The former functions to generate text images under the guidance of edge conditions specifying stroke contours. The latter blends latent noise in Latent Diffusion Models (LDM) to add typographic text naturally onto an existing background. We first show that given appropriate text edges, ControlNet can generate texts in specified fonts while incorporating effects described by prompts. We further introduce text edge manipulation as an intuitive and customizable way to produce texts with complex effects such as ``shadows'' and ``reflections''. Finally, with the proposed system, we successfully add and modify texts on a predefined background while preserving its overall coherence.","sentences":["Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design.","This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects.","The proposed system is a novel combination of two off-the-shelf methods for diffusion models, ControlNet and Blended Latent Diffusion.","The former functions to generate text images under the guidance of edge conditions specifying stroke contours.","The latter blends latent noise in Latent Diffusion Models (LDM) to add typographic text naturally onto an existing background.","We first show that given appropriate text edges, ControlNet can generate texts in specified fonts while incorporating effects described by prompts.","We further introduce text edge manipulation as an intuitive and customizable way to produce texts with complex effects such as ``shadows'' and ``reflections''.","Finally, with the proposed system, we successfully add and modify texts on a predefined background while preserving its overall coherence."],"url":"http://arxiv.org/abs/2402.14314v1","category":"cs.CV"}
{"created":"2024-02-22 05:52:55","title":"An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes","abstract":"Convolutional neural networks (CNNs) with large kernels, drawing inspiration from the key operations of vision transformers (ViTs), have demonstrated impressive performance in various vision-based applications. To address the issue of computational efficiency degradation in existing designs for supporting large-kernel convolutions, an FPGA-based inference accelerator is proposed for the efficient deployment of CNNs with arbitrary kernel sizes. Firstly, a Z-flow method is presented to optimize the computing data flow by maximizing data reuse opportunity. Besides, the proposed design, incorporating the kernel-segmentation (Kseg) scheme, enables extended support for large-kernel convolutions, significantly reducing the storage requirements for overlapped data. Moreover, based on the analysis of typical block structures in emerging CNNs, vertical-fused (VF) and horizontal-fused (HF) methods are developed to optimize CNN deployments from both computation and transmission perspectives. The proposed hardware accelerator, evaluated on Intel Arria 10 FPGA, achieves up to 3.91 times better DSP efficiency than prior art on the same network. Particularly, it demonstrates efficient support for large-kernel CNNs, achieving throughputs of 169.68 GOPS and 244.55 GOPS for RepLKNet-31 and PyConvResNet-50, respectively, both of which are implemented on hardware for the first time.","sentences":["Convolutional neural networks (CNNs) with large kernels, drawing inspiration from the key operations of vision transformers (ViTs), have demonstrated impressive performance in various vision-based applications.","To address the issue of computational efficiency degradation in existing designs for supporting large-kernel convolutions, an FPGA-based inference accelerator is proposed for the efficient deployment of CNNs with arbitrary kernel sizes.","Firstly, a Z-flow method is presented to optimize the computing data flow by maximizing data reuse opportunity.","Besides, the proposed design, incorporating the kernel-segmentation (Kseg) scheme, enables extended support for large-kernel convolutions, significantly reducing the storage requirements for overlapped data.","Moreover, based on the analysis of typical block structures in emerging CNNs, vertical-fused (VF) and horizontal-fused (HF) methods are developed to optimize CNN deployments from both computation and transmission perspectives.","The proposed hardware accelerator, evaluated on Intel Arria 10 FPGA, achieves up to 3.91 times better DSP efficiency than prior art on the same network.","Particularly, it demonstrates efficient support for large-kernel CNNs, achieving throughputs of 169.68 GOPS and 244.55 GOPS for RepLKNet-31 and PyConvResNet-50, respectively, both of which are implemented on hardware for the first time."],"url":"http://arxiv.org/abs/2402.14307v1","category":"cs.AR"}
{"created":"2024-02-22 05:50:52","title":"Design and Validation of a Very Low-Power Phasor Measurement Unit","abstract":"Phasor measurement units (PMUs) provide a high-resolution view of the power system at the locations where they are placed. As such, it is desirable to place them in bulk in low voltage distribution circuits. However, the power consumption of a PMU/micro-PMU is in the order of Watts (W) that results in them requiring an external power supply, which in turn increases the overall cost. This work details the hardware design of a PMU capable of measuring and reporting voltage and current phasors for a single-phase system at an average power consumption of only 30.8 mW -- one to two orders of magnitude lower than existing academic and commercial PMUs. This enables the proposed PMU to run for two weeks using an 11-Wh battery or indefinitely if paired with an inexpensive solar panel. A test-bench developed in accordance with the 2018 IEC/IEEE 60255-118-1 PMU Standard confirms the accuracy of this PMU. Given its low power consumption, the proposed design is expected to accelerate adoption of PMUs in modern distribution grids.","sentences":["Phasor measurement units (PMUs) provide a high-resolution view of the power system at the locations where they are placed.","As such, it is desirable to place them in bulk in low voltage distribution circuits.","However, the power consumption of a PMU/micro-PMU is in the order of Watts (W) that results in them requiring an external power supply, which in turn increases the overall cost.","This work details the hardware design of a PMU capable of measuring and reporting voltage and current phasors for a single-phase system at an average power consumption of only 30.8 mW -- one to two orders of magnitude lower than existing academic and commercial PMUs.","This enables the proposed PMU to run for two weeks using an 11-Wh battery or indefinitely if paired with an inexpensive solar panel.","A test-bench developed in accordance with the 2018 IEC/IEEE 60255-118-1 PMU Standard confirms the accuracy of this PMU.","Given its low power consumption, the proposed design is expected to accelerate adoption of PMUs in modern distribution grids."],"url":"http://arxiv.org/abs/2402.14306v1","category":"eess.SP"}
{"created":"2024-02-22 05:48:54","title":"Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings","abstract":"In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items. Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems. To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve which captures the trade-off between group fairness and user utility by identifying a finite number of Pareto optimal solutions. We further propose an efficient method by relaxing our optimization problem on the Expohedron's circumscribed $n$-sphere, which significantly improve the running time. Moreover, the approximate Pareto curve is asymptotically close to the real Pareto optimal curve as the number of substantial solutions increases. Our methods are applicable with different ranking merits that are non-decreasing functions of item relevance. The effectiveness of our methods are validated through experiments on both synthetic and real-world datasets.","sentences":["In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items.","Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items.","However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems.","To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items.","On the Expohedron, we profile the Pareto curve which captures the trade-off between group fairness and user utility by identifying a finite number of Pareto optimal solutions.","We further propose an efficient method by relaxing our optimization problem on the Expohedron's circumscribed $n$-sphere, which significantly improve the running time.","Moreover, the approximate Pareto curve is asymptotically close to the real Pareto optimal curve as the number of substantial solutions increases.","Our methods are applicable with different ranking merits that are non-decreasing functions of item relevance.","The effectiveness of our methods are validated through experiments on both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2402.14305v1","category":"cs.IR"}
{"created":"2024-02-22 05:18:07","title":"Semantics-Empowered Space-Air-Ground-Sea Integrated Network: New Paradigm, Frameworks, and Challenges","abstract":"In the coming sixth generation (6G) communication era, to provide seamless and ubiquitous connections, the space-air-ground-sea integrated network (SAGSIN) is envisioned to address the challenges of communication coverage in areas with difficult conditions, such as the forest, desert, and sea. Considering the fundamental limitations of the SAGSIN including large-scale scenarios, highly dynamic channels, and limited device capabilities, traditional communications based on Shannon information theory cannot satisfy the communication demands. Moreover, bit-level reconstruction is usually redundant for many human-to-machine or machine-to-machine applications in the SAGSIN. Therefore, it is imperative to consider high-level communications towards semantics exchange, called semantic communications. In this survey, according to the interpretations of the term \"semantics\", including \"significance\", \"meaning\", and \"effectiveness-related information\", we review state-of-the-art works on semantic communications from three perspectives, which are 1) significance representation and protection, 2) meaning similarity measurement and meaning enhancement, and 3) ultimate effectiveness and effectiveness yielding. Sequentially, three types of semantic communication systems can be correspondingly introduced, namely the significance-oriented, meaning-oriented, and effectiveness/task-oriented semantic communication systems. Implementation of the above three types of systems in the SAGSIN necessitates a new perception-communication-computing-actuation-integrated paradigm (PCCAIP), where all the available perception, computing, and actuation techniques jointly facilitates significance-oriented sampling & transmission, semantic extraction & reconstruction, and task decision. Finally, we point out some future challenges on semantic communications in the SAGSIN. ...","sentences":["In the coming sixth generation (6G) communication era, to provide seamless and ubiquitous connections, the space-air-ground-sea integrated network (SAGSIN) is envisioned to address the challenges of communication coverage in areas with difficult conditions, such as the forest, desert, and sea.","Considering the fundamental limitations of the SAGSIN including large-scale scenarios, highly dynamic channels, and limited device capabilities, traditional communications based on Shannon information theory cannot satisfy the communication demands.","Moreover, bit-level reconstruction is usually redundant for many human-to-machine or machine-to-machine applications in the SAGSIN.","Therefore, it is imperative to consider high-level communications towards semantics exchange, called semantic communications.","In this survey, according to the interpretations of the term \"semantics\", including \"significance\", \"meaning\", and \"effectiveness-related information\", we review state-of-the-art works on semantic communications from three perspectives, which are 1) significance representation and protection, 2) meaning similarity measurement and meaning enhancement, and 3) ultimate effectiveness and effectiveness yielding.","Sequentially, three types of semantic communication systems can be correspondingly introduced, namely the significance-oriented, meaning-oriented, and effectiveness/task-oriented semantic communication systems.","Implementation of the above three types of systems in the SAGSIN necessitates a new perception-communication-computing-actuation-integrated paradigm (PCCAIP), where all the available perception, computing, and actuation techniques jointly facilitates significance-oriented sampling & transmission, semantic extraction & reconstruction, and task decision.","Finally, we point out some future challenges on semantic communications in the SAGSIN. ..."],"url":"http://arxiv.org/abs/2402.14297v1","category":"cs.IT"}
{"created":"2024-02-22 05:07:31","title":"CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations","abstract":"As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three metrics while preserving semantic content, using less training data, and containing fewer parameters.","sentences":["As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application.","Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text.","In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content).","We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three metrics while preserving semantic content, using less training data, and containing fewer parameters."],"url":"http://arxiv.org/abs/2402.14290v1","category":"cs.CL"}
{"created":"2024-02-22 05:01:02","title":"Properties of H particle-admixed compact star","abstract":"We explore the potential manifestation of a hexaquark, the H particle, as a constituent within neutron stars. The H particle, characterized by a quark composition of $uuddss$, is constructed using the framework of Chromomagnetic Interaction (CMI). Specifically, we contemplate the flavor-singlet state H with $J^P=0^+$. Our computations indicate that the three-flavor hexaquark state, the H particle, possesses a lower mass of $2212.7~\\rm{MeV}$ in comparison to the $d^*(2380)$, implying greater stability than the two-flavor $d^*(2380)$. The analysis involving the H particle is carried out using the relativistic mean-field (RMF) model. We investigate the influence of H particle couplings, a key factor in determining the system stability, and focus on the potential existence of H particle within neutron stars. We find that H particle could potentially endure as a stable constituent within neutron stars, and lead to a reduction of the maximum mass.","sentences":["We explore the potential manifestation of a hexaquark, the H particle, as a constituent within neutron stars.","The H particle, characterized by a quark composition of $uuddss$, is constructed using the framework of Chromomagnetic Interaction (CMI).","Specifically, we contemplate the flavor-singlet state H with $J^P=0^+$. Our computations indicate that the three-flavor hexaquark state, the H particle, possesses a lower mass of $2212.7~\\rm{MeV}$ in comparison to the $d^*(2380)$, implying greater stability than the two-flavor $d^*(2380)$. The analysis involving the H particle is carried out using the relativistic mean-field (RMF) model.","We investigate the influence of H particle couplings, a key factor in determining the system stability, and focus on the potential existence of H particle within neutron stars.","We find that H particle could potentially endure as a stable constituent within neutron stars, and lead to a reduction of the maximum mass."],"url":"http://arxiv.org/abs/2402.14288v1","category":"nucl-th"}
{"created":"2024-02-22 04:48:15","title":"Classification of smooth Fano varieties with large pseudoindex","abstract":"Let $X$ be a complex smooth Fano variety of dimension at least four. In this paper, we classify such $X$ when the pseudoindex is at least $n-2$ and the Picard number greater than one. We also discuss the relations between pseudoindex and other invariants of Fano varieties.","sentences":["Let $X$ be a complex smooth Fano variety of dimension at least four.","In this paper, we classify such $X$ when the pseudoindex is at least $n-2$ and the Picard number greater than one.","We also discuss the relations between pseudoindex and other invariants of Fano varieties."],"url":"http://arxiv.org/abs/2402.14283v1","category":"math.AG"}
{"created":"2024-02-22 04:36:49","title":"Locality Bounds for Sampling Hamming Slices","abstract":"Spurred by the influential work of Viola (Journal of Computing 2012), the past decade has witnessed an active line of research into the complexity of (approximately) sampling distributions, in contrast to the traditional focus on the complexity of computing functions.   We build upon and make explicit earlier implicit results of Viola to provide superconstant lower bounds on the locality of Boolean functions approximately sampling the uniform distribution over binary strings of particular Hamming weights, both exactly and modulo an integer, answering questions of Viola (Journal of Computing 2012) and Filmus, Leigh, Riazanov, and Sokolov (RANDOM 2023). Applications to data structure lower bounds and quantum-classical separations are discussed.","sentences":["Spurred by the influential work of Viola (Journal of Computing 2012), the past decade has witnessed an active line of research into the complexity of (approximately) sampling distributions, in contrast to the traditional focus on the complexity of computing functions.   ","We build upon and make explicit earlier implicit results of Viola to provide superconstant lower bounds on the locality of Boolean functions approximately sampling the uniform distribution over binary strings of particular Hamming weights, both exactly and modulo an integer, answering questions of Viola (Journal of Computing 2012) and Filmus, Leigh, Riazanov, and Sokolov (RANDOM 2023).","Applications to data structure lower bounds and quantum-classical separations are discussed."],"url":"http://arxiv.org/abs/2402.14278v1","category":"cs.CC"}
{"created":"2024-02-22 04:20:14","title":"Can Language Models Act as Knowledge Bases at Scale?","abstract":"Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating responses to complex queries through large-scale pre-training. However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable. Addressing this gap, our research investigates whether LLMs can effectively store, recall, and reason with knowledge on a large scale comparable to latest knowledge bases (KBs) such as Wikidata. Specifically, we focus on three crucial aspects to study the viability: (1) the efficiency of LLMs with different sizes in memorizing the exact knowledge in the large-scale KB; (2) the flexibility of recalling the memorized knowledge in response to natural language queries; (3) the capability to infer new knowledge through reasoning. Our findings indicate that while LLMs hold promise as large-scale KBs capable of retrieving and responding with flexibility, enhancements in their reasoning capabilities are necessary to fully realize their potential.","sentences":["Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating responses to complex queries through large-scale pre-training.","However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable.","Addressing this gap, our research investigates whether LLMs can effectively store, recall, and reason with knowledge on a large scale comparable to latest knowledge bases (KBs) such as Wikidata.","Specifically, we focus on three crucial aspects to study the viability: (1) the efficiency of LLMs with different sizes in memorizing the exact knowledge in the large-scale KB; (2) the flexibility of recalling the memorized knowledge in response to natural language queries; (3) the capability to infer new knowledge through reasoning.","Our findings indicate that while LLMs hold promise as large-scale KBs capable of retrieving and responding with flexibility, enhancements in their reasoning capabilities are necessary to fully realize their potential."],"url":"http://arxiv.org/abs/2402.14273v1","category":"cs.CL"}
{"created":"2024-02-22 04:14:10","title":"Qsnail: A Questionnaire Dataset for Sequential Question Generation","abstract":"The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors. However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure. Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure. Specifically, the questions should be relevant and specific to the given research topic and intent. The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly. Moreover, the sequence of questions should follow a logical order, grouping similar topics together. As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets. To address these issues, we present Qsnail, the first dataset specifically constructed for the questionnaire generation task, which comprises 13,168 human-written questionnaires gathered from online platforms. We further conduct experiments on Qsnail, and the results reveal that retrieval models and traditional generative models do not fully align with the given research topic and intents. Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires. Therefore, questionnaire generation is challenging and needs to be further explored. The dataset is available at: https://github.com/LeiyanGithub/qsnail.","sentences":["The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors.","However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure.","Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure.","Specifically, the questions should be relevant and specific to the given research topic and intent.","The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly.","Moreover, the sequence of questions should follow a logical order, grouping similar topics together.","As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets.","To address these issues, we present Qsnail, the first dataset specifically constructed for the questionnaire generation task, which comprises 13,168 human-written questionnaires gathered from online platforms.","We further conduct experiments on Qsnail, and the results reveal that retrieval models and traditional generative models do not fully align with the given research topic and intents.","Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity.","Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires.","Therefore, questionnaire generation is challenging and needs to be further explored.","The dataset is available at: https://github.com/LeiyanGithub/qsnail."],"url":"http://arxiv.org/abs/2402.14272v1","category":"cs.CL"}
{"created":"2024-02-22 04:14:06","title":"Hyers-Ulam stability of the first order difference equation with average growth rate","abstract":"The first order difference equation induced by the sequence of maps on $ \\mathbb{C} $ has Hyers-Ulam stability where the limit of the geometric average of growth rate is convergent and not equal to one. %The average growth rate is a generalization of contracting or expanding constant of maps. We show no Hyers-Ulam stability where the average growth rate is (pre)periodic even though each periodic growth rate is strictly less than one. Examples of difference equation generated by time dependent maps which contains contracting maps and expanding maps are given.","sentences":["The first order difference equation induced by the sequence of maps on $ \\mathbb{C} $ has Hyers-Ulam stability where the limit of the geometric average of growth rate is convergent and not equal to one.","%","The average growth rate is a generalization of contracting or expanding constant of maps.","We show no Hyers-Ulam stability where the average growth rate is (pre)periodic even though each periodic growth rate is strictly less than one.","Examples of difference equation generated by time dependent maps which contains contracting maps and expanding maps are given."],"url":"http://arxiv.org/abs/2402.14271v1","category":"math.DS"}
{"created":"2024-02-22 04:10:57","title":"Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization","abstract":"In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protocols. Through rigorous experimentation with various models and datasets, our findings indicate that our sample-targeted methods significantly improve LLM performance across multiple benchmarks, in both continual pre-training and instruction tuning scenarios. Our codes are available at https://github.com/VITA-Group/HardFocusTraining.","sentences":["In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data.","Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses.","These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity.","We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO).","IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protocols.","Through rigorous experimentation with various models and datasets, our findings indicate that our sample-targeted methods significantly improve LLM performance across multiple benchmarks, in both continual pre-training and instruction tuning scenarios.","Our codes are available at https://github.com/VITA-Group/HardFocusTraining."],"url":"http://arxiv.org/abs/2402.14270v1","category":"cs.LG"}
{"created":"2024-02-22 04:04:50","title":"Efficient Solvers for Wyner Common Information with Application to Multi-Modal Clustering","abstract":"We propose two novel extensions of the Wyner common information optimization problem. Each relaxes one fundamental constraints in Wyner's formulation. The \\textit{Variational Wyner Common Information} relaxes the matching constraint to the known distribution while imposing conditional independence to the feasible solution set. We derive a tight surrogate upper bound of the obtained unconstrained Lagrangian via the theory of variational inference, which can be minimized efficiently. Our solver caters to problems where conditional independence holds with significantly reduced computation complexity; On the other hand, the \\textit{Bipartite Wyner Common Information} relaxes the conditional independence constraint whereas the matching condition is enforced on the feasible set. By leveraging the difference-of-convex structure of the formulated optimization problem, we show that our solver is resilient to conditional dependent sources. Both solvers are provably convergent (local stationary points), and empirically, they obtain more accurate solutions to Wyner's formulation with substantially less runtime. Moreover, them can be extended to unknown distribution settings by parameterizing the common randomness as a member of the exponential family of distributions. Our approaches apply to multi-modal clustering problems, where multiple modalities of observations come from the same cluster. Empirically, our solvers outperform the state-of-the-art multi-modal clustering algorithms with significantly improved performance.","sentences":["We propose two novel extensions of the Wyner common information optimization problem.","Each relaxes one fundamental constraints in Wyner's formulation.","The \\textit{Variational Wyner Common Information} relaxes the matching constraint to the known distribution while imposing conditional independence to the feasible solution set.","We derive a tight surrogate upper bound of the obtained unconstrained Lagrangian via the theory of variational inference, which can be minimized efficiently.","Our solver caters to problems where conditional independence holds with significantly reduced computation complexity; On the other hand, the \\textit{Bipartite Wyner Common Information} relaxes the conditional independence constraint whereas the matching condition is enforced on the feasible set.","By leveraging the difference-of-convex structure of the formulated optimization problem, we show that our solver is resilient to conditional dependent sources.","Both solvers are provably convergent (local stationary points), and empirically, they obtain more accurate solutions to Wyner's formulation with substantially less runtime.","Moreover, them can be extended to unknown distribution settings by parameterizing the common randomness as a member of the exponential family of distributions.","Our approaches apply to multi-modal clustering problems, where multiple modalities of observations come from the same cluster.","Empirically, our solvers outperform the state-of-the-art multi-modal clustering algorithms with significantly improved performance."],"url":"http://arxiv.org/abs/2402.14266v1","category":"cs.IT"}
{"created":"2024-02-22 04:02:55","title":"Innovation Diffusion in EV Charging Location Decisions: Integrating Demand & Supply through Market Dynamics","abstract":"This paper offers a strategic approach to Electric Vehicles (EVs) charging network planning, emphasizing the integration of demand and supply dynamics via continuous-time fluid queue models and discrete flow refueling location modeling, all in the context of innovation diffusion principles. We employ a continuous-time approximation based on Ordinary Differential Equations (ODEs) to design multi-year supply curves, a method that stands in contrast to conventional practices which often overlook inter-year transitions and ongoing processes. For medium-term charging station location planning (CSLP), we apply a flow refueling location model (FRLM) within grid-based multi-level networks, considering both multiple-path networks and capacity constraints. The grid-based network planning strategy uses a three-tier (Macro-Meso-Micro) approach for thorough EV charging station placement, with the macro-level covering entire cities, the meso-level assessing detailed EV routes and bridging the macro to micro levels, and the micro-level focusing on precise station placement for accessibility and efficiency. Our investigation into overutilization and underutilization scenarios delivers valuable insights for policymaking and cost-benefit analyses. Illustrating our approach with the example of the Chicago sketch network, we introduce an integrated demand-supply model suitable for a single region and extendable to multiple regions, thereby addressing a gap in the existing literature. Our proposed methodology focuses on EV station placement, taking into account future needs, geographical capacities, and the importance of scenario analysis, which empowers strategic resource planning for EV charging networks over extended timeframes, thus aiding the transition towards a more sustainable and efficient transportation system.","sentences":["This paper offers a strategic approach to Electric Vehicles (EVs) charging network planning, emphasizing the integration of demand and supply dynamics via continuous-time fluid queue models and discrete flow refueling location modeling, all in the context of innovation diffusion principles.","We employ a continuous-time approximation based on Ordinary Differential Equations (ODEs) to design multi-year supply curves, a method that stands in contrast to conventional practices which often overlook inter-year transitions and ongoing processes.","For medium-term charging station location planning (CSLP), we apply a flow refueling location model (FRLM) within grid-based multi-level networks, considering both multiple-path networks and capacity constraints.","The grid-based network planning strategy uses a three-tier (Macro-Meso-Micro) approach for thorough EV charging station placement, with the macro-level covering entire cities, the meso-level assessing detailed EV routes and bridging the macro to micro levels, and the micro-level focusing on precise station placement for accessibility and efficiency.","Our investigation into overutilization and underutilization scenarios delivers valuable insights for policymaking and cost-benefit analyses.","Illustrating our approach with the example of the Chicago sketch network, we introduce an integrated demand-supply model suitable for a single region and extendable to multiple regions, thereby addressing a gap in the existing literature.","Our proposed methodology focuses on EV station placement, taking into account future needs, geographical capacities, and the importance of scenario analysis, which empowers strategic resource planning for EV charging networks over extended timeframes, thus aiding the transition towards a more sustainable and efficient transportation system."],"url":"http://arxiv.org/abs/2402.14263v1","category":"math.OC"}
{"created":"2024-02-22 03:45:17","title":"The semi-discrete complex modified Korteweg-de Vries equation with zero and non-zero boundary conditions: Riemann-Hilbert approach and N-soliton solutions","abstract":"We focus on the semi-discrete complex modified Korteweg-de Vries (DcmKdV) equation in this paper. The direct and inverse scattering theory is developed with zero and non-zero boundary conditions (BCs) of the potential. For direct problem, the properties of the eigenfunctions and the scattering matrix, including analyticity, asymptotics and symmetries, are investigated, which facilitates the establishment of the Riemann-Hilbert (RH) problems. By solving the RH problems in the inverse problem part, the reconstruction potential formulas are obtained, which allows us to derive the N-soliton solutions in the reflectionless case. Meanwhile, the trace formulas are derived by means of studying the corresponding RH problems. Furthermore, the dynamic characteristics of the 1-soliton and 2-soliton with zero and non-zero boundary are demonstrated by graphical simulation.","sentences":["We focus on the semi-discrete complex modified Korteweg-de Vries (DcmKdV) equation in this paper.","The direct and inverse scattering theory is developed with zero and non-zero boundary conditions (BCs) of the potential.","For direct problem, the properties of the eigenfunctions and the scattering matrix, including analyticity, asymptotics and symmetries, are investigated, which facilitates the establishment of the Riemann-Hilbert (RH) problems.","By solving the RH problems in the inverse problem part, the reconstruction potential formulas are obtained, which allows us to derive the N-soliton solutions in the reflectionless case.","Meanwhile, the trace formulas are derived by means of studying the corresponding RH problems.","Furthermore, the dynamic characteristics of the 1-soliton and 2-soliton with zero and non-zero boundary are demonstrated by graphical simulation."],"url":"http://arxiv.org/abs/2402.14257v1","category":"nlin.SI"}
{"created":"2024-02-22 03:44:45","title":"Distributed Partial Quantum Consensus of Qubit Networks with Connected Topologies","abstract":"In this paper, we consider the partial quantum consensus problem of a qubit network in a distributed view. The local quantum operation is designed based on the Hamiltonian by using the local information of each quantum system in a network of qubits. We construct the unitary transformation for each quantum system to achieve the partial quantum consensus, i.e., the directions of the quantum states in the Bloch ball will reach an agreement. A simple case of two-qubit quantum systems is considered first, and a minimum completing time of reaching partial consensus is obtained based on the geometric configuration of each qubit. Furthermore, we extend the approaches to deal with the more general N-qubit networks. Two partial quantum consensus protocols, based on the Lyapunov method for chain graphs and the geometry method for connected graphs, are proposed. The geometry method can be utilized to deal with more general connected graphs, while for the Lyapunov method, the global consensus can be obtained. The numerical simulation over a qubit network is demonstrated to verify the validity and the effectiveness of the theoretical results.","sentences":["In this paper, we consider the partial quantum consensus problem of a qubit network in a distributed view.","The local quantum operation is designed based on the Hamiltonian by using the local information of each quantum system in a network of qubits.","We construct the unitary transformation for each quantum system to achieve the partial quantum consensus, i.e., the directions of the quantum states in the Bloch ball will reach an agreement.","A simple case of two-qubit quantum systems is considered first, and a minimum completing time of reaching partial consensus is obtained based on the geometric configuration of each qubit.","Furthermore, we extend the approaches to deal with the more general N-qubit networks.","Two partial quantum consensus protocols, based on the Lyapunov method for chain graphs and the geometry method for connected graphs, are proposed.","The geometry method can be utilized to deal with more general connected graphs, while for the Lyapunov method, the global consensus can be obtained.","The numerical simulation over a qubit network is demonstrated to verify the validity and the effectiveness of the theoretical results."],"url":"http://arxiv.org/abs/2402.14256v1","category":"quant-ph"}
{"created":"2024-02-22 03:32:00","title":"Make Interaction Situated: Designing User Acceptable Interaction for Situated Visualization in Public Environments","abstract":"Situated visualization blends data into the real world to fulfill individuals' contextual information needs. However, interacting with situated visualization in public environments faces challenges posed by user acceptance and contextual constraints. To explore appropriate interaction design, we first conduct a formative study to identify user needs for data and interaction. Informed by the findings, we summarize appropriate interaction modalities with eye-based, hand-based and spatially-aware object interaction for situated visualization in public environments. Then, through an iterative design process with six users, we explore and implement interactive techniques for activating and analyzing with situated visualization. To assess the effectiveness and acceptance of these interactions, we integrate them into an AR prototype and conduct a within-subjects study in public scenarios using conventional hand-only interactions as the baseline. The results show that participants preferred our prototype over the baseline, attributing their preference to the interactions being more acceptable, flexible, and practical in public.","sentences":["Situated visualization blends data into the real world to fulfill individuals' contextual information needs.","However, interacting with situated visualization in public environments faces challenges posed by user acceptance and contextual constraints.","To explore appropriate interaction design, we first conduct a formative study to identify user needs for data and interaction.","Informed by the findings, we summarize appropriate interaction modalities with eye-based, hand-based and spatially-aware object interaction for situated visualization in public environments.","Then, through an iterative design process with six users, we explore and implement interactive techniques for activating and analyzing with situated visualization.","To assess the effectiveness and acceptance of these interactions, we integrate them into an AR prototype and conduct a within-subjects study in public scenarios using conventional hand-only interactions as the baseline.","The results show that participants preferred our prototype over the baseline, attributing their preference to the interactions being more acceptable, flexible, and practical in public."],"url":"http://arxiv.org/abs/2402.14251v1","category":"cs.HC"}
{"created":"2024-02-22 03:30:05","title":"Anomalies in Light Scattering: A Circuit Model Approach","abstract":"In experimental physics, it is essential to understand electromagnetic (EM) wave scattering across EM spectrum, from radio waves to X-rays, and is pivotal in driving photonics innovations. Recent advancements have uncovered phenomena like bound states in the continuum (BICs) and parity-time (PT) symmetric systems, which are closely associated with the characteristics of the scattering matrix and are governed by passivity and causality. The emergence of complex frequency excitations has transcended the constraints imposed by passivity and causality in a system, revealing effects such as virtual critical coupling and virtual gain. However, applying the concepts of complex frequency excitation in more complicated systems remains challenging. In this work, we demonstrate the extension of the lumped element model of circuit theory to the analysis of anomalies in light scattering in the complex frequency domain. We demonstrate that the circuit model approach can facilitate design and analysis of effects such as virtual perfect absorption, BICs, real and virtual critical coupling, exceptional points, and anisotropic transmission resonances (ATRs). These findings broaden comprehension of EM wave phenomena and pave the way for significant advancements in photonics, offering new methods for designing and optimizing optical devices and systems with broad-ranging applications.","sentences":["In experimental physics, it is essential to understand electromagnetic (EM) wave scattering across EM spectrum, from radio waves to X-rays, and is pivotal in driving photonics innovations.","Recent advancements have uncovered phenomena like bound states in the continuum (BICs) and parity-time (PT) symmetric systems, which are closely associated with the characteristics of the scattering matrix and are governed by passivity and causality.","The emergence of complex frequency excitations has transcended the constraints imposed by passivity and causality in a system, revealing effects such as virtual critical coupling and virtual gain.","However, applying the concepts of complex frequency excitation in more complicated systems remains challenging.","In this work, we demonstrate the extension of the lumped element model of circuit theory to the analysis of anomalies in light scattering in the complex frequency domain.","We demonstrate that the circuit model approach can facilitate design and analysis of effects such as virtual perfect absorption, BICs, real and virtual critical coupling, exceptional points, and anisotropic transmission resonances (ATRs).","These findings broaden comprehension of EM wave phenomena and pave the way for significant advancements in photonics, offering new methods for designing and optimizing optical devices and systems with broad-ranging applications."],"url":"http://arxiv.org/abs/2402.14250v1","category":"physics.optics"}
{"created":"2024-02-22 03:15:13","title":"Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training","abstract":"Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task. Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability. Most existing reconstruction-based methods only use normal samples to construct model. If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved. However, usually only weakly labeled anomalous samples are available, which limits the improvement. In many cases, we can obtain some knowledge of anomalies summarized by domain experts. Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance. In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction model through self-training. Specifically, KIST utilizes weakly labeled anomalous samples in addition to the normal ones and exploits knowledge to yield pixel-level pseudo-labels of the anomalous samples. Based on the pseudo labels, a novel loss which promotes the reconstruction of normal pixels while suppressing the reconstruction of anomalous pixels is used. We conduct experiments on different datasets and demonstrate the advantages of KIST over the existing reconstruction-based methods.","sentences":["Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task.","Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability.","Most existing reconstruction-based methods only use normal samples to construct model.","If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved.","However, usually only weakly labeled anomalous samples are available, which limits the improvement.","In many cases, we can obtain some knowledge of anomalies summarized by domain experts.","Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance.","In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction model through self-training.","Specifically, KIST utilizes weakly labeled anomalous samples in addition to the normal ones and exploits knowledge to yield pixel-level pseudo-labels of the anomalous samples.","Based on the pseudo labels, a novel loss which promotes the reconstruction of normal pixels while suppressing the reconstruction of anomalous pixels is used.","We conduct experiments on different datasets and demonstrate the advantages of KIST over the existing reconstruction-based methods."],"url":"http://arxiv.org/abs/2402.14246v1","category":"cs.LG"}
{"created":"2024-02-22 03:04:34","title":"Irregular Bloch Zener oscillations in two-dimensional flat-band Dirac materials","abstract":"When a static electrical field is applied to a two-dimensional (2D) Dirac material, Landau-Zener transition (LZT) and Bloch-Zener oscillations can occur. Employing alpha-T3 lattices as a paradigm for a broad class of 2D Dirac materials, we uncover two phenomena. First, due to the arbitrarily small energy gaps near a Dirac point that make it more likely for LZTs to occur than in other regions of the Brillouin zone, the distribution of differential LZT probability in the momentum space can form a complicated morphological pattern. Second, a change in the LZT morphology as induced by a mutual switching of the two distinct Dirac points can lead to irregular Bloch-Zener oscillations characterized by a non-smooth behavior in the time evolution of the electrical current density associated with the oscillation. These phenomena are due to mixed interference of quantum states in multiple bands modulated by the geometric and dynamic phases. We demonstrate that the adiabatic-impulse model describing Landau-Zener-Stuckelberg interferometry can be exploited to calculate the phases, due to the equivalence between the alpha-T3 lattice subject to a constant electrical field and strongly periodically driven two- or three-level systems. The degree of irregularity of Bloch-Zener oscillations can be harnessed by selecting the morphology pattern, which is potentially experimentally realizable.","sentences":["When a static electrical field is applied to a two-dimensional (2D) Dirac material, Landau-Zener transition (LZT) and Bloch-Zener oscillations can occur.","Employing alpha-T3 lattices as a paradigm for a broad class of 2D Dirac materials, we uncover two phenomena.","First, due to the arbitrarily small energy gaps near a Dirac point that make it more likely for LZTs to occur than in other regions of the Brillouin zone, the distribution of differential LZT probability in the momentum space can form a complicated morphological pattern.","Second, a change in the LZT morphology as induced by a mutual switching of the two distinct Dirac points can lead to irregular Bloch-Zener oscillations characterized by a non-smooth behavior in the time evolution of the electrical current density associated with the oscillation.","These phenomena are due to mixed interference of quantum states in multiple bands modulated by the geometric and dynamic phases.","We demonstrate that the adiabatic-impulse model describing Landau-Zener-Stuckelberg interferometry can be exploited to calculate the phases, due to the equivalence between the alpha-T3 lattice subject to a constant electrical field and strongly periodically driven two-","or three-level systems.","The degree of irregularity of Bloch-Zener oscillations can be harnessed by selecting the morphology pattern, which is potentially experimentally realizable."],"url":"http://arxiv.org/abs/2402.14243v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-22 02:23:13","title":"On Gibbs Equilibrium and Hillert Nonequilibrium Thermodynamics","abstract":"During his time at Royal Institute of Technology (Kungliga Tekniska Hogskolan, KTH) in Sweden, the present author learned nonequilibrium thermodynamics from Mats Hillert. The key concept is the separation of internal and external variables of a system. In equilibrium thermodynamics derived by Gibbs, the internal variables are not independent and can be fully evaluated from given external variables. While irreversible thermodynamics led by Onsager focuses on internal variables though often mixed with external variables. Hillert integrated them together by first emphasizing their differences and then examining their connections. His philosophy was reflected by the title of his book \"Phase Equilibria, Phase Diagrams and Phase Transformations\" that puts equilibrium, nonequilibrium, and internal processes on equal footing. In the present paper honoring Hillert, the present author reflects his experiences with Hillert in last 35 years and expresses his gratitude for all the wisdom and support from him in terms of \"Hillert nonequilibrium thermodynamics\" and discusses some recent topics that the present author has been working on.","sentences":["During his time at Royal Institute of Technology (Kungliga Tekniska Hogskolan, KTH) in Sweden, the present author learned nonequilibrium thermodynamics from Mats Hillert.","The key concept is the separation of internal and external variables of a system.","In equilibrium thermodynamics derived by Gibbs, the internal variables are not independent and can be fully evaluated from given external variables.","While irreversible thermodynamics led by Onsager focuses on internal variables though often mixed with external variables.","Hillert integrated them together by first emphasizing their differences and then examining their connections.","His philosophy was reflected by the title of his book \"Phase Equilibria, Phase Diagrams and Phase Transformations\" that puts equilibrium, nonequilibrium, and internal processes on equal footing.","In the present paper honoring Hillert, the present author reflects his experiences with Hillert in last 35 years and expresses his gratitude for all the wisdom and support from him in terms of \"Hillert nonequilibrium thermodynamics\" and discusses some recent topics that the present author has been working on."],"url":"http://arxiv.org/abs/2402.14231v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-22 02:20:24","title":"Sample-Efficient Linear Regression with Self-Selection Bias","abstract":"We consider the problem of linear regression with self-selection bias in the unknown-index setting, as introduced in recent work by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$ i.i.d. samples $(\\mathbf{x}_{\\ell},z_{\\ell})_{\\ell=1}^m$ where $z_{\\ell}=\\max_{i\\in [k]}\\{\\mathbf{x}_{\\ell}^T\\mathbf{w}_i+\\eta_{i,\\ell}\\}$, but the maximizing index $i_{\\ell}$ is unobserved. Here, the $\\mathbf{x}_{\\ell}$ are assumed to be $\\mathcal{N}(0,I_n)$ and the noise distribution $\\mathbf{\\eta}_{\\ell}\\sim \\mathcal{D}$ is centered and independent of $\\mathbf{x}_{\\ell}$. We provide a novel and near optimally sample-efficient (in terms of $k$) algorithm to recover $\\mathbf{w}_1,\\ldots,\\mathbf{w}_k\\in \\mathbb{R}^n$ up to additive $\\ell_2$-error $\\varepsilon$ with polynomial sample complexity $\\tilde{O}(n)\\cdot \\mathsf{poly}(k,1/\\varepsilon)$ and significantly improved time complexity $\\mathsf{poly}(n,k,1/\\varepsilon)+O(\\log(k)/\\varepsilon)^{O(k)}$. When $k=O(1)$, our algorithm runs in $\\mathsf{poly}(n,1/\\varepsilon)$ time, generalizing the polynomial guarantee of an explicit moment matching algorithm of Cherapanamjeri, et al. for $k=2$ and when it is known that $\\mathcal{D}=\\mathcal{N}(0,I_k)$. Our algorithm succeeds under significantly relaxed noise assumptions, and therefore also succeeds in the related setting of max-linear regression where the added noise is taken outside the maximum. For this problem, our algorithm is efficient in a much larger range of $k$ than the state-of-the-art due to Ghosh, Pananjady, Guntuboyina, and Ramchandran [IEEE Trans. Inf. Theory 2022] for not too small $\\varepsilon$, and leads to improved algorithms for any $\\varepsilon$ by providing a warm start for existing local convergence methods.","sentences":["We consider the problem of linear regression with self-selection bias in the unknown-index setting, as introduced in recent work by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis","[STOC 2023].","In this model, one observes $m$ i.i.d. samples $(\\mathbf{x}_{\\ell},z_{\\ell})_{\\ell=1}^m$ where $z_{\\ell}=\\max_{i\\in","[k]}\\{\\mathbf{x}_{\\ell}^T\\mathbf{w}_i+\\eta_{i,\\ell}\\}$, but the maximizing index $i_{\\ell}$ is unobserved.","Here, the $\\mathbf{x}_{\\ell}$ are assumed to be $\\mathcal{N}(0,I_n)$ and the noise distribution $\\mathbf{\\eta}_{\\ell}\\sim \\mathcal{D}$ is centered and independent of $\\mathbf{x}_{\\ell}$. We provide a novel and near optimally sample-efficient (in terms of $k$) algorithm to recover $\\mathbf{w}_1,\\ldots,\\mathbf{w}_k\\in \\mathbb{R}^n$ up to additive $\\ell_2$-error $\\varepsilon$ with polynomial sample complexity $\\tilde{O}(n)\\cdot \\mathsf{poly}(k,1/\\varepsilon)$ and significantly improved time complexity $\\mathsf{poly}(n,k,1/\\varepsilon)+O(\\log(k)/\\varepsilon)^{O(k)}$.","When $k=O(1)$, our algorithm runs in $\\mathsf{poly}(n,1/\\varepsilon)$ time, generalizing the polynomial guarantee of an explicit moment matching algorithm of Cherapanamjeri, et al. for $k=2$ and when it is known that $\\mathcal{D}=\\mathcal{N}(0,I_k)$. Our algorithm succeeds under significantly relaxed noise assumptions, and therefore also succeeds in the related setting of max-linear regression where the added noise is taken outside the maximum.","For this problem, our algorithm is efficient in a much larger range of $k$ than the state-of-the-art due to Ghosh, Pananjady, Guntuboyina, and Ramchandran [IEEE Trans.","Inf.","Theory 2022] for not too small $\\varepsilon$, and leads to improved algorithms for any $\\varepsilon$ by providing a warm start for existing local convergence methods."],"url":"http://arxiv.org/abs/2402.14229v1","category":"math.ST"}
{"created":"2024-02-22 01:54:54","title":"Towards singular optimality in the presence of local initial knowledge","abstract":"The Knowledge Till rho CONGEST model is a variant of the classical CONGEST model of distributed computing in which each vertex v has initial knowledge of the radius-rho ball centered at v. The most commonly studied variants of the CONGEST model are KT0 CONGEST in which nodes initially know nothing about their neighbors and KT1 CONGEST in which nodes initially know the IDs of all their neighbors. It has been shown that having access to neighbors' IDs (as in the KT1 CONGEST model) can substantially reduce the message complexity of algorithms for fundamental problems such as BROADCAST and MST. For example, King, Kutten, and Thorup (PODC 2015) show how to construct an MST using just Otilde(n) messages in the KT1 CONGEST model, whereas there is an Omega(m) message lower bound for MST in the KT0 CONGEST model. Building on this result, Gmyr and Pandurangen (DISC 2018) present a family of distributed randomized algorithms for various global problems that exhibit a trade-off between message and round complexity. These algorithms are based on constructing a sparse, spanning subgraph called a danner. Specifically, given a graph G and any delta in [0,1], their algorithm constructs (with high probability) a danner that has diameter Otilde(D + n^{1-delta}) and Otilde(min{m,n^{1+delta}}) edges in Otilde(n^{1-delta}) rounds while using Otilde(min{m,n^{1+\\delta}}) messages, where n, m, and D are the number of nodes, edges, and the diameter of G, respectively. In the main result of this paper, we show that if we assume the KT2 CONGEST model, it is possible to substantially improve the time-message trade-off in constructing a danner. Specifically, we show in the KT2 CONGEST model, how to construct a danner that has diameter Otilde(D + n^{1-2delta}) and Otilde(min{m,n^{1+delta}}) edges in Otilde(n^{1-2delta}) rounds while using Otilde(min{m,n^{1+\\delta}}) messages for any delta in [0,1/2].","sentences":["The Knowledge Till rho CONGEST model is a variant of the classical CONGEST model of distributed computing in which each vertex v has initial knowledge of the radius-rho ball centered at v. The most commonly studied variants of the CONGEST model are KT0 CONGEST in which nodes initially know nothing about their neighbors and KT1 CONGEST in which nodes initially know the IDs of all their neighbors.","It has been shown that having access to neighbors' IDs (as in the KT1 CONGEST model) can substantially reduce the message complexity of algorithms for fundamental problems such as BROADCAST and MST.","For example, King, Kutten, and Thorup (PODC 2015) show how to construct an MST using just Otilde(n) messages in the KT1 CONGEST model, whereas there is an Omega(m) message lower bound for MST in the KT0 CONGEST model.","Building on this result, Gmyr and Pandurangen (DISC 2018) present a family of distributed randomized algorithms for various global problems that exhibit a trade-off between message and round complexity.","These algorithms are based on constructing a sparse, spanning subgraph called a danner.","Specifically, given a graph G and any delta in [0,1], their algorithm constructs (with high probability) a danner that has diameter Otilde(D","+ n^{1-delta}) and Otilde(min{m,n^{1+delta}}) edges in Otilde(n^{1-delta}) rounds while using Otilde(min{m,n^{1+\\delta}}) messages, where n, m, and D are the number of nodes, edges, and the diameter of G, respectively.","In the main result of this paper, we show that if we assume the KT2 CONGEST model, it is possible to substantially improve the time-message trade-off in constructing a danner.","Specifically, we show in the KT2 CONGEST model, how to construct a danner that has diameter Otilde(D","+ n^{1-2delta}) and Otilde(min{m,n^{1+delta}}) edges in Otilde(n^{1-2delta}) rounds while using Otilde(min{m,n^{1+\\delta}}) messages for any delta in [0,1/2]."],"url":"http://arxiv.org/abs/2402.14221v1","category":"cs.DC"}
{"created":"2024-02-22 01:53:56","title":"Estimating Unknown Population Sizes Using the Hypergeometric Distribution","abstract":"The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an informative latent space. We demonstrate our method's versatility through applications in NLP, by inferring and estimating the complexity of latent vocabularies in text excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data.","sentences":["The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories.","Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown.","Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling.","We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework.","Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an informative latent space.","We demonstrate our method's versatility through applications in NLP, by inferring and estimating the complexity of latent vocabularies in text excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data."],"url":"http://arxiv.org/abs/2402.14220v1","category":"cs.LG"}
{"created":"2024-02-22 01:53:38","title":"Large Array Antenna Spectrum Sensing in Cognitive Radio Networks","abstract":"We investigate the problem of spectrum sensing in cognitive radios (CRs) when the receivers are equipped with a large array of antennas. We propose and derive three detectors based on the concept of linear spectral statistics (LSS) in the field of random matrix theory (RMT). These detectors correspond to the generalized likelihood ratio (GLR), Frobenius norm, and Rao tests employed in conventional multiple antenna spectrum sensing (MASS). Subsequently, we compute the Gaussian distribution of the proposed detectors under the noise-only hypothesis, leveraging the central limit theorem (CLT) applied to high-dimensional random matrices. We evaluate the performance of the proposed detectors and analyze the impact of the number of antennas and samples on their efficacy. Furthermore, we assess the accuracy of the theoretical results by comparing them with simulation outcomes. The simulation results provide evidence that the proposed detectors exhibit efficient performance in wireless networks featuring large array antennas. These detectors find practical applications in diverse domains, including massive MIMO wireless communications, radar systems, and astronomical applications.","sentences":["We investigate the problem of spectrum sensing in cognitive radios (CRs) when the receivers are equipped with a large array of antennas.","We propose and derive three detectors based on the concept of linear spectral statistics (LSS) in the field of random matrix theory (RMT).","These detectors correspond to the generalized likelihood ratio (GLR), Frobenius norm, and Rao tests employed in conventional multiple antenna spectrum sensing (MASS).","Subsequently, we compute the Gaussian distribution of the proposed detectors under the noise-only hypothesis, leveraging the central limit theorem (CLT) applied to high-dimensional random matrices.","We evaluate the performance of the proposed detectors and analyze the impact of the number of antennas and samples on their efficacy.","Furthermore, we assess the accuracy of the theoretical results by comparing them with simulation outcomes.","The simulation results provide evidence that the proposed detectors exhibit efficient performance in wireless networks featuring large array antennas.","These detectors find practical applications in diverse domains, including massive MIMO wireless communications, radar systems, and astronomical applications."],"url":"http://arxiv.org/abs/2402.14219v1","category":"eess.SP"}
{"created":"2024-02-22 18:58:05","title":"Difference Learning for Air Quality Forecasting Transport Emulation","abstract":"Human health is negatively impacted by poor air quality including increased risk for respiratory and cardiovascular disease. Due to a recent increase in extreme air quality events, both globally and locally in the United States, finer resolution air quality forecasting guidance is needed to effectively adapt to these events. The National Oceanic and Atmospheric Administration provides air quality forecasting guidance for the Continental United States. Their air quality forecasting model is based on a 15 km spatial resolution; however, the goal is to reach a three km spatial resolution. This is currently not feasible due in part to prohibitive computational requirements for modeling the transport of chemical species. In this work, we describe a deep learning transport emulator that is able to reduce computations while maintaining skill comparable with the existing numerical model. We show how this method maintains skill in the presence of extreme air quality events, making it a potential candidate for operational use. We also explore evaluating how well this model maintains the physical properties of the modeled transport for a given set of species.","sentences":["Human health is negatively impacted by poor air quality including increased risk for respiratory and cardiovascular disease.","Due to a recent increase in extreme air quality events, both globally and locally in the United States, finer resolution air quality forecasting guidance is needed to effectively adapt to these events.","The National Oceanic and Atmospheric Administration provides air quality forecasting guidance for the Continental United States.","Their air quality forecasting model is based on a 15 km spatial resolution; however, the goal is to reach a three km spatial resolution.","This is currently not feasible due in part to prohibitive computational requirements for modeling the transport of chemical species.","In this work, we describe a deep learning transport emulator that is able to reduce computations while maintaining skill comparable with the existing numerical model.","We show how this method maintains skill in the presence of extreme air quality events, making it a potential candidate for operational use.","We also explore evaluating how well this model maintains the physical properties of the modeled transport for a given set of species."],"url":"http://arxiv.org/abs/2402.14806v1","category":"cs.LG"}
{"created":"2024-02-22 18:49:41","title":"Measure structured deformations","abstract":"Measure structured deformations are introduced to present a unified theory of deformations of continua. The energy associated with a measure structured deformation is defined via relaxation departing either from energies associated with classical deformations or from energies associated with structured deformations. A concise integral representation of the energy functional is provided both in the unconstrained case and under Dirichlet conditions on a part of the boundary.","sentences":["Measure structured deformations are introduced to present a unified theory of deformations of continua.","The energy associated with a measure structured deformation is defined via relaxation departing either from energies associated with classical deformations or from energies associated with structured deformations.","A concise integral representation of the energy functional is provided both in the unconstrained case and under Dirichlet conditions on a part of the boundary."],"url":"http://arxiv.org/abs/2402.14790v1","category":"math.AP"}
{"created":"2024-02-22 18:33:31","title":"Localised Natural Causal Learning Algorithms for Weak Consistency Conditions","abstract":"By relaxing conditions for natural structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (LoNS). We also provide a set of necessary and sufficient assumptions for consistency of LoNS, which can be thought of as a strict relaxation of the restricted faithfulness assumption. We provide a practical LoNS algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely PC/SGS and the relatively recent Sparsest Permutation algorithm. Simulation studies are also provided.","sentences":["By relaxing conditions for natural structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (LoNS).","We also provide a set of necessary and sufficient assumptions for consistency of LoNS, which can be thought of as a strict relaxation of the restricted faithfulness assumption.","We provide a practical LoNS algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely PC/SGS and the relatively recent Sparsest Permutation algorithm.","Simulation studies are also provided."],"url":"http://arxiv.org/abs/2402.14775v1","category":"stat.ME"}
{"created":"2024-02-22 18:24:20","title":"Proposed real-time charge noise measurement via valley state reflectometry","abstract":"We theoretically propose a method to perform \\textit{in situ} measurements of charge noise during logical operations in silicon quantum dot spin qubits. Our method does not require ancillary spectator qubits but makes use of the valley degree of freedom in silicon. Sharp interface steps or alloy disorder in the well provide a valley transition dipole element that couples to the field of an on-chip microwave resonator, allowing rapid reflectometry of valley splitting fluctuations caused by charge noise. We derive analytic expressions for the signal-to-noise ratio that can be expected and use tight binding simulations to extract the key parameters (valley splitting and valley dipole elements) under realistic disorder. We find that unity signal-to-noise ratio can often be obtained with measurement times below 1ms, faster than typical decoherence times, opening the potential for closed-loop control, real-time recalibration, and feedforward circuits","sentences":["We theoretically propose a method to perform \\textit{in situ} measurements of charge noise during logical operations in silicon quantum dot spin qubits.","Our method does not require ancillary spectator qubits but makes use of the valley degree of freedom in silicon.","Sharp interface steps or alloy disorder in the well provide a valley transition dipole element that couples to the field of an on-chip microwave resonator, allowing rapid reflectometry of valley splitting fluctuations caused by charge noise.","We derive analytic expressions for the signal-to-noise ratio that can be expected and use tight binding simulations to extract the key parameters (valley splitting and valley dipole elements) under realistic disorder.","We find that unity signal-to-noise ratio can often be obtained with measurement times below 1ms, faster than typical decoherence times, opening the potential for closed-loop control, real-time recalibration, and","feedforward circuits"],"url":"http://arxiv.org/abs/2402.14765v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-22 18:23:13","title":"A Combinatorial Central Limit Theorem for Stratified Randomization","abstract":"This paper establishes a combinatorial central limit theorem for stratified randomization that holds under Lindeberg-type conditions and allows for a growing number of large and small strata. The result is then applied to derive the asymptotic distributions of two test statistics proposed in a finite population setting with randomly assigned instruments and a super population instrumental variables model, both having many strata.","sentences":["This paper establishes a combinatorial central limit theorem for stratified randomization that holds under Lindeberg-type conditions and allows for a growing number of large and small strata.","The result is then applied to derive the asymptotic distributions of two test statistics proposed in a finite population setting with randomly assigned instruments and a super population instrumental variables model, both having many strata."],"url":"http://arxiv.org/abs/2402.14764v1","category":"math.ST"}
{"created":"2024-02-22 17:56:05","title":"New scattered quadrinomials","abstract":"Let $1<t<n$ be integers, where $t$ is a divisor of $n$. An R-$q^t$-partially scattered polynomial is a $\\mathbb F_q$-linearized polynomial $f$ in $\\mathbb F_{q^n}[X]$ that satisfies the condition that for all $x,y\\in\\mathbb F_{q^n}^*$ such that $x/y\\in\\mathbb F_{q^t}$, if $f(x)/x=f(y)/y$, then $x/y\\in\\mathbb F_q$; $f$ is called scattered if this implication holds for all $x,y\\in\\mathbb F_{q^n}^*$. Two polynomials in $\\mathbb F_{q^n}[X]$ are said to be equivalent if their graphs are in the same orbit under the action of the group $\\Gamma L(2,q^n)$. For $n>8$ only three families of scattered polynomials in $\\mathbb F_{q^n}[X]$ are known: $(i)$~monomials of pseudoregulus type, $(ii)$~binomials of Lunardon-Polverino type, and $(iii)$~a family of quadrinomials defined in [9] and extended in [7,12]. In this paper we prove that the polynomial $\\varphi_{m,q^J}=X^{q^{J(t-1)}}+X^{q^{J(2t-1)}}+m(X^{q^J}-X^{q^{J(t+1)}})\\in\\mathbb F_{q^{2t}}[X]$, $q$ odd, $t\\ge3$ is R-$q^t$-partially scattered for every value of $m\\in\\mathbb F_{q^t}^*$ and $J$ coprime with $2t$. Moreover, for every $t>4$ and $q>5$ there exist values of $m$ for which $\\varphi_{m,q}$ is scattered and new with respect to the polynomials mentioned in $(i)$, $(ii)$ and $(iii)$ above. The related linear sets are of $\\Gamma L$-class at least two.","sentences":["Let $1<t<n$ be integers, where $t$ is a divisor of $n$. An R-$q^t$-partially scattered polynomial is a $\\mathbb F_q$-linearized polynomial $f$ in $\\mathbb F_{q^n}[X]$ that satisfies the condition that for all $x,y\\in\\mathbb F_{q^n}^*$ such that $x/y\\in\\mathbb F_{q^t}$, if $f(x)/x=f(y)/y$, then $x/y\\in\\mathbb F_q$; $f$ is called scattered if this implication holds for all $x,y\\in\\mathbb F_{q^n}^*$. Two polynomials in $\\mathbb F_{q^n}[X]$ are said to be equivalent if their graphs are in the same orbit under the action of the group $\\Gamma L(2,q^n)$.","For $n>8$ only three families of scattered polynomials in $\\mathbb F_{q^n}[X]$ are known: $(i)$~monomials of pseudoregulus type, $(ii)$~binomials of Lunardon-Polverino type, and $(iii)$~a family of quadrinomials defined in [9] and extended in [7,12].","In this paper we prove that the polynomial $\\varphi_{m,q^J}=X^{q^{J(t-1)}}+X^{q^{J(2t-1)}}+m(X^{q^J}-X^{q^{J(t+1)}})\\in\\mathbb F_{q^{2t}}[X]$, $q$ odd, $t\\ge3$ is R-$q^t$-partially scattered for every value of $m\\in\\mathbb F_{q^t}^*$ and $J$ coprime with $2t$. Moreover, for every $t>4$ and $q>5$ there exist values of $m$ for which $\\varphi_{m,q}$ is scattered and new with respect to the polynomials mentioned in $(i)$, $(ii)$ and $(iii)$ above.","The related linear sets are of $\\Gamma L$-class at least two."],"url":"http://arxiv.org/abs/2402.14742v1","category":"math.CO"}
{"created":"2024-02-22 17:35:40","title":"On the difficulty of determining Karman 'constants' from DNS","abstract":"The difficulty of determining the slope of the famed logarithmic law in the mean velocity profile in wall-bounded turbulent flows, the inverse of the Karman 'constant' $\\kappa$, from direct numerical simulations (DNS) is discussed for channel flow. Unusual approaches, as well as the analysis of the standard log-indicator function are considered and analyzed, leading to the conclusion that a definitive determination of the channel flow $\\kappa$ from DNS with an uncertainty of, say, 2-3% will require the residue of the mean stream-wise momentum equation in the simulations to be reduced by at least an order of magnitude.","sentences":["The difficulty of determining the slope of the famed logarithmic law in the mean velocity profile in wall-bounded turbulent flows, the inverse of the Karman 'constant' $\\kappa$, from direct numerical simulations (DNS) is discussed for channel flow.","Unusual approaches, as well as the analysis of the standard log-indicator function are considered and analyzed, leading to the conclusion that a definitive determination of the channel flow $\\kappa$ from DNS with an uncertainty of, say, 2-3% will require the residue of the mean stream-wise momentum equation in the simulations to be reduced by at least an order of magnitude."],"url":"http://arxiv.org/abs/2402.14729v1","category":"physics.flu-dyn"}
{"created":"2024-02-22 17:34:47","title":"Solitons of the mean curvature flow in $\\mathbb{s}^2\\times\\mathbb{R}$","abstract":"A soliton of the mean curvature flow in the product space $\\mathbb{s}^2\\times\\mathbb{R}$ as a surface whose mean curvature $H$ satisfies the equation $H=\\langle N,X\\rangle$, where $N$ is the unit normal of the surface and $X$ is a Killing vector field. In this paper we consider the vector field tangent to the fibers and the vector field associated to a rotations about an axis of $\\mathbb{s}^2$, respectively. We give a classification of the solitons with respect to these vector fields assuming that the surface is invariant under a one-parameter group of vertical translations or under a group of rotations of $\\mathbb{s}^2$.","sentences":["A soliton of the mean curvature flow in the product space $\\mathbb{s}^2\\times\\mathbb{R}$ as a surface whose mean curvature $H$ satisfies the equation $H=\\langle N,X\\rangle$, where $N$ is the unit normal of the surface and $X$ is a Killing vector field.","In this paper we consider the vector field tangent to the fibers and the vector field associated to a rotations about an axis of $\\mathbb{s}^2$, respectively.","We give a classification of the solitons with respect to these vector fields assuming that the surface is invariant under a one-parameter group of vertical translations or under a group of rotations of $\\mathbb{s}^2$."],"url":"http://arxiv.org/abs/2402.14727v1","category":"math.DG"}
{"created":"2024-02-22 16:41:34","title":"Room-temperature ladder-type optical memory compatible with single photons from InGaAs quantum dots","abstract":"On-demand storage and retrieval of quantum information in coherent light-matter interfaces is a key requirement for future quantum networking and quantum communication applications. Alkali vapor memories offer scalable and robust high-bandwidth storage at high repetition rates which makes them a natural fit to interface with solid-state single-photon sources. Here, we experimentally realize a room-temperature ladder-type atomic vapor memory that operates on the Cs D1 line. We provide a detailed experimental characterization and demonstration of on-demand storage and retrieval of weak coherent laser pulses with 0.06 photons per pulse at a high signal-to-noise ratio of SNR$=830(80)$. The memory achieves a maximum internal storage efficiency of $\\eta_{\\text{int}}=15(1)\\%$ and an estimated $1/e$-storage time of $\\tau_{\\mathrm{s}}\\approx32\\,$ns. Benchmark properties for the storage of single photons from inhomogeneously broadened state-of-the-art solid-state emitters are estimated from the performance of the memory. Together with the immediate availability of high-quality InGaAs quantum dots emitting at 895\\,nm, these results provide clear prospects for the development of a heterogeneous on-demand quantum light interface.","sentences":["On-demand storage and retrieval of quantum information in coherent light-matter interfaces is a key requirement for future quantum networking and quantum communication applications.","Alkali vapor memories offer scalable and robust high-bandwidth storage at high repetition rates which makes them a natural fit to interface with solid-state single-photon sources.","Here, we experimentally realize a room-temperature ladder-type atomic vapor memory that operates on the Cs D1 line.","We provide a detailed experimental characterization and demonstration of on-demand storage and retrieval of weak coherent laser pulses with 0.06 photons per pulse at a high signal-to-noise ratio of SNR$=830(80)$. The memory achieves a maximum internal storage efficiency of $\\eta_{\\text{int}}=15(1)\\%$ and an estimated $1/e$-storage time of $\\tau_{\\mathrm{s}}\\approx32\\,$ns.","Benchmark properties for the storage of single photons from inhomogeneously broadened state-of-the-art solid-state emitters are estimated from the performance of the memory.","Together with the immediate availability of high-quality InGaAs quantum dots emitting at 895\\,nm, these results provide clear prospects for the development of a heterogeneous on-demand quantum light interface."],"url":"http://arxiv.org/abs/2402.14686v1","category":"quant-ph"}
{"created":"2024-02-22 16:05:33","title":"Evaluating Cognitive and Neuropsychological Assessments -- A Comprehensive Review","abstract":"Cognitive impairments in older adults represent a significant public health concern, necessitating accurate diagnostic and monitoring strategies. In this study, the principal cognitive and neuropsychological evaluations employed for the diagnosis and longitudinal observation of cognitive deficits in the elderly are investigated. An analytical review of instruments including the Mini-Mental State Examination (MMSE), Digit Symbol Substitution Test (DSST), Montreal Cognitive Assessment (MoCA), and Trail Making Test (TMT) is conducted. This examination encompasses an assessment of each instrument's methodology, efficacy, advantages, and limitations. The objective is to enhance comprehension of these assessments for the early identification and effective management of conditions such as dementia and mild cognitive impairment, thereby contributing to the advancement of cognitive health within the geriatric population.","sentences":["Cognitive impairments in older adults represent a significant public health concern, necessitating accurate diagnostic and monitoring strategies.","In this study, the principal cognitive and neuropsychological evaluations employed for the diagnosis and longitudinal observation of cognitive deficits in the elderly are investigated.","An analytical review of instruments including the Mini-Mental State Examination (MMSE), Digit Symbol Substitution Test (DSST), Montreal Cognitive Assessment (MoCA), and Trail Making Test (TMT) is conducted.","This examination encompasses an assessment of each instrument's methodology, efficacy, advantages, and limitations.","The objective is to enhance comprehension of these assessments for the early identification and effective management of conditions such as dementia and mild cognitive impairment, thereby contributing to the advancement of cognitive health within the geriatric population."],"url":"http://arxiv.org/abs/2402.14655v1","category":"q-bio.NC"}
{"created":"2024-02-22 15:34:44","title":"Upper Limits of $^{44}$Ti Decay Emission in Four Nearby Thermonuclear Supernova Remnants","abstract":"To identify progenitors and investigate evidence of He burning, we searched for decay radiation of freshly synthesized $^{44}$Ti in four young nearby thermonuclear supernova remnants: Kepler, SN 1885, G1.9+0.3 and SN 1006, by analysing the up-to-date NuSTAR archival data. No apparent flux excess from the 68 and 78 keV line emissions accompanying decay was detected above the power law continuum applied for the remnants and the absorbed stray light. By comparing the inferred upper limits of the line flux and the initial $^{44}$Ti masses with a wide variety of supernova nucleosynthesis models, we placed constraints on the supernova progenitors. We derived the first NuSTAR line flux upper limit for Kepler and ruled out most of the double-detonation scenarios with a thick He layer under low density. We estimated, for the first time, the upper limit for SN 1885, which is high because of the large distance yet still remains consistent with the He shell detonation. The new flux and mass limit of G1.9+0.3 derived from a longer total exposure is lower than the results from previous studies and evidently excludes explosive burning of He-rich matter. The relatively advanced age and the large spatial extent of SN 1006 have prevented meaningful constraints.","sentences":["To identify progenitors and investigate evidence of He burning, we searched for decay radiation of freshly synthesized $^{44}$Ti in four young nearby thermonuclear supernova remnants: Kepler, SN 1885, G1.9+0.3 and SN 1006, by analysing the up-to-date NuSTAR archival data.","No apparent flux excess from the 68 and 78 keV line emissions accompanying decay was detected above the power law continuum applied for the remnants and the absorbed stray light.","By comparing the inferred upper limits of the line flux and the initial $^{44}$Ti masses with a wide variety of supernova nucleosynthesis models, we placed constraints on the supernova progenitors.","We derived the first NuSTAR line flux upper limit for Kepler and ruled out most of the double-detonation scenarios with a thick He layer under low density.","We estimated, for the first time, the upper limit for SN 1885, which is high because of the large distance yet still remains consistent with the He shell detonation.","The new flux and mass limit of G1.9+0.3 derived from a longer total exposure is lower than the results from previous studies and evidently excludes explosive burning of He-rich matter.","The relatively advanced age and the large spatial extent of SN 1006 have prevented meaningful constraints."],"url":"http://arxiv.org/abs/2402.14637v1","category":"astro-ph.HE"}
{"created":"2024-02-22 15:14:19","title":"Self-arresting earthquakes and critical sliding nucleation theory","abstract":"We develop a statistical thermodynamic approach for understanding earthquake nucleation on homogeneous faults, explaining the occurrence of both self-arresting and run-away unstable ruptures (subshear and supershear events) previously observed in numerical simulations. Our theory identifies the conditions under which self-arresting earthquakes occur, based on critical sliding distance and dynamical stress drop. We also derive the Gutenberg-Richter distribution for these earthquakes, linking the fractal nature of faulting with the nucleation physics through the critical size's dependence on dynamical stress drop. Furthermore, we connect our findings to the dragon-king theory, which suggests that the largest earthquakes differ significantly in physical and statistical properties from smaller ones, offering new insights for earthquake prediction and risk assessment.","sentences":["We develop a statistical thermodynamic approach for understanding earthquake nucleation on homogeneous faults, explaining the occurrence of both self-arresting and run-away unstable ruptures (subshear and supershear events) previously observed in numerical simulations.","Our theory identifies the conditions under which self-arresting earthquakes occur, based on critical sliding distance and dynamical stress drop.","We also derive the Gutenberg-Richter distribution for these earthquakes, linking the fractal nature of faulting with the nucleation physics through the critical size's dependence on dynamical stress drop.","Furthermore, we connect our findings to the dragon-king theory, which suggests that the largest earthquakes differ significantly in physical and statistical properties from smaller ones, offering new insights for earthquake prediction and risk assessment."],"url":"http://arxiv.org/abs/2402.14626v1","category":"physics.geo-ph"}
{"created":"2024-02-22 15:04:24","title":"The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations","abstract":"When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.","sentences":["When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords.","What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words?","We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words.","Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words.","Their similarity values, however, must be interpreted with caution."],"url":"http://arxiv.org/abs/2402.14616v1","category":"cs.CL"}
{"created":"2024-02-22 13:30:24","title":"Interference Produces False-Positive Pricing Experiments","abstract":"It is standard practice in online retail to run pricing experiments by randomizing at the article-level, i.e. by changing prices of different products to identify treatment effects. Due to customers' cross-price substitution behavior, such experiments suffer from interference bias: the observed difference between treatment groups in the experiment is typically significantly larger than the global effect that could be expected after a roll-out decision of the tested pricing policy. We show in simulations that such bias can be as large as 100%, and report experimental data implying bias of similar magnitude. Finally, we discuss approaches for de-biased pricing experiments, suggesting observational methods as a potentially attractive alternative to clustering.","sentences":["It is standard practice in online retail to run pricing experiments by randomizing at the article-level, i.e. by changing prices of different products to identify treatment effects.","Due to customers' cross-price substitution behavior, such experiments suffer from interference bias: the observed difference between treatment groups in the experiment is typically significantly larger than the global effect that could be expected after a roll-out decision of the tested pricing policy.","We show in simulations that such bias can be as large as 100%, and report experimental data implying bias of similar magnitude.","Finally, we discuss approaches for de-biased pricing experiments, suggesting observational methods as a potentially attractive alternative to clustering."],"url":"http://arxiv.org/abs/2402.14538v1","category":"stat.AP"}
{"created":"2024-02-22 13:24:43","title":"A Framework for Variational Inference of Lightweight Bayesian Neural Networks with Heteroscedastic Uncertainties","abstract":"Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural Network (BNN) is vital to many applications. Often, heteroscedastic aleatoric uncertainties are learned as outputs of the BNN in addition to the predictive means, however doing so may necessitate adding more learnable parameters to the network. In this work, we demonstrate that both the heteroscedastic aleatoric and epistemic variance can be embedded into the variances of learned BNN parameters, improving predictive performance for lightweight networks. By complementing this approach with a moment propagation approach to inference, we introduce a relatively simple framework for sampling-free variational inference suitable for lightweight BNNs.","sentences":["Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural Network (BNN) is vital to many applications.","Often, heteroscedastic aleatoric uncertainties are learned as outputs of the BNN in addition to the predictive means, however doing so may necessitate adding more learnable parameters to the network.","In this work, we demonstrate that both the heteroscedastic aleatoric and epistemic variance can be embedded into the variances of learned BNN parameters, improving predictive performance for lightweight networks.","By complementing this approach with a moment propagation approach to inference, we introduce a relatively simple framework for sampling-free variational inference suitable for lightweight BNNs."],"url":"http://arxiv.org/abs/2402.14532v1","category":"cs.LG"}
{"created":"2024-02-22 13:23:49","title":"Groups having minimal covering number 2 of diagonal type","abstract":"Garonzi and Lucchini~\\cite{GL} explored finite groups $G$ possessing a normal $2$-covering, where no proper quotient of $G$ exhibits such a covering. Their investigation offered a comprehensive overview of these groups, delineating that such groups fall into distinct categories: almost simple, affine, product action, or diagonal.   In this paper, we focus on the family falling under the diagonal type. Specifically, we present a thorough classification of finite diagonal groups possessing a normal $2$-covering, with the attribute that no proper quotient of $G$ has such a covering.","sentences":["Garonzi and Lucchini~\\cite{GL} explored finite groups $G$ possessing a normal $2$-covering, where no proper quotient of $G$ exhibits such a covering.","Their investigation offered a comprehensive overview of these groups, delineating that such groups fall into distinct categories: almost simple, affine, product action, or diagonal.   ","In this paper, we focus on the family falling under the diagonal type.","Specifically, we present a thorough classification of finite diagonal groups possessing a normal $2$-covering, with the attribute that no proper quotient of $G$ has such a covering."],"url":"http://arxiv.org/abs/2402.14529v1","category":"math.GR"}
{"created":"2024-02-22 13:01:07","title":"Realization of the Pascal based on Argon using a Fabry-P\u00e9rot refractometer","abstract":"Based on a recent experimental determination of the static polarizability and a first-principles calculation of the frequency-dependent dipole polarizability of argon, this work presents, by use of a Fabry-P\\'erot refractometer operated at 1550 nm, a realization of the SI unit of pressure, the pascal, for pressures up to 100 kPa, with an uncertainty of [(0.98 mPa)$^2 + (5.8 \\times 10^{-6} P)^2 + (26\\times10^{-12}P^2)^2]^{1/2}$. The work also presents a value of the molar polarizability of N$_2$ at 1550 nm of 4.396572(26)$\\times 10^{-6}$m$^{3}$/mol, which agrees well with previously determined ones in the literature.","sentences":["Based on a recent experimental determination of the static polarizability and a first-principles calculation of the frequency-dependent dipole polarizability of argon, this work presents, by use of a Fabry-P\\'erot refractometer operated at 1550 nm, a realization of the SI unit of pressure, the pascal, for pressures up to 100 kPa, with an uncertainty of [(0.98 mPa)$^2 + (5.8 \\times 10^{-6} P)^2 + (26\\times10^{-12}P^2)^2]^{1/2}$. The work also presents a value of the molar polarizability of N$_2$ at 1550 nm of 4.396572(26)$\\times 10^{-6}$m$^{3}$/mol, which agrees well with previously determined ones in the literature."],"url":"http://arxiv.org/abs/2402.14511v1","category":"physics.optics"}
{"created":"2024-02-22 12:38:42","title":"An Improved Pseudopolynomial Time Algorithm for Subset Sum","abstract":"We investigate pseudo-polynomial time algorithms for Subset Sum. Given a multi-set $X$ of $n$ positive integers and a target $t$, Subset Sum asks whether some subset of $X$ sums to $t$. Bringmann proposes an $\\tilde{O}(n + t)$-time algorithm [Bringmann SODA'17], and an open question has naturally arisen: can Subset Sum be solved in $O(n + w)$ time? Here $w$ is the maximum integer in $X$. We make a progress towards resolving the open question by proposing an $\\tilde{O}(n + \\sqrt{wt})$-time algorithm.","sentences":["We investigate pseudo-polynomial time algorithms for Subset Sum.","Given a multi-set $X$ of $n$ positive integers and a target $t$, Subset Sum asks whether some subset of $X$ sums to $t$. Bringmann proposes an $\\tilde{O}(n + t)$-time algorithm","[Bringmann SODA'17], and an open question has naturally arisen: can Subset Sum be solved in $O(n + w)$ time?","Here $w$ is the maximum integer in $X$. We make a progress towards resolving the open question by proposing an $\\tilde{O}(n + \\sqrt{wt})$-time algorithm."],"url":"http://arxiv.org/abs/2402.14493v1","category":"cs.DS"}
{"created":"2024-02-22 11:53:00","title":"Magnitude homology is a derived functor","abstract":"We prove that the magnitude (co)homology of an enriched category can, under some technical assumptions, be described in terms of derived functors between certain abelian categories. We show how this statement is specified for the cases of quasimetric spaces, finite quasimetric spaces, and finite digraphs. For quasimetric spaces, we define the notion of a magnitude module over a quasimetric space, define the functor of (co)invariants of a magnitude module and show that the magnitude (co)homology can be presented via its derived functors. As a corollary we obtain that the magnitude cohomology of a quasimetric space can be presented in terms of Ext functors in the category of magnitude modules. For finite quasimetric spaces, we show that magnitude (co)homology can be presented in terms of Tor and Ext functors over a certain graded algebra. For finite digraphs, this graded algebra is a bound quiver algebra. In addition, we show that the magnitude cohomology algebra of a finite quasimetric space can be described as a Yoneda algebra.","sentences":["We prove that the magnitude (co)homology of an enriched category can, under some technical assumptions, be described in terms of derived functors between certain abelian categories.","We show how this statement is specified for the cases of quasimetric spaces, finite quasimetric spaces, and finite digraphs.","For quasimetric spaces, we define the notion of a magnitude module over a quasimetric space, define the functor of (co)invariants of a magnitude module and show that the magnitude (co)homology can be presented via its derived functors.","As a corollary we obtain that the magnitude cohomology of a quasimetric space can be presented in terms of Ext functors in the category of magnitude modules.","For finite quasimetric spaces, we show that magnitude (co)homology can be presented in terms of Tor and Ext functors over a certain graded algebra.","For finite digraphs, this graded algebra is a bound quiver algebra.","In addition, we show that the magnitude cohomology algebra of a finite quasimetric space can be described as a Yoneda algebra."],"url":"http://arxiv.org/abs/2402.14466v1","category":"math.KT"}
{"created":"2024-02-22 10:03:21","title":"Algorithm-agnostic significance testing in supervised learning with multimodal data","abstract":"Valid statistical inference is crucial for decision-making but difficult to obtain in supervised learning with multimodal data, e.g., combinations of clinical features, genomic data, and medical images. Multimodal data often warrants the use of black-box algorithms, for instance, random forests or neural networks, which impede the use of traditional variable significance tests. We address this problem by proposing the use of COvariance Measure Tests (COMETs), which are calibrated and powerful tests that can be combined with any sufficiently predictive supervised learning algorithm. We apply COMETs to several high-dimensional, multimodal data sets to illustrate (i) variable significance testing for finding relevant mutations modulating drug-activity, (ii) modality selection for predicting survival in liver cancer patients with multiomics data, and (iii) modality selection with clinical features and medical imaging data. In all applications, COMETs yield results consistent with domain knowledge without requiring data-driven pre-processing which may invalidate type I error control. These novel applications with high-dimensional multimodal data corroborate prior results on the power and robustness of COMETs for significance testing. The comets R package and source code for reproducing all results is available at https://github.com/LucasKook/comets. All data sets used in this work are openly available.","sentences":["Valid statistical inference is crucial for decision-making but difficult to obtain in supervised learning with multimodal data, e.g., combinations of clinical features, genomic data, and medical images.","Multimodal data often warrants the use of black-box algorithms, for instance, random forests or neural networks, which impede the use of traditional variable significance tests.","We address this problem by proposing the use of COvariance Measure Tests (COMETs), which are calibrated and powerful tests that can be combined with any sufficiently predictive supervised learning algorithm.","We apply COMETs to several high-dimensional, multimodal data sets to illustrate (i) variable significance testing for finding relevant mutations modulating drug-activity, (ii) modality selection for predicting survival in liver cancer patients with multiomics data, and (iii) modality selection with clinical features and medical imaging data.","In all applications, COMETs yield results consistent with domain knowledge without requiring data-driven pre-processing which may invalidate type I error control.","These novel applications with high-dimensional multimodal data corroborate prior results on the power and robustness of COMETs for significance testing.","The comets R package and source code for reproducing all results is available at https://github.com/LucasKook/comets.","All data sets used in this work are openly available."],"url":"http://arxiv.org/abs/2402.14416v1","category":"stat.AP"}
{"created":"2024-02-22 10:02:27","title":"The PORTSEA (Portuguese School of Extremes and Applications) and a few personal scientific achievements","abstract":"The Portuguese School of Extremes and Applications is nowadays well recognised by the international scientific community, and in my opinion, the organisation of a NATO Advanced Study Institute on Statistical Extremes and Applications, which took place at Vimeiro in the summer of 1983, was a landmark for the international recognition of the group. The dynamic of publication has been very high and the topics under investigation in the area of Extremes have been quite diverse. In this article, attention will be paid essentially to some of the scientific achievements of the author in this field.","sentences":["The Portuguese School of Extremes and Applications is nowadays well recognised by the international scientific community, and in my opinion, the organisation of a NATO Advanced Study Institute on Statistical Extremes and Applications, which took place at Vimeiro in the summer of 1983, was a landmark for the international recognition of the group.","The dynamic of publication has been very high and the topics under investigation in the area of Extremes have been quite diverse.","In this article, attention will be paid essentially to some of the scientific achievements of the author in this field."],"url":"http://arxiv.org/abs/2402.14414v1","category":"stat.OT"}
{"created":"2024-02-22 09:43:25","title":"Global Safe Sequential Learning via Efficient Knowledge Transfer","abstract":"Sequential learning methods such as active learning and Bayesian optimization select the most informative data to learn about a task. In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence. However, accurate safety modeling requires prior knowledge or consumes data. In addition, the safety confidence centers around the given observations which leads to local exploration. As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. In this paper, we theoretically analyze the maximum explorable safe regions of conventional safe learning methods. Furthermore, we empirically demonstrate that our approach 1) learns a task with lower data consumption, 2) globally explores multiple disjoint safe regions under guidance of the source knowledge, and 3) operates with computation comparable to conventional safe learning methods.","sentences":["Sequential learning methods such as active learning and Bayesian optimization select the most informative data to learn about a task.","In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions.","A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence.","However, accurate safety modeling requires prior knowledge or consumes data.","In addition, the safety confidence centers around the given observations which leads to local exploration.","As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety.","We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data.","In this paper, we theoretically analyze the maximum explorable safe regions of conventional safe learning methods.","Furthermore, we empirically demonstrate that our approach 1) learns a task with lower data consumption, 2) globally explores multiple disjoint safe regions under guidance of the source knowledge, and 3) operates with computation comparable to conventional safe learning methods."],"url":"http://arxiv.org/abs/2402.14402v1","category":"cs.LG"}
{"created":"2024-02-22 09:20:54","title":"Quantum Circuit Optimization with AlphaTensor","abstract":"A key challenge in realizing fault-tolerant quantum computers is circuit optimization. Focusing on the most expensive gates in fault-tolerant quantum computation (namely, the T gates), we address the problem of T-count optimization, i.e., minimizing the number of T gates that are needed to implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a method based on deep reinforcement learning that exploits the relationship between optimizing T-count and tensor decomposition. Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms the existing methods for T-count optimization on a set of arithmetic benchmarks (even when compared without making use of gadgets). Remarkably, it discovers an efficient algorithm akin to Karatsuba's method for multiplication in finite fields. AlphaTensor-Quantum also finds the best human-designed solutions for relevant arithmetic computations used in Shor's algorithm and for quantum chemistry simulation, thus demonstrating it can save hundreds of hours of research by optimizing relevant quantum circuits in a fully automated way.","sentences":["A key challenge in realizing fault-tolerant quantum computers is circuit optimization.","Focusing on the most expensive gates in fault-tolerant quantum computation (namely, the T gates), we address the problem of T-count optimization, i.e., minimizing the number of T gates that are needed to implement a given circuit.","To achieve this, we develop AlphaTensor-Quantum, a method based on deep reinforcement learning that exploits the relationship between optimizing T-count and tensor decomposition.","Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits.","AlphaTensor-Quantum outperforms the existing methods for T-count optimization on a set of arithmetic benchmarks (even when compared without making use of gadgets).","Remarkably, it discovers an efficient algorithm akin to Karatsuba's method for multiplication in finite fields.","AlphaTensor-Quantum also finds the best human-designed solutions for relevant arithmetic computations used in Shor's algorithm and for quantum chemistry simulation, thus demonstrating it can save hundreds of hours of research by optimizing relevant quantum circuits in a fully automated way."],"url":"http://arxiv.org/abs/2402.14396v1","category":"quant-ph"}
{"created":"2024-02-22 08:51:39","title":"Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning","abstract":"Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction. To address the third issue, we design CoH as a paly-and-plug module to enhance the performance of graph-based models for TKG prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH.","sentences":["Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories.","Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities.","Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged.","However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited.","(2) LLMs struggle with optimal reasoning performance under heavy historical information loads.","(3) For TKG prediction, the temporal reasoning capability of LLM alone is limited.","To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction.","To address the third issue, we design CoH as a paly-and-plug module to enhance the performance of graph-based models for TKG prediction.","Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH."],"url":"http://arxiv.org/abs/2402.14382v1","category":"cs.CL"}
{"created":"2024-02-22 08:21:46","title":"HR-APR: APR-agnostic Framework with Uncertainty Estimation and Hierarchical Refinement for Camera Relocalisation","abstract":"Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnostic framework, HR-APR, that formulates uncertainty estimation as cosine similarity estimation between the query and database features. It does not rely on or affect APR network architecture, which is flexible and computationally efficient. In addition, we take advantage of the uncertainty for pose refinement to enhance the performance of APR. The extensive experiments demonstrate the effectiveness of our framework, reducing 27.4\\% and 15.2\\% of computational overhead on the 7Scenes and Cambridge Landmarks datasets while maintaining the SOTA accuracy in single-image APRs.","sentences":["Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries.","Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions.","However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods.","This work introduces a novel APR-agnostic framework, HR-APR, that formulates uncertainty estimation as cosine similarity estimation between the query and database features.","It does not rely on or affect APR network architecture, which is flexible and computationally efficient.","In addition, we take advantage of the uncertainty for pose refinement to enhance the performance of APR.","The extensive experiments demonstrate the effectiveness of our framework, reducing 27.4\\% and 15.2\\% of computational overhead on the 7Scenes and Cambridge Landmarks datasets while maintaining the SOTA accuracy in single-image APRs."],"url":"http://arxiv.org/abs/2402.14371v1","category":"cs.CV"}
{"created":"2024-02-22 08:04:49","title":"Bayesian Model Averaging (BMA) for nuclear data evaluation","abstract":"To ensure agreement between theoretical calculations and experimental data, parameters to selected nuclear physics models, are perturbed, and fine-tuned in nuclear data evaluations. This approach assumes that the chosen set of models accurately represents the `true' distribution. Furthermore, the models are chosen globally, indicating their applicability across the entire energy range of interest. However, this approach overlooks uncertainties inherent in the models themselves. As a result, achieving satisfactory fits to experimental data within certain energy regions for specific channels becomes challenging, as the evaluation is constrained by the deficiencies of the selected models. In this work, we propose that instead of selecting globally a winning model set and proceeding with it as if it was the `true' model set, we instead, take a weighted average over multiple models within a BMA framework, each weighted by its posterior probability. The method involves executing a set of TALYS calculations by randomly varying multiple nuclear physics models and their parameters to yield a vector of calculated observables. Next, the likelihood function was computed at each considered incident energy point for selected cross sections by comparing the vector of calculated observables with that of the selected differential experimental data. As the cross sections and elastic angular distributions were updated locally on a per-energy-point basis, the approach typically results in discontinuities or \"kinks\" in the curves, and these were addressed using spline interpolation. The proposed BMA method was applied to the evaluation of proton induced reactions on $^{58}$Ni within 1 - 100 MeV. The results demonstrate favorable comparisons with experimental data, as well as with the TENDL-2021 evaluation.","sentences":["To ensure agreement between theoretical calculations and experimental data, parameters to selected nuclear physics models, are perturbed, and fine-tuned in nuclear data evaluations.","This approach assumes that the chosen set of models accurately represents the `true' distribution.","Furthermore, the models are chosen globally, indicating their applicability across the entire energy range of interest.","However, this approach overlooks uncertainties inherent in the models themselves.","As a result, achieving satisfactory fits to experimental data within certain energy regions for specific channels becomes challenging, as the evaluation is constrained by the deficiencies of the selected models.","In this work, we propose that instead of selecting globally a winning model set and proceeding with it as if it was the `true' model set, we instead, take a weighted average over multiple models within a BMA framework, each weighted by its posterior probability.","The method involves executing a set of TALYS calculations by randomly varying multiple nuclear physics models and their parameters to yield a vector of calculated observables.","Next, the likelihood function was computed at each considered incident energy point for selected cross sections by comparing the vector of calculated observables with that of the selected differential experimental data.","As the cross sections and elastic angular distributions were updated locally on a per-energy-point basis, the approach typically results in discontinuities or \"kinks\" in the curves, and these were addressed using spline interpolation.","The proposed BMA method was applied to the evaluation of proton induced reactions on $^{58}$Ni within 1 - 100 MeV.","The results demonstrate favorable comparisons with experimental data, as well as with the TENDL-2021 evaluation."],"url":"http://arxiv.org/abs/2402.14363v1","category":"nucl-th"}
{"created":"2024-02-22 07:52:20","title":"Exploring Emerging Trends in 5G Malicious Traffic Analysis and Incremental Learning Intrusion Detection Strategies","abstract":"The popularity of 5G networks poses a huge challenge for malicious traffic detection technology. The reason for this is that as the use of 5G technology increases, so does the risk of malicious traffic activity on 5G networks. Malicious traffic activity in 5G networks not only has the potential to disrupt communication services, but also to compromise sensitive data. This can have serious consequences for individuals and organizations. In this paper, we first provide an in-depth study of 5G technology and 5G security. Next we analyze and discuss the latest malicious traffic detection under AI and their applicability to 5G networks, and compare the various traffic detection aspects addressed by SOTA. The SOTA in 5G traffic detection is also analyzed. Next, we propose seven criteria for traffic monitoring datasets to confirm their suitability for future traffic detection studies. Finally, we present three major issues that need to be addressed for traffic detection in 5G environment. The concept of incremental learning techniques is proposed and applied in the experiments, and the experimental results prove to be able to solve the three problems to some extent.","sentences":["The popularity of 5G networks poses a huge challenge for malicious traffic detection technology.","The reason for this is that as the use of 5G technology increases, so does the risk of malicious traffic activity on 5G networks.","Malicious traffic activity in 5G networks not only has the potential to disrupt communication services, but also to compromise sensitive data.","This can have serious consequences for individuals and organizations.","In this paper, we first provide an in-depth study of 5G technology and 5G security.","Next we analyze and discuss the latest malicious traffic detection under AI and their applicability to 5G networks, and compare the various traffic detection aspects addressed by SOTA.","The SOTA in 5G traffic detection is also analyzed.","Next, we propose seven criteria for traffic monitoring datasets to confirm their suitability for future traffic detection studies.","Finally, we present three major issues that need to be addressed for traffic detection in 5G environment.","The concept of incremental learning techniques is proposed and applied in the experiments, and the experimental results prove to be able to solve the three problems to some extent."],"url":"http://arxiv.org/abs/2402.14353v1","category":"cs.CR"}
{"created":"2024-02-22 07:39:41","title":"Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation","abstract":"Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularization constraint to optimize SwinUNETR. Second, an adversarial training strategy is introduced to calibrate the segmentation feature map and consider the multi-scale feature differences between the uncertainty-guided predictive segmentation and the ground truth segmentation, synthesizing the multi-scale adversarial loss directly improves the ability to discriminate the similarity between organizations. Experiments on both the cardiac public MRI dataset (ACDC) and the real-world clinical cohort EAT dataset show that the proposed network outperforms mainstream models, validating that uncertainty-driven and adversarial calibration learning can be used to provide additional information for modeling multi-scale ambiguities.","sentences":["Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries.","EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT.","However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts.","we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation.","The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularization constraint to optimize SwinUNETR.","Second, an adversarial training strategy is introduced to calibrate the segmentation feature map and consider the multi-scale feature differences between the uncertainty-guided predictive segmentation and the ground truth segmentation, synthesizing the multi-scale adversarial loss directly improves the ability to discriminate the similarity between organizations.","Experiments on both the cardiac public MRI dataset (ACDC) and the real-world clinical cohort EAT dataset show that the proposed network outperforms mainstream models, validating that uncertainty-driven and adversarial calibration learning can be used to provide additional information for modeling multi-scale ambiguities."],"url":"http://arxiv.org/abs/2402.14349v1","category":"eess.IV"}
{"created":"2024-02-22 07:20:59","title":"Cluster-then-Match: Efficient Management of Human-Centric, Cell-Less 6G Networks","abstract":"In 5G and beyond (5GB) networks, the notion of cell tends to blur, as a set of points-of-access (PoAs) using different technologies often cover overlapping areas. In this context, high-quality decisions are needed about (i) which PoA to use when serving an end user and (ii) how to manage PoAs, e.g., how to set their power levels. To address this challenge, we present Cluster-then-Match (CtM), an efficient algorithm making joint decisions about user assignment and PoA management. Following the human-centric networking paradigm, such decisions account not only for the performance of the network, but also for the level of electromagnetic field exposure to which human bodies incur and energy consumption. Our performance evaluation shows how CtM can match the performance of state-of-the-art network management schemes, while reducing electromagnetic emissions and energy consumption by over 80%.","sentences":["In 5G and beyond (5GB) networks, the notion of cell tends to blur, as a set of points-of-access (PoAs) using different technologies often cover overlapping areas.","In this context, high-quality decisions are needed about (i) which PoA to use when serving an end user and (ii) how to manage PoAs, e.g., how to set their power levels.","To address this challenge, we present Cluster-then-Match (CtM), an efficient algorithm making joint decisions about user assignment and PoA management.","Following the human-centric networking paradigm, such decisions account not only for the performance of the network, but also for the level of electromagnetic field exposure to which human bodies incur and energy consumption.","Our performance evaluation shows how CtM can match the performance of state-of-the-art network management schemes, while reducing electromagnetic emissions and energy consumption by over 80%."],"url":"http://arxiv.org/abs/2402.14344v1","category":"cs.NI"}
{"created":"2024-02-22 07:19:02","title":"Light-Metal Functionalized Boron Monoxide Monolayers as Efficient Hydrogen Storage Material: Insights from DFT Simulations","abstract":"Exceptionally high energy density by mass, natural abundance, widespread applications, and environmental friendliness make hydrogen (H2) a front-runner among clean energy options. However, the transition toward clean and renewable energy applications and the actualization of H2 economy require an efficient H2 storage medium. Material-based H2 storage is a viable option, as liquefaction and storage under pressure require ultra-low temperature (-253{\\deg}C) and tremendously high pressure (700 atm), respectively. In this work, we highlight the exceptional H2 storage capabilities of recently synthesized boron monoxide (BO) monolayer functionalized with light metals (Li, Na, K, and Ca). Our computational approach, employing density functional theory (DFT), ab initio molecular dynamics (AIMD), and thermodynamic analysis, reveals promising results. We found that up to four metal dopants (Li, Na, K, and Ca) can be adsorbed onto BO monolayer with significantly strong binding energies. Importantly, these bindings surpass the cohesive counterparts of the parental metal bulks, consequently stabilizing the crystal integrities, as confirmed by AIMD simulations. Each metal dopant on BO efficiently adsorbs multiple H2 molecules through electrostatic and van der Waals interactions. Interestingly, the metal-functionalized BO monolayers exhibit exceptionally high H2 gravimetric capacities up to 11.75 wt%. These promising capacities exceed the 5.50 wt% target set by the US Department of Energy for 2025. Following the same guidelines, the average binding energy per H2 molecule is within the range of -0.17 to -0.32 eV. The adsorption and desorption of H2 under practical working conditions are investigated by Langmuir adsorption model based statistical thermodynamic analysis, further supporting the potential of metal-functionalized BO monolayers for material-based H2 storage applications.","sentences":["Exceptionally high energy density by mass, natural abundance, widespread applications, and environmental friendliness make hydrogen (H2) a front-runner among clean energy options.","However, the transition toward clean and renewable energy applications and the actualization of H2 economy require an efficient H2 storage medium.","Material-based H2 storage is a viable option, as liquefaction and storage under pressure require ultra-low temperature (-253{\\deg}C) and tremendously high pressure (700 atm), respectively.","In this work, we highlight the exceptional H2 storage capabilities of recently synthesized boron monoxide (BO) monolayer functionalized with light metals (Li, Na, K, and Ca).","Our computational approach, employing density functional theory (DFT), ab initio molecular dynamics (AIMD), and thermodynamic analysis, reveals promising results.","We found that up to four metal dopants (Li, Na, K, and Ca) can be adsorbed onto BO monolayer with significantly strong binding energies.","Importantly, these bindings surpass the cohesive counterparts of the parental metal bulks, consequently stabilizing the crystal integrities, as confirmed by AIMD simulations.","Each metal dopant on BO efficiently adsorbs multiple H2 molecules through electrostatic and van der Waals interactions.","Interestingly, the metal-functionalized BO monolayers exhibit exceptionally high H2 gravimetric capacities up to 11.75 wt%.","These promising capacities exceed the 5.50 wt% target set by the US Department of Energy for 2025.","Following the same guidelines, the average binding energy per H2 molecule is within the range of -0.17 to -0.32 eV. The adsorption and desorption of H2 under practical working conditions are investigated by Langmuir adsorption model based statistical thermodynamic analysis, further supporting the potential of metal-functionalized BO monolayers for material-based H2 storage applications."],"url":"http://arxiv.org/abs/2402.14342v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 06:53:35","title":"From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection","abstract":"In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuristic. We validate our theoretical analysis with empirical results, observing that on real-world clustering instances, we can use a subsample of as little as 5% of the data to identify which algorithm is best on the full dataset.","sentences":["In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use.","We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries.","Ideally, the clustering algorithm's output will be structurally close to the ground truth.","We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy.","We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance.","We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuristic.","We validate our theoretical analysis with empirical results, observing that on real-world clustering instances, we can use a subsample of as little as 5% of the data to identify which algorithm is best on the full dataset."],"url":"http://arxiv.org/abs/2402.14332v1","category":"cs.LG"}
{"created":"2024-02-22 06:25:10","title":"Estimation of Spectral Risk Measure for Left Truncated and Right Censored Data","abstract":"Left truncated and right censored data are encountered frequently in insurance loss data due to deductibles and policy limits. Risk estimation is an important task in insurance as it is a necessary step for determining premiums under various policy terms. Spectral risk measures are inherently coherent and have the benefit of connecting the risk measure to the user's risk aversion. In this paper we study the estimation of spectral risk measure based on left truncated and right censored data. We propose a non parametric estimator of spectral risk measure using the product limit estimator and establish the asymptotic normality for our proposed estimator. We also develop an Edgeworth expansion of our proposed estimator. The bootstrap is employed to approximate the distribution of our proposed estimator and shown to be second order ``accurate''. Monte Carlo studies are conducted to compare the proposed spectral risk measure estimator with the existing parametric and non parametric estimators for left truncated and right censored data. Based on our simulation study we estimate the exponential spectral risk measure for three data sets viz; Norwegian fire claims data set, Spain automobile insurance claims and French marine losses.","sentences":["Left truncated and right censored data are encountered frequently in insurance loss data due to deductibles and policy limits.","Risk estimation is an important task in insurance as it is a necessary step for determining premiums under various policy terms.","Spectral risk measures are inherently coherent and have the benefit of connecting the risk measure to the user's risk aversion.","In this paper we study the estimation of spectral risk measure based on left truncated and right censored data.","We propose a non parametric estimator of spectral risk measure using the product limit estimator and establish the asymptotic normality for our proposed estimator.","We also develop an Edgeworth expansion of our proposed estimator.","The bootstrap is employed to approximate the distribution of our proposed estimator and shown to be second order ``accurate''.","Monte Carlo studies are conducted to compare the proposed spectral risk measure estimator with the existing parametric and non parametric estimators for left truncated and right censored data.","Based on our simulation study we estimate the exponential spectral risk measure for three data sets viz; Norwegian fire claims data set, Spain automobile insurance claims and French marine losses."],"url":"http://arxiv.org/abs/2402.14322v1","category":"stat.ME"}
{"created":"2024-02-22 05:58:03","title":"Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge","abstract":"Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned Llemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We make our code and dataset publicly available at \\url{https://github.com/jinlanfu/HSP}.","sentences":["Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains.","Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes.","To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps.","Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings.","The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7.","(2) Beyond exploring training-free LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned Llemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B.","We make our code and dataset publicly available at \\url{https://github.com/jinlanfu/HSP}."],"url":"http://arxiv.org/abs/2402.14310v1","category":"cs.CL"}
{"created":"2024-02-22 05:43:39","title":"Open Meshed Anatomy: Towards a comprehensive finite element hexahedral mesh derived from open atlases","abstract":"Computational simulations using methods such as the finite element (FE) method rely on high-quality meshes for achieving accurate results. This study introduces a method for creating a high-quality hexahedral mesh using the Open Anatomy Project's brain atlas. Our atlas-based FE hexahedral mesh of the brain mitigates potential inaccuracies and uncertainties due to segmentation - a process that often requires input of an inexperienced analyst. It accomplishes this by leveraging existing segmentation from the atlas. We further extend the mesh's usability by forming a two-way correspondence between the atlas and mesh. This feature facilitates property assignment for computational simulations and enhances result analysis within an anatomical context. We demonstrate the application of the mesh by solving the electroencephalography (EEG) forward problem. Our method simplifies the mesh creation process, reducing time and effort, and provides a more comprehensive and contextually enriched visualisation of simulation outcomes.","sentences":["Computational simulations using methods such as the finite element (FE) method rely on high-quality meshes for achieving accurate results.","This study introduces a method for creating a high-quality hexahedral mesh using the Open Anatomy Project's brain atlas.","Our atlas-based FE hexahedral mesh of the brain mitigates potential inaccuracies and uncertainties due to segmentation - a process that often requires input of an inexperienced analyst.","It accomplishes this by leveraging existing segmentation from the atlas.","We further extend the mesh's usability by forming a two-way correspondence between the atlas and mesh.","This feature facilitates property assignment for computational simulations and enhances result analysis within an anatomical context.","We demonstrate the application of the mesh by solving the electroencephalography (EEG) forward problem.","Our method simplifies the mesh creation process, reducing time and effort, and provides a more comprehensive and contextually enriched visualisation of simulation outcomes."],"url":"http://arxiv.org/abs/2402.14303v1","category":"cs.CE"}
{"created":"2024-02-22 05:17:49","title":"Mitigating Biases of Large Language Models in Stance Detection with Calibration","abstract":"Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.","sentences":["Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks.","However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance.","Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal).","In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs.","Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases.","Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results."],"url":"http://arxiv.org/abs/2402.14296v1","category":"cs.CL"}
{"created":"2024-02-22 05:05:30","title":"TinyLLaVA: A Framework of Small-scale Large Multimodal Models","abstract":"We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.","sentences":["We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs).","We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes.","Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs.","Under our framework, we train a family of small-scale LMMs.","Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL.","We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections.","Our model weights and codes will be made public."],"url":"http://arxiv.org/abs/2402.14289v1","category":"cs.LG"}
{"created":"2024-02-22 04:55:43","title":"Almost rigidity results of Green functions with non-negative Ricci curvature","abstract":"This short note provides a survey on rigidity and almost rigidity results of Green functions in a non-smooth setting. We also make some observation on the Cheeger-Yau inequality on RCD spaces of non-negative Ricci curvature with applications.","sentences":["This short note provides a survey on rigidity and almost rigidity results of Green functions in a non-smooth setting.","We also make some observation on the Cheeger-Yau inequality on RCD spaces of non-negative Ricci curvature with applications."],"url":"http://arxiv.org/abs/2402.14284v1","category":"math.DG"}
{"created":"2024-02-22 04:41:56","title":"Secure Navigation using Landmark-based Localization in a GPS-denied Environment","abstract":"In modern battlefield scenarios, the reliance on GPS for navigation can be a critical vulnerability. Adversaries often employ tactics to deny or deceive GPS signals, necessitating alternative methods for the localization and navigation of mobile troops. Range-free localization methods such as DV-HOP rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in a dynamic and sparse network topology. Vision-based approaches like SLAM and Visual Odometry use sensor fusion techniques for map generation and pose estimation that are more sophisticated and computationally expensive. This paper proposes a novel framework that integrates landmark-based localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the future state of moving entities along the battlefield. Our framework utilizes safe trajectory information generated by the troop control center by considering identifiable landmarks and pre-defined hazard maps. It performs point inclusion tests on the convex hull of the trajectory segments to ensure the safety and survivability of a moving entity and determines the next point forward decisions. We present a simulated battlefield scenario for two different approaches (with EKF and without EKF) that guide a moving entity through an obstacle and hazard-free path. Using the proposed method, we observed a percent error of 6.51% lengthwise in safe trajectory estimation with an Average Displacement Error (ADE) of 2.97m and a Final Displacement Error (FDE) of 3.27m. The results demonstrate that our approach not only ensures the safety of the mobile units by keeping them within the secure trajectory but also enhances operational effectiveness by adapting to the evolving threat landscape.","sentences":["In modern battlefield scenarios, the reliance on GPS for navigation can be a critical vulnerability.","Adversaries often employ tactics to deny or deceive GPS signals, necessitating alternative methods for the localization and navigation of mobile troops.","Range-free localization methods such as DV-HOP rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in a dynamic and sparse network topology.","Vision-based approaches like SLAM and Visual Odometry use sensor fusion techniques for map generation and pose estimation that are more sophisticated and computationally expensive.","This paper proposes a novel framework that integrates landmark-based localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the future state of moving entities along the battlefield.","Our framework utilizes safe trajectory information generated by the troop control center by considering identifiable landmarks and pre-defined hazard maps.","It performs point inclusion tests on the convex hull of the trajectory segments to ensure the safety and survivability of a moving entity and determines the next point forward decisions.","We present a simulated battlefield scenario for two different approaches (with EKF and without EKF) that guide a moving entity through an obstacle and hazard-free path.","Using the proposed method, we observed a percent error of 6.51% lengthwise in safe trajectory estimation with an Average Displacement Error (ADE) of 2.97m and a Final Displacement Error (FDE) of 3.27m. The results demonstrate that our approach not only ensures the safety of the mobile units by keeping them within the secure trajectory but also enhances operational effectiveness by adapting to the evolving threat landscape."],"url":"http://arxiv.org/abs/2402.14280v1","category":"cs.RO"}
{"created":"2024-02-22 03:50:47","title":"Linear Discriminant Regularized Regression","abstract":"Linear Discriminant Analysis (LDA) is an important classification approach. Its simple linear form makes it easy to interpret and it is capable to handle multi-class responses. It is closely related to other classical multivariate statistical techniques, such as Fisher's discriminant analysis, canonical correlation analysis and linear regression. In this paper we strengthen its connection to multivariate response regression by characterizing the explicit relationship between the discriminant directions and the regression coefficient matrix. This key characterization leads to a new regression-based multi-class classification procedure that is flexible enough to deploy any existing structured, regularized, and even non-parametric, regression methods. Moreover, our new formulation is generically easy to analyze compared to existing regression-based LDA procedures. In particular, we provide complete theoretical guarantees for using the widely used $\\ell_1$-regularization that has not yet been fully analyzed in the LDA context. Our theoretical findings are corroborated by extensive simulation studies and real data analysis.","sentences":["Linear Discriminant Analysis (LDA) is an important classification approach.","Its simple linear form makes it easy to interpret and it is capable to handle multi-class responses.","It is closely related to other classical multivariate statistical techniques, such as Fisher's discriminant analysis, canonical correlation analysis and linear regression.","In this paper we strengthen its connection to multivariate response regression by characterizing the explicit relationship between the discriminant directions and the regression coefficient matrix.","This key characterization leads to a new regression-based multi-class classification procedure that is flexible enough to deploy any existing structured, regularized, and even non-parametric, regression methods.","Moreover, our new formulation is generically easy to analyze compared to existing regression-based LDA procedures.","In particular, we provide complete theoretical guarantees for using the widely used $\\ell_1$-regularization that has not yet been fully analyzed in the LDA context.","Our theoretical findings are corroborated by extensive simulation studies and real data analysis."],"url":"http://arxiv.org/abs/2402.14260v1","category":"stat.ME"}
{"created":"2024-02-22 03:41:05","title":"A hierarchical decomposition for explaining ML performance discrepancies","abstract":"Machine learning (ML) algorithms can often differ in performance across domains. Understanding $\\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Existing methods focus on $\\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(X)$ versus the impact of a shift in the conditional distribution of the outcome $p(Y|X)$; however, such coarse explanations offer only a few options for how one can close the performance gap. $\\textit{Detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. However, existing methods assume knowledge of the full causal graph or make strong parametric assumptions. We introduce a nonparametric hierarchical framework that provides both aggregate and detailed decompositions for explaining why the performance of an ML algorithm differs across domains, without requiring causal knowledge. We derive debiased, computationally-efficient estimators, and statistical inference procedures for asymptotically valid confidence intervals.","sentences":["Machine learning (ML) algorithms can often differ in performance across domains.","Understanding $\\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps.","Existing methods focus on $\\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(X)$ versus the impact of a shift in the conditional distribution of the outcome $p(Y|X)$; however, such coarse explanations offer only a few options for how one can close the performance gap.","$\\textit{Detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions.","However, existing methods assume knowledge of the full causal graph or make strong parametric assumptions.","We introduce a nonparametric hierarchical framework that provides both aggregate and detailed decompositions for explaining why the performance of an ML algorithm differs across domains, without requiring causal knowledge.","We derive debiased, computationally-efficient estimators, and statistical inference procedures for asymptotically valid confidence intervals."],"url":"http://arxiv.org/abs/2402.14254v1","category":"cs.LG"}
{"created":"2024-02-22 03:20:43","title":"Spectrum of the Dirac operator on Compact Riemannian Manifolds","abstract":"In this paper, we consider the eigenvalue problem of Dirac operator on a compact Riemannian manifold isometrically immersed into Euclidean space and derive some extrinsic estimates for the sum of arbitrary consecutive $n$ eigenvalues of the square of the Dirac operator acting on some Dirac invariant subbundles. As some applications, we deduce some eigenvalue inequalities on the compact submanifolds immersed into Euclidean space, unit sphere or projective spaces and further get some bounds of general Reilly type. In addition, we also establish some universal bounds under certain curvature condition and on the meanwhile provide an alternative proof for Anghel's result. In particular, utilizing Atiyah-Singer index theorem, we drive an upper bound estimate for the sum of the first $n$ nontrivial eigenvalues of Atiyah-Singer Laplacian acting on the spin manifolds without dimensional assumption.","sentences":["In this paper, we consider the eigenvalue problem of Dirac operator on a compact Riemannian manifold isometrically immersed into Euclidean space and derive some extrinsic estimates for the sum of arbitrary consecutive $n$ eigenvalues of the square of the Dirac operator acting on some Dirac invariant subbundles.","As some applications, we deduce some eigenvalue inequalities on the compact submanifolds immersed into Euclidean space, unit sphere or projective spaces and further get some bounds of general Reilly type.","In addition, we also establish some universal bounds under certain curvature condition and on the meanwhile provide an alternative proof for Anghel's result.","In particular, utilizing Atiyah-Singer index theorem, we drive an upper bound estimate for the sum of the first $n$ nontrivial eigenvalues of Atiyah-Singer Laplacian acting on the spin manifolds without dimensional assumption."],"url":"http://arxiv.org/abs/2402.14247v1","category":"math.DG"}
{"created":"2024-02-22 02:42:33","title":"Optical properties of two dimensional Dirac Weyl materials with a flatband","abstract":"The emergence of a flat band in Dirac-Weyl materials offers new possibilities for electronic transitions, leading to stronger interaction with light. As a result, the optical conductivity can be significantly enhanced in these flat-band materials as compared with graphene, making them potentially better candidates for optical sensing and modulation. Recently, a comprehensive theory for the optical conductivity of a spectrum of flat-band Dirac-Weyl materials has been developed, with explicit formulas for both the real and imaginary parts of the conductivity derived through two independent approaches. This Perspective offers a review of the development. An understanding of the optical properties of the flat-band Dirac-Weyl materials paves the way for optical device applications in the terahertz-frequency domain.","sentences":["The emergence of a flat band in Dirac-Weyl materials offers new possibilities for electronic transitions, leading to stronger interaction with light.","As a result, the optical conductivity can be significantly enhanced in these flat-band materials as compared with graphene, making them potentially better candidates for optical sensing and modulation.","Recently, a comprehensive theory for the optical conductivity of a spectrum of flat-band Dirac-Weyl materials has been developed, with explicit formulas for both the real and imaginary parts of the conductivity derived through two independent approaches.","This Perspective offers a review of the development.","An understanding of the optical properties of the flat-band Dirac-Weyl materials paves the way for optical device applications in the terahertz-frequency domain."],"url":"http://arxiv.org/abs/2402.14238v1","category":"physics.optics"}
{"created":"2024-02-22 02:33:21","title":"The sum of the reciprocals of the prime divisors of an odd perfect or odd primitive non-deficient number","abstract":"Write $T(n)$ as the sum of the reciprocals of the primes which divide $n$. Write $H(n) = \\prod_{p|n}p/(p-1)$ where the product is over the prime divisors of $n$. We prove new bounds for $T(n)$ and $H(n)$ in terms of the smallest prime factor of $n$, under the assumption that $n$ is an odd perfect number. Some of the results also apply under the weaker assumption that $n$ is odd and primitive non-deficient.","sentences":["Write $T(n)$ as the sum of the reciprocals of the primes which divide $n$. Write $H(n) = \\prod_{p|n}p/(p-1)$ where the product is over the prime divisors of $n$. We prove new bounds for $T(n)$ and","$H(n)$ in terms of the smallest prime factor of $n$, under the assumption that $n$ is an odd perfect number.","Some of the results also apply under the weaker assumption that $n$ is odd and primitive non-deficient."],"url":"http://arxiv.org/abs/2402.14234v1","category":"math.NT"}
{"created":"2024-02-22 02:29:07","title":"The quark flavor-violating ALPs in light of B mesons and hadron colliders","abstract":"The axion-like particle (ALP) may induce flavor-changing neutral currents (FCNCs) when their Peccei-Quinn charges are not generation universal. The search for flavor-violating ALP couplings with a bottom quark so far focused on FCNC processes of $B$ mesons at low energies. The recent measurements of $B\\to K +X$ rare decays place stringent bounds on the quark flavor violations of a light ALP in different decay modes. In this work we propose a novel direct search for bottom flavor-violating interaction of a heavy ALP at the LHC and its upgrades, namely QCD production of an ALP associated with one $b$ jet and one light jet $p~p\\to b~j~a$. We consider the decay of the ALP to photons, muons and invisible ALP decays. The Boosted Decision Tree (BDT) algorithm is used to analyze the events and we train the BDT classifier by feeding in the kinematic observables of signal and backgrounds. Finally, we show the complementarity between the search prospects of hadron colliders and the low-energy $B$ meson constraints from $B$ meson mixing and $B$ meson decays to a light ALP.","sentences":["The axion-like particle (ALP) may induce flavor-changing neutral currents (FCNCs) when their Peccei-Quinn charges are not generation universal.","The search for flavor-violating ALP couplings with a bottom quark so far focused on FCNC processes of $B$ mesons at low energies.","The recent measurements of $B\\to K +X$ rare decays place stringent bounds on the quark flavor violations of a light ALP in different decay modes.","In this work we propose a novel direct search for bottom flavor-violating interaction of a heavy ALP at the LHC and its upgrades, namely QCD production of an ALP associated with one $b$ jet and one light jet $p~p\\to b~j~a$.","We consider the decay of the ALP to photons, muons and invisible ALP decays.","The Boosted Decision Tree (BDT) algorithm is used to analyze the events and we train the BDT classifier by feeding in the kinematic observables of signal and backgrounds.","Finally, we show the complementarity between the search prospects of hadron colliders and the low-energy $B$ meson constraints from $B$ meson mixing and $B$ meson decays to a light ALP."],"url":"http://arxiv.org/abs/2402.14232v1","category":"hep-ph"}
{"created":"2024-02-22 02:17:50","title":"Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion","abstract":"We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequences, support the analysis.","sentences":["We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers.","This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function.","While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data.","Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations.","Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequences, support the analysis."],"url":"http://arxiv.org/abs/2402.14227v1","category":"cs.LG"}
{"created":"2024-02-22 02:07:21","title":"Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News","abstract":"The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the general state of the economy. Then, for every numerical quantity reported in the article, we learn to identify whether it corresponds to an economic indicator and whether it is being reported in a positive or negative way. To perform our analysis, we track six American publishers and each article that appeared in the top 10 slots of their landing page between 2015 and 2023.","sentences":["The mainstream media has much leeway in what it chooses to cover and how it covers it.","These choices have real-world consequences on what people know and their subsequent behaviors.","However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult.","In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup.","We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications.","Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it.","To do this, we define frame prediction as a set of interdependent tasks.","At the article level, we learn to identify the reported stance towards the general state of the economy.","Then, for every numerical quantity reported in the article, we learn to identify whether it corresponds to an economic indicator and whether it is being reported in a positive or negative way.","To perform our analysis, we track six American publishers and each article that appeared in the top 10 slots of their landing page between 2015 and 2023."],"url":"http://arxiv.org/abs/2402.14224v1","category":"cs.CL"}
{"created":"2024-02-22 01:53:24","title":"Velocity Dispersions of Quiescent Galaxies in IllustirisTNG","abstract":"We examine the central stellar velocity dispersion of subhalos based on IllustrisTNG cosmological hydrodynamic simulations. The central velocity dispersion is a fundamental observable that links galaxies with their dark matter subhalos. We carefully explore simulated stellar velocity dispersions derived with different definitions to assess possible systematics. We explore the impact of variation in the identification of member stellar particles, the viewing axes, the velocity dispersion computation technique, and simulation resolution. None of these issues impact the velocity dispersion significantly; any systematic uncertainties are smaller than the random error. We examine the stellar mass-velocity dispersion relation as an observational test of the simulations. At fixed stellar mass, the observed velocity dispersions significantly exceed the simulation results. This discrepancy is an interesting benchmark for the IllustrisTNG simulations because the simulations are not explicitly tuned to match this relation. We demonstrate that the stellar velocity dispersion provides measures of the dark matter velocity dispersion and the dark matter subhalo mass.","sentences":["We examine the central stellar velocity dispersion of subhalos based on IllustrisTNG cosmological hydrodynamic simulations.","The central velocity dispersion is a fundamental observable that links galaxies with their dark matter subhalos.","We carefully explore simulated stellar velocity dispersions derived with different definitions to assess possible systematics.","We explore the impact of variation in the identification of member stellar particles, the viewing axes, the velocity dispersion computation technique, and simulation resolution.","None of these issues impact the velocity dispersion significantly; any systematic uncertainties are smaller than the random error.","We examine the stellar mass-velocity dispersion relation as an observational test of the simulations.","At fixed stellar mass, the observed velocity dispersions significantly exceed the simulation results.","This discrepancy is an interesting benchmark for the IllustrisTNG simulations because the simulations are not explicitly tuned to match this relation.","We demonstrate that the stellar velocity dispersion provides measures of the dark matter velocity dispersion and the dark matter subhalo mass."],"url":"http://arxiv.org/abs/2402.14218v1","category":"astro-ph.GA"}
{"created":"2024-02-22 01:47:29","title":"The diagonal derivative of a skew Schur polynomial","abstract":"We prove a formula for the image of a skew Schur polynomial $s_{\\lambda/\\mu}\\left( x_{1}, x_{2}, \\ldots, x_{N}\\right) $ under the differential operator $\\nabla:= \\dfrac{\\partial}{\\partial x_{1}} +\\dfrac{\\partial}{\\partial x_{2}}+\\cdots+\\dfrac{\\partial}{\\partial x_{N}}$. This generalizes a formula of Weigandt for $\\nabla\\left( s_{\\lambda}\\right) $.","sentences":["We prove a formula for the image of a skew Schur polynomial $s_{\\lambda/\\mu}\\left( x_{1}, x_{2}, \\ldots, x_{N}\\right) $ under the differential operator $\\nabla:= \\dfrac{\\partial}{\\partial x_{1}} +\\dfrac{\\partial}{\\partial x_{2}}+\\cdots+\\dfrac{\\partial}{\\partial x_{N}}$.","This generalizes a formula of Weigandt for $\\nabla\\left( s_{\\lambda}\\right) $."],"url":"http://arxiv.org/abs/2402.14217v1","category":"math.CO"}
{"created":"2024-02-22 01:15:31","title":"A sufficient condition for the height function to be constant in $ I_g\\times_\u03c1\\mathbb{P}^n $","abstract":"This paper makes some modifications to the warped product space. Based on Alias,Impera and Rigoli, a warping function is added to the warped product space. This new function affects the Riemannian metric of the warped product space. In this new warped product space, we continue to discuss the sufficient condition for calculating the height of the immersed surface.","sentences":["This paper makes some modifications to the warped product space.","Based on Alias,Impera and Rigoli, a warping function is added to the warped product space.","This new function affects the Riemannian metric of the warped product space.","In this new warped product space, we continue to discuss the sufficient condition for calculating the height of the immersed surface."],"url":"http://arxiv.org/abs/2402.14204v1","category":"math.DG"}
{"created":"2024-02-22 01:04:18","title":"Random-Order Online Interval Scheduling and Geometric Generalizations","abstract":"In the Maximum Independent Set of Hyperrectangles problem, we are given a set of $n$ (possibly overlapping) $d$-dimensional axis-aligned hyperrectangles, and the goal is to find a subset of non-overlapping hyperrectangles of maximum cardinality. For $d=1$, this corresponds to the classical Interval Scheduling problem, where a simple greedy algorithm returns an optimal solution. In the offline setting, for $d$-dimensional hyperrectangles, polynomial time $(\\log n)^{O(d)}$-approximation algorithms are known. However, the problem becomes notably challenging in the online setting, where the input objects (hyperrectangles) appear one by one in an adversarial order, and on the arrival of an object, the algorithm needs to make an immediate and irrevocable decision whether or not to select the object while maintaining the feasibility. Even for interval scheduling, an $\\Omega(n)$ lower bound is known on the competitive ratio.   To circumvent these negative results, in this work, we study the online maximum independent set of axis-aligned hyperrectangles in the random-order arrival model, where the adversary specifies the set of input objects which then arrive in a uniformly random order. Starting from the prototypical secretary problem, the random-order model has received significant attention to study algorithms beyond the worst-case competitive analysis. Surprisingly, we show that the problem in the random-order model almost matches the best-known offline approximation guarantees, up to polylogarithmic factors. In particular, we give a simple $(\\log n)^{O(d)}$-competitive algorithm for $d$-dimensional hyperrectangles in this model, which runs in $\\tilde{O_d}(n)$ time. Our approach also yields $(\\log n)^{O(d)}$-competitive algorithms in the random-order model for more general objects such as $d$-dimensional fat objects and ellipsoids.","sentences":["In the Maximum Independent Set of Hyperrectangles problem, we are given a set of $n$ (possibly overlapping) $d$-dimensional axis-aligned hyperrectangles, and the goal is to find a subset of non-overlapping hyperrectangles of maximum cardinality.","For $d=1$, this corresponds to the classical Interval Scheduling problem, where a simple greedy algorithm returns an optimal solution.","In the offline setting, for $d$-dimensional hyperrectangles, polynomial time $(\\log n)^{O(d)}$-approximation algorithms are known.","However, the problem becomes notably challenging in the online setting, where the input objects (hyperrectangles) appear one by one in an adversarial order, and on the arrival of an object, the algorithm needs to make an immediate and irrevocable decision whether or not to select the object while maintaining the feasibility.","Even for interval scheduling, an $\\Omega(n)$ lower bound is known on the competitive ratio.   ","To circumvent these negative results, in this work, we study the online maximum independent set of axis-aligned hyperrectangles in the random-order arrival model, where the adversary specifies the set of input objects which then arrive in a uniformly random order.","Starting from the prototypical secretary problem, the random-order model has received significant attention to study algorithms beyond the worst-case competitive analysis.","Surprisingly, we show that the problem in the random-order model almost matches the best-known offline approximation guarantees, up to polylogarithmic factors.","In particular, we give a simple $(\\log n)^{O(d)}$-competitive algorithm for $d$-dimensional hyperrectangles in this model, which runs in $\\tilde{O_d}(n)$ time.","Our approach also yields $(\\log n)^{O(d)}$-competitive algorithms in the random-order model for more general objects such as $d$-dimensional fat objects and ellipsoids."],"url":"http://arxiv.org/abs/2402.14201v1","category":"cs.DS"}
{"created":"2024-02-22 00:38:43","title":"BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay","abstract":"Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environment dynamics. We test BeTAIL on three challenges with expert-level demonstrations of real human gameplay in Gran Turismo Sport. Our proposed residual BeTAIL reduces environment interactions and improves racing performance and stability, even when the BeT is pretrained on different tracks than downstream learning. Videos and code available at: https://sites.google.com/berkeley.edu/BeTAIL/home.","sentences":["Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions.","In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making.","Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks.","In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns.","Thus, we propose BeTAIL:","Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL.","BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environment dynamics.","We test BeTAIL on three challenges with expert-level demonstrations of real human gameplay in Gran Turismo Sport.","Our proposed residual BeTAIL reduces environment interactions and improves racing performance and stability, even when the BeT is pretrained on different tracks than downstream learning.","Videos and code available at: https://sites.google.com/berkeley.edu/BeTAIL/home."],"url":"http://arxiv.org/abs/2402.14194v1","category":"cs.LG"}
{"created":"2024-02-22 00:30:46","title":"Shadow and greybody bounding of a regular scale-dependent black hole solution","abstract":"In this manuscript, we explore the shadow and the greybody bounding characteristics of a regular black hole within 4-dimensional space-time, employing the context of gravity that is scale-dependent. Our focus lies in determining limitations on the parameter denoted as $\\tilde{\\epsilon}$, which serves as a descriptor for the scale-dependent solution with respect to the classically observed shadow radius $R_\\text{sh}$ as indicated in the documented data from the Event Horizon Telescope (EHT). Our result indicates that there is a unique value for $\\tilde{\\epsilon}$ occurring at the reported mean of $R_\\text{sh}$ and the uncertainties could be the result of the fluctuating value of the scale-dependent parameter. We found that $\\tilde{\\epsilon} > 0$ is positive for Sgr. A*, but $\\tilde{\\epsilon}<0$ for M87*. Utilizing M87* as a reference model, we scrutinized the shadow radius and weak deflection angle within the specified constraints. Discrepancies were observed not only in the shadow but also in the deflection angle of photons, particularly when the photon's impact parameter closely approached the critical impact parameter.","sentences":["In this manuscript, we explore the shadow and the greybody bounding characteristics of a regular black hole within 4-dimensional space-time, employing the context of gravity that is scale-dependent.","Our focus lies in determining limitations on the parameter denoted as $\\tilde{\\epsilon}$, which serves as a descriptor for the scale-dependent solution with respect to the classically observed shadow radius $R_\\text{sh}$ as indicated in the documented data from the Event Horizon Telescope (EHT).","Our result indicates that there is a unique value for $\\tilde{\\epsilon}$ occurring at the reported mean of $R_\\text{sh}$ and the uncertainties could be the result of the fluctuating value of the scale-dependent parameter.","We found that $\\tilde{\\epsilon} > 0$ is positive for Sgr.","A*, but $\\tilde{\\epsilon}<0$ for M87*.","Utilizing M87* as a reference model, we scrutinized the shadow radius and weak deflection angle within the specified constraints.","Discrepancies were observed not only in the shadow but also in the deflection angle of photons, particularly when the photon's impact parameter closely approached the critical impact parameter."],"url":"http://arxiv.org/abs/2402.14190v1","category":"gr-qc"}
{"created":"2024-02-22 00:04:21","title":"Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis","abstract":"Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.","sentences":["Ensembles are important tools for improving the performance of machine learning models.","In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source.","However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models.","We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other.","By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble.","The quality improves for both text classification accuracy and relevant uncertainty estimation."],"url":"http://arxiv.org/abs/2402.14184v1","category":"cs.LG"}
{"created":"2024-02-21 23:18:03","title":"A Temporal Bias Correction using a Machine Learning Attention model","abstract":"Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and Tokyo, Japan, we show striking results compared to current climate model outputs and alternative BC methods.","sentences":["Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies.","The suite of statistical methods that enable such calibrations is called bias correction (BC).","However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points.","As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics.","In this paper, we offer a novel BC methodology to correct for temporal biases.","This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task.","With a case study of heatwave duration statistics in Abuja, Nigeria, and Tokyo, Japan, we show striking results compared to current climate model outputs and alternative BC methods."],"url":"http://arxiv.org/abs/2402.14169v1","category":"cs.LG"}
{"created":"2024-02-21 23:08:54","title":"T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching","abstract":"Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. Our key insight is that different diffusion models learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps. Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL, for example, 40% of the early timesteps can be safely replaced with a 10x faster DiT-S without performance drop on class-conditional ImageNet generation. We further show that our method can also be used as a drop-in technique to not only accelerate the popular pretrained stable diffusion (SD) models but also improve the prompt alignment of stylized SD models from the public model zoo. Code is released at https://github.com/NVlabs/T-Stitch","sentences":["Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model.","In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation.","Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage.","Our key insight is that different diffusion models learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps.","Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with flexible speed and quality trade-offs.","On DiT-XL, for example, 40% of the early timesteps can be safely replaced with a 10x faster DiT-S without performance drop on class-conditional ImageNet generation.","We further show that our method can also be used as a drop-in technique to not only accelerate the popular pretrained stable diffusion (SD) models but also improve the prompt alignment of stylized SD models from the public model zoo.","Code is released at https://github.com/NVlabs/T-Stitch"],"url":"http://arxiv.org/abs/2402.14167v1","category":"cs.CV"}
{"created":"2024-02-21 23:04:07","title":"Scalar Electrodynamics and Higgs Mechanism in the Unfolded Dynamics Approach","abstract":"We put forward a new method of constructing unfolded formulations of field theories, which is based on initial fixation of the form of an unfolded field and subsequent looking for the corresponding unfolded equation as an identity that this field satisfies. Making use of this method, we find an unfolded formulation for 4d scalar electrodynamics. By considering a symmetry-breaking scalar potential, we study the implementation of the Higgs mechanism within the framework of the unfolded dynamics approach. We explore a deformation of unfolded modules in the symmetry-broken phase and identify a non-invertible unfolded field redefinition that diagonalizes the higgsed system.","sentences":["We put forward a new method of constructing unfolded formulations of field theories, which is based on initial fixation of the form of an unfolded field and subsequent looking for the corresponding unfolded equation as an identity that this field satisfies.","Making use of this method, we find an unfolded formulation for 4d scalar electrodynamics.","By considering a symmetry-breaking scalar potential, we study the implementation of the Higgs mechanism within the framework of the unfolded dynamics approach.","We explore a deformation of unfolded modules in the symmetry-broken phase and identify a non-invertible unfolded field redefinition that diagonalizes the higgsed system."],"url":"http://arxiv.org/abs/2402.14164v1","category":"hep-th"}
{"created":"2024-02-21 22:27:40","title":"MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms","abstract":"Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models' social understanding capabilities. Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement.","sentences":["Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces.","Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation.","This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content.","MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation.","Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models' social understanding capabilities.","Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks.","However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement."],"url":"http://arxiv.org/abs/2402.14154v1","category":"cs.CL"}
{"created":"2024-02-21 22:16:03","title":"New evidence on the lost giant Chinguetti meteorite","abstract":"The giant Chinguetti meteorite that Gaston Ripert reported seeing in 1916 has never been found. A radionuclide analysis by Welten et al (2001) of the 4.5kg mesosiderite that Ripert recovered, supposedly sitting on the larger object, has convinced many that Ripert was mistaken, and interest in the giant meteorite has subsequently faded. Aspects of Ripert's account of the giant meteorite are nevertheless compelling, particularly his description of ductile metal needles in one area of the surface explored. Several visual searches for the giant meteorite, beginning in 1924, might have failed because the object was already by then covered in sand. Using DEM data we have measured dune heights and established their drift speed. This has allowed us to create a map of locations where the meteorite could lie. The 2004 PRISM-I aeromagnetics surveys, acquired by Fugro for the Mauritanian Government for Mining Sector capacity building purposes, have the necessary area coverage, spatial resolution, and sensitivity to establish if the meteorite exists. In Jan 2023 we requested the PRISM data from the Ministry of Petroleum Energy and Mines, explaining, under Confidentiality, the scientific purpose of the request. To date the data have not been made available to us.","sentences":["The giant Chinguetti meteorite that Gaston Ripert reported seeing in 1916 has never been found.","A radionuclide analysis by Welten et al (2001) of the 4.5kg mesosiderite that Ripert recovered, supposedly sitting on the larger object, has convinced many that Ripert was mistaken, and interest in the giant meteorite has subsequently faded.","Aspects of Ripert's account of the giant meteorite are nevertheless compelling, particularly his description of ductile metal needles in one area of the surface explored.","Several visual searches for the giant meteorite, beginning in 1924, might have failed because the object was already by then covered in sand.","Using DEM data we have measured dune heights and established their drift speed.","This has allowed us to create a map of locations where the meteorite could lie.","The 2004 PRISM-I aeromagnetics surveys, acquired by Fugro for the Mauritanian Government for Mining Sector capacity building purposes, have the necessary area coverage, spatial resolution, and sensitivity to establish if the meteorite exists.","In Jan 2023 we requested the PRISM data from the Ministry of Petroleum Energy and Mines, explaining, under Confidentiality, the scientific purpose of the request.","To date the data have not been made available to us."],"url":"http://arxiv.org/abs/2402.14150v1","category":"astro-ph.EP"}
{"created":"2024-02-21 22:01:10","title":"Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains","abstract":"Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks. We also assess its effectiveness on a user city prediction dataset from a large technology company.","sentences":["Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another.","We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment.","We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis.","The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment.","Our method is designed to be implemented with commonly used off-the-shelf machine learning models.","We establish theoretical guarantees on the generalization bound of the method on the test risk.","With extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks.","We also assess its effectiveness on a user city prediction dataset from a large technology company."],"url":"http://arxiv.org/abs/2402.14145v1","category":"stat.ML"}
{"created":"2024-02-21 22:00:35","title":"Extending identifiability results from isolated networks to embedded networks","abstract":"This paper deals with the design of Excitation and Measurement Patterns (EMPs) for the identification of dynamical networks, when the objective is to identify only a subnetwork embedded in a larger network. Recent results have shown how to construct EMPs that guarantee identifiability for a range of networks with specific graph topologies, such as trees, loops, or Directed Acyclic Graphs (DAGs). However, an EMP that is valid for the identification of a subnetwork taken in isolation may no longer be valid when that subnetwork is embedded in a larger network. Our main contribution is to exhibit conditions under which it does remain valid, and to propose ways to enhance such EMP when these conditions are not satisfied.","sentences":["This paper deals with the design of Excitation and Measurement Patterns (EMPs) for the identification of dynamical networks, when the objective is to identify only a subnetwork embedded in a larger network.","Recent results have shown how to construct EMPs that guarantee identifiability for a range of networks with specific graph topologies, such as trees, loops, or Directed Acyclic Graphs (DAGs).","However, an EMP that is valid for the identification of a subnetwork taken in isolation may no longer be valid when that subnetwork is embedded in a larger network.","Our main contribution is to exhibit conditions under which it does remain valid, and to propose ways to enhance such EMP when these conditions are not satisfied."],"url":"http://arxiv.org/abs/2402.14144v1","category":"eess.SY"}
{"created":"2024-02-21 21:33:07","title":"NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning","abstract":"Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the CNNs into blocks based on GPU memory usage and further attaches an auxiliary network to each layer in these blocks. This disrupts the typical layer dependencies under a new training paradigm - 'adaptive local learning'. Moreover, NeuroFlux adeptly caches intermediate activations, eliminating redundant forward passes over previously trained blocks, further accelerating the training process. The results are twofold when compared to Backpropagation: on various hardware platforms, NeuroFlux demonstrates training speed-ups of 2.3$\\times$ to 6.1$\\times$ under stringent GPU memory budgets, and NeuroFlux generates streamlined models that have 10.9$\\times$ to 29.4$\\times$ fewer parameters without sacrificing accuracy.","sentences":["Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge.","Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory.","This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time.","We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios.","We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process.","NeuroFlux segments the CNNs into blocks based on GPU memory usage and further attaches an auxiliary network to each layer in these blocks.","This disrupts the typical layer dependencies under a new training paradigm - 'adaptive local learning'.","Moreover, NeuroFlux adeptly caches intermediate activations, eliminating redundant forward passes over previously trained blocks, further accelerating the training process.","The results are twofold when compared to Backpropagation: on various hardware platforms, NeuroFlux demonstrates training speed-ups of 2.3$\\times$ to 6.1$\\times$ under stringent GPU memory budgets, and NeuroFlux generates streamlined models that have 10.9$\\times$ to 29.4$\\times$ fewer parameters without sacrificing accuracy."],"url":"http://arxiv.org/abs/2402.14139v1","category":"cs.LG"}
{"created":"2024-02-21 21:10:12","title":"Random forests for detecting weak signals and extracting physical information: a case study of magnetic navigation","abstract":"It was recently demonstrated that two machine-learning architectures, reservoir computing and time-delayed feed-forward neural networks, can be exploited for detecting the Earth's anomaly magnetic field immersed in overwhelming complex signals for magnetic navigation in a GPS-denied environment. The accuracy of the detected anomaly field corresponds to a positioning accuracy in the range of 10 to 40 meters. To increase the accuracy and reduce the uncertainty of weak signal detection as well as to directly obtain the position information, we exploit the machine-learning model of random forests that combines the output of multiple decision trees to give optimal values of the physical quantities of interest. In particular, from time-series data gathered from the cockpit of a flying airplane during various maneuvering stages, where strong background complex signals are caused by other elements of the Earth's magnetic field and the fields produced by the electronic systems in the cockpit, we demonstrate that the random-forest algorithm performs remarkably well in detecting the weak anomaly field and in filtering the position of the aircraft. With the aid of the conventional inertial navigation system, the positioning error can be reduced to less than 10 meters. We also find that, contrary to the conventional wisdom, the classic Tolles-Lawson model for calibrating and removing the magnetic field generated by the body of the aircraft is not necessary and may even be detrimental for the success of the random-forest method.","sentences":["It was recently demonstrated that two machine-learning architectures, reservoir computing and time-delayed feed-forward neural networks, can be exploited for detecting the Earth's anomaly magnetic field immersed in overwhelming complex signals for magnetic navigation in a GPS-denied environment.","The accuracy of the detected anomaly field corresponds to a positioning accuracy in the range of 10 to 40 meters.","To increase the accuracy and reduce the uncertainty of weak signal detection as well as to directly obtain the position information, we exploit the machine-learning model of random forests that combines the output of multiple decision trees to give optimal values of the physical quantities of interest.","In particular, from time-series data gathered from the cockpit of a flying airplane during various maneuvering stages, where strong background complex signals are caused by other elements of the Earth's magnetic field and the fields produced by the electronic systems in the cockpit, we demonstrate that the random-forest algorithm performs remarkably well in detecting the weak anomaly field and in filtering the position of the aircraft.","With the aid of the conventional inertial navigation system, the positioning error can be reduced to less than 10 meters.","We also find that, contrary to the conventional wisdom, the classic Tolles-Lawson model for calibrating and removing the magnetic field generated by the body of the aircraft is not necessary and may even be detrimental for the success of the random-forest method."],"url":"http://arxiv.org/abs/2402.14131v1","category":"eess.SP"}
{"created":"2024-02-21 20:42:30","title":"Automated Resonance Identification in Nuclear Data Evaluation","abstract":"Global and national efforts to deliver high-quality nuclear data to users have a broad impact across applications such as national security, reactor operation, basic science, medical fields, and more. Cross section evaluation is a large part this effort as it combines theory and experiment to produce suggested values and uncertainty for reaction probabilities. In most isotopes, the cross section exhibits resonant behavior in what is called the resonance region of incident neutron energy. Resonance region evaluation is a specialized type of nuclear data evaluation that can require significant, manual effort and months of time from expert scientists. In this article, non-convex, non-linear optimization methods are combined with concepts of inferential statistics to infer a set of optimized resonance models from experimental data in an automated manner that is not dependent on prior evaluation(s). This methodology aims to enhance the workflow of a resonance evaluator by minimizing time, effort, and prior biases while improving reproducibility and document-ability, addressing widely recognized challenges in the field.","sentences":["Global and national efforts to deliver high-quality nuclear data to users have a broad impact across applications such as national security, reactor operation, basic science, medical fields, and more.","Cross section evaluation is a large part this effort as it combines theory and experiment to produce suggested values and uncertainty for reaction probabilities.","In most isotopes, the cross section exhibits resonant behavior in what is called the resonance region of incident neutron energy.","Resonance region evaluation is a specialized type of nuclear data evaluation that can require significant, manual effort and months of time from expert scientists.","In this article, non-convex, non-linear optimization methods are combined with concepts of inferential statistics to infer a set of optimized resonance models from experimental data in an automated manner that is not dependent on prior evaluation(s).","This methodology aims to enhance the workflow of a resonance evaluator by minimizing time, effort, and prior biases while improving reproducibility and document-ability, addressing widely recognized challenges in the field."],"url":"http://arxiv.org/abs/2402.14122v1","category":"physics.comp-ph"}
{"created":"2024-02-21 20:42:03","title":"The Solar Neighborhood LI: A Variability Survey of Nearby M Dwarfs with Planets from Months to Decades with TESS and the CTIO/SMARTS 0.9 m","abstract":"We present the optical photometric variability of 32 planet-hosting M dwarfs within 25 parsecs over timescales of months to decades. The primary goal of this project, ATLAS -- A Trail to Life Around Stars, is to follow the trail to life by revealing nearby M dwarfs with planets that are also \"quiet\", which may make them more amiable to habitability. There are 69 reported exoplanets orbiting the 32 stars discussed here, providing a rich sample of worlds for which environmental evaluations are needed. We examine the optical flux environments of these planets over month-long timescales for 23 stars observed by TESS, and find that 17 vary by less than 1% ($\\sim$11 mmag). All 32 stars are being observed at the CTIO/SMARTS 0.9 m, with a median duration of 19.1 years of optical photometric data in the $VRI$ bands. We find over these extended timescales that six stars show optical flux variations less than 2%, 25 vary from 2--6% ($\\sim$22-67 mmag), and only one, Proxima Centauri, varies by more than 6%. Overall, LHS 1678 exhibits the lowest optical variability levels measured over all timescales examined, thereby providing one of the most stable photometric environments among planets reported around M dwarfs within 25 parsecs. More than 600 of the nearest M dwarfs are being observed at the 0.9 m in the RECONS program that began in 1999, and many more planet hosts will undoubtedly be revealed, providing more destinations to be added to the ATLAS sample in the future.","sentences":["We present the optical photometric variability of 32 planet-hosting M dwarfs within 25 parsecs over timescales of months to decades.","The primary goal of this project, ATLAS -- A Trail to Life Around Stars, is to follow the trail to life by revealing nearby M dwarfs with planets that are also \"quiet\", which may make them more amiable to habitability.","There are 69 reported exoplanets orbiting the 32 stars discussed here, providing a rich sample of worlds for which environmental evaluations are needed.","We examine the optical flux environments of these planets over month-long timescales for 23 stars observed by TESS, and find that 17 vary by less than 1% ($\\sim$11 mmag).","All 32 stars are being observed at the CTIO/SMARTS 0.9 m, with a median duration of 19.1 years of optical photometric data in the $VRI$ bands.","We find over these extended timescales that six stars show optical flux variations less than 2%, 25 vary from 2--6% ($\\sim$22-67 mmag), and only one, Proxima Centauri, varies by more than 6%.","Overall, LHS 1678 exhibits the lowest optical variability levels measured over all timescales examined, thereby providing one of the most stable photometric environments among planets reported around M dwarfs within 25 parsecs.","More than 600 of the nearest M dwarfs are being observed at the 0.9 m in the RECONS program that began in 1999, and many more planet hosts will undoubtedly be revealed, providing more destinations to be added to the ATLAS sample in the future."],"url":"http://arxiv.org/abs/2402.14121v1","category":"astro-ph.SR"}
{"created":"2024-02-21 20:41:19","title":"Efficient Wait-Free Linearizable Implementations of Approximate Bounded Counters Using Read-Write Registers","abstract":"Relaxing the sequential specification of a shared object is a way to obtain an implementation with better performance compared to implementing the original specification. We apply this approach to the Counter object, under the assumption that the number of times the Counter is incremented in any execution is at most a known bound $m$. We consider the $k$-multiplicative-accurate Counter object, where each read operation returns an approximate value that is within a multiplicative factor $k$ of the accurate value. More specifically, a read is allowed to return an approximate value $x$ of the number $v$ of increments previously applied to the counter such that $v/k \\le x \\le vk$. We present three algorithms to implement this object in a wait-free linearizable manner in the shared memory model using read-write registers. All the algorithms have read operations whose worst-case step complexity improves exponentially on that for an exact $m$-bounded counter (which in turn improves exponentially on that for an exact unbounded counter). Two of the algorithms have read step complexity that is asymptotically optimal. The algorithms differ in their requirements on $k$, step complexity of the increment operation, and space complexity.","sentences":["Relaxing the sequential specification of a shared object is a way to obtain an implementation with better performance compared to implementing the original specification.","We apply this approach to the Counter object, under the assumption that the number of times the Counter is incremented in any execution is at most a known bound $m$. We consider the $k$-multiplicative-accurate Counter object, where each read operation returns an approximate value that is within a multiplicative factor $k$ of the accurate value.","More specifically, a read is allowed to return an approximate value $x$ of the number $v$ of increments previously applied to the counter such that $v/k \\le x \\le vk$.","We present three algorithms to implement this object in a wait-free linearizable manner in the shared memory model using read-write registers.","All the algorithms have read operations whose worst-case step complexity improves exponentially on that for an exact $m$-bounded counter (which in turn improves exponentially on that for an exact unbounded counter).","Two of the algorithms have read step complexity that is asymptotically optimal.","The algorithms differ in their requirements on $k$, step complexity of the increment operation, and space complexity."],"url":"http://arxiv.org/abs/2402.14120v1","category":"cs.DC"}
{"created":"2024-02-21 20:29:27","title":"Constraints on new physics with (anti)neutrino-nucleon scattering data","abstract":"New physics contributions to the (anti)neutrino-nucleon elastic scattering process can be constrained by precision measurements, with controlled Standard Model uncertainties. In a large class of new physics models, interactions involving charged leptons of different flavor can be related, and the large muon flavor component of accelerator neutrino beams can mitigate the lepton mass suppression that occurs in other low-energy measurements. We employ the recent high-statistics measurement of the cross section for $\\bar{\\nu}_\\mu p \\to \\mu^+ n$ scattering on the hydrogen atom by MINERvA to place new confidence intervals on tensor and scalar neutrino-nucleon interactions: $\\mathfrak{Re} C_T = -1^{+14}_{-13} \\times 10^{-4}$, $|\\mathfrak{Im} C_T| \\le 1.3 \\times 10^{-3}$, and $|\\mathfrak{Im} C_S| = 45^{+13}_{-19} \\times 10^{-3}$. These results represent a reduction in uncertainty by a factor of $2.1$, $3.1$, and $1.2$, respectively, compared to existing constraints from precision beta decay.","sentences":["New physics contributions to the (anti)neutrino-nucleon elastic scattering process can be constrained by precision measurements, with controlled Standard Model uncertainties.","In a large class of new physics models, interactions involving charged leptons of different flavor can be related, and the large muon flavor component of accelerator neutrino beams can mitigate the lepton mass suppression that occurs in other low-energy measurements.","We employ the recent high-statistics measurement of the cross section for $\\bar{\\nu}_\\mu p \\to \\mu^+ n$ scattering on the hydrogen atom by MINERvA to place new confidence intervals on tensor and scalar neutrino-nucleon interactions: $\\mathfrak{Re} C_T = -1^{+14}_{-13} \\times 10^{-4}$, $|\\mathfrak{Im} C_T| \\le 1.3 \\times 10^{-3}$, and $|\\mathfrak{Im} C_S| = 45^{+13}_{-19} \\times 10^{-3}$.","These results represent a reduction in uncertainty by a factor of $2.1$, $3.1$, and $1.2$, respectively, compared to existing constraints from precision beta decay."],"url":"http://arxiv.org/abs/2402.14115v1","category":"hep-ph"}
{"created":"2024-02-21 20:25:36","title":"Unveiling Crowdfunding Futures: Analyzing Campaign Outcomes through Distributed Models and Big Data Perspectives","abstract":"Crowdfunding has emerged as a widespread strategy for startups seeking financing, particularly through reward-based methods. However, understanding its economic impact at both micro and macro levels requires thorough analysis, often involving advanced studies on past campaigns to extract insights that aiding companies in optimizing their crowdfunding project types and launch methodologies. Such analyses are often beyond the scope of basic data analysis techniques and frequently demand advanced machine learning tools, such as distributed computing, due to the large volume of data involved. This study aims to investigate and analyse the targets of reward-based crowdfunding campaigns through machine learning techniques, employing distributed models and structures. By harnessing the power of distributed computing, it unravels intricate patterns and trends within crowdfunding data, thereby empowering companies to refine their strategies and enhance the efficacy of their funding endeavors. Through this multifaceted approach, a deeper understanding of the economic dynamics underlying crowdfunding ecosystems can be attained, fostering informed decision-making and sustainable growth within the startup landscape.","sentences":["Crowdfunding has emerged as a widespread strategy for startups seeking financing, particularly through reward-based methods.","However, understanding its economic impact at both micro and macro levels requires thorough analysis, often involving advanced studies on past campaigns to extract insights that aiding companies in optimizing their crowdfunding project types and launch methodologies.","Such analyses are often beyond the scope of basic data analysis techniques and frequently demand advanced machine learning tools, such as distributed computing, due to the large volume of data involved.","This study aims to investigate and analyse the targets of reward-based crowdfunding campaigns through machine learning techniques, employing distributed models and structures.","By harnessing the power of distributed computing, it unravels intricate patterns and trends within crowdfunding data, thereby empowering companies to refine their strategies and enhance the efficacy of their funding endeavors.","Through this multifaceted approach, a deeper understanding of the economic dynamics underlying crowdfunding ecosystems can be attained, fostering informed decision-making and sustainable growth within the startup landscape."],"url":"http://arxiv.org/abs/2402.14111v1","category":"cs.DC"}
{"created":"2024-02-21 20:21:48","title":"Driving Towards Stability and Efficiency: A Variable Time Gap Strategy for Adaptive Cruise Control","abstract":"Automated vehicle technologies offer a promising avenue for enhancing traffic efficiency, safety, and energy consumption. Among these, Adaptive Cruise Control (ACC) systems stand out as a prevalent form of automation on today's roads, with their time gap settings holding paramount importance. While decreasing the average time headway tends to enhance traffic capacity, it simultaneously raises concerns regarding safety and string stability. This study introduces a novel variable time gap feedback control policy aimed at striking a balance between maintaining a minimum time gap setting under equilibrium car-following conditions, thereby improving traffic capacity, while ensuring string stability to mitigate disturbances away from the equilibrium flow. Leveraging nonlinear $H_\\infty$ control technique, the strategy employs a variable time gap component as the manipulated control signal, complemented by a constant time gap component that predominates during car-following equilibrium. The effectiveness of the proposed scheme is evaluated against its constant time-gap counterpart calibrated using field platoon data from the OpenACC dataset. Through numerical and traffic simulations, our findings illustrate that the proposed algorithm effectively dampens perturbations within vehicle platoons, leading to a more efficient and safer mixed traffic flow.","sentences":["Automated vehicle technologies offer a promising avenue for enhancing traffic efficiency, safety, and energy consumption.","Among these, Adaptive Cruise Control (ACC) systems stand out as a prevalent form of automation on today's roads, with their time gap settings holding paramount importance.","While decreasing the average time headway tends to enhance traffic capacity, it simultaneously raises concerns regarding safety and string stability.","This study introduces a novel variable time gap feedback control policy aimed at striking a balance between maintaining a minimum time gap setting under equilibrium car-following conditions, thereby improving traffic capacity, while ensuring string stability to mitigate disturbances away from the equilibrium flow.","Leveraging nonlinear $H_\\infty$ control technique, the strategy employs a variable time gap component as the manipulated control signal, complemented by a constant time gap component that predominates during car-following equilibrium.","The effectiveness of the proposed scheme is evaluated against its constant time-gap counterpart calibrated using field platoon data from the OpenACC dataset.","Through numerical and traffic simulations, our findings illustrate that the proposed algorithm effectively dampens perturbations within vehicle platoons, leading to a more efficient and safer mixed traffic flow."],"url":"http://arxiv.org/abs/2402.14110v1","category":"eess.SY"}
{"created":"2024-02-21 19:59:10","title":"The energetic inception of breaking in surface gravity waves under wind forcing","abstract":"The breaking of surface gravity waves is a key process contributing to air-sea fluxes and turbulent ocean mixing. The highly nonlinear nature of wave breaking, combined with the challenges of observing this process in a laboratory or field setting, leaves our understanding of the energetic processes underpinning wave breaking incomplete. Progress towards refining this understanding was made in a recent study (D. G. Boettger et. al., An energetic signature for breaking inception in surface gravity waves, Journal of Fluid Mechanics 959, A33 (2023)), which identified an energetic signature in the wave kinetic energy evolution that preceded breaking onset and correlated with the strength of the breaking event. In this study, we examine the influence of wind forcing on this energetic signature. We develop a numerical wave tank that simulates wind flowing over mechanically generated waves and construct an ensemble of cases with varying wave steepness and wind forcing speed. The wind is shown to modulate the wave geometry and elevate kinetic energy at crest tip by up to 35 \\%. Despite these influences, the energetic inception signature was found to robustly indicate breaking inception in all cases examined, with a threshold value in the kinetic energy growth rate at this instant separating breaking and non-breaking waves. Under wind forcing, the timing of the energetic inception point occurred slightly earlier that unforced breaking waves, giving advance warning of breaking 0.3 wave periods prior to breaking onset.","sentences":["The breaking of surface gravity waves is a key process contributing to air-sea fluxes and turbulent ocean mixing.","The highly nonlinear nature of wave breaking, combined with the challenges of observing this process in a laboratory or field setting, leaves our understanding of the energetic processes underpinning wave breaking incomplete.","Progress towards refining this understanding was made in a recent study (D. G. Boettger et.","al., An energetic signature for breaking inception in surface gravity waves, Journal of Fluid Mechanics 959, A33 (2023)), which identified an energetic signature in the wave kinetic energy evolution that preceded breaking onset and correlated with the strength of the breaking event.","In this study, we examine the influence of wind forcing on this energetic signature.","We develop a numerical wave tank that simulates wind flowing over mechanically generated waves and construct an ensemble of cases with varying wave steepness and wind forcing speed.","The wind is shown to modulate the wave geometry and elevate kinetic energy at crest tip by up to 35 \\%.","Despite these influences, the energetic inception signature was found to robustly indicate breaking inception in all cases examined, with a threshold value in the kinetic energy growth rate at this instant separating breaking and non-breaking waves.","Under wind forcing, the timing of the energetic inception point occurred slightly earlier that unforced breaking waves, giving advance warning of breaking 0.3 wave periods prior to breaking onset."],"url":"http://arxiv.org/abs/2402.14106v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 19:40:07","title":"Rigid frameworks with dilation constraints","abstract":"We consider the rigidity and global rigidity of bar-joint frameworks in Euclidean $d$-space under additional dilation constraints in specified coordinate directions. In this setting we obtain a complete characterisation of generic rigidity. We then consider generic global rigidity. In particular, we provide an algebraic sufficient condition and a weak necessary condition. We also construct a large family of globally rigid frameworks and conjecture a combinatorial characterisation when most coordinate directions have dilation constraints.","sentences":["We consider the rigidity and global rigidity of bar-joint frameworks in Euclidean $d$-space under additional dilation constraints in specified coordinate directions.","In this setting we obtain a complete characterisation of generic rigidity.","We then consider generic global rigidity.","In particular, we provide an algebraic sufficient condition and a weak necessary condition.","We also construct a large family of globally rigid frameworks and conjecture a combinatorial characterisation when most coordinate directions have dilation constraints."],"url":"http://arxiv.org/abs/2402.14093v1","category":"math.CO"}
{"created":"2024-02-21 19:17:41","title":"Diverse Oxygen Abundance in Early Galaxies Unveiled by Auroral Line Analysis with JWST","abstract":"We present deep JWST NIRSpec observations in the sightline of MACS J1149.5+2223, a massive cluster of galaxies at $z=0.54$. We report the spectroscopic redshift of 28 sources at $3<z<9.1$, including 9 sources with the detection of the [OIII]4363 auroral line. Combining these with 16 [OIII]4363-detected sources from publicly available JWST data, our sample consists of 25 galaxies with robust gas-phase metallicity measurements via the direct method. We observe a positive correlation between stellar mass and metallicity, with a $\\sim0.5$\\,dex offset down below the local relation. Interestingly, we find a larger than expected scatter of $\\sim0.3$\\,dex around the relation, which cannot be explained by redshift evolution among our sample or other third parameter. The scatter increases at higher redshift, and we attribute this to the enrichment process having higher stochasticity due to shallower potential wells, more intense feedback processes, and a higher galaxy merger rate. Despite reaching to a considerably low-mass regime ($\\log M_*/M_\\odot \\sim7.3$), our samples have metallicity of $\\log$(O/H)$+12>7$, i.e. comparable to the most metal poor galaxies in the local Universe. The search of primordial galaxies may be accomplished by extending toward a lower mass and/or by investigating inhomogeneities at smaller spatial scales. Lastly, we investigate potential systematics caused by the limitation of JWST's MSA observations. Caution is warranted when the target exceeds the slit size, as this situation could allow an overestimation of ``global\" metallicity, especially under the presence of strong negative metallicity gradient.","sentences":["We present deep JWST NIRSpec observations in the sightline of MACS J1149.5+2223, a massive cluster of galaxies at $z=0.54$. We report the spectroscopic redshift of 28 sources at $3<z<9.1$, including 9 sources with the detection of the [OIII]4363 auroral line.","Combining these with 16 [OIII]4363-detected sources from publicly available JWST data, our sample consists of 25 galaxies with robust gas-phase metallicity measurements via the direct method.","We observe a positive correlation between stellar mass and metallicity, with a $\\sim0.5$\\,dex offset down below the local relation.","Interestingly, we find a larger than expected scatter of $\\sim0.3$\\,dex around the relation, which cannot be explained by redshift evolution among our sample or other third parameter.","The scatter increases at higher redshift, and we attribute this to the enrichment process having higher stochasticity due to shallower potential wells, more intense feedback processes, and a higher galaxy merger rate.","Despite reaching to a considerably low-mass regime ($\\log M_*/M_\\odot \\sim7.3$), our samples have metallicity of $\\log$(O/H)$+12>7$, i.e. comparable to the most metal poor galaxies in the local Universe.","The search of primordial galaxies may be accomplished by extending toward a lower mass and/or by investigating inhomogeneities at smaller spatial scales.","Lastly, we investigate potential systematics caused by the limitation of JWST's MSA observations.","Caution is warranted when the target exceeds the slit size, as this situation could allow an overestimation of ``global\" metallicity, especially under the presence of strong negative metallicity gradient."],"url":"http://arxiv.org/abs/2402.14084v1","category":"astro-ph.GA"}
{"created":"2024-02-21 19:11:33","title":"Landau damping, collisionless limit, and stability threshold for the Vlasov-Poisson equation with nonlinear Fokker-Planck collisions","abstract":"In this paper, we study the Vlasov-Poisson-Fokker-Planck (VPFP) equation with a small collision frequency $0 < \\nu \\ll 1$, exploring the interplay between the regularity and size of perturbations in the context of the asymptotic stability of the global Maxwellian. Our main result establishes the Landau damping and enhanced dissipation phenomena under the condition that the perturbation of the global Maxwellian falls within the Gevrey-$\\frac{1}{s}$ class and obtain that the stability threshold for the Gevrey-$\\frac{1}{s}$ class with $s>s_{\\mathrm{k}}$ can not be larger than $\\gamma=\\frac{1-3s_{\\mathrm{k}}}{3-3s_{\\mathrm{k}}}$ for $s_{\\mathrm{k}}\\in [0,\\frac{1}{3}]$. Moreover, we show that for Gevrey-$\\frac{1}{s}$ with $s>3$, and for $t\\ll \\nu^{\\frac13}$, the solution to VPFP converges to the solution to Vlasov-Poisson equation without collision.","sentences":["In this paper, we study the Vlasov-Poisson-Fokker-Planck (VPFP) equation with a small collision frequency $0 <","\\nu \\ll 1$, exploring the interplay between the regularity and size of perturbations in the context of the asymptotic stability of the global Maxwellian.","Our main result establishes the Landau damping and enhanced dissipation phenomena under the condition that the perturbation of the global Maxwellian falls within the Gevrey-$\\frac{1}{s}$ class and obtain that the stability threshold for the Gevrey-$\\frac{1}{s}$ class with $s>s_{\\mathrm{k}}$ can not be larger than $\\gamma=\\frac{1-3s_{\\mathrm{k}}}{3-3s_{\\mathrm{k}}}$ for $s_{\\mathrm{k}}\\in [0,\\frac{1}{3}]$.","Moreover, we show that for Gevrey-$\\frac{1}{s}$ with $s>3$, and for $t\\ll \\nu^{\\frac13}$, the solution to VPFP converges to the solution to Vlasov-Poisson equation without collision."],"url":"http://arxiv.org/abs/2402.14082v1","category":"math.AP"}
{"created":"2024-02-21 19:09:49","title":"Primordial magnetic fields: consistent initial conditions and impact on high-z structures","abstract":"Primordial magnetic fields (PMFs) can enhance matter power spectrum on small scales ($\\lesssim$ Mpc) and still agree with bounds from cosmic microwave background (CMB) and Faraday rotation measurements. As modes on scales smaller than Mpc have already become non-linear today, exploring PMFs' impact on small-scale structures requires dedicated cosmological simulations. Here, for the first time, we perform a suite of hydrodynamical simulations that take into account the different impacts of PMFs on baryons and dark matter. Specifically, in the initial conditions we displace particles according to the Lorentz force from PMFs. We also highlight the large theoretical uncertainty in the peak enhancement of the matter power spectrum due to PMFs, which was not considered in previous studies. We present halo mass functions and show that they can be accurately reproduced using Sheth-Torman formalism. Moreover, we show that PMFs can generate galaxies with baryon fraction several times larger than the cosmic average at high redshifts. This is simply a consequence of the fact that PMFs enhance baryon perturbations, causing them to be larger than dark matter perturbations. We argue that this scenario could be tested soon by obtaining accurate estimates of the baryon fraction in high redshift galaxies.","sentences":["Primordial magnetic fields (PMFs) can enhance matter power spectrum on small scales ($\\lesssim$ Mpc) and still agree with bounds from cosmic microwave background (CMB) and Faraday rotation measurements.","As modes on scales smaller than Mpc have already become non-linear today, exploring PMFs' impact on small-scale structures requires dedicated cosmological simulations.","Here, for the first time, we perform a suite of hydrodynamical simulations that take into account the different impacts of PMFs on baryons and dark matter.","Specifically, in the initial conditions we displace particles according to the Lorentz force from PMFs.","We also highlight the large theoretical uncertainty in the peak enhancement of the matter power spectrum due to PMFs, which was not considered in previous studies.","We present halo mass functions and show that they can be accurately reproduced using Sheth-Torman formalism.","Moreover, we show that PMFs can generate galaxies with baryon fraction several times larger than the cosmic average at high redshifts.","This is simply a consequence of the fact that PMFs enhance baryon perturbations, causing them to be larger than dark matter perturbations.","We argue that this scenario could be tested soon by obtaining accurate estimates of the baryon fraction in high redshift galaxies."],"url":"http://arxiv.org/abs/2402.14079v1","category":"astro-ph.CO"}
{"created":"2024-02-21 19:01:02","title":"Fleeting but not Forgotten: the Imprint of Escaping Hydrogen Atmospheres on Super-Earth Interiors","abstract":"Small, close-in exoplanets are divided into two sub-populations: super-Earths and sub-Neptunes. Most super-Earths are thought to have lost their primordially accreted hydrogen-dominated atmospheres via thermally driven winds. We consider the global chemical equilibrium of super-Earths and the lasting impacts of their fleeting hydrogen atmospheres. We find that hydrogen is efficiently sequestered into the interior, oxidising iron and endogenously producing $\\sim0.5-1.0\\%$ water by mass. As the atmospheres of super-Earths are continuously sculpted by mass loss and chemical equilibration, they remain hydrogen-dominated by mole (number) fraction but become steam-dominated by mass, which may be observable with \\textit{JWST} for planets transitioning across the radius valley. One of the main effects of efficient sequestration of hydrogen into the interior is to produce an under-dense bulk interior compared to that of Earth. We predict bulk densities of super-Earths to be $\\sim 5.0 \\text{ g cm}^{-3}$ for a $1M_\\oplus$ planet, which is consistent with high-precision mass measurements and also population-level inference analyses from atmospheric escape models.","sentences":["Small, close-in exoplanets are divided into two sub-populations: super-Earths and sub-Neptunes.","Most super-Earths are thought to have lost their primordially accreted hydrogen-dominated atmospheres via thermally driven winds.","We consider the global chemical equilibrium of super-Earths and the lasting impacts of their fleeting hydrogen atmospheres.","We find that hydrogen is efficiently sequestered into the interior, oxidising iron and endogenously producing $\\sim0.5-1.0\\%$ water by mass.","As the atmospheres of super-Earths are continuously sculpted by mass loss and chemical equilibration, they remain hydrogen-dominated by mole (number) fraction but become steam-dominated by mass, which may be observable with \\textit{JWST} for planets transitioning across the radius valley.","One of the main effects of efficient sequestration of hydrogen into the interior is to produce an under-dense bulk interior compared to that of Earth.","We predict bulk densities of super-Earths to be $\\sim 5.0 \\text{ g cm}^{-3}$ for a $1M_\\oplus$ planet, which is consistent with high-precision mass measurements and also population-level inference analyses from atmospheric escape models."],"url":"http://arxiv.org/abs/2402.14072v1","category":"astro-ph.EP"}
{"created":"2024-02-21 19:00:20","title":"New Mass Window for Primordial Black Holes as Dark Matter from Memory Burden Effect","abstract":"The mass ranges allowed for Primordial Black Holes (PBHs) to constitute all of Dark Matter (DM) are broadly constrained. However, these constraints rely on the standard semiclassical approximation which assumes that the evaporation process is self-similar. Quantum effects such as memory burden take the evaporation process out of the semiclassical regime latest by half-decay time. What happens beyond this time is currently not known. However, theoretical evidence based on prototype models indicates that the evaporation slows down thereby extending the lifetime of a black hole. This modifies the mass ranges constrained, in particular, by BBN and CMB spectral distortions. We show that previous constraints are largely relaxed when the PBH lifetime is extended, making it possible for PBHs to constitute all of DM in previously excluded mass ranges. In particular, this is the case for PBHs lighter than $10^9$g which enter the memory burden stage before BBN and are still present today as DM.","sentences":["The mass ranges allowed for Primordial Black Holes (PBHs) to constitute all of Dark Matter (DM) are broadly constrained.","However, these constraints rely on the standard semiclassical approximation which assumes that the evaporation process is self-similar.","Quantum effects such as memory burden take the evaporation process out of the semiclassical regime latest by half-decay time.","What happens beyond this time is currently not known.","However, theoretical evidence based on prototype models indicates that the evaporation slows down thereby extending the lifetime of a black hole.","This modifies the mass ranges constrained, in particular, by BBN and CMB spectral distortions.","We show that previous constraints are largely relaxed when the PBH lifetime is extended, making it possible for PBHs to constitute all of DM in previously excluded mass ranges.","In particular, this is the case for PBHs lighter than $10^9$g which enter the memory burden stage before BBN and are still present today as DM."],"url":"http://arxiv.org/abs/2402.14069v1","category":"hep-ph"}
{"created":"2024-02-21 19:00:18","title":"The Peculiar Bursting Nature of CP Pup","abstract":"The classical nova CP Puppis has been observed to have particularly puzzling and peculiar properties. In particular, this classical nova displays occasional bursts in its long-term ASAS-SN light curve. Here we report on 5 sectors of TESS data displaying 2 of these rapid bursts, lasting ~1 day. Based on the estimated lower energy limits of the bursts we discuss whether the bursts may be examples of micronovae resulting from localised thermonuclear explosion. Furthermore, its orbital period remains uncertain, with several inconsistent periodic signals appearing in spectroscopic and photometric observations at various wavelengths. Although we cannot unambiguously unravel the physical origin of the signals, the previously suggested nature of CP Puppis as a long orbital period system may be a viable explanation. The recurrence time of the bursts in CP Puppis, together with the unexplained variable modulations make it a prime candidate for intense monitoring.","sentences":["The classical nova CP Puppis has been observed to have particularly puzzling and peculiar properties.","In particular, this classical nova displays occasional bursts in its long-term ASAS-SN light curve.","Here we report on 5 sectors of TESS data displaying 2 of these rapid bursts, lasting ~1 day.","Based on the estimated lower energy limits of the bursts we discuss whether the bursts may be examples of micronovae resulting from localised thermonuclear explosion.","Furthermore, its orbital period remains uncertain, with several inconsistent periodic signals appearing in spectroscopic and photometric observations at various wavelengths.","Although we cannot unambiguously unravel the physical origin of the signals, the previously suggested nature of CP Puppis as a long orbital period system may be a viable explanation.","The recurrence time of the bursts in CP Puppis, together with the unexplained variable modulations make it a prime candidate for intense monitoring."],"url":"http://arxiv.org/abs/2402.14068v1","category":"astro-ph.HE"}
{"created":"2024-02-21 19:00:01","title":"GeV ALP from TeV Vector-like Leptons","abstract":"The generation of a mass for an axion-like-particle is a long-standing open issue. We propose a model where a GeV mass for this pseudo-scalar particle is predicted in a large portion of the parameter space due to the presence of explicit Peccei-Quinn symmetry-breaking terms in an exotic leptonic sector. The latter provides a solution to the muon $g-2$ anomaly, within the framework of the Linear Seesaw neutrino mechanism. The spectrum is extended by a complex scalar singlet only transforming under the Peccei-Quinn symmetry, which generates the axion-like-particle. Its couplings with fermions can continuously span over many orders of magnitude, which constitutes a specific feature of this model in contrast to generic ultraviolet constructions.","sentences":["The generation of a mass for an axion-like-particle is a long-standing open issue.","We propose a model where a GeV mass for this pseudo-scalar particle is predicted in a large portion of the parameter space due to the presence of explicit Peccei-Quinn symmetry-breaking terms in an exotic leptonic sector.","The latter provides a solution to the muon $g-2$ anomaly, within the framework of the Linear Seesaw neutrino mechanism.","The spectrum is extended by a complex scalar singlet only transforming under the Peccei-Quinn symmetry, which generates the axion-like-particle.","Its couplings with fermions can continuously span over many orders of magnitude, which constitutes a specific feature of this model in contrast to generic ultraviolet constructions."],"url":"http://arxiv.org/abs/2402.14059v1","category":"hep-ph"}
{"created":"2024-02-21 19:00:00","title":"Perturbative reheating and thermalization of pure Yang-Mills plasma","abstract":"We investigate the thermalization of high-energy particles injected from the perturbative decay of inflaton during the pre-thermal phase of reheating in detail. In general, thermalization takes a relatively long time in a low-temperature plasma; therefore, the instantaneous thermalization approximation is not justified, even for the reheating of the Standard Model (SM) sector. We consider a pure Yang-Mills (YM) theory as an approximation of the SM sector or a possible dark sector, considering the Landau-Pomeranchuk-Migdal effect, a quantum interference effect in a finite temperature plasma. We perform the first numerical calculation to solve the time evolution of the system, including the redshift due to the expansion of the Universe, and show the details of the temperature evolution near the maximum and the behavior of the quasi-attractors at later times. The maximal temperature $T_\\text{max}$ and time scale $t_\\text{max}$ are determined quantitatively, such as $T_\\text{max} \\simeq 0.05 \\times (\\Gamma_I M_\\text{Pl}^2/m_I^3)^{2/5} m_I$ and $t_\\text{max} \\simeq 2 \\times 10^3 \\times (\\Gamma_I M_\\text{Pl}^2/m_I^3)^{-3/5} m_I^{-1}$ in the SM-like system, where $m_I$ and $\\Gamma_I$ are the mass and decay rate of inflaton. We also provide a similar formula for pure $\\operatorname*{SU}(N)$ and $\\operatorname*{SO}(N)$ YM theories for general values of $N$ and coupling constant $\\alpha$, including $T_\\text{max} \\propto \\alpha^{4/5}$ and $t_\\text{max} \\propto N^{-2} \\alpha^{-16/5}$ behaviors and their numerical coefficients. The thermalization occurs in a finite time scale, resulting in a lower maximal temperature of the Universe after inflation than that under the instantaneous thermalization approximation.","sentences":["We investigate the thermalization of high-energy particles injected from the perturbative decay of inflaton during the pre-thermal phase of reheating in detail.","In general, thermalization takes a relatively long time in a low-temperature plasma; therefore, the instantaneous thermalization approximation is not justified, even for the reheating of the Standard Model (SM) sector.","We consider a pure Yang-Mills (YM) theory as an approximation of the SM sector or a possible dark sector, considering the Landau-Pomeranchuk-Migdal effect, a quantum interference effect in a finite temperature plasma.","We perform the first numerical calculation to solve the time evolution of the system, including the redshift due to the expansion of the Universe, and show the details of the temperature evolution near the maximum and the behavior of the quasi-attractors at later times.","The maximal temperature $T_\\text{max}$ and time scale $t_\\text{max}$ are determined quantitatively, such as $T_\\text{max} \\simeq 0.05 \\times (\\Gamma_I M_\\text{Pl}^2/m_I^3)^{2/5} m_I$ and $t_\\text{max} \\simeq 2 \\times 10^3","\\times (\\Gamma_I M_\\text{Pl}^2/m_I^3)^{-3/5} m_I^{-1}$ in the SM-like system, where $m_I$ and $\\Gamma_I$ are the mass and decay rate of inflaton.","We also provide a similar formula for pure $\\operatorname*{SU}(N)$ and $\\operatorname*{SO}(N)$ YM theories for general values of $N$ and coupling constant $\\alpha$, including $T_\\text{max} \\propto \\alpha^{4/5}$ and $t_\\text{max} \\propto N^{-2} \\alpha^{-16/5}$ behaviors and their numerical coefficients.","The thermalization occurs in a finite time scale, resulting in a lower maximal temperature of the Universe after inflation than that under the instantaneous thermalization approximation."],"url":"http://arxiv.org/abs/2402.14054v1","category":"hep-ph"}
{"created":"2024-02-21 19:00:00","title":"Taking the pulse of the outer Milky Way with HOWVAST: an RR Lyrae density profile out to $>$200 kpc","abstract":"In order to constrain the evolutionary history of the Milky Way, we hunt for faint RR Lyrae stars (RRLs) using Dark Energy Camera data from the High cadence Transient Survey (HiTS) and the Halo Outskirts With Variable Stars (HOWVAST) survey. We report the detection of $\\sim500$ RRLs, including previously identified stars and $\\sim90$ RRLs not yet reported. We identify 9 new RRLs beyond $100$ kpc from the Sun, most of which are classified as fundamental-mode pulsators. The periods and amplitudes of the distant RRLs do not place them in either one of the two classical Oosterhoff groups, but in the Oosterhoff intermediate region. We detect two groups of clumped distant RRLs with similar distances and equatorial coordinates, which we interpret as an indication of their association with undiscovered bound or unbound satellites. We study the halo density profile using spheroidal and ellipsoidal ($q=0.7$) models, following a Markov chain Monte Carlo methodology. For a spheroidal halo, our derived radial profile is consistent with a broken power-law with a break at $18.1^{+2.1}_{-1.1}$ kpc separating the inner and the outer halo, and an outer slope of $-4.47^{+0.11}_{-0.18}$. For an ellipsoidal halo, the break is located at $24.3^{+2.6}_{-3.2}$ kpc and the outer slope is $-4.57^{+0.17}_{-0.25}$. The break in the density profile is a feature visible in different directions of the halo. The similarity of these radial distributions with previous values reported in the literature seems to depend on the regions of the sky surveyed (direction and total area) and halo tracer used. Our findings are compatible with simulations and observations that predict that the outer regions of Milky Way-like galaxies are mainly composed of accreted material.","sentences":["In order to constrain the evolutionary history of the Milky Way, we hunt for faint RR Lyrae stars (RRLs) using Dark Energy Camera data from the High cadence Transient Survey (HiTS) and the Halo Outskirts With Variable Stars (HOWVAST) survey.","We report the detection of $\\sim500$ RRLs, including previously identified stars and $\\sim90$ RRLs not yet reported.","We identify 9 new RRLs beyond $100$ kpc from the Sun, most of which are classified as fundamental-mode pulsators.","The periods and amplitudes of the distant RRLs do not place them in either one of the two classical Oosterhoff groups, but in the Oosterhoff intermediate region.","We detect two groups of clumped distant RRLs with similar distances and equatorial coordinates, which we interpret as an indication of their association with undiscovered bound or unbound satellites.","We study the halo density profile using spheroidal and ellipsoidal ($q=0.7$) models, following a Markov chain Monte Carlo methodology.","For a spheroidal halo, our derived radial profile is consistent with a broken power-law with a break at $18.1^{+2.1}_{-1.1}$ kpc separating the inner and the outer halo, and an outer slope of $-4.47^{+0.11}_{-0.18}$. For an ellipsoidal halo, the break is located at $24.3^{+2.6}_{-3.2}$ kpc and the outer slope is $-4.57^{+0.17}_{-0.25}$.","The break in the density profile is a feature visible in different directions of the halo.","The similarity of these radial distributions with previous values reported in the literature seems to depend on the regions of the sky surveyed (direction and total area) and halo tracer used.","Our findings are compatible with simulations and observations that predict that the outer regions of Milky Way-like galaxies are mainly composed of accreted material."],"url":"http://arxiv.org/abs/2402.14055v1","category":"astro-ph.GA"}
{"created":"2024-02-21 18:59:13","title":"Coercing LLMs to do and reveal (almost) anything","abstract":"It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons.","sentences":["It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements.","In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking.","We provide a broad overview of possible attack surfaces and attack goals.","Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   ","We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons."],"url":"http://arxiv.org/abs/2402.14020v1","category":"cs.LG"}
{"created":"2024-02-22 18:56:37","title":"Pseudorandom unitaries with non-adaptive security","abstract":"Pseudorandom unitaries (PRUs) are ensembles of efficiently implementable unitary operators that cannot be distinguished from Haar random unitaries by any quantum polynomial-time algorithm with query access to the unitary. We present a simple PRU construction that is a concatenation of a random Clifford unitary, a pseudorandom binary phase operator, and a pseudorandom permutation operator. We prove that this PRU construction is secure against non-adaptive distinguishers assuming the existence of quantum-secure one-way functions. This means that no efficient quantum query algorithm that is allowed a single application of $U^{\\otimes \\mathrm{poly}(n)}$ can distinguish whether an $n$-qubit unitary $U$ was drawn from the Haar measure or our PRU ensemble. We conjecture that our PRU construction remains secure against adaptive distinguishers, i.e. secure against distinguishers that can query the unitary polynomially many times in sequence, not just in parallel.","sentences":["Pseudorandom unitaries (PRUs) are ensembles of efficiently implementable unitary operators that cannot be distinguished from Haar random unitaries by any quantum polynomial-time algorithm with query access to the unitary.","We present a simple PRU construction that is a concatenation of a random Clifford unitary, a pseudorandom binary phase operator, and a pseudorandom permutation operator.","We prove that this PRU construction is secure against non-adaptive distinguishers assuming the existence of quantum-secure one-way functions.","This means that no efficient quantum query algorithm that is allowed a single application of $U^{\\otimes \\mathrm{poly}(n)}$ can distinguish whether an $n$-qubit unitary $U$ was drawn from the Haar measure or our PRU ensemble.","We conjecture that our PRU construction remains secure against adaptive distinguishers, i.e. secure against distinguishers that can query the unitary polynomially many times in sequence, not just in parallel."],"url":"http://arxiv.org/abs/2402.14803v1","category":"quant-ph"}
{"created":"2024-02-22 18:35:05","title":"2D Matryoshka Sentence Embeddings","abstract":"Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feasible. In this paper, we introduce a novel sentence embedding model called Two-dimensional Matryoshka Sentence Embedding (2DMSE). It supports elastic settings for both embedding sizes and Transformer layers, offering greater flexibility and efficiency than MRL. We conduct extensive experiments on STS tasks and downstream applications. The experimental results demonstrate the effectiveness of our proposed model in dynamically supporting different embedding sizes and Transformer layers, allowing it to be highly adaptable to various scenarios.","sentences":["Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS).","Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications.","Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks.","Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks.","Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption.","This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feasible.","In this paper, we introduce a novel sentence embedding model called Two-dimensional Matryoshka Sentence Embedding (2DMSE).","It supports elastic settings for both embedding sizes and Transformer layers, offering greater flexibility and efficiency than MRL.","We conduct extensive experiments on STS tasks and downstream applications.","The experimental results demonstrate the effectiveness of our proposed model in dynamically supporting different embedding sizes and Transformer layers, allowing it to be highly adaptable to various scenarios."],"url":"http://arxiv.org/abs/2402.14776v1","category":"cs.CL"}
{"created":"2024-02-22 18:26:02","title":"DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models","abstract":"We present DualFocus, a novel framework for integrating macro and micro perspectives within multi-modal large language models (MLLMs) to enhance vision-language task performance. Current MLLMs typically singularly focus on inputs at a predefined resolution, resulting in deficiencies in detailed questions involving local regions. We introduced a DualFocus mechanism where the model concentrates on the image from a macro perspective, responses to the question, and identifies suitable sub-regions to zoom in for subsequent micro perspective analysis. Via the integration of answers from both macro and micro perspectives, the model is adept at addressing tasks that encompass global, detailed, and combined considerations. To endows the DualFocus mechanism in MLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and adapted it to align with the training regimen of DualFocus. Through comparative studies across different model sizes and benchmarks, we demonstrate DualFocus's superiority in balancing detailed examination with holistic insight, significantly reducing hallucination instances in MLLMs and improving their performance in various vision-language tasks.","sentences":["We present DualFocus, a novel framework for integrating macro and micro perspectives within multi-modal large language models (MLLMs) to enhance vision-language task performance.","Current MLLMs typically singularly focus on inputs at a predefined resolution, resulting in deficiencies in detailed questions involving local regions.","We introduced a DualFocus mechanism where the model concentrates on the image from a macro perspective, responses to the question, and identifies suitable sub-regions to zoom in for subsequent micro perspective analysis.","Via the integration of answers from both macro and micro perspectives, the model is adept at addressing tasks that encompass global, detailed, and combined considerations.","To endows the DualFocus mechanism in MLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and adapted it to align with the training regimen of DualFocus.","Through comparative studies across different model sizes and benchmarks, we demonstrate DualFocus's superiority in balancing detailed examination with holistic insight, significantly reducing hallucination instances in MLLMs and improving their performance in various vision-language tasks."],"url":"http://arxiv.org/abs/2402.14767v1","category":"cs.CV"}
{"created":"2024-02-22 17:52:34","title":"Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs","abstract":"AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \\textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \\textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed \"RL-free\" methods such as DPO and RAFT. Our work suggests that careful adaptation to LLMs alignment characteristics enables benefiting from online RL optimization at low cost.","sentences":["AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models.","\\textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF.","However, it involves both high computational cost and sensitive hyperparameter tuning.","We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance.","We revisit the \\textit{formulation} of alignment from human preferences in the context of RL.","Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed \"RL-free\" methods such as DPO and RAFT.","Our work suggests that careful adaptation to LLMs alignment characteristics enables benefiting from online RL optimization at low cost."],"url":"http://arxiv.org/abs/2402.14740v1","category":"cs.LG"}
{"created":"2024-02-22 16:11:00","title":"ConPA: A Contention-free Mechanism with Power Adaptation for Beyond Listen-Before-Talk","abstract":"In view of the need to find novel means to utilize the unlicensed spectrum to meet the rising latency and reliability requirements of new applications, we propose a novel mechanism that allows devices to transmit anytime that a packet has to be delivered. The proposed mechanism, Contention-free with Power Adaptation (ConPA), aims to bypass the contention periods of current Listen-Before-Talk (LBT) approaches, which are the main source of unreliability in unlicensed technologies like Wi-Fi. To assess the feasibility of ConPA, we provide an analytical method based on Markov chains, which allows deriving relevant performance metrics, including throughput, airtime, and quality of transmissions. Using such a model, we study the performance of ConPA in various scenarios, and compare it to baseline channel access approaches like the Distributed Coordination Function (DCF) and the IEEE 802.11ax Overlapping Basic Service Set (OBSS) Packet Detect (PD)-based Spatial Reuse (SR). Our results prove the effectiveness of ConPA in reusing the space to offer substantial throughput gains with respect to the baselines (up to 76% improvement).","sentences":["In view of the need to find novel means to utilize the unlicensed spectrum to meet the rising latency and reliability requirements of new applications, we propose a novel mechanism that allows devices to transmit anytime that a packet has to be delivered.","The proposed mechanism, Contention-free with Power Adaptation (ConPA), aims to bypass the contention periods of current Listen-Before-Talk (LBT) approaches, which are the main source of unreliability in unlicensed technologies like Wi-Fi.","To assess the feasibility of ConPA, we provide an analytical method based on Markov chains, which allows deriving relevant performance metrics, including throughput, airtime, and quality of transmissions.","Using such a model, we study the performance of ConPA in various scenarios, and compare it to baseline channel access approaches like the Distributed Coordination Function (DCF) and the IEEE 802.11ax","Overlapping Basic Service Set (OBSS) Packet Detect (PD)-based Spatial Reuse (SR).","Our results prove the effectiveness of ConPA in reusing the space to offer substantial throughput gains with respect to the baselines (up to 76% improvement)."],"url":"http://arxiv.org/abs/2402.14667v1","category":"cs.NI"}
{"created":"2024-02-22 15:45:31","title":"CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations","abstract":"This work introduces reduced models based on Continuous Low Rank Adaptation (CoLoRA) that pre-train neural networks for a given partial differential equation and then continuously adapt low-rank weights in time to rapidly predict the evolution of solution fields at new physics parameters and new initial conditions. The adaptation can be either purely data-driven or via an equation-driven variational approach that provides Galerkin-optimal approximations. Because CoLoRA approximates solution fields locally in time, the rank of the weights can be kept small, which means that only few training trajectories are required offline so that CoLoRA is well suited for data-scarce regimes. Predictions with CoLoRA are orders of magnitude faster than with classical methods and their accuracy and parameter efficiency is higher compared to other neural network approaches.","sentences":["This work introduces reduced models based on Continuous Low Rank Adaptation (CoLoRA) that pre-train neural networks for a given partial differential equation and then continuously adapt low-rank weights in time to rapidly predict the evolution of solution fields at new physics parameters and new initial conditions.","The adaptation can be either purely data-driven or via an equation-driven variational approach that provides Galerkin-optimal approximations.","Because CoLoRA approximates solution fields locally in time, the rank of the weights can be kept small, which means that only few training trajectories are required offline so that CoLoRA is well suited for data-scarce regimes.","Predictions with CoLoRA are orders of magnitude faster than with classical methods and their accuracy and parameter efficiency is higher compared to other neural network approaches."],"url":"http://arxiv.org/abs/2402.14646v1","category":"cs.LG"}
{"created":"2024-02-22 15:39:29","title":"Object permanence in newborn chicks is robust against opposing evidence","abstract":"Newborn animals have advanced perceptual skills at birth, but the nature of this initial knowledge is unknown. Is initial knowledge flexible, continuously adapting to the statistics of experience? Or can initial knowledge be rigid and robust to change, even in the face of opposing evidence? We address this question through controlled-rearing experiments on newborn chicks. First, we reared chicks in an impoverished virtual world, where objects never occluded one another, and found that chicks still succeed on object permanence tasks. Second, we reared chicks in a virtual world in which objects teleported from one location to another while out of view: an unnatural event that violates the continuity of object motion. Despite seeing thousands of these violations of object permanence, and not a single non-violation, the chicks behaved as if object permanence were true, exhibiting the same behavior as chicks reared with natural object permanence events. We conclude that object permanence develops prenatally and is robust to change from opposing evidence.","sentences":["Newborn animals have advanced perceptual skills at birth, but the nature of this initial knowledge is unknown.","Is initial knowledge flexible, continuously adapting to the statistics of experience?","Or can initial knowledge be rigid and robust to change, even in the face of opposing evidence?","We address this question through controlled-rearing experiments on newborn chicks.","First, we reared chicks in an impoverished virtual world, where objects never occluded one another, and found that chicks still succeed on object permanence tasks.","Second, we reared chicks in a virtual world in which objects teleported from one location to another while out of view: an unnatural event that violates the continuity of object motion.","Despite seeing thousands of these violations of object permanence, and not a single non-violation, the chicks behaved as if object permanence were true, exhibiting the same behavior as chicks reared with natural object permanence events.","We conclude that object permanence develops prenatally and is robust to change from opposing evidence."],"url":"http://arxiv.org/abs/2402.14641v1","category":"q-bio.NC"}
{"created":"2024-02-22 15:02:13","title":"Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation","abstract":"Self-supervised learning (SSL) approaches have achieved great success when the amount of labeled data is limited. Within SSL, models learn robust feature representations by solving pretext tasks. One such pretext task is contrastive learning, which involves forming pairs of similar and dissimilar input samples, guiding the model to distinguish between them. In this work, we investigate the application of contrastive learning to the domain of medical image analysis. Our findings reveal that MoCo v2, a state-of-the-art contrastive learning method, encounters dimensional collapse when applied to medical images. This is attributed to the high degree of inter-image similarity shared between the medical images. To address this, we propose two key contributions: local feature learning and feature decorrelation. Local feature learning improves the ability of the model to focus on the local regions of the image, while feature decorrelation removes the linear dependence among the features. Our experimental findings demonstrate that our contributions significantly enhance the model's performance in the downstream task of medical segmentation, both in the linear evaluation and full fine-tuning settings. This work illustrates the importance of effectively adapting SSL techniques to the characteristics of medical imaging tasks.","sentences":["Self-supervised learning (SSL) approaches have achieved great success when the amount of labeled data is limited.","Within SSL, models learn robust feature representations by solving pretext tasks.","One such pretext task is contrastive learning, which involves forming pairs of similar and dissimilar input samples, guiding the model to distinguish between them.","In this work, we investigate the application of contrastive learning to the domain of medical image analysis.","Our findings reveal that MoCo v2, a state-of-the-art contrastive learning method, encounters dimensional collapse when applied to medical images.","This is attributed to the high degree of inter-image similarity shared between the medical images.","To address this, we propose two key contributions: local feature learning and feature decorrelation.","Local feature learning improves the ability of the model to focus on the local regions of the image, while feature decorrelation removes the linear dependence among the features.","Our experimental findings demonstrate that our contributions significantly enhance the model's performance in the downstream task of medical segmentation, both in the linear evaluation and full fine-tuning settings.","This work illustrates the importance of effectively adapting SSL techniques to the characteristics of medical imaging tasks."],"url":"http://arxiv.org/abs/2402.14611v1","category":"cs.CV"}
{"created":"2024-02-22 13:13:31","title":"Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond","abstract":"Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-model scenarios, whilst maintaining their performance to be comparable to architecture-specific methods.","sentences":["Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability.","However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner.","Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs.","To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space.","Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-model scenarios, whilst maintaining their performance to be comparable to architecture-specific methods."],"url":"http://arxiv.org/abs/2402.14522v1","category":"cs.CL"}
{"created":"2024-02-22 09:34:48","title":"Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks","abstract":"Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.","sentences":["Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions.","Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment.","However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures.","Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns.","We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age.","We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features."],"url":"http://arxiv.org/abs/2402.14400v1","category":"cs.CV"}
{"created":"2024-02-22 07:53:34","title":"GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints","abstract":"Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes.The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth.","sentences":["Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss.","While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries.","To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints.","The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes.","The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model.","Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation.","Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth."],"url":"http://arxiv.org/abs/2402.14354v1","category":"cs.CV"}
{"created":"2024-02-22 05:41:24","title":"GenSERP: Large Language Models for Whole Page Presentation","abstract":"The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query. Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result. (2) An answer generation phase where the LLM populates the layouts with the retrieved content. In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the page to each item, along with the UX display details. (3) A scoring phase where an LLM with vision scores all the generated SERPs based on how likely it can satisfy the user. It then send the one with highest score to rendering. GenSERP features two generation paradigms. First, coarse-to-fine, which allow it to approach optimal layout in a more manageable way, (2) beam search, which give it a better chance to hit the optimal solution compared to greedy decoding. Offline experimental results on real-world data demonstrate how LLMs can contextually organize heterogeneous search results on-the-fly and provide a promising user experience.","sentences":["The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization.","In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query.","Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result.","(2) An answer generation phase where the LLM populates the layouts with the retrieved content.","In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP.","Consequently, it assigns a location on the page to each item, along with the UX display details.","(3) A scoring phase where an LLM with vision scores all the generated SERPs based on how likely it can satisfy the user.","It then send the one with highest score to rendering.","GenSERP features two generation paradigms.","First, coarse-to-fine, which allow it to approach optimal layout in a more manageable way, (2) beam search, which give it a better chance to hit the optimal solution compared to greedy decoding.","Offline experimental results on real-world data demonstrate how LLMs can contextually organize heterogeneous search results on-the-fly and provide a promising user experience."],"url":"http://arxiv.org/abs/2402.14301v1","category":"cs.IR"}
{"created":"2024-02-22 05:34:22","title":"A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation","abstract":"Conventional deep learning models deal with images one-by-one, requiring costly and time-consuming expert labeling in the field of medical imaging, and domain-specific restriction limits model generalizability. Visual in-context learning (ICL) is a new and exciting area of research in computer vision. Unlike conventional deep learning, ICL emphasizes the model's ability to adapt to new tasks based on given examples quickly. Inspired by MAE-VQGAN, we proposed a new simple visual ICL method called SimICL, combining visual ICL pairing images with masked image modeling (MIM) designed for self-supervised learning. We validated our method on bony structures segmentation in a wrist ultrasound (US) dataset with limited annotations, where the clinical objective was to segment bony structures to help with further fracture detection. We used a test set containing 3822 images from 18 patients for bony region segmentation. SimICL achieved an remarkably high Dice coeffient (DC) of 0.96 and Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and visual ICL models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU increasing up to 0.10 and 0.16. This remarkably high agreement with limited manual annotations indicates SimICL could be used for training AI models even on small US datasets. This could dramatically decrease the human expert time required for image labeling compared to conventional approaches, and enhance the real-world use of AI assistance in US image analysis.","sentences":["Conventional deep learning models deal with images one-by-one, requiring costly and time-consuming expert labeling in the field of medical imaging, and domain-specific restriction limits model generalizability.","Visual in-context learning (ICL) is a new and exciting area of research in computer vision.","Unlike conventional deep learning, ICL emphasizes the model's ability to adapt to new tasks based on given examples quickly.","Inspired by MAE-VQGAN, we proposed a new simple visual ICL method called SimICL, combining visual ICL pairing images with masked image modeling (MIM) designed for self-supervised learning.","We validated our method on bony structures segmentation in a wrist ultrasound (US) dataset with limited annotations, where the clinical objective was to segment bony structures to help with further fracture detection.","We used a test set containing 3822 images from 18 patients for bony region segmentation.","SimICL achieved an remarkably high Dice coeffient (DC) of 0.96 and Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and visual ICL models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU increasing up to 0.10 and 0.16.","This remarkably high agreement with limited manual annotations indicates SimICL could be used for training AI models even on small US datasets.","This could dramatically decrease the human expert time required for image labeling compared to conventional approaches, and enhance the real-world use of AI assistance in US image analysis."],"url":"http://arxiv.org/abs/2402.14300v1","category":"cs.CV"}
{"created":"2024-02-21 23:45:57","title":"Linear Transformers are Versatile In-Context Learners","abstract":"Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.","sentences":["Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step.","However, their capability in handling more complex problems remains unexplored.","In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent.","We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise.","Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines.","We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels.","Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies."],"url":"http://arxiv.org/abs/2402.14180v1","category":"cs.LG"}
{"created":"2024-02-21 19:26:19","title":"The origin of the Slow-to-Alfv\u00e9n Wave Cascade Power Ratio and its Implications for Particle Heating in Accretion Flows","abstract":"The partition of turbulent heating between ions and electrons in radiatively inefficient accretion flows plays a crucial role in determining the observational appearance of accreting black holes. Modeling this partition is, however, a challenging problem because of the large scale separation between the macroscopic scales at which energy is injected by turbulence and the microscopic ones at which it is dissipated into heat. Recent studies of particle heating from collisionless damping of turbulent energy have shown that the partition of energy between ions and electrons is dictated by the ratio of the energy injected into the slow and Alfv\\'en wave cascades as well as the plasma $\\beta$ parameter. In this paper, we study the mechanism of the injection of turbulent energy into slow- and Alfv\\'en- wave cascades in magnetized shear flows. We show that this ratio depends on the particular ($r\\phi$) components of the Maxwell and Reynolds stress tensors that cause the transport of angular momentum, the shearing rate, and the orientation of the mean magnetic field with respect to the shear. We then use numerical magnetohydrodynamic shearing-box simulations with background conditions relevant to black hole accretion disks to compute the magnitudes of the stress tensors for turbulence driven by the magneto-rotational instability and derive the injection power ratio between slow and Alfv\\'en wave cascades. We use these results to formulate a local subgrid model for the ion-to-electron heating ratio that depends on the macroscopic characteristics of the accretion flow.","sentences":["The partition of turbulent heating between ions and electrons in radiatively inefficient accretion flows plays a crucial role in determining the observational appearance of accreting black holes.","Modeling this partition is, however, a challenging problem because of the large scale separation between the macroscopic scales at which energy is injected by turbulence and the microscopic ones at which it is dissipated into heat.","Recent studies of particle heating from collisionless damping of turbulent energy have shown that the partition of energy between ions and electrons is dictated by the ratio of the energy injected into the slow and Alfv\\'en wave cascades as well as the plasma $\\beta$ parameter.","In this paper, we study the mechanism of the injection of turbulent energy into slow- and Alfv\\'en- wave cascades in magnetized shear flows.","We show that this ratio depends on the particular ($r\\phi$) components of the Maxwell and Reynolds stress tensors that cause the transport of angular momentum, the shearing rate, and the orientation of the mean magnetic field with respect to the shear.","We then use numerical magnetohydrodynamic shearing-box simulations with background conditions relevant to black hole accretion disks to compute the magnitudes of the stress tensors for turbulence driven by the magneto-rotational instability and derive the injection power ratio between slow and Alfv\\'en wave cascades.","We use these results to formulate a local subgrid model for the ion-to-electron heating ratio that depends on the macroscopic characteristics of the accretion flow."],"url":"http://arxiv.org/abs/2402.14089v1","category":"astro-ph.HE"}
{"created":"2024-02-21 19:09:49","title":"A unified framework for the analysis of accuracy and stability of a class of approximate Gaussian filters for the Navier-Stokes Equations","abstract":"Bayesian state estimation of a dynamical system utilising a stream of noisy measurements is important in many geophysical and engineering applications. We establish rigorous results on (time-asymptotic) accuracy and stability of these algorithms with general covariance and observation operators. The accuracy and stability results for EnKF and EnSRKF for dissipative PDEs are, to the best of our knowledge, completely new in this general setting. It turns out that a hitherto unexploited cancellation property involving the ensemble covariance and observation operators and the concept of covariance localization in conjunction with covariance inflation play a pivotal role in the accuracy and stability for EnKF and EnSRKF. Our approach also elucidates the links, via determining functionals, between the approximate-Bayesian and control-theoretic approaches to data assimilation. We consider the `model' dynamics governed by the two-dimensional incompressible Navier-Stokes equations and observations given by noisy measurements of averaged volume elements or spectral/modal observations of the velocity field. In this setup, several continuous-time data assimilation techniques, namely the so-called 3DVar, EnKF and EnSRKF reduce to a stochastically forced Navier-Stokes equations. For the first time, we derive conditions for accuracy and stability of EnKF and EnSRKF. The derived bounds are given for the limit supremum of the expected value of the $L^2$ norm and of the $\\mathbb{H}^1$ Sobolev norm of the difference between the approximating solution and the actual solution as the time tends to infinity. Moreover, our analysis reveals an interplay between the resolution of the observations associated with the observation operator underlying the data assimilation algorithms and covariance inflation and localization which are employed in practice for improved filter performance.","sentences":["Bayesian state estimation of a dynamical system utilising a stream of noisy measurements is important in many geophysical and engineering applications.","We establish rigorous results on (time-asymptotic) accuracy and stability of these algorithms with general covariance and observation operators.","The accuracy and stability results for EnKF and EnSRKF for dissipative PDEs are, to the best of our knowledge, completely new in this general setting.","It turns out that a hitherto unexploited cancellation property involving the ensemble covariance and observation operators and the concept of covariance localization in conjunction with covariance inflation play a pivotal role in the accuracy and stability for EnKF and EnSRKF.","Our approach also elucidates the links, via determining functionals, between the approximate-Bayesian and control-theoretic approaches to data assimilation.","We consider the `model' dynamics governed by the two-dimensional incompressible Navier-Stokes equations and observations given by noisy measurements of averaged volume elements or spectral/modal observations of the velocity field.","In this setup, several continuous-time data assimilation techniques, namely the so-called 3DVar, EnKF and EnSRKF reduce to a stochastically forced Navier-Stokes equations.","For the first time, we derive conditions for accuracy and stability of EnKF and EnSRKF.","The derived bounds are given for the limit supremum of the expected value of the $L^2$ norm and of the $\\mathbb{H}^1$ Sobolev norm of the difference between the approximating solution and the actual solution as the time tends to infinity.","Moreover, our analysis reveals an interplay between the resolution of the observations associated with the observation operator underlying the data assimilation algorithms and covariance inflation and localization which are employed in practice for improved filter performance."],"url":"http://arxiv.org/abs/2402.14078v1","category":"math.AP"}
{"created":"2024-02-21 18:36:26","title":"Real-time 3D-aware Portrait Editing from a Single Image","abstract":"This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner. To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively. Such a design brings two compelling advantages over existing approaches. First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor. Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case). The code, the model, and the interface will be made publicly available to facilitate future research.","sentences":["This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner.","To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively.","Such a design brings two compelling advantages over existing approaches.","First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor.","Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case).","The code, the model, and the interface will be made publicly available to facilitate future research."],"url":"http://arxiv.org/abs/2402.14000v1","category":"cs.CV"}
{"created":"2024-02-21 18:21:21","title":"A General Theory of Static Response for Markov Jump Processes","abstract":"We consider Markov jump processes on a graph described by a rate matrix that depends on various control parameters. We derive explicit expressions for the static responses of edge currents and steady-state probabilities. We show that they are constrained by the graph topology (i.e. the incidence matrix) by deriving response relations (i.e. linear constraints linking the different responses) and topology-dependent bounds. For unicyclic networks, all scaled current sensitivities are between zero and one and must sum to one. Applying these results to stochastic thermodynamics, we derive explicit expressions for the static response of fundamental currents (which carry the full dissipation) to fundamental thermodynamic forces (which drive the system away from equilibrium).","sentences":["We consider Markov jump processes on a graph described by a rate matrix that depends on various control parameters.","We derive explicit expressions for the static responses of edge currents and steady-state probabilities.","We show that they are constrained by the graph topology (i.e. the incidence matrix) by deriving response relations (i.e. linear constraints linking the different responses) and topology-dependent bounds.","For unicyclic networks, all scaled current sensitivities are between zero and one and must sum to one.","Applying these results to stochastic thermodynamics, we derive explicit expressions for the static response of fundamental currents (which carry the full dissipation) to fundamental thermodynamic forces (which drive the system away from equilibrium)."],"url":"http://arxiv.org/abs/2402.13990v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-21 18:19:20","title":"FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning","abstract":"Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect. The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions. Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client. Extensive numerical experiments on both synthetic and real-world datasets are conducted. As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM.","sentences":["Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy.","The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources.","Recently developed FedADMM methods show great resilience to both data and system heterogeneity.","However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned.","To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa.","First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy.","This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect.","The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions.","Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client.","Extensive numerical experiments on both synthetic and real-world datasets are conducted.","As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM."],"url":"http://arxiv.org/abs/2402.13989v1","category":"cs.LG"}
{"created":"2024-02-21 18:03:38","title":"Non-Markovian maximal couplings and a vertical reflection principle on a class of sub-Riemannian manifolds","abstract":"We develop non-Markovian, non-co-adapted couplings for sub-Riemannian Brownian motions in various sub-Riemannian manifolds, namely the three-dimensional Heisenberg group, higher-dimensional non-isotropic Heisenberg groups, SL(2,R) and its universal cover, and SU(2,C). Our primary focus is on the situation when the processes start from two points on the same vertical fiber, since in general co-adapted couplings cannot give the sharp rate for the coupling time in this case. Then for general points, we use this vertical coupling as the second stage of a two-stage coupling. Non-Markovian couplings in this context were first introduced by Banerjee-Gordina-Mariano, for the three-dimensional Heisenberg group, and were more recently extended by B\\'en\\'efice to SL(2,R) and SU(2,C), using a detailed consideration of the Brownian bridge. In contrast, our couplings are based on global isometries of the space, giving couplings that are maximal, as well as making the construction relatively simple and uniform across different manifolds. Moreover, this construction gives a coupling time that equals a hitting time for the vertical component of one of the Brownian motions, and this vertical component satisfies a reflection principle, which is useful in explicitly bounding the tail probability of the coupling time. We estimate the coupling time in these various situations and give applications to inequalities for the heat semigroup.","sentences":["We develop non-Markovian, non-co-adapted couplings for sub-Riemannian Brownian motions in various sub-Riemannian manifolds, namely the three-dimensional Heisenberg group, higher-dimensional non-isotropic Heisenberg groups, SL(2,R) and its universal cover, and SU(2,C).","Our primary focus is on the situation when the processes start from two points on the same vertical fiber, since in general co-adapted couplings cannot give the sharp rate for the coupling time in this case.","Then for general points, we use this vertical coupling as the second stage of a two-stage coupling.","Non-Markovian couplings in this context were first introduced by Banerjee-Gordina-Mariano, for the three-dimensional Heisenberg group, and were more recently extended by B\\'en\\'efice to SL(2,R) and SU(2,C), using a detailed consideration of the Brownian bridge.","In contrast, our couplings are based on global isometries of the space, giving couplings that are maximal, as well as making the construction relatively simple and uniform across different manifolds.","Moreover, this construction gives a coupling time that equals a hitting time for the vertical component of one of the Brownian motions, and this vertical component satisfies a reflection principle, which is useful in explicitly bounding the tail probability of the coupling time.","We estimate the coupling time in these various situations and give applications to inequalities for the heat semigroup."],"url":"http://arxiv.org/abs/2402.13976v1","category":"math.PR"}
{"created":"2024-02-21 17:47:20","title":"Towards Building Multilingual Language Model for Medicine","abstract":"In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.","sentences":["In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions.","In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs.","second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench.","We will make the resources publicly available, including code, model weights, and datasets."],"url":"http://arxiv.org/abs/2402.13963v1","category":"cs.CL"}
{"created":"2024-02-21 17:37:30","title":"Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges","abstract":"Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The \"constellation\" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications.","sentences":["Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition.","However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability.","This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy.","Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions.","Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction.","The \"constellation\" concept and fingerprint hashing enable unique song identification.","Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency.","Storage analysis highlights the critical space-speed trade-off for practical implementation.","This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications."],"url":"http://arxiv.org/abs/2402.13957v1","category":"cs.SD"}
{"created":"2024-02-21 16:59:53","title":"Tumor segmentation on whole slide images: training or prompting?","abstract":"Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.","sentences":["Tumor segmentation stands as a pivotal task in cancer diagnosis.","Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level.","However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask.","Downsampling the WSI and performing semantic segmentation is another possible approach.","While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss.","Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself.","Such approach has demonstrated promising results on many computer vision tasks.","In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs.","In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning."],"url":"http://arxiv.org/abs/2402.13932v1","category":"cs.CV"}
{"created":"2024-02-21 16:52:26","title":"Enhancing Reinforcement Learning Agents with Local Guides","abstract":"This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.","sentences":["This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent.","For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure.","This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions.","We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences.","In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages."],"url":"http://arxiv.org/abs/2402.13930v1","category":"cs.LG"}
{"created":"2024-02-21 16:48:38","title":"Supporting the next generation lithography roadmap using partial state-feedback reduced-order switching predictive models","abstract":"To support the ever-increasing performance requirements of lithography systems in terms of throughput and accuracy, in this paper, we introduce a design framework for partial state-feedback reduced-order switching predictive models. By combining measurements and predictions, this method aims to: 1) improve overall system performance by reducing the placement errors in a die within and across the full-wafer and 2) eliminate redundant measurements by using predictions to improve system throughput. We primarily focus on well-known trade-off introduced by measurement time, which can correct errors at a cost of throughput, noise and not being robust to nonlinearities. The proposed predictive model consists of a reduced-order model with a switching logic that acts a scheduler to deal with uncertain operating conditions. The utilization of linear predictive models as a basis for the control design appeals to the ease and cost of implementation therewith enhancing the applicability. For the add-on part, the scheduler logic is adapted based on expected operating conditions of the system while guaranteeing global uniform ultimate bounded asymptotic stability. Lastly, to deal with measurement layouts, the predictor combines the measurement into model using partial state-feedback. Effectiveness of the proposed strategy is demonstrated in practice on a high-precision industrial scanner.","sentences":["To support the ever-increasing performance requirements of lithography systems in terms of throughput and accuracy, in this paper, we introduce a design framework for partial state-feedback reduced-order switching predictive models.","By combining measurements and predictions, this method aims to: 1) improve overall system performance by reducing the placement errors in a die within and across the full-wafer and 2) eliminate redundant measurements by using predictions to improve system throughput.","We primarily focus on well-known trade-off introduced by measurement time, which can correct errors at a cost of throughput, noise and not being robust to nonlinearities.","The proposed predictive model consists of a reduced-order model with a switching logic that acts a scheduler to deal with uncertain operating conditions.","The utilization of linear predictive models as a basis for the control design appeals to the ease and cost of implementation therewith enhancing the applicability.","For the add-on part, the scheduler logic is adapted based on expected operating conditions of the system while guaranteeing global uniform ultimate bounded asymptotic stability.","Lastly, to deal with measurement layouts, the predictor combines the measurement into model using partial state-feedback.","Effectiveness of the proposed strategy is demonstrated in practice on a high-precision industrial scanner."],"url":"http://arxiv.org/abs/2402.13928v1","category":"math.OC"}
{"created":"2024-02-21 16:32:43","title":"BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery","abstract":"Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}.","sentences":["Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena.","In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains.","Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis.","Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response.","Within this context, this paper focus on the cloud segmentation from remote sensing imagery.","Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications.","The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline.","This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones.","To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed.","Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations.","The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets.","This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}."],"url":"http://arxiv.org/abs/2402.13918v1","category":"cs.CV"}
{"created":"2024-02-22 18:32:17","title":"On universality for the kinetic wave equation","abstract":"On compact Riemannian manifolds with chaotic geometries, specifically those exhibiting the random wave model conjectured by Berry, we derive heuristically a homogeneous kinetic wave equation that is universal for all such manifolds.","sentences":["On compact Riemannian manifolds with chaotic geometries, specifically those exhibiting the random wave model conjectured by Berry, we derive heuristically a homogeneous kinetic wave equation that is universal for all such manifolds."],"url":"http://arxiv.org/abs/2402.14773v1","category":"math.AP"}
{"created":"2024-02-22 17:32:16","title":"Rotating Rayleigh-Benard convection: Attractors, bifurcations and heat transport via a Galerkin hierarchy","abstract":"Motivated by the need for energetically consistent climate models, the Boussinessq-Coriolis (BC) equations are studied with a focus on the Nusselt number, defined as the averaged vertical heat transport. The Howard-Krishnamurthy-Coriolis (HKC) hierarchy is defined by explicitly computing the Galerkin truncations and selecting the Fourier modes in a particular way, such that this hierarchy can be shown to energy relations consistent with the PDE. Well-posedness and the existence of an attractor are then proven for the BC model, and an explicit upper bound on the attractor dimension is given. By studying the local bifurcations at the origin, a lower bound on the attractor dimension is provided. Finally, a series of numerical studies are performed by implementing the HKC hierarchy in MATLAB, which investigate convergence of the Nusselt number as one ascends HKC hierarchy as well as other issues related to heat transport parameterization.","sentences":["Motivated by the need for energetically consistent climate models, the Boussinessq-Coriolis (BC) equations are studied with a focus on the Nusselt number, defined as the averaged vertical heat transport.","The Howard-Krishnamurthy-Coriolis (HKC) hierarchy is defined by explicitly computing the Galerkin truncations and selecting the Fourier modes in a particular way, such that this hierarchy can be shown to energy relations consistent with the PDE.","Well-posedness and the existence of an attractor are then proven for the BC model, and an explicit upper bound on the attractor dimension is given.","By studying the local bifurcations at the origin, a lower bound on the attractor dimension is provided.","Finally, a series of numerical studies are performed by implementing the HKC hierarchy in MATLAB, which investigate convergence of the Nusselt number as one ascends HKC hierarchy as well as other issues related to heat transport parameterization."],"url":"http://arxiv.org/abs/2402.14724v1","category":"math.AP"}
{"created":"2024-02-22 16:59:09","title":"InfFeed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks","abstract":"Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction. Our objectives in this paper are twofold. First we incorporate influence functions as a feedback into the model to improve its performance. Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially `silver' annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance. To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance. Toward the first objective, we adjust the label of the target instance based on its influencer(s) label. In doing this, InfFeed outperforms the state-of-the-art baselines (including LLMs) by a maximum macro F1-score margin of almost 4% for hate speech classification, 3.5% for stance classification, and 3% for irony and 2% for sarcasm detection. Toward the second objective we show that manually re-annotating only those silver annotated data points in the extension set that have a negative influence can immensely improve the model performance bringing it very close to the scenario where all the data points in the extension set have gold labels. This allows for huge reduction of the number of data points that need to be manually annotated since out of the silver annotated extension dataset, the influence function scheme picks up ~1/1000 points that need manual correction.","sentences":["Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction.","Our objectives in this paper are twofold.","First we incorporate influence functions as a feedback into the model to improve its performance.","Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially `silver' annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance.","To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance.","Toward the first objective, we adjust the label of the target instance based on its influencer(s) label.","In doing this, InfFeed outperforms the state-of-the-art baselines (including LLMs) by a maximum macro F1-score margin of almost 4% for hate speech classification, 3.5% for stance classification, and 3% for irony and 2% for sarcasm detection.","Toward the second objective we show that manually re-annotating only those silver annotated data points in the extension set that have a negative influence can immensely improve the model performance bringing it very close to the scenario where all the data points in the extension set have gold labels.","This allows for huge reduction of the number of data points that need to be manually annotated since out of the silver annotated extension dataset, the influence function scheme picks up ~1/1000 points that need manual correction."],"url":"http://arxiv.org/abs/2402.14702v1","category":"cs.CL"}
{"created":"2024-02-22 16:46:20","title":"Error Estimates for First- and Second-Order Lagrange-Galerkin Moving Mesh Schemes for the One-Dimensional Convection-Diffusion Equation","abstract":"A mass-conservative Lagrange--Galerkin scheme of second order in time for convection-diffusion problems is presented, and convergence with optimal error estimates is proved in the framework of $L^2$-theory. The introduced scheme maintains the advantages of the Lagrange--Galerkin method, i.e., CFL-free robustness for convection-dominated problems and a symmetric and positive coefficient matrix resulting from the discretization. In addition, the scheme conserves the mass on the discrete level. Unconditional stability and error estimates of second order in time are proved by employing two new key lemmas on the truncation error of the material derivative in conservative form and on a discrete Gronwall inequality for multistep methods. The mass-conservation property is achieved by the Jacobian multiplication technique introduced by Rui and Tabata in 2010, and the accuracy of second order in time is obtained based on the idea of the multistep Galerkin method along characteristics originally introduced by Ewing and Russel in 1981. For the first time step, the mass-conservative scheme of first order in time by Rui and Tabata in 2010 is employed, which is efficient and does not cause any loss of convergence order in the $\\ell^\\infty(L^2)$- and $\\ell^2(H^1_0)$-norms. For the time increment~$\\Delta t$, the mesh size~$h$ and a conforming finite element space of polynomial degree~$k \\in \\mathbb{N}$, the convergence order is of $O(\\Delta t^2 + h^k)$ in the $\\ell^\\infty(L^2)\\cap \\ell^2(H^1_0)$-norm and of $O(\\Delta t^2 + h^{k+1})$ in the $\\ell^\\infty(L^2)$-norm if the duality argument can be employed. Error estimates of $O(\\Delta t^{3/2}+h^k)$ in discrete versions of the $L^\\infty(H^1_0)$- and $H^1(L^2)$-norm are additionally proved. Numerical results confirm the theoretical convergence orders in one, two and three dimensions.","sentences":["A mass-conservative Lagrange--Galerkin scheme of second order in time for convection-diffusion problems is presented, and convergence with optimal error estimates is proved in the framework of $L^2$-theory.","The introduced scheme maintains the advantages of the Lagrange--Galerkin method, i.e., CFL-free robustness for convection-dominated problems and a symmetric and positive coefficient matrix resulting from the discretization.","In addition, the scheme conserves the mass on the discrete level.","Unconditional stability and error estimates of second order in time are proved by employing two new key lemmas on the truncation error of the material derivative in conservative form and on a discrete Gronwall inequality for multistep methods.","The mass-conservation property is achieved by the Jacobian multiplication technique introduced by Rui and Tabata in 2010, and the accuracy of second order in time is obtained based on the idea of the multistep Galerkin method along characteristics originally introduced by Ewing and Russel in 1981.","For the first time step, the mass-conservative scheme of first order in time by Rui and Tabata in 2010 is employed, which is efficient and does not cause any loss of convergence order in the $\\ell^\\infty(L^2)$- and $\\ell^2(H^1_0)$-norms.","For the time increment~$\\Delta t$, the mesh size~$h$ and a conforming finite element space of polynomial degree~$k \\in \\mathbb{N}$, the convergence order is of $O(\\Delta t^2 + h^k)$ in the $\\ell^\\infty(L^2)\\cap \\ell^2(H^1_0)$-norm and of $O(\\Delta t^2 + h^{k+1})$ in the $\\ell^\\infty(L^2)$-norm if the duality argument can be employed.","Error estimates of $O(\\Delta t^{3/2}+h^k)$ in discrete versions of the $L^\\infty(H^1_0)$- and $H^1(L^2)$-norm are additionally proved.","Numerical results confirm the theoretical convergence orders in one, two and three dimensions."],"url":"http://arxiv.org/abs/2402.14691v1","category":"math.NA"}
{"created":"2024-02-22 16:06:33","title":"On electroweak corrections to neutral current Drell-Yan with the POWHEG BOX","abstract":"Motivated by the requirement of a refined and flexible treatment of electroweak corrections to the neutral current Drell-Yan process, we report on recent developments on various input parameter/renormalization schemes for the calculation of fully differential cross sections, including both on-shell and MSbar schemes. The latter are particularly interesting for direct determinations of running couplings at the highest LHC energies. The calculations feature next-to-leading order precision with additional higher order contributions from universal corrections such as $\\Delta \\alpha$ and $\\Delta \\rho$. All the discussed input parameter/renormalization scheme options are implemented in the package of POWHEG-BOX-V2 dedicated to the neutral current Drell-Yan simulation, i.e. Z_ew-BMNNPV, which is used to obtain the presented numerical results. In particular, a comprehensive analysis on physical observables calculated with different input parameter/renormalization schemes is presented, addressing the Z peak invariant mass region as well as the high energy window. We take the opportunity of reporting also on additional improvements and options introduced in the package Z_ew-BMNNPV after svn revision 3376, such as different options for the treatment of the hadronic contribution to the running of the electromagnetic coupling and for the handling of the unstable Z resonance.","sentences":["Motivated by the requirement of a refined and flexible treatment of electroweak corrections to the neutral current Drell-Yan process, we report on recent developments on various input parameter/renormalization schemes for the calculation of fully differential cross sections, including both on-shell and MSbar schemes.","The latter are particularly interesting for direct determinations of running couplings at the highest LHC energies.","The calculations feature next-to-leading order precision with additional higher order contributions from universal corrections such as $\\Delta \\alpha$ and $\\Delta \\rho$. All the discussed input parameter/renormalization scheme options are implemented in the package of POWHEG-BOX-V2 dedicated to the neutral current Drell-Yan simulation, i.e. Z_ew-BMNNPV, which is used to obtain the presented numerical results.","In particular, a comprehensive analysis on physical observables calculated with different input parameter/renormalization schemes is presented, addressing the Z peak invariant mass region as well as the high energy window.","We take the opportunity of reporting also on additional improvements and options introduced in the package Z_ew-BMNNPV after svn revision 3376, such as different options for the treatment of the hadronic contribution to the running of the electromagnetic coupling and for the handling of the unstable Z resonance."],"url":"http://arxiv.org/abs/2402.14659v1","category":"hep-ph"}
{"created":"2024-02-22 16:00:20","title":"GaussianPro: 3D Gaussian Splatting with Progressive Propagation","abstract":"The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR.","sentences":["The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed.","However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques.","When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS.","As a result, 3DGS suffers from difficult optimization and low-quality renderings.","In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians.","Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations.","Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR."],"url":"http://arxiv.org/abs/2402.14650v1","category":"cs.CV"}
{"created":"2024-02-22 15:37:05","title":"Structure and thermodynamics of defects in Na-feldspar from a neural network potential","abstract":"The diffusive phase transformations occurring in feldspar, a common mineral in the crust of the Earth, are essential for reconstructing the thermal histories of magmatic and metamorphic rocks. Due to the long timescales over which these transformations proceed, the mechanism responsible for sodium diffusion and its possible anisotropy has remained a topic of debate. To elucidate this defect-controlled process, we have developed a Neural Network Potential (NNP) trained on first-principle calculations of Na-feldspar (Albite) and its charged defects. This new force field reproduces various experimentally known properties of feldspar, including its lattice parameters, elastic constants as well as heat capacity and DFT-calculated defect formation energies. A new type of dumbbell interstitial defect is found to be most favorable and its free energy of formation at finite temperature is calculated using thermodynamic integration. The necessity of including electrostatic corrections before training an NNP is demonstrated by predicting more consistent defect formation energies.","sentences":["The diffusive phase transformations occurring in feldspar, a common mineral in the crust of the Earth, are essential for reconstructing the thermal histories of magmatic and metamorphic rocks.","Due to the long timescales over which these transformations proceed, the mechanism responsible for sodium diffusion and its possible anisotropy has remained a topic of debate.","To elucidate this defect-controlled process, we have developed a Neural Network Potential (NNP) trained on first-principle calculations of Na-feldspar (Albite) and its charged defects.","This new force field reproduces various experimentally known properties of feldspar, including its lattice parameters, elastic constants as well as heat capacity and DFT-calculated defect formation energies.","A new type of dumbbell interstitial defect is found to be most favorable and its free energy of formation at finite temperature is calculated using thermodynamic integration.","The necessity of including electrostatic corrections before training an NNP is demonstrated by predicting more consistent defect formation energies."],"url":"http://arxiv.org/abs/2402.14640v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 14:45:28","title":"Bayesian inference of thermal effects in dense matter within the covariant density functional theory","abstract":"The high temperatures reached in a proto-neutron star or during the post-merger phase of a binary neutron star coalescence lead to non-negligible thermal effects on the equation of state (EOS) of dense nuclear matter. Here we study these effects within the covariant density functional theory employing the posteriors of a Bayesian inference, which encompasses a large sample of EOS models. Different densities and temperatures are considered. We find that for a number of quantities thermal effects are strongly correlated with the Dirac effective mass ($m^*$) of the nucleons and/or its logarithmic derivative as a function of density. These results can be explained within the low temperature approximation though they survive beyond this limit.","sentences":["The high temperatures reached in a proto-neutron star or during the post-merger phase of a binary neutron star coalescence lead to non-negligible thermal effects on the equation of state (EOS) of dense nuclear matter.","Here we study these effects within the covariant density functional theory employing the posteriors of a Bayesian inference, which encompasses a large sample of EOS models.","Different densities and temperatures are considered.","We find that for a number of quantities thermal effects are strongly correlated with the Dirac effective mass ($m^*$) of the nucleons and/or its logarithmic derivative as a function of density.","These results can be explained within the low temperature approximation though they survive beyond this limit."],"url":"http://arxiv.org/abs/2402.14593v1","category":"nucl-th"}
{"created":"2024-02-22 10:37:43","title":"On Poisson Electrodynamics With Charged Fields","abstract":"Poisson electrodynamics is the low-energy limit of a rank-one noncommutative gauge theory. It admits a closed formulation in terms of a Poisson structure on the space-time manifold and reproduces ordinary classical electrodynamics in the commutative limit. In this paper, we address and solve the problem of minimal coupling to charged matter fields with a proper commutative limit. Our construction essentially relies on the geometry of symplectic groupoids and works for all integrable Poisson manifolds. An additional advantage of our approach is that the corresponding Lagrangians can be defined on an arbitrary metric background.","sentences":["Poisson electrodynamics is the low-energy limit of a rank-one noncommutative gauge theory.","It admits a closed formulation in terms of a Poisson structure on the space-time manifold and reproduces ordinary classical electrodynamics in the commutative limit.","In this paper, we address and solve the problem of minimal coupling to charged matter fields with a proper commutative limit.","Our construction essentially relies on the geometry of symplectic groupoids and works for all integrable Poisson manifolds.","An additional advantage of our approach is that the corresponding Lagrangians can be defined on an arbitrary metric background."],"url":"http://arxiv.org/abs/2402.14439v1","category":"hep-th"}
{"created":"2024-02-22 10:07:50","title":"Chiral covers of regular maps of given type","abstract":"With the help of the theory of holomorphic and anti-holomorphic differentials, G. A. Jones [Chiral covers of hypermaps, Ars Math. Contemp. 8 (2015), 425-431] proved that every regular hypermap of a non-spherical type is covered by an infinite number of orientably-regular but chiral hypermaps of the same type. We present a different proof of the same result for regular maps, based on parallel products of maps and existence of chiral maps of a given hyperbolic type with a symmetric or an alternating automorphism group.","sentences":["With the help of the theory of holomorphic and anti-holomorphic differentials, G. A. Jones","[Chiral covers of hypermaps, Ars Math.","Contemp.","8 (2015)",", 425-431] proved that every regular hypermap of a non-spherical type is covered by an infinite number of orientably-regular but chiral hypermaps of the same type.","We present a different proof of the same result for regular maps, based on parallel products of maps and existence of chiral maps of a given hyperbolic type with a symmetric or an alternating automorphism group."],"url":"http://arxiv.org/abs/2402.14420v1","category":"math.GR"}
{"created":"2024-02-22 09:26:16","title":"Closed-Form Bounds for DP-SGD against Record-level Inference","abstract":"Machine learning models trained with differentially-private (DP) algorithms such as DP-SGD enjoy resilience against a wide range of privacy attacks. Although it is possible to derive bounds for some attacks based solely on an $(\\varepsilon,\\delta)$-DP guarantee, meaningful bounds require a small enough privacy budget (i.e., injecting a large amount of noise), which results in a large loss in utility. This paper presents a new approach to evaluate the privacy of machine learning models against specific record-level threats, such as membership and attribute inference, without the indirection through DP. We focus on the popular DP-SGD algorithm, and derive simple closed-form bounds. Our proofs model DP-SGD as an information theoretic channel whose inputs are the secrets that an attacker wants to infer (e.g., membership of a data record) and whose outputs are the intermediate model parameters produced by iterative optimization. We obtain bounds for membership inference that match state-of-the-art techniques, whilst being orders of magnitude faster to compute. Additionally, we present a novel data-dependent bound against attribute inference. Our results provide a direct, interpretable, and practical way to evaluate the privacy of trained models against specific inference threats without sacrificing utility.","sentences":["Machine learning models trained with differentially-private (DP) algorithms such as DP-SGD enjoy resilience against a wide range of privacy attacks.","Although it is possible to derive bounds for some attacks based solely on an $(\\varepsilon,\\delta)$-DP guarantee, meaningful bounds require a small enough privacy budget (i.e., injecting a large amount of noise), which results in a large loss in utility.","This paper presents a new approach to evaluate the privacy of machine learning models against specific record-level threats, such as membership and attribute inference, without the indirection through DP.","We focus on the popular DP-SGD algorithm, and derive simple closed-form bounds.","Our proofs model DP-SGD as an information theoretic channel whose inputs are the secrets that an attacker wants to infer (e.g., membership of a data record) and whose outputs are the intermediate model parameters produced by iterative optimization.","We obtain bounds for membership inference that match state-of-the-art techniques, whilst being orders of magnitude faster to compute.","Additionally, we present a novel data-dependent bound against attribute inference.","Our results provide a direct, interpretable, and practical way to evaluate the privacy of trained models against specific inference threats without sacrificing utility."],"url":"http://arxiv.org/abs/2402.14397v1","category":"cs.CR"}
{"created":"2024-02-22 08:51:29","title":"Long-time asymptotics of the damped nonlinear Klein-Gordon equation with a delta potential","abstract":"We consider the damped nonlinear Klein-Gordon equation with a delta potential \\begin{align*} \\partial_{t}^2u-\\partial_{x}^2u+2\\alpha \\partial_{t}u+u-\\gamma {\\delta}_0u-|u|^{p-1}u=0, \\ & (t,x) \\in \\mathbb{R} \\times \\mathbb{R}, \\end{align*} where $p>2$, $\\alpha>0,\\ \\gamma<2$, and $\\delta_0=\\delta_0 (x)$ denotes the Dirac delta with the mass at the origin. When $\\gamma=0$, C\\^{o}te, Martel and Yuan proved that any global solution either converges to 0 or to the sum of $K\\geq 1$ decoupled solitary waves which have alternative signs. In this paper, we first prove that any global solution either converges to 0 or to the sum of $K\\geq 1$ decoupled solitary waves. Next we construct a single solitary wave solution that moves away from the origin when $\\gamma<0$ and construct an even 2-solitary wave solution when $\\gamma\\leq -2$. Last we give single solitary wave solutions and even 2-solitary wave solutions an upper bound for the distance between the origin and the solitary wave.","sentences":["We consider the damped nonlinear Klein-Gordon equation with a delta potential \\begin{align*} \\partial_{t}^2u-\\partial_{x}^2u+2\\alpha \\partial_{t}u+u-\\gamma {\\delta}_0u-|u|^{p-1}u=0, \\ & (t,x) \\in \\mathbb{R} \\times \\mathbb{R}, \\end{align*} where $p>2$, $\\alpha>0,\\ \\gamma<2$, and $\\delta_0=\\delta_0 (x)$ denotes the Dirac delta with the mass at the origin.","When $\\gamma=0$, C\\^{o}te, Martel and Yuan proved that any global solution either converges to 0 or to the sum of $K\\geq 1$ decoupled solitary waves which have alternative signs.","In this paper, we first prove that any global solution either converges to 0 or to the sum of $K\\geq 1$ decoupled solitary waves.","Next we construct a single solitary wave solution that moves away from the origin when $\\gamma<0$ and construct an even 2-solitary wave solution when $\\gamma\\leq -2$.","Last we give single solitary wave solutions and even 2-solitary wave solutions an upper bound for the distance between the origin and the solitary wave."],"url":"http://arxiv.org/abs/2402.14381v1","category":"math.AP"}
{"created":"2024-02-22 08:11:22","title":"Representation Learning for Frequent Subgraph Mining","abstract":"Identifying frequent subgraphs, also called network motifs, is crucial in analyzing and predicting properties of real-world networks. However, finding large commonly-occurring motifs remains a challenging problem not only due to its NP-hard subroutine of subgraph counting, but also the exponential growth of the number of possible subgraphs patterns. Here we present Subgraph Pattern Miner (SPMiner), a novel neural approach for approximately finding frequent subgraphs in a large target graph. SPMiner combines graph neural networks, order embedding space, and an efficient search strategy to identify network subgraph patterns that appear most frequently in the target graph. SPMiner first decomposes the target graph into many overlapping subgraphs and then encodes each subgraph into an order embedding space. SPMiner then uses a monotonic walk in the order embedding space to identify frequent motifs. Compared to existing approaches and possible neural alternatives, SPMiner is more accurate, faster, and more scalable. For 5- and 6-node motifs, we show that SPMiner can almost perfectly identify the most frequent motifs while being 100x faster than exact enumeration methods. In addition, SPMiner can also reliably identify frequent 10-node motifs, which is well beyond the size limit of exact enumeration approaches. And last, we show that SPMiner can find large up to 20 node motifs with 10-100x higher frequency than those found by current approximate methods.","sentences":["Identifying frequent subgraphs, also called network motifs, is crucial in analyzing and predicting properties of real-world networks.","However, finding large commonly-occurring motifs remains a challenging problem not only due to its NP-hard subroutine of subgraph counting, but also the exponential growth of the number of possible subgraphs patterns.","Here we present Subgraph Pattern Miner (SPMiner), a novel neural approach for approximately finding frequent subgraphs in a large target graph.","SPMiner combines graph neural networks, order embedding space, and an efficient search strategy to identify network subgraph patterns that appear most frequently in the target graph.","SPMiner first decomposes the target graph into many overlapping subgraphs and then encodes each subgraph into an order embedding space.","SPMiner then uses a monotonic walk in the order embedding space to identify frequent motifs.","Compared to existing approaches and possible neural alternatives, SPMiner is more accurate, faster, and more scalable.","For 5- and 6-node motifs, we show that SPMiner can almost perfectly identify the most frequent motifs while being 100x faster than exact enumeration methods.","In addition, SPMiner can also reliably identify frequent 10-node motifs, which is well beyond the size limit of exact enumeration approaches.","And last, we show that SPMiner can find large up to 20 node motifs with 10-100x higher frequency than those found by current approximate methods."],"url":"http://arxiv.org/abs/2402.14367v1","category":"cs.LG"}
{"created":"2024-02-22 07:58:06","title":"Development of a gyrokinetic-MHD energetic particle simulation code Part II: Linear simulations of Alfv\u00e9n eigenmodes driven by energetic particles","abstract":"We have developed a hybrid code GMEC: Gyro-kinetic Magnetohydrodynamics (MHD) Energetic-particle Code that can numerically simulate energetic particle-driven Alfv\\'en eigenmodes and energetic particle transport in tokamak plasmas. In order to resolve the Alfv\\'en eigenmodes with high toroidal numbers effectively, the field-aligned coordinates and meshes are adopted. The extended MHD equations are solved with five-points finite difference method and fourth order Runge-Kutta method. The gyrokinetic equations are solved by particle-in-cell (PIC) method for the perturbed energetic particle pressures that are coupled into the MHD equations. Up to now, a simplified version of the hybrid code has been completed with several successful verifications including linear simulations of toroidal Alfv\\'en eigenmodes and reversed shear Alfv\\'en eigenmodes.","sentences":["We have developed a hybrid code GMEC: Gyro-kinetic Magnetohydrodynamics (MHD) Energetic-particle Code that can numerically simulate energetic particle-driven Alfv\\'en eigenmodes and energetic particle transport in tokamak plasmas.","In order to resolve the Alfv\\'en eigenmodes with high toroidal numbers effectively, the field-aligned coordinates and meshes are adopted.","The extended MHD equations are solved with five-points finite difference method and fourth order Runge-Kutta method.","The gyrokinetic equations are solved by particle-in-cell (PIC) method for the perturbed energetic particle pressures that are coupled into the MHD equations.","Up to now, a simplified version of the hybrid code has been completed with several successful verifications including linear simulations of toroidal Alfv\\'en eigenmodes and reversed shear Alfv\\'en eigenmodes."],"url":"http://arxiv.org/abs/2402.14357v1","category":"physics.plasm-ph"}
{"created":"2024-02-22 06:48:13","title":"Local Wellposedness of dispersive equations with quasi-periodic initial data","abstract":"We prove unconditional local well-posedness in a space of quasi-periodic functions for dispersive equations of the form $$\\partial_tu + Lu + \\partial_x(u^{p+1})=0,$$ where $L$ is a multiplier operator with purely imaginary symbol which grows at most exponentially. The class of equations to which our method applies includes the generalized Korteweg-de Vries equation, the generalized Benjamin-Ono equation, and the derivative nonlinear Schr\\\"odinger equation. We also discuss well-posedness of some dispersive models which do not have a problematic derivative in the nonlinearity, namely, the nonlinear Schr\\\"odinger equation and the generalized Benjamin-Bona-Mahony equation, with quasi-periodic initial data. In this way, we recover and improve upon results from arXiv:1212.2674v3 [math.AP], arXiv:2110.11263v1 [math.AP] and arXiv:2201.02920v1 [math.AP] by shorter arguments.","sentences":["We prove unconditional local well-posedness in a space of quasi-periodic functions for dispersive equations of the form $$\\partial_tu + Lu + \\partial_x(u^{p+1})=0,$$ where $L$ is a multiplier operator with purely imaginary symbol which grows at most exponentially.","The class of equations to which our method applies includes the generalized Korteweg-de Vries equation, the generalized Benjamin-Ono equation, and the derivative nonlinear Schr\\\"odinger equation.","We also discuss well-posedness of some dispersive models which do not have a problematic derivative in the nonlinearity, namely, the nonlinear Schr\\\"odinger equation and the generalized Benjamin-Bona-Mahony equation, with quasi-periodic initial data.","In this way, we recover and improve upon results from arXiv:1212.2674v3","[math.AP], arXiv:2110.11263v1","[math.AP] and arXiv:2201.02920v1","[math.AP] by shorter arguments."],"url":"http://arxiv.org/abs/2402.14329v1","category":"math.AP"}
{"created":"2024-02-22 06:21:58","title":"Existence of solutions to a fractional semilinear heat equation in uniformly local weak Zygmund type spaces","abstract":"In this paper we introduce uniformly local weak Zygmund type spaces, and obtain an optimal sufficient condition for the existence of solutions to the critical fractional semilinear heat equation.","sentences":["In this paper we introduce uniformly local weak Zygmund type spaces, and obtain an optimal sufficient condition for the existence of solutions to the critical fractional semilinear heat equation."],"url":"http://arxiv.org/abs/2402.14319v1","category":"math.AP"}
{"created":"2024-02-22 06:04:49","title":"Learning to Kern -- Set-wise Estimation of Optimal Letter Space","abstract":"Kerning is the task of setting appropriate horizontal spaces for all possible letter pairs of a certain font. One of the difficulties of kerning is that the appropriate space differs for each letter pair. Therefore, for a total of 52 capital and small letters, we need to adjust $52 \\times 52 = 2704$ different spaces. Another difficulty is that there is neither a general procedure nor criterion for automatic kerning; therefore, kerning is still done manually or with heuristics. In this paper, we tackle kerning by proposing two machine-learning models, called pairwise and set-wise models. The former is a simple deep neural network that estimates the letter space for two given letter images. In contrast, the latter is a Transformer-based model and estimates the letter spaces for three or more given letter images. For example, the set-wise model simultaneously estimates 2704 spaces for 52 letter images for a certain font. Among the two models, the set-wise model is not only more efficient but also more accurate because its internal self-attention mechanism allows for more consistent kerning for all letters. Experimental results on about 2500 Google fonts and their quantitative and qualitative analyses show that the set-wise model has an average estimation error of only about 5.3 pixels when the average letter space of all fonts and letter pairs is about 115 pixels.","sentences":["Kerning is the task of setting appropriate horizontal spaces for all possible letter pairs of a certain font.","One of the difficulties of kerning is that the appropriate space differs for each letter pair.","Therefore, for a total of 52 capital and small letters, we need to adjust $52 \\times 52 = 2704$ different spaces.","Another difficulty is that there is neither a general procedure nor criterion for automatic kerning; therefore, kerning is still done manually or with heuristics.","In this paper, we tackle kerning by proposing two machine-learning models, called pairwise and set-wise models.","The former is a simple deep neural network that estimates the letter space for two given letter images.","In contrast, the latter is a Transformer-based model and estimates the letter spaces for three or more given letter images.","For example, the set-wise model simultaneously estimates 2704 spaces for 52 letter images for a certain font.","Among the two models, the set-wise model is not only more efficient but also more accurate because its internal self-attention mechanism allows for more consistent kerning for all letters.","Experimental results on about 2500 Google fonts and their quantitative and qualitative analyses show that the set-wise model has an average estimation error of only about 5.3 pixels when the average letter space of all fonts and letter pairs is about 115 pixels."],"url":"http://arxiv.org/abs/2402.14313v1","category":"cs.CV"}
{"created":"2024-02-22 04:55:58","title":"Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion","abstract":"We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quality and rule-based controllability, outperforming current state-of-the-art generators in a variety of settings. For detailed demonstrations, please visit our project site: https://scg-rule-guided-music.github.io/.","sentences":["We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance.","Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion.","We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time.","Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion.","Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quality and rule-based controllability, outperforming current state-of-the-art generators in a variety of settings.","For detailed demonstrations, please visit our project site: https://scg-rule-guided-music.github.io/."],"url":"http://arxiv.org/abs/2402.14285v1","category":"cs.SD"}
{"created":"2024-02-22 04:26:03","title":"Stable Comodule Deformations and the Synthetic Adams-Novikov Spectral Sequence","abstract":"We study the Adams-Novikov spectral sequence in $\\mathbb{F}_p$-synthetic spectra, computing the synthetic analogs of $\\mathrm{BP}$ and its cooperations to identify the synthetic Adams-Novikov $\\mathrm{E}_2$-page, computed in a range with a synthetic algebraic Novikov spectral sequence. We then identify deformations associated to the Cartan-Eilenberg and algebraic Novikov spectral sequences in terms of stable comodule categories, categorifying an algebraic Novikov spectral sequence result of Gheorghe-Wang-Xu. We then apply Isaksen-Wang-Xu methods in $\\mathbb{F}_p$-synthetic spectra to deduce differentials in the synthetic Adams-Novikov for the sphere, producing almost entirely algebraic computations through the 45-stem.","sentences":["We study the Adams-Novikov spectral sequence in $\\mathbb{F}_p$-synthetic spectra, computing the synthetic analogs of $\\mathrm{BP}$ and its cooperations to identify the synthetic Adams-Novikov $\\mathrm{E}_2$-page, computed in a range with a synthetic algebraic Novikov spectral sequence.","We then identify deformations associated to the Cartan-Eilenberg and algebraic Novikov spectral sequences in terms of stable comodule categories, categorifying an algebraic Novikov spectral sequence result of Gheorghe-Wang-Xu.","We then apply Isaksen-Wang-Xu methods in $\\mathbb{F}_p$-synthetic spectra to deduce differentials in the synthetic Adams-Novikov for the sphere, producing almost entirely algebraic computations through the 45-stem."],"url":"http://arxiv.org/abs/2402.14274v1","category":"math.AT"}
{"created":"2024-02-22 04:03:38","title":"Quasinormal modes of a charged black hole with scalar hair","abstract":"From a five-dimensional Einstein-Maxwell theory, Bah et al. constructed a singularity free topology star/black hole [Phys. Rev. Lett. 126, 151101 (2021)]. After the Klein-Kluza reduction, i.e., integrating the extra space dimension, it can obtain an effective four-dimensional static spherical charged black hole with scalar hair. In this paper, we study the quasinormal modes (QNMs) of the scalar field, electromagnetic field, and gravitational field on the background of this effective four-dimensional charged black hole. The radial parts of the perturbed fields all satisfy a Schr\\\"{o}dinger-like equation. Using the asymptotic iteration method, we obtain the QNM frequencies semianalytically. For low overtone QNMs, the results obtained from the asymptotic iteration method and the Wentzel-Kramers-Brillouin approximation method agree well. In the null coordinates, the evolution of a Gaussian package is also studied. The QNM frequencies obtained by fitting the evolution data also agree well with the results obtained by the asymptotic iteration method.","sentences":["From a five-dimensional Einstein-Maxwell theory, Bah et al. constructed a singularity free topology star/black hole [Phys.","Rev. Lett.","126, 151101 (2021)].","After the Klein-Kluza reduction, i.e., integrating the extra space dimension, it can obtain an effective four-dimensional static spherical charged black hole with scalar hair.","In this paper, we study the quasinormal modes (QNMs) of the scalar field, electromagnetic field, and gravitational field on the background of this effective four-dimensional charged black hole.","The radial parts of the perturbed fields all satisfy a Schr\\\"{o}dinger-like equation.","Using the asymptotic iteration method, we obtain the QNM frequencies semianalytically.","For low overtone QNMs, the results obtained from the asymptotic iteration method and the Wentzel-Kramers-Brillouin approximation method agree well.","In the null coordinates, the evolution of a Gaussian package is also studied.","The QNM frequencies obtained by fitting the evolution data also agree well with the results obtained by the asymptotic iteration method."],"url":"http://arxiv.org/abs/2402.14265v1","category":"gr-qc"}
{"created":"2024-02-22 03:29:47","title":"On the Cohomology of Restricted Heisenberg Lie Algebras","abstract":"We show that the Heisenberg Lie algebras over an algebraically closed field $\\mathbb{F}$ of characteristic $p>0$ admit a family of restricted Lie algebras. We use the ordinary 1- and 2-cohomology spaces with trivial coefficients to compute the restricted 1- and 2-cohomology spaces of these restricted Heisenberg Lie algebras. We describe the restricted 1-dimensional central extensions, including explicit formulas for the Lie brackets and $\\cdot^{[p]}$-operators.","sentences":["We show that the Heisenberg Lie algebras over an algebraically closed field $\\mathbb{F}$ of characteristic $p>0$ admit a family of restricted Lie algebras.","We use the ordinary 1- and 2-cohomology spaces with trivial coefficients to compute the restricted 1- and 2-cohomology spaces of these restricted Heisenberg Lie algebras.","We describe the restricted 1-dimensional central extensions, including explicit formulas for the Lie brackets and $\\cdot^{[p]}$-operators."],"url":"http://arxiv.org/abs/2402.14249v1","category":"math.RT"}
{"created":"2024-02-22 03:04:30","title":"Non-equilibrium theory of the linear viscoelasticity of glass and gel forming liquids","abstract":"We propose a first-principles theoretical approach for the description of the aging of the linear viscoelastic properties of a colloidal liquid after a sudden quench into a dynamically arrested (glass or gel) state. Specifically, we couple a general expression for the time-evolving shear-stress relaxation function $\\eta(\\tau;t)$ (whose $\\tau$-integral is the instantaneous viscosity $\\eta(t)$), written in terms of the non-equilibrium structure factor $S(k;t)$ and intermediate scattering function $F(k,\\tau;t)$, with the equations that determine $S(k;t)$ and $F(k,\\tau;t)$, provided by the non-equilibrium self-consistent generalized Langevin equation (NE-SCGLE) theory. In this manner, we obtain a closed theoretical scheme that directly connects inter-particle forces with experimentally accessible rheological properties of non-equilibrium amorphous states of matter. The predictive capability of the resulting theoretical formalism is illustrated here with its concrete application to the Weeks-Chandler-Andersen (WCA) model of a soft-sphere fluid.","sentences":["We propose a first-principles theoretical approach for the description of the aging of the linear viscoelastic properties of a colloidal liquid after a sudden quench into a dynamically arrested (glass or gel) state.","Specifically, we couple a general expression for the time-evolving shear-stress relaxation function $\\eta(\\tau;t)$ (whose $\\tau$-integral is the instantaneous viscosity $\\eta(t)$), written in terms of the non-equilibrium structure factor $S(k;t)$ and intermediate scattering function $F(k,\\tau;t)$, with the equations that determine $S(k;t)$ and $F(k,\\tau;t)$, provided by the non-equilibrium self-consistent generalized Langevin equation (NE-SCGLE) theory.","In this manner, we obtain a closed theoretical scheme that directly connects inter-particle forces with experimentally accessible rheological properties of non-equilibrium amorphous states of matter.","The predictive capability of the resulting theoretical formalism is illustrated here with its concrete application to the Weeks-Chandler-Andersen (WCA) model of a soft-sphere fluid."],"url":"http://arxiv.org/abs/2402.14242v1","category":"cond-mat.soft"}
{"created":"2024-02-22 02:49:26","title":"A Principle of Maximum Entropy for the Navier-Stokes Equations","abstract":"A principle of maximum entropy is proposed in the context of viscous incompressible flow in Eulerian coordinates. The relative entropy functional, defined over the space of $L^2$ divergence-free velocity fields, is maximized relative to alternate measures supported over the energy--enstrophy surface. Since thermodynamic equilibrium distributions are characterized by maximum entropy, connections are drawn with stationary statistical solutions of the incompressible Navier-Stokes equations. Special emphasis is on the correspondence with the final statistics described by Kolmogorov's theory of fully developed turbulence.","sentences":["A principle of maximum entropy is proposed in the context of viscous incompressible flow in Eulerian coordinates.","The relative entropy functional, defined over the space of $L^2$ divergence-free velocity fields, is maximized relative to alternate measures supported over the energy--enstrophy surface.","Since thermodynamic equilibrium distributions are characterized by maximum entropy, connections are drawn with stationary statistical solutions of the incompressible Navier-Stokes equations.","Special emphasis is on the correspondence with the final statistics described by Kolmogorov's theory of fully developed turbulence."],"url":"http://arxiv.org/abs/2402.14240v1","category":"physics.flu-dyn"}
{"created":"2024-02-22 02:48:03","title":"Velocity recostruction with graph neural networks","abstract":"In this work, we seek to improve the velocity reconstruction of clusters by using Graph Neural Networks -- a type of deep neural network designed to analyze sparse, unstructured data. In comparison to the Convolutional Neural Network (CNN) which is built for structured data such as regular grids, GNN is particularly suitable for analyzing galaxy catalogs. In our GNNs, galaxies as represented as nodes that are connected with edges. The galaxy positions and properties -- stellar mass, star formation rate, and total number of galaxies within 100~\\mpc -- are combined to predict the line-of-sight velocity of the clusters. To train our networks, we use mock SDSS galaxies and clusters constructed from the Magneticum hydrodynamic simulations. Our GNNs reach a precision in reconstructed line-of-sight velocity of $\\Delta v$=163 km/s, outperforming by $\\approx$10\\% the perturbation theory~($\\Delta v$=181 km/s) or the CNN~($\\Delta v$=179 km/s). The stellar mass provides additional information, improving the precision by $\\approx$6\\% beyond the position-only GNN, while other properties add little information. Our GNNs remain capable of reconstructing the velocity field when redshift-space distortion is included, with $\\Delta v$=210 km/s which is again 10\\% better than CNN with RSD. Finally, we find that even with an impressive, nearly 70\\% increase in galaxy number density from SDSS to DESI, our GNNs only show an underwhelming 2\\% improvement, in line with previous works using other methods. Our work demonstrates that, while the efficiency in velocity reconstruction may have plateaued already at SDSS number density, further improvements are still hopeful with new reconstruction models such as the GNNs studied here.","sentences":["In this work, we seek to improve the velocity reconstruction of clusters by using Graph Neural Networks -- a type of deep neural network designed to analyze sparse, unstructured data.","In comparison to the Convolutional Neural Network (CNN) which is built for structured data such as regular grids, GNN is particularly suitable for analyzing galaxy catalogs.","In our GNNs, galaxies as represented as nodes that are connected with edges.","The galaxy positions and properties -- stellar mass, star formation rate, and total number of galaxies within 100~\\mpc -- are combined to predict the line-of-sight velocity of the clusters.","To train our networks, we use mock SDSS galaxies and clusters constructed from the Magneticum hydrodynamic simulations.","Our GNNs reach a precision in reconstructed line-of-sight velocity of $\\Delta v$=163 km/s, outperforming by $\\approx$10\\% the perturbation theory~($\\Delta v$=181 km/s) or the CNN~($\\Delta v$=179 km/s).","The stellar mass provides additional information, improving the precision by $\\approx$6\\% beyond the position-only GNN, while other properties add little information.","Our GNNs remain capable of reconstructing the velocity field when redshift-space distortion is included, with $\\Delta v$=210 km/s which is again 10\\% better than CNN with RSD.","Finally, we find that even with an impressive, nearly 70\\% increase in galaxy number density from SDSS to DESI, our GNNs only show an underwhelming 2\\% improvement, in line with previous works using other methods.","Our work demonstrates that, while the efficiency in velocity reconstruction may have plateaued already at SDSS number density, further improvements are still hopeful with new reconstruction models such as the GNNs studied here."],"url":"http://arxiv.org/abs/2402.14239v1","category":"astro-ph.CO"}
{"created":"2024-02-22 01:42:12","title":"Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience","abstract":"Neural representations induced by naturalistic stimuli offer insights into how humans respond to peripheral stimuli in daily life. The key to understanding the general neural mechanisms underlying naturalistic stimuli processing involves aligning neural activities across individuals and extracting inter-subject shared neural representations. Targeting the Electroencephalogram (EEG) technique, known for its rich spatial and temporal information, this study presents a general framework for Contrastive Learning of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER). Harnessing the representational capabilities of contrastive learning, CL-SSTER utilizes a neural network to maximize the similarity of EEG representations across individuals for identical stimuli, contrasting with those for varied stimuli. The network employed spatial and temporal convolutions to simultaneously learn the spatial and temporal patterns inherent in EEG. The versatility of CL-SSTER was demonstrated on three EEG datasets, including a synthetic dataset, a speech audio EEG dataset, and an emotional video EEG dataset. CL-SSTER attained the highest inter-subject correlation (ISC) values compared to the state-of-the-art ISC methods. The latent representations generated by CL-SSTER exhibited reliable spatiotemporal EEG patterns, which can be explained by specific aspects of the stimuli. CL-SSTER serves as an interpretable and scalable foundational framework for the identification of inter-subject shared neural representations in the realm of naturalistic neuroscience.","sentences":["Neural representations induced by naturalistic stimuli offer insights into how humans respond to peripheral stimuli in daily life.","The key to understanding the general neural mechanisms underlying naturalistic stimuli processing involves aligning neural activities across individuals and extracting inter-subject shared neural representations.","Targeting the Electroencephalogram (EEG) technique, known for its rich spatial and temporal information, this study presents a general framework for Contrastive Learning of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER).","Harnessing the representational capabilities of contrastive learning, CL-SSTER utilizes a neural network to maximize the similarity of EEG representations across individuals for identical stimuli, contrasting with those for varied stimuli.","The network employed spatial and temporal convolutions to simultaneously learn the spatial and temporal patterns inherent in EEG.","The versatility of CL-SSTER was demonstrated on three EEG datasets, including a synthetic dataset, a speech audio EEG dataset, and an emotional video EEG dataset.","CL-SSTER attained the highest inter-subject correlation (ISC) values compared to the state-of-the-art ISC methods.","The latent representations generated by CL-SSTER exhibited reliable spatiotemporal EEG patterns, which can be explained by specific aspects of the stimuli.","CL-SSTER serves as an interpretable and scalable foundational framework for the identification of inter-subject shared neural representations in the realm of naturalistic neuroscience."],"url":"http://arxiv.org/abs/2402.14213v1","category":"q-bio.NC"}
{"created":"2024-02-22 01:18:55","title":"Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer","abstract":"Many deep learning synthetic speech generation tools are readily available. The use of synthetic speech has caused financial fraud, impersonation of people, and misinformation to spread. For this reason forensic methods that can detect synthetic speech have been proposed. Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic speech shared on social platforms. In this paper we propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a synthetic speech detector that converts a time domain speech signal to a mel-spectrogram and processes it in patches using a transformer neural network. We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic speech detection. We also investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT generalizes well than several existing methods on detecting synthetic speech from an out-of-distribution dataset. We also evaluate robustness of PS3DT to detect telephone quality synthetic speech and synthetic speech shared on social platforms (compressed speech). PS3DT is robust to compression and can detect telephone quality synthetic speech better than several existing methods.","sentences":["Many deep learning synthetic speech generation tools are readily available.","The use of synthetic speech has caused financial fraud, impersonation of people, and misinformation to spread.","For this reason forensic methods that can detect synthetic speech have been proposed.","Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic speech shared on social platforms.","In this paper we propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a synthetic speech detector that converts a time domain speech signal to a mel-spectrogram and processes it in patches using a transformer neural network.","We evaluate the detection performance of PS3DT on ASVspoof2019 dataset.","Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic speech detection.","We also investigate generalization performance of PS3DT on In-the-Wild dataset.","PS3DT generalizes well than several existing methods on detecting synthetic speech from an out-of-distribution dataset.","We also evaluate robustness of PS3DT to detect telephone quality synthetic speech and synthetic speech shared on social platforms (compressed speech).","PS3DT is robust to compression and can detect telephone quality synthetic speech better than several existing methods."],"url":"http://arxiv.org/abs/2402.14205v1","category":"cs.SD"}
{"created":"2024-02-22 00:45:40","title":"Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields","abstract":"Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering \"jaggies\" or \"blurry\" images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see https://stnamjef.github.io/mipgrid.github.io/.","sentences":["Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering \"jaggies\" or \"blurry\" images at varying camera distances, remains unresolved in most existing approaches.","The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays.","However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods.","In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time.","The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids.","To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes.","Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time.","For code and demo videos, please see https://stnamjef.github.io/mipgrid.github.io/."],"url":"http://arxiv.org/abs/2402.14196v1","category":"cs.CV"}
{"created":"2024-02-21 23:20:07","title":"Infinite quantum twisting at the Cauchy horizon of rotating black holes","abstract":"We present a numerical calculation of the expectation value of the quantum angular-momentum current flux density for a scalar field in the Unruh state near the inner horizon of a Kerr-de Sitter black hole. Our results indicate that this flux diverges as $V_-^{-1}$ in a suitable Kruskal coordinate such that $V_-=0$ at the inner horizon. Depending on the parameter values of the scalar field and black hole that we consider, and depending on the polar angle (latitude), this flux can have different signs. In the near extremal cases considered, the angle average of the expectation value of the quantum angular momentum current flux is of the opposite sign as the angular momentum of the background itself, suggesting that, in the cases considered, quantum effects tend to decrease the total angular momentum of the spheres away from the extremal value. We also numerically calculate the energy flux component, which provides the leading order divergence of the quantum stress energy tensor, dominant over the classical stress energy tensor, at the inner horizon. Taking our expectation value of the quantum stress tensor as the source in the semiclassical Einstein equation, our analysis suggests that the spheres approaching the inner horizon can undergo an infinite twisting due to quantum effects along latitudes separating regions of infinite expansion and contraction.","sentences":["We present a numerical calculation of the expectation value of the quantum angular-momentum current flux density for a scalar field in the Unruh state near the inner horizon of a Kerr-de Sitter black hole.","Our results indicate that this flux diverges as $V_-^{-1}$ in a suitable Kruskal coordinate such that $V_-=0$ at the inner horizon.","Depending on the parameter values of the scalar field and black hole that we consider, and depending on the polar angle (latitude), this flux can have different signs.","In the near extremal cases considered, the angle average of the expectation value of the quantum angular momentum current flux is of the opposite sign as the angular momentum of the background itself, suggesting that, in the cases considered, quantum effects tend to decrease the total angular momentum of the spheres away from the extremal value.","We also numerically calculate the energy flux component, which provides the leading order divergence of the quantum stress energy tensor, dominant over the classical stress energy tensor, at the inner horizon.","Taking our expectation value of the quantum stress tensor as the source in the semiclassical Einstein equation, our analysis suggests that the spheres approaching the inner horizon can undergo an infinite twisting due to quantum effects along latitudes separating regions of infinite expansion and contraction."],"url":"http://arxiv.org/abs/2402.14171v1","category":"gr-qc"}
{"created":"2024-02-21 22:34:18","title":"Solving Maxwells Equations using Variational Quantum Imaginary Time Evolution","abstract":"Maxwells equations are fundamental to our understanding of electromagnetic fields, but their solution can be computationally demanding, even for high-performance computing clusters. Quantum computers offer a promising alternative for solving these equations, as they can simulate larger and more complex systems more efficiently both in time and resources. In this paper we investigate the potential of using the variational quantum imaginary time evolution (VarQITE) algorithm on near-term quantum hardware to solve for the Maxwells equations. Our objective is to analyze the trade-off between the accuracy of the simulated fields and the depth of the quantum circuit required to implement the VarQITE algorithm. We demonstrate that VarQITE can efficiently approximate the solution of these equations with high accuracy, and show that its performance can be enhanced by optimizing the quantum circuit depth. Our findings suggest that VarQITE on near-term quantum devices could provide a powerful tool for solving PDEs in electromagnetics and other fields.","sentences":["Maxwells equations are fundamental to our understanding of electromagnetic fields, but their solution can be computationally demanding, even for high-performance computing clusters.","Quantum computers offer a promising alternative for solving these equations, as they can simulate larger and more complex systems more efficiently both in time and resources.","In this paper we investigate the potential of using the variational quantum imaginary time evolution (VarQITE) algorithm on near-term quantum hardware to solve for the Maxwells equations.","Our objective is to analyze the trade-off between the accuracy of the simulated fields and the depth of the quantum circuit required to implement the VarQITE algorithm.","We demonstrate that VarQITE can efficiently approximate the solution of these equations with high accuracy, and show that its performance can be enhanced by optimizing the quantum circuit depth.","Our findings suggest that VarQITE on near-term quantum devices could provide a powerful tool for solving PDEs in electromagnetics and other fields."],"url":"http://arxiv.org/abs/2402.14156v1","category":"quant-ph"}
{"created":"2024-02-21 22:11:01","title":"Neural Networks and Friction: Slide, Hold, Learn","abstract":"In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.","sentences":["In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data.","The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution.","A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training.","It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes."],"url":"http://arxiv.org/abs/2402.14148v1","category":"physics.geo-ph"}
{"created":"2024-02-21 21:31:46","title":"An analytical solution for vertical infiltration in bounded profiles","abstract":"In this study, we derive an analytical solution to address the problem of one-dimensional vertical infiltration within bounded profiles. We consider the Richards equation together with various boundary conditions, simulating different scenarios of water application onto the surface of a homogeneous and bounded medium. To solve the corresponding initial boundary value problem over a finite interval, we apply the unified transform, commonly known as the Fokas method. Through this methodology, we obtain an integral representation that can be efficiently and directly computed numerically, yielding a convergent scheme.","sentences":["In this study, we derive an analytical solution to address the problem of one-dimensional vertical infiltration within bounded profiles.","We consider the Richards equation together with various boundary conditions, simulating different scenarios of water application onto the surface of a homogeneous and bounded medium.","To solve the corresponding initial boundary value problem over a finite interval, we apply the unified transform, commonly known as the Fokas method.","Through this methodology, we obtain an integral representation that can be efficiently and directly computed numerically, yielding a convergent scheme."],"url":"http://arxiv.org/abs/2402.14138v1","category":"math.AP"}
{"created":"2024-02-21 20:51:57","title":"Attractors in $k$-dimensional discrete systems of mixed monotonicity","abstract":"We consider $k$-dimensional discrete-time systems of the form $x_{n+1}=F(x_n,\\ldots,x_{n-k+1})$ in which the map $F$ is continuous and monotonic in each one of its arguments. We define a partial order on $\\mathbb{R}^{2k}_+$, compatible with the monotonicity of $F$, and then use it to embed the $k$-dimensional system into a $2k$-dimensional system that is monotonic with respect to this poset structure. An analogous construction is given for periodic systems. Using the characteristics of the higher-dimensional monotonic system, global stability results are obtained for the original system. Our results apply to a large class of difference equations that are pertinent in a variety of contexts. As an application of the developed theory, we provide two examples that cover a wide class of difference equations, and in a subsequent paper, we provide additional applications of general interest.","sentences":["We consider $k$-dimensional discrete-time systems of the form $x_{n+1}=F(x_n,\\ldots,x_{n-k+1})$ in which the map $F$ is continuous and monotonic in each one of its arguments.","We define a partial order on $\\mathbb{R}^{2k}_+$, compatible with the monotonicity of $F$, and then use it to embed the $k$-dimensional system into a $2k$-dimensional system that is monotonic with respect to this poset structure.","An analogous construction is given for periodic systems.","Using the characteristics of the higher-dimensional monotonic system, global stability results are obtained for the original system.","Our results apply to a large class of difference equations that are pertinent in a variety of contexts.","As an application of the developed theory, we provide two examples that cover a wide class of difference equations, and in a subsequent paper, we provide additional applications of general interest."],"url":"http://arxiv.org/abs/2402.14127v1","category":"math.DS"}
{"created":"2024-02-21 20:49:44","title":"Integro-differential diffusion equations on graded Lie groups","abstract":"We first study the existence, uniqueness and well-posedness of a general class of integro-differential diffusion equation on $L^p(\\mathbb{G})$ $(1<p<+\\infty$, $\\mathbb{G}$ is a graded Lie group). We show the explicit solution of the considered equation by using the Fourier analysis of the group. The equation involves a nonlocal in time operator (with a general kernel) and a positive Rockland operator acting on $\\mathbb{G}.$ Also, we provide $L^p(\\mathbb{G})-L^q(\\mathbb{G})$ $(1<p\\leqslant 2\\leqslant q<+\\infty)$ norm estimates and time decay rate for the solutions. In fact, by using some contemporary results, one can translate the latter regularity problem to the study of boundedness of its propagator which strongly depends on the traces of the spectral projections of the Rockland operator. Moreover, in many cases, we can obtain time asymptotic decay for the solutions which depends intrinsically on the considered kernel. As a complement, we give some norm estimates for the solutions in terms of a homogeneous Sobolev space in $L^{2}(\\mathbb{G})$ that involves the Rockland operator. We also give a counterpart of our results in the setting of compact Lie groups. Illustrative examples are also given.","sentences":["We first study the existence, uniqueness and well-posedness of a general class of integro-differential diffusion equation on $L^p(\\mathbb{G})$ $(1<p<+\\infty$, $\\mathbb{G}$ is a graded Lie group).","We show the explicit solution of the considered equation by using the Fourier analysis of the group.","The equation involves a nonlocal in time operator (with a general kernel) and a positive Rockland operator acting on $\\mathbb{G}.$ Also, we provide $L^p(\\mathbb{G})-L^q(\\mathbb{G})$ $(1<p\\leqslant 2\\leqslant q<+\\infty)$ norm estimates and time decay rate for the solutions.","In fact, by using some contemporary results, one can translate the latter regularity problem to the study of boundedness of its propagator which strongly depends on the traces of the spectral projections of the Rockland operator.","Moreover, in many cases, we can obtain time asymptotic decay for the solutions which depends intrinsically on the considered kernel.","As a complement, we give some norm estimates for the solutions in terms of a homogeneous Sobolev space in $L^{2}(\\mathbb{G})$ that involves the Rockland operator.","We also give a counterpart of our results in the setting of compact Lie groups.","Illustrative examples are also given."],"url":"http://arxiv.org/abs/2402.14125v1","category":"math.AP"}
{"created":"2024-02-21 19:54:25","title":"Learning dynamic representations of the functional connectome in neurobiological networks","abstract":"The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an unsupervised approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times. The inference occurs in two major steps. First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF). Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals. Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome. Since time codes the different experimental variables (e.g., application of chemical stimuli), this provides an atlas of neural motifs active during separate stages of an experiment (e.g., stimulus application or spontaneous behaviors). Results from our analysis are experimentally validated, confirming that our method is able to robustly predict causal interactions between neurons to generate behavior. Code is available at https://github.com/dyballa/dynamic-connectomes.","sentences":["The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function.","As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times.","We introduce an unsupervised approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times.","The inference occurs in two major steps.","First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF).","Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals.","Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome.","Since time codes the different experimental variables (e.g., application of chemical stimuli), this provides an atlas of neural motifs active during separate stages of an experiment (e.g., stimulus application or spontaneous behaviors).","Results from our analysis are experimentally validated, confirming that our method is able to robustly predict causal interactions between neurons to generate behavior.","Code is available at https://github.com/dyballa/dynamic-connectomes."],"url":"http://arxiv.org/abs/2402.14102v1","category":"q-bio.NC"}
{"created":"2024-02-21 19:45:15","title":"Introductory visual lecture on QCD at large-$N_{c}$: bound states, chiral models, and phase diagram","abstract":"In these lectures, we present the behavior of conventional $\\bar{q}q$ mesons, glueballs, and hybrids in the large-$N_{c}$ limit of QCD. To this end, we use an approach based on rather simple NJL-like bound-state equations. The obtained large-$N_{c}$ scaling laws are general and coincide with the known results. A series of consequences, such as the narrowness of certain mesons and interaction types, the behavior of chiral and dilaton models at large-$N_{c},$ and the relation to the compositeness condition and the standard derivation of large-$N_{c}$ results, are explained. The bound-state formalism shows also that mesonic molecular and dynamically generated states do not form in the large-$N_{c}$ limit. Next, following the same approach, baryons are studied as bound states of a generalized diquark and a quark. Similarities and differences with regular mesons are discussed. All the standard scaling laws for baryons and their interaction with mesons are correctly reproduced. The behavior of chiral models involving baryons and describing chirally invariant mass generation is investigated. Finally, properties of QCD in the medium at large-$N_{c}$ are studied: the deconfinement phase transition is investigated along the temperature and the chemical potential directions. While the critical temperature for deconfinement $T_{dec}$ is $N_{c}$ independent$,$ the critical chemical potential is not and increases for growing $N_{c}$. In the confined phase but for large densities, one has a stiff-matter phase whose pressure is proportional to $N_{c}$ (just as a gas of quarks would do) in agreement with a quarkyonic phase. Within the QCD phase diagrams, the features of different models at large-$N_{c}$ are reviewed and the location of the critical endpoint is discussed. In the end, the very existence of nuclei and the implications of large-$N_{c}$ arguments for neutron stars are outlined.","sentences":["In these lectures, we present the behavior of conventional $\\bar{q}q$ mesons, glueballs, and hybrids in the large-$N_{c}$ limit of QCD.","To this end, we use an approach based on rather simple NJL-like bound-state equations.","The obtained large-$N_{c}$ scaling laws are general and coincide with the known results.","A series of consequences, such as the narrowness of certain mesons and interaction types, the behavior of chiral and dilaton models at large-$N_{c},$ and the relation to the compositeness condition and the standard derivation of large-$N_{c}$ results, are explained.","The bound-state formalism shows also that mesonic molecular and dynamically generated states do not form in the large-$N_{c}$ limit.","Next, following the same approach, baryons are studied as bound states of a generalized diquark and a quark.","Similarities and differences with regular mesons are discussed.","All the standard scaling laws for baryons and their interaction with mesons are correctly reproduced.","The behavior of chiral models involving baryons and describing chirally invariant mass generation is investigated.","Finally, properties of QCD in the medium at large-$N_{c}$ are studied: the deconfinement phase transition is investigated along the temperature and the chemical potential directions.","While the critical temperature for deconfinement $T_{dec}$ is $N_{c}$ independent$,$ the critical chemical potential is not and increases for growing $N_{c}$. In the confined phase but for large densities, one has a stiff-matter phase whose pressure is proportional to $N_{c}$ (just as a gas of quarks would do) in agreement with a quarkyonic phase.","Within the QCD phase diagrams, the features of different models at large-$N_{c}$ are reviewed and the location of the critical endpoint is discussed.","In the end, the very existence of nuclei and the implications of large-$N_{c}$ arguments for neutron stars are outlined."],"url":"http://arxiv.org/abs/2402.14097v1","category":"hep-ph"}
{"created":"2024-02-21 19:29:35","title":"Effect of retarded friction and added mass on the swimming speed of a vibrating two-sphere","abstract":"A theoretical expression is derived for the mechanical contribution to the mean swimming speed of a vibrating two-sphere immersed in a viscous incompressible fluid. The two spheres are connected by an elastic spring which provides a harmonic potential for oscillations about a mean distance between centers. The system is made to oscillate at a chosen frequency by activating forces which sum to zero. The mechanical contribution to the resulting mean swimming velocity is calculated from the mechanical equations of motion and the corresponding impedance matrix of linear response. The frequency-dependent pair friction coefficients are found from approximate expressions derived earlier. The mechanical contribution is calculated to second order in the amplitude of stroke as a function of the scaling number, a dimensionless combination of size, frequency, and kinematic viscosity. Retarded friction and added mass determine the functional behavior.","sentences":["A theoretical expression is derived for the mechanical contribution to the mean swimming speed of a vibrating two-sphere immersed in a viscous incompressible fluid.","The two spheres are connected by an elastic spring which provides a harmonic potential for oscillations about a mean distance between centers.","The system is made to oscillate at a chosen frequency by activating forces which sum to zero.","The mechanical contribution to the resulting mean swimming velocity is calculated from the mechanical equations of motion and the corresponding impedance matrix of linear response.","The frequency-dependent pair friction coefficients are found from approximate expressions derived earlier.","The mechanical contribution is calculated to second order in the amplitude of stroke as a function of the scaling number, a dimensionless combination of size, frequency, and kinematic viscosity.","Retarded friction and added mass determine the functional behavior."],"url":"http://arxiv.org/abs/2402.14091v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 19:05:45","title":"$\\mathcal{O}\\,(1/\\sqrt\u03c9)$ anomaly in Brans-Dicke gravity with trace-carrying matter","abstract":"We present an exact static spherisymmetric solution for the Brans-Dicke action sourced by a self-gravitating massless Klein-Gordon helicity-0 field. In contrast to the Maxwell electromagnetic field, a Klein-Gordon field possesses an energy-momentum tensor with $\\textit{non-vanishing trace}$. Upon a Weyl mapping into the Einstein frame, the transformed Brans-Dicke scalar field takes on the role of a \"dilaton\" coupled with the Klein-Gordon field. Despite this dilatonic coupling, the field equations of the resulting Einstein-Klein-Gordon-dilaton action are fully soluble when employing the harmonic radial coordinate. The exact solution derived herein can serve as a prototype for future Brans-Dicke gravity studies involving trace-carrying matter fields. Notably, in the limit of infinite $\\omega$, the Brans-Dicke scalar field exhibits an anomalous behavior of ${\\cal O}\\,(1/\\sqrt\\omega)$ as opposed to ${\\cal O}\\,(1/\\omega)$. As a consequence, the solution converges to a spacetime configuration of General Relativity sourced by the original Klein-Gordon field and a free scalar field, the latter of which is the ${\\cal O}\\,(1/\\sqrt\\omega)$ \"remnant\" of the Brans-Dicke scalar field. Furthermore, we provide a formal mathematical proof substantiating these two conclusions. Although the ${\\cal O}\\,(1/\\sqrt\\omega)$ anomaly has been previously discovered for Brans-Dicke vacuum and Brans-Dicke-Maxwell electrovacuum, our findings establish its prevalence in Brans-Dicke gravity $\\textit{regardless}$ of the trace of the energy-momentum tensor of the source. Taken together, the ${\\cal O}\\,(1/\\sqrt\\omega)$ anomaly challenges the conventional belief in the ${\\cal O}\\,(1/\\omega)$ signature commonly associated with Brans-Dicke gravity. In particular, it may have implications in improving the relativistic corrections to Newtonian gravity beyond the weak-field parametrized post-Newtonian formalism.","sentences":["We present an exact static spherisymmetric solution for the Brans-Dicke action sourced by a self-gravitating massless Klein-Gordon helicity-0 field.","In contrast to the Maxwell electromagnetic field, a Klein-Gordon field possesses an energy-momentum tensor with $\\textit{non-vanishing trace}$. Upon a Weyl mapping into the Einstein frame, the transformed Brans-Dicke scalar field takes on the role of a \"dilaton\" coupled with the Klein-Gordon field.","Despite this dilatonic coupling, the field equations of the resulting Einstein-Klein-Gordon-dilaton action are fully soluble when employing the harmonic radial coordinate.","The exact solution derived herein can serve as a prototype for future Brans-Dicke gravity studies involving trace-carrying matter fields.","Notably, in the limit of infinite $\\omega$, the Brans-Dicke scalar field exhibits an anomalous behavior of ${\\cal O}\\,(1/\\sqrt\\omega)$ as opposed to ${\\cal O}\\,(1/\\omega)$. As a consequence, the solution converges to a spacetime configuration of General Relativity sourced by the original Klein-Gordon field and a free scalar field, the latter of which is the ${\\cal O}\\,(1/\\sqrt\\omega)$ \"remnant\" of the Brans-Dicke scalar field.","Furthermore, we provide a formal mathematical proof substantiating these two conclusions.","Although the ${\\cal O}\\,(1/\\sqrt\\omega)$ anomaly has been previously discovered for Brans-Dicke vacuum and Brans-Dicke-Maxwell electrovacuum, our findings establish its prevalence in Brans-Dicke gravity $\\textit{regardless}$ of the trace of the energy-momentum tensor of the source.","Taken together, the ${\\cal O}\\,(1/\\sqrt\\omega)$ anomaly challenges the conventional belief in the ${\\cal O}\\,(1/\\omega)$ signature commonly associated with Brans-Dicke gravity.","In particular, it may have implications in improving the relativistic corrections to Newtonian gravity beyond the weak-field parametrized post-Newtonian formalism."],"url":"http://arxiv.org/abs/2402.14076v1","category":"gr-qc"}
{"created":"2024-02-21 19:00:03","title":"Absence of localization in Weyl semimetals","abstract":"One of the fundamental facts of condensed matter physics is that sufficient amount of disorder always turns a Fermi liquid metal into an Anderson insulator: a compressible, but non-conducting phase of matter. Recently, topological semimetals have emerged as another way a metallic phase may be realized. In this paper we point out that, unlike ordinary metals, at least some topological semimetals are immune to localization, provided certain conditions are satisfied. We present several physical arguments, based on diagrammatic perturbation theory and Keldysh field theory, as well as domain wall network construction, to back up this claim.","sentences":["One of the fundamental facts of condensed matter physics is that sufficient amount of disorder always turns a Fermi liquid metal into an Anderson insulator: a compressible, but non-conducting phase of matter.","Recently, topological semimetals have emerged as another way a metallic phase may be realized.","In this paper we point out that, unlike ordinary metals, at least some topological semimetals are immune to localization, provided certain conditions are satisfied.","We present several physical arguments, based on diagrammatic perturbation theory and Keldysh field theory, as well as domain wall network construction, to back up this claim."],"url":"http://arxiv.org/abs/2402.14063v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-21 19:00:02","title":"Many-body effects on superconductivity mediated by double-magnon processes in altermagnets","abstract":"Altermagnets exhibit a large electron spin splitting which can be understood as a result of strong coupling between itinerant electrons and localized spins. We consider superconductivity due to electron-magnon scattering, using strong-coupling Eliashberg theory to capture many-body effects that are not covered by a weak-coupling approach. The characteristic band structure of altermagnets puts significant constraints on the spin structure of electron scattering on the Fermi surface. We emphasize the role of spin-preserving, double-magnon scattering processes compared to conventional spin-flip processes involving a single magnon. Then, we derive the Eliashberg equations for a situation where double-magnon scattering mediates spin-polarized Cooper pairs, while both double-magnon and single-magnon scatterings contribute to many-body effects. These many-body effects impact superconducting properties in a way that differs significantly from systems where conventional spin-flip processes mediate superconductivity. To highlight the role of $d$-wave magnetism on superconductivity in altermagnets, we compare our results to those found in ferromagnetic half-metals and conventional antiferromagnetic metals.","sentences":["Altermagnets exhibit a large electron spin splitting which can be understood as a result of strong coupling between itinerant electrons and localized spins.","We consider superconductivity due to electron-magnon scattering, using strong-coupling Eliashberg theory to capture many-body effects that are not covered by a weak-coupling approach.","The characteristic band structure of altermagnets puts significant constraints on the spin structure of electron scattering on the Fermi surface.","We emphasize the role of spin-preserving, double-magnon scattering processes compared to conventional spin-flip processes involving a single magnon.","Then, we derive the Eliashberg equations for a situation where double-magnon scattering mediates spin-polarized Cooper pairs, while both double-magnon and single-magnon scatterings contribute to many-body effects.","These many-body effects impact superconducting properties in a way that differs significantly from systems where conventional spin-flip processes mediate superconductivity.","To highlight the role of $d$-wave magnetism on superconductivity in altermagnets, we compare our results to those found in ferromagnetic half-metals and conventional antiferromagnetic metals."],"url":"http://arxiv.org/abs/2402.14061v1","category":"cond-mat.supr-con"}
{"created":"2024-02-21 18:56:03","title":"D-Flow: Differentiating through Flows for Controlled Generation","abstract":"Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.","sentences":["Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general.","In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point.","We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process.","We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all."],"url":"http://arxiv.org/abs/2402.14017v1","category":"cs.LG"}
{"created":"2024-02-21 18:50:12","title":"Geometry-Informed Neural Networks","abstract":"We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.","sentences":["We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks.","Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints.","We add an explicit diversity loss to mitigate mode collapse.","We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory.","Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity."],"url":"http://arxiv.org/abs/2402.14009v1","category":"cs.LG"}
{"created":"2024-02-21 18:35:27","title":"Asymptotics of Learning with Deep Structured (Random) Features","abstract":"For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. This characterization is formulated in terms of the population covariance of the features. Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.","sentences":["For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large.","This characterization is formulated in terms of the population covariance of the features.","Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers.","For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices.","We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent."],"url":"http://arxiv.org/abs/2402.13999v1","category":"stat.ML"}
{"created":"2024-02-21 18:17:17","title":"Hamiltonian Descent and Coordinate Hamiltonian Descent","abstract":"We propose an optimization algorithm called Hamiltonian Descent, which is a direct counterpart of classical Hamiltonian Monte Carlo in sampling. We show that Hamiltonian Descent has a better convergence rate than the Chebyshev method for solving a linear system of equations, albeit with a high computational cost. To overcome the cost issue, we then propose Coordinate Hamiltonian Descent and its parallelizable variant, which turns out to encapsulate the classical Gauss-Seidel method, Successive Over-relaxation, Jacobi method, and more. The result not only offers a new perspective on these existing algorithms but also leads to a broader class of update schemes that guarantee the convergence.","sentences":["We propose an optimization algorithm called Hamiltonian Descent, which is a direct counterpart of classical Hamiltonian Monte Carlo in sampling.","We show that Hamiltonian Descent has a better convergence rate than the Chebyshev method for solving a linear system of equations, albeit with a high computational cost.","To overcome the cost issue, we then propose Coordinate Hamiltonian Descent and its parallelizable variant, which turns out to encapsulate the classical Gauss-Seidel method, Successive Over-relaxation, Jacobi method, and more.","The result not only offers a new perspective on these existing algorithms but also leads to a broader class of update schemes that guarantee the convergence."],"url":"http://arxiv.org/abs/2402.13988v1","category":"math.OC"}
{"created":"2024-02-21 18:16:48","title":"A Simple and Yet Fairly Effective Defense for Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data. However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs. To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture. We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity. The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures. Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results. Our code is publicly available at: https://github.com/Sennadir/NoisyGNN.","sentences":["Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data.","However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations.","Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs.","To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture.","We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach.","We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN.","The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity.","The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures.","Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results.","Our code is publicly available at: https://github.com/Sennadir/NoisyGNN."],"url":"http://arxiv.org/abs/2402.13987v1","category":"cs.LG"}
{"created":"2024-02-21 18:12:07","title":"Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators","abstract":"Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures. In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables. In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger. As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets.","sentences":["Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations.","However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales.","To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs.","StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable.","The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities.","We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures.","In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables.","In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger.","As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets."],"url":"http://arxiv.org/abs/2402.13984v1","category":"cs.LG"}
{"created":"2024-02-21 18:09:12","title":"Geometry-induced wavefunction collapse","abstract":"When a quantum particle moves in a curved space, a geometric potential can arise. In spite of a long history of extensive theoretical studies, to experimentally observe the geometric potential remains to be a challenge. What are the physically observable consequences of such a geometric potential? Solving the Schrodinger equation on a truncated conic surface, we uncover a class of quantum scattering states that bear a strong resemblance with the quasi-resonant states associated with atomic collapse about a Coulomb impurity, a remarkable quantum phenomenon in which an infinite number of quasi-resonant states emerge. A characteristic defining feature of such collapse states is the infinite oscillations of the local density of states (LDOS) about the zero energy point separating the scattering from the bound states. The emergence of such states in the curved (Riemannian) space requires neither a relativistic quantum mechanism nor any Coulomb impurity: they have zero angular momentum and their origin is purely geometrical - henceforth the term geometry-induced wavefunction collapse. We establish the collapsing nature of these states through a detailed comparative analysis of the behavior of the LDOS for both the zero and finite angular-momentum states as well as the corresponding classical picture. Potential experimental schemes to realize the geometry-induced collapse states are articulated. Not only has our study uncovered an intrinsic connection between the geometric potential and atomic collapse, it also provides a method to experimentally observe and characterize geometric potentials arising from different subfields of physics. For example, in nanoscience and nanotechnology, curved geometry has become increasingly common. Our finding suggests that wavefunction collapse should be an important factor of consideration in designing and developing nanodevices.","sentences":["When a quantum particle moves in a curved space, a geometric potential can arise.","In spite of a long history of extensive theoretical studies, to experimentally observe the geometric potential remains to be a challenge.","What are the physically observable consequences of such a geometric potential?","Solving the Schrodinger equation on a truncated conic surface, we uncover a class of quantum scattering states that bear a strong resemblance with the quasi-resonant states associated with atomic collapse about a Coulomb impurity, a remarkable quantum phenomenon in which an infinite number of quasi-resonant states emerge.","A characteristic defining feature of such collapse states is the infinite oscillations of the local density of states (LDOS) about the zero energy point separating the scattering from the bound states.","The emergence of such states in the curved (Riemannian) space requires neither a relativistic quantum mechanism nor any Coulomb impurity: they have zero angular momentum and their origin is purely geometrical - henceforth the term geometry-induced wavefunction collapse.","We establish the collapsing nature of these states through a detailed comparative analysis of the behavior of the LDOS for both the zero and finite angular-momentum states as well as the corresponding classical picture.","Potential experimental schemes to realize the geometry-induced collapse states are articulated.","Not only has our study uncovered an intrinsic connection between the geometric potential and atomic collapse, it also provides a method to experimentally observe and characterize geometric potentials arising from different subfields of physics.","For example, in nanoscience and nanotechnology, curved geometry has become increasingly common.","Our finding suggests that wavefunction collapse should be an important factor of consideration in designing and developing nanodevices."],"url":"http://arxiv.org/abs/2402.13980v1","category":"quant-ph"}
{"created":"2024-02-21 17:58:10","title":"Linear-Time Graph Neural Networks for Scalable Recommendations","abstract":"In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.","sentences":["In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users.","The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions.","Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems.","Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages.","Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods.","In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy.","Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm.","Our implementation based on PyTorch is available."],"url":"http://arxiv.org/abs/2402.13973v1","category":"cs.IR"}
{"created":"2024-02-21 17:57:40","title":"Multi-indice B-series","abstract":"We propose a novel way to describe numerical methods for ordinary differential equations via the notion of multi-indice. The main idea is to replace rooted trees in Butcher's B-series by multi-indices. The latter were introduced recently in the context of describing solutions of singular stochastic partial differential equations. The combinatorial shift away from rooted trees allows for a compressed description of numerical schemes. Moreover, these multi-indices B-series characterise uniquely the Taylor development of local and affine equivariant maps.","sentences":["We propose a novel way to describe numerical methods for ordinary differential equations via the notion of multi-indice.","The main idea is to replace rooted trees in Butcher's B-series by multi-indices.","The latter were introduced recently in the context of describing solutions of singular stochastic partial differential equations.","The combinatorial shift away from rooted trees allows for a compressed description of numerical schemes.","Moreover, these multi-indices B-series characterise uniquely the Taylor development of local and affine equivariant maps."],"url":"http://arxiv.org/abs/2402.13971v1","category":"math.NA"}
{"created":"2024-02-21 17:36:07","title":"Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment","abstract":"Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text. We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers.","sentences":["Do LMs infer the semantics of text from co-occurrence patterns in their training data?","Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al.","In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs.","We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs.","This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns.","However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test.","We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text.","We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers."],"url":"http://arxiv.org/abs/2402.13956v1","category":"cs.CL"}
{"created":"2024-02-21 17:35:51","title":"BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions","abstract":"In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.","sentences":["In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language.","To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET.","We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process.","Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space.","Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation.","To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE).","Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%."],"url":"http://arxiv.org/abs/2402.13955v1","category":"cs.CV"}
{"created":"2024-02-21 17:29:05","title":"A Post-Newtonian Analysis of Regularized 4D-EGB Theory: Complete Set of PPN Parameters and Observational Constraints","abstract":"We performed a post-Newtonian analysis of the regularized four-dimensional Einstein-Gauss-Bonnet gravitational theory (4D-EGB). The resulting metric differs from the classical parametrized post-Newtonian (PPN) formalism in that a new gravitational potential arises from the integration of the approximate field equations. We also investigated the conserved quantities and equations of motion for massive bodies and light rays to a certain degree. By computing the predicted periastron advance rate in a binary system, we obtained an observational constraint that is stronger than those of previous analyses. Although the usual 10 PPN parameters can still be derived within the PPN framework, an extra parameter is needed to account for the full post-Newtonian tests.","sentences":["We performed a post-Newtonian analysis of the regularized four-dimensional Einstein-Gauss-Bonnet gravitational theory (4D-EGB).","The resulting metric differs from the classical parametrized post-Newtonian (PPN) formalism in that a new gravitational potential arises from the integration of the approximate field equations.","We also investigated the conserved quantities and equations of motion for massive bodies and light rays to a certain degree.","By computing the predicted periastron advance rate in a binary system, we obtained an observational constraint that is stronger than those of previous analyses.","Although the usual 10 PPN parameters can still be derived within the PPN framework, an extra parameter is needed to account for the full post-Newtonian tests."],"url":"http://arxiv.org/abs/2402.13951v1","category":"gr-qc"}
{"created":"2024-02-21 17:23:29","title":"Improved Syndrome-based Neural Decoder for Linear Block Codes","abstract":"In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes. We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable. The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime.","sentences":["In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes.","We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable.","The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime."],"url":"http://arxiv.org/abs/2402.13948v2","category":"cs.IT"}
{"created":"2024-02-21 17:18:25","title":"AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning","abstract":"Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all GNNs considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated. For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against GNNs for all classes of problems.","sentences":["Machine learning has shown great promise in addressing several critical hardware security problems.","In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few.","These techniques have demonstrated outstanding accuracy and have received much attention in the community.","However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   ","In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security.","To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques.","We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent.","We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation.","Through our approach, we craft circuits that fool all GNNs considered in this work.","For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated.","For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits.","We obtain a similar 100% success rate against GNNs for all classes of problems."],"url":"http://arxiv.org/abs/2402.13946v1","category":"cs.LG"}
{"created":"2024-02-21 17:11:07","title":"The Maintenance of Coherent Vortex Topology by Lagrangian Chaos in Drift-Rossby Wave Turbulence","abstract":"This work introduces the \"potential vorticity (PV) bucket brigade\", a conceptual mechanism for explaining the resilience of coherent vortex structures in drift-Rossby wave turbulence, critical for understanding turbulent transport in magnetically confined fusion plasmas and geophysical flows. Drawing parallels with the PV staircase, we show how spatially inhomogeneous patterns of mixing can reinforce, rather than destroy non-zonal structures in the flow. We accomplish this through an exact stochastic Lagrangian representation of vorticity transport, together with a wave eigenmode near-integrability property which elucidates the relationship between coherent flow topology and fluid relabeling symmetries. For concreteness, we demonstrate these ideas in a transitional regime of gradient-driven drift wave turbulence modeled by the flux-balanced Hasegawa-Wakatani equations. However, the tools we develop here are model-agnostic and have potential relevance to fluid and plasma systems well beyond the model studied here.","sentences":["This work introduces the \"potential vorticity (PV) bucket brigade\", a conceptual mechanism for explaining the resilience of coherent vortex structures in drift-Rossby wave turbulence, critical for understanding turbulent transport in magnetically confined fusion plasmas and geophysical flows.","Drawing parallels with the PV staircase, we show how spatially inhomogeneous patterns of mixing can reinforce, rather than destroy non-zonal structures in the flow.","We accomplish this through an exact stochastic Lagrangian representation of vorticity transport, together with a wave eigenmode near-integrability property which elucidates the relationship between coherent flow topology and fluid relabeling symmetries.","For concreteness, we demonstrate these ideas in a transitional regime of gradient-driven drift wave turbulence modeled by the flux-balanced Hasegawa-Wakatani equations.","However, the tools we develop here are model-agnostic and have potential relevance to fluid and plasma systems well beyond the model studied here."],"url":"http://arxiv.org/abs/2402.13942v1","category":"physics.plasm-ph"}
{"created":"2024-02-21 17:09:29","title":"On the topological classification of complex plane curve singularities","abstract":"This final degree project is devoted to study the topological classification of complex plane curves. These are subsets of $\\mathbb{C}^2$ that can be described by an equation $f(x,y)=0$. Loosely speaking, curves are said to be equivalent in a topological sense whenever they are ambient homeomorphic, i.e., there exists an orientation-preserving homeomorphism of the ambient space carrying one curve to the other. The project's aim is to develop operative and clear conditions to determine whether two curves are equivalent. Curves will be shown to be decomposable into branches: sets that can be explicitly parametrised in the form $x=\\phi(t), y=\\psi(t)$. These parametric expressions will be analysed to extract a complete numerical invariant for the classification of branches: the Puiseux characteristic. The intermediate key result to lend this notion with topological weight is that a branch can be completely described through its associated knot, arising from the intersection of the branch with a small enough 3-sphere. The combination of these above-mentioned facts will then culminate in the project's most powerful result, which assures that two curves are equivalent if and only if their branches share the same Puiseux characteristics and intersection numbers.","sentences":["This final degree project is devoted to study the topological classification of complex plane curves.","These are subsets of $\\mathbb{C}^2$ that can be described by an equation $f(x,y)=0$. Loosely speaking, curves are said to be equivalent in a topological sense whenever they are ambient homeomorphic, i.e., there exists an orientation-preserving homeomorphism of the ambient space carrying one curve to the other.","The project's aim is to develop operative and clear conditions to determine whether two curves are equivalent.","Curves will be shown to be decomposable into branches: sets that can be explicitly parametrised in the form $x=\\phi(t),","y=\\psi(t)$. These parametric expressions will be analysed to extract a complete numerical invariant for the classification of branches: the Puiseux characteristic.","The intermediate key result to lend this notion with topological weight is that a branch can be completely described through its associated knot, arising from the intersection of the branch with a small enough 3-sphere.","The combination of these above-mentioned facts will then culminate in the project's most powerful result, which assures that two curves are equivalent if and only if their branches share the same Puiseux characteristics and intersection numbers."],"url":"http://arxiv.org/abs/2402.13941v1","category":"math.AG"}
{"created":"2024-02-21 17:05:27","title":"Verifying message-passing neural networks via topology-based bounds tightening","abstract":"Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classification problems and consider topological attacks that both add and remove edges.","sentences":["Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them.","We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function.","Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications.","Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds.","We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds.","To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP.","We test on both node and graph classification problems and consider topological attacks that both add and remove edges."],"url":"http://arxiv.org/abs/2402.13937v1","category":"math.OC"}
{"created":"2024-02-21 17:00:41","title":"Powerful Large-scale Inference in High Dimensional Mediation Analysis","abstract":"In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation. Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects. Testing for mediation effects lead to a composite null hypothesis. Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden. To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region. We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al).","sentences":["In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation.","Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects.","Testing for mediation effects lead to a composite null hypothesis.","Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden.","To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region.","We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al)."],"url":"http://arxiv.org/abs/2402.13933v1","category":"stat.ME"}
{"created":"2024-02-21 16:31:45","title":"Bias correction of wind power forecasts with SCADA data and continuous learning","abstract":"Wind energy plays a critical role in the transition towards renewable energy sources. However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity. To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling. In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models. Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model. The models are evaluated on datasets from a wind park comprising 65 wind turbines. The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts. Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline. Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available.","sentences":["Wind energy plays a critical role in the transition towards renewable energy sources.","However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity.","To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling.","In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models.","Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model.","The models are evaluated on datasets from a wind park comprising 65 wind turbines.","The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts.","Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline.","Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available."],"url":"http://arxiv.org/abs/2402.13916v1","category":"cs.LG"}
{"created":"2024-02-22 18:06:19","title":"Scaling Efficient LLMs","abstract":"Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \\sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.","sentences":["Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency.","In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus.","Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size.","Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \\sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills."],"url":"http://arxiv.org/abs/2402.14746v1","category":"cs.CL"}
{"created":"2024-02-22 15:09:13","title":"latrend: A Framework for Clustering Longitudinal Data","abstract":"Clustering of longitudinal data is used to explore common trends among subjects over time for a numeric measurement of interest. Various R packages have been introduced throughout the years for identifying clusters of longitudinal patterns, summarizing the variability in trajectories between subject in terms of one or more trends. We introduce the R package \"latrend\" as a framework for the unified application of methods for longitudinal clustering, enabling comparisons between methods with minimal coding. The package also serves as an interface to commonly used packages for clustering longitudinal data, including \"dtwclust\", \"flexmix\", \"kml\", \"lcmm\", \"mclust\", \"mixAK\", and \"mixtools\". This enables researchers to easily compare different approaches, implementations, and method specifications. Furthermore, researchers can build upon the standard tools provided by the framework to quickly implement new cluster methods, enabling rapid prototyping. We demonstrate the functionality and application of the latrend package on a synthetic dataset based on the therapy adherence patterns of patients with sleep apnea.","sentences":["Clustering of longitudinal data is used to explore common trends among subjects over time for a numeric measurement of interest.","Various R packages have been introduced throughout the years for identifying clusters of longitudinal patterns, summarizing the variability in trajectories between subject in terms of one or more trends.","We introduce the R package \"latrend\" as a framework for the unified application of methods for longitudinal clustering, enabling comparisons between methods with minimal coding.","The package also serves as an interface to commonly used packages for clustering longitudinal data, including \"dtwclust\", \"flexmix\", \"kml\", \"lcmm\", \"mclust\", \"mixAK\", and \"mixtools\".","This enables researchers to easily compare different approaches, implementations, and method specifications.","Furthermore, researchers can build upon the standard tools provided by the framework to quickly implement new cluster methods, enabling rapid prototyping.","We demonstrate the functionality and application of the latrend package on a synthetic dataset based on the therapy adherence patterns of patients with sleep apnea."],"url":"http://arxiv.org/abs/2402.14621v1","category":"cs.LG"}
{"created":"2024-02-22 14:33:54","title":"Multivariate Online Linear Regression for Hierarchical Forecasting","abstract":"In this paper, we consider a deterministic online linear regression model where we allow the responses to be multivariate. To address this problem, we introduce MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth algorithm to the multivariate setting, and show that it also enjoys logarithmic regret in time. We apply our results to the online hierarchical forecasting problem and recover an algorithm from this literature as a special case, allowing us to relax the hypotheses usually made for its analysis.","sentences":["In this paper, we consider a deterministic online linear regression model where we allow the responses to be multivariate.","To address this problem, we introduce MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth algorithm to the multivariate setting, and show that it also enjoys logarithmic regret in time.","We apply our results to the online hierarchical forecasting problem and recover an algorithm from this literature as a special case, allowing us to relax the hypotheses usually made for its analysis."],"url":"http://arxiv.org/abs/2402.14578v1","category":"stat.ML"}
{"created":"2024-02-22 14:04:41","title":"Self-supervised Visualisation of Medical Image Datasets","abstract":"Self-supervised learning methods based on data augmentations, such as SimCLR, BYOL, or DINO, allow obtaining semantically meaningful representations of image datasets and are widely used prior to supervised fine-tuning. A recent self-supervised learning method, $t$-SimCNE, uses contrastive learning to directly train a 2D representation suitable for visualisation. When applied to natural image datasets, $t$-SimCNE yields 2D visualisations with semantically meaningful clusters. In this work, we used $t$-SimCNE to visualise medical image datasets, including examples from dermatology, histology, and blood microscopy. We found that increasing the set of data augmentations to include arbitrary rotations improved the results in terms of class separability, compared to data augmentations used for natural images. Our 2D representations show medically relevant structures and can be used to aid data exploration and annotation, improving on common approaches for data visualisation.","sentences":["Self-supervised learning methods based on data augmentations, such as SimCLR, BYOL, or DINO, allow obtaining semantically meaningful representations of image datasets and are widely used prior to supervised fine-tuning.","A recent self-supervised learning method, $t$-SimCNE, uses contrastive learning to directly train a 2D representation suitable for visualisation.","When applied to natural image datasets, $t$-SimCNE yields 2D visualisations with semantically meaningful clusters.","In this work, we used $t$-SimCNE to visualise medical image datasets, including examples from dermatology, histology, and blood microscopy.","We found that increasing the set of data augmentations to include arbitrary rotations improved the results in terms of class separability, compared to data augmentations used for natural images.","Our 2D representations show medically relevant structures and can be used to aid data exploration and annotation, improving on common approaches for data visualisation."],"url":"http://arxiv.org/abs/2402.14566v1","category":"cs.CV"}
{"created":"2024-02-22 13:38:47","title":"Transition State Clustering for Interaction Segmentation and Learning","abstract":"Hidden Markov Models with an underlying Mixture of Gaussian structure have proven effective in learning Human-Robot Interactions from demonstrations for various interactive tasks via Gaussian Mixture Regression. However, a mismatch occurs when segmenting the interaction using only the observed state of the human compared to the joint state of the human and the robot. To enhance this underlying segmentation and subsequently the predictive abilities of such Gaussian Mixture-based approaches, we take a hierarchical approach by learning an additional mixture distribution on the states at the transition boundary. This helps prevent misclassifications that usually occur in such states. We find that our framework improves the performance of the underlying Gaussian Mixture-based approach, which we evaluate on various interactive tasks such as handshaking and fistbumps.","sentences":["Hidden Markov Models with an underlying Mixture of Gaussian structure have proven effective in learning Human-Robot Interactions from demonstrations for various interactive tasks via Gaussian Mixture Regression.","However, a mismatch occurs when segmenting the interaction using only the observed state of the human compared to the joint state of the human and the robot.","To enhance this underlying segmentation and subsequently the predictive abilities of such Gaussian Mixture-based approaches, we take a hierarchical approach by learning an additional mixture distribution on the states at the transition boundary.","This helps prevent misclassifications that usually occur in such states.","We find that our framework improves the performance of the underlying Gaussian Mixture-based approach, which we evaluate on various interactive tasks such as handshaking and fistbumps."],"url":"http://arxiv.org/abs/2402.14548v1","category":"cs.RO"}
{"created":"2024-02-22 13:21:26","title":"Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs","abstract":"Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications. For example, precision medicine tailors medical treatments to patients on the basis of individual biomarkers, cellular and molecular states, etc. However, the data required is sensitive, voluminous, heterogeneous, and typically distributed across locations where dedicated machine learning hardware is not available. Due to privacy and regulatory reasons, it is also problematic to aggregate all data at a trusted third party.Federated learning is a promising solution to this dilemma, because it enables decentralized, collaborative machine learning without exchanging raw data. In this paper, we perform comparative experiments with the federated learning frameworks TensorFlow Federated and Flower. Our test case is the training of disease prognosis and cell type classification models. We train the models with distributed transcriptomic data, considering both data heterogeneity and architectural heterogeneity. We measure model quality, robustness against privacy-enhancing noise, computational performance and resource overhead. Each of the federated learning frameworks has different strengths. However, our experiments confirm that both frameworks can readily build models on transcriptomic data, without transferring personal raw data to a third party with abundant computational resources.","sentences":["Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications.","For example, precision medicine tailors medical treatments to patients on the basis of individual biomarkers, cellular and molecular states, etc.","However, the data required is sensitive, voluminous, heterogeneous, and typically distributed across locations where dedicated machine learning hardware is not available.","Due to privacy and regulatory reasons, it is also problematic to aggregate all data at a trusted third party.","Federated learning is a promising solution to this dilemma, because it enables decentralized, collaborative machine learning without exchanging raw data.","In this paper, we perform comparative experiments with the federated learning frameworks TensorFlow Federated and Flower.","Our test case is the training of disease prognosis and cell type classification models.","We train the models with distributed transcriptomic data, considering both data heterogeneity and architectural heterogeneity.","We measure model quality, robustness against privacy-enhancing noise, computational performance and resource overhead.","Each of the federated learning frameworks has different strengths.","However, our experiments confirm that both frameworks can readily build models on transcriptomic data, without transferring personal raw data to a third party with abundant computational resources."],"url":"http://arxiv.org/abs/2402.14527v1","category":"cs.LG"}
{"created":"2024-02-22 13:15:49","title":"Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition","abstract":"We often verbally express emotions in a multifaceted manner, they may vary in their intensities and may be expressed not just as a single but as a mixture of emotions. This wide spectrum of emotions is well-studied in the structural model of emotions, which represents variety of emotions as derivative products of primary emotions with varying degrees of intensity. In this paper, we propose an emotional text-to-speech design to simulate a wider spectrum of emotions grounded on the structural model. Our proposed design, Daisy-TTS, incorporates a prosody encoder to learn emotionally-separable prosody embedding as a proxy for emotion. This emotion representation allows the model to simulate: (1) Primary emotions, as learned from the training samples, (2) Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by scaling the emotion embedding, and (4) Emotions polarity, by negating the emotion embedding. Through a series of perceptual evaluations, Daisy-TTS demonstrated overall higher emotional speech naturalness and emotion perceiveability compared to the baseline.","sentences":["We often verbally express emotions in a multifaceted manner, they may vary in their intensities and may be expressed not just as a single but as a mixture of emotions.","This wide spectrum of emotions is well-studied in the structural model of emotions, which represents variety of emotions as derivative products of primary emotions with varying degrees of intensity.","In this paper, we propose an emotional text-to-speech design to simulate a wider spectrum of emotions grounded on the structural model.","Our proposed design, Daisy-TTS, incorporates a prosody encoder to learn emotionally-separable prosody embedding as a proxy for emotion.","This emotion representation allows the model to simulate: (1) Primary emotions, as learned from the training samples, (2) Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by scaling the emotion embedding, and (4) Emotions polarity, by negating the emotion embedding.","Through a series of perceptual evaluations, Daisy-TTS demonstrated overall higher emotional speech naturalness and emotion perceiveability compared to the baseline."],"url":"http://arxiv.org/abs/2402.14523v1","category":"cs.CL"}
{"created":"2024-02-22 12:57:15","title":"Deep vessel segmentation based on a new combination of vesselness filters","abstract":"Vascular segmentation represents a crucial clinical task, yet its automation remains challenging. Because of the recent strides in deep learning, vesselness filters, which can significantly aid the learning process, have been overlooked. This study introduces an innovative filter fusion method crafted to amplify the effectiveness of vessel segmentation models. Our investigation seeks to establish the merits of a filter-based learning approach through a comparative analysis. Specifically, we contrast the performance of a U-Net model trained on CT images with an identical U-Net configuration trained on vesselness hyper-volumes using matching parameters. Our findings, based on two vascular datasets, highlight improved segmentations, especially for small vessels, when the model's learning is exposed to vessel-enhanced inputs.","sentences":["Vascular segmentation represents a crucial clinical task, yet its automation remains challenging.","Because of the recent strides in deep learning, vesselness filters, which can significantly aid the learning process, have been overlooked.","This study introduces an innovative filter fusion method crafted to amplify the effectiveness of vessel segmentation models.","Our investigation seeks to establish the merits of a filter-based learning approach through a comparative analysis.","Specifically, we contrast the performance of a U-Net model trained on CT images with an identical U-Net configuration trained on vesselness hyper-volumes using matching parameters.","Our findings, based on two vascular datasets, highlight improved segmentations, especially for small vessels, when the model's learning is exposed to vessel-enhanced inputs."],"url":"http://arxiv.org/abs/2402.14509v1","category":"eess.IV"}
{"created":"2024-02-22 12:45:36","title":"A Sparse Bayesian Committee Machine Potential for Hydrocarbons","abstract":"Accurate and scalable universal interatomic potentials are key for understanding material properties at the atomic level, a task often hindered by the steep computational scaling. Although recent developments of machine learning potential has made significant progress, the flexibility and expansion to a wide range of compounds within a single model seems still challenging to build in particular for the kernel-based models. Here, we introduce the Bayesian Committee Machine (BCM) potential to handle a wide array of hydrocarbons in various phases (gas, clusters, liquid, and solid). The BCM potential leverages the committee approach to bypass the poor scaling of kernel regressors when dealing with large datasets. Its committee-based structure allows for easy and cost-effective expansion, maintaining both transferrability and scalability. Demonstrating its robustness, the BCM accurately models challenging examples like the Diels-Alder reaction, structural strains, and pi-pi interactions. Our systematic benchmarking positions the sparse BCM potential as a promising candidate in the quest for a universal ab initio machine learning potential.","sentences":["Accurate and scalable universal interatomic potentials are key for understanding material properties at the atomic level, a task often hindered by the steep computational scaling.","Although recent developments of machine learning potential has made significant progress, the flexibility and expansion to a wide range of compounds within a single model seems still challenging to build in particular for the kernel-based models.","Here, we introduce the Bayesian Committee Machine (BCM) potential to handle a wide array of hydrocarbons in various phases (gas, clusters, liquid, and solid).","The BCM potential leverages the committee approach to bypass the poor scaling of kernel regressors when dealing with large datasets.","Its committee-based structure allows for easy and cost-effective expansion, maintaining both transferrability and scalability.","Demonstrating its robustness, the BCM accurately models challenging examples like the Diels-Alder reaction, structural strains, and pi-pi interactions.","Our systematic benchmarking positions the sparse BCM potential as a promising candidate in the quest for a universal ab initio machine learning potential."],"url":"http://arxiv.org/abs/2402.14497v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-22 10:26:46","title":"Parallelized Midpoint Randomization for Langevin Monte Carlo","abstract":"We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible. Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities. We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version. Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities. These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable.","sentences":["We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible.","Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities.","We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version.","Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities.","These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable."],"url":"http://arxiv.org/abs/2402.14434v1","category":"math.ST"}
{"created":"2024-02-22 10:19:34","title":"Robust Training of Federated Models with Extremely Label Deficiency","abstract":"Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency. Advanced FSSL methods predominantly focus on training a single model on each client. However, this approach could lead to a discrepancy between the objective functions of labeled and unlabeled data, resulting in gradient conflicts. To alleviate gradient conflict, we propose a novel twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data. In particular, Twin-sight concurrently trains a supervised model with a supervised objective function while training an unsupervised model using an unsupervised objective function. To enhance the synergy between these two models, Twin-sight introduces a neighbourhood-preserving constraint, which encourages the preservation of the neighbourhood relationship among data features extracted by both models. Our comprehensive experiments on four benchmark datasets provide substantial evidence that Twin-sight can significantly outperform state-of-the-art methods across various experimental settings, demonstrating the efficacy of the proposed Twin-sight.","sentences":["Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency.","Advanced FSSL methods predominantly focus on training a single model on each client.","However, this approach could lead to a discrepancy between the objective functions of labeled and unlabeled data, resulting in gradient conflicts.","To alleviate gradient conflict, we propose a novel twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data.","In particular, Twin-sight concurrently trains a supervised model with a supervised objective function while training an unsupervised model using an unsupervised objective function.","To enhance the synergy between these two models, Twin-sight introduces a neighbourhood-preserving constraint, which encourages the preservation of the neighbourhood relationship among data features extracted by both models.","Our comprehensive experiments on four benchmark datasets provide substantial evidence that Twin-sight can significantly outperform state-of-the-art methods across various experimental settings, demonstrating the efficacy of the proposed Twin-sight."],"url":"http://arxiv.org/abs/2402.14430v1","category":"cs.LG"}
{"created":"2024-02-22 08:26:56","title":"Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction","abstract":"Recently, large language models (LLMs) have been successful in relational extraction (RE) tasks, especially in the few-shot learning. An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches. Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem. In our framework, We use the ``\\textit{Training-Guide-Predict}'' strategy to combine the strengths of pre-trained language models (PLMs) and LLMs, where a task-specific PLM framework acts as a tutor, transfers task knowledge to the LLM, and guides the LLM in performing RE tasks. Our experiments on a RE dataset rich in relation types show that the approach in this paper facilitates RE of long-tail relation types.","sentences":["Recently, large language models (LLMs) have been successful in relational extraction (RE) tasks, especially in the few-shot learning.","An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches.","Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem.","In our framework, We use the ``\\textit{Training-Guide-Predict}'' strategy to combine the strengths of pre-trained language models (PLMs) and LLMs, where a task-specific PLM framework acts as a tutor, transfers task knowledge to the LLM, and guides the LLM in performing RE tasks.","Our experiments on a RE dataset rich in relation types show that the approach in this paper facilitates RE of long-tail relation types."],"url":"http://arxiv.org/abs/2402.14373v1","category":"cs.CL"}
{"created":"2024-02-22 05:24:19","title":"Multi-modal Stance Detection: New Datasets and Model","abstract":"Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.","sentences":["Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets.","Previous work on stance detection largely focused on pure texts.","In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages.","To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image.","In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities.","Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection."],"url":"http://arxiv.org/abs/2402.14298v1","category":"cs.CL"}
{"created":"2024-02-22 05:16:04","title":"High-arity PAC learning via exchangeability","abstract":"We develop a theory of high-arity PAC learning, which is statistical learning in the presence of \"structured correlation\". In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.","sentences":["We develop a theory of high-arity PAC learning, which is statistical learning in the presence of \"structured correlation\".","In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution.","We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence."],"url":"http://arxiv.org/abs/2402.14294v1","category":"cs.LG"}
{"created":"2024-02-22 04:03:32","title":"Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation","abstract":"Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATTE), as well as weighted variants of the former, which arise in policy evaluation.","sentences":["Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines.","While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation.","In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process.","Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATTE), as well as weighted variants of the former, which arise in policy evaluation."],"url":"http://arxiv.org/abs/2402.14264v1","category":"stat.ML"}
{"created":"2024-02-22 01:46:39","title":"Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding","abstract":"Data diversity and abundance are essential for improving the performance and generalization of models in natural language processing and 2D vision. However, 3D vision domain suffers from the lack of 3D data, and simply combining multiple 3D datasets for pretraining a 3D backbone does not yield significant improvement, due to the domain discrepancies among different 3D datasets that impede effective feature learning. In this work, we identify the main sources of the domain discrepancies between 3D indoor scene datasets, and propose Swin3D++, an enhanced architecture based on Swin3D for efficient pretraining on multi-source 3D point clouds. Swin3D++ introduces domain-specific mechanisms to Swin3D's modules to address domain discrepancies and enhance the network capability on multi-source pretraining. Moreover, we devise a simple source-augmentation strategy to increase the pretraining data scale and facilitate supervised pretraining. We validate the effectiveness of our design, and demonstrate that Swin3D++ surpasses the state-of-the-art 3D pretraining methods on typical indoor scene understanding tasks. Our code and models will be released at https://github.com/microsoft/Swin3D","sentences":["Data diversity and abundance are essential for improving the performance and generalization of models in natural language processing and 2D vision.","However, 3D vision domain suffers from the lack of 3D data, and simply combining multiple 3D datasets for pretraining a 3D backbone does not yield significant improvement, due to the domain discrepancies among different 3D datasets that impede effective feature learning.","In this work, we identify the main sources of the domain discrepancies between 3D indoor scene datasets, and propose Swin3D++, an enhanced architecture based on Swin3D for efficient pretraining on multi-source 3D point clouds.","Swin3D++ introduces domain-specific mechanisms to Swin3D's modules to address domain discrepancies and enhance the network capability on multi-source pretraining.","Moreover, we devise a simple source-augmentation strategy to increase the pretraining data scale and facilitate supervised pretraining.","We validate the effectiveness of our design, and demonstrate that Swin3D++ surpasses the state-of-the-art 3D pretraining methods on typical indoor scene understanding tasks.","Our code and models will be released at https://github.com/microsoft/Swin3D"],"url":"http://arxiv.org/abs/2402.14215v1","category":"cs.CV"}
{"created":"2024-02-22 01:22:19","title":"Developing an Automated Detection, Tracking and Analysis Method for Solar Filaments Observed by CHASE via Machine Learning","abstract":"Studies on the dynamics of solar filaments have significant implications for understanding their formation, evolution, and eruption, which are of great importance for space weather warning and forecasting. The H$\\alpha$ Imaging Spectrograph (HIS) onboard the recently launched Chinese H$\\alpha$ Solar Explorer (CHASE) can provide full-disk solar H$\\alpha$ spectroscopic observations, which bring us an opportunity to systematically explore and analyze the plasma dynamics of filaments. The dramatically increased observation data require automate processing and analysis which are impossible if dealt with manually. In this paper, we utilize the U-Net model to identify filaments and implement the Channel and Spatial Reliability Tracking (CSRT) algorithm for automated filament tracking. In addition, we use the cloud model to invert the line-of-sight velocity of filaments and employ the graph theory algorithm to extract the filament spine, which can advance our understanding of the dynamics of filaments. The favorable test performance confirms the validity of our method, which will be implemented in the following statistical analyses of filament features and dynamics of CHASE/HIS observations.","sentences":["Studies on the dynamics of solar filaments have significant implications for understanding their formation, evolution, and eruption, which are of great importance for space weather warning and forecasting.","The H$\\alpha$ Imaging Spectrograph (HIS) onboard the recently launched Chinese H$\\alpha$ Solar Explorer (CHASE) can provide full-disk solar H$\\alpha$ spectroscopic observations, which bring us an opportunity to systematically explore and analyze the plasma dynamics of filaments.","The dramatically increased observation data require automate processing and analysis which are impossible if dealt with manually.","In this paper, we utilize the U-Net model to identify filaments and implement the Channel and Spatial Reliability Tracking (CSRT) algorithm for automated filament tracking.","In addition, we use the cloud model to invert the line-of-sight velocity of filaments and employ the graph theory algorithm to extract the filament spine, which can advance our understanding of the dynamics of filaments.","The favorable test performance confirms the validity of our method, which will be implemented in the following statistical analyses of filament features and dynamics of CHASE/HIS observations."],"url":"http://arxiv.org/abs/2402.14209v1","category":"astro-ph.SR"}
{"created":"2024-02-22 01:07:48","title":"Comparing Graph Transformers via Positional Encodings","abstract":"The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in terms of graph transformers. Based on our theoretical results, we provide a study on several APEs and RPEs (including the resistance distance and the recently introduced stable and expressive positional encoding (SPE)) and compare their distinguishing power in terms of transformers. We believe our work will help navigate the huge number of choices of positional encoding and will provide guidance on the future design of positional encodings for graph transformers.","sentences":["The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph.","There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs).","APEs assign features to each node and are given as input to the transformer.","RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block.","A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer.","In this paper, we aim to understand the relationship between these different types of positional encodings.","Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power.","In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in terms of graph transformers.","Based on our theoretical results, we provide a study on several APEs and RPEs (including the resistance distance and the recently introduced stable and expressive positional encoding (SPE)) and compare their distinguishing power in terms of transformers.","We believe our work will help navigate the huge number of choices of positional encoding and will provide guidance on the future design of positional encodings for graph transformers."],"url":"http://arxiv.org/abs/2402.14202v1","category":"cs.LG"}
{"created":"2024-02-22 00:41:23","title":"Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models","abstract":"Large Language Models (LLMs) have been widely used as general-purpose AI agents showing comparable performance on many downstream tasks. However, existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.   In this paper, we propose a framework, Learning to Reduce, that fine-tunes a language model to generate a reduced version of an input context, given a task description and context input. The model learns to reduce the input context using On-Policy Reinforcement Learning and aims to improve the reasoning performance of a fixed LLM. Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets. We further show that our model helps improve the LLM's performance on downstream tasks especially when the context is long.","sentences":["Large Language Models (LLMs) have been widely used as general-purpose AI agents showing comparable performance on many downstream tasks.","However, existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.   ","In this paper, we propose a framework, Learning to Reduce, that fine-tunes a language model to generate a reduced version of an input context, given a task description and context input.","The model learns to reduce the input context using On-Policy Reinforcement Learning and aims to improve the reasoning performance of a fixed LLM.","Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets.","We further show that our model helps improve the LLM's performance on downstream tasks especially when the context is long."],"url":"http://arxiv.org/abs/2402.14195v1","category":"cs.CL"}
{"created":"2024-02-21 22:41:38","title":"TOOLVERIFIER: Generalization to New Tools via Self-Verification","abstract":"Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.","sentences":["Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem.","While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations.","In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation.","We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly.","Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced."],"url":"http://arxiv.org/abs/2402.14158v1","category":"cs.CL"}
{"created":"2024-02-21 22:02:37","title":"Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation","abstract":"Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that dynamic weighting generally outperforms static weighting approaches, and we explore its effectiveness in 2- and 3-style control, even compared to strong baselines like plug-and-play model. All code and data for RL pipelines with multiple style attributes will be publicly available.","sentences":["Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust).","Humans often employ multiple styles simultaneously.","An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic.","Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes.","In this paper, we expand this into controlling multiple styles simultaneously.","Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation.","These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes.","We find that dynamic weighting generally outperforms static weighting approaches, and we explore its effectiveness in 2- and 3-style control, even compared to strong baselines like plug-and-play model.","All code and data for RL pipelines with multiple style attributes will be publicly available."],"url":"http://arxiv.org/abs/2402.14146v1","category":"cs.CL"}
{"created":"2024-02-21 21:24:57","title":"GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors","abstract":"Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.","sentences":["Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure.","Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types.","However, such datasets are not readily available.","Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements.","Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances.","A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM."],"url":"http://arxiv.org/abs/2402.14136v1","category":"cs.RO"}
{"created":"2024-02-21 20:29:21","title":"Multi-organ Self-supervised Contrastive Learning for Breast Lesion Segmentation","abstract":"Self-supervised learning has proven to be an effective way to learn representations in domains where annotated labels are scarce, such as medical imaging. A widely adopted framework for this purpose is contrastive learning and it has been applied to different scenarios. This paper seeks to advance our understanding of the contrastive learning framework by exploring a novel perspective: employing multi-organ datasets for pre-training models tailored to specific organ-related target tasks. More specifically, our target task is breast tumour segmentation in ultrasound images. The pre-training datasets include ultrasound images from other organs, such as the lungs and heart, and large datasets of natural images. Our results show that conventional contrastive learning pre-training improves performance compared to supervised baseline approaches. Furthermore, our pre-trained models achieve comparable performance when fine-tuned with only half of the available labelled data. Our findings also show the advantages of pre-training on diverse organ data for improving performance in the downstream task.","sentences":["Self-supervised learning has proven to be an effective way to learn representations in domains where annotated labels are scarce, such as medical imaging.","A widely adopted framework for this purpose is contrastive learning and it has been applied to different scenarios.","This paper seeks to advance our understanding of the contrastive learning framework by exploring a novel perspective: employing multi-organ datasets for pre-training models tailored to specific organ-related target tasks.","More specifically, our target task is breast tumour segmentation in ultrasound images.","The pre-training datasets include ultrasound images from other organs, such as the lungs and heart, and large datasets of natural images.","Our results show that conventional contrastive learning pre-training improves performance compared to supervised baseline approaches.","Furthermore, our pre-trained models achieve comparable performance when fine-tuned with only half of the available labelled data.","Our findings also show the advantages of pre-training on diverse organ data for improving performance in the downstream task."],"url":"http://arxiv.org/abs/2402.14114v1","category":"cs.CV"}
{"created":"2024-02-21 19:57:08","title":"Formal Definitions and Performance Comparison of Consistency Models for Parallel File Systems","abstract":"The semantics of HPC storage systems are defined by the consistency models to which they abide. Storage consistency models have been less studied than their counterparts in memory systems, with the exception of the POSIX standard and its strict consistency model. The use of POSIX consistency imposes a performance penalty that becomes more significant as the scale of parallel file systems increases and the access time to storage devices, such as node-local solid storage devices, decreases. While some efforts have been made to adopt relaxed storage consistency models, these models are often defined informally and ambiguously as by-products of a particular implementation. In this work, we establish a connection between memory consistency models and storage consistency models and revisit the key design choices of storage consistency models from a high-level perspective. Further, we propose a formal and unified framework for defining storage consistency models and a layered implementation that can be used to easily evaluate their relative performance for different I/O workloads. Finally, we conduct a comprehensive performance comparison of two relaxed consistency models on a range of commonly-seen parallel I/O workloads, such as checkpoint/restart of scientific applications and random reads of deep learning applications. We demonstrate that for certain I/O scenarios, a weaker consistency model can significantly improve the I/O performance. For instance, in small random reads that typically found in deep learning applications, session consistency achieved an 5x improvement in I/O bandwidth compared to commit consistency, even at small scales.","sentences":["The semantics of HPC storage systems are defined by the consistency models to which they abide.","Storage consistency models have been less studied than their counterparts in memory systems, with the exception of the POSIX standard and its strict consistency model.","The use of POSIX consistency imposes a performance penalty that becomes more significant as the scale of parallel file systems increases and the access time to storage devices, such as node-local solid storage devices, decreases.","While some efforts have been made to adopt relaxed storage consistency models, these models are often defined informally and ambiguously as by-products of a particular implementation.","In this work, we establish a connection between memory consistency models and storage consistency models and revisit the key design choices of storage consistency models from a high-level perspective.","Further, we propose a formal and unified framework for defining storage consistency models and a layered implementation that can be used to easily evaluate their relative performance for different I/O workloads.","Finally, we conduct a comprehensive performance comparison of two relaxed consistency models on a range of commonly-seen parallel I/O workloads, such as checkpoint/restart of scientific applications and random reads of deep learning applications.","We demonstrate that for certain I/O scenarios, a weaker consistency model can significantly improve the I/O performance.","For instance, in small random reads that typically found in deep learning applications, session consistency achieved an 5x improvement in I/O bandwidth compared to commit consistency, even at small scales."],"url":"http://arxiv.org/abs/2402.14105v1","category":"cs.DC"}
{"created":"2024-02-21 19:55:01","title":"Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression","abstract":"We study computational-statistical gaps for improper learning in sparse linear regression. More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. Information-theoretically this can be achieved using $\\Theta(k \\log (d/k))$ samples. Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\\Theta(d)$ samples without additional restrictions on the model. Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.   We give evidence that efficient algorithms for this task require at least (roughly) $\\Omega(k^2)$ samples. In particular, we show that an improper learning algorithm for sparse linear regression can be used to solve sparse PCA problems (with a negative spike) in their Wishart form, in regimes in which efficient algorithms are widely believed to require at least $\\Omega(k^2)$ samples. We complement our reduction with low-degree and statistical query lower bounds for the sparse PCA problems from which we reduce.   Our hardness results apply to the (correlated) random design setting in which the covariates are drawn i.i.d. from a mean-zero Gaussian distribution with unknown covariance.","sentences":["We study computational-statistical gaps for improper learning in sparse linear regression.","More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples.","Information-theoretically this can be achieved using $\\Theta(k \\log (d/k))$ samples.","Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\\Theta(d)$ samples without additional restrictions on the model.","Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.   ","We give evidence that efficient algorithms for this task require at least (roughly) $\\Omega(k^2)$ samples.","In particular, we show that an improper learning algorithm for sparse linear regression can be used to solve sparse PCA problems (with a negative spike) in their Wishart form, in regimes in which efficient algorithms are widely believed to require at least $\\Omega(k^2)$ samples.","We complement our reduction with low-degree and statistical query lower bounds for the sparse PCA problems from which we reduce.   ","Our hardness results apply to the (correlated) random design setting in which the covariates are drawn i.i.d.","from a mean-zero Gaussian distribution with unknown covariance."],"url":"http://arxiv.org/abs/2402.14103v1","category":"cs.LG"}
{"created":"2024-02-21 19:49:12","title":"EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy","abstract":"Lung cancer is a devastating disease with the highest mortality rate among cancer types. Over 60% of non-small cell lung cancer (NSCLC) patients, which accounts for 87% of diagnoses, require radiation therapy. Rapid treatment initiation significantly increases the patient's survival rate and reduces the mortality rate. Accurate tumor segmentation is a critical step in the diagnosis and treatment of NSCLC. Manual segmentation is time and labor-consuming and causes delays in treatment initiation. Although many lung nodule detection methods, including deep learning-based models, have been proposed, there is still a long-standing problem of high false positives (FPs) with most of these methods. Here, we developed an electronic health record (EHR) guided lung tumor auto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor segmentation), where the extracted information from EHRs using a pre-trained large language model (LLM), was used to remove the FPs and keep the TP nodules only. The auto-segmentation model was trained on NSCLC patients' computed tomography (CT), and the pre-trained LLM was used with the zero-shot learning approach. Our approach resulted in a 250% boost in successful nodule detection using the data from ten NSCLC patients treated in our institution.","sentences":["Lung cancer is a devastating disease with the highest mortality rate among cancer types.","Over 60% of non-small cell lung cancer (NSCLC) patients, which accounts for 87% of diagnoses, require radiation therapy.","Rapid treatment initiation significantly increases the patient's survival rate and reduces the mortality rate.","Accurate tumor segmentation is a critical step in the diagnosis and treatment of NSCLC.","Manual segmentation is time and labor-consuming and causes delays in treatment initiation.","Although many lung nodule detection methods, including deep learning-based models, have been proposed, there is still a long-standing problem of high false positives (FPs) with most of these methods.","Here, we developed an electronic health record (EHR) guided lung tumor auto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor segmentation), where the extracted information from EHRs using a pre-trained large language model (LLM), was used to remove the FPs and keep the TP nodules only.","The auto-segmentation model was trained on NSCLC patients' computed tomography (CT), and the pre-trained LLM was used with the zero-shot learning approach.","Our approach resulted in a 250% boost in successful nodule detection using the data from ten NSCLC patients treated in our institution."],"url":"http://arxiv.org/abs/2402.14099v1","category":"eess.IV"}
{"created":"2024-02-22 17:19:03","title":"Quantum Annealing Inspired Algorithms for Track Reconstruction at High Energy Colliders","abstract":"Charged particle reconstruction or track reconstruction is one of the most crucial components of pattern recognition in high energy collider physics. It is known for enormous consumption of the computing resources, especially when the particle multiplicity is high. This would indeed be the conditions at future colliders such as the High Luminosity Large Hadron Collider and Super Proton Proton Collider. Track reconstruction can be formulated as a quadratic unconstrained binary optimization (QUBO) problem, for which various quantum algorithms have been investigated and evaluated with both the quantum simulator and hardware. Simulated bifurcation algorithms are a set of quantum annealing inspired algorithms, and are serious competitors to the quantum annealing, other Ising machines and their classical counterparts. In this study, we show that the simulated bifurcation algorithms can be employed for solving the particle tracking problem. As the simulated bifurcation algorithms run on classical computers and are suitable for parallel processing and usage of the graphical processing units, they can handle significantly large data at high speed. These algorithms exhibit compatible or sometimes improved reconstruction efficiency and purity than the simulated annealing, but the running time can be reduced by as much as four orders of magnitude. These results suggest that QUBO models together with the quantum annealing inspired algorithms are valuable for the current and future particle tracking problems.","sentences":["Charged particle reconstruction or track reconstruction is one of the most crucial components of pattern recognition in high energy collider physics.","It is known for enormous consumption of the computing resources, especially when the particle multiplicity is high.","This would indeed be the conditions at future colliders such as the High Luminosity Large Hadron Collider and Super Proton Proton Collider.","Track reconstruction can be formulated as a quadratic unconstrained binary optimization (QUBO) problem, for which various quantum algorithms have been investigated and evaluated with both the quantum simulator and hardware.","Simulated bifurcation algorithms are a set of quantum annealing inspired algorithms, and are serious competitors to the quantum annealing, other Ising machines and their classical counterparts.","In this study, we show that the simulated bifurcation algorithms can be employed for solving the particle tracking problem.","As the simulated bifurcation algorithms run on classical computers and are suitable for parallel processing and usage of the graphical processing units, they can handle significantly large data at high speed.","These algorithms exhibit compatible or sometimes improved reconstruction efficiency and purity than the simulated annealing, but the running time can be reduced by as much as four orders of magnitude.","These results suggest that QUBO models together with the quantum annealing inspired algorithms are valuable for the current and future particle tracking problems."],"url":"http://arxiv.org/abs/2402.14718v1","category":"quant-ph"}
{"created":"2024-02-22 17:11:45","title":"Gilbert-Varshamov Bound for Codes in $L_1$ Metric using Multivariate Analytic Combinatorics","abstract":"Analytic combinatorics in several variables refers to a suite of tools that provide sharp asymptotic estimates for certain combinatorial quantities. In this paper, we apply these tools to determine the Gilbert--Varshamov lower bound on the rate of optimal codes in $L_1$ metric. Several different code spaces are analyzed, including the simplex and the hypercube in $\\mathbb{Z^n}$, all of which are inspired by concrete data storage and transmission models such as the sticky insertion channel, the permutation channel, the adjacent transposition (bit-shift) channel, the multilevel flash memory channel, etc.","sentences":["Analytic combinatorics in several variables refers to a suite of tools that provide sharp asymptotic estimates for certain combinatorial quantities.","In this paper, we apply these tools to determine the Gilbert--Varshamov lower bound on the rate of optimal codes in $L_1$ metric.","Several different code spaces are analyzed, including the simplex and the hypercube in $\\mathbb{Z^n}$, all of which are inspired by concrete data storage and transmission models such as the sticky insertion channel, the permutation channel, the adjacent transposition (bit-shift) channel, the multilevel flash memory channel, etc."],"url":"http://arxiv.org/abs/2402.14712v1","category":"cs.IT"}
{"created":"2024-02-22 16:30:01","title":"On semi-restricted Rock, Paper, Scissors","abstract":"Spiro, Surya and Zeng (Electron. J. Combin. 2023; arXiv:2207.11272) recently studied a semi-restricted variant of the well-known game Rock, Paper, Scissors; in this variant the game is played for $3n$ rounds, but one of the two players is restricted and has to use each of the three moves exactly $n$ times. They find the optimal strategy, and they show that it results in an expected score for the unrestricted player $\\Theta(\\sqrt{n})$; they conjecture, based on numerical evidence, that the expectation is $\\approx 1.46\\sqrt{n}$.   We analyse the result of the strategy further and show that the average is $\\sim c \\sqrt{n}$ with $c=3\\sqrt{3}/2\\sqrt{\\pi}=1.466$, verifying the conjecture. We also find the asymptotic distribution of the score, and compute its variance.","sentences":["Spiro, Surya and Zeng (Electron.","J. Combin. 2023; arXiv:2207.11272) recently studied a semi-restricted variant of the well-known game Rock, Paper, Scissors; in this variant the game is played for $3n$ rounds, but one of the two players is restricted and has to use each of the three moves exactly $n$ times.","They find the optimal strategy, and they show that it results in an expected score for the unrestricted player $\\Theta(\\sqrt{n})$; they conjecture, based on numerical evidence, that the expectation is $\\approx 1.46\\sqrt{n}$.   ","We analyse the result of the strategy further and show that the average is $\\sim c \\sqrt{n}$ with $c=3\\sqrt{3}/2\\sqrt{\\pi}=1.466$, verifying the conjecture.","We also find the asymptotic distribution of the score, and compute its variance."],"url":"http://arxiv.org/abs/2402.14676v1","category":"math.PR"}
{"created":"2024-02-22 14:46:37","title":"Agile Requirement Change Management Model for Global Software Development","abstract":"We propose a noble, comprehensive and robust agile requirements change management (ARCM) model that addresses the limitations of existing models and is tailored for agile software development in the global software development paradigm. To achieve this goal, we conducted an exhaustive literature review and an empirical study with RCM industry experts. Our study evaluated the effectiveness of the proposed RCM model in a real-world setting and identifies any limitations or areas for improvement. The results of our study provide valuable insights into how the proposed RCM model can be applied in agile global software development environments to improve software development practices and optimize project success rates.","sentences":["We propose a noble, comprehensive and robust agile requirements change management (ARCM) model that addresses the limitations of existing models and is tailored for agile software development in the global software development paradigm.","To achieve this goal, we conducted an exhaustive literature review and an empirical study with RCM industry experts.","Our study evaluated the effectiveness of the proposed RCM model in a real-world setting and identifies any limitations or areas for improvement.","The results of our study provide valuable insights into how the proposed RCM model can be applied in agile global software development environments to improve software development practices and optimize project success rates."],"url":"http://arxiv.org/abs/2402.14595v1","category":"cs.SE"}
{"created":"2024-02-22 13:32:02","title":"Extending the definition of set tolerances","abstract":"Optimal solutions of combinatorial optimization problems can be sensitive to changes in the cost of one or more elements. Single and set tolerances measure the largest / smallest possible change such that the current solution remains optimal and other solutions become non-optimal for cost changes in one or more elements, respectively. The current definition only applies to subsets of elements. In this paper, we broaden the definition to all elements, for single tolerances, and to all subsets of elements for set tolerances, while proving that key computational and theoretical properties still apply to the new definitions.","sentences":["Optimal solutions of combinatorial optimization problems can be sensitive to changes in the cost of one or more elements.","Single and set tolerances measure the largest / smallest possible change such that the current solution remains optimal and other solutions become non-optimal for cost changes in one or more elements, respectively.","The current definition only applies to subsets of elements.","In this paper, we broaden the definition to all elements, for single tolerances, and to all subsets of elements for set tolerances, while proving that key computational and theoretical properties still apply to the new definitions."],"url":"http://arxiv.org/abs/2402.14542v1","category":"cs.DM"}
{"created":"2024-02-22 13:31:02","title":"On Truthful Item-Acquiring Mechanisms for Reward Maximization","abstract":"In this research, we study the problem that a collector acquires items from the owner based on the item qualities the owner declares and an independent appraiser's assessments. The owner is interested in maximizing the probability that the collector acquires the items and is the only one who knows the items' factual quality. The appraiser performs her duties with impartiality, but her assessment may be subject to random noises, so it may not accurately reflect the factual quality of the items. The main challenge lies in devising mechanisms that prompt the owner to reveal accurate information, thereby optimizing the collector's expected reward. We consider the menu size of mechanisms as a measure of their practicability and study its impact on the attainable expected reward. For the single-item setting, we design optimal mechanisms with a monotone increasing menu size. Although the reward gap between the simplest and optimal mechanisms is bounded, we show that simple mechanisms with a small menu size cannot ensure any positive fraction of the optimal reward of mechanisms with a larger menu size. For the multi-item setting, we show that an ordinal mechanism that only takes the owner's ordering of the items as input is not incentive-compatible. We then propose a set of Union mechanisms that combine single-item mechanisms. Moreover, we run experiments to examine these mechanisms' robustness against the independent appraiser's assessment accuracy and the items' acquiring rate.","sentences":["In this research, we study the problem that a collector acquires items from the owner based on the item qualities the owner declares and an independent appraiser's assessments.","The owner is interested in maximizing the probability that the collector acquires the items and is the only one who knows the items' factual quality.","The appraiser performs her duties with impartiality, but her assessment may be subject to random noises, so it may not accurately reflect the factual quality of the items.","The main challenge lies in devising mechanisms that prompt the owner to reveal accurate information, thereby optimizing the collector's expected reward.","We consider the menu size of mechanisms as a measure of their practicability and study its impact on the attainable expected reward.","For the single-item setting, we design optimal mechanisms with a monotone increasing menu size.","Although the reward gap between the simplest and optimal mechanisms is bounded, we show that simple mechanisms with a small menu size cannot ensure any positive fraction of the optimal reward of mechanisms with a larger menu size.","For the multi-item setting, we show that an ordinal mechanism that only takes the owner's ordering of the items as input is not incentive-compatible.","We then propose a set of Union mechanisms that combine single-item mechanisms.","Moreover, we run experiments to examine these mechanisms' robustness against the independent appraiser's assessment accuracy and the items' acquiring rate."],"url":"http://arxiv.org/abs/2402.14540v1","category":"cs.GT"}
{"created":"2024-02-22 13:10:19","title":"Auxiliary Calculations for Graphene-Based Quantum Hall Arrays Using Partially Recursive Star-Mesh Transformations","abstract":"A previous mathematical approach adopted for optimizing the number of total device elements required for obtaining high effective quantized resistances in graphene-based quantum Hall array devices (QHARS) has been further explored with partial recursion patterns. Designs would assume the use of epitaxial graphene elements, whose quantized Hall resistance at the {\\nu}=2 plateau (R_H \\approx 12906.4 \\Ohm) becomes the building block for larger effective, quantized resistances. Auxiliary calculations suggest the importance of applying full recursions at least once to maximize the reduction of total QHARS elements needed for high resistances.","sentences":["A previous mathematical approach adopted for optimizing the number of total device elements required for obtaining high effective quantized resistances in graphene-based quantum Hall array devices (QHARS) has been further explored with partial recursion patterns.","Designs would assume the use of epitaxial graphene elements, whose quantized Hall resistance at the {\\nu}=2 plateau (R_H \\approx 12906.4 \\Ohm) becomes the building block for larger effective, quantized resistances.","Auxiliary calculations suggest the importance of applying full recursions at least once to maximize the reduction of total QHARS elements needed for high resistances."],"url":"http://arxiv.org/abs/2402.14520v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-22 12:20:05","title":"Mathematical Modeling and Hyers-Ulam Stability for a Nonlinear Epidemiological Model with $\u03a6_p$ Operator and Mittag-Leffler Kernel","abstract":"This paper investigates a novel nonlinear singular fractional SI model with the $\\Phi_p$ operator and the Mittag-Leffler kernel. The initial investigation includes the existence, uniqueness, boundedness, and non-negativity of the solution. We then establish Hyers-Ulam stability for the proposed model in Banach space. Optimal control analysis is performed to minimize the spread of infection and maximize the population of susceptible individuals. Finally, the theoretical results are supported by numerical simulations.","sentences":["This paper investigates a novel nonlinear singular fractional SI model with the $\\Phi_p$ operator and the Mittag-Leffler kernel.","The initial investigation includes the existence, uniqueness, boundedness, and non-negativity of the solution.","We then establish Hyers-Ulam stability for the proposed model in Banach space.","Optimal control analysis is performed to minimize the spread of infection and maximize the population of susceptible individuals.","Finally, the theoretical results are supported by numerical simulations."],"url":"http://arxiv.org/abs/2402.14487v1","category":"nlin.CD"}
{"created":"2024-02-22 11:04:52","title":"Rare-earth doped yttrium silicate (Y2SiO5) thin films grown by chemical vapour deposition for quantum technologies","abstract":"Yttrium orthosilicate (Y2SiO5 - YSO) is one of the most promising crystals to host rare-earth (RE) ions for quantum technologies applications. In this matrix, they indeed exhibit narrow optical and spin linewidths that can be exploited to develop quantum memories or quantum information processing capabilities. In this paper, we propose a new method to grow RE doped silicate thin films on silicon wafers based on direct liquid injection chemical vapour deposition (DLI-CVD). We optimize the deposition and annealing conditions to achieve formation of the high temperature X2-YSO phase. The phase purity and crystalline quality of the films are assessed by evaluating the optical properties of Eu3+ ions embedded in this oxide matrix. In view of the results, we discuss the possible phase formation mechanisms, and the potential of this new wafer-compatible form of YSO for quantum technologies applications.","sentences":["Yttrium orthosilicate (Y2SiO5 - YSO) is one of the most promising crystals to host rare-earth (RE) ions for quantum technologies applications.","In this matrix, they indeed exhibit narrow optical and spin linewidths that can be exploited to develop quantum memories or quantum information processing capabilities.","In this paper, we propose a new method to grow RE doped silicate thin films on silicon wafers based on direct liquid injection chemical vapour deposition (DLI-CVD).","We optimize the deposition and annealing conditions to achieve formation of the high temperature X2-YSO phase.","The phase purity and crystalline quality of the films are assessed by evaluating the optical properties of Eu3+ ions embedded in this oxide matrix.","In view of the results, we discuss the possible phase formation mechanisms, and the potential of this new wafer-compatible form of YSO for quantum technologies applications."],"url":"http://arxiv.org/abs/2402.14445v1","category":"physics.optics"}
{"created":"2024-02-22 10:18:52","title":"Low Polarization Sensitive O-band SOA on InP Membrane for Advanced Photonic Integration","abstract":"Managing insertion losses, polarizations and device footprint is crucial in developing large-scale photonic integrated circuits (PICs). This paper presents a solution to these critical challenges by designing a semiconductor optical amplifier (SOA) in the O-band with reduced polarization sensitivity, leveraging the ultra-compact InP Membrane on Silicon (IMOS) platform. The platform is compatible with close integration atop electronics, via densely populated vertical interconnects. The SOA incorporates a thin tensile-strained bulk active layer to mitigate polarization sensitivity. The developed 500 um long SOA has a peak gain of 11.5 dB at 1350 nm and an optimal polarization dependency of less than 1 dB across a 25 nm bandwidth, ranging from 1312 nm to 1337 nm. The device is practical for integrated circuits where multiple amplifiers work in cascades with a minimal 6.5 dB noise figure (NF) measured at the gain peak. The designed vertical active-passive transition, achieved through inverse tapering, allows for effective field coupling in the vertical direction resulting in a transmission efficiency of over 95% at the transition and minimal polarization sensitivity of less than 3%. The device yields significant gain at a small current density of less than 3 kA/cm2 as the result of minimalist gain medium structure, reducing joule heating and improving energy efficiency. This is especially relevant in applications such as optical switching, where multiple SOAs populate the PIC within a small area. Consequently, the simulated and fabricated low polarization sensitive O-band SOA is a suitable candidate for integration into large-scale, ultra-compact photonic integrated circuits.","sentences":["Managing insertion losses, polarizations and device footprint is crucial in developing large-scale photonic integrated circuits (PICs).","This paper presents a solution to these critical challenges by designing a semiconductor optical amplifier (SOA) in the O-band with reduced polarization sensitivity, leveraging the ultra-compact InP Membrane on Silicon (IMOS) platform.","The platform is compatible with close integration atop electronics, via densely populated vertical interconnects.","The SOA incorporates a thin tensile-strained bulk active layer to mitigate polarization sensitivity.","The developed 500 um long SOA has a peak gain of 11.5 dB at 1350 nm and an optimal polarization dependency of less than 1 dB across a 25 nm bandwidth, ranging from 1312 nm to 1337 nm.","The device is practical for integrated circuits where multiple amplifiers work in cascades with a minimal 6.5 dB noise figure (NF) measured at the gain peak.","The designed vertical active-passive transition, achieved through inverse tapering, allows for effective field coupling in the vertical direction resulting in a transmission efficiency of over 95% at the transition and minimal polarization sensitivity of less than 3%.","The device yields significant gain at a small current density of less than 3 kA/cm2 as the result of minimalist gain medium structure, reducing joule heating and improving energy efficiency.","This is especially relevant in applications such as optical switching, where multiple SOAs populate the PIC within a small area.","Consequently, the simulated and fabricated low polarization sensitive O-band SOA is a suitable candidate for integration into large-scale, ultra-compact photonic integrated circuits."],"url":"http://arxiv.org/abs/2402.14429v1","category":"physics.optics"}
{"created":"2024-02-22 10:03:34","title":"Spatially sparse optimization problems in fractional order Sobolev spaces","abstract":"We investigate time-dependent optimization problems in fractional Sobolev spaces with the sparsity promoting $L^p$-pseudo norm for $0<p<1$ in the objective functional. In order to avoid computing the fractional Laplacian on the time-space cylinder $I\\times \\Omega$, we introduce an auxiliary function $w$ on $\\Omega$ that is an upper bound for the function $u\\in L^2(I\\times\\Omega)$. We prove existence and regularity results and derive a necessary optimality condition. This is done by smoothing the $L^p$-pseudo norm and by penalizing the inequality constraint regarding $u$ and $w$. The problem is solved numerically with an iterative scheme whose weak limit points satisfy a weaker form of the necessary optimality condition.","sentences":["We investigate time-dependent optimization problems in fractional Sobolev spaces with the sparsity promoting $L^p$-pseudo norm for $0<p<1$ in the objective functional.","In order to avoid computing the fractional Laplacian on the time-space cylinder $I\\times \\Omega$, we introduce an auxiliary function $w$ on $\\Omega$ that is an upper bound for the function $u\\in L^2(I\\times\\Omega)$.","We prove existence and regularity results and derive a necessary optimality condition.","This is done by smoothing the $L^p$-pseudo norm and by penalizing the inequality constraint regarding $u$ and $w$. The problem is solved numerically with an iterative scheme whose weak limit points satisfy a weaker form of the necessary optimality condition."],"url":"http://arxiv.org/abs/2402.14417v1","category":"math.OC"}
{"created":"2024-02-22 06:21:41","title":"Assessing generalization capability of text ranking models in Polish","abstract":"Retrieval-augmented generation (RAG) is becoming an increasingly popular technique for integrating internal knowledge bases with large language models. In a typical RAG pipeline, three models are used, responsible for the retrieval, reranking, and generation stages. In this article, we focus on the reranking problem for the Polish language, examining the performance of rerankers and comparing their results with available retrieval models. We conduct a comprehensive evaluation of existing models and those trained by us, utilizing a benchmark of 41 diverse information retrieval tasks for the Polish language. The results of our experiments show that most models struggle with out-of-domain generalization. However, a combination of effective optimization method and a large training dataset allows for building rerankers that are both compact in size and capable of generalization. The best of our models establishes a new state-of-the-art for reranking in the Polish language, outperforming existing models with up to 30 times more parameters.","sentences":["Retrieval-augmented generation (RAG) is becoming an increasingly popular technique for integrating internal knowledge bases with large language models.","In a typical RAG pipeline, three models are used, responsible for the retrieval, reranking, and generation stages.","In this article, we focus on the reranking problem for the Polish language, examining the performance of rerankers and comparing their results with available retrieval models.","We conduct a comprehensive evaluation of existing models and those trained by us, utilizing a benchmark of 41 diverse information retrieval tasks for the Polish language.","The results of our experiments show that most models struggle with out-of-domain generalization.","However, a combination of effective optimization method and a large training dataset allows for building rerankers that are both compact in size and capable of generalization.","The best of our models establishes a new state-of-the-art for reranking in the Polish language, outperforming existing models with up to 30 times more parameters."],"url":"http://arxiv.org/abs/2402.14318v1","category":"cs.CL"}
{"created":"2024-02-22 01:26:58","title":"Inclined junction in monolayer graphene: A gateway toward tailoring valley polarization of Dirac fermions","abstract":"Generating discernible valley contrasts and segregating valley-indexed fermions in real space within graphene poses considerable challenges due to the isotropic transport within the continuum energy range for degenerate valleys. This study unveils an interesting finding: introducing valley contrast through anisotropic chiral transport in isotropic Dirac systems like graphene, achieved by implementing a tilted PN junction. The tilted junction shifts the angular spectrum to larger angles in accordance with the tilt angle. This modifies the pseudospin-conserved modes across the junction, resulting in valley-resolved chiral transport. This approach not only induces valley splitting within the real space but also preserves the remarkable mobility of fermions, offering distinct advantages over alternative strategies. The comprehensive analysis includes optimizing the experimental setup, scrutinizing factors such as the sequence of the doped region, and examining critical parameters like the tilt angle delta and transition width d across the junction. Surprisingly, an increased transition width enhances transmission, attributed to specular edge scattering. Importantly, the system remains resilient to Anderson short-range edge disorder. The broader implication lies in the transformative potential of inducing analogous anisotropic chiral transport behaviors in isotropic Dirac systems, resembling the characteristics of tilted Dirac-Weyl semimetals, by incorporating a tilted PNJ.","sentences":["Generating discernible valley contrasts and segregating valley-indexed fermions in real space within graphene poses considerable challenges due to the isotropic transport within the continuum energy range for degenerate valleys.","This study unveils an interesting finding: introducing valley contrast through anisotropic chiral transport in isotropic Dirac systems like graphene, achieved by implementing a tilted PN junction.","The tilted junction shifts the angular spectrum to larger angles in accordance with the tilt angle.","This modifies the pseudospin-conserved modes across the junction, resulting in valley-resolved chiral transport.","This approach not only induces valley splitting within the real space but also preserves the remarkable mobility of fermions, offering distinct advantages over alternative strategies.","The comprehensive analysis includes optimizing the experimental setup, scrutinizing factors such as the sequence of the doped region, and examining critical parameters like the tilt angle delta and transition width d across the junction.","Surprisingly, an increased transition width enhances transmission, attributed to specular edge scattering.","Importantly, the system remains resilient to Anderson short-range edge disorder.","The broader implication lies in the transformative potential of inducing analogous anisotropic chiral transport behaviors in isotropic Dirac systems, resembling the characteristics of tilted Dirac-Weyl semimetals, by incorporating a tilted PNJ."],"url":"http://arxiv.org/abs/2402.14210v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-22 00:28:31","title":"Optimal transmission expansion minimally reduces decarbonization costs of U.S. electricity","abstract":"Solar and wind power are cost-competitive with fossil fuels, yet their intermittent nature presents challenges. Significant temporal and geographic differences in land, wind, and solar resources suggest that long-distance transmission could be particularly beneficial. Using a detailed, open-source model, we analyze optimal transmission expansion jointly with storage, generation, and hourly operations across the three primary interconnects in the United States. Transmission expansion offers far more benefits in a high-renewable system than in a system with mostly conventional generation. Yet while an optimal nationwide plan would have more than triple current interregional transmission, transmission decreases the cost of a 100% clean system by only 4% compared to a plan that relies solely on current transmission. Expanding capacity only within existing interconnects can achieve most of these savings. Adjustments to energy storage and generation mix can leverage the current interregional transmission infrastructure to build a clean power system at a reasonable cost.","sentences":["Solar and wind power are cost-competitive with fossil fuels, yet their intermittent nature presents challenges.","Significant temporal and geographic differences in land, wind, and solar resources suggest that long-distance transmission could be particularly beneficial.","Using a detailed, open-source model, we analyze optimal transmission expansion jointly with storage, generation, and hourly operations across the three primary interconnects in the United States.","Transmission expansion offers far more benefits in a high-renewable system than in a system with mostly conventional generation.","Yet while an optimal nationwide plan would have more than triple current interregional transmission, transmission decreases the cost of a 100% clean system by only 4% compared to a plan that relies solely on current transmission.","Expanding capacity only within existing interconnects can achieve most of these savings.","Adjustments to energy storage and generation mix can leverage the current interregional transmission infrastructure to build a clean power system at a reasonable cost."],"url":"http://arxiv.org/abs/2402.14189v1","category":"econ.GN"}
{"created":"2024-02-21 23:39:03","title":"Perfect Tracking of Time-Varying Optimum by Extremum Seeking","abstract":"This paper introduces extremum seeking (ES) algorithms designed to achieve perfect tracking of arbitrary time-varying extremum. In contrast to classical ES approaches that employ constant frequencies and controller gains, our algorithms leverage time-varying parameters, growing either asymptotically or exponentially, to achieve desired convergence behaviors. Our stability analysis involves state transformation, time-dilation transformation, and Lie bracket averaging. The state transformation is based on the multiplication of the input state by asymptotic or exponential growth functions. The time transformation enables tracking of the extremum as it gradually converges to a constant value when viewed in the dilated time domain. Finally, Lie bracket averaging is applied to the transformed system, ensuring practical uniform stability in the dilated time domain as well as asymptotic or exponential stability of the original system in the original time domain. We validate the feasibility of these designs through numerical simulations.","sentences":["This paper introduces extremum seeking (ES) algorithms designed to achieve perfect tracking of arbitrary time-varying extremum.","In contrast to classical ES approaches that employ constant frequencies and controller gains, our algorithms leverage time-varying parameters, growing either asymptotically or exponentially, to achieve desired convergence behaviors.","Our stability analysis involves state transformation, time-dilation transformation, and Lie bracket averaging.","The state transformation is based on the multiplication of the input state by asymptotic or exponential growth functions.","The time transformation enables tracking of the extremum as it gradually converges to a constant value when viewed in the dilated time domain.","Finally, Lie bracket averaging is applied to the transformed system, ensuring practical uniform stability in the dilated time domain as well as asymptotic or exponential stability of the original system in the original time domain.","We validate the feasibility of these designs through numerical simulations."],"url":"http://arxiv.org/abs/2402.14178v1","category":"math.OC"}
{"created":"2024-02-21 20:34:52","title":"Strategies and safety simulations for ultrasonic cervical spinal cord neuromodulation","abstract":"Focused ultrasound spinal cord neuromodulation studies have demonstrated spinal cord neuromodulation in small animals. The safe and efficacious translation of these approaches to human scale requires an understanding of ultrasound propagation and heat deposition within the human spine. To address this, combined acoustic and thermal modelling was used to assess the pressure and heat distributions produced by a 500 kHz source focused to the C5/C6 level of the cervical spine via two approaches a) the posterior acoustic window between vertebral posterior arches, or b) the lateral intervertebral foramen from which the C6 spinal nerve exits. Pulse trains of 150 0.1 s pulses with a pulse repetition frequency of 0.33 Hz and free-field spatial peak pulse-averaged intensity of 10 W/cm^2 were simulated for the CT volumes of four subjects and for $\\pm$10 mm translational and $\\pm$10{\\deg} rotational source positioning errors. Target pressures ranged between 20% and 70% of free-field spatial peak pressures with the posterior approach, and 20% and 100% with the lateral approach. When the source was optimally positioned with the posterior approach, peak spine heating values were below 1{\\deg}C, but source mis-positioning resulted in bone heating up to 4{\\deg}C. Heating with the lateral approach did not exceed 2{\\deg}C within the mispositioning range. There were substantial inter-subject differences in target pressures and peak heating values. Target pressure varied three to four-fold between subjects, depending on approach, while peak heating varied approximately two-fold between subjects. This results in a near ten-fold range in the target pressure achieved per degree of peak heating between subjects. This highlights the importance of developing trans-spine ultrasound simulation software for the assurance of subject-specific safety and efficacy of focused ultrasound spinal cord therapies.","sentences":["Focused ultrasound spinal cord neuromodulation studies have demonstrated spinal cord neuromodulation in small animals.","The safe and efficacious translation of these approaches to human scale requires an understanding of ultrasound propagation and heat deposition within the human spine.","To address this, combined acoustic and thermal modelling was used to assess the pressure and heat distributions produced by a 500 kHz source focused to the C5/C6 level of the cervical spine via two approaches a) the posterior acoustic window between vertebral posterior arches, or b) the lateral intervertebral foramen from which the C6 spinal nerve exits.","Pulse trains of 150 0.1 s pulses with a pulse repetition frequency of 0.33 Hz and free-field spatial peak pulse-averaged intensity of 10 W/cm^2 were simulated for the CT volumes of four subjects and for $\\pm$10 mm translational and $\\pm$10{\\deg} rotational source positioning errors.","Target pressures ranged between 20% and 70% of free-field spatial peak pressures with the posterior approach, and 20% and 100% with the lateral approach.","When the source was optimally positioned with the posterior approach, peak spine heating values were below 1{\\deg}C, but source mis-positioning resulted in bone heating up to 4{\\deg}C. Heating with the lateral approach did not exceed 2{\\deg}C within the mispositioning range.","There were substantial inter-subject differences in target pressures and peak heating values.","Target pressure varied three to four-fold between subjects, depending on approach, while peak heating varied approximately two-fold between subjects.","This results in a near ten-fold range in the target pressure achieved per degree of peak heating between subjects.","This highlights the importance of developing trans-spine ultrasound simulation software for the assurance of subject-specific safety and efficacy of focused ultrasound spinal cord therapies."],"url":"http://arxiv.org/abs/2402.14117v1","category":"physics.med-ph"}
{"created":"2024-02-21 19:52:22","title":"A Note on Optimal Liquidation with Linear Price Impact","abstract":"In this note we consider the maximization of the expected terminal wealth for the setup of quadratic transaction costs. First, we provide a very simple probabilistic solution to the problem. Although the problem was largely studied, as far as we know up to date this simple and probabilistic form of the solution has not appeared in the literature. Next, we apply the general result for the study of the case where the risky asset is given by a fractional Brownian Motion and the information flow of the investor can be diversified.","sentences":["In this note we consider the maximization of the expected terminal wealth for the setup of quadratic transaction costs.","First, we provide a very simple probabilistic solution to the problem.","Although the problem was largely studied, as far as we know up to date this simple and probabilistic form of the solution has not appeared in the literature.","Next, we apply the general result for the study of the case where the risky asset is given by a fractional Brownian Motion and the information flow of the investor can be diversified."],"url":"http://arxiv.org/abs/2402.14100v1","category":"q-fin.CP"}
{"created":"2024-02-21 19:00:01","title":"An optimized basis for hadronic light-by-light scattering","abstract":"We present a new basis for the hadronic light-by-light (HLbL) tensor that is optimized for the evaluation of narrow-resonance contributions to HLbL scattering in the anomalous magnetic moment of the muon. As main advantage, kinematic singularities are manifestly absent for pseudoscalar, scalar, and axial-vector states, while the remaining singularities for tensor resonances are minimized, even avoided for special cases, and simple crossing relations among the scalar functions maintained. We scrutinize the properties of this new basis for the scalar-QED pion box, demonstrating that the partial-wave convergence even slightly improves compared to our previous work, and discuss the physical sum rules that ensure basis independence of the HLbL contribution. Finally, we provide explicit expressions for narrow (pseudo-)scalar, axial-vector, and tensor intermediate states in terms of their respective transition form factors.","sentences":["We present a new basis for the hadronic light-by-light (HLbL) tensor that is optimized for the evaluation of narrow-resonance contributions to HLbL scattering in the anomalous magnetic moment of the muon.","As main advantage, kinematic singularities are manifestly absent for pseudoscalar, scalar, and axial-vector states, while the remaining singularities for tensor resonances are minimized, even avoided for special cases, and simple crossing relations among the scalar functions maintained.","We scrutinize the properties of this new basis for the scalar-QED pion box, demonstrating that the partial-wave convergence even slightly improves compared to our previous work, and discuss the physical sum rules that ensure basis independence of the HLbL contribution.","Finally, we provide explicit expressions for narrow (pseudo-)scalar, axial-vector, and tensor intermediate states in terms of their respective transition form factors."],"url":"http://arxiv.org/abs/2402.14060v1","category":"hep-ph"}
{"created":"2024-02-21 18:57:54","title":"On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation","abstract":"This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify a favorable parameter allocation towards model depth rather than width when employing encoder-decoder architectures initialized with encoder-only PLMs. The study sheds light on the potential of utilizing encoder-only PLMs for advancing KPG systems and provides a groundwork for future KPG methods. Our code and pre-trained checkpoints are released at https://github.com/uclanlp/DeepKPG.","sentences":["This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models.","We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings.","Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions.","Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs.","We also identify a favorable parameter allocation towards model depth rather than width when employing encoder-decoder architectures initialized with encoder-only PLMs.","The study sheds light on the potential of utilizing encoder-only PLMs for advancing KPG systems and provides a groundwork for future KPG methods.","Our code and pre-trained checkpoints are released at https://github.com/uclanlp/DeepKPG."],"url":"http://arxiv.org/abs/2402.14052v1","category":"cs.CL"}
{"created":"2024-02-21 18:52:20","title":"Misalignment, Learning, and Ranking: Harnessing Users Limited Attention","abstract":"In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds. The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item. This method systematically narrows down the item choices to enhance learning efficiency and payoff.   Second, we consider adversarial payoffs and stochastic iid window sizes. We start from the full-information problem of finding the permutation that maximizes the expected payoff. By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation. Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret.","sentences":["In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs.","This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs.","Our paper tackles this issue by utilizing users' limited attention spans.","We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time.","Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item).","We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   ","We first consider adversarial window sizes and stochastic iid payoffs.","We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds.","The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item.","This method systematically narrows down the item choices to enhance learning efficiency and payoff.   ","Second, we consider adversarial payoffs and stochastic iid window sizes.","We start from the full-information problem of finding the permutation that maximizes the expected payoff.","By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation.","Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret."],"url":"http://arxiv.org/abs/2402.14013v1","category":"cs.LG"}
{"created":"2024-02-21 18:51:42","title":"Chasing Convex Functions with Long-term Constraints","abstract":"We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems. We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments.","sentences":["We introduce and study a family of online metric problems with long-term constraints.","In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric.","Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems.","We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments."],"url":"http://arxiv.org/abs/2402.14012v1","category":"cs.DS"}
{"created":"2024-02-21 18:09:37","title":"Plug-and-play analytical paradigm for the scattering of plane waves by \"layer-cake\" periodic systems","abstract":"We investigate the scattering of scalar plane waves in two dimensions by a heterogeneous layer that is periodic in the direction parallel to its boundary. On describing the layer as a union of periodic laminae, we develop a solution of the scattering problem by merging the concept of propagator matrices and that of Bloch eigenstates featured by the unit cell of each lamina. The featured Bloch eigenstates are obtained by solving the quadratic eigenvalue problem (QEVP) that seeks a complex-valued wavenumber normal to the layer boundary given (i) the excitation frequency, and (ii) real-valued wavenumber parallel to the boundary -- that is preserved throughout the system. Spectral analysis of the QEVP reveals sufficient conditions for discreteness of the eigenvalue spectrum and the fact that all eigenvalues come in complex-conjugate pairs. By deploying the factorization afforded by the propagator matrix approach, we demonstrate that the contribution of individual eigenvalues (and so eigenmodes) to the solution diminishes exponentially with absolute value of their imaginary part, which then forms a rational basis for truncation of the factorized Bloch-wave solution. The proposed methodology caters for the optimal design of rainbow traps, energy harvesters, and metasurfaces, whose potency to manipulate waves is decided not only by the individual dispersion characteristics of the component laminae, but also by ordering and generally fitting of the latter into a composite layer. Using the factorized Bloch approach, evaluation of trial configurations -- as generated by the permutation and window translation/stretching of the component laminae -- can be accelerated by decades.","sentences":["We investigate the scattering of scalar plane waves in two dimensions by a heterogeneous layer that is periodic in the direction parallel to its boundary.","On describing the layer as a union of periodic laminae, we develop a solution of the scattering problem by merging the concept of propagator matrices and that of Bloch eigenstates featured by the unit cell of each lamina.","The featured Bloch eigenstates are obtained by solving the quadratic eigenvalue problem (QEVP) that seeks a complex-valued wavenumber normal to the layer boundary given (i) the excitation frequency, and (ii) real-valued wavenumber parallel to the boundary -- that is preserved throughout the system.","Spectral analysis of the QEVP reveals sufficient conditions for discreteness of the eigenvalue spectrum and the fact that all eigenvalues come in complex-conjugate pairs.","By deploying the factorization afforded by the propagator matrix approach, we demonstrate that the contribution of individual eigenvalues (and so eigenmodes) to the solution diminishes exponentially with absolute value of their imaginary part, which then forms a rational basis for truncation of the factorized Bloch-wave solution.","The proposed methodology caters for the optimal design of rainbow traps, energy harvesters, and metasurfaces, whose potency to manipulate waves is decided not only by the individual dispersion characteristics of the component laminae, but also by ordering and generally fitting of the latter into a composite layer.","Using the factorized Bloch approach, evaluation of trial configurations -- as generated by the permutation and window translation/stretching of the component laminae -- can be accelerated by decades."],"url":"http://arxiv.org/abs/2402.13981v1","category":"math-ph"}
{"created":"2024-02-21 17:45:03","title":"Evaluating Ground State Energies of Chemical Systems with Low-Depth Quantum Circuits and High Accuracy","abstract":"Solving electronic structure problems is considered one of the most promising applications of quantum computing. However, due to limitations imposed by the coherence time of qubits in the Noisy Intermediate Scale Quantum (NISQ) era or the capabilities of early fault-tolerant quantum devices, it is vital to design algorithms with low-depth circuits. In this work, we develop an enhanced Variational Quantum Eigensolver (VQE) ansatz based on the Qubit Coupled Cluster (QCC) approach, which demands optimization over only $n$ parameters rather than the usual $n+2m$ parameters, where $n$ represents the number of Pauli string time evolution gates $e^{-itP}$, and $m$ is the number of qubits involved. We evaluate the ground state energies of $\\mathrm{O_3}$, $\\mathrm{Li_4}$, and $\\mathrm{Cr_2}$, using CAS(2,2), (4,4) and (6,6) respectively in conjunction with our enhanced QCC ansatz, UCCSD (Unitary Coupled Cluster Single Double) ansatz, and canonical CCSD method as the active space solver, and compare with CASCI results. Finally, we assess our enhanced QCC ansatz on two distinct quantum hardware, IBM Kolkata and Quantinuum H1-1.","sentences":["Solving electronic structure problems is considered one of the most promising applications of quantum computing.","However, due to limitations imposed by the coherence time of qubits in the Noisy Intermediate Scale Quantum (NISQ) era or the capabilities of early fault-tolerant quantum devices, it is vital to design algorithms with low-depth circuits.","In this work, we develop an enhanced Variational Quantum Eigensolver (VQE) ansatz based on the Qubit Coupled Cluster (QCC) approach, which demands optimization over only $n$ parameters rather than the usual $n+2m$ parameters, where $n$ represents the number of Pauli string time evolution gates $e^{-itP}$, and $m$ is the number of qubits involved.","We evaluate the ground state energies of $\\mathrm{O_3}$, $\\mathrm{Li_4}$, and $\\mathrm{Cr_2}$, using CAS(2,2), (4,4) and (6,6) respectively in conjunction with our enhanced QCC ansatz, UCCSD (Unitary Coupled Cluster Single Double) ansatz, and canonical CCSD method as the active space solver, and compare with CASCI results.","Finally, we assess our enhanced QCC ansatz on two distinct quantum hardware, IBM Kolkata and Quantinuum H1-1."],"url":"http://arxiv.org/abs/2402.13960v1","category":"quant-ph"}
{"created":"2024-02-21 17:40:51","title":"Improving threshold for fault-tolerant color code quantum computing by flagged weight optimization","abstract":"Color codes are promising quantum error correction (QEC) codes because they have an advantage over surface codes in that all Clifford gates can be implemented transversally. However, thresholds of color codes under circuit-level noise are relatively low mainly because measurements of their high-weight stabilizer generators cause an increase in a circuit depth, and thus, substantial errors are introduced. This makes color codes not the best candidate. Here, we propose a method to suppress the impact of such errors by optimizing weights of decoders using flag qubits and reducing the circuit depth using cat states. We set the weights based on conditional error probabilities conditioned on the measurement outcomes of flag qubits. In numerical simulations, we improve the threshold of the (4.8.8) color code under the circuit-level noise from 0.14% to around 0.27%, which is calculated by using an integer programming decoder. Furthermore, in the (6.6.6) color code, we achieved a circuit-level threshold of around 0.36%, which is almost the same value as the highest value in the previous studies employing the same noise model. In both cases, the achieved logical error rates at low physical error rates are almost one order of magnitude lower than a conventional method that uses a single ancilla qubit for each stabilizer measurement. This method can also be applied to other weight-based decoders, making the color codes more promising for the candidate of experimental implementation of QEC. Furthermore, one can utilize this approach to improve a threshold of wider classes of QEC codes, such as high-rate quantum low-density parity check codes.","sentences":["Color codes are promising quantum error correction (QEC) codes because they have an advantage over surface codes in that all Clifford gates can be implemented transversally.","However, thresholds of color codes under circuit-level noise are relatively low mainly because measurements of their high-weight stabilizer generators cause an increase in a circuit depth, and thus, substantial errors are introduced.","This makes color codes not the best candidate.","Here, we propose a method to suppress the impact of such errors by optimizing weights of decoders using flag qubits and reducing the circuit depth using cat states.","We set the weights based on conditional error probabilities conditioned on the measurement outcomes of flag qubits.","In numerical simulations, we improve the threshold of the (4.8.8) color code under the circuit-level noise from 0.14% to around 0.27%, which is calculated by using an integer programming decoder.","Furthermore, in the (6.6.6) color code, we achieved a circuit-level threshold of around 0.36%, which is almost the same value as the highest value in the previous studies employing the same noise model.","In both cases, the achieved logical error rates at low physical error rates are almost one order of magnitude lower than a conventional method that uses a single ancilla qubit for each stabilizer measurement.","This method can also be applied to other weight-based decoders, making the color codes more promising for the candidate of experimental implementation of QEC.","Furthermore, one can utilize this approach to improve a threshold of wider classes of QEC codes, such as high-rate quantum low-density parity check codes."],"url":"http://arxiv.org/abs/2402.13958v1","category":"quant-ph"}
{"created":"2024-02-21 17:32:00","title":"On Courant and Pleijel theorems for sub-Riemannian Laplacians","abstract":"We are interested in the number of nodal domains of eigenfunctions of sub-Laplacians on sub-Riemannian manifolds. Specifically, we investigate the validity of Pleijel's theorem, which states that, as soon as the dimension is strictly larger than 1, the number of nodal domains of an eigenfunction corresponding to the k-th eigenvalue is strictly (and uniformly, in a certain sense) smaller than k for large k.   In the first part of this paper we reduce this question from the case of general sub-Riemannian manifolds to that of nilpotent groups.   In the second part, we analyze in detail the case where the nilpotent group is a Heisenberg group times a Euclidean space. Along the way we improve known bounds on the optimal constants in the Faber-Krahn and isoperimetric inequalities on these groups.","sentences":["We are interested in the number of nodal domains of eigenfunctions of sub-Laplacians on sub-Riemannian manifolds.","Specifically, we investigate the validity of Pleijel's theorem, which states that, as soon as the dimension is strictly larger than 1, the number of nodal domains of an eigenfunction corresponding to the k-th eigenvalue is strictly (and uniformly, in a certain sense) smaller than k for large k.   ","In the first part of this paper we reduce this question from the case of general sub-Riemannian manifolds to that of nilpotent groups.   ","In the second part, we analyze in detail the case where the nilpotent group is a Heisenberg group times a Euclidean space.","Along the way we improve known bounds on the optimal constants in the Faber-Krahn and isoperimetric inequalities on these groups."],"url":"http://arxiv.org/abs/2402.13953v1","category":"math.SP"}
{"created":"2024-02-21 17:23:43","title":"Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements","abstract":"The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles. Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern. However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern. To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics. Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning. We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices.","sentences":["The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles.","Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern.","However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern.","To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics.","Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning.","We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices."],"url":"http://arxiv.org/abs/2402.13949v1","category":"cs.RO"}
{"created":"2024-02-21 17:08:00","title":"Microstructured large-area photoconductive terahertz emitters driven at high average power","abstract":"Emitters based on photoconductive materials excited by ultrafast lasers are well established and popular devices for THz generation. However, so far, these emitters, both photoconductive antennas and large area emitters, were mostly explored using driving lasers with moderate average powers (either fiber lasers with up to hundreds of milliwatts or Ti:Sapphire systems up to few watts). In this paper, we explore the use of high power, MHz repetition rate Ytterbium (Yb) based oscillator for THz emission using a microstructured large area photoconductive emitter, consist of semi insulating GaAs with a 10 by 10 mm2 active area. As a driving source, we use a frequency doubled home built high average power ultrafast Yb oscillator, delivering 22 W of average power, 115 fs pulses with 91 MHz repetition rate at a central wavelength of 516 nm. When applying 9 W of average power (after an optical chopper with a duty cycle of 50 percent) on the structure without optimized heatsinking, we obtain 65 uW THz average power, 4 THz bandwidth; furthermore, we safely apply up to 18 W of power on the structure without observing damage. We investigate the impact of excitation power, bias voltage, optical fluence, and their interplay on the emitter performance and explore in detail the sources of thermal load originating from electrical and optical power. Optical power is found to have a more critical impact on LAE saturation than electrical power, thus optimized heatsinking will allow us to improve the conversion efficiency in the near future towards much higher emitter power. This work paves the way towards achieving hundreds of MHz or even GHz repetition rates, high power THz sources based on photoconductive emitters, that are of great interest for example for future THz imaging applications.","sentences":["Emitters based on photoconductive materials excited by ultrafast lasers are well established and popular devices for THz generation.","However, so far, these emitters, both photoconductive antennas and large area emitters, were mostly explored using driving lasers with moderate average powers (either fiber lasers with up to hundreds of milliwatts or Ti:Sapphire systems up to few watts).","In this paper, we explore the use of high power, MHz repetition rate Ytterbium (Yb) based oscillator for THz emission using a microstructured large area photoconductive emitter, consist of semi insulating GaAs with a 10 by 10 mm2 active area.","As a driving source, we use a frequency doubled home built high average power ultrafast Yb oscillator, delivering 22 W of average power, 115 fs pulses with 91 MHz repetition rate at a central wavelength of 516 nm.","When applying 9 W of average power (after an optical chopper with a duty cycle of 50 percent) on the structure without optimized heatsinking, we obtain 65 uW THz average power, 4 THz bandwidth; furthermore, we safely apply up to 18 W of power on the structure without observing damage.","We investigate the impact of excitation power, bias voltage, optical fluence, and their interplay on the emitter performance and explore in detail the sources of thermal load originating from electrical and optical power.","Optical power is found to have a more critical impact on LAE saturation than electrical power, thus optimized heatsinking will allow us to improve the conversion efficiency in the near future towards much higher emitter power.","This work paves the way towards achieving hundreds of MHz or even GHz repetition rates, high power THz sources based on photoconductive emitters, that are of great interest for example for future THz imaging applications."],"url":"http://arxiv.org/abs/2402.13940v1","category":"physics.optics"}
{"created":"2024-02-21 17:05:06","title":"Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning","abstract":"Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs. Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image. This objective acts as an additional learning signal grounded to the distribution of the GT captions. Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate. Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality.","sentences":["Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility.","Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions.","Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions.","However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework.","We propose a new image captioning model training strategy that makes use of GT captions in different ways.","Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs.","Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image.","This objective acts as an additional learning signal grounded to the distribution of the GT captions.","Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate.","Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality."],"url":"http://arxiv.org/abs/2402.13936v1","category":"cs.CL"}
{"created":"2024-02-21 16:39:28","title":"Practical algorithms for Hierarchical overlap graphs","abstract":"Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings. Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss. The scalable de Bruijn graphs come at the price of losing crucial overlap information. The complete overlap information is stored in overlap graphs using quadratic space. Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space. After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021], where only the former was seemingly practical.   We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm [CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory. However, it uses non-intuitive approach and non-trivial data structures. We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal. Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings. Loukides et al. [CPM2023] recently presented state-of-the-art algorithms for these queries. However, these algorithms require complex black-box data structures and are seemingly impractical. Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters.","sentences":["Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings.","Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss.","The scalable de Bruijn graphs come at the price of losing crucial overlap information.","The complete overlap information is stored in overlap graphs using quadratic space.","Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space.","After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms","[CPM2021], where only the former was seemingly practical.   ","We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm","[CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory.","However, it uses non-intuitive approach and non-trivial data structures.","We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal.","Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   ","We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings.","Loukides et al.","[CPM2023] recently presented state-of-the-art algorithms for these queries.","However, these algorithms require complex black-box data structures and are seemingly impractical.","Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters."],"url":"http://arxiv.org/abs/2402.13920v1","category":"cs.DS"}
{"created":"2024-02-21 16:23:14","title":"Quadratic inference with dense functional responses","abstract":"We address the challenge of estimation in the context of constant linear effect models with dense functional responses. In this framework, the conditional expectation of the response curve is represented by a linear combination of functional covariates with constant regression parameters. In this paper, we present an alternative solution by employing the quadratic inference approach, a well-established method for analyzing correlated data, to estimate the regression coefficients. Our approach leverages non-parametrically estimated basis functions, eliminating the need for choosing working correlation structures. Furthermore, we demonstrate that our method achieves a parametric $\\sqrt{n}$-convergence rate, contingent on an appropriate choice of bandwidth. This convergence is observed when the number of repeated measurements per trajectory exceeds a certain threshold, specifically, when it surpasses $n^{a_{0}}$, with $n$ representing the number of trajectories. Additionally, we establish the asymptotic normality of the resulting estimator. The performance of the proposed method is compared with that of existing methods through extensive simulation studies, where our proposed method outperforms. Real data analysis is also conducted to demonstrate the proposed method.","sentences":["We address the challenge of estimation in the context of constant linear effect models with dense functional responses.","In this framework, the conditional expectation of the response curve is represented by a linear combination of functional covariates with constant regression parameters.","In this paper, we present an alternative solution by employing the quadratic inference approach, a well-established method for analyzing correlated data, to estimate the regression coefficients.","Our approach leverages non-parametrically estimated basis functions, eliminating the need for choosing working correlation structures.","Furthermore, we demonstrate that our method achieves a parametric $\\sqrt{n}$-convergence rate, contingent on an appropriate choice of bandwidth.","This convergence is observed when the number of repeated measurements per trajectory exceeds a certain threshold, specifically, when it surpasses $n^{a_{0}}$, with $n$ representing the number of trajectories.","Additionally, we establish the asymptotic normality of the resulting estimator.","The performance of the proposed method is compared with that of existing methods through extensive simulation studies, where our proposed method outperforms.","Real data analysis is also conducted to demonstrate the proposed method."],"url":"http://arxiv.org/abs/2402.13907v1","category":"stat.ME"}
