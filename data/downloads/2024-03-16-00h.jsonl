{"created":"2024-03-14 17:59:55","title":"SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior","abstract":"Semantic image synthesis (SIS) shows good promises for sensor simulation. However, current best practices in this field, based on GANs, have not yet reached the desired level of quality. As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page.","sentences":["Semantic image synthesis (SIS) shows good promises for sensor simulation.","However, current best practices in this field, based on GANs, have not yet reached the desired level of quality.","As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities.","Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask.","Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage.","To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference.","This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page."],"url":"http://arxiv.org/abs/2403.09638v1","category":"cs.CV"}
{"created":"2024-03-14 17:59:46","title":"GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping","abstract":"Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models. With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks. Data and codes can be available at https://github.com/MrSecant/GaussianGrasper.","sentences":["Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics.","Such technology facilitates robots in executing object manipulations based on human language directives.","To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields.","However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference.","Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives.","Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field.","In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models.","With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates.","Furthermore, we propose a normal-guided grasp module to select the best grasp pose.","Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks.","Data and codes can be available at https://github.com/MrSecant/GaussianGrasper."],"url":"http://arxiv.org/abs/2403.09637v1","category":"cs.RO"}
{"created":"2024-03-14 17:59:26","title":"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference","abstract":"Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget.","sentences":["Transformers have emerged as the backbone of large language models (LLMs).","However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size.","As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time.","Most importantly, the model learns to apply different compression rates in different heads and layers.","We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU.","DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters.","We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA).","GQA and DMC can be even combined to obtain compounded gains.","As a result DMC fits longer contexts and larger batches within any given memory budget."],"url":"http://arxiv.org/abs/2403.09636v1","category":"cs.CL"}
{"created":"2024-03-14 17:59:14","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models","abstract":"In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for image classification.","sentences":["In spite of their huge success, transformer models remain difficult to scale in depth.","In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model.","Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores.","We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers.","We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes.","These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for image classification."],"url":"http://arxiv.org/abs/2403.09635v1","category":"cs.CL"}
{"created":"2024-03-14 17:59:13","title":"OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning","abstract":"Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance.","sentences":["Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame.","Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking.","Despite the different input modalities, the core aspect of tracking is the temporal matching.","Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker.","OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker.","This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object.","Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker.","Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks.","To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.09634v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:56","title":"Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image","abstract":"At the core of portrait photography is the search for ideal lighting and viewpoint. The process often requires advanced knowledge in photography and an elaborate studio setup. In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image. Holo-Relighting leverages the pretrained 3D GAN (EG3D) to reconstruct geometry and appearance from an input portrait as a set of 3D-aware features. We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering. Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects. With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors. We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system. Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability.","sentences":["At the core of portrait photography is the search for ideal lighting and viewpoint.","The process often requires advanced knowledge in photography and an elaborate studio setup.","In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image.","Holo-Relighting leverages the pretrained 3D GAN (EG3D) to reconstruct geometry and appearance from an input portrait as a set of 3D-aware features.","We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering.","Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects.","With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors.","We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system.","Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability."],"url":"http://arxiv.org/abs/2403.09632v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:41","title":"3D-VLA: A 3D Vision-Language-Action Generative World Model","abstract":"Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.","sentences":["Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world.","Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics.","In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly.","To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model.","Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment.","Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds.","To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets.","Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications."],"url":"http://arxiv.org/abs/2403.09631v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:33","title":"Generalized Predictive Model for Autonomous Driving","abstract":"In this paper, we introduce the first large-scale video prediction model in the autonomous driving discipline. To eliminate the restriction of high-cost data collection and empower the generalization ability of our model, we acquire massive data from the web and pair it with diverse and high-quality text descriptions. The resultant dataset accumulates over 2000 hours of driving videos, spanning areas all over the world with diverse weather conditions and traffic scenarios. Inheriting the merits from recent latent diffusion models, our model, dubbed GenAD, handles the challenging dynamics in driving scenes with novel temporal reasoning blocks. We showcase that it can generalize to various unseen driving datasets in a zero-shot manner, surpassing general or driving-specific video prediction counterparts. Furthermore, GenAD can be adapted into an action-conditioned prediction model or a motion planner, holding great potential for real-world driving applications.","sentences":["In this paper, we introduce the first large-scale video prediction model in the autonomous driving discipline.","To eliminate the restriction of high-cost data collection and empower the generalization ability of our model, we acquire massive data from the web and pair it with diverse and high-quality text descriptions.","The resultant dataset accumulates over 2000 hours of driving videos, spanning areas all over the world with diverse weather conditions and traffic scenarios.","Inheriting the merits from recent latent diffusion models, our model, dubbed GenAD, handles the challenging dynamics in driving scenes with novel temporal reasoning blocks.","We showcase that it can generalize to various unseen driving datasets in a zero-shot manner, surpassing general or driving-specific video prediction counterparts.","Furthermore, GenAD can be adapted into an action-conditioned prediction model or a motion planner, holding great potential for real-world driving applications."],"url":"http://arxiv.org/abs/2403.09630v1","category":"cs.CV"}
{"created":"2024-03-14 17:58:16","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking","abstract":"When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.","sentences":["When writing and talking, people sometimes pause to think.","Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text.","For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation.","In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer.","This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text.","We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions.","We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens.","To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique.","Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions.","In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\\rightarrow$10.9%) and CommonsenseQA","(36.3%$\\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text.","Crucially, these improvements require no fine-tuning on these tasks.","Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way."],"url":"http://arxiv.org/abs/2403.09629v1","category":"cs.CL"}
{"created":"2024-03-14 17:57:04","title":"Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation","abstract":"Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts. In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimization and subject-prior optimization, respectively. Extensive experiments demonstrate that our method can produce high-quality, consistent, and subject-specific 3D content with text-driven modifications that are unseen in subject image.","sentences":["Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language.","However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts.","In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes.","Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject.","Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimization and subject-prior optimization, respectively.","Extensive experiments demonstrate that our method can produce high-quality, consistent, and subject-specific 3D content with text-driven modifications that are unseen in subject image."],"url":"http://arxiv.org/abs/2403.09625v1","category":"cs.CV"}
{"created":"2024-03-14 17:55:33","title":"Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering","abstract":"Visual text rendering poses a fundamental challenge for contemporary text-to-image generation models, with the core problem lying in text encoder deficiencies. To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs. Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by fine-tuning the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset. We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation. This significantly enhances text rendering accuracy, improving it from less than $20\\%$ to nearly $90\\%$ on our design image benchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images. These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks.","sentences":["Visual text rendering poses a fundamental challenge for contemporary text-to-image generation models, with the core problem lying in text encoder deficiencies.","To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs.","Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by fine-tuning the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset.","We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation.","This significantly enhances text rendering accuracy, improving it from less than $20\\%$ to nearly $90\\%$ on our design image benchmark.","Noteworthy is Glyph-SDXL's newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts.","Finally, through fine-tuning Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images.","These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks."],"url":"http://arxiv.org/abs/2403.09622v1","category":"cs.CV"}
{"created":"2024-03-14 17:55:10","title":"Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning","abstract":"Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depend on a variety of new techniques, involving a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest.","sentences":["Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces.","However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation.","Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL.","Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL.","Our algorithms and theoretical results crucially depend on a variety of new techniques, involving a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest."],"url":"http://arxiv.org/abs/2403.09621v1","category":"cs.LG"}
{"created":"2024-03-14 17:55:03","title":"PosSAM: Panoptic Open-vocabulary Segment Anything","abstract":"In this paper, we introduce an open-vocabulary panoptic segmentation model that effectively unifies the strengths of the Segment Anything Model (SAM) with the vision-language CLIP model in an end-to-end framework. While SAM excels in generating spatially-aware masks, it's decoder falls short in recognizing object class information and tends to oversegment without additional guidance. Existing approaches address this limitation by using multi-stage techniques and employing separate models to generate class-aware prompts, such as bounding boxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model which leverages SAM's spatially rich features to produce instance-aware masks and harnesses CLIP's semantically discriminative features for effective instance classification. Specifically, we address the limitations of SAM and propose a novel Local Discriminative Pooling (LDP) module leveraging class-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary classification. Furthermore, we introduce a Mask-Aware Selective Ensembling (MASE) algorithm that adaptively enhances the quality of generated masks and boosts the performance of open-vocabulary classification during inference for each image. We conducted extensive experiments to demonstrate our methods strong generalization properties across multiple datasets, achieving state-of-the-art performance with substantial improvements over SOTA open-vocabulary panoptic segmentation methods. In both COCO to ADE20K and ADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art methods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website: https://vibashan.github.io/possam-web/.","sentences":["In this paper, we introduce an open-vocabulary panoptic segmentation model that effectively unifies the strengths of the Segment Anything Model (SAM) with the vision-language CLIP model in an end-to-end framework.","While SAM excels in generating spatially-aware masks, it's decoder falls short in recognizing object class information and tends to oversegment without additional guidance.","Existing approaches address this limitation by using multi-stage techniques and employing separate models to generate class-aware prompts, such as bounding boxes or segmentation masks.","Our proposed method, PosSAM is an end-to-end model which leverages SAM's spatially rich features to produce instance-aware masks and harnesses CLIP's semantically discriminative features for effective instance classification.","Specifically, we address the limitations of SAM and propose a novel Local Discriminative Pooling (LDP) module leveraging class-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary classification.","Furthermore, we introduce a Mask-Aware Selective Ensembling (MASE) algorithm that adaptively enhances the quality of generated masks and boosts the performance of open-vocabulary classification during inference for each image.","We conducted extensive experiments to demonstrate our methods strong generalization properties across multiple datasets, achieving state-of-the-art performance with substantial improvements over SOTA open-vocabulary panoptic segmentation methods.","In both COCO to ADE20K and ADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art methods by a large margin, 2.4 PQ and 4.6 PQ, respectively.","Project Website: https://vibashan.github.io/possam-web/."],"url":"http://arxiv.org/abs/2403.09620v1","category":"cs.CV"}
{"created":"2024-03-14 17:54:27","title":"Dynamics of Pseudoentanglement","abstract":"The dynamics of quantum entanglement plays a central role in explaining the emergence of thermal equilibrium in isolated many-body systems. However, entanglement is notoriously hard to measure, and can in fact be forged: recent works have introduced a notion of pseudoentanglement describing ensembles of many-body states that, while only weakly entangled, cannot be efficiently distinguished from states with much higher entanglement, such as random states in the Hilbert space. In this work we initiate the study of the dynamical generation and propagation of pseudoentanglement. As generic quantum dynamics tends to maximize actual entanglement, we consider constrained models of time evolution: automaton (i.e. reversible classical) circuits that, when fed suitable input states, provably produce the \"standard models\" of pseudoentangled ensembles--uniformly random subset(-phase) states--at late times, a phenomenon we name 'pseudothermalization'. We examine (i) how a pseudoentangled ensemble on a small subsystem spreads to the whole system as a function of time, and (ii) how a pseudoentangled ensemble is generated from an initial product state. We map the above problems onto a family of classical Markov chains on subsets of the computational basis. The mixing times of such Markov chains are related to the time scales at which the states produced from the dynamics become indistinguishable from Haar-random states at the level of each statistical moment (or number of copies). Based on a combination of rigorous bounds and conjectures supported by numerics, we argue that each Markov chain's relaxation time and mixing time have different asymptotic behavior in the limit of large system size. This is a necessary condition for a cutoff phenomenon: an abrupt dynamical transition to equilibrium. We thus conjecture that our random circuits give rise to asymptotically sharp pseudothermalization transitions.","sentences":["The dynamics of quantum entanglement plays a central role in explaining the emergence of thermal equilibrium in isolated many-body systems.","However, entanglement is notoriously hard to measure, and can in fact be forged: recent works have introduced a notion of pseudoentanglement describing ensembles of many-body states that, while only weakly entangled, cannot be efficiently distinguished from states with much higher entanglement, such as random states in the Hilbert space.","In this work we initiate the study of the dynamical generation and propagation of pseudoentanglement.","As generic quantum dynamics tends to maximize actual entanglement, we consider constrained models of time evolution: automaton (i.e. reversible classical) circuits that, when fed suitable input states, provably produce the \"standard models\" of pseudoentangled ensembles--uniformly random subset(-phase) states--at late times, a phenomenon we name 'pseudothermalization'.","We examine (i) how a pseudoentangled ensemble on a small subsystem spreads to the whole system as a function of time, and (ii) how a pseudoentangled ensemble is generated from an initial product state.","We map the above problems onto a family of classical Markov chains on subsets of the computational basis.","The mixing times of such Markov chains are related to the time scales at which the states produced from the dynamics become indistinguishable from Haar-random states at the level of each statistical moment (or number of copies).","Based on a combination of rigorous bounds and conjectures supported by numerics, we argue that each Markov chain's relaxation time and mixing time have different asymptotic behavior in the limit of large system size.","This is a necessary condition for a cutoff phenomenon: an abrupt dynamical transition to equilibrium.","We thus conjecture that our random circuits give rise to asymptotically sharp pseudothermalization transitions."],"url":"http://arxiv.org/abs/2403.09619v1","category":"quant-ph"}
{"created":"2024-03-14 17:54:20","title":"Generating functional of correlators of twist-$2$ operators in $\\mathcal{N} = 1$ SUSY Yang-Mills theory, I","abstract":"Extending our previous work in pure Yang-Mills (YM) theory, we compute the generating functional of correlators of collinear twist-$2$ operators that enter the components of balanced superfields -- i.e., superfields with an equal number of dotted and undotted indices in their spinor representation -- in $\\mathcal{N} = 1$ SUSY SU($N$) YM theory in Minkowskian and Euclidean space-time, in the conformal limit and renormalization-group improved form, and to the leading and next-to-leading order in the large-$N$ expansion. The latter calculation sets strong UV asymptotic constraints on the nonperturbative solution of large-$N$ $\\mathcal{N} = 1$ SUSY YM theory that may be a pivotal guide for the search of such a solution.","sentences":["Extending our previous work in pure Yang-Mills (YM) theory, we compute the generating functional of correlators of collinear twist-$2$ operators that enter the components of balanced superfields -- i.e., superfields with an equal number of dotted and undotted indices in their spinor representation -- in $\\mathcal{N} = 1$ SUSY SU($N$) YM theory in Minkowskian and Euclidean space-time, in the conformal limit and renormalization-group improved form, and to the leading and next-to-leading order in the large-$N$ expansion.","The latter calculation sets strong UV asymptotic constraints on the nonperturbative solution of large-$N$ $\\mathcal{N} = 1$ SUSY YM theory that may be a pivotal guide for the search of such a solution."],"url":"http://arxiv.org/abs/2403.09617v1","category":"hep-th"}
{"created":"2024-03-14 17:52:31","title":"Explore In-Context Segmentation via Latent Diffusion Models","abstract":"In-context segmentation has drawn more attention with the introduction of vision foundation models. Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries. In this work, we explore this problem from a new perspective, using one representative generation model, the latent diffusion model (LDM). We observe a task gap between generation and segmentation in diffusion models, but LDM is still an effective minimalist for in-context segmentation. In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies. We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and in-context instructions. Moreover, we build a new and fair in-context segmentation benchmark that includes both image and video datasets. Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual foundation models. Our study shows that LDMs can also achieve good enough results for challenging in-context segmentation tasks.","sentences":["In-context segmentation has drawn more attention with the introduction of vision foundation models.","Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries.","In this work, we explore this problem from a new perspective, using one representative generation model, the latent diffusion model (LDM).","We observe a task gap between generation and segmentation in diffusion models, but LDM is still an effective minimalist for in-context segmentation.","In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies.","We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and in-context instructions.","Moreover, we build a new and fair in-context segmentation benchmark that includes both image and video datasets.","Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual foundation models.","Our study shows that LDMs can also achieve good enough results for challenging in-context segmentation tasks."],"url":"http://arxiv.org/abs/2403.09616v1","category":"cs.CV"}
{"created":"2024-03-14 17:52:17","title":"PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation","abstract":"Generative text-to-image models, which allow users to create appealing images through a text prompt, have seen a dramatic increase in popularity in recent years. However, most users have a limited understanding of how such models work and it often requires many trials and errors to achieve satisfactory results. The prompt history contains a wealth of information that could provide users with insights into what have been explored and how the prompt changes impact the output image, yet little research attention has been paid to the visual analysis of such process to support users. We propose the Image Variant Graph, a novel visual representation designed to support comparing prompt-image pairs and exploring the editing history. The Image Variant Graph models prompt differences as edges between corresponding images and presents the distances between images through projection. Based on the graph, we developed the PrompTHis system through co-design with artists. Besides Image Variant Graph, PrompTHis also incorporates a detailed prompt-image history and a navigation mini-map. Based on the review and analysis of the prompting history, users can better understand the impact of prompt changes and have a more effective control of image generation. A quantitative user study with eleven amateur participants and qualitative interviews with five professionals and one amateur user were conducted to evaluate the effectiveness of PrompTHis. The results demonstrate PrompTHis can help users review the prompt history, make sense of the model, and plan their creative process.","sentences":["Generative text-to-image models, which allow users to create appealing images through a text prompt, have seen a dramatic increase in popularity in recent years.","However, most users have a limited understanding of how such models work and it often requires many trials and errors to achieve satisfactory results.","The prompt history contains a wealth of information that could provide users with insights into what have been explored and how the prompt changes impact the output image, yet little research attention has been paid to the visual analysis of such process to support users.","We propose the Image Variant Graph, a novel visual representation designed to support comparing prompt-image pairs and exploring the editing history.","The Image Variant Graph models prompt differences as edges between corresponding images and presents the distances between images through projection.","Based on the graph, we developed the PrompTHis system through co-design with artists.","Besides Image Variant Graph, PrompTHis also incorporates a detailed prompt-image history and a navigation mini-map.","Based on the review and analysis of the prompting history, users can better understand the impact of prompt changes and have a more effective control of image generation.","A quantitative user study with eleven amateur participants and qualitative interviews with five professionals and one amateur user were conducted to evaluate the effectiveness of PrompTHis.","The results demonstrate PrompTHis can help users review the prompt history, make sense of the model, and plan their creative process."],"url":"http://arxiv.org/abs/2403.09615v1","category":"cs.HC"}
{"created":"2024-03-14 17:50:24","title":"Generative reconstruction of 3D volume elements for Ti-6Al-4V basketweave microstructure by optimization of CNN-based microstructural descriptors","abstract":"We present a methodology for the generative reconstruction of 3D Volume Elements (VE) for numerical multiscale analysis of Ti-6Al-4V processed by Additive Manufacturing (AM). The basketweave morphology, which is typically dominant in AM-processed Ti-6Al-4V, is analyzed in conventional Electron Backscatter Diffusion (EBSD) micrographs. Prior \\b{eta}-grain reconstruction is performed to obtain the out-of-plane orientation of the observed grains leveraging Burgers orientation relationship. Convolutional Neural Network (CNN) - based microstructure descriptors are extracted from the 2D data, and used for cross-section-based optimization of pixel values on orthogonal planes in 3D, using the Microstructure Characterization and Reconstruction (MCR) implementation MCRpy [16]. In order to utilize MCRpy, which performs best for binary systems, the basketweave microstructure, which consists of up to twelve distinct grain orientations, is decomposed into several separate two-phase systems. Our reconstructions capture key characteristics of the titanium basketweave morphology and show qualitative resemblance to experimentally obtained 3D data. The preservation of volume fraction during assembly of the reconstruction remains an unadressed challenge at this stage.","sentences":["We present a methodology for the generative reconstruction of 3D Volume Elements (VE) for numerical multiscale analysis of Ti-6Al-4V processed by Additive Manufacturing (AM).","The basketweave morphology, which is typically dominant in AM-processed Ti-6Al-4V, is analyzed in conventional Electron Backscatter Diffusion (EBSD) micrographs.","Prior \\b{eta}-grain reconstruction is performed to obtain the out-of-plane orientation of the observed grains leveraging Burgers orientation relationship.","Convolutional Neural Network (CNN) - based microstructure descriptors are extracted from the 2D data, and used for cross-section-based optimization of pixel values on orthogonal planes in 3D, using the Microstructure Characterization and Reconstruction (MCR) implementation MCRpy","[16].","In order to utilize MCRpy, which performs best for binary systems, the basketweave microstructure, which consists of up to twelve distinct grain orientations, is decomposed into several separate two-phase systems.","Our reconstructions capture key characteristics of the titanium basketweave morphology and show qualitative resemblance to experimentally obtained 3D data.","The preservation of volume fraction during assembly of the reconstruction remains an unadressed challenge at this stage."],"url":"http://arxiv.org/abs/2403.09609v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 17:49:58","title":"Interaction-Driven Instabilities in the Random-Field XXZ Chain","abstract":"Despite enormous efforts devoted to the study of the many-body localization (MBL) phenomenon, the nature of the high-energy behavior of the Heisenberg spin chain in a strong random magnetic field is lacking consensus. Here, we take a step back by exploring the weak interaction limit starting from the Anderson localized (AL) insulator. Through shift-invert diagonalization, we find that below a certain disorder threshold $h^*$, weak interactions necessarily lead to ergodic instability, whereas at strong disorder the AL insulator directly turns into MBL. This agrees with a simple interpretation of the avalanche theory for restoration of ergodicity. We further map the phase diagram for the generic XXZ model in the disorder $h$ -- interaction $\\Delta$ plane. Taking advantage of the total magnetization conservation, our results unveil the remarkable behavior of the spin-spin correlation functions: in the regime indicated as MBL by standard observables, their exponential decay undergoes a unique inversion of orientation $\\xi_z>\\xi_x$. We find that the longitudinal length $\\xi_z$ is a key quantity for capturing ergodic instabilities, as it increases with system size near the thermal phase, in sharp contrast to its transverse counterpart $\\xi_x$.","sentences":["Despite enormous efforts devoted to the study of the many-body localization (MBL) phenomenon, the nature of the high-energy behavior of the Heisenberg spin chain in a strong random magnetic field is lacking consensus.","Here, we take a step back by exploring the weak interaction limit starting from the Anderson localized (AL) insulator.","Through shift-invert diagonalization, we find that below a certain disorder threshold $h^*$, weak interactions necessarily lead to ergodic instability, whereas at strong disorder the AL insulator directly turns into MBL.","This agrees with a simple interpretation of the avalanche theory for restoration of ergodicity.","We further map the phase diagram for the generic XXZ model in the disorder $h$ -- interaction $\\Delta$ plane.","Taking advantage of the total magnetization conservation, our results unveil the remarkable behavior of the spin-spin correlation functions: in the regime indicated as MBL by standard observables, their exponential decay undergoes a unique inversion of orientation $\\xi_z>\\xi_x$. We find that the longitudinal length $\\xi_z$ is a key quantity for capturing ergodic instabilities, as it increases with system size near the thermal phase, in sharp contrast to its transverse counterpart $\\xi_x$."],"url":"http://arxiv.org/abs/2403.09608v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-14 17:47:20","title":"Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey","abstract":"Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems.","sentences":["Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables.","The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities.","This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality.","Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations.","This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems."],"url":"http://arxiv.org/abs/2403.09606v1","category":"cs.CL"}
{"created":"2024-03-14 17:47:01","title":"Counterfactual contrastive learning: robust representations via causal image synthesis","abstract":"Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training.","sentences":["Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings.","However, it is sensitive to the choice of augmentation pipeline.","Positive pairs should preserve semantic information while destroying domain-specific information.","Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead?","In this work, we show how to utilise recent progress in counterfactual image generation to this effect.","We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation.","Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and out-of-distribution data, particularly for domains which are under-represented during training."],"url":"http://arxiv.org/abs/2403.09605v1","category":"cs.CV"}
{"created":"2024-03-14 17:44:35","title":"Optimistic Verifiable Training by Controlling Hardware Nondeterminism","abstract":"The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and \"optimistic\" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for nondeterminism. Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2 (117M) models. Our verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems.","sentences":["The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources.","However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges.","Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and \"optimistic\" methods that consider a trusted third-party auditor who replicates the training process.","A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust.","We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for nondeterminism.","Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2 (117M) models.","Our verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems."],"url":"http://arxiv.org/abs/2403.09603v1","category":"cs.CR"}
{"created":"2024-03-14 17:44:22","title":"Parafermions with symmetry-protected non-Abelian statistics","abstract":"Non-Abelian anyons have garnered extensive attention for obeying exotic non-Abelian statistics and potential applications to fault-tolerant quantum computation. Although the prior research has predominantly focused on non-Abelian statistics without the necessity of symmetry protection, recent progresses have shown that symmetries can play essential roles and bring about a notion of the symmetry-protected non-Abelian (SPNA) statistics. In this work, we extend the concept of SPNA statistics to strongly-correlated systems which host parafermion zero modes (PZMs). This study involves a few fundamental results proved here. First, we unveil a generic unitary symmetry mechanism that protects PZMs from local couplings. Then, with this symmetry protection, the PZMs can be categorized into two nontrivial sectors, each maintaining its own parity conservation, even though the whole system cannot be dismantled into separate subsystems due to nonlinear interactions. Finally, by leveraging the parity conservation of each sector and the general properties of the effective braiding Hamiltonian, we prove rigorously that the PZMs intrinsically obey SPNA statistics. To further confirm the results, we derive the braiding matrix at a tri-junction. In addition, we propose a physical model that accommodates a pair of PZMs protected by mirror symmetry and satisfying the generic theory. This work shows a broad spectrum of strongly-correlated systems capable of hosting fractional SPNA quasiparticles and enriches our comprehension of fundamental quantum statistics linked to the symmetries that govern the exchange dynamics.","sentences":["Non-Abelian anyons have garnered extensive attention for obeying exotic non-Abelian statistics and potential applications to fault-tolerant quantum computation.","Although the prior research has predominantly focused on non-Abelian statistics without the necessity of symmetry protection, recent progresses have shown that symmetries can play essential roles and bring about a notion of the symmetry-protected non-Abelian (SPNA) statistics.","In this work, we extend the concept of SPNA statistics to strongly-correlated systems which host parafermion zero modes (PZMs).","This study involves a few fundamental results proved here.","First, we unveil a generic unitary symmetry mechanism that protects PZMs from local couplings.","Then, with this symmetry protection, the PZMs can be categorized into two nontrivial sectors, each maintaining its own parity conservation, even though the whole system cannot be dismantled into separate subsystems due to nonlinear interactions.","Finally, by leveraging the parity conservation of each sector and the general properties of the effective braiding Hamiltonian, we prove rigorously that the PZMs intrinsically obey SPNA statistics.","To further confirm the results, we derive the braiding matrix at a tri-junction.","In addition, we propose a physical model that accommodates a pair of PZMs protected by mirror symmetry and satisfying the generic theory.","This work shows a broad spectrum of strongly-correlated systems capable of hosting fractional SPNA quasiparticles and enriches our comprehension of fundamental quantum statistics linked to the symmetries that govern the exchange dynamics."],"url":"http://arxiv.org/abs/2403.09602v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 17:42:10","title":"Network-Controlled Repeater -- An Introduction","abstract":"In fifth generation (5G) wireless cellular networks, millimeter wave spectrum opens room for several potential improvements in throughput, reliability, latency, among other aspects. However, it also brings challenges, such as a higher influence of blockage which may significantly limit the coverage. In this context, network-controlled repeaters (NCRs) are network nodes with low complexity that represent a technique to overcome coverage problems. In this paper, we introduce the NCR concept and study its performance gains and deployment options. Particularly, presenting the main specifications of NCR as agreed in 3rd generation partnership project (3GPP) Rel-18, we analyze different NCR deployments in an urban scenario and compare its performance with alternative deployments. As demonstrated, with a proper network planning and beamforming design, NCR is an attractive solution to cover blind spots the base stations (BSs) may have.","sentences":["In fifth generation (5G) wireless cellular networks, millimeter wave spectrum opens room for several potential improvements in throughput, reliability, latency, among other aspects.","However, it also brings challenges, such as a higher influence of blockage which may significantly limit the coverage.","In this context, network-controlled repeaters (NCRs) are network nodes with low complexity that represent a technique to overcome coverage problems.","In this paper, we introduce the NCR concept and study its performance gains and deployment options.","Particularly, presenting the main specifications of NCR as agreed in 3rd generation partnership project (3GPP) Rel-18, we analyze different NCR deployments in an urban scenario and compare its performance with alternative deployments.","As demonstrated, with a proper network planning and beamforming design, NCR is an attractive solution to cover blind spots the base stations (BSs) may have."],"url":"http://arxiv.org/abs/2403.09601v1","category":"cs.NI"}
{"created":"2024-03-14 17:40:20","title":"Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis","abstract":"Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests. This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.","sentences":["Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex.","Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning.","We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests.","This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations.","We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text."],"url":"http://arxiv.org/abs/2403.09599v1","category":"cs.IR"}
{"created":"2024-03-14 17:37:03","title":"Tidal evolution of cored and cuspy dark matter halos","abstract":"The internal structure and abundance of dark matter halos and subhalos are powerful probes of the nature of dark matter. In order to compare observations with dark matter models, accurate theoretical predictions of these quantities are needed. We present a fast and accurate method to describe the tidal evolution of subhalos within their parent halo, based on a semi-analytic approach. We first consider idealized N-body simulations of subhalos within their host halo, using a generalized mass density profile that describes their properties in a variety of dark matter models at infall, including popular warm, cold, and self-interacting ones. Using these simulations we construct tidal \"tracks\" for the evolution of subhalos based on their conditions at infall. Second, we use the results of these simulations to build semi-analytic models (SAMs) for tidal effects, including stripping and heating and implement them within the code GALACTICUS. Our SAMs can accurately predict the tidal evolution of both cored and cuspy subhalos, including the bound mass and density profiles, providing a powerful and efficient tool for studying the post-infall properties of subhalos in different dark matter models.","sentences":["The internal structure and abundance of dark matter halos and subhalos are powerful probes of the nature of dark matter.","In order to compare observations with dark matter models, accurate theoretical predictions of these quantities are needed.","We present a fast and accurate method to describe the tidal evolution of subhalos within their parent halo, based on a semi-analytic approach.","We first consider idealized N-body simulations of subhalos within their host halo, using a generalized mass density profile that describes their properties in a variety of dark matter models at infall, including popular warm, cold, and self-interacting ones.","Using these simulations we construct tidal \"tracks\" for the evolution of subhalos based on their conditions at infall.","Second, we use the results of these simulations to build semi-analytic models (SAMs) for tidal effects, including stripping and heating and implement them within the code GALACTICUS.","Our SAMs can accurately predict the tidal evolution of both cored and cuspy subhalos, including the bound mass and density profiles, providing a powerful and efficient tool for studying the post-infall properties of subhalos in different dark matter models."],"url":"http://arxiv.org/abs/2403.09597v1","category":"astro-ph.GA"}
{"created":"2024-03-14 17:35:32","title":"Renovating Names in Open-Vocabulary Segmentation Benchmarks","abstract":"Names are essential to both human cognition and vision-language models. Open-vocabulary models utilize class names as text prompts to generalize to categories unseen during training. However, name qualities are often overlooked and lack sufficient precision in existing datasets. In this paper, we address this underexplored problem by presenting a framework for \"renovating\" names in open-vocabulary segmentation benchmarks (RENOVATE). Through human study, we demonstrate that the names generated by our model are more precise descriptions of the visual segments and hence enhance the quality of existing datasets by means of simple renaming. We further demonstrate that using our renovated names enables training of stronger open-vocabulary segmentation models. Using open-vocabulary segmentation for name quality evaluation, we show that our renovated names lead to up to 16% relative improvement from the original names on various benchmarks across various state-of-the-art models. We provide our code and relabelings for several popular segmentation datasets (ADE20K, Cityscapes, PASCAL Context) to the research community.","sentences":["Names are essential to both human cognition and vision-language models.","Open-vocabulary models utilize class names as text prompts to generalize to categories unseen during training.","However, name qualities are often overlooked and lack sufficient precision in existing datasets.","In this paper, we address this underexplored problem by presenting a framework for \"renovating\" names in open-vocabulary segmentation benchmarks (RENOVATE).","Through human study, we demonstrate that the names generated by our model are more precise descriptions of the visual segments and hence enhance the quality of existing datasets by means of simple renaming.","We further demonstrate that using our renovated names enables training of stronger open-vocabulary segmentation models.","Using open-vocabulary segmentation for name quality evaluation, we show that our renovated names lead to up to 16% relative improvement from the original names on various benchmarks across various state-of-the-art models.","We provide our code and relabelings for several popular segmentation datasets (ADE20K, Cityscapes, PASCAL Context) to the research community."],"url":"http://arxiv.org/abs/2403.09593v1","category":"cs.CV"}
{"created":"2024-03-14 17:35:19","title":"DungeonMaker: Embedding Tangible Creation and Destruction in Hybrid Board Games through Personal Fabrication Technology","abstract":"Hybrid board games (HBGs) augment their analog origins digitally (e.g., through apps) and are an increasingly popular pastime activity. Continuous world and character development and customization, known to facilitate engagement in video games, remain rare in HBGs. If present, they happen digitally or imaginarily, often leaving physical aspects generic. We developed DungeonMaker, a fabrication-augmented HBG bridging physical and digital game elements: 1) the setup narrates a story and projects a digital game board onto a laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3) DungeonMaker's modified laser head senses and moves player- and non-player figures, and 4) can physically damage figures. An evaluation (n=4x3) indicated that DungeonMaker provides an engaging experience, may support players' connection to their figures, and potentially spark novices' interest in fabrication. DungeonMaker provides a rich constellation to play HBGs by blending aspects of craft and automation to couple the physical and digital elements of an HBG tightly.","sentences":["Hybrid board games (HBGs) augment their analog origins digitally (e.g., through apps) and are an increasingly popular pastime activity.","Continuous world and character development and customization, known to facilitate engagement in video games, remain rare in HBGs.","If present, they happen digitally or imaginarily, often leaving physical aspects generic.","We developed DungeonMaker, a fabrication-augmented HBG bridging physical and digital game elements: 1) the setup narrates a story and projects a digital game board onto a laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3) DungeonMaker's modified laser head senses and moves player- and non-player figures, and 4) can physically damage figures.","An evaluation (n=4x3) indicated that DungeonMaker provides an engaging experience, may support players' connection to their figures, and potentially spark novices' interest in fabrication.","DungeonMaker provides a rich constellation to play HBGs by blending aspects of craft and automation to couple the physical and digital elements of an HBG tightly."],"url":"http://arxiv.org/abs/2403.09592v1","category":"cs.HC"}
{"created":"2024-03-14 17:19:01","title":"Functions Analytic at Infinity and Normality","abstract":"Given a charge and current distribution with compact support, the associated potentials and fields are generally not integrable in the classical sense. However, it is convenient to be able to define their Fourier transform in order to create solutions to the wave equation. This paper develops the technology for this by considering the class of quasi split normal functions, examples of which are the solutions to Poisson's equation with a forcing term having compact support.","sentences":["Given a charge and current distribution with compact support, the associated potentials and fields are generally not integrable in the classical sense.","However, it is convenient to be able to define their Fourier transform in order to create solutions to the wave equation.","This paper develops the technology for this by considering the class of quasi split normal functions, examples of which are the solutions to Poisson's equation with a forcing term having compact support."],"url":"http://arxiv.org/abs/2403.09584v1","category":"math-ph"}
{"created":"2024-03-14 17:15:30","title":"Universal Definitions of the Roman Factorial: Introduction to Foundational Functions and the Generalization Process","abstract":"This paper introduces a new method for redefining the Roman factorial using universally applicable functions that are not expressed in closed form. We present a set of foundational functions, similar to Boolean operations, to simplify the factorial expression. Through a systematic process of generalization, termed generalization process, we aim to use these foundational functions to create recursive and non-recursive, global definitions of the Roman factorial.","sentences":["This paper introduces a new method for redefining the Roman factorial using universally applicable functions that are not expressed in closed form.","We present a set of foundational functions, similar to Boolean operations, to simplify the factorial expression.","Through a systematic process of generalization, termed generalization process, we aim to use these foundational functions to create recursive and non-recursive, global definitions of the Roman factorial."],"url":"http://arxiv.org/abs/2403.09581v1","category":"math.CO"}
{"created":"2024-03-14 17:14:53","title":"Algorithmic syntactic causal identification","abstract":"Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. This allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general ID algorithm through fixing. Our description is given entirely in terms of the non-parametric ADMG structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. We use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.","sentences":["Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle.","However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs.","However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms.","We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories.","In this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model.","This allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general ID algorithm through fixing.","Our description is given entirely in terms of the non-parametric ADMG structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained.","We use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model."],"url":"http://arxiv.org/abs/2403.09580v1","category":"cs.AI"}
{"created":"2024-03-14 17:11:48","title":"On the high-temperature expansion of the thermal energy on Einstein cylinders","abstract":"Some exact high temperature expansions are derived using a temperature inversion symmetry of the internal energy for conformal scalars and spinors on the Einstein Universe.","sentences":["Some exact high temperature expansions are derived using a temperature inversion symmetry of the internal energy for conformal scalars and spinors on the Einstein Universe."],"url":"http://arxiv.org/abs/2403.09576v1","category":"gr-qc"}
{"created":"2024-03-14 17:06:50","title":"Scalable Parity Architecture With a Shuttling-Based Spin Qubit Processor","abstract":"Motivated by the prospect of a two-dimensional square-lattice geometry for semiconductor spin qubits, we explore the realization of the Parity Architecture with quantum dots (QDs). This is part of the endeavor of developing architectures that advance the utilization of spin qubits for quantum computing while harnessing their advantages, such as their fast timescales -- especially of the nearest-neighbor interaction -- and small size. We present sequences of spin shuttling and quantum gates that implement the Parity Quantum Approximate Optimization Algorithm (QAOA) on a lattice constructed of identical unit cells, where the circuit depth is independent of the problem Hamiltonian and the system size. We further develop an error model, including a general description of the shuttling errors as a function of the probability distribution function of the valley splitting, and estimate the errors during one round of Parity QAOA, which is mainly limited by the valley splitting. Finally, we discuss the possibility of decoding the logical quantum state and of quantum error mitigation. We find that already with near-term spin qubit devices a sufficiently low physical error probability can be expected to reliably perform Parity QAOA with a short depth in a regime where the success probability compares favorably to standard QAOA.","sentences":["Motivated by the prospect of a two-dimensional square-lattice geometry for semiconductor spin qubits, we explore the realization of the Parity Architecture with quantum dots (QDs).","This is part of the endeavor of developing architectures that advance the utilization of spin qubits for quantum computing while harnessing their advantages, such as their fast timescales -- especially of the nearest-neighbor interaction -- and small size.","We present sequences of spin shuttling and quantum gates that implement the Parity Quantum Approximate Optimization Algorithm (QAOA) on a lattice constructed of identical unit cells, where the circuit depth is independent of the problem Hamiltonian and the system size.","We further develop an error model, including a general description of the shuttling errors as a function of the probability distribution function of the valley splitting, and estimate the errors during one round of Parity QAOA, which is mainly limited by the valley splitting.","Finally, we discuss the possibility of decoding the logical quantum state and of quantum error mitigation.","We find that already with near-term spin qubit devices a sufficiently low physical error probability can be expected to reliably perform Parity QAOA with a short depth in a regime where the success probability compares favorably to standard QAOA."],"url":"http://arxiv.org/abs/2403.09574v1","category":"quant-ph"}
{"created":"2024-03-14 17:03:04","title":"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation","abstract":"Multimodal large language models (MLLMs) have shown impressive reasoning abilities, which, however, are also more vulnerable to jailbreak attacks than their LLM predecessors. Although still capable of detecting unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features. To construct robust MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO enhances model safety significantly (e.g., a 37.6% improvement on the MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks. Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention.","sentences":["Multimodal large language models (MLLMs) have shown impressive reasoning abilities, which, however, are also more vulnerable to jailbreak attacks than their LLM predecessors.","Although still capable of detecting unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features.","To construct robust MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.","Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO enhances model safety significantly (e.g., a 37.6% improvement on the MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks.","Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention."],"url":"http://arxiv.org/abs/2403.09572v1","category":"cs.CV"}
{"created":"2024-03-14 17:00:00","title":"Non-Hermitian Persistent Current Transport","abstract":"Persistent currents circulate continuously without requiring external power sources. Here, we extend their theory to include dissipation within the framework of non-Hermitian quantum Hamiltonians. Using Green's function formalism, we introduce a non-Hermitian Fermi-Dirac distribution and derive an analytical expression for the persistent current that relies solely on the complex spectrum. We apply our formula to two dissipative models supporting persistent currents: ($i$) a phase-biased superconducting-normal-superconducting junction; ($ii$) a normal ring threaded by a magnetic flux. We show that the persistent currents in both systems exhibit no anomalies at any emergent exceptional points, whose signatures are only discernible in the current susceptibility. We validate our findings by exact diagonalization and extend them to account for finite temperatures and interaction effects. Our formalism offers a general framework for computing quantum many-body observables of non-Hermitian systems in equilibrium, with potential extensions to non-equilibrium scenarios.","sentences":["Persistent currents circulate continuously without requiring external power sources.","Here, we extend their theory to include dissipation within the framework of non-Hermitian quantum Hamiltonians.","Using Green's function formalism, we introduce a non-Hermitian Fermi-Dirac distribution and derive an analytical expression for the persistent current that relies solely on the complex spectrum.","We apply our formula to two dissipative models supporting persistent currents: ($i$) a phase-biased superconducting-normal-superconducting junction; ($ii$) a normal ring threaded by a magnetic flux.","We show that the persistent currents in both systems exhibit no anomalies at any emergent exceptional points, whose signatures are only discernible in the current susceptibility.","We validate our findings by exact diagonalization and extend them to account for finite temperatures and interaction effects.","Our formalism offers a general framework for computing quantum many-body observables of non-Hermitian systems in equilibrium, with potential extensions to non-equilibrium scenarios."],"url":"http://arxiv.org/abs/2403.09569v1","category":"quant-ph"}
{"created":"2024-03-14 16:57:18","title":"Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models","abstract":"The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box. The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities. This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios.","sentences":["The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns.","Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users.","Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings.","Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions.","This work presents an accountability and explainability architecture implemented for ROS-based mobile robots.","The proposed solution consists of two main components.","Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology.","Secondly, a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box.","The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities.","This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios."],"url":"http://arxiv.org/abs/2403.09567v1","category":"cs.RO"}
{"created":"2024-03-14 16:56:52","title":"Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models","abstract":"DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is \"Hazard Analysis & Risk Assessment\" (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs).   Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with LLMs. Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.","sentences":["DevOps is a necessity in many industries, including the development of Autonomous Vehicles.","In those settings, there are iterative activities that reduce the speed of SafetyOps cycles.","One of these activities is \"Hazard Analysis & Risk Assessment\" (HARA), which is an essential step to start the safety requirements specification.","As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs).   ","Our objective is to systematically assess their potential for application in the field of safety engineering.","To that end, we propose a framework to support a higher degree of automation of HARA with LLMs.","Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly."],"url":"http://arxiv.org/abs/2403.09565v1","category":"cs.SE"}
{"created":"2024-03-14 16:54:17","title":"PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps","abstract":"The pre-training and fine-tuning paradigm has demonstrated its effectiveness and has become the standard approach for tailoring language models to various tasks. Currently, community-based platforms offer easy access to various pre-trained models, as anyone can publish without strict validation processes. However, a released pre-trained model can be a privacy trap for fine-tuning datasets if it is carefully designed. In this work, we propose PreCurious framework to reveal the new attack surface where the attacker releases the pre-trained model and gets a black-box access to the final fine-tuned model. PreCurious aims to escalate the general privacy risk of both membership inference and data extraction. The key intuition behind PreCurious is to manipulate the memorization stage of the pre-trained model and guide fine-tuning with a seemingly legitimate configuration. The effectiveness of defending against privacy attacks on a fine-tuned model seems promising, as empirical and theoretical evidence suggests that parameter-efficient and differentially private fine-tuning techniques are invulnerable to privacy attacks. But PreCurious demonstrates the possibility of breaking up invulnerability in a stealthy manner compared to fine-tuning on a benign model. By further leveraging a sanitized dataset, PreCurious can extract originally unexposed secrets under differentially private fine-tuning. Thus, PreCurious raises warnings for users who download pre-trained models from unknown sources, rely solely on tutorials or common-sense defenses, and previously release sanitized datasets even after perfect scrubbing.","sentences":["The pre-training and fine-tuning paradigm has demonstrated its effectiveness and has become the standard approach for tailoring language models to various tasks.","Currently, community-based platforms offer easy access to various pre-trained models, as anyone can publish without strict validation processes.","However, a released pre-trained model can be a privacy trap for fine-tuning datasets if it is carefully designed.","In this work, we propose PreCurious framework to reveal the new attack surface where the attacker releases the pre-trained model and gets a black-box access to the final fine-tuned model.","PreCurious aims to escalate the general privacy risk of both membership inference and data extraction.","The key intuition behind PreCurious is to manipulate the memorization stage of the pre-trained model and guide fine-tuning with a seemingly legitimate configuration.","The effectiveness of defending against privacy attacks on a fine-tuned model seems promising, as empirical and theoretical evidence suggests that parameter-efficient and differentially private fine-tuning techniques are invulnerable to privacy attacks.","But PreCurious demonstrates the possibility of breaking up invulnerability in a stealthy manner compared to fine-tuning on a benign model.","By further leveraging a sanitized dataset, PreCurious can extract originally unexposed secrets under differentially private fine-tuning.","Thus, PreCurious raises warnings for users who download pre-trained models from unknown sources, rely solely on tutorials or common-sense defenses, and previously release sanitized datasets even after perfect scrubbing."],"url":"http://arxiv.org/abs/2403.09562v1","category":"cs.CR"}
{"created":"2024-03-14 16:52:57","title":"Self-Consistency Training for Hamiltonian Prediction","abstract":"Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios, and the better efficiency from the amortization. These benefits push forward the applicability of Hamiltonian prediction to an ever larger scale.","sentences":["Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems.","Yet, its applicability is limited by insufficient labeled data for training.","In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data.","This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures.","We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios, and the better efficiency from the amortization.","These benefits push forward the applicability of Hamiltonian prediction to an ever larger scale."],"url":"http://arxiv.org/abs/2403.09560v1","category":"cs.LG"}
{"created":"2024-03-14 16:42:47","title":"Testing MOND on small bodies in the remote solar system","abstract":"Modified Newtonian dynamics (MOND), which postulates a breakdown of Newton's laws of gravity/dynamics below some critical acceleration threshold, can explain many otherwise puzzling observational phenomena on galactic scales. MOND competes with the hypothesis of dark matter, which successfully explains the cosmic microwave background and large-scale structure. Here we provide the first solar-system test of MOND that probes the sub-critical acceleration regime. Using the Bekenstein-Milgrom AQUAL formulation, we simulate the evolution of myriads of test particles (planetesimals or comets) born in the trans-Neptunian region and scattered by the giant planets over the lifetime of the Sun to heliocentric distances of $10^2$-$10^5$ au. We include the effects of the Galactic tidal field and passing stars. While Newtonian simulations reproduce the distribution of binding energies of long-period and Oort-cloud comets detectable from Earth, MOND-based simulations do not. This conclusion is robust to plausible changes in the migration history of the planets, the migration history of the Sun, the MOND transition function, effects of the Sun's birth cluster, and the fading properties of long-period comets. For the most popular version of AQUAL, characterized by a gradual transition between the Newtonian and MOND regimes, our MOND-based simulations also fail to reproduce the orbital distribution of trans-Neptunian objects in the detached disk (perihelion > 38 au). Our results do not rule out some MOND theories more elaborate than AQUAL, in which non-Newtonian effects are screened on small spatial scales, at small masses, or in external gravitational fields comparable in strength to the critical acceleration.","sentences":["Modified Newtonian dynamics (MOND), which postulates a breakdown of Newton's laws of gravity/dynamics below some critical acceleration threshold, can explain many otherwise puzzling observational phenomena on galactic scales.","MOND competes with the hypothesis of dark matter, which successfully explains the cosmic microwave background and large-scale structure.","Here we provide the first solar-system test of MOND that probes the sub-critical acceleration regime.","Using the Bekenstein-Milgrom AQUAL formulation, we simulate the evolution of myriads of test particles (planetesimals or comets) born in the trans-Neptunian region and scattered by the giant planets over the lifetime of the Sun to heliocentric distances of $10^2$-$10^5$ au.","We include the effects of the Galactic tidal field and passing stars.","While Newtonian simulations reproduce the distribution of binding energies of long-period and Oort-cloud comets detectable from Earth, MOND-based simulations do not.","This conclusion is robust to plausible changes in the migration history of the planets, the migration history of the Sun, the MOND transition function, effects of the Sun's birth cluster, and the fading properties of long-period comets.","For the most popular version of AQUAL, characterized by a gradual transition between the Newtonian and MOND regimes, our MOND-based simulations also fail to reproduce the orbital distribution of trans-Neptunian objects in the detached disk (perihelion > 38 au).","Our results do not rule out some MOND theories more elaborate than AQUAL, in which non-Newtonian effects are screened on small spatial scales, at small masses, or in external gravitational fields comparable in strength to the critical acceleration."],"url":"http://arxiv.org/abs/2403.09555v1","category":"astro-ph.CO"}
{"created":"2024-03-14 16:41:26","title":"Cloud gap-filling with deep learning for improved grassland monitoring","abstract":"Uninterrupted optical image time series are crucial for the timely monitoring of agricultural land changes. However, the continuity of such time series is often disrupted by clouds. In response to this challenge, we propose a deep learning method that integrates cloud-free optical (Sentinel-2) observations and weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using a combined Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN) architecture to generate continuous Normalized Difference Vegetation Index (NDVI) time series. We emphasize the significance of observation continuity by assessing the impact of the generated time series on the detection of grassland mowing events. We focus on Lithuania, a country characterized by extensive cloud coverage, and compare our approach with alternative interpolation techniques (i.e., linear, Akima, quadratic). Our method surpasses these techniques, with an average MAE of 0.024 and R^2 of 0.92. It not only improves the accuracy of event detection tasks by employing a continuous time series, but also effectively filters out sudden shifts and noise originating from cloudy observations that cloud masks often fail to detect.","sentences":["Uninterrupted optical image time series are crucial for the timely monitoring of agricultural land changes.","However, the continuity of such time series is often disrupted by clouds.","In response to this challenge, we propose a deep learning method that integrates cloud-free optical (Sentinel-2) observations and weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using a combined Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN) architecture to generate continuous Normalized Difference Vegetation Index (NDVI) time series.","We emphasize the significance of observation continuity by assessing the impact of the generated time series on the detection of grassland mowing events.","We focus on Lithuania, a country characterized by extensive cloud coverage, and compare our approach with alternative interpolation techniques (i.e., linear, Akima, quadratic).","Our method surpasses these techniques, with an average MAE of 0.024 and R^2 of 0.92.","It not only improves the accuracy of event detection tasks by employing a continuous time series, but also effectively filters out sudden shifts and noise originating from cloudy observations that cloud masks often fail to detect."],"url":"http://arxiv.org/abs/2403.09554v1","category":"cs.CV"}
{"created":"2024-03-14 16:38:02","title":"Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields","abstract":"Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic positions compared to an equilibrium structure. This makes denoising non-equilibrium structures an ill-posed problem since the target of denoising is not uniquely defined. Our key insight is to additionally encode the forces of the original non-equilibrium structure to specify which non-equilibrium structure we are denoising. Concretely, given a corrupted non-equilibrium structure and the forces of the original one, we predict the non-equilibrium structure satisfying the input forces instead of any arbitrary structures. Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in node embeddings. We study the effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-art results on OC20 and OC22 and significantly improve training efficiency on MD17.","sentences":["Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design.","However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks.","In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance.","For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise.","Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures.","The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic positions compared to an equilibrium structure.","This makes denoising non-equilibrium structures an ill-posed problem since the target of denoising is not uniquely defined.","Our key insight is to additionally encode the forces of the original non-equilibrium structure to specify which non-equilibrium structure we are denoising.","Concretely, given a corrupted non-equilibrium structure and the forces of the original one, we predict the non-equilibrium structure satisfying the input forces instead of any arbitrary structures.","Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in node embeddings.","We study the effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-art results on OC20 and OC22 and significantly improve training efficiency on MD17."],"url":"http://arxiv.org/abs/2403.09549v1","category":"cs.LG"}
{"created":"2024-03-14 16:28:33","title":"RANDAO-based RNG: Last Revealer Attacks in Ethereum 2.0 Randomness and a Potential Solution","abstract":"Ethereum 2.0 is a major upgrade to improve its scalability, throughput, and security. In this version, RANDAO is the scheme to randomly select the users who propose, confirm blocks, and get rewards. However, a vulnerability, referred to as the `Last Revealer Attack' (LRA), compromises the randomness of this scheme by introducing bias to the Random Number Generator (RNG) process. This vulnerability is first clarified again in this study. After that, we propose a Shamir's Secret Sharing (SSS)-based RANDAO scheme to mitigate the LRA. Through our analysis, the proposed method can prevent the LRA under favorable network conditions.","sentences":["Ethereum 2.0 is a major upgrade to improve its scalability, throughput, and security.","In this version, RANDAO is the scheme to randomly select the users who propose, confirm blocks, and get rewards.","However, a vulnerability, referred to as the `Last Revealer Attack' (LRA), compromises the randomness of this scheme by introducing bias to the Random Number Generator (RNG) process.","This vulnerability is first clarified again in this study.","After that, we propose a Shamir's Secret Sharing (SSS)-based RANDAO scheme to mitigate the LRA.","Through our analysis, the proposed method can prevent the LRA under favorable network conditions."],"url":"http://arxiv.org/abs/2403.09541v1","category":"cs.CR"}
{"created":"2024-03-14 16:28:28","title":"A general and sharp regularity condition for integro-differential equations with non-dominated measures","abstract":"The aim of this work is to present the regularity condition (also known in the literature as structure condition) an integro-differential operator may satisfy in order for the domination principle to hold for (sub-,super-) solutions of polynomial growth. More precisely, the framework presented in Hollender [13], in which power functions are used in order to determine the integrability conditions, is weakened by substituting the power functions with Young functions. The use of Young functions allows for sharp integrability conditions, which are crucial when one deals with limit theorems. As an immediate application, it is considered the case of parabolic Hamilton-Jacobi-Bellman (HJB) operators, for which the regularity condition is satisfied and, consequently, the comparison principle as well. The parabolic HJB operator presented in this work can be associated to second-order (decoupled) forward-backward stochastic differential equations with jumps.","sentences":["The aim of this work is to present the regularity condition (also known in the literature as structure condition) an integro-differential operator may satisfy in order for the domination principle to hold for (sub-,super-) solutions of polynomial growth.","More precisely, the framework presented in Hollender","[13], in which power functions are used in order to determine the integrability conditions, is weakened by substituting the power functions with Young functions.","The use of Young functions allows for sharp integrability conditions, which are crucial when one deals with limit theorems.","As an immediate application, it is considered the case of parabolic Hamilton-Jacobi-Bellman (HJB) operators, for which the regularity condition is satisfied and, consequently, the comparison principle as well.","The parabolic HJB operator presented in this work can be associated to second-order (decoupled) forward-backward stochastic differential equations with jumps."],"url":"http://arxiv.org/abs/2403.09540v1","category":"math.AP"}
{"created":"2024-03-14 16:27:49","title":"Logits of API-Protected LLMs Leak Proprietary Information","abstract":"The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.","sentences":["The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models.","In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo).","Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space.","We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters.","Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.","Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability."],"url":"http://arxiv.org/abs/2403.09539v1","category":"cs.CL"}
{"created":"2024-03-14 16:26:40","title":"Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub","abstract":"Background: Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform. A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster. Objective: The goal of this study is to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measure to what extent LLMs could be used for removing misconfiguration. We also want to investigate whether there are false positives in both the LLM refactorings and the tool outputs. Method: We propose a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools, such as Checkov and KICS. First, such a pipeline will run several chart analyzers and identify the common and unique misconfigurations reported by each tool. Secondly, it will use LLMs to suggest mitigation for each misconfiguration. Finally, the chart refactoring previously generated will be analyzed again by the same tools to see whether it satisfies the tool's policies. At the same time, we will also perform a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the LLM refactoring.","sentences":["Background:","Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform.","A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster.","Objective: The goal of this study is to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measure to what extent LLMs could be used for removing misconfiguration.","We also want to investigate whether there are false positives in both the LLM refactorings and the tool outputs.","Method: We propose a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools, such as Checkov and KICS.","First, such a pipeline will run several chart analyzers and identify the common and unique misconfigurations reported by each tool.","Secondly, it will use LLMs to suggest mitigation for each misconfiguration.","Finally, the chart refactoring previously generated will be analyzed again by the same tools to see whether it satisfies the tool's policies.","At the same time, we will also perform a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the LLM refactoring."],"url":"http://arxiv.org/abs/2403.09537v1","category":"cs.SE"}
{"created":"2024-03-14 16:23:48","title":"Mixed Algorithm of SINDy and HAVOK for Measure-Based Analysis of Power System with Inverter-based Resources","abstract":"Artificial intelligence and machine learning is enhancing electric grids by offering data analysis tools that can be used to operate the power grid more reliably. However, the complex nonlinear dynamics, particularly when coupled with multi-scale interactions among Inverter-based renewable energy Resources, calls for effective algorithms for power system application. This paper presents affective novel algorithm to detect various nonlinear dynamics, which is built upon: the Sparse Identification of Nonlinear Dynamics method for nonlinear dynamics detection; and Hankel Alternative View of Koopman method for multi-scale decomposition. We show that, by an appropriate integration of the strengths of the two, the mixed algorithm not only can detect the nonlinearity, but also it distinguishes the nonlinearity caused by coupled Inverter-based resources from the more familiar ones caused synchronous generators. This shows that the proposal algorithm can be a promising application of artificial intelligence and machine learning for data measure-based analysis to support operation of power system with integrated renewables.","sentences":["Artificial intelligence and machine learning is enhancing electric grids by offering data analysis tools that can be used to operate the power grid more reliably.","However, the complex nonlinear dynamics, particularly when coupled with multi-scale interactions among Inverter-based renewable energy Resources, calls for effective algorithms for power system application.","This paper presents affective novel algorithm to detect various nonlinear dynamics, which is built upon: the Sparse Identification of Nonlinear Dynamics method for nonlinear dynamics detection; and Hankel Alternative View of Koopman method for multi-scale decomposition.","We show that, by an appropriate integration of the strengths of the two, the mixed algorithm not only can detect the nonlinearity, but also it distinguishes the nonlinearity caused by coupled Inverter-based resources from the more familiar ones caused synchronous generators.","This shows that the proposal algorithm can be a promising application of artificial intelligence and machine learning for data measure-based analysis to support operation of power system with integrated renewables."],"url":"http://arxiv.org/abs/2403.09536v1","category":"eess.SY"}
{"created":"2024-03-14 16:22:08","title":"Study of measure-valued Markov processes. Explicit bounds for the convergence in distribution of mean-field models","abstract":"The aim of the paper is to prove that the rates of convergence in distribution for $N$-particle mean-field models are the expected one: $N^{-1}$ in Law of Large Numbers regime, and $N^{-1/2}$ in Central Limit Theorem regime. These proofs require to study empirical measures of McKean-Vlasov particle systems, and conditional laws of McKean-Vlasov processes, as measure-valued Markov processes. In particular, the expressions of the infinitesimal generators of such processes are established for measure-valued processes with general jumps. The generators being differential operators, all the proofs rely on the analytical properties of measure-variable functions, and the differentiation of such functions.","sentences":["The aim of the paper is to prove that the rates of convergence in distribution for $N$-particle mean-field models are the expected one: $N^{-1}$ in Law of Large Numbers regime, and $N^{-1/2}$ in Central Limit Theorem regime.","These proofs require to study empirical measures of McKean-Vlasov particle systems, and conditional laws of McKean-Vlasov processes, as measure-valued Markov processes.","In particular, the expressions of the infinitesimal generators of such processes are established for measure-valued processes with general jumps.","The generators being differential operators, all the proofs rely on the analytical properties of measure-variable functions, and the differentiation of such functions."],"url":"http://arxiv.org/abs/2403.09534v1","category":"math.PR"}
{"created":"2024-03-14 16:13:00","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding","abstract":"The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development of vision-oriented AI. VisionGPT-3D provides a versatile multimodal framework building upon the strengths of multimodal foundation models. It seamlessly integrates various SOTA vision models and brings the automation in the selection of SOTA vision models, identifies the suitable 3D mesh creation algorithms corresponding to 2D depth maps analysis, generates optimal results based on diverse multimodal inputs such as text prompts.   Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent","sentences":["The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images.","Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects.","Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts.","OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations.","However, the mismatching between the algorithms with the problem could lead to undesired results.","In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development of vision-oriented AI.","VisionGPT-3D provides a versatile multimodal framework building upon the strengths of multimodal foundation models.","It seamlessly integrates various SOTA vision models and brings the automation in the selection of SOTA vision models, identifies the suitable 3D mesh creation algorithms corresponding to 2D depth maps analysis, generates optimal results based on diverse multimodal inputs such as text prompts.   ","Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent"],"url":"http://arxiv.org/abs/2403.09530v1","category":"cs.CV"}
{"created":"2024-03-14 16:11:14","title":"A general-purpose neural network potential for Ti-Al-Nb alloys towards large-scale molecular dynamics with ab initio accuracy","abstract":"High Nb-containing TiAl alloys exhibit exceptional high-temperature strength and room-temperature ductility, making them widely used in hot-section components of automotive and aerospace engines. However, the lack of accurate interatomic interaction potentials for large-scale modeling severely hampers a comprehensive understanding of the failure mechanism of Ti-Al-Nb alloys and the development of strategies to enhance the mechanical properties. Here, we develop a general-purpose machine-learned potential (MLP) for the Ti-Al-Nb ternary system by combining the neural evolution potentials framework with an active learning scheme. The developed MLP, trained on extensive first-principles datasets, demonstrates remarkable accuracy in predicting various lattice and defect properties, as well as high-temperature characteristics such as thermal expansion and melting point for TiAl systems. Notably, this potential can effectively describe the key effect of Nb doping on stacking fault energies and formation energies. Of practical importance is that our MLP enables large-scale molecular dynamics simulations involving tens of millions of atoms with ab initio accuracy, achieving an outstanding balance between computational speed and accuracy. These results pave the way for studying micro-mechanical behaviors in TiAl lamellar structures and developing high-performance TiAl alloys towards applications at elevated temperatures.","sentences":["High Nb-containing TiAl alloys exhibit exceptional high-temperature strength and room-temperature ductility, making them widely used in hot-section components of automotive and aerospace engines.","However, the lack of accurate interatomic interaction potentials for large-scale modeling severely hampers a comprehensive understanding of the failure mechanism of Ti-Al-Nb alloys and the development of strategies to enhance the mechanical properties.","Here, we develop a general-purpose machine-learned potential (MLP) for the Ti-Al-Nb ternary system by combining the neural evolution potentials framework with an active learning scheme.","The developed MLP, trained on extensive first-principles datasets, demonstrates remarkable accuracy in predicting various lattice and defect properties, as well as high-temperature characteristics such as thermal expansion and melting point for TiAl systems.","Notably, this potential can effectively describe the key effect of Nb doping on stacking fault energies and formation energies.","Of practical importance is that our MLP enables large-scale molecular dynamics simulations involving tens of millions of atoms with ab initio accuracy, achieving an outstanding balance between computational speed and accuracy.","These results pave the way for studying micro-mechanical behaviors in TiAl lamellar structures and developing high-performance TiAl alloys towards applications at elevated temperatures."],"url":"http://arxiv.org/abs/2403.09529v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 16:10:34","title":"WavCraft: Audio Editing and Generation with Natural Language Prompts","abstract":"We introduce WavCraft, a collective system that leverages large language models (LLMs) to connect diverse task-specific models for audio content creation and editing. Specifically, WavCraft describes the content of raw sound materials in natural language and prompts the LLM conditioned on audio descriptions and users' requests. WavCraft leverages the in-context learning ability of the LLM to decomposes users' instructions into several tasks and tackle each task collaboratively with audio expert modules. Through task decomposition along with a set of task-specific models, WavCraft follows the input instruction to create or edit audio content with more details and rationales, facilitating users' control. In addition, WavCraft is able to cooperate with users via dialogue interaction and even produce the audio content without explicit user commands. Experiments demonstrate that WavCraft yields a better performance than existing methods, especially when adjusting the local regions of audio clips. Moreover, WavCraft can follow complex instructions to edit and even create audio content on the top of input recordings, facilitating audio producers in a broader range of applications. Our implementation and demos are available at https://github.com/JinhuaLiang/WavCraft.","sentences":["We introduce WavCraft, a collective system that leverages large language models (LLMs) to connect diverse task-specific models for audio content creation and editing.","Specifically, WavCraft describes the content of raw sound materials in natural language and prompts the LLM conditioned on audio descriptions and users' requests.","WavCraft leverages the in-context learning ability of the LLM to decomposes users' instructions into several tasks and tackle each task collaboratively with audio expert modules.","Through task decomposition along with a set of task-specific models, WavCraft follows the input instruction to create or edit audio content with more details and rationales, facilitating users' control.","In addition, WavCraft is able to cooperate with users via dialogue interaction and even produce the audio content without explicit user commands.","Experiments demonstrate that WavCraft yields a better performance than existing methods, especially when adjusting the local regions of audio clips.","Moreover, WavCraft can follow complex instructions to edit and even create audio content on the top of input recordings, facilitating audio producers in a broader range of applications.","Our implementation and demos are available at https://github.com/JinhuaLiang/WavCraft."],"url":"http://arxiv.org/abs/2403.09527v1","category":"eess.AS"}
{"created":"2024-03-14 16:10:23","title":"Optimizing the Electrical Interface for Large-Scale Color-Center Quantum Processors","abstract":"Quantum processors based on color centers in diamond are promising candidates for future large-scale quantum computers thanks to their flexible optical interface, (relatively) high operating temperature, and high-fidelity operation. Similar to other quantum-computing platforms, the electrical interface required to control and read out such qubits may limit both the performance of the whole system and its scalability. To address this challenge, this work analyzes the requirements of the electrical interface and investigates how to efficiently implement the electronic controller in a scalable architecture comprising a large number of identical unit cells. Among the different discussed functionalities, a specific focus is devoted to the generation of the static and dynamic magnetic fields driving the electron and nuclear spins, because of their major impact on fidelity and scalability. Following the derived requirements, different system architectures, such as a qubit frequency-multiplexing scheme, are considered to identify the most power efficient approach, especially in the presence of inhomogeneity of the qubit Larmor frequency across the processor. As a result, a non-frequency-multiplexed, 1-mm$^2$ unit-cell architecture is proposed as the optimal solution, able to address up to one electron-spin qubit and 9 nuclear-spin qubits within a 3-mW average power consumption, thus establishing the baseline for the scalable electrical interface for future large-scale color-center quantum computers.","sentences":["Quantum processors based on color centers in diamond are promising candidates for future large-scale quantum computers thanks to their flexible optical interface, (relatively) high operating temperature, and high-fidelity operation.","Similar to other quantum-computing platforms, the electrical interface required to control and read out such qubits may limit both the performance of the whole system and its scalability.","To address this challenge, this work analyzes the requirements of the electrical interface and investigates how to efficiently implement the electronic controller in a scalable architecture comprising a large number of identical unit cells.","Among the different discussed functionalities, a specific focus is devoted to the generation of the static and dynamic magnetic fields driving the electron and nuclear spins, because of their major impact on fidelity and scalability.","Following the derived requirements, different system architectures, such as a qubit frequency-multiplexing scheme, are considered to identify the most power efficient approach, especially in the presence of inhomogeneity of the qubit Larmor frequency across the processor.","As a result, a non-frequency-multiplexed, 1-mm$^2$ unit-cell architecture is proposed as the optimal solution, able to address up to one electron-spin qubit and 9 nuclear-spin qubits within a 3-mW average power consumption, thus establishing the baseline for the scalable electrical interface for future large-scale color-center quantum computers."],"url":"http://arxiv.org/abs/2403.09526v1","category":"quant-ph"}
{"created":"2024-03-14 16:07:39","title":"MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation","abstract":"Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the student MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve translation performances on unseen contexts and words.","sentences":["Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency.","Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction.","However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge.","In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner.","Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher.","Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.","Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the student MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve translation performances on unseen contexts and words."],"url":"http://arxiv.org/abs/2403.09522v1","category":"cs.CL"}
{"created":"2024-03-14 15:58:25","title":"Free groups are $L^2$-subgroup rigid","abstract":"In this paper, we introduce the notion of $L^2$-subgroup rigid groups and demonstrate that free groups are $L^2$-subgroup rigid. As a consequence, we establish the equivalence between compressibility, inertness, strong inertness, and $L^2$-independence for a finitely generated subgroup of a free group, confirming a conjecture by Dicks and Ventura as well as the one by Antolin and Jaikin-Zapirain.","sentences":["In this paper, we introduce the notion of $L^2$-subgroup rigid groups and demonstrate that free groups are $L^2$-subgroup rigid.","As a consequence, we establish the equivalence between compressibility, inertness, strong inertness, and $L^2$-independence for a finitely generated subgroup of a free group, confirming a conjecture by Dicks and Ventura as well as the one by Antolin and Jaikin-Zapirain."],"url":"http://arxiv.org/abs/2403.09515v1","category":"math.GR"}
{"created":"2024-03-14 15:57:13","title":"AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting","abstract":"With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., \"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender). These components collaboratively and iteratively communicate to generate a defense prompt. Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://github.com/rain305f/AdaShield.","sentences":["With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced.","However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., \"harmful text\") has been injected into the images to mislead MLLMs.","In this work, we aim to defend against such threats.","Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector).","Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries.","Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender).","These components collaboratively and iteratively communicate to generate a defense prompt.","Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks.","Our code is available at https://github.com/rain305f/AdaShield."],"url":"http://arxiv.org/abs/2403.09513v1","category":"cs.CR"}
{"created":"2024-03-14 15:56:39","title":"Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation","abstract":"There is general agreement that some form of regulation is necessary both for AI creators to be incentivised to develop trustworthy systems, and for users to actually trust those systems. But there is much debate about what form these regulations should take and how they should be implemented. Most work in this area has been qualitative, and has not been able to make formal predictions. Here, we propose that evolutionary game theory can be used to quantitatively model the dilemmas faced by users, AI creators, and regulators, and provide insights into the possible effects of different regulatory regimes. We show that creating trustworthy AI and user trust requires regulators to be incentivised to regulate effectively. We demonstrate the effectiveness of two mechanisms that can achieve this. The first is where governments can recognise and reward regulators that do a good job. In that case, if the AI system is not too risky for users then some level of trustworthy development and user trust evolves. We then consider an alternative solution, where users can condition their trust decision on the effectiveness of the regulators. This leads to effective regulation, and consequently the development of trustworthy AI and user trust, provided that the cost of implementing regulations is not too high. Our findings highlight the importance of considering the effect of different regulatory regimes from an evolutionary game theoretic perspective.","sentences":["There is general agreement that some form of regulation is necessary both for AI creators to be incentivised to develop trustworthy systems, and for users to actually trust those systems.","But there is much debate about what form these regulations should take and how they should be implemented.","Most work in this area has been qualitative, and has not been able to make formal predictions.","Here, we propose that evolutionary game theory can be used to quantitatively model the dilemmas faced by users, AI creators, and regulators, and provide insights into the possible effects of different regulatory regimes.","We show that creating trustworthy AI and user trust requires regulators to be incentivised to regulate effectively.","We demonstrate the effectiveness of two mechanisms that can achieve this.","The first is where governments can recognise and reward regulators that do a good job.","In that case, if the AI system is not too risky for users then some level of trustworthy development and user trust evolves.","We then consider an alternative solution, where users can condition their trust decision on the effectiveness of the regulators.","This leads to effective regulation, and consequently the development of trustworthy AI and user trust, provided that the cost of implementing regulations is not too high.","Our findings highlight the importance of considering the effect of different regulatory regimes from an evolutionary game theoretic perspective."],"url":"http://arxiv.org/abs/2403.09510v1","category":"cs.AI"}
{"created":"2024-03-14 15:53:04","title":"Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition","abstract":"Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to learn appearance invariant representations. Comprehensive empirical evaluation across various architectures and different datasets solidly validates the effectiveness and generalization ability of MCA, and the application of VA in other augmentation methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.","sentences":["Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice.","In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information.","Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances.","Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to learn appearance invariant representations.","Comprehensive empirical evaluation across various architectures and different datasets solidly validates the effectiveness and generalization ability of MCA, and the application of VA in other augmentation methods.","Code is available at https://github.com/BeSpontaneous/MCA-pytorch."],"url":"http://arxiv.org/abs/2403.09506v1","category":"cs.CV"}
{"created":"2024-03-14 15:44:19","title":"EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning","abstract":"Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. EquiAV outperforms previous works across various audio-visual benchmarks.","sentences":["Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations.","However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs.","To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning.","Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor.","It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision.","Notably, this is achieved with minimal computational overhead.","Extensive ablation studies and qualitative results verify the effectiveness of our method.","EquiAV outperforms previous works across various audio-visual benchmarks."],"url":"http://arxiv.org/abs/2403.09502v1","category":"cs.LG"}
{"created":"2024-03-14 15:42:26","title":"A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning","abstract":"Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in a dairy farm setting. This research also explores the effect of the proposed algorithm by adding wind generation data and considering additional case studies. The proposed algorithm reduces the cost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and 24.49\\% when utilizing wind generation. These results underline how reinforcement learning is highly effective in managing batteries in the dairy farming sector.","sentences":["Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture.","Integrating renewable energy generation into dairy farming could help address this challenge.","Effective battery management is important for integrating renewable energy generation.","Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices.","Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain.","This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources.","This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in a dairy farm setting.","This research also explores the effect of the proposed algorithm by adding wind generation data and considering additional case studies.","The proposed algorithm reduces the cost of imported electricity from the grid by 13.41\\%, peak demand by 2\\%, and 24.49\\% when utilizing wind generation.","These results underline how reinforcement learning is highly effective in managing batteries in the dairy farming sector."],"url":"http://arxiv.org/abs/2403.09499v1","category":"cs.LG"}
{"created":"2024-03-14 15:40:13","title":"From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News","abstract":"In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions. Our simulation results uncover patterns in fake news propagation related to topic relevance, and individual traits, aligning with real-world observations. Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications. Our study underscores the significant utility and potential of LLMs in combating fake news.","sentences":["In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation.","Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift.","However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text.","The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion.","Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail.","Specifically, each agent in the simulation represents an individual with a distinct personality.","They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking.","Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions.","Our simulation results uncover patterns in fake news propagation related to topic relevance, and individual traits, aligning with real-world observations.","Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications.","Our study underscores the significant utility and potential of LLMs in combating fake news."],"url":"http://arxiv.org/abs/2403.09498v1","category":"cs.SI"}
{"created":"2024-03-14 15:39:32","title":"On the Gotzmann threshold of monomials","abstract":"Let $R_n=K[x_1,\\dots,x_n]$ be the $n$-variable polynomial ring over a field $K$. Let $S_n$ denote the set of monomials in $R_n$. A monomial $u \\in S_n$ is a \\textit{Gotzmann monomial} if the Borel-stable monomial ideal $\\langle u \\rangle$ it generates in $R_n$ is a Gotzmann ideal. A longstanding open problem is to determine all Gotzmann monomials in $R_n$. Given $u_0 \\in S_{n-1}$, its \\textit{Gotzmann threshold} is the unique nonnegative integer $t_0=\\tau_n(u_0)$ such that $u_0x_n^t$ is a Gotzmann monomial in $R_n$ if and only if $t \\ge t_0$. Currently, the function $\\tau_n$ is exactly known for $n \\le 4$ only. We present here an efficient procedure to determine $\\tau_n(u_0)$ for all $n$ and all $u_0 \\in S_{n-1}$. As an application, in the critical case $u_0=x_2^d$, we determine $\\tau_5(x_2^d)$ for all $d$ and we conjecture that for $n \\ge 6$, $\\tau_n(x_2^d)$ is a polynomial in $d$ of degree $2^{n-2}$ and dominant term equal to that of the $(n-2)$-iterated binomial coefficient $$ \\binom {\\binom {\\binom d2}2}{\\stackrel{\\cdots}2}. $$","sentences":["Let $R_n=K[x_1,\\dots,x_n]$ be the $n$-variable polynomial ring over a field $K$. Let $S_n$ denote the set of monomials in $R_n$. A monomial $u \\in S_n$ is a \\textit{Gotzmann monomial} if the Borel-stable monomial ideal $\\langle u \\rangle$ it generates in $R_n$ is a Gotzmann ideal.","A longstanding open problem is to determine all Gotzmann monomials in $R_n$. Given $u_0 \\in S_{n-1}$, its \\textit{Gotzmann threshold} is the unique nonnegative integer $t_0=\\tau_n(u_0)$ such that $u_0x_n^t$ is a Gotzmann monomial in $R_n$ if and only if $t \\ge t_0$.","Currently, the function $\\tau_n$ is exactly known for $n \\le 4$ only.","We present here an efficient procedure to determine $\\tau_n(u_0)$ for all $n$ and all $u_0 \\in S_{n-1}$. As an application, in the critical case $u_0=x_2^d$, we determine $\\tau_5(x_2^d)$ for all $d$ and we conjecture that for $n \\ge 6$, $\\tau_n(x_2^d)$ is a polynomial in $d$ of degree $2^{n-2}$ and dominant term equal to that of the $(n-2)$-iterated binomial coefficient $$ \\binom {\\binom {\\binom d2}2}{\\stackrel{\\cdots}2}.","$$"],"url":"http://arxiv.org/abs/2403.09497v1","category":"math.AC"}
{"created":"2024-03-14 15:39:24","title":"The Development of Investment Planning Models for the United Kingdoms Wind and Solar Fleets","abstract":"Previous work has resulted in the development of an energy model able to calculate wind and solar fleet efficiencies. However, for investment planning purposes, it is necessary to calculate from the lowest economically acceptable efficiencies how much wind and solar generation would be economically justified. The paper explains how this objective has been achieved with arrays (investment planning tables) created after carrying out a structured investigation of the behaviour of the electricity system over the whole of its operational range. The tables are then applied to National Grid prediction of the size and composition of the system in the year 2035. A conclusion is reached that wind and solar generation will only be able to supply about 70% of electrical demand, the other 30% being provided by dispatchable sources of generation, which must be sufficiently fast acting to maintain electricity system stability, such as the use of combined cycle gas turbines. This limit on deployment of wind and solar generation restricts their ability to decarbonise the electricity system and is likely to lead in 2035 to a residual of 72 million tonnes per annum of carbon dioxide emissions which wind and solar generations will be unable to address","sentences":["Previous work has resulted in the development of an energy model able to calculate wind and solar fleet efficiencies.","However, for investment planning purposes, it is necessary to calculate from the lowest economically acceptable efficiencies how much wind and solar generation would be economically justified.","The paper explains how this objective has been achieved with arrays (investment planning tables) created after carrying out a structured investigation of the behaviour of the electricity system over the whole of its operational range.","The tables are then applied to National Grid prediction of the size and composition of the system in the year 2035.","A conclusion is reached that wind and solar generation will only be able to supply about 70% of electrical demand, the other 30% being provided by dispatchable sources of generation, which must be sufficiently fast acting to maintain electricity system stability, such as the use of combined cycle gas turbines.","This limit on deployment of wind and solar generation restricts their ability to decarbonise the electricity system and is likely to lead in 2035 to a residual of 72 million tonnes per annum of carbon dioxide emissions which wind and solar generations will be unable to address"],"url":"http://arxiv.org/abs/2403.09496v1","category":"eess.SY"}
{"created":"2024-03-14 15:30:19","title":"Gravitational waves from glitch-induced f-mode oscillations in quark and neutron stars","abstract":"Matter in compact stars is dense enough that transient events within the star could have sufficiently high energies to produce detectable gravitational waves (GWs). These GWs could be used to constrain the equation of state (EoS) for matter in the star and could reveal that there is more than one type of EoS at play in the population, implying that multiple types of compact stars exist. One of these types could be quark stars, composed almost entirely of stable quark matter, and observing GWs is a way to test for the strange matter EoS. Here we explore the possibility that, if fundamental (f-) mode oscillations in pulsars are induced by a pulsar glitch, then these oscillations might produce detectable GWs. We use the existing population of pulsars and their glitches, as well as a much larger synthesized population, along with 15 EoSs (8 for neutron stars and 7 for quark stars) to generate frequencies, damping times, and GW strengths for each. We find that of the EoSs examined, all quark star EoSs produce narrower distributions of f-mode frequency than neutron star EoSs. This result, along with other elements of the data, could be used to differentiate between GWs (or other signals from f-modes) originating from neutron stars and quark stars and thus could confirm the existence of quark stars. We also find that GW astronomy is a potentially viable method for detecting a larger population of pulsars which are not observable electromagnetically and that future GW observatories have the possibility to greatly expand this capability.","sentences":["Matter in compact stars is dense enough that transient events within the star could have sufficiently high energies to produce detectable gravitational waves (GWs).","These GWs could be used to constrain the equation of state (EoS) for matter in the star and could reveal that there is more than one type of EoS at play in the population, implying that multiple types of compact stars exist.","One of these types could be quark stars, composed almost entirely of stable quark matter, and observing GWs is a way to test for the strange matter","EoS.","Here we explore the possibility that, if fundamental (f-) mode oscillations in pulsars are induced by a pulsar glitch, then these oscillations might produce detectable GWs.","We use the existing population of pulsars and their glitches, as well as a much larger synthesized population, along with 15 EoSs (8 for neutron stars and 7 for quark stars) to generate frequencies, damping times, and GW strengths for each.","We find that of the EoSs examined, all quark star EoSs produce narrower distributions of f-mode frequency than neutron star EoSs.","This result, along with other elements of the data, could be used to differentiate between GWs (or other signals from f-modes) originating from neutron stars and quark stars and thus could confirm the existence of quark stars.","We also find that GW astronomy is a potentially viable method for detecting a larger population of pulsars which are not observable electromagnetically and that future GW observatories have the possibility to greatly expand this capability."],"url":"http://arxiv.org/abs/2403.09489v1","category":"gr-qc"}
{"created":"2024-03-14 15:30:14","title":"Rectifying Demonstration Shortcut in In-Context Learning","abstract":"Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In both settings, In-Context Calibration demonstrates substantial improvements, with results generalized across three LLM families (OPT, GPT, and Llama2) under various configurations.","sentences":["Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities.","However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction.","In this work, we term this phenomenon as the `Demonstration Shortcut'.","While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations.","To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method.","We evaluate the effectiveness of the proposed method in two settings: (1)","the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens.","In both settings, In-Context Calibration demonstrates substantial improvements, with results generalized across three LLM families (OPT, GPT, and Llama2) under various configurations."],"url":"http://arxiv.org/abs/2403.09488v1","category":"cs.CL"}
{"created":"2024-03-14 15:29:21","title":"Tamed loops: A try for non-renormalizable Einstein gravity in UV-free scheme","abstract":"How to describe loop corrections is a fundamental challenge in the quantization of Einstein gravity. In this paper, we give it a try in UV-free scheme, and the result seems to be effective for graviton loops. This indicates that both loops of the renormalizable Standard Model and the non-renormalizable Einstein gravity can be described by the method of UV-free scheme.","sentences":["How to describe loop corrections is a fundamental challenge in the quantization of Einstein gravity.","In this paper, we give it a try in UV-free scheme, and the result seems to be effective for graviton loops.","This indicates that both loops of the renormalizable Standard Model and the non-renormalizable Einstein gravity can be described by the method of UV-free scheme."],"url":"http://arxiv.org/abs/2403.09487v1","category":"hep-ph"}
{"created":"2024-03-14 15:29:09","title":"SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams","abstract":"Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image. Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. Moreover, the quality of reconstructed images is capped by the generated images based on motion analysis interpolation, which inherently differs from the actual scene, affecting the generalization ability of these methods in real high-speed scenarios. To address these challenges, we propose the first self-supervised framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With knowledge distillation and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models will be available at \\url{https://github.com/chenkang455/S-SDM}.","sentences":["Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image.","Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem.","Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain.","Moreover, the quality of reconstructed images is capped by the generated images based on motion analysis interpolation, which inherently differs from the actual scene, affecting the generalization ability of these methods in real high-speed scenarios.","To address these challenges, we propose the first self-supervised framework for the task of spike-guided motion deblurring.","Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences.","We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model.","With knowledge distillation and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input.","Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework.","Our code, data and trained models will be available at \\url{https://github.com/chenkang455/S-SDM}."],"url":"http://arxiv.org/abs/2403.09486v1","category":"cs.CV"}
{"created":"2024-03-14 15:27:53","title":"Dynamical pressure boundary condition for weakly-compressible smoothed particle hydrodynamics","abstract":"This paper introduces a novel dynamical pressure boundary condition for weakly-compressible smoothed particle hydrodynamics (WCSPH). Unlike previous methods that rely on indirect approaches or ghost particles, our method integrates the dynamical boundary pressure directly into the SPH approximation of the pressure gradient on near-boundary particles. Additionally, we develop a meshfree bidirectional in-/outflow buffer by periodically relabelling buffer particles at each time step, a concept that has not been explored before. This simple yet effective buffer facilitates the simulation of both uni- and bidirectional flows, especially those with mixed in-/outflow boundary conditions. We validate the accuracy and convergence of our method through benchmark cases with available analytical solutions. Furthermore, we demonstrate its versatility in hemodynamic simulations by investigating generic carotid and aorta flows with the Windkessel model, paving the way for studying the cardiovascular system within a unified meshfree computational framework.","sentences":["This paper introduces a novel dynamical pressure boundary condition for weakly-compressible smoothed particle hydrodynamics (WCSPH).","Unlike previous methods that rely on indirect approaches or ghost particles, our method integrates the dynamical boundary pressure directly into the SPH approximation of the pressure gradient on near-boundary particles.","Additionally, we develop a meshfree bidirectional in-/outflow buffer by periodically relabelling buffer particles at each time step, a concept that has not been explored before.","This simple yet effective buffer facilitates the simulation of both uni- and bidirectional flows, especially those with mixed in-/outflow boundary conditions.","We validate the accuracy and convergence of our method through benchmark cases with available analytical solutions.","Furthermore, we demonstrate its versatility in hemodynamic simulations by investigating generic carotid and aorta flows with the Windkessel model, paving the way for studying the cardiovascular system within a unified meshfree computational framework."],"url":"http://arxiv.org/abs/2403.09485v1","category":"physics.flu-dyn"}
{"created":"2024-03-14 15:27:14","title":"Artificial Bugs for Crowdsearch","abstract":"Bug bounty programs, where external agents are invited to search and report vulnerabilities (bugs) in exchange for rewards (bounty), have become a major tool for companies to improve their systems. We suggest augmenting such programs by inserting artificial bugs to increase the incentives to search for real (organic) bugs. Using a model of crowdsearch, we identify the efficiency gains by artificial bugs, and we show that for this, it is sufficient to insert only one artificial bug. Artificial bugs are particularly beneficial, for instance, if the designer places high valuations on finding organic bugs or if the budget for bounty is not sufficiently high. We discuss how to implement artificial bugs and outline their further benefits.","sentences":["Bug bounty programs, where external agents are invited to search and report vulnerabilities (bugs) in exchange for rewards (bounty), have become a major tool for companies to improve their systems.","We suggest augmenting such programs by inserting artificial bugs to increase the incentives to search for real (organic) bugs.","Using a model of crowdsearch, we identify the efficiency gains by artificial bugs, and we show that for this, it is sufficient to insert only one artificial bug.","Artificial bugs are particularly beneficial, for instance, if the designer places high valuations on finding organic bugs or if the budget for bounty is not sufficiently high.","We discuss how to implement artificial bugs and outline their further benefits."],"url":"http://arxiv.org/abs/2403.09484v1","category":"econ.TH"}
{"created":"2024-03-14 15:25:23","title":"Clinical Reasoning over Tabular Data and Text with Bayesian Networks","abstract":"Bayesian networks are well-suited for clinical reasoning on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework. This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner. This is illustrated with simulation results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context.","sentences":["Bayesian networks are well-suited for clinical reasoning on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework.","This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner.","This is illustrated with simulation results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context."],"url":"http://arxiv.org/abs/2403.09481v1","category":"cs.AI"}
{"created":"2024-03-14 15:22:33","title":"What Sketch Explainability Really Means for Downstream Tasks","abstract":"In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies. Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks. We propose a lightweight and portable explainability solution -- a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training. Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch adversarial attacks. The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks. By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-SLA), each with its advantages for specific downstream tasks.","sentences":["In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies.","Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks.","We propose a lightweight and portable explainability solution -- a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training.","Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch adversarial attacks.","The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks.","By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-SLA), each with its advantages for specific downstream tasks."],"url":"http://arxiv.org/abs/2403.09480v1","category":"cs.CV"}
{"created":"2024-03-14 15:20:54","title":"Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks","abstract":"Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks. Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks. Promisingly, the skill generalization exhibit effective in cross-dataset and cross-domain scenarios. Complex reasoning can also help enhance atomic skills. Our findings offer valuable guidance for designing better training strategies for complex reasoning tasks.","sentences":["Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion.","Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks.","In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks.","Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization.","In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks.","By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks.","Promisingly, the skill generalization exhibit effective in cross-dataset and cross-domain scenarios.","Complex reasoning can also help enhance atomic skills.","Our findings offer valuable guidance for designing better training strategies for complex reasoning tasks."],"url":"http://arxiv.org/abs/2403.09479v1","category":"cs.LG"}
{"created":"2024-03-14 15:19:52","title":"A syntactic characterization of weakly Mal'tsev varieties","abstract":"The notion of a weakly Mal'tsev category, as it was introduced in 2008 by the third author, is a generalization of the classical notion of a Mal'tsev category. It is well-known that a variety of universal algebras is a Mal'tsev category if and only if its theory admits a Mal'tsev term. In the main theorem of this paper, we prove a syntactic characterization of the varieties that are weakly Mal'tsev categories. We apply our result to the variety of distributive lattices which was known to be a weakly Mal'tsev category before. By a result of Z. Janelidze and the third author, a finitely complete category is weakly Mal'tsev if and only if any internal strong reflexive relation is an equivalence relation. In the last part of this paper, we give a syntactic characterization of those varieties in which any regular reflexive relation is an equivalence relation.","sentences":["The notion of a weakly Mal'tsev category, as it was introduced in 2008 by the third author, is a generalization of the classical notion of a Mal'tsev category.","It is well-known that a variety of universal algebras is a Mal'tsev category if and only if its theory admits a Mal'tsev term.","In the main theorem of this paper, we prove a syntactic characterization of the varieties that are weakly Mal'tsev categories.","We apply our result to the variety of distributive lattices which was known to be a weakly Mal'tsev category before.","By a result of Z. Janelidze and the third author, a finitely complete category is weakly Mal'tsev if and only if any internal strong reflexive relation is an equivalence relation.","In the last part of this paper, we give a syntactic characterization of those varieties in which any regular reflexive relation is an equivalence relation."],"url":"http://arxiv.org/abs/2403.09478v1","category":"math.CT"}
{"created":"2024-03-14 15:14:24","title":"An Industrial Experience Report about Challenges from Continuous Monitoring, Improvement, and Deployment for Autonomous Driving Features","abstract":"Using continuous development, deployment, and monitoring (CDDM) to understand and improve applications in a customer's context is widely used for non-safety applications such as smartphone apps or web applications to enable rapid and innovative feature improvements. Having demonstrated its potential in such domains, it may have the potential to also improve the software development for automotive functions as some OEMs described on a high level in their financial company communiqus. However, the application of a CDDM strategy also faces challenges from a process adherence and documentation perspective as required by safety-related products such as autonomous driving systems (ADS) and guided by industry standards such as ISO-26262 and ISO21448. There are publications on CDDM in safety-relevant contexts that focus on safety-critical functions on a rather generic level and thus, not specifically ADS or automotive, or that are concentrating only on software and hence, missing out the particular context of an automotive OEM: Well-established legacy processes and the need of their adaptations, and aspects originating from the role of being a system integrator for software/software, hardware/hardware, and hardware/software. In this paper, particular challenges from the automotive domain to better adopt CDDM are identified and discussed to shed light on research gaps to enhance CDDM, especially for the software development of safe ADS. The challenges are identified from today's industrial well-established ways of working by conducting interviews with domain experts and complemented by a literature study.","sentences":["Using continuous development, deployment, and monitoring (CDDM) to understand and improve applications in a customer's context is widely used for non-safety applications such as smartphone apps or web applications to enable rapid and innovative feature improvements.","Having demonstrated its potential in such domains, it may have the potential to also improve the software development for automotive functions as some OEMs described on a high level in their financial company communiqus.","However, the application of a CDDM strategy also faces challenges from a process adherence and documentation perspective as required by safety-related products such as autonomous driving systems (ADS) and guided by industry standards such as ISO-26262 and ISO21448.","There are publications on CDDM in safety-relevant contexts that focus on safety-critical functions on a rather generic level and thus, not specifically ADS or automotive, or that are concentrating only on software and hence, missing out the particular context of an automotive OEM:","Well-established legacy processes and the need of their adaptations, and aspects originating from the role of being a system integrator for software/software, hardware/hardware, and hardware/software.","In this paper, particular challenges from the automotive domain to better adopt CDDM are identified and discussed to shed light on research gaps to enhance CDDM, especially for the software development of safe ADS.","The challenges are identified from today's industrial well-established ways of working by conducting interviews with domain experts and complemented by a literature study."],"url":"http://arxiv.org/abs/2403.09474v1","category":"cs.SE"}
{"created":"2024-03-14 15:14:23","title":"Analysis of a continuous opinion and discrete action dynamics model coupled with an external observation dynamics","abstract":"We consider a set of consumers in a city or town (who thus generate pollution) whose opinion is governed by a continuous opinion and discrete action (CODA) dynamics model. This dynamics is coupled with an observation signal dynamics, which defines the information the consumers have access to regarding the common pollution. We show that the external observation signal has a significant impact on the asymptotic behavior of the CODA model. When the coupling is strong, it induces either a chaotic behavior or convergence towards a limit cycle. When the coupling is weak, a more classical behavior characterized by local agreements in polarized clusters is observed. In both cases, conditions under which clusters of consumers don't change their actions are provided.Numerical examples are provided to illustrate the derived analytical results.","sentences":["We consider a set of consumers in a city or town (who thus generate pollution) whose opinion is governed by a continuous opinion and discrete action (CODA) dynamics model.","This dynamics is coupled with an observation signal dynamics, which defines the information the consumers have access to regarding the common pollution.","We show that the external observation signal has a significant impact on the asymptotic behavior of the CODA model.","When the coupling is strong, it induces either a chaotic behavior or convergence towards a limit cycle.","When the coupling is weak, a more classical behavior characterized by local agreements in polarized clusters is observed.","In both cases, conditions under which clusters of consumers don't change their actions are provided.","Numerical examples are provided to illustrate the derived analytical results."],"url":"http://arxiv.org/abs/2403.09473v1","category":"math.OC"}
{"created":"2024-03-14 15:12:38","title":"Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision","abstract":"Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \\textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such \\textit{easy-to-hard generalization from evaluators} can enable \\textit{easy-to-hard generalizations in generators} either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.","sentences":["Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result.","This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans?","This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \\textit{easy-to-hard generalization}.","Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks.","Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems.","We show that such \\textit{easy-to-hard generalization from evaluators} can enable \\textit{easy-to-hard generalizations in generators} either through re-ranking or reinforcement learning (RL).","Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500, despite only using human supervision on easy problems.","Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision."],"url":"http://arxiv.org/abs/2403.09472v1","category":"cs.LG"}
{"created":"2024-03-14 15:10:54","title":"MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models","abstract":"Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model and attention mechanisms to improve gesture synthesis. However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge. We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through multimodal integration. Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models.","sentences":["Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality.","Recent advancements have utilized the diffusion model and attention mechanisms to improve gesture synthesis.","However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge.","We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures.","Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through multimodal integration.","Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models."],"url":"http://arxiv.org/abs/2403.09471v1","category":"cs.CV"}
{"created":"2024-03-14 15:09:04","title":"Climate Immobility Traps: A Household-Level Test","abstract":"The complex relationship between climate shocks, migration, and adaptation hampers a rigorous understanding of the heterogeneous mobility outcomes of farm households exposed to climate risk. To unpack this heterogeneity, the analysis combines longitudinal multi-topic household survey data from Nigeria with a causal machine learning approach, tailored to a conceptual framework bridging economic migration theory and the poverty traps literature. The results show that pre-shock asset levels, in situ adaptive capacity, and cumulative shock exposure drive not just the magnitude but also the sign of the impact of agriculture-relevant weather anomalies on the mobility outcomes of farming households. While local adaptation acts as a substitute for migration, the roles played by wealth constraints and repeated shock exposure suggest the presence of climate-induced immobility traps.","sentences":["The complex relationship between climate shocks, migration, and adaptation hampers a rigorous understanding of the heterogeneous mobility outcomes of farm households exposed to climate risk.","To unpack this heterogeneity, the analysis combines longitudinal multi-topic household survey data from Nigeria with a causal machine learning approach, tailored to a conceptual framework bridging economic migration theory and the poverty traps literature.","The results show that pre-shock asset levels, in situ adaptive capacity, and cumulative shock exposure drive not just the magnitude but also the sign of the impact of agriculture-relevant weather anomalies on the mobility outcomes of farming households.","While local adaptation acts as a substitute for migration, the roles played by wealth constraints and repeated shock exposure suggest the presence of climate-induced immobility traps."],"url":"http://arxiv.org/abs/2403.09470v1","category":"econ.GN"}
{"created":"2024-03-14 15:07:36","title":"Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing","abstract":"Diffusion models have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing. A commonly adopted strategy for editing real images involves inverting the diffusion process to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits. However, current methods for diffusion inversion often struggle to produce edits that are both faithful to the specified text prompt and closely resemble the source image. To overcome these limitations, we introduce a novel and adaptable diffusion inversion technique for real image editing, which is grounded in a theoretical analysis of the role of $\\eta$ in the DDIM sampling equation for enhanced editability. By designing a universal diffusion inversion method with a time- and region-dependent $\\eta$ function, we enable flexible control over the editing extent. Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach. Our method not only sets a new benchmark in the field but also significantly outperforms existing strategies. Our code is available at https://github.com/furiosa-ai/eta-inversion","sentences":["Diffusion models have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing.","A commonly adopted strategy for editing real images involves inverting the diffusion process to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits.","However, current methods for diffusion inversion often struggle to produce edits that are both faithful to the specified text prompt and closely resemble the source image.","To overcome these limitations, we introduce a novel and adaptable diffusion inversion technique for real image editing, which is grounded in a theoretical analysis of the role of $\\eta$ in the DDIM sampling equation for enhanced editability.","By designing a universal diffusion inversion method with a time- and region-dependent $\\eta$ function, we enable flexible control over the editing extent.","Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach.","Our method not only sets a new benchmark in the field but also significantly outperforms existing strategies.","Our code is available at https://github.com/furiosa-ai/eta-inversion"],"url":"http://arxiv.org/abs/2403.09468v1","category":"cs.CV"}
{"created":"2024-03-14 15:04:45","title":"Outlier Robust Multivariate Polynomial Regression","abstract":"We study the problem of robust multivariate polynomial regression: let $p\\colon\\mathbb{R}^n\\to\\mathbb{R}$ be an unknown $n$-variate polynomial of degree at most $d$ in each variable. We are given as input a set of random samples $(\\mathbf{x}_i,y_i) \\in [-1,1]^n \\times \\mathbb{R}$ that are noisy versions of $(\\mathbf{x}_i,p(\\mathbf{x}_i))$. More precisely, each $\\mathbf{x}_i$ is sampled independently from some distribution $\\chi$ on $[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an outlier) with probability at most $\\rho < 1/2$, and otherwise satisfies $|y_i-p(\\mathbf{x}_i)|\\leq\\sigma$. The goal is to output a polynomial $\\hat{p}$, of degree at most $d$ in each variable, within an $\\ell_\\infty$-distance of at most $O(\\sigma)$ from $p$.   Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We generalize their results to the $n$-variate setting, showing an algorithm that achieves a sample complexity of $O_n(d^n\\log d)$, where the hidden constant depends on $n$, if $\\chi$ is the $n$-dimensional Chebyshev distribution. The sample complexity is $O_n(d^{2n}\\log d)$, if the samples are drawn from the uniform distribution instead. The approximation error is guaranteed to be at most $O(\\sigma)$, and the run-time depends on $\\log(1/\\sigma)$. In the setting where each $\\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the run-time's dependence on $N$ is linear. We also show that our sample complexities are optimal in terms of $d^n$. Furthermore, we show that it is possible to have the run-time be independent of $1/\\sigma$, at the cost of a higher sample complexity.","sentences":["We study the problem of robust multivariate polynomial regression: let $p\\colon\\mathbb{R}^n\\to\\mathbb{R}$ be an unknown $n$-variate polynomial of degree at most $d$ in each variable.","We are given as input a set of random samples $(\\mathbf{x}_i,y_i)","\\in","[-1,1]^n \\times \\mathbb{R}$","that are noisy versions of $(\\mathbf{x}_i,p(\\mathbf{x}_i))$. More precisely, each $\\mathbf{x}_i$ is sampled independently from some distribution $\\chi$ on $[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an outlier) with probability at most $\\rho < 1/2$, and otherwise satisfies $|y_i-p(\\mathbf{x}_i)|\\leq\\sigma$.","The goal is to output a polynomial $\\hat{p}$, of degree at most $d$ in each variable, within an $\\ell_\\infty$-distance of at most $O(\\sigma)$ from $p$.   Kane, Karmalkar, and","Price","[FOCS'17] solved this problem for $n=1$. We generalize their results to the $n$-variate setting, showing an algorithm that achieves a sample complexity of $O_n(d^n\\log d)$, where the hidden constant depends on $n$, if $\\chi$ is the $n$-dimensional Chebyshev distribution.","The sample complexity is $O_n(d^{2n}\\log d)$, if the samples are drawn from the uniform distribution instead.","The approximation error is guaranteed to be at most $O(\\sigma)$, and the run-time depends on $\\log(1/\\sigma)$. In the setting where each $\\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the run-time's dependence on $N$ is linear.","We also show that our sample complexities are optimal in terms of $d^n$. Furthermore, we show that it is possible to have the run-time be independent of $1/\\sigma$, at the cost of a higher sample complexity."],"url":"http://arxiv.org/abs/2403.09465v1","category":"cs.DS"}
{"created":"2024-03-14 15:02:49","title":"Flat-top plasma operational space of the STEP power plant","abstract":"STEP is a spherical tokamak prototype power plant that is being designed to demonstrate net electric power. The design phase involves the exploitation of plasma models to optimise fusion performance subject to satisfying various physics and engineering constraints. A modelling workflow, including integrated core plasma modelling, MHD stability analysis, SOL and pedestal modelling, coil set and free boundary equilibrium solvers, and whole plant design, has been developed to specify the design parameters and to develop viable scenarios. The integrated core plasma model JETTO is used to develop individual flat-top operating points that satisfy imposed criteria for fusion power performance within operational constraints. Key plasma parameters such as normalised beta, Greenwald density fraction, auxiliary power and radiated power have been scanned to scope the operational space and to derive a collection of candidate non-inductive flat-top points. The assumed auxiliary heating and current drive is either from electron cyclotron systems only or a combination of electron cyclotron and electron Bernstein waves. At present stages of transport modelling, there is a large uncertainty in overall confinement for relevant parameter regimes. For each of the two auxiliary heating and current drive systems scenarios, two candidate flat-top points have been developed based on different confinement assumptions, totalling to four operating points. A lower confinement assumption generally suggests operating points in high-density, high auxiliary power regimes, whereas higher confinement would allow access to a broader parameter regime in density and power while maintaining target fusion power performance.","sentences":["STEP is a spherical tokamak prototype power plant that is being designed to demonstrate net electric power.","The design phase involves the exploitation of plasma models to optimise fusion performance subject to satisfying various physics and engineering constraints.","A modelling workflow, including integrated core plasma modelling, MHD stability analysis, SOL and pedestal modelling, coil set and free boundary equilibrium solvers, and whole plant design, has been developed to specify the design parameters and to develop viable scenarios.","The integrated core plasma model JETTO is used to develop individual flat-top operating points that satisfy imposed criteria for fusion power performance within operational constraints.","Key plasma parameters such as normalised beta, Greenwald density fraction, auxiliary power and radiated power have been scanned to scope the operational space and to derive a collection of candidate non-inductive flat-top points.","The assumed auxiliary heating and current drive is either from electron cyclotron systems only or a combination of electron cyclotron and electron Bernstein waves.","At present stages of transport modelling, there is a large uncertainty in overall confinement for relevant parameter regimes.","For each of the two auxiliary heating and current drive systems scenarios, two candidate flat-top points have been developed based on different confinement assumptions, totalling to four operating points.","A lower confinement assumption generally suggests operating points in high-density, high auxiliary power regimes, whereas higher confinement would allow access to a broader parameter regime in density and power while maintaining target fusion power performance."],"url":"http://arxiv.org/abs/2403.09460v1","category":"physics.plasm-ph"}
{"created":"2024-03-14 14:58:39","title":"Vertex coupling interpolation in quantum chain graphs","abstract":"We analyze band spectrum of the periodic quantum graph in the form of a chain of rings connected by line segments with the vertex coupling which violates the time reversal invariance, interpolating between the $\\delta$ coupling and the one determined by a simple circulant matrix. We find that flat bands are generically absent and that the negative spectrum is nonempty even for interpolation with a non-attractive $\\delta$ coupling; we also determine the high-energy asymptotic behavior of the bands.","sentences":["We analyze band spectrum of the periodic quantum graph in the form of a chain of rings connected by line segments with the vertex coupling which violates the time reversal invariance, interpolating between the $\\delta$ coupling and the one determined by a simple circulant matrix.","We find that flat bands are generically absent and that the negative spectrum is nonempty even for interpolation with a non-attractive $\\delta$ coupling; we also determine the high-energy asymptotic behavior of the bands."],"url":"http://arxiv.org/abs/2403.09457v1","category":"math-ph"}
{"created":"2024-03-14 14:55:16","title":"Edge-apexing in hereditary classes of graphs","abstract":"A class $\\mathcal{G}$ of graphs is called hereditary if it is closed under taking induced subgraphs. We denote by $G^{epex}$ the class of graphs that are at most one edge away from being in $\\mathcal{G}$. We note that $G^{epex}$ is hereditary and prove that if a hereditary class $\\mathcal{G}$ has finitely many forbidden induced subgraphs, then so does $G^{epex}$.   The hereditary class of cographs consists of all graphs $G$ that can be generated from $K_1$ using complementation and disjoint union. Cographs are precisely the graphs that do not have the $4$-vertex path as an induced subgraph. For the class of edge-apex cographs our main result bounds the order of such forbidden induced subgraphs by 8 and finds all of them by computer search.","sentences":["A class $\\mathcal{G}$ of graphs is called hereditary if it is closed under taking induced subgraphs.","We denote by $G^{epex}$ the class of graphs that are at most one edge away from being in $\\mathcal{G}$. We note that $G^{epex}$ is hereditary and prove that if a hereditary class $\\mathcal{G}$ has finitely many forbidden induced subgraphs, then so does $G^{epex}$.   The hereditary class of cographs consists of all graphs $G$ that can be generated from $K_1$ using complementation and disjoint union.","Cographs are precisely the graphs that do not have the $4$-vertex path as an induced subgraph.","For the class of edge-apex cographs our main result bounds the order of such forbidden induced subgraphs by 8 and finds all of them by computer search."],"url":"http://arxiv.org/abs/2403.09456v1","category":"math.CO"}
{"created":"2024-03-14 14:53:18","title":"Machine learning for structural design models of continuous beam systems via influence zones","abstract":"This work develops a machine learned structural design model for continuous beam systems from the inverse problem perspective. After demarcating between forward, optimisation and inverse machine learned operators, the investigation proposes a novel methodology based on the recently developed influence zone concept which represents a fundamental shift in approach compared to traditional structural design methods. The aim of this approach is to conceptualise a non-iterative structural design model that predicts cross-section requirements for continuous beam systems of arbitrary system size. After generating a dataset of known solutions, an appropriate neural network architecture is identified, trained, and tested against unseen data. The results show a mean absolute percentage testing error of 1.6% for cross-section property predictions, along with a good ability of the neural network to generalise well to structural systems of variable size. The CBeamXP dataset generated in this work and an associated python-based neural network training script are available at an open-source data repository to allow for the reproducibility of results and to encourage further investigations.","sentences":["This work develops a machine learned structural design model for continuous beam systems from the inverse problem perspective.","After demarcating between forward, optimisation and inverse machine learned operators, the investigation proposes a novel methodology based on the recently developed influence zone concept which represents a fundamental shift in approach compared to traditional structural design methods.","The aim of this approach is to conceptualise a non-iterative structural design model that predicts cross-section requirements for continuous beam systems of arbitrary system size.","After generating a dataset of known solutions, an appropriate neural network architecture is identified, trained, and tested against unseen data.","The results show a mean absolute percentage testing error of 1.6% for cross-section property predictions, along with a good ability of the neural network to generalise well to structural systems of variable size.","The CBeamXP dataset generated in this work and an associated python-based neural network training script are available at an open-source data repository to allow for the reproducibility of results and to encourage further investigations."],"url":"http://arxiv.org/abs/2403.09454v1","category":"cs.LG"}
{"created":"2024-03-14 14:52:34","title":"Combinatorics of Essential Sets for Positroids","abstract":"Positroids are families of matroids introduced by Postnikov in the study of non-negative Grassmannians. Postnikov identified several combinatorial objects in bijections with positroids, among which are bounded affine permutations. On the other hand, the notion of essential sets, introduced for permutations by Fulton, was used by Knutson in the study of the special family of interval rank positroids. We generalize Fulton's essential sets to bounded affine permutations. The bijection of the latter with positroids, allows the study of their relation. From the point of view of positroids, essential sets are maximally dependent cyclic interval. We define connected essential sets and prove that they give a facet description of the positroid polytope, as well as equations defining the positroid variety. We define a subset of essential sets, called core, which contains minimal rank conditions to uniquely recover a positroid. We provide an algorithm to retrieve the positroid satisfying the rank conditions in the core or any compatible rank condition on cyclic intervals.","sentences":["Positroids are families of matroids introduced by Postnikov in the study of non-negative Grassmannians.","Postnikov identified several combinatorial objects in bijections with positroids, among which are bounded affine permutations.","On the other hand, the notion of essential sets, introduced for permutations by Fulton, was used by Knutson in the study of the special family of interval rank positroids.","We generalize Fulton's essential sets to bounded affine permutations.","The bijection of the latter with positroids, allows the study of their relation.","From the point of view of positroids, essential sets are maximally dependent cyclic interval.","We define connected essential sets and prove that they give a facet description of the positroid polytope, as well as equations defining the positroid variety.","We define a subset of essential sets, called core, which contains minimal rank conditions to uniquely recover a positroid.","We provide an algorithm to retrieve the positroid satisfying the rank conditions in the core or any compatible rank condition on cyclic intervals."],"url":"http://arxiv.org/abs/2403.09453v1","category":"math.CO"}
{"created":"2024-03-14 14:48:37","title":"Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk","abstract":"While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized. Codes are available at https://github.com/VITA-Group/Shake-to-Leak.","sentences":["While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information.","In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks.","We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations.","In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain.","This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized.","Codes are available at https://github.com/VITA-Group/Shake-to-Leak."],"url":"http://arxiv.org/abs/2403.09450v1","category":"cs.LG"}
{"created":"2024-03-14 14:37:25","title":"Sequential optimal experimental design for vapor-liquid equilibrium modeling","abstract":"We propose a general methodology of sequential locally optimal design of experiments for explicit or implicit nonlinear models, as they abound in chemical engineering and, in particular, in vapor-liquid equilibrium modeling. As a sequential design method, our method iteratively alternates between performing experiments, updating parameter estimates, and computing new experiments. Specifically, our sequential design method computes a whole batch of new experiments in each iteration and this batch of new experiments is designed in a two-stage locally optimal manner. In essence, this means that in every iteration the combined information content of the newly proposed experiments and of the already performed experiments is maximized. In order to solve these two-stage locally optimal design problems, a recent and efficient adaptive discretization algorithm is used. We demonstrate the benefits of the proposed methodology on the example of of the parameter estimation for the non-random two-liquid model for narrow azeotropic vapor-liquid equilibria. As it turns out, our sequential optimal design method requires substantially fewer experiments than traditional factorial design to achieve the same model precision and prediction quality. Consequently, our method can contribute to a substantially reduced experimental effort in vapor-liquid equilibrium modeling and beyond.","sentences":["We propose a general methodology of sequential locally optimal design of experiments for explicit or implicit nonlinear models, as they abound in chemical engineering and, in particular, in vapor-liquid equilibrium modeling.","As a sequential design method, our method iteratively alternates between performing experiments, updating parameter estimates, and computing new experiments.","Specifically, our sequential design method computes a whole batch of new experiments in each iteration and this batch of new experiments is designed in a two-stage locally optimal manner.","In essence, this means that in every iteration the combined information content of the newly proposed experiments and of the already performed experiments is maximized.","In order to solve these two-stage locally optimal design problems, a recent and efficient adaptive discretization algorithm is used.","We demonstrate the benefits of the proposed methodology on the example of of the parameter estimation for the non-random two-liquid model for narrow azeotropic vapor-liquid equilibria.","As it turns out, our sequential optimal design method requires substantially fewer experiments than traditional factorial design to achieve the same model precision and prediction quality.","Consequently, our method can contribute to a substantially reduced experimental effort in vapor-liquid equilibrium modeling and beyond."],"url":"http://arxiv.org/abs/2403.09443v1","category":"math.OC"}
{"created":"2024-03-14 14:35:53","title":"LLM-based agents for automating the enhancement of user story quality: An early report","abstract":"In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous LLM-based Agent System and implemented it at the company. The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting.","sentences":["In agile software development, maintaining high-quality user stories is crucial, but also challenging.","This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams.","We developed a reference model for an Autonomous LLM-based Agent System and implemented it at the company.","The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams.","Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting."],"url":"http://arxiv.org/abs/2403.09442v1","category":"cs.SE"}
{"created":"2024-03-14 14:32:05","title":"On a problem posed by Bjorn Poonen","abstract":"Bjorn Poonen asked whether there exists a polynomial giving a surjection $\\mathbb{Z} \\times \\mathbb{Z} \\to \\mathbb{N}$. We answer this question in the negative, conditional on a conjecture of Vojta. More precisely, we show that if such a function exists, there is a family of open surfaces with dense integral points despite the surfaces being of log general type.","sentences":["Bjorn Poonen asked whether there exists a polynomial giving a surjection $\\mathbb{Z} \\times \\mathbb{Z} \\to \\mathbb{N}$.","We answer this question in the negative, conditional on a conjecture of Vojta.","More precisely, we show that if such a function exists, there is a family of open surfaces with dense integral points despite the surfaces being of log general type."],"url":"http://arxiv.org/abs/2403.09440v1","category":"math.NT"}
{"created":"2024-03-14 14:31:22","title":"3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation","abstract":"Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global 3D information of the current scene. Our extensive experiments demonstrate that, in comparison to previous methods, our approach supports wide variety of scene generation and arbitrary camera trajectories with improved visual quality and 3D consistency.","sentences":["Text-driven 3D scene generation techniques have made rapid progress in recent years.","Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes.","However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios).","To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene.","Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global 3D information of the current scene.","Our extensive experiments demonstrate that, in comparison to previous methods, our approach supports wide variety of scene generation and arbitrary camera trajectories with improved visual quality and 3D consistency."],"url":"http://arxiv.org/abs/2403.09439v1","category":"cs.CV"}
{"created":"2024-03-14 14:31:03","title":"On some extensions of shape-constrained generalized additive modelling in R","abstract":"Regression models that incorporate smooth functions of predictor variables to explain the relationships with a response variable have gained widespread usage and proved successful in various applications. By incorporating smooth functions of predictor variables, these models can capture complex relationships between the response and predictors while still allowing for interpretation of the results. In situations where the relationships between a response variable and predictors are explored, it is not uncommon to assume that these relationships adhere to certain shape constraints. Examples of such constraints include monotonicity and convexity. The scam package for R has become a popular package to carry out the full fitting of exponential family generalized additive modelling with shape restrictions on smooths. The paper aims to extend the existing framework of shape-constrained generalized additive models (SCAM) to accommodate smooth interactions of covariates, linear functionals of shape-constrained smooths and incorporation of residual autocorrelation. The methods described in this paper are implemented in the recent version of the package scam, available on the Comprehensive R Archive Network (CRAN).","sentences":["Regression models that incorporate smooth functions of predictor variables to explain the relationships with a response variable have gained widespread usage and proved successful in various applications.","By incorporating smooth functions of predictor variables, these models can capture complex relationships between the response and predictors while still allowing for interpretation of the results.","In situations where the relationships between a response variable and predictors are explored, it is not uncommon to assume that these relationships adhere to certain shape constraints.","Examples of such constraints include monotonicity and convexity.","The scam package for R has become a popular package to carry out the full fitting of exponential family generalized additive modelling with shape restrictions on smooths.","The paper aims to extend the existing framework of shape-constrained generalized additive models (SCAM) to accommodate smooth interactions of covariates, linear functionals of shape-constrained smooths and incorporation of residual autocorrelation.","The methods described in this paper are implemented in the recent version of the package scam, available on the Comprehensive R Archive Network (CRAN)."],"url":"http://arxiv.org/abs/2403.09438v1","category":"stat.CO"}
{"created":"2024-03-14 14:30:12","title":"Emergent time scales of epistasis in protein evolution","abstract":"We introduce a data-driven epistatic model of protein evolution, capable of generating evolutionary trajectories spanning very different time scales reaching from individual mutations to diverged homologs. Our in silico evolution encompasses random nucleotide mutations, insertions and deletions, and models selection using a fitness landscape, which is inferred via a generative probabilistic model for protein families. We show that the proposed framework accurately reproduces the sequence statistics of both short-time (experimental) and long-time (natural) protein evolution, suggesting applicability also to relatively data-poor intermediate evolutionary time scales, which are currently inaccessible to evolution experiments. Our model uncovers a highly collective nature of epistasis, gradually changing the fitness effect of mutations in a diverging sequence context, rather than acting via strong interactions between individual mutations. This collective nature triggers the emergence of a long evolutionary time scale, separating fast mutational processes inside a given sequence context, from the slow evolution of the context itself. The model quantitatively reproduces the extent of contingency and entrenchment, as well as the loss of predictability in protein evolution observed in deep mutational scanning experiments of distant homologs. It thereby deepens our understanding of the interplay between mutation and selection in shaping protein diversity and novel functions, allows to statistically forecast evolution, and challenges the prevailing independent-site models of protein evolution, which are unable to capture the fundamental importance of epistasis.","sentences":["We introduce a data-driven epistatic model of protein evolution, capable of generating evolutionary trajectories spanning very different time scales reaching from individual mutations to diverged homologs.","Our in silico evolution encompasses random nucleotide mutations, insertions and deletions, and models selection using a fitness landscape, which is inferred via a generative probabilistic model for protein families.","We show that the proposed framework accurately reproduces the sequence statistics of both short-time (experimental) and long-time (natural) protein evolution, suggesting applicability also to relatively data-poor intermediate evolutionary time scales, which are currently inaccessible to evolution experiments.","Our model uncovers a highly collective nature of epistasis, gradually changing the fitness effect of mutations in a diverging sequence context, rather than acting via strong interactions between individual mutations.","This collective nature triggers the emergence of a long evolutionary time scale, separating fast mutational processes inside a given sequence context, from the slow evolution of the context itself.","The model quantitatively reproduces the extent of contingency and entrenchment, as well as the loss of predictability in protein evolution observed in deep mutational scanning experiments of distant homologs.","It thereby deepens our understanding of the interplay between mutation and selection in shaping protein diversity and novel functions, allows to statistically forecast evolution, and challenges the prevailing independent-site models of protein evolution, which are unable to capture the fundamental importance of epistasis."],"url":"http://arxiv.org/abs/2403.09436v1","category":"q-bio.BM"}
{"created":"2024-03-14 14:29:01","title":"StarMalloc: A Formally Verified, Concurrent, Performant, and Security-Oriented Memory Allocator","abstract":"In this work, we present StarMalloc, a verified, security-oriented, concurrent memory allocator that can be used as a drop-in replacement in real-world projects. Using the Steel separation logic framework, we show how to specify and verify StarMalloc, relying on dependent types and modular abstractions to enable efficient verification. As part of StarMalloc, we also develop several generic datastructures and proof libraries directly reusable in future systems verification projects. We finally show that StarMalloc can be used with real-world projects, including the Firefox browser, and evaluate it against 10 state-of-the-art memory allocators, demonstrating its competitiveness.","sentences":["In this work, we present StarMalloc, a verified, security-oriented, concurrent memory allocator that can be used as a drop-in replacement in real-world projects.","Using the Steel separation logic framework, we show how to specify and verify StarMalloc, relying on dependent types and modular abstractions to enable efficient verification.","As part of StarMalloc, we also develop several generic datastructures and proof libraries directly reusable in future systems verification projects.","We finally show that StarMalloc can be used with real-world projects, including the Firefox browser, and evaluate it against 10 state-of-the-art memory allocators, demonstrating its competitiveness."],"url":"http://arxiv.org/abs/2403.09435v1","category":"cs.PL"}
{"created":"2024-03-14 14:25:10","title":"Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization","abstract":"Classical object detectors are incapable of detecting novel class objects that are not encountered before. Regarding this issue, Open-Vocabulary Object Detection (OVOD) is proposed, which aims to detect the objects in the candidate class list. However, current OVOD models are suffering from overfitting on the base classes, heavily relying on the large-scale extra data, and complex training process. To overcome these issues, we propose a novel framework with Meta prompt and Instance Contrastive learning (MIC) schemes. Firstly, we simulate a novel-class-emerging scenario to help the prompt learner that learns class and background prompts generalize to novel classes. Secondly, we design an instance-level contrastive strategy to promote intra-class compactness and inter-class separation, which benefits generalization of the detector to novel class objects. Without using knowledge distillation, ensemble model or extra training data during detector training, our proposed MIC outperforms previous SOTA methods trained with these complex techniques on LVIS. Most importantly, MIC shows great generalization ability on novel classes, e.g., with $+4.3\\%$ and $+1.9\\% \\ \\mathrm{AP}$ improvement compared with previous SOTA on COCO and Objects365, respectively.","sentences":["Classical object detectors are incapable of detecting novel class objects that are not encountered before.","Regarding this issue, Open-Vocabulary Object Detection (OVOD) is proposed, which aims to detect the objects in the candidate class list.","However, current OVOD models are suffering from overfitting on the base classes, heavily relying on the large-scale extra data, and complex training process.","To overcome these issues, we propose a novel framework with Meta prompt and Instance Contrastive learning (MIC) schemes.","Firstly, we simulate a novel-class-emerging scenario to help the prompt learner that learns class and background prompts generalize to novel classes.","Secondly, we design an instance-level contrastive strategy to promote intra-class compactness and inter-class separation, which benefits generalization of the detector to novel class objects.","Without using knowledge distillation, ensemble model or extra training data during detector training, our proposed MIC outperforms previous SOTA methods trained with these complex techniques on LVIS.","Most importantly, MIC shows great generalization ability on novel classes, e.g., with $+4.3\\%$ and $+1.9\\% \\ \\mathrm{AP}$ improvement compared with previous SOTA on COCO and Objects365, respectively."],"url":"http://arxiv.org/abs/2403.09433v1","category":"cs.CV"}
{"created":"2024-03-14 14:25:10","title":"Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians","abstract":"Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. This includes future prediction and simulation under varying initial states and environmental parameters. Project page: https://zlicheng.com/spring_gaus.","sentences":["Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics.","Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects.","We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos.","Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance.","This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles.","We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects.","This includes future prediction and simulation under varying initial states and environmental parameters.","Project page: https://zlicheng.com/spring_gaus."],"url":"http://arxiv.org/abs/2403.09434v1","category":"cs.CV"}
{"created":"2024-03-14 14:14:47","title":"Mitigating attribute amplification in counterfactual image generation","abstract":"Causal generative modelling is gaining interest in medical imaging due to its ability to answer interventional and counterfactual queries. Most work focuses on generating counterfactual images that look plausible, using auxiliary classifiers to enforce effectiveness of simulated interventions. We investigate pitfalls in this approach, discovering the issue of attribute amplification, where unrelated attributes are spuriously affected during interventions, leading to biases across protected characteristics and disease status. We show that attribute amplification is caused by the use of hard labels in the counterfactual training process and propose soft counterfactual fine-tuning to mitigate this issue. Our method substantially reduces the amplification effect while maintaining effectiveness of generated images, demonstrated on a large chest X-ray dataset. Our work makes an important advancement towards more faithful and unbiased causal modelling in medical imaging.","sentences":["Causal generative modelling is gaining interest in medical imaging due to its ability to answer interventional and counterfactual queries.","Most work focuses on generating counterfactual images that look plausible, using auxiliary classifiers to enforce effectiveness of simulated interventions.","We investigate pitfalls in this approach, discovering the issue of attribute amplification, where unrelated attributes are spuriously affected during interventions, leading to biases across protected characteristics and disease status.","We show that attribute amplification is caused by the use of hard labels in the counterfactual training process and propose soft counterfactual fine-tuning to mitigate this issue.","Our method substantially reduces the amplification effect while maintaining effectiveness of generated images, demonstrated on a large chest X-ray dataset.","Our work makes an important advancement towards more faithful and unbiased causal modelling in medical imaging."],"url":"http://arxiv.org/abs/2403.09422v1","category":"cs.CV"}
{"created":"2024-03-14 14:14:43","title":"Fake turbulence","abstract":"High-dimensional dynamical systems projected onto a reduced-order model cease to be deterministic and are best described by probability distributions in state space. Their equations of motion map onto an evolution operator with a deterministic component describing the projected dynamics, and a stochastic one from the neglected dimensions. It is shown that, for projections in which the deterministic component is dominant, `physics-free' stochastic Markovian models can be constructed that mimic many of the statistics of the real flow, even for fairly crude operator approximations. Deterministic models converge to steady states. This is related to general properties of Markov chains and illustrated with data-driven models for a moderate-Reynolds number turbulent channel.","sentences":["High-dimensional dynamical systems projected onto a reduced-order model cease to be deterministic and are best described by probability distributions in state space.","Their equations of motion map onto an evolution operator with a deterministic component describing the projected dynamics, and a stochastic one from the neglected dimensions.","It is shown that, for projections in which the deterministic component is dominant, `physics-free' stochastic Markovian models can be constructed that mimic many of the statistics of the real flow, even for fairly crude operator approximations.","Deterministic models converge to steady states.","This is related to general properties of Markov chains and illustrated with data-driven models for a moderate-Reynolds number turbulent channel."],"url":"http://arxiv.org/abs/2403.09421v1","category":"physics.flu-dyn"}
{"created":"2024-03-14 14:07:31","title":"GPT on a Quantum Computer","abstract":"Large Language Models (LLMs) such as ChatGPT have transformed how we interact with and understand the capabilities of Artificial Intelligence (AI). However, the intersection of LLMs with the burgeoning field of Quantum Machine Learning (QML) is only in its nascent stages. This paper presents an exploration of this niche by detailing a comprehensive framework for implementing the foundational Transformer architecture -- integral to ChatGPT -- within a quantum computing paradigm. We meticulously design quantum circuits that implement adapted versions of the transformer's core components and the generative pre-training phase. By integrating quantum computing with LLMs, we aspire to open new avenues for research in QML and contribute to the ongoing evolution of AI technologies.","sentences":["Large Language Models (LLMs) such as ChatGPT have transformed how we interact with and understand the capabilities of Artificial Intelligence (AI).","However, the intersection of LLMs with the burgeoning field of Quantum Machine Learning (QML) is only in its nascent stages.","This paper presents an exploration of this niche by detailing a comprehensive framework for implementing the foundational Transformer architecture -- integral to ChatGPT -- within a quantum computing paradigm.","We meticulously design quantum circuits that implement adapted versions of the transformer's core components and the generative pre-training phase.","By integrating quantum computing with LLMs, we aspire to open new avenues for research in QML and contribute to the ongoing evolution of AI technologies."],"url":"http://arxiv.org/abs/2403.09418v1","category":"quant-ph"}
{"created":"2024-03-14 14:04:44","title":"Scalability of Metropolis-within-Gibbs schemes for high-dimensional Bayesian models","abstract":"We study general coordinate-wise MCMC schemes (such as Metropolis-within-Gibbs samplers), which are commonly used to fit Bayesian non-conjugate hierarchical models. We relate their convergence properties to the ones of the corresponding (potentially not implementable) Gibbs sampler through the notion of conditional conductance. This allows us to study the performances of popular Metropolis-within-Gibbs schemes for non-conjugate hierarchical models, in high-dimensional regimes where both number of datapoints and parameters increase. Given random data-generating assumptions, we establish dimension-free convergence results, which are in close accordance with numerical evidences. Applications to Bayesian models for binary regression with unknown hyperparameters and discretely observed diffusions are also discussed. Motivated by such statistical applications, auxiliary results of independent interest on approximate conductances and perturbation of Markov operators are provided.","sentences":["We study general coordinate-wise MCMC schemes (such as Metropolis-within-Gibbs samplers), which are commonly used to fit Bayesian non-conjugate hierarchical models.","We relate their convergence properties to the ones of the corresponding (potentially not implementable)","Gibbs sampler through the notion of conditional conductance.","This allows us to study the performances of popular Metropolis-within-Gibbs schemes for non-conjugate hierarchical models, in high-dimensional regimes where both number of datapoints and parameters increase.","Given random data-generating assumptions, we establish dimension-free convergence results, which are in close accordance with numerical evidences.","Applications to Bayesian models for binary regression with unknown hyperparameters and discretely observed diffusions are also discussed.","Motivated by such statistical applications, auxiliary results of independent interest on approximate conductances and perturbation of Markov operators are provided."],"url":"http://arxiv.org/abs/2403.09416v1","category":"stat.CO"}
{"created":"2024-03-14 14:04:29","title":"Region-based U-net for accelerated training and enhanced precision in deep brain segmentation","abstract":"Segmentation of brain structures on MRI is the primary step for further quantitative analysis of brain diseases. Manual segmentation is still considered the gold standard in terms of accuracy; however, such data is extremely time-consuming to generate. This paper presents a deep learning-based segmentation approach for 12 deep-brain structures, utilizing multiple region-based U-Nets. The brain is divided into three focal regions of interest that encompass the brainstem, the ventricular system, and the striatum. Next, three region-based U-nets are run in parallel to parcellate these larger structures into their respective four substructures. This approach not only greatly reduces the training and processing times but also significantly enhances the segmentation accuracy, compared to segmenting the entire MRI image at once. Our approach achieves remarkable accuracy with an average Dice Similarity Coefficient (DSC) of 0.901 and 95% Hausdorff Distance (HD95) of 1.155 mm. The method was compared with state-of-the-art segmentation approaches, demonstrating a high level of accuracy and robustness of the proposed method.","sentences":["Segmentation of brain structures on MRI is the primary step for further quantitative analysis of brain diseases.","Manual segmentation is still considered the gold standard in terms of accuracy; however, such data is extremely time-consuming to generate.","This paper presents a deep learning-based segmentation approach for 12 deep-brain structures, utilizing multiple region-based U-Nets.","The brain is divided into three focal regions of interest that encompass the brainstem, the ventricular system, and the striatum.","Next, three region-based U-nets are run in parallel to parcellate these larger structures into their respective four substructures.","This approach not only greatly reduces the training and processing times but also significantly enhances the segmentation accuracy, compared to segmenting the entire MRI image at once.","Our approach achieves remarkable accuracy with an average Dice Similarity Coefficient (DSC) of 0.901 and 95% Hausdorff Distance (HD95) of 1.155 mm.","The method was compared with state-of-the-art segmentation approaches, demonstrating a high level of accuracy and robustness of the proposed method."],"url":"http://arxiv.org/abs/2403.09414v1","category":"eess.IV"}
{"created":"2024-03-14 14:03:29","title":"OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments","abstract":"Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including multimodal retrieval and open-set classes. However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities. Moreover, the absence of topological relationships further complicates the accurate querying of specific instances. In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical graph structure designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images using 2D foundation models, encoding the captions with features to enhance textual reasoning. Subsequently, 3D incremental panoramic mapping with feature embedding is achieved by projecting images onto LiDAR point clouds. Finally, the environment is segmented based on lane graph connectivity to construct a hierarchical graph. Validation results from real public dataset SemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph exhibits the ability to generalize to novel semantic classes and achieve the highest segmentation and query accuracy. The source code of OpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph.","sentences":["Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks.","Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including multimodal retrieval and open-set classes.","However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities.","Moreover, the absence of topological relationships further complicates the accurate querying of specific instances.","In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical graph structure designed for large-scale outdoor environments.","OpenGraph initially extracts instances and their captions from visual images using 2D foundation models, encoding the captions with features to enhance textual reasoning.","Subsequently, 3D incremental panoramic mapping with feature embedding is achieved by projecting images onto LiDAR point clouds.","Finally, the environment is segmented based on lane graph connectivity to construct a hierarchical graph.","Validation results from real public dataset SemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph exhibits the ability to generalize to novel semantic classes and achieve the highest segmentation and query accuracy.","The source code of OpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph."],"url":"http://arxiv.org/abs/2403.09412v1","category":"cs.CV"}
{"created":"2024-03-14 14:02:10","title":"Near-Field Channel Modeling for Holographic MIMO Communications","abstract":"Empowered by the latest progress on innovative metamaterials/metasurfaces and advanced antenna technologies, holographic multiple-input multiple-output (H-MIMO) emerges as a promising technology to fulfill the extreme goals of the sixth-generation (6G) wireless networks. The antenna arrays utilized in H-MIMO comprise massive (possibly to extreme extent) numbers of antenna elements, densely spaced less than half-a-wavelength and integrated into a compact space, realizing an almost continuous aperture. Thanks to the expected low cost, size, weight, and power consumption, such apertures are expected to be largely fabricated for near-field communications. In addition, the physical features of H-MIMO enable manipulations directly on the electromagnetic (EM) wave domain and spatial multiplexing. To fully leverage this potential, near-field H-MIMO channel modeling, especially from the EM perspective, is of paramount significance. In this article, we overview near-field H-MIMO channel models elaborating on the various modeling categories and respective features, as well as their challenges and evaluation criteria. We also present EM-domain channel models that address the inherit computational and measurement complexities. Finally, the article is concluded with a set of future research directions on the topic.","sentences":["Empowered by the latest progress on innovative metamaterials/metasurfaces and advanced antenna technologies, holographic multiple-input multiple-output (H-MIMO) emerges as a promising technology to fulfill the extreme goals of the sixth-generation (6G) wireless networks.","The antenna arrays utilized in H-MIMO comprise massive (possibly to extreme extent) numbers of antenna elements, densely spaced less than half-a-wavelength and integrated into a compact space, realizing an almost continuous aperture.","Thanks to the expected low cost, size, weight, and power consumption, such apertures are expected to be largely fabricated for near-field communications.","In addition, the physical features of H-MIMO enable manipulations directly on the electromagnetic (EM) wave domain and spatial multiplexing.","To fully leverage this potential, near-field H-MIMO channel modeling, especially from the EM perspective, is of paramount significance.","In this article, we overview near-field H-MIMO channel models elaborating on the various modeling categories and respective features, as well as their challenges and evaluation criteria.","We also present EM-domain channel models that address the inherit computational and measurement complexities.","Finally, the article is concluded with a set of future research directions on the topic."],"url":"http://arxiv.org/abs/2403.09411v1","category":"eess.SP"}
{"created":"2024-03-14 14:02:01","title":"XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization","abstract":"Utilizing potent representations of the large vision-language models (VLMs) to accomplish various downstream tasks has attracted increasing attention. Within this research field, soft prompt learning has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification. However, most existing prompt learning methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare. To address this issue, we propose a novel explainable prompt learning framework that leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities. Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from large language models and offers both visual and textual explanations for the prompts. Extensive experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of foundation models in facilitating XAI. The code will be made publically available.","sentences":["Utilizing potent representations of the large vision-language models (VLMs) to accomplish various downstream tasks has attracted increasing attention.","Within this research field, soft prompt learning has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification.","However, most existing prompt learning methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare.","To address this issue, we propose a novel explainable prompt learning framework that leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities.","Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from large language models and offers both visual and textual explanations for the prompts.","Extensive experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of foundation models in facilitating XAI.","The code will be made publically available."],"url":"http://arxiv.org/abs/2403.09410v1","category":"cs.CV"}
{"created":"2024-03-14 14:01:26","title":"\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS Students using Large Language Models","abstract":"Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts. We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with LLMs. Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant.","sentences":["Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings.","To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding.","However, creating effective educational analogies is difficult even for experienced instructors.","We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand.","Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students.","They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts.","We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with LLMs.","Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant."],"url":"http://arxiv.org/abs/2403.09409v1","category":"cs.HC"}
{"created":"2024-03-14 13:59:04","title":"LM2D: Lyrics- and Music-Driven Dance Synthesis","abstract":"Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, including dancers and choreographers. The results demonstrate LM2D is able to produce realistic and diverse dance matching both lyrics and music. A video summary can be accessed at: https://youtu.be/4XCgvYookvA.","sentences":["Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content.","The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings.","However, existing dance synthesis methods tend to model motions only conditioned on audio signals.","In this work, we make two contributions to bridge this gap.","First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step.","Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies.","We evaluate our model against music-only baseline models with objective metrics and human evaluations, including dancers and choreographers.","The results demonstrate LM2D is able to produce realistic and diverse dance matching both lyrics and music.","A video summary can be accessed at: https://youtu.be/4XCgvYookvA."],"url":"http://arxiv.org/abs/2403.09407v1","category":"cs.SD"}
{"created":"2024-03-14 13:55:34","title":"Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration","abstract":"Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and emotion expression, that affect the extent to which they are treated as subjects of moral consideration. However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems. We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features. All 11 features increased how morally wrong participants considered it to harm the AIs. The largest effects were from human-like physical bodies and prosociality (i.e., emotion expression, emotion recognition, cooperation, and moral judgment). For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions.","sentences":["Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and emotion expression, that affect the extent to which they are treated as subjects of moral consideration.","However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems.","We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features.","All 11 features increased how morally wrong participants considered it to harm the AIs.","The largest effects were from human-like physical bodies and prosociality (i.e., emotion expression, emotion recognition, cooperation, and moral judgment).","For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions."],"url":"http://arxiv.org/abs/2403.09405v1","category":"cs.HC"}
{"created":"2024-03-14 13:53:05","title":"Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption","abstract":"We propose a novel program of heuristic reasoning within artificial intelligence (AI) systems. Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics). We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally. We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory.","sentences":["We propose a novel program of heuristic reasoning within artificial intelligence (AI) systems.","Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics).","We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally.","We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory."],"url":"http://arxiv.org/abs/2403.09404v1","category":"cs.AI"}
{"created":"2024-03-14 13:52:03","title":"Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning","abstract":"Identifying highlight moments of raw video materials is crucial for improving the efficiency of editing videos that are pervasive on internet platforms. However, the extensive work of manually labeling footage has created obstacles to applying supervised methods to videos of unseen categories. The absence of an audio modality that contains valuable cues for highlight detection in many videos also makes it difficult to use multimodal strategies. In this paper, we propose a novel model with cross-modal perception for unsupervised highlight detection. The proposed model learns representations with visual-audio level semantics from image-audio pair data via a self-reconstruction task. To achieve unsupervised highlight detection, we investigate the latent representations of the network and propose the representation activation sequence learning (RASL) module with k-point contrastive learning to learn significant representation activations. To connect the visual modality with the audio modality, we use the symmetric contrastive learning (SCL) module to learn the paired visual and audio representations. Furthermore, an auxiliary task of masked feature vector sequence (FVS) reconstruction is simultaneously conducted during pretraining for representation enhancement. During inference, the cross-modal pretrained model can generate representations with paired visual-audio semantics given only the visual modality. The RASL module is used to output the highlight scores. The experimental results show that the proposed framework achieves superior performance compared to other state-of-the-art approaches.","sentences":["Identifying highlight moments of raw video materials is crucial for improving the efficiency of editing videos that are pervasive on internet platforms.","However, the extensive work of manually labeling footage has created obstacles to applying supervised methods to videos of unseen categories.","The absence of an audio modality that contains valuable cues for highlight detection in many videos also makes it difficult to use multimodal strategies.","In this paper, we propose a novel model with cross-modal perception for unsupervised highlight detection.","The proposed model learns representations with visual-audio level semantics from image-audio pair data via a self-reconstruction task.","To achieve unsupervised highlight detection, we investigate the latent representations of the network and propose the representation activation sequence learning (RASL) module with k-point contrastive learning to learn significant representation activations.","To connect the visual modality with the audio modality, we use the symmetric contrastive learning (SCL) module to learn the paired visual and audio representations.","Furthermore, an auxiliary task of masked feature vector sequence (FVS) reconstruction is simultaneously conducted during pretraining for representation enhancement.","During inference, the cross-modal pretrained model can generate representations with paired visual-audio semantics given only the visual modality.","The RASL module is used to output the highlight scores.","The experimental results show that the proposed framework achieves superior performance compared to other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.09401v1","category":"cs.CV"}
{"created":"2024-03-14 13:50:44","title":"ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization","abstract":"Medical data often exhibits distribution shifts, which cause test-time performance degradation for deep learning models trained using standard supervised learning pipelines. This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data. Existing disentanglement-based SDG methods heavily rely on structural information embedded in segmentation masks, however classification labels do not provide such dense information. This work introduces a novel SDG method aimed at medical image classification that leverages channel-wise contrastive disentanglement. It is further enhanced with reconstruction-based style regularization to ensure extraction of distinct style and structure feature representations. We evaluate our method on the complex task of multicenter histopathology image classification, comparing it against state-of-the-art (SOTA) SDG baselines. Results demonstrate that our method surpasses the SOTA by a margin of 1% in average accuracy while also showing more stable performance. This study highlights the importance and challenges of exploring SDG frameworks in the context of the classification task. The code is publicly available at https://github.com/BioMedIA-MBZUAI/ConDiSR","sentences":["Medical data often exhibits distribution shifts, which cause test-time performance degradation for deep learning models trained using standard supervised learning pipelines.","This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data.","Existing disentanglement-based SDG methods heavily rely on structural information embedded in segmentation masks, however classification labels do not provide such dense information.","This work introduces a novel SDG method aimed at medical image classification that leverages channel-wise contrastive disentanglement.","It is further enhanced with reconstruction-based style regularization to ensure extraction of distinct style and structure feature representations.","We evaluate our method on the complex task of multicenter histopathology image classification, comparing it against state-of-the-art (SOTA) SDG baselines.","Results demonstrate that our method surpasses the SOTA by a margin of 1% in average accuracy while also showing more stable performance.","This study highlights the importance and challenges of exploring SDG frameworks in the context of the classification task.","The code is publicly available at https://github.com/BioMedIA-MBZUAI/ConDiSR"],"url":"http://arxiv.org/abs/2403.09400v1","category":"cs.CV"}
{"created":"2024-03-14 13:45:09","title":"Event-based Asynchronous HDR Imaging by Temporal Incident Light Modulation","abstract":"Dynamic Range (DR) is a pivotal characteristic of imaging systems. Current frame-based cameras struggle to achieve high dynamic range imaging due to the conflict between globally uniform exposure and spatially variant scene illumination. In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR imaging system, based on key insights into the challenges in HDR imaging and the unique event-generating mechanism of Dynamic Vision Sensors (DVS). Our proposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD panels modulate the irradiance incident upon the DVS by altering their transparency, thereby triggering the pixel-independent event streams. The HDR image is subsequently decoded from the event streams through our temporal-weighted algorithm. Experiments under standard test platform and several challenging scenes have verified the feasibility of the system in HDR imaging task.","sentences":["Dynamic Range (DR) is a pivotal characteristic of imaging systems.","Current frame-based cameras struggle to achieve high dynamic range imaging due to the conflict between globally uniform exposure and spatially variant scene illumination.","In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR imaging system, based on key insights into the challenges in HDR imaging and the unique event-generating mechanism of Dynamic Vision Sensors (DVS).","Our proposed AsynHDR system integrates the DVS with a set of LCD panels.","The LCD panels modulate the irradiance incident upon the DVS by altering their transparency, thereby triggering the pixel-independent event streams.","The HDR image is subsequently decoded from the event streams through our temporal-weighted algorithm.","Experiments under standard test platform and several challenging scenes have verified the feasibility of the system in HDR imaging task."],"url":"http://arxiv.org/abs/2403.09392v1","category":"eess.IV"}
{"created":"2024-03-14 13:45:01","title":"A hollow-core fiber based stand-alone multimodal (2-photon, 3-photon, SHG, THG) nonlinear flexible imaging endoscope","abstract":"Multimodal nonlinear endoscopes have been a topic of intense research over the past two decades, enabling sub-cellular and label-free imaging in areas not reachable with table-top microscopes. They are sophisticated systems that can be implemented on an optical table in a lab environment, but they cannot be easily moved within or out of the lab. We present here a multimodal and flexible nonlinear endoscope system able to perform two photon excited fluorescence and second harmonic generation imaging with a stand-alone and moveable kart integrating a compact ultrashort laser source. In addition, the system can perform three photon excited fluorescence and third harmonic generation thanks to a delivery optical fiber used to deliver ultrashort pulses from massive and not movable laser systems into the stand-alone kart. The endoscopic fiber probes and delivery optical fibers are based on functionalized negative curvature hollow core fibers. The endoscope distal head has a diameter <2.2mm and can perform nonlinear imaging at max 10 frames/s over a field of view up to 600 $\\mu$m with a ~1 $\\mu$m spatial resolution.","sentences":["Multimodal nonlinear endoscopes have been a topic of intense research over the past two decades, enabling sub-cellular and label-free imaging in areas not reachable with table-top microscopes.","They are sophisticated systems that can be implemented on an optical table in a lab environment, but they cannot be easily moved within or out of the lab.","We present here a multimodal and flexible nonlinear endoscope system able to perform two photon excited fluorescence and second harmonic generation imaging with a stand-alone and moveable kart integrating a compact ultrashort laser source.","In addition, the system can perform three photon excited fluorescence and third harmonic generation thanks to a delivery optical fiber used to deliver ultrashort pulses from massive and not movable laser systems into the stand-alone kart.","The endoscopic fiber probes and delivery optical fibers are based on functionalized negative curvature hollow core fibers.","The endoscope distal head has a diameter <2.2mm and can perform nonlinear imaging at max 10 frames/s over a field of view up to 600 $\\mu$m with a ~1 $\\mu$m spatial resolution."],"url":"http://arxiv.org/abs/2403.09391v1","category":"physics.optics"}
{"created":"2024-03-14 13:39:18","title":"Imaging a semi-classical horizonless compact object with strong redshift","abstract":"The recent advancements in black hole imaging have opened a new era of probing horizon-scale physics with electromagnetic radiation. However, a feature of the observed images, a bright ring encircling a relatively dark region, has not sufficiently proved the existence of event horizons. It thus requires extreme care when studying the possibility of using such image features to examine quantum effects that may change the classical picture of black holes slightly or drastically. In this work, we investigate the image of a horizonless compact object, whose interior metric satisfies the 4D semi-classical Einstein equation non-perturbatively for the Planck constant, and whose entropy agrees with the Bekenstein-Hawking formula. Although the absence of an event horizon allows light rays to pass through the dense interior, the extremely strong redshift significantly darkens the image, making it almost identical to the classical black-hole image. In particular, if there is light emission a bit inside the surface of the object, the intensity around the inner shadow is slightly enhanced, which could be a future observable prediction to characterize the object. We also find through a phenomenological parameter that the image is further darkened due to interactions inside. Thus, the image is consistent with current observations, and the object could be a candidate for black holes in quantum theory.","sentences":["The recent advancements in black hole imaging have opened a new era of probing horizon-scale physics with electromagnetic radiation.","However, a feature of the observed images, a bright ring encircling a relatively dark region, has not sufficiently proved the existence of event horizons.","It thus requires extreme care when studying the possibility of using such image features to examine quantum effects that may change the classical picture of black holes slightly or drastically.","In this work, we investigate the image of a horizonless compact object, whose interior metric satisfies the 4D semi-classical Einstein equation non-perturbatively for the Planck constant, and whose entropy agrees with the Bekenstein-Hawking formula.","Although the absence of an event horizon allows light rays to pass through the dense interior, the extremely strong redshift significantly darkens the image, making it almost identical to the classical black-hole image.","In particular, if there is light emission a bit inside the surface of the object, the intensity around the inner shadow is slightly enhanced, which could be a future observable prediction to characterize the object.","We also find through a phenomenological parameter that the image is further darkened due to interactions inside.","Thus, the image is consistent with current observations, and the object could be a candidate for black holes in quantum theory."],"url":"http://arxiv.org/abs/2403.09388v1","category":"gr-qc"}
{"created":"2024-03-14 13:33:00","title":"Optimized generation of entanglement based on the f-STIRAP technique","abstract":"We consider generating maximally entangled states (Bell states) between two qubits coupled to a common bosonic mode, based on f-STIRAP. Utilizing the systematic approach developed in New J. Phys. 19 093016 (2017), we quantify the effects of non-adiabatic leakage and system dissipation on the entanglement generation, and optimize the entanglement by balancing non-adiabatic leakage and system dissipation. We find the analytical expressions of the optimal coupling profile, the operation time, and the maximal entanglement. Our findings have broad applications in quantum state engineering, especially in solid-state devices where dissipative effects cannot be neglected.","sentences":["We consider generating maximally entangled states (Bell states) between two qubits coupled to a common bosonic mode, based on f-STIRAP.","Utilizing the systematic approach developed in New J. Phys. 19 093016 (2017), we quantify the effects of non-adiabatic leakage and system dissipation on the entanglement generation, and optimize the entanglement by balancing non-adiabatic leakage and system dissipation.","We find the analytical expressions of the optimal coupling profile, the operation time, and the maximal entanglement.","Our findings have broad applications in quantum state engineering, especially in solid-state devices where dissipative effects cannot be neglected."],"url":"http://arxiv.org/abs/2403.09381v1","category":"quant-ph"}
{"created":"2024-03-14 13:29:23","title":"The Fekete problem in segmental polynomial interpolation","abstract":"In this article, we study the Fekete problem in segmental and combined nodal-segmental univariate polynomial interpolation by investigating sets of segments, or segments combined with nodes, such that the Vandermonde determinant for the respective polynomial interpolation problem is maximized. For particular families of segments, we will be able to find explicit solutions of the corresponding maximization problem. The quality of the Fekete segments depends hereby strongly on the utilized normalization of the segmental information in the Vandermonde matrix. To measure the quality of the Fekete segments in interpolation, we analyse the asymptotic behaviour of the generalized Lebesgue constant linked to the interpolation problem. For particular sets of Fekete segments we will get, similar to the nodal case, a favourable logarithmic growth of this constant.","sentences":["In this article, we study the Fekete problem in segmental and combined nodal-segmental univariate polynomial interpolation by investigating sets of segments, or segments combined with nodes, such that the Vandermonde determinant for the respective polynomial interpolation problem is maximized.","For particular families of segments, we will be able to find explicit solutions of the corresponding maximization problem.","The quality of the Fekete segments depends hereby strongly on the utilized normalization of the segmental information in the Vandermonde matrix.","To measure the quality of the Fekete segments in interpolation, we analyse the asymptotic behaviour of the generalized Lebesgue constant linked to the interpolation problem.","For particular sets of Fekete segments we will get, similar to the nodal case, a favourable logarithmic growth of this constant."],"url":"http://arxiv.org/abs/2403.09378v1","category":"math.NA"}
{"created":"2024-03-14 13:29:23","title":"Principal 2-bundles and quotient 2-stacks","abstract":"We generalize principal bundles and quotient stacks to the two-categorical context of bisites. We introduce a notion of principal 2-bundle that makes sense for a 2-category with finite flexible limits, endowed with a bitopology. We then use principal 2-bundles to explicitly construct quotient-pre-2-stacks, which are the analogues of quotient stacks one dimension higher. In order to perform this construction, we prove that principal 2-bundles are closed under iso-comma objects and we restrict ourselves to (2,1)-categories. Finally, we prove that, if the bisite is subcanonical and the underlying (2,1)-category satisfies some mild conditions, quotient pre-2-stacks are 2-stacks.","sentences":["We generalize principal bundles and quotient stacks to the two-categorical context of bisites.","We introduce a notion of principal 2-bundle that makes sense for a 2-category with finite flexible limits, endowed with a bitopology.","We then use principal 2-bundles to explicitly construct quotient-pre-2-stacks, which are the analogues of quotient stacks one dimension higher.","In order to perform this construction, we prove that principal 2-bundles are closed under iso-comma objects and we restrict ourselves to (2,1)-categories.","Finally, we prove that, if the bisite is subcanonical and the underlying (2,1)-category satisfies some mild conditions, quotient pre-2-stacks are 2-stacks."],"url":"http://arxiv.org/abs/2403.09379v1","category":"math.CT"}
{"created":"2024-03-14 13:20:35","title":"Gravitational Waves in Chern-Simons-Gauss-Bonnet Gravity","abstract":"It is known that the four-dimensional effective field theory arising from heterotic string theory is general relativity with both a Chern-Simons and Gauss-Bonnet term. We study the propagation of gravitational waves in this combination of Chern-Simons and Gauss-Bonnet gravity, both of which have an associated scalar field, the axion and the dilaton respectively, that are kinetically coupled. We review how the combination of dynamical Chern-Simons and Gauss-Bonnet gravities can arise from string theory as corrections to general relativity and show how the gravitational wave waveform is modified in such a theory. We compare our results to a novel framework recently introduced for parametrizing the parity-violating sector (Chern-Simons), and use that to guide our construction of a similar parametrization for the parity-conserving (Gauss-Bonnet) sector. In general, we find that the contributions from the parity-violating and parity-conserving sectors are similar. Moreover, the kinetic coupling between the axion and dilaton introduces an extra contribution to the parity-violating sector of the gravitational waves. Using our parametrization, we are able to comment on initial constraints for the theory parameters, including the time variations of the axion and dilaton.","sentences":["It is known that the four-dimensional effective field theory arising from heterotic string theory is general relativity with both a Chern-Simons and Gauss-Bonnet term.","We study the propagation of gravitational waves in this combination of Chern-Simons and Gauss-Bonnet gravity, both of which have an associated scalar field, the axion and the dilaton respectively, that are kinetically coupled.","We review how the combination of dynamical Chern-Simons and Gauss-Bonnet gravities can arise from string theory as corrections to general relativity and show how the gravitational wave waveform is modified in such a theory.","We compare our results to a novel framework recently introduced for parametrizing the parity-violating sector (Chern-Simons), and use that to guide our construction of a similar parametrization for the parity-conserving (Gauss-Bonnet) sector.","In general, we find that the contributions from the parity-violating and parity-conserving sectors are similar.","Moreover, the kinetic coupling between the axion and dilaton introduces an extra contribution to the parity-violating sector of the gravitational waves.","Using our parametrization, we are able to comment on initial constraints for the theory parameters, including the time variations of the axion and dilaton."],"url":"http://arxiv.org/abs/2403.09373v1","category":"gr-qc"}
{"created":"2024-03-14 13:17:37","title":"Noise-aware neural network for stochastic dynamics simulation","abstract":"In the presence of system-environment coupling, classical complex systems undergo stochastic dynamics, where rich phenomena can emerge at large spatio-temporal scales. To investigate these phenomena, numerical approaches for simulating stochastic dynamics are indispensable and can be computationally expensive. In light of the recent fast development in machine learning techniques, here, we establish a generic machine learning approach to simulate the stochastic dynamics, dubbed the noise-aware neural network (NANN). One key feature of this approach is its ability to generate the long-time stochastic dynamics of complex large-scale systems by just training NANN with the one-step dynamics of smaller-scale systems, thus reducing the computational cost. Furthermore, this NANN based approach is quite generic. Case-by-case special design of the architecture of NANN is not necessary when it is employed to investigate different stochastic complex systems. Using the noisy Kuramoto model and the Vicsek model as concrete examples, we demonstrate its capability in simulating stochastic dynamics. We believe that this novel machine learning approach can be a useful tool in investigating the large spatio-temporal scaling behavior of complex systems subjected to the influences of the environmental noise.","sentences":["In the presence of system-environment coupling, classical complex systems undergo stochastic dynamics, where rich phenomena can emerge at large spatio-temporal scales.","To investigate these phenomena, numerical approaches for simulating stochastic dynamics are indispensable and can be computationally expensive.","In light of the recent fast development in machine learning techniques, here, we establish a generic machine learning approach to simulate the stochastic dynamics, dubbed the noise-aware neural network (NANN).","One key feature of this approach is its ability to generate the long-time stochastic dynamics of complex large-scale systems by just training NANN with the one-step dynamics of smaller-scale systems, thus reducing the computational cost.","Furthermore, this NANN based approach is quite generic.","Case-by-case special design of the architecture of NANN is not necessary when it is employed to investigate different stochastic complex systems.","Using the noisy Kuramoto model and the Vicsek model as concrete examples, we demonstrate its capability in simulating stochastic dynamics.","We believe that this novel machine learning approach can be a useful tool in investigating the large spatio-temporal scaling behavior of complex systems subjected to the influences of the environmental noise."],"url":"http://arxiv.org/abs/2403.09370v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 13:16:47","title":"PreConfig: A Pretrained Model for Automating Network Configuration","abstract":"Manual network configuration automation (NCA) tools face significant challenges in versatility and flexibility due to their reliance on extensive domain expertise and manual design, limiting their adaptability to diverse scenarios and complex application needs. This paper introduces PreConfig, an innovative NCA tool that leverages a pretrained language model for automating network configuration tasks. PreConfig is designed to address the complexity and variety of NCA tasks by framing them as text-to-text transformation problems, thus unifying the tasks of configuration generation, translation, and analysis under a single, versatile model. Our approach overcomes existing tools' limitations by utilizing advances in natural language processing to automatically comprehend and generate network configurations without extensive manual re-engineering. We confront the challenges of integrating domain-specific knowledge into pretrained models and the scarcity of supervision data in the network configuration field. Our solution involves constructing a specialized corpus and further pretraining on network configuration data, coupled with a novel data mining technique for generating task supervision data. The proposed model demonstrates robustness in configuration generation, translation, and analysis, outperforming conventional tools in handling complex networking environments. The experimental results validate the effectiveness of PreConfig, establishing a new direction for automating network configuration tasks with pretrained language models.","sentences":["Manual network configuration automation (NCA) tools face significant challenges in versatility and flexibility due to their reliance on extensive domain expertise and manual design, limiting their adaptability to diverse scenarios and complex application needs.","This paper introduces PreConfig, an innovative NCA tool that leverages a pretrained language model for automating network configuration tasks.","PreConfig is designed to address the complexity and variety of NCA tasks by framing them as text-to-text transformation problems, thus unifying the tasks of configuration generation, translation, and analysis under a single, versatile model.","Our approach overcomes existing tools' limitations by utilizing advances in natural language processing to automatically comprehend and generate network configurations without extensive manual re-engineering.","We confront the challenges of integrating domain-specific knowledge into pretrained models and the scarcity of supervision data in the network configuration field.","Our solution involves constructing a specialized corpus and further pretraining on network configuration data, coupled with a novel data mining technique for generating task supervision data.","The proposed model demonstrates robustness in configuration generation, translation, and analysis, outperforming conventional tools in handling complex networking environments.","The experimental results validate the effectiveness of PreConfig, establishing a new direction for automating network configuration tasks with pretrained language models."],"url":"http://arxiv.org/abs/2403.09369v1","category":"cs.NI"}
{"created":"2024-03-14 13:16:00","title":"Emergence of entanglement and breakdown of causality in the quantum realm","abstract":"Entanglement is the most striking but also most weird property in quantum mechanics. It has been confirmed by many experiments over decades through the criterion of violating Bell's inequality. However, a more fundamental problem arisen from EPR paradox is still not fully understood, that is, why quantum world emerges entanglement that classical physics does not. In this paper, we investigate the quantum dynamics of two photonic modes (or any two bosonic modes) coupled to each other through a beam splitting. Such a coupling fails to produce two-mode entanglement. We also start with a decoupled two-mode initial pure state, namely, no entanglement and no statistic feature to begin with. By solving the quantum equation of motion exactly without relying on the probabilistic interpretation, we find that when the initial wave function of one mode is different from a wave packet obeying minimum Heisenberg uncertainty (which corresponds to a well-defined classically particle), the causality in the time-evolution of another mode is broken down explicitly. It also leads to the emergence of quantum entanglement between the two modes. The lack of causality is the nature of statistics. The Bell's inequality only excludes the possible existence of local hidden variables for the probabilistic interpretation of quantum mechanics. The internally causality breaking in the dynamical evolution of subsystems in isolated systems may answer the question how quantum dynamics generate naturally the probabilistic phenomena, even though the dynamical evolution of the whole system is completely described by the deterministic Schr\\\"{o}dinger equation.","sentences":["Entanglement is the most striking but also most weird property in quantum mechanics.","It has been confirmed by many experiments over decades through the criterion of violating Bell's inequality.","However, a more fundamental problem arisen from EPR paradox is still not fully understood, that is, why quantum world emerges entanglement that classical physics does not.","In this paper, we investigate the quantum dynamics of two photonic modes (or any two bosonic modes) coupled to each other through a beam splitting.","Such a coupling fails to produce two-mode entanglement.","We also start with a decoupled two-mode initial pure state, namely, no entanglement and no statistic feature to begin with.","By solving the quantum equation of motion exactly without relying on the probabilistic interpretation, we find that when the initial wave function of one mode is different from a wave packet obeying minimum Heisenberg uncertainty (which corresponds to a well-defined classically particle), the causality in the time-evolution of another mode is broken down explicitly.","It also leads to the emergence of quantum entanglement between the two modes.","The lack of causality is the nature of statistics.","The Bell's inequality only excludes the possible existence of local hidden variables for the probabilistic interpretation of quantum mechanics.","The internally causality breaking in the dynamical evolution of subsystems in isolated systems may answer the question how quantum dynamics generate naturally the probabilistic phenomena, even though the dynamical evolution of the whole system is completely described by the deterministic Schr\\\"{o}dinger equation."],"url":"http://arxiv.org/abs/2403.09368v1","category":"quant-ph"}
{"created":"2024-03-14 13:14:52","title":"Delay Dispersion in IRS-assisted FSO Links","abstract":"The line-of-sight (LOS) requirement of free-space optical (FSO) systems can be relaxed by employing optical intelligent reflecting surfaces (IRSs). In this paper, we model the impact of the IRS-induced delay dispersion and derive the channel impulse response (CIR) of IRS-assisted FSO links. The proposed model takes into account the characteristics of the incident and reflected beams' wavefronts, the position of transmitter and receiver, the size of the IRS, and the incident beamwidth on the IRS. Our simulation results reveal that a maximum effective delay spread of 0.7 ns is expected for a square IRS with area 1 $\\mathrm{m}^2$, which induces inter-symbol interference for bit rates larger than 10 Gbps. We show that the IRS-induced delay dispersion can be mitigated via equalization at the receiver.","sentences":["The line-of-sight (LOS) requirement of free-space optical (FSO) systems can be relaxed by employing optical intelligent reflecting surfaces (IRSs).","In this paper, we model the impact of the IRS-induced delay dispersion and derive the channel impulse response (CIR) of IRS-assisted FSO links.","The proposed model takes into account the characteristics of the incident and reflected beams' wavefronts, the position of transmitter and receiver, the size of the IRS, and the incident beamwidth on the IRS.","Our simulation results reveal that a maximum effective delay spread of 0.7 ns is expected for a square IRS with area 1 $\\mathrm{m}^2$, which induces inter-symbol interference for bit rates larger than 10 Gbps.","We show that the IRS-induced delay dispersion can be mitigated via equalization at the receiver."],"url":"http://arxiv.org/abs/2403.09365v1","category":"eess.SP"}
{"created":"2024-03-14 13:13:47","title":"Spinfoam Models for Quantum Gravity: Overview","abstract":"In the quest of a physical theory of quantum gravity, spin foam models, or in short spinfoams, propose a well-defined path integral summing over quantized discrete space-time geometries. At the crossroad of topological quantum field theory, dynamical triangulations, Regge calculus, and loop quantum gravity, this framework provides a non-perturbative and background independent quantization of general relativity. It defines transition amplitudes between quantum states of geometry, and gives a precise picture of the Planck scale geometry with quantized areas and volumes. Gravity in three space-time dimensions is exactly quantized in terms of the Ponzano-Regge state-sum and Turaev-Viro topological invariants. In four space-time dimensions, gravity is formulated as a topological theory, of the BF type, with extra constraints, and hence quantized as a topological state-sum filled with defects. This leads to the Engle-Pereira-Rovelli-Livine (EPRL) spinfoam model, that can be used for explicit quantum gravity computations, for example for resolving the Big Bang singularity by a bounce or in black-to-white hole transition probability amplitudes.","sentences":["In the quest of a physical theory of quantum gravity, spin foam models, or in short spinfoams, propose a well-defined path integral summing over quantized discrete space-time geometries.","At the crossroad of topological quantum field theory, dynamical triangulations, Regge calculus, and loop quantum gravity, this framework provides a non-perturbative and background independent quantization of general relativity.","It defines transition amplitudes between quantum states of geometry, and gives a precise picture of the Planck scale geometry with quantized areas and volumes.","Gravity in three space-time dimensions is exactly quantized in terms of the Ponzano-Regge state-sum and Turaev-Viro topological invariants.","In four space-time dimensions, gravity is formulated as a topological theory, of the BF type, with extra constraints, and hence quantized as a topological state-sum filled with defects.","This leads to the Engle-Pereira-Rovelli-Livine (EPRL) spinfoam model, that can be used for explicit quantum gravity computations, for example for resolving the Big Bang singularity by a bounce or in black-to-white hole transition probability amplitudes."],"url":"http://arxiv.org/abs/2403.09364v1","category":"gr-qc"}
{"created":"2024-03-14 13:12:49","title":"Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure","abstract":"With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data. It consists of a teacher model, a student model and a generator that links both model entities. The teacher model serves as a sentinel on behalf of the data owner, replacing real data, to guide the student model at the AI service provider's end during training. Considering the disparity of knowledge space between the teacher and student, we introduce two variants of the teacher model: the omniscient and the quasi-omniscient teachers. Under these teachers' guidance, the student model seeks to match the teacher model's performance and explores domains that the teacher has not covered. To trade off between privacy and performance, we further introduce two distinct security-level training protocols: white-box and black-box, enhancing the paradigm's adaptability. Despite the inherent challenges of real data absence in the SG-ZSL paradigm, it consistently outperforms in ZSL and GZSL tasks, notably in the white-box protocol. Our comprehensive evaluation further attests to its robustness and efficiency across various setups, including stringent black-box training protocol.","sentences":["With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work.","SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data.","It consists of a teacher model, a student model and a generator that links both model entities.","The teacher model serves as a sentinel on behalf of the data owner, replacing real data, to guide the student model at the AI service provider's end during training.","Considering the disparity of knowledge space between the teacher and student, we introduce two variants of the teacher model: the omniscient and the quasi-omniscient teachers.","Under these teachers' guidance, the student model seeks to match the teacher model's performance and explores domains that the teacher has not covered.","To trade off between privacy and performance, we further introduce two distinct security-level training protocols: white-box and black-box, enhancing the paradigm's adaptability.","Despite the inherent challenges of real data absence in the SG-ZSL paradigm, it consistently outperforms in ZSL and GZSL tasks, notably in the white-box protocol.","Our comprehensive evaluation further attests to its robustness and efficiency across various setups, including stringent black-box training protocol."],"url":"http://arxiv.org/abs/2403.09363v1","category":"cs.CV"}
{"created":"2024-03-14 13:11:30","title":"A Multi-population Integrated Approach for Capacitated Location Routing","abstract":"The capacitated location-routing problem involves determining the depots from a set of candidate capacitated depot locations and finding the required routes from the selected depots to serve a set of customers whereas minimizing a cost function that includes the cost of opening the chosen depots, the fixed utilization cost per vehicle used, and the total cost (distance) of the routes. This paper presents a multi-population integrated framework in which a multi-depot edge assembly crossover generates promising offspring solutions from the perspective of both depot location and route edge assembly. The method includes an effective neighborhood-based local search, a feasibility-restoring procedure and a diversification-oriented mutation. Of particular interest is the multi-population scheme which organizes the population into multiple subpopulations based on depot configurations. Extensive experiments on 281 benchmark instances from the literature show that the algorithm performs remarkably well, by improving 101 best-known results (new upper bounds) and matching 84 best-known results. Additional experiments are presented to gain insight into the role of the key elements of the algorithm.","sentences":["The capacitated location-routing problem involves determining the depots from a set of candidate capacitated depot locations and finding the required routes from the selected depots to serve a set of customers whereas minimizing a cost function that includes the cost of opening the chosen depots, the fixed utilization cost per vehicle used, and the total cost (distance) of the routes.","This paper presents a multi-population integrated framework in which a multi-depot edge assembly crossover generates promising offspring solutions from the perspective of both depot location and route edge assembly.","The method includes an effective neighborhood-based local search, a feasibility-restoring procedure and a diversification-oriented mutation.","Of particular interest is the multi-population scheme which organizes the population into multiple subpopulations based on depot configurations.","Extensive experiments on 281 benchmark instances from the literature show that the algorithm performs remarkably well, by improving 101 best-known results (new upper bounds) and matching 84 best-known results.","Additional experiments are presented to gain insight into the role of the key elements of the algorithm."],"url":"http://arxiv.org/abs/2403.09361v1","category":"cs.AI"}
{"created":"2024-03-14 13:08:15","title":"Quality factor of dielectric optical resonators","abstract":"We show analytically that the quality ($Q$) factor of magnetic and electric Mie modes in a lossless dielectric spherical resonator with high refractive index ($n \\gg 1$) scales as $n^{2j+1}$ and $n^{2j+3}$ respectively, where $j$ denotes the multipolar order. We numerically validate these results and show that our high-$n$ analytical relation is accurate for the dipolar modes when $n>5$. For higher multipolar orders, the analytical relation becomes valid for increasingly lower $n$. We study the dependence of the $Q$ factor on absorption losses and determine a general functional form that describes the $Q$ factor for all Mie modes for any complex refractive index. Finally, we observe that this functional form predicts a multipolar-dependent singular value of optical gain which gives rise to a lasing condition with an infinite $Q$ factor.","sentences":["We show analytically that the quality ($Q$) factor of magnetic and electric Mie modes in a lossless dielectric spherical resonator with high refractive index ($n \\gg 1$) scales as $n^{2j+1}$ and $n^{2j+3}$ respectively, where $j$ denotes the multipolar order.","We numerically validate these results and show that our high-$n$ analytical relation is accurate for the dipolar modes when $n>5$. For higher multipolar orders, the analytical relation becomes valid for increasingly lower $n$. We study the dependence of the $Q$ factor on absorption losses and determine a general functional form that describes the $Q$ factor for all Mie modes for any complex refractive index.","Finally, we observe that this functional form predicts a multipolar-dependent singular value of optical gain which gives rise to a lasing condition with an infinite $Q$ factor."],"url":"http://arxiv.org/abs/2403.09360v1","category":"physics.optics"}
{"created":"2024-03-14 13:05:43","title":"D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection","abstract":"Domain adaptation for object detection typically entails transferring knowledge from one visible domain to another visible domain. However, there are limited studies on adapting from the visible to the thermal domain, because the domain gap between the visible and thermal domains is much larger than expected, and traditional domain adaptation can not successfully facilitate learning in this situation. To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each domain. Specifically, we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each domain. The framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal domains during training. We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST. Source code is available at https://github.com/EdwardDo69/D3T .","sentences":["Domain adaptation for object detection typically entails transferring knowledge from one visible domain to another visible domain.","However, there are limited studies on adapting from the visible to the thermal domain, because the domain gap between the visible and thermal domains is much larger than expected, and traditional domain adaptation can not successfully facilitate learning in this situation.","To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each domain.","Specifically, we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each domain.","The framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal domains during training.","We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.","Source code is available at https://github.com/EdwardDo69/D3T ."],"url":"http://arxiv.org/abs/2403.09359v1","category":"cs.CV"}
{"created":"2024-03-14 12:58:28","title":"Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction","abstract":"Sparse-view Computed Tomography (CT) image reconstruction is a promising approach to reduce radiation exposure, but it inevitably leads to image degradation. Although diffusion model-based approaches are computationally expensive and suffer from the training-sampling discrepancy, they provide a potential solution to the problem. This study introduces a novel Cascaded Diffusion with Discrepancy Mitigation (CDDM) framework, including the low-quality image generation in latent space and the high-quality image generation in pixel space which contains data consistency and discrepancy mitigation in a one-step reconstruction process. The cascaded framework minimizes computational costs by moving some inference steps from pixel space to latent space. The discrepancy mitigation technique addresses the training-sampling gap induced by data consistency, ensuring the data distribution is close to the original manifold. A specialized Alternating Direction Method of Multipliers (ADMM) is employed to process image gradients in separate directions, offering a more targeted approach to regularization. Experimental results across two datasets demonstrate CDDM's superior performance in high-quality image generation with clearer boundaries compared to existing methods, highlighting the framework's computational efficiency.","sentences":["Sparse-view Computed Tomography (CT) image reconstruction is a promising approach to reduce radiation exposure, but it inevitably leads to image degradation.","Although diffusion model-based approaches are computationally expensive and suffer from the training-sampling discrepancy, they provide a potential solution to the problem.","This study introduces a novel Cascaded Diffusion with Discrepancy Mitigation (CDDM) framework, including the low-quality image generation in latent space and the high-quality image generation in pixel space which contains data consistency and discrepancy mitigation in a one-step reconstruction process.","The cascaded framework minimizes computational costs by moving some inference steps from pixel space to latent space.","The discrepancy mitigation technique addresses the training-sampling gap induced by data consistency, ensuring the data distribution is close to the original manifold.","A specialized Alternating Direction Method of Multipliers (ADMM) is employed to process image gradients in separate directions, offering a more targeted approach to regularization.","Experimental results across two datasets demonstrate CDDM's superior performance in high-quality image generation with clearer boundaries compared to existing methods, highlighting the framework's computational efficiency."],"url":"http://arxiv.org/abs/2403.09355v1","category":"eess.IV"}
{"created":"2024-03-14 12:58:15","title":"Magnetic distillation of intrinsic electric dipoles in mixed-stacking tetralayer graphene","abstract":"Polytypes of tetralayer graphene (TLG: Bernal, rhombohedral and mixed stacking) offer a choice of crystalline structures with different symmetries. Among those, mixed-stacking tetralayers lack inversion symmetry, which allows for intrinsic spontaneous out-of-plane electrical polarisation, inverted in the mirror-image pair, ABCB and ABAC stackings. Here, we compare the intrinsic polarisation with the symmetry-breaking effect of a substrate. We find that a potential induced by the substrate on the bottom layer of the TLG can generate out-of-plane electric dipole moments with different sizes in all four polytypes and, also, different for the ABCB and ABAC pair. This undermines a straightforward interpretation of experimentally measured Kelvin probe microscopy maps of tetralayer flakes in terms of their intrinsic polarisation. To overcome this difficulty, we analyse the influence of an external magnetic field on electrical polarisation of all four TLGs and find that Landau level quantisation highlights the intrinsic asymmetry of mixed stacking polytypes, making the difference between ABCB and ABAC polarisations almost independent of the substrate effect. We also notice a non-monotonic (hump-like) feature in the polarisation in a magnetic field range of 0.5-3 Tesla, caused by the interplay between time inversion symmetry breaking and spatial inversion symmetry breaking specific for those two polytypes.","sentences":["Polytypes of tetralayer graphene (TLG: Bernal, rhombohedral and mixed stacking) offer a choice of crystalline structures with different symmetries.","Among those, mixed-stacking tetralayers lack inversion symmetry, which allows for intrinsic spontaneous out-of-plane electrical polarisation, inverted in the mirror-image pair, ABCB and ABAC stackings.","Here, we compare the intrinsic polarisation with the symmetry-breaking effect of a substrate.","We find that a potential induced by the substrate on the bottom layer of the TLG can generate out-of-plane electric dipole moments with different sizes in all four polytypes and, also, different for the ABCB and ABAC pair.","This undermines a straightforward interpretation of experimentally measured Kelvin probe microscopy maps of tetralayer flakes in terms of their intrinsic polarisation.","To overcome this difficulty, we analyse the influence of an external magnetic field on electrical polarisation of all four TLGs and find that Landau level quantisation highlights the intrinsic asymmetry of mixed stacking polytypes, making the difference between ABCB and ABAC polarisations almost independent of the substrate effect.","We also notice a non-monotonic (hump-like) feature in the polarisation in a magnetic field range of 0.5-3 Tesla, caused by the interplay between time inversion symmetry breaking and spatial inversion symmetry breaking specific for those two polytypes."],"url":"http://arxiv.org/abs/2403.09354v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-14 12:58:03","title":"Intelligent Reflecting Surfaces vs. Full-Duplex Relays: A Comparison in the Air","abstract":"This letter aims to provide a fundamental analytical comparison for the two major types of relaying methods: intelligent reflecting surfaces and full-duplex relays, particularly focusing on unmanned aerial vehicle communication scenarios. Both amplify-and-forward and decode-and-forward relaying schemes are included in the comparison. In addition, optimal 3D UAV deployment and minimum transmit power under the quality of service constraint are derived. Our numerical results show that IRSs of medium size exhibit comparable performance to AF relays, meanwhile outperforming DF relays under extremely large surface size and high data rates.","sentences":["This letter aims to provide a fundamental analytical comparison for the two major types of relaying methods: intelligent reflecting surfaces and full-duplex relays, particularly focusing on unmanned aerial vehicle communication scenarios.","Both amplify-and-forward and decode-and-forward relaying schemes are included in the comparison.","In addition, optimal 3D UAV deployment and minimum transmit power under the quality of service constraint are derived.","Our numerical results show that IRSs of medium size exhibit comparable performance to AF relays, meanwhile outperforming DF relays under extremely large surface size and high data rates."],"url":"http://arxiv.org/abs/2403.09353v1","category":"eess.SP"}
{"created":"2024-03-14 12:51:07","title":"AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions","abstract":"Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs. Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores the importance of enhancing the robustness, security, and fairness of LVLMs. The source code and benchmark will be made publicly available.","sentences":["Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users.","However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks.","Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited.","To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others).","We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias.","We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance.","AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs.","Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores the importance of enhancing the robustness, security, and fairness of LVLMs.","The source code and benchmark will be made publicly available."],"url":"http://arxiv.org/abs/2403.09346v1","category":"cs.CV"}
{"created":"2024-03-14 12:49:29","title":"SketchINR: A First Look into Sketches as Implicit Neural Representations","abstract":"We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned function predicts the $xy$ point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives $60\\times$ and $10\\times$ data compression over raster and vector sketches, respectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render $\\sim$$100\\times$ faster than other learned vector representations such as SketchRNN. (iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes. As a first look at implicit sketches, SketchINR's compact high-fidelity representation will support future work in modelling long and complex sketches.","sentences":["We propose SketchINR, to advance the representation of vector sketches with implicit neural models.","A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes.","The learned function predicts the $xy$ point coordinates in a sketch at each time and stroke.","Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives $60\\times$ and $10\\times$ data compression over raster and vector sketches, respectively.","(ii) SketchINR's auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO.","(iii) SketchINR supports parallelisation that can decode/render $\\sim$$100\\times$ faster than other learned vector representations such as SketchRNN.","(iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes.","As a first look at implicit sketches, SketchINR's compact high-fidelity representation will support future work in modelling long and complex sketches."],"url":"http://arxiv.org/abs/2403.09344v1","category":"cs.CV"}
{"created":"2024-03-14 12:44:47","title":"Geometric quantum discord of an arbitrary two-qudit state: the exact value and general upper bounds","abstract":"The geometric quantum discord of a two-qudit state has been studied in many papers, however, its exact analytical value in the explicit form is known only for a general two-qubit state, a general qubit-qudit state and some special families of two-qudit states. Based on the general Bloch vectors formalism [J. Phys. A: Math. Theor. 54 195301 (2021)], we find the explicit exact analytical value of the geometric quantum discord for a general two-qudit state of an arbitrary dimension via the parameters of its correlation matrix and the Bloch vectors of its reduced states. This new general analytical result indicates that the lower bound on the geometric quantum discord found in [Phys. Rev. A. 85, 204102 (2012)] is attained on each two-qudit state and also, includes all the known exact results on the geometric discord only as particular cases. Moreover, it allows us to find for an arbitrary two-qudit state, pure or mixed, the new general upper bounds on its geometric quantum discord, expressed via the Hilbert space characteristics of this state and in case of a pure two-qudit state -- in terms of its concurrence.","sentences":["The geometric quantum discord of a two-qudit state has been studied in many papers, however, its exact analytical value in the explicit form is known only for a general two-qubit state, a general qubit-qudit state and some special families of two-qudit states.","Based on the general Bloch vectors formalism [J. Phys.","A: Math.","Theor.","54 195301 (2021)], we find the explicit exact analytical value of the geometric quantum discord for a general two-qudit state of an arbitrary dimension via the parameters of its correlation matrix and the Bloch vectors of its reduced states.","This new general analytical result indicates that the lower bound on the geometric quantum discord found in [Phys.","Rev. A. 85, 204102 (2012)] is attained on each two-qudit state and also, includes all the known exact results on the geometric discord only as particular cases.","Moreover, it allows us to find for an arbitrary two-qudit state, pure or mixed, the new general upper bounds on its geometric quantum discord, expressed via the Hilbert space characteristics of this state and in case of a pure two-qudit state -- in terms of its concurrence."],"url":"http://arxiv.org/abs/2403.09342v1","category":"quant-ph"}
{"created":"2024-03-14 12:32:40","title":"LocalMamba: Visual State Space Model with Windowed Selective Scan","abstract":"Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the optimal scan choices for each layer, substantially improving performance. Extensive experiments across both plain and hierarchical models underscore our approach's superiority in effectively capturing image representations. For example, our model significantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs. Code is available at: https://github.com/hunto/LocalMamba.","sentences":["Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding.","Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).","This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling.","Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens.","We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective.","Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the optimal scan choices for each layer, substantially improving performance.","Extensive experiments across both plain and hierarchical models underscore our approach's superiority in effectively capturing image representations.","For example, our model significantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.","Code is available at: https://github.com/hunto/LocalMamba."],"url":"http://arxiv.org/abs/2403.09338v1","category":"cs.CV"}
{"created":"2024-03-14 12:31:10","title":"Differential identities of matrix algebras","abstract":"We study the differential identities of the algebra $M_k(F)$ of $k\\times k$ matrices over a field $F$ of characteristic zero when its full Lie algebra of derivations, $L=\\mbox{Der}(M_k(F))$, acts on it. We determine a set of 2 generators of the ideal of differential identities of $M_k(F)$ for $k\\geq 2$. Moreover, we obtain the exact values of the corresponding differential codimensions and differential cocharacters. Finally we prove that, unlike the ordinary case, the variety of differential algebras with $L$-action generated by $M_k(F)$ has almost polynomial growth for all $k\\geq 2$.","sentences":["We study the differential identities of the algebra $M_k(F)$ of $k\\times k$ matrices over a field $F$ of characteristic zero when its full Lie algebra of derivations, $L=\\mbox{Der}(M_k(F))$, acts on it.","We determine a set of 2 generators of the ideal of differential identities of $M_k(F)$ for $k\\geq 2$.","Moreover, we obtain the exact values of the corresponding differential codimensions and differential cocharacters.","Finally we prove that, unlike the ordinary case, the variety of differential algebras with $L$-action generated by $M_k(F)$ has almost polynomial growth for all $k\\geq 2$."],"url":"http://arxiv.org/abs/2403.09337v1","category":"math.RA"}
{"created":"2024-03-14 12:24:55","title":"Generalized Euler-Maclaurin formula and Signatures","abstract":"The Euler-Maclaurin formula which relates a discrete sum with an integral, is generalised to the setting of Riemann-Stieltjes sums and integrals on stochastic processes whose paths are a.s. rectifiable, that is continuous and bounded variation. For this purpose, new variants of the signature are introduced, such as the flip and the sawtooth signature. The counterparts of the Bernoulli numbers that arise in the classical Euler-Maclaurin formula are obtained by choosing the appropriate integration constants in the repeated integration by parts to ``minimise the error'' of every truncation level.","sentences":["The Euler-Maclaurin formula which relates a discrete sum with an integral, is generalised to the setting of Riemann-Stieltjes sums and integrals on stochastic processes whose paths are a.s. rectifiable, that is continuous and bounded variation.","For this purpose, new variants of the signature are introduced, such as the flip and the sawtooth signature.","The counterparts of the Bernoulli numbers that arise in the classical Euler-Maclaurin formula are obtained by choosing the appropriate integration constants in the repeated integration by parts to ``minimise the error'' of every truncation level."],"url":"http://arxiv.org/abs/2403.09335v1","category":"math.PR"}
{"created":"2024-03-14 12:22:54","title":"Video Editing via Factorized Diffusion Distillation","abstract":"We introduce Emu Video Edit (EVE), a model that establishes a new state-of-the art in video editing without relying on any supervised video editing data. To develop EVE we separately train an image editing adapter and a video generation adapter, and attach both to the same text-to-image model. Then, to align the adapters towards video editing we introduce a new unsupervised distillation procedure, Factorized Diffusion Distillation. This procedure distills knowledge from one or more teachers simultaneously, without any supervised data. We utilize this procedure to teach EVE to edit videos by jointly distilling knowledge to (i) precisely edit each individual frame from the image editing adapter, and (ii) ensure temporal consistency among the edited frames using the video generation adapter. Finally, to demonstrate the potential of our approach in unlocking other capabilities, we align additional combinations of adapters","sentences":["We introduce Emu Video Edit (EVE), a model that establishes a new state-of-the art in video editing without relying on any supervised video editing data.","To develop EVE we separately train an image editing adapter and a video generation adapter, and attach both to the same text-to-image model.","Then, to align the adapters towards video editing we introduce a new unsupervised distillation procedure, Factorized Diffusion Distillation.","This procedure distills knowledge from one or more teachers simultaneously, without any supervised data.","We utilize this procedure to teach EVE to edit videos by jointly distilling knowledge to (i) precisely edit each individual frame from the image editing adapter, and (ii) ensure temporal consistency among the edited frames using the video generation adapter.","Finally, to demonstrate the potential of our approach in unlocking other capabilities, we align additional combinations of adapters"],"url":"http://arxiv.org/abs/2403.09334v1","category":"cs.CV"}
{"created":"2024-03-14 12:21:37","title":"Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring","abstract":"Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \\etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects. Building upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer. It enables user-friendly interaction with flexible target images, free-form texts and even coordinates. Experiments demonstrate that Griffon v2 can localize any objects of interest with visual and textual referring, achieve state-of-the-art performance on REC, phrase grounding, and REG tasks, and outperform expert models in object detection and object counting. Data, codes and models will be released at https://github.com/jefferyZhan/Griffon.","sentences":["Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios.","Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \\etc.","To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts.","To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models.","This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects.","Building upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer.","It enables user-friendly interaction with flexible target images, free-form texts and even coordinates.","Experiments demonstrate that Griffon v2 can localize any objects of interest with visual and textual referring, achieve state-of-the-art performance on REC, phrase grounding, and REG tasks, and outperform expert models in object detection and object counting.","Data, codes and models will be released at https://github.com/jefferyZhan/Griffon."],"url":"http://arxiv.org/abs/2403.09333v1","category":"cs.CV"}
{"created":"2024-03-14 12:18:45","title":"Radar Rainbow Beams For Wideband mmWave Communication: Beam Training And Tracking","abstract":"We propose a novel integrated sensing and communication (ISAC) system that leverages sensing to assist communication, ensuring fast initial access, seamless user tracking, and uninterrupted communication for millimeter wave (mmWave) wideband systems. True-time-delayers (TTDs) are utilized to generate frequency-dependent radar rainbow beams by controlling the beam squint effect. These beams cover users across the entire angular space simultaneously for fast beam training using just one orthogonal frequency-division multiplexing (OFDM) symbol. Three detection and estimation schemes are proposed based on radar rainbow beams for estimation of the users' angles, distances, and velocities, which are then exploited for communication beamformer design. The first proposed scheme utilizes a single-antenna radar receiver and one set of rainbow beams, but may cause a Doppler ambiguity. To tackle this limitation, two additional schemes are introduced, utilizing two sets of rainbow beams and a multi-antenna receiver, respectively. Furthermore, the proposed detection and estimation schemes are extended to realize user tracking by choosing different subsets of OFDM subcarriers. This approach eliminates the need to switch phase shifters and TTDs, which are typically necessary in existing tracking technologies, thereby reducing the demands on the control circurity. Simulation results reveal the effectiveness of the proposed rainbow beam-based training and tracking methods for mobile users. Notably, the scheme employing a multi-antenna radar receiver can accurately estimate the channel parameters and can support communication rates comparable to those achieved with perfect channel information.","sentences":["We propose a novel integrated sensing and communication (ISAC) system that leverages sensing to assist communication, ensuring fast initial access, seamless user tracking, and uninterrupted communication for millimeter wave (mmWave) wideband systems.","True-time-delayers (TTDs) are utilized to generate frequency-dependent radar rainbow beams by controlling the beam squint effect.","These beams cover users across the entire angular space simultaneously for fast beam training using just one orthogonal frequency-division multiplexing (OFDM) symbol.","Three detection and estimation schemes are proposed based on radar rainbow beams for estimation of the users' angles, distances, and velocities, which are then exploited for communication beamformer design.","The first proposed scheme utilizes a single-antenna radar receiver and one set of rainbow beams, but may cause a Doppler ambiguity.","To tackle this limitation, two additional schemes are introduced, utilizing two sets of rainbow beams and a multi-antenna receiver, respectively.","Furthermore, the proposed detection and estimation schemes are extended to realize user tracking by choosing different subsets of OFDM subcarriers.","This approach eliminates the need to switch phase shifters and TTDs, which are typically necessary in existing tracking technologies, thereby reducing the demands on the control circurity.","Simulation results reveal the effectiveness of the proposed rainbow beam-based training and tracking methods for mobile users.","Notably, the scheme employing a multi-antenna radar receiver can accurately estimate the channel parameters and can support communication rates comparable to those achieved with perfect channel information."],"url":"http://arxiv.org/abs/2403.09330v1","category":"eess.SP"}
{"created":"2024-03-14 12:15:23","title":"HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation","abstract":"We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency.","sentences":["We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance.","HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation.","To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features.","Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance.","Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency."],"url":"http://arxiv.org/abs/2403.09326v1","category":"cs.GR"}
{"created":"2024-03-14 12:11:25","title":"Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors","abstract":"IoT devices have become indispensable components of our lives, and the advancement of AI technologies will make them even more pervasive, increasing the vulnerability to malfunctions or cyberattacks and raising privacy concerns. Encryption can mitigate these challenges; however, most existing anomaly detection techniques decrypt the data to perform the analysis, potentially undermining the encryption protection provided during transit or storage. Homomorphic encryption schemes are promising solutions as they enable the processing and execution of operations on IoT data while still encrypted, however, these schemes offer only limited operations, which poses challenges to their practical usage. In this paper, we propose a novel privacy-preserving anomaly detection solution designed for homomorphically encrypted data generated by IoT devices that efficiently detects abnormal values without performing decryption. We have adapted the Histogram-based anomaly detection technique for TFHE scheme to address limitations related to the input size and the depth of computation by implementing vectorized support operations. These operations include addition, value placement in buckets, labeling abnormal buckets based on a threshold frequency, labeling abnormal values based on their range, and bucket labels. Evaluation results show that the solution effectively detects anomalies without requiring data decryption and achieves consistent results comparable to the mechanism operating on plain data. Also, it shows robustness and resilience against various challenges commonly encountered in IoT environments, such as noisy sensor data, adversarial attacks, communication failures, and device malfunctions. Moreover, the time and computational overheads determined for several solution configurations, despite being large, are reasonable compared to those reported in existing literature.","sentences":["IoT devices have become indispensable components of our lives, and the advancement of AI technologies will make them even more pervasive, increasing the vulnerability to malfunctions or cyberattacks and raising privacy concerns.","Encryption can mitigate these challenges; however, most existing anomaly detection techniques decrypt the data to perform the analysis, potentially undermining the encryption protection provided during transit or storage.","Homomorphic encryption schemes are promising solutions as they enable the processing and execution of operations on IoT data while still encrypted, however, these schemes offer only limited operations, which poses challenges to their practical usage.","In this paper, we propose a novel privacy-preserving anomaly detection solution designed for homomorphically encrypted data generated by IoT devices that efficiently detects abnormal values without performing decryption.","We have adapted the Histogram-based anomaly detection technique for TFHE scheme to address limitations related to the input size and the depth of computation by implementing vectorized support operations.","These operations include addition, value placement in buckets, labeling abnormal buckets based on a threshold frequency, labeling abnormal values based on their range, and bucket labels.","Evaluation results show that the solution effectively detects anomalies without requiring data decryption and achieves consistent results comparable to the mechanism operating on plain data.","Also, it shows robustness and resilience against various challenges commonly encountered in IoT environments, such as noisy sensor data, adversarial attacks, communication failures, and device malfunctions.","Moreover, the time and computational overheads determined for several solution configurations, despite being large, are reasonable compared to those reported in existing literature."],"url":"http://arxiv.org/abs/2403.09322v1","category":"cs.CR"}
{"created":"2024-03-14 12:08:44","title":"SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios","abstract":"Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios. The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the domain gap between real and synthetic data. To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training domain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion. Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes. Additionally, we build an effective filtering algorithm on predicted keypoint to dynamically eliminate multiple ambiguity and outlier keypoint candidates. At the domain adaptation stage, we propose the self-training framework using a student-teacher training scheme. To carefully distinguish reliable predictions, we harnesses a tailored heuristics for 3D geometry pseudo labelling based on semi-chamfer distance. On public Sil'eane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%. Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method. The code is available at https://github.com/dingthuang/SD-Net.","sentences":["Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios.","The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the domain gap between real and synthetic data.","To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training domain adaptation (SD-Net).","SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion.","Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes.","Additionally, we build an effective filtering algorithm on predicted keypoint to dynamically eliminate multiple ambiguity and outlier keypoint candidates.","At the domain adaptation stage, we propose the self-training framework using a student-teacher training scheme.","To carefully distinguish reliable predictions, we harnesses a tailored heuristics for 3D geometry pseudo labelling based on semi-chamfer distance.","On public Sil'eane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%.","Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method.","The code is available at https://github.com/dingthuang/SD-Net."],"url":"http://arxiv.org/abs/2403.09317v1","category":"cs.CV"}
{"created":"2024-03-14 12:03:28","title":"Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection","abstract":"In this paper we present YOLOX-ViT, a novel object detection model, and investigate the efficacy of knowledge distillation for model size reduction without sacrificing performance. Focused on underwater robotics, our research addresses key questions about the viability of smaller models and the impact of the visual transformer layer in YOLOX. Furthermore, we introduce a new side-scan sonar image dataset, and use it to evaluate our object detector's performance. Results show that knowledge distillation effectively reduces false positives in wall detection. Additionally, the introduced visual transformer layer significantly improves object detection accuracy in the underwater environment. The source code of the knowledge distillation in the YOLOX-ViT is at https://github.com/remaro-network/KD-YOLOX-ViT.","sentences":["In this paper we present YOLOX-ViT, a novel object detection model, and investigate the efficacy of knowledge distillation for model size reduction without sacrificing performance.","Focused on underwater robotics, our research addresses key questions about the viability of smaller models and the impact of the visual transformer layer in YOLOX.","Furthermore, we introduce a new side-scan sonar image dataset, and use it to evaluate our object detector's performance.","Results show that knowledge distillation effectively reduces false positives in wall detection.","Additionally, the introduced visual transformer layer significantly improves object detection accuracy in the underwater environment.","The source code of the knowledge distillation in the YOLOX-ViT is at https://github.com/remaro-network/KD-YOLOX-ViT."],"url":"http://arxiv.org/abs/2403.09313v1","category":"cs.CV"}
{"created":"2024-03-14 12:02:25","title":"Modular parametric PGD enabling online solution of partial differential equations","abstract":"In the present work, a new methodology is proposed for building surrogate parametric models of engineering systems based on modular assembly of pre-solved modules. Each module is a generic parametric solution considering parametric geometry, material and boundary conditions. By assembling these modules and satisfying continuity constraints at the interfaces, a parametric surrogate model of the full problem can be obtained. In the present paper, the PGD technique in connection with NURBS geometry representation is used to create a parametric model for each module. In this technique, the NURBS objects allow to map the governing boundary value problem from a parametric non-regular domain into a regular reference domain and the PGD is used to create a reduced model in the reference domain. In the assembly stage, an optimization problem is solved to satisfy the continuity constraints at the interfaces. The proposed procedure is based on the offline--online paradigm: the offline stage consists of creating multiple pre-solved modules which can be afterwards assembled in almost real-time during the online stage, enabling quick evaluations of the full system response. To show the potential of the proposed approach some numerical examples in heat conduction and structural plates under bending are presented.","sentences":["In the present work, a new methodology is proposed for building surrogate parametric models of engineering systems based on modular assembly of pre-solved modules.","Each module is a generic parametric solution considering parametric geometry, material and boundary conditions.","By assembling these modules and satisfying continuity constraints at the interfaces, a parametric surrogate model of the full problem can be obtained.","In the present paper, the PGD technique in connection with NURBS geometry representation is used to create a parametric model for each module.","In this technique, the NURBS objects allow to map the governing boundary value problem from a parametric non-regular domain into a regular reference domain and the PGD is used to create a reduced model in the reference domain.","In the assembly stage, an optimization problem is solved to satisfy the continuity constraints at the interfaces.","The proposed procedure is based on the offline--online paradigm: the offline stage consists of creating multiple pre-solved modules which can be afterwards assembled in almost real-time during the online stage, enabling quick evaluations of the full system response.","To show the potential of the proposed approach some numerical examples in heat conduction and structural plates under bending are presented."],"url":"http://arxiv.org/abs/2403.09312v1","category":"cs.CE"}
{"created":"2024-03-14 12:00:53","title":"Binary Stretch Embedding of Weighted Graphs","abstract":"In this paper, we introduce and study the problem of \\textit{binary stretch embedding} of edge-weighted graph. This problem is closely related to the well-known \\textit{addressing problem} of Graham and Pollak. Addressing problem is the problem of assigning the shortest possible length strings (called ``addresses\") over the alphabet $\\{0,1,*\\}$ to the vertices of an input graph $G$ with the following property. For every pair $u,v$ of vertices, the number of positions in which one of their addresses is $1$, and the other is $0$ is exactly equal to the distance of $u,v$ in graph $G$. When the addresses do not contain the symbol $*$, the problem is called \\textit{isometric hypercube embedding}. As far as we know, the isometric hypercube embedding was introduced by Firsov in 1965. It is known that such addresses do not exist for general graphs.   Inspired by the addressing problem, in this paper, we introduce the \\textit{binary stretch embedding problem}, or BSEP for short, for the edge-weighted undirected graphs. We also argue how this problem is related to other graph embedding problems in the literature.   Using tools and techniques such as Hadamard codes and the theory of linear programming, several upper and lower bounds as well as exact solutions for certain classes of graphs will be discovered.   As an application of the results in this paper, we derive improved upper bounds or exact values for the maximum size of Lee metric codes of certain parameters.","sentences":["In this paper, we introduce and study the problem of \\textit{binary stretch embedding} of edge-weighted graph.","This problem is closely related to the well-known \\textit{addressing problem} of Graham and Pollak.","Addressing problem is the problem of assigning the shortest possible length strings (called ``addresses\") over the alphabet $\\{0,1,*\\}$ to the vertices of an input graph $G$ with the following property.","For every pair $u,v$ of vertices, the number of positions in which one of their addresses is $1$, and the other is $0$ is exactly equal to the distance of $u,v$ in graph $G$. When the addresses do not contain the symbol $*$, the problem is called \\textit{isometric hypercube embedding}.","As far as we know, the isometric hypercube embedding was introduced by Firsov in 1965.","It is known that such addresses do not exist for general graphs.   ","Inspired by the addressing problem, in this paper, we introduce the \\textit{binary stretch embedding problem}, or BSEP for short, for the edge-weighted undirected graphs.","We also argue how this problem is related to other graph embedding problems in the literature.   ","Using tools and techniques such as Hadamard codes and the theory of linear programming, several upper and lower bounds as well as exact solutions for certain classes of graphs will be discovered.   ","As an application of the results in this paper, we derive improved upper bounds or exact values for the maximum size of Lee metric codes of certain parameters."],"url":"http://arxiv.org/abs/2403.09311v1","category":"cs.DM"}
{"created":"2024-03-14 11:59:07","title":"Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality","abstract":"Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).","sentences":["Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints.","We propose a framework that simplifies robot deployment by allowing direct communication using natural language.","It uses large language models (LLM) for prompt processing, workspace understanding, and waypoint generation.","It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome.","We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot.","Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping)."],"url":"http://arxiv.org/abs/2403.09308v1","category":"cs.HC"}
{"created":"2024-03-14 11:57:58","title":"Annotation Free Semantic Segmentation with Vision Foundation Models","abstract":"Semantic Segmentation is one of the most challenging vision tasks, usually requiring large amounts of training data with expensive pixel-level annotations. With the success of foundation models and especially vision-language models, recent works attempt to achieve zero-shot semantic segmentation while requiring either large scale training or additional image/pixel-level annotations. In this work, we build a lightweight module on top of a self-supervised pretrained vision encoder to align patch features with a pre-trained text encoder. Importantly, we generate free annotations for any semantic segmentation dataset using existing foundation models and train our alignment module cost free. We use CLIP to detect objects and SAM to generate high quality object masks. Our approach can bring language-based semantics to any pre-trained vision encoder with minimal training. Our module is lightweight, uses foundation models as a sole source of supervision and shows impressive generalization capability from little training data with no annotation.","sentences":["Semantic Segmentation is one of the most challenging vision tasks, usually requiring large amounts of training data with expensive pixel-level annotations.","With the success of foundation models and especially vision-language models, recent works attempt to achieve zero-shot semantic segmentation while requiring either large scale training or additional image/pixel-level annotations.","In this work, we build a lightweight module on top of a self-supervised pretrained vision encoder to align patch features with a pre-trained text encoder.","Importantly, we generate free annotations for any semantic segmentation dataset using existing foundation models and train our alignment module cost free.","We use CLIP to detect objects and SAM to generate high quality object masks.","Our approach can bring language-based semantics to any pre-trained vision encoder with minimal training.","Our module is lightweight, uses foundation models as a sole source of supervision and shows impressive generalization capability from little training data with no annotation."],"url":"http://arxiv.org/abs/2403.09307v1","category":"cs.CV"}
{"created":"2024-03-14 11:57:32","title":"Partially coherent beam: Theory and simulation","abstract":"Since the development of lasers, we have continuously sought to advance techniques and theory to obtain beams with a high degree of coherence, as natural light sources provide incoherent light. However, there are applications where it is advantageous to use partially coherent (PC) beams in a controlled manner, such as in propagation through turbulent media. To generate a PC beam in the laboratory or in simulations, specific theories and methods are required. In this article, we provide an introduction to PC beam theory, describing how to generate them through modal decompositions and a step-by-step guide for simulating and analyzing beam generation inspired by experiments. To illustrate the methods, we present Gaussian Schell-Model beams as an example.","sentences":["Since the development of lasers, we have continuously sought to advance techniques and theory to obtain beams with a high degree of coherence, as natural light sources provide incoherent light.","However, there are applications where it is advantageous to use partially coherent (PC) beams in a controlled manner, such as in propagation through turbulent media.","To generate a PC beam in the laboratory or in simulations, specific theories and methods are required.","In this article, we provide an introduction to PC beam theory, describing how to generate them through modal decompositions and a step-by-step guide for simulating and analyzing beam generation inspired by experiments.","To illustrate the methods, we present Gaussian Schell-Model beams as an example."],"url":"http://arxiv.org/abs/2403.09306v1","category":"physics.optics"}
{"created":"2024-03-14 11:49:14","title":"CRB Analysis for Mixed-ADC Based DOA Estimation","abstract":"We consider a mixed analog-to-digital converter (ADC) based architecture consisting of high-precision and one-bit ADCs with the antenna-varying threshold for direction of arrival (DOA) estimation using a uniform linear array (ULA), which utilizes fixed but different thresholds for one-bit ADCs across different receive antennas. The Cram{\\'e}r-Rao bound (CRB) with the antenna-varying threshold is obtained. Then based on the lower bound of the CRB, we derive the asymptotic CRB of the DOA, which depends on the placement of mixed-ADC. Our analysis shows that distributing high-precision ADCs evenly around the two edges of the ULA yields improved performance. This result can be extended to a more general case where the ULA is equipped with two types of ADCs with different quantization precisions. To efficiently obtain the maximum likelihood DOA estimates, we propose a two-step algorithm. Firstly, we formulate the model as a sparse signal representation problem, and modify the sparse learning via iterative minimization (SLIM) approach to the mixed-ADC based DOA estimation. In the second step, we use the relaxation-based approach to cyclically refine the estimates of SLIM, further enhancing the DOA estimation performance. Numerical examples are presented to demonstrate the validity of the CRB analysis and the effectiveness of our methods.","sentences":["We consider a mixed analog-to-digital converter (ADC) based architecture consisting of high-precision and one-bit ADCs with the antenna-varying threshold for direction of arrival (DOA) estimation using a uniform linear array (ULA), which utilizes fixed but different thresholds for one-bit ADCs across different receive antennas.","The Cram{\\'e}r-Rao bound (CRB) with the antenna-varying threshold is obtained.","Then based on the lower bound of the CRB, we derive the asymptotic CRB of the DOA, which depends on the placement of mixed-ADC.","Our analysis shows that distributing high-precision ADCs evenly around the two edges of the ULA yields improved performance.","This result can be extended to a more general case where the ULA is equipped with two types of ADCs with different quantization precisions.","To efficiently obtain the maximum likelihood DOA estimates, we propose a two-step algorithm.","Firstly, we formulate the model as a sparse signal representation problem, and modify the sparse learning via iterative minimization (SLIM) approach to the mixed-ADC based DOA estimation.","In the second step, we use the relaxation-based approach to cyclically refine the estimates of SLIM, further enhancing the DOA estimation performance.","Numerical examples are presented to demonstrate the validity of the CRB analysis and the effectiveness of our methods."],"url":"http://arxiv.org/abs/2403.09301v1","category":"eess.SP"}
{"created":"2024-03-14 11:36:36","title":"Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models","abstract":"Large-scale vision-language models (VLMs) have shown a strong zero-shot generalization capability on unseen-domain data. However, when adapting pre-trained VLMs to a sequence of downstream tasks, they are prone to forgetting previously learned knowledge and degrade their zero-shot classification capability. To tackle this problem, we propose a unique Selective Dual-Teacher Knowledge Transfer framework that leverages the most recent fine-tuned and the original pre-trained VLMs as dual teachers to preserve the previously learned knowledge and zero-shot capabilities, respectively. With only access to an unlabeled reference dataset, our proposed framework performs a selective knowledge distillation mechanism by measuring the feature discrepancy from the dual teacher VLMs. Consequently, our selective dual-teacher knowledge distillation would mitigate catastrophic forgetting of previously learned knowledge while preserving the zero-shot capabilities from pre-trained VLMs. Through extensive experiments on benchmark datasets, we show that our proposed framework is favorable against state-of-the-art continual learning approaches for preventing catastrophic forgetting and zero-shot degradation.","sentences":["Large-scale vision-language models (VLMs) have shown a strong zero-shot generalization capability on unseen-domain data.","However, when adapting pre-trained VLMs to a sequence of downstream tasks, they are prone to forgetting previously learned knowledge and degrade their zero-shot classification capability.","To tackle this problem, we propose a unique Selective Dual-Teacher Knowledge Transfer framework that leverages the most recent fine-tuned and the original pre-trained VLMs as dual teachers to preserve the previously learned knowledge and zero-shot capabilities, respectively.","With only access to an unlabeled reference dataset, our proposed framework performs a selective knowledge distillation mechanism by measuring the feature discrepancy from the dual teacher VLMs.","Consequently, our selective dual-teacher knowledge distillation would mitigate catastrophic forgetting of previously learned knowledge while preserving the zero-shot capabilities from pre-trained VLMs.","Through extensive experiments on benchmark datasets, we show that our proposed framework is favorable against state-of-the-art continual learning approaches for preventing catastrophic forgetting and zero-shot degradation."],"url":"http://arxiv.org/abs/2403.09296v1","category":"cs.CV"}
{"created":"2024-03-14 11:24:32","title":"Parametric tuning of dynamical phase transitions in ultracold reactions","abstract":"Advances in ultracold chemistry have led to the possibility of a coherent transformation between ultracold atoms and molecules including between completely bosonic condensates. Such transformations are enabled by the magneto-association of atoms at a Feshbach resonance which results in a passage through a quantum critical point. In this study, we show that the presence of generic interaction between the formed molecules can fundamentally alter the nature of the critical point, change the yield of the reaction and the order of the consequent phase transition. We find that the correlations introduced by this rather general interaction induce nontrivial many-body physics such as coherent oscillations between atoms and molecules, and a selective formation of squeezed molecular quantum states and quantum cat states. We provide analytical and numerical descriptions of these many-body effects, along with scaling laws for the reaction yield in both the adiabatic and non-adiabatic regimes, and highlight the potential experimental relevance in quantum sensing.","sentences":["Advances in ultracold chemistry have led to the possibility of a coherent transformation between ultracold atoms and molecules including between completely bosonic condensates.","Such transformations are enabled by the magneto-association of atoms at a Feshbach resonance which results in a passage through a quantum critical point.","In this study, we show that the presence of generic interaction between the formed molecules can fundamentally alter the nature of the critical point, change the yield of the reaction and the order of the consequent phase transition.","We find that the correlations introduced by this rather general interaction induce nontrivial many-body physics such as coherent oscillations between atoms and molecules, and a selective formation of squeezed molecular quantum states and quantum cat states.","We provide analytical and numerical descriptions of these many-body effects, along with scaling laws for the reaction yield in both the adiabatic and non-adiabatic regimes, and highlight the potential experimental relevance in quantum sensing."],"url":"http://arxiv.org/abs/2403.09291v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-14 11:23:39","title":"SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival","abstract":"Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life. Multimodal prediction of cancer patient survival offers a more comprehensive and precise approach. However, existing methods still grapple with challenges related to missing multimodal data and information interaction within modalities. This paper introduces SELECTOR, a heterogeneous graph-aware network based on convolutional mask encoders for robust multimodal prediction of cancer patient survival. SELECTOR comprises feature edge reconstruction, convolutional mask encoder, feature cross-fusion, and multimodal survival prediction modules. Initially, we construct a multimodal heterogeneous graph and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature information from graph edges and effective embedding of nodes. To mitigate the impact of missing features within the modality on prediction accuracy, we devised a convolutional masked autoencoder (CMAE) to process the heterogeneous graph post-feature reconstruction. Subsequently, the feature cross-fusion module facilitates communication between modalities, ensuring that output features encompass all features of the modality and relevant information from other modalities. Extensive experiments and analysis on six cancer datasets from TCGA demonstrate that our method significantly outperforms state-of-the-art methods in both modality-missing and intra-modality information-confirmed cases. Our codes are made available at https://github.com/panliangrui/Selector.","sentences":["Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life.","Multimodal prediction of cancer patient survival offers a more comprehensive and precise approach.","However, existing methods still grapple with challenges related to missing multimodal data and information interaction within modalities.","This paper introduces SELECTOR, a heterogeneous graph-aware network based on convolutional mask encoders for robust multimodal prediction of cancer patient survival.","SELECTOR comprises feature edge reconstruction, convolutional mask encoder, feature cross-fusion, and multimodal survival prediction modules.","Initially, we construct a multimodal heterogeneous graph and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature information from graph edges and effective embedding of nodes.","To mitigate the impact of missing features within the modality on prediction accuracy, we devised a convolutional masked autoencoder (CMAE) to process the heterogeneous graph post-feature reconstruction.","Subsequently, the feature cross-fusion module facilitates communication between modalities, ensuring that output features encompass all features of the modality and relevant information from other modalities.","Extensive experiments and analysis on six cancer datasets from TCGA demonstrate that our method significantly outperforms state-of-the-art methods in both modality-missing and intra-modality information-confirmed cases.","Our codes are made available at https://github.com/panliangrui/Selector."],"url":"http://arxiv.org/abs/2403.09290v1","category":"cs.CV"}
{"created":"2024-03-14 11:22:51","title":"Silico-centric Theory of Mind","abstract":"Theory of Mind (ToM) refers to the ability to attribute mental states, such as beliefs, desires, intentions, and knowledge, to oneself and others, and to understand that these mental states can differ from one's own and from reality. We investigate ToM in environments with multiple, distinct, independent AI agents, each possessing unique internal states, information, and objectives. Inspired by human false-belief experiments, we present an AI ('focal AI') with a scenario where its clone undergoes a human-centric ToM assessment. We prompt the focal AI to assess whether its clone would benefit from additional instructions. Concurrently, we give its clones the ToM assessment, both with and without the instructions, thereby engaging the focal AI in higher-order counterfactual reasoning akin to human mentalizing--with respect to humans in one test and to other AI in another. We uncover a discrepancy: Contemporary AI demonstrates near-perfect accuracy on human-centric ToM assessments. Since information embedded in one AI is identically embedded in its clone, additional instructions are redundant. Yet, we observe AI crafting elaborate instructions for their clones, erroneously anticipating a need for assistance. An independent referee AI agrees with these unsupported expectations. Neither the focal AI nor the referee demonstrates ToM in our 'silico-centric' test.","sentences":["Theory of Mind (ToM) refers to the ability to attribute mental states, such as beliefs, desires, intentions, and knowledge, to oneself and others, and to understand that these mental states can differ from one's own and from reality.","We investigate ToM in environments with multiple, distinct, independent AI agents, each possessing unique internal states, information, and objectives.","Inspired by human false-belief experiments, we present an AI ('focal AI') with a scenario where its clone undergoes a human-centric ToM assessment.","We prompt the focal AI to assess whether its clone would benefit from additional instructions.","Concurrently, we give its clones the ToM assessment, both with and without the instructions, thereby engaging the focal AI in higher-order counterfactual reasoning akin to human mentalizing--with respect to humans in one test and to other AI in another.","We uncover a discrepancy: Contemporary AI demonstrates near-perfect accuracy on human-centric ToM assessments.","Since information embedded in one AI is identically embedded in its clone, additional instructions are redundant.","Yet, we observe AI crafting elaborate instructions for their clones, erroneously anticipating a need for assistance.","An independent referee AI agrees with these unsupported expectations.","Neither the focal AI nor the referee demonstrates ToM in our 'silico-centric' test."],"url":"http://arxiv.org/abs/2403.09289v1","category":"cs.AI"}
{"created":"2024-03-14 11:22:06","title":"Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering","abstract":"Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer questions related to the text content. Most existing methods heavily rely on the accuracy of Optical Character Recognition (OCR) systems, and aggressive fine-tuning based on limited spatial location information and erroneous OCR text information often leads to inevitable overfitting. In this paper, we propose a multimodal adversarial training architecture with spatial awareness capabilities. Specifically, we introduce an Adversarial OCR Enhancement (AOE) module, which leverages adversarial training in the embedding space of OCR modality to enhance fault-tolerant representation of OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We add a Spatial-Aware Self-Attention (SASA) mechanism to help the model better capture the spatial relationships among OCR tokens. Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for multimodal adversarial training.","sentences":["Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer questions related to the text content.","Most existing methods heavily rely on the accuracy of Optical Character Recognition (OCR) systems, and aggressive fine-tuning based on limited spatial location information and erroneous OCR text information often leads to inevitable overfitting.","In this paper, we propose a multimodal adversarial training architecture with spatial awareness capabilities.","Specifically, we introduce an Adversarial OCR Enhancement (AOE) module, which leverages adversarial training in the embedding space of OCR modality to enhance fault-tolerant representation of OCR texts, thereby reducing noise caused by OCR errors.","Simultaneously, We add a Spatial-Aware Self-Attention (SASA) mechanism to help the model better capture the spatial relationships among OCR tokens.","Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for multimodal adversarial training."],"url":"http://arxiv.org/abs/2403.09288v1","category":"cs.CV"}
{"created":"2024-03-14 11:18:32","title":"Duality and hidden symmetry breaking in the q-deformed Affleck-Kennedy-Lieb-Tasaki model","abstract":"We revisit the question of string order and hidden symmetry breaking in the q-deformed AKLT model, an example of a spin chain that possesses generalized symmetry. We first argue that the non-local Kennedy-Tasaki duality transformation that was previously proposed to relate the string order to a local order parameter leads to a non-local Hamiltonian and thus does not provide a physically adequate description of the symmetry breaking. We then present a modified non-local transformation which is based on a recently developed generalization of Witten's Conjugation to frustration-free lattice models and capable of resolving this issue.","sentences":["We revisit the question of string order and hidden symmetry breaking in the q-deformed AKLT model, an example of a spin chain that possesses generalized symmetry.","We first argue that the non-local Kennedy-Tasaki duality transformation that was previously proposed to relate the string order to a local order parameter leads to a non-local Hamiltonian and thus does not provide a physically adequate description of the symmetry breaking.","We then present a modified non-local transformation which is based on a recently developed generalization of Witten's Conjugation to frustration-free lattice models and capable of resolving this issue."],"url":"http://arxiv.org/abs/2403.09287v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 11:08:33","title":"CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification","abstract":"The CLIP (Contrastive Language-Image Pretraining) model has exhibited outstanding performance in recognition problems, such as zero-shot image classification and object detection. However, its ability to count remains understudied due to the inherent challenges of transforming counting--a regression task--into a recognition task. In this paper, we investigate CLIP's potential in counting, focusing specifically on estimating crowd sizes. Existing classification-based crowd-counting methods have encountered issues, including inappropriate discretization strategies, which impede the application of CLIP and result in suboptimal performance. To address these challenges, we propose the Enhanced Blockwise Classification (EBC) framework. In contrast to previous methods, EBC relies on integer-valued bins that facilitate the learning of robust decision boundaries. Within our model-agnostic EBC framework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting model capable of generating density maps. Comprehensive evaluations across diverse crowd-counting datasets demonstrate the state-of-the-art performance of our methods. Particularly, EBC can improve existing models by up to 76.9%. Moreover, our CLIP-EBC model surpasses current crowd-counting methods, achieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part B datasets, respectively. The code will be made publicly available.","sentences":["The CLIP (Contrastive Language-Image Pretraining) model has exhibited outstanding performance in recognition problems, such as zero-shot image classification and object detection.","However, its ability to count remains understudied due to the inherent challenges of transforming counting--a regression task--into a recognition task.","In this paper, we investigate CLIP's potential in counting, focusing specifically on estimating crowd sizes.","Existing classification-based crowd-counting methods have encountered issues, including inappropriate discretization strategies, which impede the application of CLIP and result in suboptimal performance.","To address these challenges, we propose the Enhanced Blockwise Classification (EBC) framework.","In contrast to previous methods, EBC relies on integer-valued bins that facilitate the learning of robust decision boundaries.","Within our model-agnostic EBC framework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting model capable of generating density maps.","Comprehensive evaluations across diverse crowd-counting datasets demonstrate the state-of-the-art performance of our methods.","Particularly, EBC can improve existing models by up to 76.9%.","Moreover, our CLIP-EBC model surpasses current crowd-counting methods, achieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part B datasets, respectively.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2403.09281v1","category":"cs.CV"}
{"created":"2024-03-14 10:59:24","title":"Nonlinear Kinematics of Recursive Origami Inspired by Spidron","abstract":"Non-periodic folding of periodic crease patterns paves the way to novel nonlinear phenomena that cannot be feasible through periodic folding. This paper focuses on the non-periodic folding of recursive crease patterns generalized from Spidron. Although it is known that Spidron has a 1-DOF isotropic rigid folding motion, its general kinematics and dependence on the crease pattern remain unclear. Using the kinematics of a single unit cell of Spidron and the recursive construction of the folded state of multiple unit cells, we consider the folding of Spidron that is not necessarily isotropic. We found that as the number of unit cells increases, the non-periodic folding is restricted and the isotropic folding becomes dominant. Then, we analyze the three kinds of isotropic folding modes by constructing 1-dimensional dynamical systems governing each of them. We show that the dynamical system can possess different recursive natures depending on folding modes even in an identical crease pattern. Furthermore, we show their novel nonlinear nature, including the period-doubling cascade leading to the emergence of chaos.","sentences":["Non-periodic folding of periodic crease patterns paves the way to novel nonlinear phenomena that cannot be feasible through periodic folding.","This paper focuses on the non-periodic folding of recursive crease patterns generalized from Spidron.","Although it is known that Spidron has a 1-DOF isotropic rigid folding motion, its general kinematics and dependence on the crease pattern remain unclear.","Using the kinematics of a single unit cell of Spidron and the recursive construction of the folded state of multiple unit cells, we consider the folding of Spidron that is not necessarily isotropic.","We found that as the number of unit cells increases, the non-periodic folding is restricted and the isotropic folding becomes dominant.","Then, we analyze the three kinds of isotropic folding modes by constructing 1-dimensional dynamical systems governing each of them.","We show that the dynamical system can possess different recursive natures depending on folding modes even in an identical crease pattern.","Furthermore, we show their novel nonlinear nature, including the period-doubling cascade leading to the emergence of chaos."],"url":"http://arxiv.org/abs/2403.09278v1","category":"cond-mat.soft"}
{"created":"2024-03-14 10:58:02","title":"Folding $\u03c0$","abstract":"It is well known that the set of origami constructible numbers is larger than the classical straight-edge and compass constructible numbers. However, the Huzita-Justin-Hatori origami constructible numbers remain algebraic so that the transcendental number $\\pi$ can only be approximated using a finite number of straight line folds. Using these methods we give a convergent sequence for folding $\\pi$ as well as other methods to approximate $\\pi$. Folding along curved creases, however, allows for the construction of transcendental numbers. We here give a method to construct $\\pi$ exactly by folding along a parabola, and we discuss generalizations for folding other transcendental numbers such as $\\Gamma(1/4)$.","sentences":["It is well known that the set of origami constructible numbers is larger than the classical straight-edge and compass constructible numbers.","However, the Huzita-Justin-Hatori origami constructible numbers remain algebraic so that the transcendental number $\\pi$ can only be approximated using a finite number of straight line folds.","Using these methods we give a convergent sequence for folding $\\pi$ as well as other methods to approximate $\\pi$. Folding along curved creases, however, allows for the construction of transcendental numbers.","We here give a method to construct $\\pi$ exactly by folding along a parabola, and we discuss generalizations for folding other transcendental numbers such as $\\Gamma(1/4)$."],"url":"http://arxiv.org/abs/2403.09277v1","category":"math.NT"}
{"created":"2024-03-14 10:55:36","title":"Static Grouping Strategy Design for Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"Beyond diagonal reconfigurable intelligent surface (BD-RIS) extends conventional RIS through novel architectures, such as group-connected RIS, with scattering matrix not restricted to being diagonal. However, it remains unexplored how to optimally group the elements in group-connected RISs to maximize the performance while maintaining a low-complexity circuit. In this study, we propose and model BD-RIS with a static grouping strategy optimized based on the channel statistics. After formulating the corresponding problems, we design the grouping in single- and multi-user systems. Numerical results reveal the benefits of grouping optimization, i.e., up to 60% sum rate improvement, especially in highly correlated channels.","sentences":["Beyond diagonal reconfigurable intelligent surface (BD-RIS) extends conventional RIS through novel architectures, such as group-connected RIS, with scattering matrix not restricted to being diagonal.","However, it remains unexplored how to optimally group the elements in group-connected RISs to maximize the performance while maintaining a low-complexity circuit.","In this study, we propose and model BD-RIS with a static grouping strategy optimized based on the channel statistics.","After formulating the corresponding problems, we design the grouping in single- and multi-user systems.","Numerical results reveal the benefits of grouping optimization, i.e., up to 60% sum rate improvement, especially in highly correlated channels."],"url":"http://arxiv.org/abs/2403.09275v1","category":"cs.IT"}
{"created":"2024-03-14 10:52:45","title":"EventRPG: Event Data Augmentation with Relevance Propagation Guidance","abstract":"Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range. Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability. Data augmentation is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field. However, there is no approach available for extracting saliency maps from SNNs. Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps. Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation. Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%. Our code is available at https://github.com/myuansun/EventRPG.","sentences":["Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range.","Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability.","Data augmentation is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field.","However, there is no approach available for extracting saliency maps from SNNs.","Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps.","Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation.","Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%.","Our code is available at https://github.com/myuansun/EventRPG."],"url":"http://arxiv.org/abs/2403.09274v1","category":"cs.CV"}
{"created":"2024-03-14 10:49:16","title":"Global Shipyard Capacities Limiting the Ramp-Up of Global Hydrogen-based Transportation","abstract":"Decarbonizing the global energy system requires significant expansions of renewable energy technologies. Given that cost-effective renewable sources are not necessarily situated in proximity to the largest energy demand centers globally, the maritime transportation of low-carbon energy carriers, such as renewable-based hydrogen or ammonia, will be needed. However, whether existent shipyards possess the required capacity to provide the necessary global fleet has not yet been answered. Therefore, this study estimates global tanker demand based on projections for global hydrogen demand, while comparing these projections with historic shipyard production. Our findings reveal a potential bottleneck until 2033-2039 if relying on liquefied hydrogen exclusively. This bottleneck could be circumvented by increasing local hydrogen production, utilizing pipelines, or liquefied ammonia as an energy carrier for hydrogen. Furthermore, the regional concentration of shipyard locations raises concerns about diversification. Increasing demand for container vessels could substantially hinder the scale-up of maritime hydrogen transport.","sentences":["Decarbonizing the global energy system requires significant expansions of renewable energy technologies.","Given that cost-effective renewable sources are not necessarily situated in proximity to the largest energy demand centers globally, the maritime transportation of low-carbon energy carriers, such as renewable-based hydrogen or ammonia, will be needed.","However, whether existent shipyards possess the required capacity to provide the necessary global fleet has not yet been answered.","Therefore, this study estimates global tanker demand based on projections for global hydrogen demand, while comparing these projections with historic shipyard production.","Our findings reveal a potential bottleneck until 2033-2039 if relying on liquefied hydrogen exclusively.","This bottleneck could be circumvented by increasing local hydrogen production, utilizing pipelines, or liquefied ammonia as an energy carrier for hydrogen.","Furthermore, the regional concentration of shipyard locations raises concerns about diversification.","Increasing demand for container vessels could substantially hinder the scale-up of maritime hydrogen transport."],"url":"http://arxiv.org/abs/2403.09272v1","category":"econ.GN"}
{"created":"2024-03-14 10:47:01","title":"A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces","abstract":"A reconfigurable intelligent surface (RIS) is a prospective wireless technology that enhances wireless channel quality. An RIS is often equipped with passive array of elements and provides cost and power-efficient solutions for coverage extension of wireless communication systems. Without any radio frequency (RF) chains or computing resources, however, the RIS requires control information to be sent to it from an external unit, e.g., a base station (BS). The control information can be delivered by wired or wireless channels, and the BS must be aware of the RIS and the RIS-related channel conditions in order to effectively configure its behavior. Recent works have introduced hybrid RIS structures possessing a few active elements that can sense and digitally process received data. Here, we propose the operation of an entirely autonomous RIS that operates without a control link between the RIS and BS. Using a few sensing elements, the autonomous RIS employs a deep Q network (DQN) based on reinforcement learning in order to enhance the sum rate of the network. Our results illustrate the potential of deploying autonomous RISs in wireless networks with essentially no network overhead.","sentences":["A reconfigurable intelligent surface (RIS) is a prospective wireless technology that enhances wireless channel quality.","An RIS is often equipped with passive array of elements and provides cost and power-efficient solutions for coverage extension of wireless communication systems.","Without any radio frequency (RF) chains or computing resources, however, the RIS requires control information to be sent to it from an external unit, e.g., a base station (BS).","The control information can be delivered by wired or wireless channels, and the BS must be aware of the RIS and the RIS-related channel conditions in order to effectively configure its behavior.","Recent works have introduced hybrid RIS structures possessing a few active elements that can sense and digitally process received data.","Here, we propose the operation of an entirely autonomous RIS that operates without a control link between the RIS and BS.","Using a few sensing elements, the autonomous RIS employs a deep Q network (DQN) based on reinforcement learning in order to enhance the sum rate of the network.","Our results illustrate the potential of deploying autonomous RISs in wireless networks with essentially no network overhead."],"url":"http://arxiv.org/abs/2403.09270v1","category":"cs.IT"}
{"created":"2024-03-14 10:46:45","title":"Direct observation of nanometer-scale orbital angular momentum accumulation","abstract":"Conversion of charge to orbital angular momentum through the orbital Hall effect (OHE) holds transformative potential for the development of orbital-based electronics, however, it is challenging to directly observe the electrically generated orbital accumulation. Here, we detect the OHE by directly quantifying the orbital accumulation along the edges of a titanium thin film using a scanning transmission electron microscope. We measure the Ti L-edge using electron energy-loss spectroscopy with nanometer resolution and find a sizable orbital accumulation at the sample's outer perimeters, consistent with all signatures expected for the OHE, and determine an orbital diffusion length $\\ell_o \\approx 7.3$ nm. Our data points to a surprising dependence of the orbital diffusion length on the nano-structural morphology.","sentences":["Conversion of charge to orbital angular momentum through the orbital Hall effect (OHE) holds transformative potential for the development of orbital-based electronics, however, it is challenging to directly observe the electrically generated orbital accumulation.","Here, we detect the OHE by directly quantifying the orbital accumulation along the edges of a titanium thin film using a scanning transmission electron microscope.","We measure the Ti L-edge using electron energy-loss spectroscopy with nanometer resolution and find a sizable orbital accumulation at the sample's outer perimeters, consistent with all signatures expected for the OHE, and determine an orbital diffusion length $\\ell_o \\approx 7.3$ nm.","Our data points to a surprising dependence of the orbital diffusion length on the nano-structural morphology."],"url":"http://arxiv.org/abs/2403.09269v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-14 10:41:31","title":"Zonal vs. Nodal Pricing: An Analysis of Different Pricing Rules in the German Day-Ahead Market","abstract":"The European electricity market is based on large pricing zones with a uniform day-ahead price. The energy transition leads to shifts in supply and demand and increasing redispatch costs. In an attempt to ensure efficient market clearing and congestion management, the EU Commission has mandated the Bidding Zone Review (BZR) to reevaluate the configuration of European bidding zones. Based on a unique data set published in the context of the BZR, we compare various pricing rules for the German power market. We compare market clearing and pricing for national, zonal, and nodal models, including their generation costs and associated redispatch costs. Moreover, we investigate different non-uniform pricing rules and their economic implications for the German electricity market. Our results indicate that the differences in the average prices in different zones are small. The total costs across different configurations are similar and the reduction of standard deviations in prices is also small based on this data set. A nodal pricing rule leads to the lowest total costs. We also analyze the quality of different pricing rules and their differences with respect to the quality of the price signals and the necessary uplift payments. While the study focuses on Germany, the analysis is relevant beyond and feeds into the broader discussion about pricing rules.","sentences":["The European electricity market is based on large pricing zones with a uniform day-ahead price.","The energy transition leads to shifts in supply and demand and increasing redispatch costs.","In an attempt to ensure efficient market clearing and congestion management, the EU Commission has mandated the Bidding Zone Review (BZR) to reevaluate the configuration of European bidding zones.","Based on a unique data set published in the context of the BZR, we compare various pricing rules for the German power market.","We compare market clearing and pricing for national, zonal, and nodal models, including their generation costs and associated redispatch costs.","Moreover, we investigate different non-uniform pricing rules and their economic implications for the German electricity market.","Our results indicate that the differences in the average prices in different zones are small.","The total costs across different configurations are similar and the reduction of standard deviations in prices is also small based on this data set.","A nodal pricing rule leads to the lowest total costs.","We also analyze the quality of different pricing rules and their differences with respect to the quality of the price signals and the necessary uplift payments.","While the study focuses on Germany, the analysis is relevant beyond and feeds into the broader discussion about pricing rules."],"url":"http://arxiv.org/abs/2403.09265v1","category":"econ.GN"}
{"created":"2024-03-14 10:35:27","title":"Hadamard property of the Unruh state for massless fermions on Kerr spacetime : the large $a$ case","abstract":"In a recent paper by G\\'erard, H\\\"afner, and Wrochna, the Unruh state for massless fermions on a Kerr spacetime was constructed and the authors showed its Hadmard property in the case of very slowly rotating black holes $\\vert a\\vert\\ll M$. In this note, we extend this result to the full non extreme case $\\vert a\\vert<M$.","sentences":["In a recent paper by G\\'erard, H\\\"afner, and Wrochna, the Unruh state for massless fermions on a Kerr spacetime was constructed and the authors showed its Hadmard property in the case of very slowly rotating black holes $\\vert a\\vert\\ll M$. In this note, we extend this result to the full non extreme case $\\vert a\\vert<M$."],"url":"http://arxiv.org/abs/2403.09261v1","category":"math-ph"}
{"created":"2024-03-14 10:22:01","title":"Gun Culture in Fringe Social Media","abstract":"The increasing frequency of mass shootings in the United States has, unfortunately, become a norm. While the issue of gun control in the US involves complex legal concerns, there are also societal issues at play. One such social issue is so-called \"gun culture,\" i.e., a general set of beliefs and actions related to gun ownership. However relatively little is known about gun culture, and even less is known when it comes to fringe online communities. This is especially worrying considering the aforementioned rise in mass shootings and numerous instances of shooters being radicalized online.   To address this gap, we explore gun culture on /k/, 4chan's weapons board. More specifically, using a variety of quantitative techniques, we examine over 4M posts on /k/ and position their discussion within the larger body of theoretical understanding of gun culture. Among other things, our findings suggest that gun culture on /k/ covers a relatively diverse set of topics (with a particular focus on legal discussion), some of which are signals of fetishism.","sentences":["The increasing frequency of mass shootings in the United States has, unfortunately, become a norm.","While the issue of gun control in the US involves complex legal concerns, there are also societal issues at play.","One such social issue is so-called \"gun culture,\" i.e., a general set of beliefs and actions related to gun ownership.","However relatively little is known about gun culture, and even less is known when it comes to fringe online communities.","This is especially worrying considering the aforementioned rise in mass shootings and numerous instances of shooters being radicalized online.   ","To address this gap, we explore gun culture on /k/, 4chan's weapons board.","More specifically, using a variety of quantitative techniques, we examine over 4M posts on /k/ and position their discussion within the larger body of theoretical understanding of gun culture.","Among other things, our findings suggest that gun culture on /k/","covers a relatively diverse set of topics (with a particular focus on legal discussion), some of which are signals of fetishism."],"url":"http://arxiv.org/abs/2403.09254v1","category":"cs.SI"}
{"created":"2024-03-14 10:21:39","title":"Broadband NIR photon upconversion generates NIR persistent luminescence for bioimaging","abstract":"Upconversion persistent luminescence (UCPL) phosphors that can be directly charged by near-infrared (NIR) light have gained considerable attention due to their promising applications ranging from photonics to biomedicine. However, current lanthanide-based UCPL phosphors show small absorption cross-sections and low upconversion charging efficiency. The development of UCPL phosphors faces challenges of lacking flexible upconversion charging pathways and poor design flexibility. Herein, we discovered a new lattice defect-mediated broadband photon upconversion process and the accompanied NIR-to-NIR UCPL in Cr-doped zinc gallate nanoparticles. The zinc gallate nanoparticles can be directly activated by broadband NIR light in the 700-1000 nm range to produce persistent luminescence at about 700 nm, which is also readily enhanced by rationally tailoring the lattice defects in the phosphors. This proposed UCPL phosphors achieved a signal-to-background ratio of over 200 in bioimaging by efficiently avoiding interference from autofluorescence and light scattering. Our findings reported the lattice defect-mediated photon upconversion for the first time, which significantly expanded the horizons for the flexible design of NIR-to-NIR UCPL phosphors toward broad applications.","sentences":["Upconversion persistent luminescence (UCPL) phosphors that can be directly charged by near-infrared (NIR) light have gained considerable attention due to their promising applications ranging from photonics to biomedicine.","However, current lanthanide-based UCPL phosphors show small absorption cross-sections and low upconversion charging efficiency.","The development of UCPL phosphors faces challenges of lacking flexible upconversion charging pathways and poor design flexibility.","Herein, we discovered a new lattice defect-mediated broadband photon upconversion process and the accompanied NIR-to-NIR UCPL in Cr-doped zinc gallate nanoparticles.","The zinc gallate nanoparticles can be directly activated by broadband NIR light in the 700-1000 nm range to produce persistent luminescence at about 700 nm, which is also readily enhanced by rationally tailoring the lattice defects in the phosphors.","This proposed UCPL phosphors achieved a signal-to-background ratio of over 200 in bioimaging by efficiently avoiding interference from autofluorescence and light scattering.","Our findings reported the lattice defect-mediated photon upconversion for the first time, which significantly expanded the horizons for the flexible design of NIR-to-NIR UCPL phosphors toward broad applications."],"url":"http://arxiv.org/abs/2403.09253v1","category":"physics.optics"}
{"created":"2024-03-14 10:20:28","title":"Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory","abstract":"The recent paper (IEEE Trans. IT 69, 1680) introduced an analytical method for calculating the channel capacity without the need for iteration. This method has certain limitations that restrict its applicability. Furthermore, the paper does not provide an explanation as to why the channel capacity can be solved analytically in this particular case. In order to broaden the scope of this method and address its limitations, we turn our attention to the reverse em-problem, proposed by Toyota (Information Geometry, 3, 1355 (2020)). This reverse em-problem involves iteratively applying the inverse map of the em iteration to calculate the channel capacity, which represents the maximum mutual information. However, several open problems remained unresolved in Toyota's work. To overcome these challenges, we formulate the reverse em-problem based on Bregman divergence and provide solutions to these open problems. Building upon these results, we transform the reverse em-problem into em-problems and derive a non-iterative formula for the reverse em-problem. This formula can be viewed as a generalization of the aforementioned analytical calculation method. Importantly, this derivation sheds light on the information geometrical structure underlying this special case. By effectively addressing the limitations of the previous analytical method and providing a deeper understanding of the underlying information geometrical structure, our work significantly expands the applicability of the proposed method for calculating the channel capacity without iteration.","sentences":["The recent paper (IEEE Trans.","IT 69, 1680) introduced an analytical method for calculating the channel capacity without the need for iteration.","This method has certain limitations that restrict its applicability.","Furthermore, the paper does not provide an explanation as to why the channel capacity can be solved analytically in this particular case.","In order to broaden the scope of this method and address its limitations, we turn our attention to the reverse em-problem, proposed by Toyota (Information Geometry, 3, 1355 (2020)).","This reverse em-problem involves iteratively applying the inverse map of the em iteration to calculate the channel capacity, which represents the maximum mutual information.","However, several open problems remained unresolved in Toyota's work.","To overcome these challenges, we formulate the reverse em-problem based on Bregman divergence and provide solutions to these open problems.","Building upon these results, we transform the reverse em-problem into em-problems and derive a non-iterative formula for the reverse em-problem.","This formula can be viewed as a generalization of the aforementioned analytical calculation method.","Importantly, this derivation sheds light on the information geometrical structure underlying this special case.","By effectively addressing the limitations of the previous analytical method and providing a deeper understanding of the underlying information geometrical structure, our work significantly expands the applicability of the proposed method for calculating the channel capacity without iteration."],"url":"http://arxiv.org/abs/2403.09252v1","category":"cs.IT"}
{"created":"2024-03-14 10:16:57","title":"Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem","abstract":"Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep reinforcement learning (DRL) due to its ability to generate high-quality, real-time solutions. However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances. This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both. In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance. Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP for optimal resolution as the problem is simplified. Our hybrid approach has been extensively tested on three public FJSSP benchmarks, demonstrating superior performance over five state-of-the-art DRL approaches and a widely-used CP solver. Additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known DRL method.","sentences":["Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep reinforcement learning (DRL) due to its ability to generate high-quality, real-time solutions.","However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances.","This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both.","In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance.","Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP for optimal resolution as the problem is simplified.","Our hybrid approach has been extensively tested on three public FJSSP benchmarks, demonstrating superior performance over five state-of-the-art DRL approaches and a widely-used CP solver.","Additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known DRL method."],"url":"http://arxiv.org/abs/2403.09249v1","category":"cs.AI"}
{"created":"2024-03-14 10:16:02","title":"Eulerian magnitude homology: subgraph structure and random graphs","abstract":"In this paper we explore the connection between the ranks of the magnitude homology groups of a graph and the structure of its subgraphs. To this end, we introduce variants of magnitude homology called eulerian magnitude homology and discriminant magnitude homology. Leveraging the combinatorics of the differential in magnitude homology, we illustrate a close relationship between the ranks of the eulerian magnitude homology groups on the first diagonal and counts of subgraphs which fall in specific classes. We leverage these tools to study limiting behavior of the eulerian magnitude homology groups for Erdos-Renyi random graphs and random geometric graphs, producing for both models a vanishing threshold for the eulerian magnitude homology groups on the first diagonal. This in turn provides a characterization of the generators for the corresponding magnitude homology groups. Finally, we develop an explicit asymptotic estimate the expected rank of eulerian magnitude homology along the first diagonal for these random graph models.","sentences":["In this paper we explore the connection between the ranks of the magnitude homology groups of a graph and the structure of its subgraphs.","To this end, we introduce variants of magnitude homology called eulerian magnitude homology and discriminant magnitude homology.","Leveraging the combinatorics of the differential in magnitude homology, we illustrate a close relationship between the ranks of the eulerian magnitude homology groups on the first diagonal and counts of subgraphs which fall in specific classes.","We leverage these tools to study limiting behavior of the eulerian magnitude homology groups for Erdos-Renyi random graphs and random geometric graphs, producing for both models a vanishing threshold for the eulerian magnitude homology groups on the first diagonal.","This in turn provides a characterization of the generators for the corresponding magnitude homology groups.","Finally, we develop an explicit asymptotic estimate the expected rank of eulerian magnitude homology along the first diagonal for these random graph models."],"url":"http://arxiv.org/abs/2403.09248v1","category":"math.CO"}
{"created":"2024-03-14 10:13:40","title":"On the speed of propagation in Turing patterns for reaction-diffusion systems","abstract":"This study investigates transient wave dynamics in Turing pattern formation, focusing on waves emerging from localised disturbances. While the traditional focus of diffusion-driven instability has primarily centred on stationary solutions, considerable attention has also been directed towards understanding spatio-temporal behaviours, particularly the propagation of patterning from localised disturbances. We analyse these waves of patterning using both the well-established marginal stability criterion and weakly nonlinear analysis with envelope equations. Both methods provide estimates for the wave speed but the latter method, in addition, approximates the wave profile and amplitude. We then compare these two approaches analytically near a bifurcation point and reveal that the marginal stability criterion yields exactly the same estimate for the wave speed as the weakly nonlinear analysis. Furthermore, we evaluate these estimates against numerical results for Schnakenberg and CDIMA (chlorine dioxide-iodine-malonic acid) kinetics. In particular, our study emphasises the importance of the characteristic speed of pattern propagation, determined by diffusion dynamics and a complex relation with the reaction kinetics in Turing systems. This speed serves as a vital parameter for comparison with experimental observations, akin to observed pattern length scales. Furthermore, more generally, our findings provide systematic methodologies for analysing transient wave properties in Turing systems, generating insight into the dynamic evolution of pattern formation.","sentences":["This study investigates transient wave dynamics in Turing pattern formation, focusing on waves emerging from localised disturbances.","While the traditional focus of diffusion-driven instability has primarily centred on stationary solutions, considerable attention has also been directed towards understanding spatio-temporal behaviours, particularly the propagation of patterning from localised disturbances.","We analyse these waves of patterning using both the well-established marginal stability criterion and weakly nonlinear analysis with envelope equations.","Both methods provide estimates for the wave speed but the latter method, in addition, approximates the wave profile and amplitude.","We then compare these two approaches analytically near a bifurcation point and reveal that the marginal stability criterion yields exactly the same estimate for the wave speed as the weakly nonlinear analysis.","Furthermore, we evaluate these estimates against numerical results for Schnakenberg and CDIMA (chlorine dioxide-iodine-malonic acid) kinetics.","In particular, our study emphasises the importance of the characteristic speed of pattern propagation, determined by diffusion dynamics and a complex relation with the reaction kinetics in Turing systems.","This speed serves as a vital parameter for comparison with experimental observations, akin to observed pattern length scales.","Furthermore, more generally, our findings provide systematic methodologies for analysing transient wave properties in Turing systems, generating insight into the dynamic evolution of pattern formation."],"url":"http://arxiv.org/abs/2403.09247v1","category":"nlin.PS"}
{"created":"2024-03-14 10:06:11","title":"A simple reconstruction method to infer nonreciprocal interactions and local driving in complex systems","abstract":"Data-based inference of directed interactions in complex dynamical systems is a problem common to many disciplines of science. In this work, we study networks of spatially separate dynamical entities, which could represent physical systems that interact with each other by reciprocal or nonreciprocal, instantaneous or time-delayed interactions. We present a simple approach that combines Markov state models with directed information-theoretical measures for causal inference that can accurately infer the underlying interactions from noisy time series of the dynamical system states alone. Remarkably, this is possible despite the built-in simplification of a Markov assumption and the choice of a very coarse discretization at the level of probability estimation. Our test systems are an Ising chain with nonreciprocal coupling imposed by local driving of a single spin, and a system of delay-coupled linear stochastic processes. Stepping away from physical systems, the approach infers cause-effect relationships, or more generally, the direction of mutual or one-way influence. The presented method is agnostic to the number of interacting entities and details of the dynamics, so that it is widely applicable to problems in various fields.","sentences":["Data-based inference of directed interactions in complex dynamical systems is a problem common to many disciplines of science.","In this work, we study networks of spatially separate dynamical entities, which could represent physical systems that interact with each other by reciprocal or nonreciprocal, instantaneous or time-delayed interactions.","We present a simple approach that combines Markov state models with directed information-theoretical measures for causal inference that can accurately infer the underlying interactions from noisy time series of the dynamical system states alone.","Remarkably, this is possible despite the built-in simplification of a Markov assumption and the choice of a very coarse discretization at the level of probability estimation.","Our test systems are an Ising chain with nonreciprocal coupling imposed by local driving of a single spin, and a system of delay-coupled linear stochastic processes.","Stepping away from physical systems, the approach infers cause-effect relationships, or more generally, the direction of mutual or one-way influence.","The presented method is agnostic to the number of interacting entities and details of the dynamics, so that it is widely applicable to problems in various fields."],"url":"http://arxiv.org/abs/2403.09243v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 10:03:58","title":"XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model","abstract":"Large-scale generative models have demonstrated impressive capacity in producing visually compelling images, with increasing applications in medical imaging. However, they continue to grapple with the challenge of image hallucination and the generation of anatomically inaccurate outputs. These limitations are mainly due to the sole reliance on textual inputs and lack of spatial control over the generated images, hindering the potential usefulness of such models in real-life settings. We present XReal, a novel controllable diffusion model for generating realistic chest X-ray images through precise anatomy and pathology location control. Our lightweight method can seamlessly integrate spatial control in a pre-trained text-to-image diffusion model without fine-tuning, retaining its existing knowledge while enhancing its generation capabilities. XReal outperforms state-of-the-art x-ray diffusion models in quantitative and qualitative metrics while showing 13% and 10% anatomy and pathology realism gain, respectively, based on the expert radiologist evaluation. Our model holds promise for advancing generative models in medical imaging, offering greater precision and adaptability while inviting further exploration in this evolving field. A large synthetically generated data with annotations and code is publicly available at https://github.com/BioMedIA-MBZUAI/XReal.","sentences":["Large-scale generative models have demonstrated impressive capacity in producing visually compelling images, with increasing applications in medical imaging.","However, they continue to grapple with the challenge of image hallucination and the generation of anatomically inaccurate outputs.","These limitations are mainly due to the sole reliance on textual inputs and lack of spatial control over the generated images, hindering the potential usefulness of such models in real-life settings.","We present XReal, a novel controllable diffusion model for generating realistic chest X-ray images through precise anatomy and pathology location control.","Our lightweight method can seamlessly integrate spatial control in a pre-trained text-to-image diffusion model without fine-tuning, retaining its existing knowledge while enhancing its generation capabilities.","XReal outperforms state-of-the-art x-ray diffusion models in quantitative and qualitative metrics while showing 13% and 10% anatomy and pathology realism gain, respectively, based on the expert radiologist evaluation.","Our model holds promise for advancing generative models in medical imaging, offering greater precision and adaptability while inviting further exploration in this evolving field.","A large synthetically generated data with annotations and code is publicly available at https://github.com/BioMedIA-MBZUAI/XReal."],"url":"http://arxiv.org/abs/2403.09240v1","category":"eess.IV"}
{"created":"2024-03-14 09:59:55","title":"Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph","abstract":"Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named ``Geometry and Texture Hypergraph Refiner (HGRefiner)''. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: https://github.com/yjhboy/Hyper3DG)","sentences":["Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models.","However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem.","In this work, we propose a method named ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects.","Our framework is anchored by a well-established mainflow and an essential module, named ``Geometry and Texture Hypergraph Refiner (HGRefiner)''.","This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features.","Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation.","Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework.","(Project code: https://github.com/yjhboy/Hyper3DG)"],"url":"http://arxiv.org/abs/2403.09236v1","category":"cs.CV"}
{"created":"2024-03-14 09:57:50","title":"Infrared structure beyond locality in electrodynamics","abstract":"The infrared problems of quantum electrodynamics, in contrast to ultraviolet difficulties which are of technical nature, are related to fundamental, conceptual physical questions, such as: what is a charged particle, is the particle interpretation of the electromagnetic field complete, does a vacuum state exist, or what is the quantum status of long range degrees of freedom. On the calculational level, the standard local formulations of quantum field theory have achieved procedures to deal with infinities related to long range correlations. However, the answers to the conceptual questions formulated above, based on the locality paradigm, do not seem to be fully convincing, which is confirmed by the fact that no canonical picture did emerge. This contribution briefly characterizes perspectives which open with an admission of nonlocal variables residing in infinity, or at the boundary of spacetime after compactification. Recently, this line of investigation gains popularity.","sentences":["The infrared problems of quantum electrodynamics, in contrast to ultraviolet difficulties which are of technical nature, are related to fundamental, conceptual physical questions, such as: what is a charged particle, is the particle interpretation of the electromagnetic field complete, does a vacuum state exist, or what is the quantum status of long range degrees of freedom.","On the calculational level, the standard local formulations of quantum field theory have achieved procedures to deal with infinities related to long range correlations.","However, the answers to the conceptual questions formulated above, based on the locality paradigm, do not seem to be fully convincing, which is confirmed by the fact that no canonical picture did emerge.","This contribution briefly characterizes perspectives which open with an admission of nonlocal variables residing in infinity, or at the boundary of spacetime after compactification.","Recently, this line of investigation gains popularity."],"url":"http://arxiv.org/abs/2403.09234v1","category":"math-ph"}
{"created":"2024-03-14 09:56:35","title":"Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes","abstract":"In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics. Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the reasoning behind the predictions. This growing concern has sparked the introduction of counterfactual explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions. The generation of counterfactual explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics. Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible counterfactual explanations. First, we restrict the counterfactual algorithm to generate counterfactuals that lie within a high-density region of the process data, ensuring that the proposed counterfactuals are realistic and feasible within the observed process data distribution. Additionally, we ensure plausibility by learning sequential patterns between the activities in the process cases, utilising Declare language templates. Finally, we evaluate the properties that define the validity of counterfactuals.","sentences":["In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics.","Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the reasoning behind the predictions.","This growing concern has sparked the introduction of counterfactual explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions.","The generation of counterfactual explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics.","Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible counterfactual explanations.","First, we restrict the counterfactual algorithm to generate counterfactuals that lie within a high-density region of the process data, ensuring that the proposed counterfactuals are realistic and feasible within the observed process data distribution.","Additionally, we ensure plausibility by learning sequential patterns between the activities in the process cases, utilising Declare language templates.","Finally, we evaluate the properties that define the validity of counterfactuals."],"url":"http://arxiv.org/abs/2403.09232v1","category":"cs.AI"}
{"created":"2024-03-14 09:54:31","title":"Improving Distant 3D Object Detection Using 2D Box Supervision","abstract":"Improving the detection of distant 3d objects is an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant objects. LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects. This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.","sentences":["Improving the detection of distant 3d objects is an important yet challenging task.","For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information.","As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios.","We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate.","We propose LR3D, a framework that learns to recover the missing depth of distant objects.","LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects.","This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible.","Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision.","Our framework is general, and could widely benefit 3D detection methods to a large extent."],"url":"http://arxiv.org/abs/2403.09230v1","category":"cs.CV"}
{"created":"2024-03-14 09:48:36","title":"BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation","abstract":"We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on \"what do you want robots to do for you?\". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.","sentences":["We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics.","BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on \"what do you want robots to do for you?\".","The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties.","The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids.","Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions.","To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart.","We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research.","Project website: https://behavior.stanford.edu."],"url":"http://arxiv.org/abs/2403.09227v1","category":"cs.RO"}
{"created":"2024-03-14 09:45:05","title":"Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records","abstract":"Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data. We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting. Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting.","sentences":["Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization.","Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries.","Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data.","We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting.","Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting."],"url":"http://arxiv.org/abs/2403.09226v1","category":"cs.CL"}
{"created":"2024-03-14 09:43:24","title":"PWACG: Partial Wave Analysis Code Generator supporting Newton-conjugate gradient method","abstract":"This paper introduces a novel Partial Wave Analysis Code Generator (PWACG) that automatically generates high-performance partial wave analysis codes. This is achieved by leveraging the JAX automatic differentiation library and the jinja2 template engine. The resulting code is constructed using the high-performance API of JAX, and includes support for the Newton's Conjugate Gradient optimization method, as well as the full utilization of parallel computing capabilities offered by GPUs. By harnessing these advanced computing techniques, PWACG demonstrates a significant advantage in efficiently identifying global optimal points compared to conventional partial wave analysis software packages.","sentences":["This paper introduces a novel Partial Wave Analysis Code Generator (PWACG) that automatically generates high-performance partial wave analysis codes.","This is achieved by leveraging the JAX automatic differentiation library and the jinja2 template engine.","The resulting code is constructed using the high-performance API of JAX, and includes support for the Newton's Conjugate Gradient optimization method, as well as the full utilization of parallel computing capabilities offered by GPUs.","By harnessing these advanced computing techniques, PWACG demonstrates a significant advantage in efficiently identifying global optimal points compared to conventional partial wave analysis software packages."],"url":"http://arxiv.org/abs/2403.09225v1","category":"physics.comp-ph"}
{"created":"2024-03-14 09:43:23","title":"A new approach towards quantum foundation and some consequences","abstract":"A general theory based upon 6 postulates is introduced. The basical notions are theoretical variables that are associated with an observer or with a group of communicating observers. These variables may be accessible or inaccessible. From these postulates, the ordinary formalism of quantum theory are derived. The mathematical derivations are not given in this article, but I refer to the recent articles [9, 10]. Three possible applications of the general theory can be given; 1) The variables may decision variables connected to the decisions of a person or of a group of persons. 2) The variables may be statistical parameters or future data, But most importantly here: 3) The variables are physical variables in some context. This last application gives a completely new foundation of quantum mechanics, a foundation which in my opinion is much more easy to understand than the ordinary formalism.The other applications seem also to give interesting consequences of the approach. Socalled paradoxes like that of Schr\\\"odinger's cat can be clarified under the theory. Explanations of the outcomes of David Bohm's version of the EPR experiment and of the Bell experiment are provided. Finally, references to links towards relativity theory and to quantum field theory are given.","sentences":["A general theory based upon 6 postulates is introduced.","The basical notions are theoretical variables that are associated with an observer or with a group of communicating observers.","These variables may be accessible or inaccessible.","From these postulates, the ordinary formalism of quantum theory are derived.","The mathematical derivations are not given in this article, but I refer to the recent articles","[9, 10].","Three possible applications of the general theory can be given; 1) The variables may decision variables connected to the decisions of a person or of a group of persons.","2)","The variables may be statistical parameters or future data, But most importantly here: 3)","The variables are physical variables in some context.","This last application gives a completely new foundation of quantum mechanics, a foundation which in my opinion is much more easy to understand than the ordinary formalism.","The other applications seem also to give interesting consequences of the approach.","Socalled paradoxes like that of Schr\\\"odinger's cat can be clarified under the theory.","Explanations of the outcomes of David Bohm's version of the EPR experiment and of the Bell experiment are provided.","Finally, references to links towards relativity theory and to quantum field theory are given."],"url":"http://arxiv.org/abs/2403.09224v1","category":"quant-ph"}
{"created":"2024-03-14 09:43:07","title":"MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer","abstract":"The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting. In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence). Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy. The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features. However, the CI strategy faces the challenge of interchannel correlation forgetting. To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting. Based on this strategy, we introduce MCformer, a multivariate time-series forecasting model with mixed channel features. The model blends a specific number of channels, leveraging an attention mechanism to effectively capture inter-channel correlation information when modeling long-term features. Experimental results demonstrate that the Mixed Channels strategy outperforms pure CI strategy in multivariate time-series forecasting tasks.","sentences":["The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting.","In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence).","Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy.","The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features.","However, the CI strategy faces the challenge of interchannel correlation forgetting.","To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting.","Based on this strategy, we introduce MCformer, a multivariate time-series forecasting model with mixed channel features.","The model blends a specific number of channels, leveraging an attention mechanism to effectively capture inter-channel correlation information when modeling long-term features.","Experimental results demonstrate that the Mixed Channels strategy outperforms pure CI strategy in multivariate time-series forecasting tasks."],"url":"http://arxiv.org/abs/2403.09223v1","category":"cs.LG"}
{"created":"2024-03-14 09:38:39","title":"Demonstration of universal contextuality through communication games free of both operational inequivalence and compatibility loopholes","abstract":"Universal contextuality is the leading notion of non-classicality even for single systems, showing its advantage as a more general quantum correlation than Bell non-locality, as well as preparation contextuality. However, a loophole-free experimental demonstration of universal contextuality at least requires that both operational inequivalence and compatibility loopholes are closed, which have never been simultaneously achieved to date. In our work, we experimentally test universal contextuality through (3,3) and (4,3) communication games, simultaneously restoring operational equivalence and circumventing the compatibility loophole. Our result exhibits the violation of universal non-contextuality bound by 97 standard deviations in (3,3) scenario, and 107 deviations in (4,3) scenario. Notably there are states which exhibit locality but reveal universal contextuality in both two scenarios. In addition, our result shows that universal contextuality is more general than preparation contextuality in (3,3) scenario, while equivalent to preparation contextuality in (4,3) scenario.","sentences":["Universal contextuality is the leading notion of non-classicality even for single systems, showing its advantage as a more general quantum correlation than Bell non-locality, as well as preparation contextuality.","However, a loophole-free experimental demonstration of universal contextuality at least requires that both operational inequivalence and compatibility loopholes are closed, which have never been simultaneously achieved to date.","In our work, we experimentally test universal contextuality through (3,3) and (4,3) communication games, simultaneously restoring operational equivalence and circumventing the compatibility loophole.","Our result exhibits the violation of universal non-contextuality bound by 97 standard deviations in (3,3) scenario, and 107 deviations in (4,3) scenario.","Notably there are states which exhibit locality but reveal universal contextuality in both two scenarios.","In addition, our result shows that universal contextuality is more general than preparation contextuality in (3,3) scenario, while equivalent to preparation contextuality in (4,3) scenario."],"url":"http://arxiv.org/abs/2403.09220v1","category":"quant-ph"}
{"created":"2024-03-14 09:37:54","title":"An Extensive Comparison of Static Application Security Testing Tools","abstract":"Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications. Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision. Aim: We aim to comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives. Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase. It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis. Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall. Conclusions: The paper suggests that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions.","sentences":["Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications.","Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision.","Aim: We aim to comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives.","Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase.","It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis.","Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities.","In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall.","Conclusions: The paper suggests that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions."],"url":"http://arxiv.org/abs/2403.09219v1","category":"cs.SE"}
{"created":"2024-03-14 09:28:28","title":"On the Laplace Approximation as Model Selection Criterion for Gaussian Processes","abstract":"Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once. In this work, we focus on evaluating model performance of Gaussian process models, i.e. finding a metric that provides the best trade-off between all those criteria. While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability. We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation. Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed. Our model selection criteria allow significantly faster and high quality model selection of Gaussian process models.","sentences":["Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once.","In this work, we focus on evaluating model performance of Gaussian process models, i.e. finding a metric that provides the best trade-off between all those criteria.","While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability.","We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation.","Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed.","Our model selection criteria allow significantly faster and high quality model selection of Gaussian process models."],"url":"http://arxiv.org/abs/2403.09215v1","category":"cs.LG"}
{"created":"2024-03-14 09:28:12","title":"PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest","abstract":"In this work, we present PoIFusion, a simple yet effective multi-modal 3D object detection framework to fuse the information of RGB images and LiDAR point clouds at the point of interest (abbreviated as PoI). Technically, our PoIFusion follows the paradigm of query-based object detection, formulating object queries as dynamic 3D boxes. The PoIs are adaptively generated from each query box on the fly, serving as the keypoints to represent a 3D object and play the role of basic units in multi-modal fusion. Specifically, we project PoIs into the view of each modality to sample the corresponding feature and integrate the multi-modal features at each PoI through a dynamic fusion block. Furthermore, the features of PoIs derived from the same query box are aggregated together to update the query feature. Our approach prevents information loss caused by view transformation and eliminates the computation-intensive global attention, making the multi-modal 3D object detector more applicable. We conducted extensive experiments on the nuScenes dataset to evaluate our approach. Remarkably, our PoIFusion achieves 74.9\\% NDS and 73.4\\% mAP, setting a state-of-the-art record on the multi-modal 3D object detection benchmark. Codes will be made available via \\url{https://djiajunustc.github.io/projects/poifusion}.","sentences":["In this work, we present PoIFusion, a simple yet effective multi-modal 3D object detection framework to fuse the information of RGB images and LiDAR point clouds at the point of interest (abbreviated as PoI).","Technically, our PoIFusion follows the paradigm of query-based object detection, formulating object queries as dynamic 3D boxes.","The PoIs are adaptively generated from each query box on the fly, serving as the keypoints to represent a 3D object and play the role of basic units in multi-modal fusion.","Specifically, we project PoIs into the view of each modality to sample the corresponding feature and integrate the multi-modal features at each PoI through a dynamic fusion block.","Furthermore, the features of PoIs derived from the same query box are aggregated together to update the query feature.","Our approach prevents information loss caused by view transformation and eliminates the computation-intensive global attention, making the multi-modal 3D object detector more applicable.","We conducted extensive experiments on the nuScenes dataset to evaluate our approach.","Remarkably, our PoIFusion achieves 74.9\\% NDS and 73.4\\% mAP, setting a state-of-the-art record on the multi-modal 3D object detection benchmark.","Codes will be made available via \\url{https://djiajunustc.github.io/projects/poifusion}."],"url":"http://arxiv.org/abs/2403.09212v1","category":"cs.CV"}
{"created":"2024-03-14 09:24:00","title":"Energy flux and waveforms by coalescing spinless binary system in effective one-body theory","abstract":"We present a study on the energy radiation rate and waveforms of the gravitational wave generated by coalescing spinless binary systems up to the third post-Minkowskian approximation in the effective one-body theory. To derive an analytical expansion of the null tetrad components of the gravitational perturbed Weyl tensor $\\varPsi_{4}$ in the effective spacetime, we utilize the method proposed by Sasaki $et$ $al.$ During this investigation, we discover more general integral formulas that provide a theoretical framework for computing the results in any order. Subsequently, we successfully compute the energy radiation rate and waveforms of the gravitational wave, which include the results of the Schwarzschild case and the correction terms resulting from the dimensionless parameters $a_{2}$ and $a_{3}$ in the effective metric.","sentences":["We present a study on the energy radiation rate and waveforms of the gravitational wave generated by coalescing spinless binary systems up to the third post-Minkowskian approximation in the effective one-body theory.","To derive an analytical expansion of the null tetrad components of the gravitational perturbed Weyl tensor $\\varPsi_{4}$ in the effective spacetime, we utilize the method proposed by Sasaki $et$ $al.$ During this investigation, we discover more general integral formulas that provide a theoretical framework for computing the results in any order.","Subsequently, we successfully compute the energy radiation rate and waveforms of the gravitational wave, which include the results of the Schwarzschild case and the correction terms resulting from the dimensionless parameters $a_{2}$ and $a_{3}$ in the effective metric."],"url":"http://arxiv.org/abs/2403.09211v1","category":"gr-qc"}
{"created":"2024-03-14 09:22:17","title":"LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection","abstract":"Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between activities across sequences with graph structure learning. Moreover, to mitigate the data imbalance problem in ITD, we propose a novel hybrid prediction loss, which integrates self-supervision signals {from normal activities} and supervision signals from abnormal activities into a unified loss for anomaly detection. We evaluate the performance of LAN on two widely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative experiments demonstrate the superiority of LAN, outperforming 9 state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD on CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in AUC on two datasets. Finally, the ablation study, parameter analysis, and compatibility analysis evaluate the impact of each module and hyper-parameter in LAN.","sentences":["Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences.","Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day).","However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results.","On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss.","In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN.","Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between activities across sequences with graph structure learning.","Moreover, to mitigate the data imbalance problem in ITD, we propose a novel hybrid prediction loss, which integrates self-supervision signals {from normal activities} and supervision signals from abnormal activities into a unified loss for anomaly detection.","We evaluate the performance of LAN on two widely used datasets, i.e., CERT r4.2 and CERT r5.2.","Extensive and comparative experiments demonstrate the superiority of LAN, outperforming 9 state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD on CERT r4.2 and r5.2, respectively.","Moreover, LAN can be also applied to post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in AUC on two datasets.","Finally, the ablation study, parameter analysis, and compatibility analysis evaluate the impact of each module and hyper-parameter in LAN."],"url":"http://arxiv.org/abs/2403.09209v1","category":"cs.CR"}
{"created":"2024-03-14 09:19:50","title":"Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM","abstract":"Concept Bottleneck Model (CBM) is a methods for explaining neural networks. In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values. It is expected that we can interpret the relationship between the output and concept similar to linear regression. However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks. Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties. Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model. In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture. The result indcates that the structure of partially observed concepts decreases the Bayesian generalization error compared with that of CBM (full-observed concepts).","sentences":["Concept Bottleneck Model (CBM) is a methods for explaining neural networks.","In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values.","It is expected that we can interpret the relationship between the output and concept similar to linear regression.","However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks.","Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties.","Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model.","In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture.","The result indcates that the structure of partially observed concepts decreases the Bayesian generalization error compared with that of CBM (full-observed concepts)."],"url":"http://arxiv.org/abs/2403.09206v1","category":"stat.ML"}
{"created":"2024-03-14 09:18:12","title":"Remarks on integrability of N=1 supersymmetric Ruijsenaars-Schneider three-body models","abstract":"Integrability of N=1 supersymmetric Ruijsenaars-Schneider three-body models based upon the potentials W(x)=2/x, W(x)=2/sin(x), and W(x)=2/sinh(x) is proven. The problem of constructing an algebraically resolvable set of Grassmann-odd constants of motion is reduced to finding a triplet of vectors such that all their scalar products can be expressed in terms of the original bosonic first integrals. The supersymmetric generalizations are used to build novel integrable (iso)spin extensions of the respective Ruijsenaars-Schneider three-body systems.","sentences":["Integrability of N=1 supersymmetric Ruijsenaars-Schneider three-body models based upon the potentials W(x)=2/x, W(x)=2/sin(x), and W(x)=2/sinh(x) is proven.","The problem of constructing an algebraically resolvable set of Grassmann-odd constants of motion is reduced to finding a triplet of vectors such that all their scalar products can be expressed in terms of the original bosonic first integrals.","The supersymmetric generalizations are used to build novel integrable (iso)spin extensions of the respective Ruijsenaars-Schneider three-body systems."],"url":"http://arxiv.org/abs/2403.09204v1","category":"nlin.SI"}
{"created":"2024-03-14 09:13:51","title":"Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation","abstract":"Recently, foundation models trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community. Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through prompt-based object mask generation. However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input prompts and 2) the necessity for extensive additional training to achieve optimal segmentation. To address these challenges, we propose a novel method, customized instance segmentation via prompt learning tailored to SAM. Our method involves a prompt learning module (PLM), which adjusts input prompts into the embedding space to better align with user intentions, thereby enabling more efficient training. Furthermore, we introduce a point matching module (PMM) to enhance the feature representation for finer segmentation by ensuring detailed alignment with ground truth boundaries. Experimental results on various customized instance segmentation scenarios demonstrate the effectiveness of the proposed method.","sentences":["Recently, foundation models trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community.","Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through prompt-based object mask generation.","However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input prompts and 2) the necessity for extensive additional training to achieve optimal segmentation.","To address these challenges, we propose a novel method, customized instance segmentation via prompt learning tailored to SAM.","Our method involves a prompt learning module (PLM), which adjusts input prompts into the embedding space to better align with user intentions, thereby enabling more efficient training.","Furthermore, we introduce a point matching module (PMM) to enhance the feature representation for finer segmentation by ensuring detailed alignment with ground truth boundaries.","Experimental results on various customized instance segmentation scenarios demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.09199v1","category":"cs.CV"}
{"created":"2024-03-14 09:09:15","title":"MetroGNN: Metro Network Expansion with Reinforcement Learning","abstract":"Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve. The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography. In this paper, we introduce a reinforcement learning framework to address a Markov decision process within an urban heterogeneous multi-graph. Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a graph neural network. Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30\\% when compared with state-of-the-art methods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN.","sentences":["Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve.","The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography.","In this paper, we introduce a reinforcement learning framework to address a Markov decision process within an urban heterogeneous multi-graph.","Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a graph neural network.","Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30\\% when compared with state-of-the-art methods.","Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN."],"url":"http://arxiv.org/abs/2403.09197v1","category":"cs.CY"}
{"created":"2024-03-14 09:09:06","title":"Noise Dimension of GAN: An Image Compression Perspective","abstract":"Generative adversial network (GAN) is a type of generative model that maps a high-dimensional noise to samples in target distribution. However, the dimension of noise required in GAN is not well understood. Previous approaches view GAN as a mapping from a continuous distribution to another continous distribution. In this paper, we propose to view GAN as a discrete sampler instead. From this perspective, we build a connection between the minimum noise required and the bits to losslessly compress the images. Furthermore, to understand the behaviour of GAN when noise dimension is limited, we propose divergence-entropy trade-off. This trade-off depicts the best divergence we can achieve when noise is limited. And as rate distortion trade-off, it can be numerically solved when source distribution is known. Finally, we verifies our theory with experiments on image generation.","sentences":["Generative adversial network (GAN) is a type of generative model that maps a high-dimensional noise to samples in target distribution.","However, the dimension of noise required in GAN is not well understood.","Previous approaches view GAN as a mapping from a continuous distribution to another continous distribution.","In this paper, we propose to view GAN as a discrete sampler instead.","From this perspective, we build a connection between the minimum noise required and the bits to losslessly compress the images.","Furthermore, to understand the behaviour of GAN when noise dimension is limited, we propose divergence-entropy trade-off.","This trade-off depicts the best divergence we can achieve when noise is limited.","And as rate distortion trade-off, it can be numerically solved when source distribution is known.","Finally, we verifies our theory with experiments on image generation."],"url":"http://arxiv.org/abs/2403.09196v1","category":"cs.CV"}
{"created":"2024-03-14 09:07:34","title":"SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration","abstract":"Segment Anything Model (SAM) has garnered significant attention in segmentation tasks due to their zero-shot generalization ability. However, a broader application of SAMs to real-world practice has been restricted by their low inference speed and high computational memory demands, which mainly stem from the attention mechanism. Existing work concentrated on optimizing the encoder, yet has not adequately addressed the inefficiency of the attention mechanism itself, even when distilled to a smaller model, which thus leaves space for further improvement. In response, we introduce SAM-Lightening, a variant of SAM, that features a re-engineered attention mechanism, termed Dilated Flash Attention. It not only facilitates higher parallelism, enhancing processing efficiency but also retains compatibility with the existing FlashAttention. Correspondingly, we propose a progressive distillation to enable an efficient knowledge transfer from the vanilla SAM without costly training from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening significantly outperforms the state-of-the-art methods in both run-time efficiency and segmentation accuracy. Specifically, it can achieve an inference speed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels, which is 30.1 times faster than the vanilla SAM and 2.1 times than the state-of-the-art. Moreover, it takes only 244MB memory, which is 3.5\\% of the vanilla SAM. The code and weights are available at https://anonymous.4open.science/r/SAM-LIGHTENING-BC25/.","sentences":["Segment Anything Model (SAM) has garnered significant attention in segmentation tasks due to their zero-shot generalization ability.","However, a broader application of SAMs to real-world practice has been restricted by their low inference speed and high computational memory demands, which mainly stem from the attention mechanism.","Existing work concentrated on optimizing the encoder, yet has not adequately addressed the inefficiency of the attention mechanism itself, even when distilled to a smaller model, which thus leaves space for further improvement.","In response, we introduce SAM-Lightening, a variant of SAM, that features a re-engineered attention mechanism, termed Dilated Flash Attention.","It not only facilitates higher parallelism, enhancing processing efficiency but also retains compatibility with the existing FlashAttention.","Correspondingly, we propose a progressive distillation to enable an efficient knowledge transfer from the vanilla SAM without costly training from scratch.","Experiments on COCO and LVIS reveal that SAM-Lightening significantly outperforms the state-of-the-art methods in both run-time efficiency and segmentation accuracy.","Specifically, it can achieve an inference speed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels, which is 30.1 times faster than the vanilla SAM and 2.1 times than the state-of-the-art.","Moreover, it takes only 244MB memory, which is 3.5\\% of the vanilla SAM.","The code and weights are available at https://anonymous.4open.science/r/SAM-LIGHTENING-BC25/."],"url":"http://arxiv.org/abs/2403.09195v1","category":"cs.CV"}
{"created":"2024-03-14 09:07:31","title":"Intention-driven Ego-to-Exo Video Generation","abstract":"Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI. Benefiting from advancements in diffusion model techniques, notable progress has been achieved in video generation. However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views. To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion. Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching. Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory. Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image. Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the diffusion model, ultimately warping them into the corresponding exocentric video. We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation.","sentences":["Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI.","Benefiting from advancements in diffusion model techniques, notable progress has been achieved in video generation.","However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views.","To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion.","Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching.","Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory.","Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image.","Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the diffusion model, ultimately warping them into the corresponding exocentric video.","We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation."],"url":"http://arxiv.org/abs/2403.09194v1","category":"cs.CV"}
{"created":"2024-03-14 09:07:14","title":"Are Vision Language Models Texture or Shape Biased and Can We Steer Them?","abstract":"Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.","sentences":["Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering.","Unlike pure vision models, they offer an intuitive way to access visual content through language prompting.","The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models.","One important visual bias is the texture vs. shape bias, or the dominance of local over global information.","In this paper, we study this bias in a wide range of popular VLMs.","Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models.","If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments.","For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone.","For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs."],"url":"http://arxiv.org/abs/2403.09193v1","category":"cs.CV"}
{"created":"2024-03-14 09:05:25","title":"Intention-aware Denoising Diffusion Model for Trajectory Prediction","abstract":"Trajectory prediction is an essential component in autonomous driving, particularly for collision avoidance systems. Considering the inherent uncertainty of the task, numerous studies have utilized generative models to produce multiple plausible future trajectories for each agent. However, most of them suffer from restricted representation ability or unstable training issues. To overcome these limitations, we propose utilizing the diffusion model to generate the distribution of future trajectories. Two cruxes are to be settled to realize such an idea. First, the diversity of intention is intertwined with the uncertain surroundings, making the true distribution hard to parameterize. Second, the diffusion process is time-consuming during the inference phase, rendering it unrealistic to implement in a real-time driving system. We propose an Intention-aware denoising Diffusion Model (IDM), which tackles the above two problems. We decouple the original uncertainty into intention uncertainty and action uncertainty and model them with two dependent diffusion processes. To decrease the inference time, we reduce the variable dimensions in the intention-aware diffusion process and restrict the initial distribution of the action-aware diffusion process, which leads to fewer diffusion steps. To validate our approach, we conduct experiments on the Stanford Drone Dataset (SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with an FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY dataset. Compared with the original diffusion model, IDM reduces inference time by two-thirds. Interestingly, our experiments further reveal that introducing intention information is beneficial in modeling the diffusion process of fewer steps.","sentences":["Trajectory prediction is an essential component in autonomous driving, particularly for collision avoidance systems.","Considering the inherent uncertainty of the task, numerous studies have utilized generative models to produce multiple plausible future trajectories for each agent.","However, most of them suffer from restricted representation ability or unstable training issues.","To overcome these limitations, we propose utilizing the diffusion model to generate the distribution of future trajectories.","Two cruxes are to be settled to realize such an idea.","First, the diversity of intention is intertwined with the uncertain surroundings, making the true distribution hard to parameterize.","Second, the diffusion process is time-consuming during the inference phase, rendering it unrealistic to implement in a real-time driving system.","We propose an Intention-aware denoising Diffusion Model (IDM), which tackles the above two problems.","We decouple the original uncertainty into intention uncertainty and action uncertainty and model them with two dependent diffusion processes.","To decrease the inference time, we reduce the variable dimensions in the intention-aware diffusion process and restrict the initial distribution of the action-aware diffusion process, which leads to fewer diffusion steps.","To validate our approach, we conduct experiments on the Stanford Drone Dataset (SDD) and ETH/UCY dataset.","Our methods achieve state-of-the-art results, with an FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY dataset.","Compared with the original diffusion model, IDM reduces inference time by two-thirds.","Interestingly, our experiments further reveal that introducing intention information is beneficial in modeling the diffusion process of fewer steps."],"url":"http://arxiv.org/abs/2403.09190v1","category":"cs.CV"}
{"created":"2024-03-14 08:59:22","title":"Quantum Dynamic Programming","abstract":"We introduce a quantum extension of dynamic programming, a fundamental computational method for efficiently solving recursive problems using memory. Our innovation lies in showing how to coherently generate unitaries of recursion steps using memorized intermediate quantum states. We find that quantum dynamic programming yields an exponential reduction in circuit depth for a large class of fixed-point quantum recursions, including a known recursive variant of the Grover's search. Additionally, we apply quantum dynamic programming to a recently proposed double-bracket quantum algorithm for diagonalization to obtain a new protocol for obliviously preparing a quantum state in its Schmidt basis, providing a potential pathway for revealing entanglement structures of unknown quantum states.","sentences":["We introduce a quantum extension of dynamic programming, a fundamental computational method for efficiently solving recursive problems using memory.","Our innovation lies in showing how to coherently generate unitaries of recursion steps using memorized intermediate quantum states.","We find that quantum dynamic programming yields an exponential reduction in circuit depth for a large class of fixed-point quantum recursions, including a known recursive variant of the Grover's search.","Additionally, we apply quantum dynamic programming to a recently proposed double-bracket quantum algorithm for diagonalization to obtain a new protocol for obliviously preparing a quantum state in its Schmidt basis, providing a potential pathway for revealing entanglement structures of unknown quantum states."],"url":"http://arxiv.org/abs/2403.09187v1","category":"quant-ph"}
{"created":"2024-03-14 08:59:05","title":"TangoSIDM Project: Is the Stellar Mass Tully-Fisher relation consistent with SIDM?","abstract":"Self-interacting dark matter (SIDM) has the potential to significantly influence galaxy formation in comparison to the cold, collisionless dark matter paradigm (CDM), resulting in observable effects. This study aims to elucidate this influence and to demonstrate that the stellar mass Tully-Fisher relation imposes robust constraints on the parameter space of velocity-dependent SIDM models. We present a new set of cosmological hydrodynamical simulations that include the SIDM scheme from the TangoSIDM project and the SWIFT-EAGLE galaxy formation model. Two cosmological simulations suites were generated: one (Reference model) which yields good agreement with the observed $z=0$ galaxy stellar mass function, galaxy mass-size relation, and stellar-to-halo mass relation; and another (WeakStellarFB model) in which the stellar feedback is less efficient, particularly for Milky Way-like systems. Both galaxy formation models were simulated under four dark matter cosmologies: CDM, SIDM with two different velocity-dependent cross sections, and SIDM with a constant cross section. While SIDM does not modify global galaxy properties such as stellar masses and star formation rates, it does make the galaxies more extended. In Milky Way-like galaxies, where baryons dominate the central gravitational potential, SIDM thermalises, causing dark matter to accumulate in the central regions. This accumulation results in density profiles that are steeper than those produced in CDM from adiabatic contraction. The enhanced dark matter density in the central regions of galaxies causes a deviation in the slope of the Tully-Fisher relation, which significantly diverges from the observational data. In contrast, the Tully-Fisher relation derived from CDM models aligns well with observations.","sentences":["Self-interacting dark matter (SIDM) has the potential to significantly influence galaxy formation in comparison to the cold, collisionless dark matter paradigm (CDM), resulting in observable effects.","This study aims to elucidate this influence and to demonstrate that the stellar mass Tully-Fisher relation imposes robust constraints on the parameter space of velocity-dependent SIDM models.","We present a new set of cosmological hydrodynamical simulations that include the SIDM scheme from the TangoSIDM project and the SWIFT-EAGLE galaxy formation model.","Two cosmological simulations suites were generated: one (Reference model) which yields good agreement with the observed $z=0$ galaxy stellar mass function, galaxy mass-size relation, and stellar-to-halo mass relation; and another (WeakStellarFB model) in which the stellar feedback is less efficient, particularly for Milky Way-like systems.","Both galaxy formation models were simulated under four dark matter cosmologies: CDM, SIDM with two different velocity-dependent cross sections, and SIDM with a constant cross section.","While SIDM does not modify global galaxy properties such as stellar masses and star formation rates, it does make the galaxies more extended.","In Milky Way-like galaxies, where baryons dominate the central gravitational potential, SIDM thermalises, causing dark matter to accumulate in the central regions.","This accumulation results in density profiles that are steeper than those produced in CDM from adiabatic contraction.","The enhanced dark matter density in the central regions of galaxies causes a deviation in the slope of the Tully-Fisher relation, which significantly diverges from the observational data.","In contrast, the Tully-Fisher relation derived from CDM models aligns well with observations."],"url":"http://arxiv.org/abs/2403.09186v1","category":"astro-ph.CO"}
{"created":"2024-03-14 08:58:36","title":"Synchronized states of power grids and oscillator networks by convex optimization","abstract":"Synchronization is essential for the operation of AC power systems: All generators in the power grid must rotate with fixed relative phases to enable a steady flow of electric power. Understanding the conditions for and the limitations of synchronization is of utmost practical importance. In this article, we propose a novel approach to compute and analyze the stable stationary states of a power grid or an oscillator network in terms of a convex optimization problem. This approach allows to systematically compute \\emph{all} stable states where the phase difference across an edge does not exceed $\\pi/2$.Furthermore, the optimization formulation allows to rigorously establish certain properties of synchronized states and to bound the error in the widely used linear power flow approximation.","sentences":["Synchronization is essential for the operation of AC power systems: All generators in the power grid must rotate with fixed relative phases to enable a steady flow of electric power.","Understanding the conditions for and the limitations of synchronization is of utmost practical importance.","In this article, we propose a novel approach to compute and analyze the stable stationary states of a power grid or an oscillator network in terms of a convex optimization problem.","This approach allows to systematically compute \\emph{all} stable states where the phase difference across an edge does not exceed $\\pi/2$.Furthermore, the optimization formulation allows to rigorously establish certain properties of synchronized states and to bound the error in the widely used linear power flow approximation."],"url":"http://arxiv.org/abs/2403.09185v1","category":"eess.SY"}
{"created":"2024-03-14 08:54:19","title":"Learning Algorithms for Verification of Markov Decision Processes","abstract":"We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes (MDPs), based on the ideas of Br\\'azdil, T. et al. (2014). Verification of Markov Decision Processes Using Learning Algorithms. The primary goal of the techniques presented in that work is to improve performance by avoiding an exhaustive exploration of the state space, guided by heuristics. This approach is significantly extended in this work. Several details of the base theory are refined and errors are fixed. Section 1.3 provides an overview of all differences.   The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios. The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities. It performs a heuristic-driven partial exploration of the model, yielding precise lower and upper bounds on the required probability. The second tackles the case where we may only sample the MDP without knowing the exact transition dynamics. Here, we obtain probabilistic guarantees, again in terms of both the lower and upper bounds, which provides efficient stopping criteria for the approximation. In particular, the latter is an extension of statistical model-checking (SMC) for unbounded properties in MDPs. In contrast to other related approaches, we do not restrict our attention to time-bounded (finite-horizon) or discounted properties, nor assume any particular structural properties of the MDP.","sentences":["We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes (MDPs), based on the ideas of Br\\'azdil, T. et al. (2014).","Verification of Markov Decision Processes Using Learning Algorithms.","The primary goal of the techniques presented in that work is to improve performance by avoiding an exhaustive exploration of the state space, guided by heuristics.","This approach is significantly extended in this work.","Several details of the base theory are refined and errors are fixed.","Section 1.3 provides an overview of all differences.   ","The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios.","The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities.","It performs a heuristic-driven partial exploration of the model, yielding precise lower and upper bounds on the required probability.","The second tackles the case where we may only sample the MDP without knowing the exact transition dynamics.","Here, we obtain probabilistic guarantees, again in terms of both the lower and upper bounds, which provides efficient stopping criteria for the approximation.","In particular, the latter is an extension of statistical model-checking (SMC) for unbounded properties in MDPs.","In contrast to other related approaches, we do not restrict our attention to time-bounded (finite-horizon) or discounted properties, nor assume any particular structural properties of the MDP."],"url":"http://arxiv.org/abs/2403.09184v1","category":"eess.SY"}
{"created":"2024-03-14 08:53:01","title":"Generalized Relevance Learning Grassmann Quantization","abstract":"Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions. Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them. A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold. In this contribution, we extend the application of Generalized Relevance Learning Vector Quantization to deal with Grassmann manifold. The proposed model returns a set of prototype subspaces and a relevance vector. While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task. They both provide insights into the model's decisions by highlighting influential images and pixels for predictions. Moreover, due to learning prototypes, the model complexity of the new method during inference is independent of dataset size, unlike previous works. We applied it to several recognition tasks including handwritten digit recognition, face recognition, activity recognition, and object recognition. Experiments demonstrate that it outperforms previous works with lower complexity and can successfully model the variation, such as handwritten style or lighting conditions. Moreover, the presence of relevances makes the model robust to the selection of subspaces' dimensionality.","sentences":["Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions.","Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them.","A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold.","In this contribution, we extend the application of Generalized Relevance Learning Vector Quantization to deal with Grassmann manifold.","The proposed model returns a set of prototype subspaces and a relevance vector.","While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task.","They both provide insights into the model's decisions by highlighting influential images and pixels for predictions.","Moreover, due to learning prototypes, the model complexity of the new method during inference is independent of dataset size, unlike previous works.","We applied it to several recognition tasks including handwritten digit recognition, face recognition, activity recognition, and object recognition.","Experiments demonstrate that it outperforms previous works with lower complexity and can successfully model the variation, such as handwritten style or lighting conditions.","Moreover, the presence of relevances makes the model robust to the selection of subspaces' dimensionality."],"url":"http://arxiv.org/abs/2403.09183v1","category":"cs.CV"}
{"created":"2024-03-14 08:43:43","title":"Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts","abstract":"Diffusion models have achieved remarkable success across a range of generative tasks. Recent efforts to enhance diffusion model architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level. While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively. In response, we introduce Switch Diffusion Transformer (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information. To achieve this, we employ a sparse mixture-of-experts within each transformer block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation. Additionally, we propose a diffusion prior loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones. Through these, each transformer block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the diffusion model to construct its beneficial way of synergizing denoising tasks. Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios.","sentences":["Diffusion models have achieved remarkable success across a range of generative tasks.","Recent efforts to enhance diffusion model architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level.","While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively.","In response, we introduce Switch Diffusion Transformer (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information.","To achieve this, we employ a sparse mixture-of-experts within each transformer block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation.","Additionally, we propose a diffusion prior loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones.","Through these, each transformer block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the diffusion model to construct its beneficial way of synergizing denoising tasks.","Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios."],"url":"http://arxiv.org/abs/2403.09176v1","category":"cs.CV"}
{"created":"2024-03-14 17:59:09","title":"On locally symmetric polynomial metrics: Riemannian and Finslerian surfaces","abstract":"In the paper we investigate locally symmetric polynomial metrics in special cases of Riemannian and Finslerian surfaces. The Riemannian case will be presented by a collection of basic results (regularity of second root metrics) and formulas up to Gauss curvature. In case of Finslerian surfaces we formulate necessary and sufficient conditions for a locally symmetric fourth root metric in 2D to be positive definite. They are given in terms of the coefficients of the polynomial metric to make checking the positive definiteness as simple and direct as possible. Explicit examples are also presented. The situation is more complicated in case of spaces of dimension more than two. Some necessary conditions and an explicit example are given for a positive definite locally symmetric polynomial metric in 3D. Computations are supported by the MAPLE mathematics software (LinearAlgebra).","sentences":["In the paper we investigate locally symmetric polynomial metrics in special cases of Riemannian and Finslerian surfaces.","The Riemannian case will be presented by a collection of basic results (regularity of second root metrics) and formulas up to Gauss curvature.","In case of Finslerian surfaces we formulate necessary and sufficient conditions for a locally symmetric fourth root metric in 2D to be positive definite.","They are given in terms of the coefficients of the polynomial metric to make checking the positive definiteness as simple and direct as possible.","Explicit examples are also presented.","The situation is more complicated in case of spaces of dimension more than two.","Some necessary conditions and an explicit example are given for a positive definite locally symmetric polynomial metric in 3D. Computations are supported by the MAPLE mathematics software (LinearAlgebra)."],"url":"http://arxiv.org/abs/2403.09633v1","category":"math.DG"}
{"created":"2024-03-14 17:52:17","title":"Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting","abstract":"Localization in outdoor wireless systems typically requires transmitting specific reference signals to estimate distance (trilateration methods) or angle (triangulation methods). These cause overhead on communication, need a LoS link to work well, and require multiple base stations, often imposing synchronization or specific hardware requirements. Fingerprinting has none of these drawbacks, but building its database requires high human effort to collect real-world measurements. For a long time, this issue limited the size of databases and thus their performance. This work proposes significantly reducing human effort in building fingerprinting databases by populating them with \\textit{digital twin RF maps}. These RF maps are built from ray-tracing simulations on a digital replica of the environment across several frequency bands and beamforming configurations. Online user fingerprints are then matched against this spatial database. The approach was evaluated with practical simulations using realistic propagation models and user measurements. Our experiments show sub-meter localization errors on a NLoS location 95\\% of the time using sensible user measurement report sizes. Results highlight the promising potential of the proposed digital twin approach for ubiquitous wide-area 6G localization.","sentences":["Localization in outdoor wireless systems typically requires transmitting specific reference signals to estimate distance (trilateration methods) or angle (triangulation methods).","These cause overhead on communication, need a LoS link to work well, and require multiple base stations, often imposing synchronization or specific hardware requirements.","Fingerprinting has none of these drawbacks, but building its database requires high human effort to collect real-world measurements.","For a long time, this issue limited the size of databases and thus their performance.","This work proposes significantly reducing human effort in building fingerprinting databases by populating them with \\textit{digital twin RF maps}.","These RF maps are built from ray-tracing simulations on a digital replica of the environment across several frequency bands and beamforming configurations.","Online user fingerprints are then matched against this spatial database.","The approach was evaluated with practical simulations using realistic propagation models and user measurements.","Our experiments show sub-meter localization errors on a NLoS location 95\\% of the time using sensible user measurement report sizes.","Results highlight the promising potential of the proposed digital twin approach for ubiquitous wide-area 6G localization."],"url":"http://arxiv.org/abs/2403.09614v1","category":"cs.IT"}
{"created":"2024-03-14 17:00:29","title":"Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis","abstract":"The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars. We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers. Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of 80%, which improves up to 93% when the target state information is available. Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions.","sentences":["The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases.","As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems.","Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must.","In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves.","Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars.","We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers.","Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of 80%, which improves up to 93% when the target state information is available.","Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions."],"url":"http://arxiv.org/abs/2403.09571v1","category":"cs.RO"}
{"created":"2024-03-14 17:00:01","title":"Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search","abstract":"In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks. The proposed method includes shared inter-task latent variables, which are transferred across tasks by implementing particle-based variational Bayesian updates. Experimental results across synthetic and real-world examples reveal that the proposed provident acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed.","sentences":["In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate.","For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time.","Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost.","Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task.","Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks.","The proposed method includes shared inter-task latent variables, which are transferred across tasks by implementing particle-based variational Bayesian updates.","Experimental results across synthetic and real-world examples reveal that the proposed provident acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed."],"url":"http://arxiv.org/abs/2403.09570v1","category":"cs.LG"}
{"created":"2024-03-14 15:32:25","title":"On using Machine Learning Algorithms for Motorcycle Collision Detection","abstract":"Globally, motorcycles attract vast and varied users. However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems. Impact simulations show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts. For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered. For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms. First, a series of simulations of accidents and driving operation is introduced to collect data to train machine learning classification models. Their performance is henceforth assessed and compared via multiple representative and application-oriented criteria.","sentences":["Globally, motorcycles attract vast and varied users.","However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems.","Impact simulations show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts.","For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered.","For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms.","First, a series of simulations of accidents and driving operation is introduced to collect data to train machine learning classification models.","Their performance is henceforth assessed and compared via multiple representative and application-oriented criteria."],"url":"http://arxiv.org/abs/2403.09491v1","category":"cs.LG"}
{"created":"2024-03-14 15:25:50","title":"Tracking of charged particles with nanosecond lifetimes at LHCb","abstract":"A method is presented to reconstruct charged particles with lifetimes between 10 ps and 10 ns, which considers a combination of their decay products and the partial tracks created by the initial charged particle. Using the $\\Xi^-$ baryon as a benchmark, the method is demonstrated with simulated events and proton-proton collision data at $\\sqrt{s}=13$ TeV, corresponding to an integrated luminosity of 2.0 fb${}^{-1}$ collected with the LHCb detector in 2018. Significant improvements in the angular resolution and the signal purity are obtained. The method is implemented as part of the LHCb Run 3 event trigger in a set of requirements to select detached hyperons. This is the first demonstration of the applicability of this approach at the LHC, and the first to show its scaling with instantaneous luminosity.","sentences":["A method is presented to reconstruct charged particles with lifetimes between 10 ps and 10 ns, which considers a combination of their decay products and the partial tracks created by the initial charged particle.","Using the $\\Xi^-$ baryon as a benchmark, the method is demonstrated with simulated events and proton-proton collision data at $\\sqrt{s}=13$ TeV, corresponding to an integrated luminosity of 2.0 fb${}^{-1}$ collected with the LHCb detector in 2018.","Significant improvements in the angular resolution and the signal purity are obtained.","The method is implemented as part of the LHCb Run 3 event trigger in a set of requirements to select detached hyperons.","This is the first demonstration of the applicability of this approach at the LHC, and the first to show its scaling with instantaneous luminosity."],"url":"http://arxiv.org/abs/2403.09483v1","category":"hep-ex"}
{"created":"2024-03-14 15:04:17","title":"New constraints on Triton's atmosphere from the 6 October 2022 stellar occultation","abstract":"The atmosphere of Triton was probed directly by observing a ground-based stellar occultation on 6 October 2022. This rare event yielded 23 positive light curves collected from 13 separate observation stations contributing to our campaign. The significance of this event lies in its potential to directly validate the modest pressure fluctuation on Triton, a phenomenon not definitively verified by previous observations, including only five stellar occultations, and the Voyager 2 radio occultation in 1989. Using an approach consistent with a comparable study, we precisely determined a surface pressure of $14.07_{-0.13}^{+0.21}~\\mathrm{\\mu bar}$ in 2022. This new pressure rules out any significant monotonic variation in pressure between 2017 and 2022 through direct observations, as it is in alignment with the 2017 value. Additionally, both the pressures in 2017 and 2022 align with the 1989 value. This provides further support for the conclusion drawn from the previous volatile transport model simulation, which is consistent with the observed alignment between the pressures in 1989 and 2017; that is to say, the pressure fluctuation is modest. Moreover, this conclusion suggests the existence of a northern polar cap extended down to at least $45^\\circ$N$-60^\\circ$N and the presence of nitrogen between $30^\\circ$S and $0^\\circ$.","sentences":["The atmosphere of Triton was probed directly by observing a ground-based stellar occultation on 6 October 2022.","This rare event yielded 23 positive light curves collected from 13 separate observation stations contributing to our campaign.","The significance of this event lies in its potential to directly validate the modest pressure fluctuation on Triton, a phenomenon not definitively verified by previous observations, including only five stellar occultations, and the Voyager 2 radio occultation in 1989.","Using an approach consistent with a comparable study, we precisely determined a surface pressure of $14.07_{-0.13}^{+0.21}~\\mathrm{\\mu bar}$ in 2022.","This new pressure rules out any significant monotonic variation in pressure between 2017 and 2022 through direct observations, as it is in alignment with the 2017 value.","Additionally, both the pressures in 2017 and 2022 align with the 1989 value.","This provides further support for the conclusion drawn from the previous volatile transport model simulation, which is consistent with the observed alignment between the pressures in 1989 and 2017; that is to say, the pressure fluctuation is modest.","Moreover, this conclusion suggests the existence of a northern polar cap extended down to at least $45^\\circ$N$-60^\\circ$N and the presence of nitrogen between $30^\\circ$S and $0^\\circ$."],"url":"http://arxiv.org/abs/2403.09464v1","category":"astro-ph.EP"}
{"created":"2024-03-14 14:51:35","title":"Measurements of inclusive and differential cross-sections of $t\\bar{t}\u03b3$ production in $pp$ collisions at $\\sqrt{s}=13$ TeV with the ATLAS detector","abstract":"Inclusive and differential cross-sections are measured at particle level for the associated production of a top quark pair and a photon ($t\\bar{t}\\gamma$). The analysis is performed using an integrated luminosity of 140 fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of 13 TeV collected by the ATLAS detector. The measurements are performed in the single-lepton and dilepton top quark pair decay channels focusing on $t\\bar{t}\\gamma$ topologies where the photon is radiated from an initial-state parton or one of the top quarks. The absolute and normalised differential cross-sections are measured for several variables characterising the photon, lepton and jet kinematics as well as the angular separation between those objects. The observables are found to be in good agreement with the Monte Carlo predictions. The photon transverse momentum differential distribution is used to set limits on effective field theory parameters related to the electroweak dipole moments of the top quark. The combined limits using the photon and the $Z$ boson transverse momentum measured in $t\\bar{t}$ production in associations with a $Z$ boson are also set.","sentences":["Inclusive and differential cross-sections are measured at particle level for the associated production of a top quark pair and a photon ($t\\bar{t}\\gamma$).","The analysis is performed using an integrated luminosity of 140 fb$^{-1}$ of proton-proton collisions at a centre-of-mass energy of 13 TeV collected by the ATLAS detector.","The measurements are performed in the single-lepton and dilepton top quark pair decay channels focusing on $t\\bar{t}\\gamma$ topologies where the photon is radiated from an initial-state parton or one of the top quarks.","The absolute and normalised differential cross-sections are measured for several variables characterising the photon, lepton and jet kinematics as well as the angular separation between those objects.","The observables are found to be in good agreement with the Monte Carlo predictions.","The photon transverse momentum differential distribution is used to set limits on effective field theory parameters related to the electroweak dipole moments of the top quark.","The combined limits using the photon and the $Z$ boson transverse momentum measured in $t\\bar{t}$ production in associations with a $Z$ boson are also set."],"url":"http://arxiv.org/abs/2403.09452v1","category":"hep-ex"}
{"created":"2024-03-14 13:47:47","title":"Discovery of a distinct collective mode in kagome superconductors","abstract":"The collective modes of the superconducting (SC) order parameter fluctuation can provide key insights into the nature of the superconductor, such as the pairing symmetry and orbital nature of the cooper pairs. Their detection has been challenging. Recently, a family of charge density wave (CDW) superconductors has emerged in non-magnetic kagome materials AV3Sb5 (A=K, Rb, Cs), exhibiting evidence for unprecedented time-reversal symmetry breaking CDW, roton-pair density wave (PDW), and higher-charge flux quantization. However, the collective behaviors of the cooper pairs have not been studied. Here, we report a distinct collective mode in the kagome superconductors CsV3-xTaxSb5. Using scanning tunneling microscope/spectroscopy (STM/S), we observe a \"peak-dip-hump\" feature in the tunneling conductance-a defining signature of collective excitations. The spectral lineshape is well-described by two SC gap functions, one isotropic and one anisotropic, and a bosonic mode due to electron-mode coupling. With increasing x, the two SC gaps, with different pair-breaking strengths and thus different orbital/band characters, move closer in energy, merge into two isotropic gaps of equal amplitude, and then increase synchronously. The collective mode energy, on the other hand, decreases monotonically to well below the quasi-particle excitation gap 2{\\Delta} and survives even after the CDW order is suppressed at large x. We identify the collective mode as the Leggett mode of the relative phases between different SC components in the multi-band superconductor or the exotic Bardasis-Schrieffer mode due to a subleading SC component. Our findings provide valuable insights on the nature of the SC ground state and collective excitations, their evolution with Ta-substitutions, and offer a new pathway to study the origin of superconductivity and its interplay with CDW order in kagome superconductors.","sentences":["The collective modes of the superconducting (SC) order parameter fluctuation can provide key insights into the nature of the superconductor, such as the pairing symmetry and orbital nature of the cooper pairs.","Their detection has been challenging.","Recently, a family of charge density wave (CDW) superconductors has emerged in non-magnetic kagome materials AV3Sb5","(A=K, Rb, Cs), exhibiting evidence for unprecedented time-reversal symmetry breaking CDW, roton-pair density wave (PDW), and higher-charge flux quantization.","However, the collective behaviors of the cooper pairs have not been studied.","Here, we report a distinct collective mode in the kagome superconductors CsV3-xTaxSb5.","Using scanning tunneling microscope/spectroscopy (STM/S), we observe a \"peak-dip-hump\" feature in the tunneling conductance-a defining signature of collective excitations.","The spectral lineshape is well-described by two SC gap functions, one isotropic and one anisotropic, and a bosonic mode due to electron-mode coupling.","With increasing x, the two SC gaps, with different pair-breaking strengths and thus different orbital/band characters, move closer in energy, merge into two isotropic gaps of equal amplitude, and then increase synchronously.","The collective mode energy, on the other hand, decreases monotonically to well below the quasi-particle excitation gap 2{\\Delta} and survives even after the CDW order is suppressed at large x.","We identify the collective mode as the Leggett mode of the relative phases between different SC components in the multi-band superconductor or the exotic Bardasis-Schrieffer mode due to a subleading SC component.","Our findings provide valuable insights on the nature of the SC ground state and collective excitations, their evolution with Ta-substitutions, and offer a new pathway to study the origin of superconductivity and its interplay with CDW order in kagome superconductors."],"url":"http://arxiv.org/abs/2403.09395v1","category":"cond-mat.supr-con"}
{"created":"2024-03-14 12:57:20","title":"LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy","abstract":"Local differential privacy (LDP), which enables an untrusted server to collect aggregated statistics from distributed users while protecting the privacy of those users, has been widely deployed in practice. However, LDP protocols for frequency estimation are vulnerable to poisoning attacks, in which an attacker can poison the aggregated frequencies by manipulating the data sent from malicious users. Therefore, it is an open challenge to recover the accurate aggregated frequencies from poisoned ones.   In this work, we propose LDPRecover, a method that can recover accurate aggregated frequencies from poisoning attacks, even if the server does not learn the details of the attacks. In LDPRecover, we establish a genuine frequency estimator that theoretically guides the server to recover the frequencies aggregated from genuine users' data by eliminating the impact of malicious users' data in poisoned frequencies. Since the server has no idea of the attacks, we propose an adaptive attack to unify existing attacks and learn the statistics of the malicious data within this adaptive attack by exploiting the properties of LDP protocols. By taking the estimator and the learning statistics as constraints, we formulate the problem of recovering aggregated frequencies to approach the genuine ones as a constraint inference (CI) problem. Consequently, the server can obtain accurate aggregated frequencies by solving this problem optimally. Moreover, LDPRecover can serve as a frequency recovery paradigm that recovers more accurate aggregated frequencies by integrating attack details as new constraints in the CI problem. Our evaluation on two real-world datasets, three LDP protocols, and untargeted and targeted poisoning attacks shows that LDPRecover is both accurate and widely applicable against various poisoning attacks.","sentences":["Local differential privacy (LDP), which enables an untrusted server to collect aggregated statistics from distributed users while protecting the privacy of those users, has been widely deployed in practice.","However, LDP protocols for frequency estimation are vulnerable to poisoning attacks, in which an attacker can poison the aggregated frequencies by manipulating the data sent from malicious users.","Therefore, it is an open challenge to recover the accurate aggregated frequencies from poisoned ones.   ","In this work, we propose LDPRecover, a method that can recover accurate aggregated frequencies from poisoning attacks, even if the server does not learn the details of the attacks.","In LDPRecover, we establish a genuine frequency estimator that theoretically guides the server to recover the frequencies aggregated from genuine users' data by eliminating the impact of malicious users' data in poisoned frequencies.","Since the server has no idea of the attacks, we propose an adaptive attack to unify existing attacks and learn the statistics of the malicious data within this adaptive attack by exploiting the properties of LDP protocols.","By taking the estimator and the learning statistics as constraints, we formulate the problem of recovering aggregated frequencies to approach the genuine ones as a constraint inference (CI) problem.","Consequently, the server can obtain accurate aggregated frequencies by solving this problem optimally.","Moreover, LDPRecover can serve as a frequency recovery paradigm that recovers more accurate aggregated frequencies by integrating attack details as new constraints in the CI problem.","Our evaluation on two real-world datasets, three LDP protocols, and untargeted and targeted poisoning attacks shows that LDPRecover is both accurate and widely applicable against various poisoning attacks."],"url":"http://arxiv.org/abs/2403.09351v1","category":"cs.CR"}
{"created":"2024-03-14 12:53:30","title":"From Pro, Anti to Informative and Hesitant: An Infoveillance study of COVID-19 vaccines and vaccination discourse on Twitter","abstract":"COVID-19 pandemic has brought unprecedented challenges to the world, and vaccination has been a key strategy to combat the disease. Since Twitter is one of the most widely used public microblogging platforms, researchers have analysed COVID-19 vaccines and vaccination Twitter discourse to explore the conversational dynamics around the topic. While contributing to the crisis informatics literature, we curate a large-scale geotagged Twitter dataset, GeoCovaxTweets Extended, and explore the discourse through multiple spatiotemporal analyses. This dataset covers a longer time span of 38 months, from the announcement of the first vaccine to the availability of booster doses. Results show that 43.4% of the collected tweets, although containing phrases and keywords related to vaccines and vaccinations, were unrelated to the COVID-19 context. In total, 23.1% of the discussions on vaccines and vaccinations were classified as Pro, 16% as Hesitant, 11.4% as Anti, and 6.1% as Informative. The trend shifted towards Pro and Informative tweets globally as vaccination programs progressed, indicating a change in the public's perception of COVID-19 vaccines and vaccination. Furthermore, we explored the discourse based on account attributes, i.e., followers counts and tweet counts. Results show a significant pattern of discourse differences. Our findings highlight the potential of harnessing a large-scale geotagged Twitter dataset to understand global public health communication and to inform targeted interventions aimed at addressing vaccine hesitancy.","sentences":["COVID-19 pandemic has brought unprecedented challenges to the world, and vaccination has been a key strategy to combat the disease.","Since Twitter is one of the most widely used public microblogging platforms, researchers have analysed COVID-19 vaccines and vaccination Twitter discourse to explore the conversational dynamics around the topic.","While contributing to the crisis informatics literature, we curate a large-scale geotagged Twitter dataset, GeoCovaxTweets Extended, and explore the discourse through multiple spatiotemporal analyses.","This dataset covers a longer time span of 38 months, from the announcement of the first vaccine to the availability of booster doses.","Results show that 43.4% of the collected tweets, although containing phrases and keywords related to vaccines and vaccinations, were unrelated to the COVID-19 context.","In total, 23.1% of the discussions on vaccines and vaccinations were classified as Pro, 16% as Hesitant, 11.4% as Anti, and 6.1% as Informative.","The trend shifted towards Pro and Informative tweets globally as vaccination programs progressed, indicating a change in the public's perception of COVID-19 vaccines and vaccination.","Furthermore, we explored the discourse based on account attributes, i.e., followers counts and tweet counts.","Results show a significant pattern of discourse differences.","Our findings highlight the potential of harnessing a large-scale geotagged Twitter dataset to understand global public health communication and to inform targeted interventions aimed at addressing vaccine hesitancy."],"url":"http://arxiv.org/abs/2403.09349v1","category":"cs.SI"}
{"created":"2024-03-14 12:20:23","title":"Up and Down Quark Structure of the Proton","abstract":"We measure proton structure parameters sensitive primarily to valence quarks using 8.6 fb$^{-1}$ of data collected by the D0 detector in $\\sqrt{s}=1.96$ TeV $p\\bar{p}$ collisions at the Fermilab Tevatron. We exploit the property of the forward-backward asymmetry in dilepton events to be factorized in to distinct structure parameters and electroweak quark-level asymmetries. Contributions to the asymmetry from $s$, $c$ and $b$ quarks, as well as from $u$ and $d$ quarks, are suppressed allowing valence $u$ and $d$ quarks to be separately determined. We find and $u$ to $d$ quark ratio near the peak values in the quark density distributions that is smaller than predictions from modern parton distribution functions.","sentences":["We measure proton structure parameters sensitive primarily to valence quarks using 8.6 fb$^{-1}$ of data collected by the D0 detector in $\\sqrt{s}=1.96$ TeV $p\\bar{p}$ collisions at the Fermilab Tevatron.","We exploit the property of the forward-backward asymmetry in dilepton events to be factorized in to distinct structure parameters and electroweak quark-level asymmetries.","Contributions to the asymmetry from $s$, $c$ and $b$ quarks, as well as from $u$ and $d$ quarks, are suppressed allowing valence $u$ and $d$ quarks to be separately determined.","We find and $u$ to $d$ quark ratio near the peak values in the quark density distributions that is smaller than predictions from modern parton distribution functions."],"url":"http://arxiv.org/abs/2403.09331v1","category":"hep-ex"}
{"created":"2024-03-14 11:36:53","title":"A complete logic for causal consistency","abstract":"The $\\mathrm{Caus}[-]$ construction takes a base category of ``raw materials'' and builds a category of higher order causal processes, that is a category whose types encode causal (a.k.a. signalling) constraints between collections of systems. Notable examples are categories of higher-order stochastic maps and higher-order quantum channels. Well-typedness in $\\mathrm{Caus}[-]$ corresponds to a composition of processes being causally consistent, in the sense that any choice of local processes of the prescribed types yields an overall process respecting causality constraints. It follows that closed processes always occur with probability 1, ruling out e.g. causal paradoxes arising from time loops. It has previously been shown that $\\mathrm{Caus}[\\mathcal{C}]$ gives a model of MLL+MIX and BV logic, hence these logics give sufficient conditions for causal consistency, but they fail to provide a complete characterisation. In this follow-on work, we introduce graph types as a tool to examine causal structures over graphs in this model. We explore their properties, standard forms, and equivalent definitions; in particular, a process obeys all signalling constraints of the graph iff it is expressible as an affine combination of factorisations into local causal processes connected according to the edges of the graph. The properties of graph types are then used to prove completeness for causal consistency of a new causal logic that conservatively extends pomset logic. The crucial extra ingredient is a notion of distinguished atoms that correspond to first-order states, which only admit a flow of information in one direction. Using the fact that causal logic conservatively extends pomset logic, we finish by giving a physically-meaningful interpretation to a separating statement between pomset and BV.","sentences":["The $\\mathrm{Caus}[-]$ construction takes a base category of ``raw materials'' and builds a category of higher order causal processes, that is a category whose types encode causal (a.k.a. signalling) constraints between collections of systems.","Notable examples are categories of higher-order stochastic maps and higher-order quantum channels.","Well-typedness in $\\mathrm{Caus}[-]$ corresponds to a composition of processes being causally consistent, in the sense that any choice of local processes of the prescribed types yields an overall process respecting causality constraints.","It follows that closed processes always occur with probability 1, ruling out e.g. causal paradoxes arising from time loops.","It has previously been shown that $\\mathrm{Caus}[\\mathcal{C}]$ gives a model of MLL+MIX and BV logic, hence these logics give sufficient conditions for causal consistency, but they fail to provide a complete characterisation.","In this follow-on work, we introduce graph types as a tool to examine causal structures over graphs in this model.","We explore their properties, standard forms, and equivalent definitions; in particular, a process obeys all signalling constraints of the graph iff it is expressible as an affine combination of factorisations into local causal processes connected according to the edges of the graph.","The properties of graph types are then used to prove completeness for causal consistency of a new causal logic that conservatively extends pomset logic.","The crucial extra ingredient is a notion of distinguished atoms that correspond to first-order states, which only admit a flow of information in one direction.","Using the fact that causal logic conservatively extends pomset logic, we finish by giving a physically-meaningful interpretation to a separating statement between pomset and BV."],"url":"http://arxiv.org/abs/2403.09297v1","category":"cs.LO"}
{"created":"2024-03-14 11:35:08","title":"Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score","abstract":"In this contribution, we deal with seed-based information retrieval in networks of research publications. Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures. In addition, we include the PubMed Related Article score as well as combined approaches in the comparison. We also provide a fairly comprehensive review of earlier research in which citation relations have been used for information retrieval purposes. The results show an advantage for co-citation over bibliographic coupling and direct citation. However, combining the three approaches outperforms the exclusive use of co-citation in the study. The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches enhances the performance of seed-based information retrieval. The results from the study may guide approaches combining citation-based and textual approaches in their choice of citation similarity measures. We suggest that future research use more structured approaches to evaluate methods for seed-based retrieval of publications, including comparative approaches as well as the elaboration of common data sets and baselines for evaluation.","sentences":["In this contribution, we deal with seed-based information retrieval in networks of research publications.","Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures.","In addition, we include the PubMed Related Article score as well as combined approaches in the comparison.","We also provide a fairly comprehensive review of earlier research in which citation relations have been used for information retrieval purposes.","The results show an advantage for co-citation over bibliographic coupling and direct citation.","However, combining the three approaches outperforms the exclusive use of co-citation in the study.","The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches enhances the performance of seed-based information retrieval.","The results from the study may guide approaches combining citation-based and textual approaches in their choice of citation similarity measures.","We suggest that future research use more structured approaches to evaluate methods for seed-based retrieval of publications, including comparative approaches as well as the elaboration of common data sets and baselines for evaluation."],"url":"http://arxiv.org/abs/2403.09295v1","category":"cs.IR"}
{"created":"2024-03-14 11:12:16","title":"TH\u00d6R-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction","abstract":"We present a new large dataset of indoor human and robot navigation and interaction, called TH\\\"OR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. TH\\\"OR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, TH\\\"OR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. TH\\\"OR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.","sentences":["We present a new large dataset of indoor human and robot navigation and interaction, called TH\\\"OR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context.","TH\\\"OR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI.","This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios.","Unlike existing datasets, TH\\\"OR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation.","The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot.","We also provide a set of tools for visualization and processing of the recorded data.","TH\\\"OR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions."],"url":"http://arxiv.org/abs/2403.09285v1","category":"cs.RO"}
{"created":"2024-03-14 10:07:29","title":"High precision proton beam monitor system concept design on CSNS based on SiC","abstract":"A high precision beam monitor system based on silicon carbide PIN sensor is designed for China Spallation Neutron Source 1.6 GeV proton beam to monitor the proton beam fluence.The concept design of the beam monitor system is finished together with front-end electronics with silicon carbide PIN sensors, readout system and mechanical system.Several tests are performed to study the performance of each component of the system.The charge collection of the SiC PIN sensors after proton radiation is studied with 80 MeV proton beam for continuous running. Research on the performance of the front-end electronics and readout system is finished for better data acquisition.The uncertainty of proton beam fluence is below 1% in the beam monitor system.","sentences":["A high precision beam monitor system based on silicon carbide PIN sensor is designed for China Spallation Neutron Source 1.6 GeV proton beam to monitor the proton beam fluence.","The concept design of the beam monitor system is finished together with front-end electronics with silicon carbide PIN sensors, readout system and mechanical system.","Several tests are performed to study the performance of each component of the system.","The charge collection of the SiC PIN sensors after proton radiation is studied with 80 MeV proton beam for continuous running.","Research on the performance of the front-end electronics and readout system is finished for better data acquisition.","The uncertainty of proton beam fluence is below 1% in the beam monitor system."],"url":"http://arxiv.org/abs/2403.09244v1","category":"physics.acc-ph"}
{"created":"2024-03-14 09:53:06","title":"Analysis of BMR tilt from AutoTAB catalog: Hinting towards the thin flux tube model?","abstract":"One of the intriguing mechanisms of the Sun is the formation of the bipolar magnetic regions (BMRs) in the solar convection zone which are observed as regions of concentrated magnetic fields of opposite polarity on photosphere. These BMRs are tilted with respect to the equatorial line, which statistically increases with latitude. The thin flux tube model, employing the rise of magnetically buoyant flux loops and their twist by Coriolis force, is a popular paradigm for explaining the formation of tilted BMRs. In this study, we assess the validity of the thin flux tube model by analyzing the tracked BMR data obtained through the Automatic Tracking Algorithm for BMRs (AutoTAB). Our observations reveal that the tracked BMRs exhibit the expected collective behaviors. We find that the polarity separation of BMRs increases over their normalized lifetime, supporting the assumption of a rising flux tube from the CZ. Moreover, we observe an increasing trend of the tilt with the flux of the BMR, suggesting that rising flux tubes associated with lower flux regions are primarily influenced by drag force and Coriolis force, while in higher flux regions, magnetic buoyancy dominates. Furthermore, we observe Joy's law dependence for emerging BMRs from their first detection, indicating that at least a portion of the tilt observed in BMRs can be attributed to the Coriolis force. Notably, lower flux regions exhibit a higher amount of fluctuations associated with their tilt measurement compared to stronger flux regions, suggesting that lower flux regions are more susceptible to turbulent convection.","sentences":["One of the intriguing mechanisms of the Sun is the formation of the bipolar magnetic regions (BMRs) in the solar convection zone which are observed as regions of concentrated magnetic fields of opposite polarity on photosphere.","These BMRs are tilted with respect to the equatorial line, which statistically increases with latitude.","The thin flux tube model, employing the rise of magnetically buoyant flux loops and their twist by Coriolis force, is a popular paradigm for explaining the formation of tilted BMRs.","In this study, we assess the validity of the thin flux tube model by analyzing the tracked BMR data obtained through the Automatic Tracking Algorithm for BMRs (AutoTAB).","Our observations reveal that the tracked BMRs exhibit the expected collective behaviors.","We find that the polarity separation of BMRs increases over their normalized lifetime, supporting the assumption of a rising flux tube from the CZ.","Moreover, we observe an increasing trend of the tilt with the flux of the BMR, suggesting that rising flux tubes associated with lower flux regions are primarily influenced by drag force and Coriolis force, while in higher flux regions, magnetic buoyancy dominates.","Furthermore, we observe Joy's law dependence for emerging BMRs from their first detection, indicating that at least a portion of the tilt observed in BMRs can be attributed to the Coriolis force.","Notably, lower flux regions exhibit a higher amount of fluctuations associated with their tilt measurement compared to stronger flux regions, suggesting that lower flux regions are more susceptible to turbulent convection."],"url":"http://arxiv.org/abs/2403.09229v1","category":"astro-ph.SR"}
{"created":"2024-03-14 08:47:19","title":"Synchronisation-Oriented Design Approach for Adaptive Control","abstract":"This study presents a synchronisation-oriented perspective towards adaptive control which views model-referenced adaptation as synchronisation between actual and virtual dynamic systems. In the context of adaptation, model reference adaptive control methods make the state response of the actual plant follow a reference model. In the context of synchronisation, consensus methods involving diffusive coupling induce a collective behaviour across multiple agents. We draw from the understanding about the two time-scale nature of synchronisation motivated by the study of blended dynamics. The synchronisation-oriented approach consists in the design of a coupling input to achieve desired closed-loop error dynamics followed by the input allocation process to shape the collective behaviour. We suggest that synchronisation can be a reasonable design principle allowing a more holistic and systematic approach to the design of adaptive control systems for improved transient characteristics. Most notably, the proposed approach enables not only constructive derivation but also substantial generalisation of the previously developed closed-loop reference model adaptive control method. Practical significance of the proposed generalisation lies at the capability to improve the transient response characteristics and mitigate the unwanted peaking phenomenon at the same time.","sentences":["This study presents a synchronisation-oriented perspective towards adaptive control which views model-referenced adaptation as synchronisation between actual and virtual dynamic systems.","In the context of adaptation, model reference adaptive control methods make the state response of the actual plant follow a reference model.","In the context of synchronisation, consensus methods involving diffusive coupling induce a collective behaviour across multiple agents.","We draw from the understanding about the two time-scale nature of synchronisation motivated by the study of blended dynamics.","The synchronisation-oriented approach consists in the design of a coupling input to achieve desired closed-loop error dynamics followed by the input allocation process to shape the collective behaviour.","We suggest that synchronisation can be a reasonable design principle allowing a more holistic and systematic approach to the design of adaptive control systems for improved transient characteristics.","Most notably, the proposed approach enables not only constructive derivation but also substantial generalisation of the previously developed closed-loop reference model adaptive control method.","Practical significance of the proposed generalisation lies at the capability to improve the transient response characteristics and mitigate the unwanted peaking phenomenon at the same time."],"url":"http://arxiv.org/abs/2403.09179v1","category":"eess.SY"}
{"created":"2024-03-14 08:31:39","title":"ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks","abstract":"Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data. As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs. However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones. Employing an adversarial training framework, the edge predictor utilizes the line graph transformed from the original graph to estimate the edges to be dropped, which improves the interpretability of the edge-dropping method. The proposed ADEdgeDrop is optimized alternately by stochastic gradient descent and projected gradient descent. Comprehensive experiments on six graph benchmark datasets demonstrate that the proposed ADEdgeDrop outperforms state-of-the-art baselines across various GNN backbones, demonstrating improved generalization and robustness.","sentences":["Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data.","As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention.","Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs.","However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing.","In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones.","Employing an adversarial training framework, the edge predictor utilizes the line graph transformed from the original graph to estimate the edges to be dropped, which improves the interpretability of the edge-dropping method.","The proposed ADEdgeDrop is optimized alternately by stochastic gradient descent and projected gradient descent.","Comprehensive experiments on six graph benchmark datasets demonstrate that the proposed ADEdgeDrop outperforms state-of-the-art baselines across various GNN backbones, demonstrating improved generalization and robustness."],"url":"http://arxiv.org/abs/2403.09171v1","category":"cs.LG"}
{"created":"2024-03-14 07:51:20","title":"Stoner ferromagnetism, correlated metal and thermoelectricity in partially flat-band materials","abstract":"Recent discovery of correlated electronic phases in twisted heterostructures raised a surge of interests in studying models and materials with flat bands where the electronic excitations are nearly dispersionless in momentum space. As such, the kinetic energy is quenched and the correlations are enhanced, giving rise to a plethora of unusual magnetic, superconducting and transport behaviors. Finding materials whose energy bands are completely flat is rather challenging, yet those whose dispersion is flat only in a portion of the momentum space might be more accessible in material search. In this work, we propose a partially flat-band system on a square lattice. Using the Hubbard model, it is demonstrated that the suppression of the electronic kinetic energy in the flat portion of the band dispersion drives the system to Stoner ferromagnetism even at very weak interactions, i.e., much smaller than the bandwidth, with significantly enhanced Curie temperature. While the low-energy magnon modes are well defined collective excitations, flat magnon bands can be observed at high energies. We show that the strong interaction leads to reduction of the flat portion of the magnon band. However, tuning the chemical potential at a strong interaction regime may lead to spin density wave at finite wave vectors. Then, focusing on the non-magnetic correlated phase and using dynamical mean-field theory, we demonstrate the appearance of a flat-band induced sharp peak in the density of states in addition to the correlation-induced Mott bands. Furthermore, the large seebeck coefficient and the figure of merit of the proposed partially flat-band model, compared to symmetric regular band models, put them in the category of efficient thermoelectric materials.","sentences":["Recent discovery of correlated electronic phases in twisted heterostructures raised a surge of interests in studying models and materials with flat bands where the electronic excitations are nearly dispersionless in momentum space.","As such, the kinetic energy is quenched and the correlations are enhanced, giving rise to a plethora of unusual magnetic, superconducting and transport behaviors.","Finding materials whose energy bands are completely flat is rather challenging, yet those whose dispersion is flat only in a portion of the momentum space might be more accessible in material search.","In this work, we propose a partially flat-band system on a square lattice.","Using the Hubbard model, it is demonstrated that the suppression of the electronic kinetic energy in the flat portion of the band dispersion drives the system to Stoner ferromagnetism even at very weak interactions, i.e., much smaller than the bandwidth, with significantly enhanced Curie temperature.","While the low-energy magnon modes are well defined collective excitations, flat magnon bands can be observed at high energies.","We show that the strong interaction leads to reduction of the flat portion of the magnon band.","However, tuning the chemical potential at a strong interaction regime may lead to spin density wave at finite wave vectors.","Then, focusing on the non-magnetic correlated phase and using dynamical mean-field theory, we demonstrate the appearance of a flat-band induced sharp peak in the density of states in addition to the correlation-induced Mott bands.","Furthermore, the large seebeck coefficient and the figure of merit of the proposed partially flat-band model, compared to symmetric regular band models, put them in the category of efficient thermoelectric materials."],"url":"http://arxiv.org/abs/2403.09147v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 07:40:54","title":"USimAgent: Large Language Models for Simulating Search Users","abstract":"Due to the advantages in the cost-efficiency and reproducibility, user simulation has become a promising solution to the user-centric evaluation of information retrieval systems. Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, reasoning, and planning. Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks. However, the potential of using LLMs in simulating search behaviors has not yet been fully explored. In this paper, we introduce a LLM-based user search behavior simulator, USimAgent. The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search tasks. Empirical investigation on a real user behavior dataset shows that the proposed simulator outperforms existing methods in query generation and is comparable to traditional methods in predicting user clicks and stopping behaviors. These results not only validate the effectiveness of using LLMs for user simulation but also shed light on the development of a more robust and generic user simulators.","sentences":["Due to the advantages in the cost-efficiency and reproducibility, user simulation has become a promising solution to the user-centric evaluation of information retrieval systems.","Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, reasoning, and planning.","Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks.","However, the potential of using LLMs in simulating search behaviors has not yet been fully explored.","In this paper, we introduce a LLM-based user search behavior simulator, USimAgent.","The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search tasks.","Empirical investigation on a real user behavior dataset shows that the proposed simulator outperforms existing methods in query generation and is comparable to traditional methods in predicting user clicks and stopping behaviors.","These results not only validate the effectiveness of using LLMs for user simulation but also shed light on the development of a more robust and generic user simulators."],"url":"http://arxiv.org/abs/2403.09142v1","category":"cs.IR"}
{"created":"2024-03-14 07:40:32","title":"Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices","abstract":"Initially considered as low-power units with limited autonomous processing, Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and AI accelerators. This advancement has vastly amplified their computational capabilities, emphasizing the practicality of edge AI. Such progress introduces new challenges of optimizing AI tasks for the limitations of energy and network resources typical in Edge computing environments. Our study explores methods that enable distributed data processing through AI-enabled edge devices, enhancing collaborative learning capabilities. A key focus of our research is the challenge of determining confidence levels in learning outcomes, considering the spatial and temporal variability of data sets encountered by independent agents. To address this issue, we investigate the application of Bayesian neural networks, proposing a novel approach to manage uncertainty in distributed learning environments.","sentences":["Initially considered as low-power units with limited autonomous processing, Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and AI accelerators.","This advancement has vastly amplified their computational capabilities, emphasizing the practicality of edge AI.","Such progress introduces new challenges of optimizing AI tasks for the limitations of energy and network resources typical in Edge computing environments.","Our study explores methods that enable distributed data processing through AI-enabled edge devices, enhancing collaborative learning capabilities.","A key focus of our research is the challenge of determining confidence levels in learning outcomes, considering the spatial and temporal variability of data sets encountered by independent agents.","To address this issue, we investigate the application of Bayesian neural networks, proposing a novel approach to manage uncertainty in distributed learning environments."],"url":"http://arxiv.org/abs/2403.09141v1","category":"cs.DC"}
{"created":"2024-03-14 07:27:26","title":"Study on Standardizing Working Time: A Case of XYZ Retail Store in Bandung, Indonesia","abstract":"Work time standardization helps to find and reduce wasteful movements and time in the workplace, such as chatting, mobile phone use, insufficient rest, or unproductive tasks. This study aims to map the process of displaying products from the warehouse to the shelves and calculate and determine the standard working time of employees of the Operations Division of PT XYZ Branch who oversee displaying X Milk and Y Bread. The data was collected six times in three weeks, including interviews and observations, and took a sample of 20 pieces on each product to carry out data analysis such as data sufficiency tests and control charts. Several time deviations were found in the display process of X Milk products on all observation days in different activities. Whereas in the process of displaying Y Bread, only the deviation of working time was found on the 4th observation day, which proves that the process needs to have a standard working time so that the activity work time is more controlled. Therefore, the analysis is carried out with the calculation of performance rating, time allowance, normal time, and standard time. The result of the standard time calculation for the display process of X Milk products is 15.83 minutes and Y Bread is 9.18 minutes for each product of 20 units.","sentences":["Work time standardization helps to find and reduce wasteful movements and time in the workplace, such as chatting, mobile phone use, insufficient rest, or unproductive tasks.","This study aims to map the process of displaying products from the warehouse to the shelves and calculate and determine the standard working time of employees of the Operations Division of PT XYZ Branch who oversee displaying X Milk and Y Bread.","The data was collected six times in three weeks, including interviews and observations, and took a sample of 20 pieces on each product to carry out data analysis such as data sufficiency tests and control charts.","Several time deviations were found in the display process of X Milk products on all observation days in different activities.","Whereas in the process of displaying Y Bread, only the deviation of working time was found on the 4th observation day, which proves that the process needs to have a standard working time so that the activity work time is more controlled.","Therefore, the analysis is carried out with the calculation of performance rating, time allowance, normal time, and standard time.","The result of the standard time calculation for the display process of X Milk products is 15.83 minutes and Y Bread is 9.18 minutes for each product of 20 units."],"url":"http://arxiv.org/abs/2403.09138v1","category":"econ.GN"}
{"created":"2024-03-14 06:49:16","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text","abstract":"Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our approach outperforms baselines in switching between professional and non-professional text generation.","sentences":["Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation.","However, studies into their capacity of switching between styles via fine-tuning remain underexplored.","This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning.","ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text.","Comparative analysis of ProSwitch against both general and specialized language models reveals that our approach outperforms baselines in switching between professional and non-professional text generation."],"url":"http://arxiv.org/abs/2403.09131v1","category":"cs.CL"}
{"created":"2024-03-14 06:17:20","title":"Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector","abstract":"Large Language Models (LLMs) as chatbots have drawn remarkable attention thanks to their versatile capability in natural language processing as well as in a wide range of tasks. While there has been great enthusiasm towards adopting such foundational model-based artificial intelligence tools in all sectors possible, the capabilities and limitations of such LLMs in improving the operation of the electric energy sector need to be explored, and this article identifies fruitful directions in this regard. Key future research directions include data collection systems for fine-tuning LLMs, embedding power system-specific tools in the LLMs, and retrieval augmented generation (RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in safety-critical use cases.","sentences":["Large Language Models (LLMs) as chatbots have drawn remarkable attention thanks to their versatile capability in natural language processing as well as in a wide range of tasks.","While there has been great enthusiasm towards adopting such foundational model-based artificial intelligence tools in all sectors possible, the capabilities and limitations of such LLMs in improving the operation of the electric energy sector need to be explored, and this article identifies fruitful directions in this regard.","Key future research directions include data collection systems for fine-tuning LLMs, embedding power system-specific tools in the LLMs, and retrieval augmented generation (RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in safety-critical use cases."],"url":"http://arxiv.org/abs/2403.09125v1","category":"eess.SY"}
{"created":"2024-03-14 05:35:56","title":"Global existence and asymptotic stability for the Toner-Tu model of flocking","abstract":"This paper deals with the Toner-Tu (TT) model, which is a hydrodynamic model describing the collective motion of numerous self-propelled agents. We analytically study the global-in-time well-posedness of the TT model near the steady-state solution in the ordered phase. We also show the large-time behavior of solutions showing that the steady-state solution is polynomially stable in a Sobolev space in the sense that solutions that are initially close to that steady state converge to that at least polynomially fast as time tends to infinity. Moreover, we investigate the variant of the TT model which describes the dynamics of the actin filament.","sentences":["This paper deals with the Toner-Tu (TT) model, which is a hydrodynamic model describing the collective motion of numerous self-propelled agents.","We analytically study the global-in-time well-posedness of the TT model near the steady-state solution in the ordered phase.","We also show the large-time behavior of solutions showing that the steady-state solution is polynomially stable in a Sobolev space in the sense that solutions that are initially close to that steady state converge to that at least polynomially fast as time tends to infinity.","Moreover, we investigate the variant of the TT model which describes the dynamics of the actin filament."],"url":"http://arxiv.org/abs/2403.09114v1","category":"math.AP"}
{"created":"2024-03-14 05:29:35","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning","abstract":"Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded. A meta learning based method is developed to learn these selection variables. The optimal rank is determined by thresholding the values of these variables. Our comprehensive experiments on natural language understanding, generation, and sequence labeling demonstrate the effectiveness of AutoLoRA.","sentences":["Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks.","Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed.","Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective.","Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance.","To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer.","AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded.","A meta learning based method is developed to learn these selection variables.","The optimal rank is determined by thresholding the values of these variables.","Our comprehensive experiments on natural language understanding, generation, and sequence labeling demonstrate the effectiveness of AutoLoRA."],"url":"http://arxiv.org/abs/2403.09113v1","category":"cs.CL"}
{"created":"2024-03-14 04:43:02","title":"AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications","abstract":"Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts. Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies. We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art chatbot models on the task of expert data annotation. Using the arXiv publication database as ground-truth, we experiment with prompt engineering for GPT chatbot models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy. For comparison, we fine-tune SPECTER, a transformer language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than GPT) on classifying AI publications. Our results indicate that with effective prompt engineering, chatbots can be used as reliable data annotators even where subject-area expertise is required. To evaluate the utility of chatbot-annotated datasets on downstream classification tasks, we train a new classifier on GPT-labeled data and compare its performance to the arXiv-trained model. The classifier trained on GPT-labeled data outperforms the arXiv-trained model by nine percentage points, achieving 82% accuracy.","sentences":["Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts.","Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies.","We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art chatbot models on the task of expert data annotation.","Using the arXiv publication database as ground-truth, we experiment with prompt engineering for GPT chatbot models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy.","For comparison, we fine-tune SPECTER, a transformer language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than GPT) on classifying AI publications.","Our results indicate that with effective prompt engineering, chatbots can be used as reliable data annotators even where subject-area expertise is required.","To evaluate the utility of chatbot-annotated datasets on downstream classification tasks, we train a new classifier on GPT-labeled data and compare its performance to the arXiv-trained model.","The classifier trained on GPT-labeled data outperforms the arXiv-trained model by nine percentage points, achieving 82% accuracy."],"url":"http://arxiv.org/abs/2403.09097v1","category":"cs.CL"}
{"created":"2024-03-14 04:32:13","title":"MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection","abstract":"The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets. Notably, such news has been fact-checked by 14 authoritative fact-checking agencies worldwide. In addition, various existing Chinese fake news detection methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news detection approaches in real-world scenarios.","sentences":["The prevalence of fake news across various online sources has had a significant influence on the public.","Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo.","However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context.","Methods trained on purely one single news source can hardly be applicable to real-world scenarios.","Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news.","To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets.","Notably, such news has been fact-checked by 14 authoritative fact-checking agencies worldwide.","In addition, various existing Chinese fake news detection methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways.","MCFEND, as a benchmark dataset, aims to advance Chinese fake news detection approaches in real-world scenarios."],"url":"http://arxiv.org/abs/2403.09092v1","category":"cs.CL"}
{"created":"2024-03-14 04:06:13","title":"Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance","abstract":"Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts.","sentences":["Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence.","Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities.","This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing.","In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs.","Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances.","To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes.","The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts."],"url":"http://arxiv.org/abs/2403.09085v1","category":"cs.CL"}
{"created":"2024-03-14 04:01:13","title":"Asymptotically Near-Optimal Hybrid Beamforming for mmWave IRS-Aided MIMO Systems","abstract":"Hybrid beamforming is an emerging technology for massive multiple-input multiple-output (MIMO) systems due to the advantages of lower complexity, cost, and power consumption. Recently, intelligent reflection surface (IRS) has been proposed as the cost-effective technique for robust millimeter-wave (mmWave) MIMO systems. Thus, it is required to jointly optimize a reflection vector and hybrid beamforming matrices for IRS-aided mmWave MIMO systems. Due to the lack of RF chain in the IRS, it is unavailable to acquire the TX-IRS and IRS-RX channels separately. Instead, there are efficient methods to estimate the so-called effective (or cascaded) channel in literature. We for the first time derive the near-optimal solution of the aforementioned joint optimization only using the effective channel. Based on our theoretical analysis, we develop the practical reflection vector and hybrid beamforming matrices by projecting the asymptotic solution into the modulus constraint. Via simulations, it is demonstrated that the proposed construction can outperform the state-of-the-art (SOTA) method, where the latter even requires the knowledge of the TX-IRS ad IRS-RX channels separately. Furthermore, our construction can provide the robustness for channel estimation errors, which is inevitable for practical massive MIMO systems.","sentences":["Hybrid beamforming is an emerging technology for massive multiple-input multiple-output (MIMO) systems due to the advantages of lower complexity, cost, and power consumption.","Recently, intelligent reflection surface (IRS) has been proposed as the cost-effective technique for robust millimeter-wave (mmWave) MIMO systems.","Thus, it is required to jointly optimize a reflection vector and hybrid beamforming matrices for IRS-aided mmWave MIMO systems.","Due to the lack of RF chain in the IRS, it is unavailable to acquire the TX-IRS and IRS-RX channels separately.","Instead, there are efficient methods to estimate the so-called effective (or cascaded) channel in literature.","We for the first time derive the near-optimal solution of the aforementioned joint optimization only using the effective channel.","Based on our theoretical analysis, we develop the practical reflection vector and hybrid beamforming matrices by projecting the asymptotic solution into the modulus constraint.","Via simulations, it is demonstrated that the proposed construction can outperform the state-of-the-art (SOTA) method, where the latter even requires the knowledge of the TX-IRS ad IRS-RX channels separately.","Furthermore, our construction can provide the robustness for channel estimation errors, which is inevitable for practical massive MIMO systems."],"url":"http://arxiv.org/abs/2403.09083v1","category":"eess.SP"}
{"created":"2024-03-14 03:57:10","title":"The absence of monochromatic triangle implies various properly colored spanning trees","abstract":"An edge-colored graph $G$ is called properly colored if every two adjacent edges are assigned different colors. A monochromatic triangle is a cycle of length 3 with all the edges having the same color. Given a tree $T_0$, let $\\mathcal{T}(n,T_0)$ be the collection of $n$-vertex trees that are subdivisions of $T_0$. It is conjectured that for each fixed tree $T_0$ of $k$ edges, there is a function $f(k)$ such that for each integer $n\\geq f(k)$ and each $T\\in \\mathcal{T}(n,T_0)$, every edge-colored complete graph $K_n$ without containing monochromatic triangle must contain a properly colored copy of $T$. We confirm the conjecture in the case that $T_0$ is a star. A weaker version of the above conjecture is also obtained. Moreover, to get a nice quantitative estimation of $f(k)$ requires determining the constraint Ramsey number of a monochromatic triangle and a rainbow $k$-star, which is of independent interest.","sentences":["An edge-colored graph $G$ is called properly colored if every two adjacent edges are assigned different colors.","A monochromatic triangle is a cycle of length 3 with all the edges having the same color.","Given a tree $T_0$, let $\\mathcal{T}(n,T_0)$ be the collection of $n$-vertex trees that are subdivisions of $T_0$. It is conjectured that for each fixed tree $T_0$ of $k$ edges, there is a function $f(k)$ such that for each integer $n\\geq f(k)$ and each $T\\in \\mathcal{T}(n,T_0)$, every edge-colored complete graph $K_n$ without containing monochromatic triangle must contain a properly colored copy of $T$. We confirm the conjecture in the case that $T_0$ is a star.","A weaker version of the above conjecture is also obtained.","Moreover, to get a nice quantitative estimation of $f(k)$ requires determining the constraint Ramsey number of a monochromatic triangle and a rainbow $k$-star, which is of independent interest."],"url":"http://arxiv.org/abs/2403.09082v1","category":"math.CO"}
{"created":"2024-03-14 03:33:46","title":"Large Language Models are Parallel Multilingual Learners","abstract":"In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence.","sentences":["In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities.","To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs.","Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help.","Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon.","Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added.","This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence."],"url":"http://arxiv.org/abs/2403.09073v1","category":"cs.CL"}
{"created":"2024-03-14 03:29:58","title":"UniCode: Learning a Unified Codebook for Multimodal Large Language Models","abstract":"In this paper, we propose \\textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using significantly fewer parameters and less data during training, Unicode demonstrates promising capabilities in visual reconstruction and generation. It also achieves performances comparable to leading MLLMs across a spectrum of VQA benchmarks.","sentences":["In this paper, we propose \\textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals.","This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context.","Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.","The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks.","Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation.","Despite using significantly fewer parameters and less data during training, Unicode demonstrates promising capabilities in visual reconstruction and generation.","It also achieves performances comparable to leading MLLMs across a spectrum of VQA benchmarks."],"url":"http://arxiv.org/abs/2403.09072v1","category":"cs.CV"}
{"created":"2024-03-14 03:07:58","title":"Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery","abstract":"Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable challenge and is often hindered by depth ambiguities and reduced precision. Existing works resort to either pose priors or multi-modal data such as multi-view or point cloud information, though their methods often overlook the valuable scene-depth information inherently present in a single image. Moreover, achieving robust HMR for out-of-distribution (OOD) data is exceedingly challenging due to inherent variations in pose, shape and depth. Consequently, understanding the underlying distribution becomes a vital subproblem in modeling human forms. Motivated by the need for unambiguous and robust human modeling, we introduce Distribution and depth-aware human mesh recovery (D2A-HMR), an end-to-end transformer architecture meticulously designed to minimize the disparity between distributions and incorporate scene-depth leveraging prior depth information. Our approach demonstrates superior performance in handling OOD data in certain scenarios while consistently achieving competitive results against state-of-the-art HMR methods on controlled datasets.","sentences":["Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable challenge and is often hindered by depth ambiguities and reduced precision.","Existing works resort to either pose priors or multi-modal data such as multi-view or point cloud information, though their methods often overlook the valuable scene-depth information inherently present in a single image.","Moreover, achieving robust HMR for out-of-distribution (OOD) data is exceedingly challenging due to inherent variations in pose, shape and depth.","Consequently, understanding the underlying distribution becomes a vital subproblem in modeling human forms.","Motivated by the need for unambiguous and robust human modeling, we introduce Distribution and depth-aware human mesh recovery (D2A-HMR), an end-to-end transformer architecture meticulously designed to minimize the disparity between distributions and incorporate scene-depth leveraging prior depth information.","Our approach demonstrates superior performance in handling OOD data in certain scenarios while consistently achieving competitive results against state-of-the-art HMR methods on controlled datasets."],"url":"http://arxiv.org/abs/2403.09063v1","category":"cs.CV"}
{"created":"2024-03-14 03:07:49","title":"TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research","abstract":"In this paper, we introduce a new dataset in the medical field of Traumatic Brain Injury (TBI), called TBI-IT, which includes both electronic medical records (EMRs) and head CT images. This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of TBI. This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the EMRs, extracting key content from the text information, and categorizes the annotation content of imaging data into five types: brain midline, hematoma, left cerebral ventricle, right cerebral ventricle and fracture. TBI-IT aims to be a foundational dataset for feature learning in image segmentation tasks and named entity recognition.","sentences":["In this paper, we introduce a new dataset in the medical field of Traumatic Brain Injury (TBI), called TBI-IT, which includes both electronic medical records (EMRs) and head CT images.","This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of TBI.","This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the EMRs, extracting key content from the text information, and categorizes the annotation content of imaging data into five types: brain midline, hematoma, left cerebral ventricle, right cerebral ventricle and fracture.","TBI-IT aims to be a foundational dataset for feature learning in image segmentation tasks and named entity recognition."],"url":"http://arxiv.org/abs/2403.09062v1","category":"eess.IV"}
{"created":"2024-03-14 02:56:32","title":"Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs","abstract":"This paper investigates a reconfigurable intelligent surface (RIS)-aided wideband massive multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system with low-resolution analog-to-digital converters (ADCs). Frequency-selective Rician fading channels are considered, and the OFDM data transmission process is presented in time domain. This paper derives the closed-form approximate expression of the uplink achievable rate, based on which the asymptotic system performance is analyzed when the number of the antennas at the base station and the number of reflecting elements at the RIS grow to infinity. Besides, the power scaling laws of the considered system are revealed to provide energy-saving insights. Furthermore, this paper proposes a gradient ascent-based algorithm to design the phase shifts of the RIS for maximizing the minimum user rate. Finally, numerical results are presented to verify the correctness of analytical conclusions and draw insights.","sentences":["This paper investigates a reconfigurable intelligent surface (RIS)-aided wideband massive multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system with low-resolution analog-to-digital converters (ADCs).","Frequency-selective Rician fading channels are considered, and the OFDM data transmission process is presented in time domain.","This paper derives the closed-form approximate expression of the uplink achievable rate, based on which the asymptotic system performance is analyzed when the number of the antennas at the base station and the number of reflecting elements at the RIS grow to infinity.","Besides, the power scaling laws of the considered system are revealed to provide energy-saving insights.","Furthermore, this paper proposes a gradient ascent-based algorithm to design the phase shifts of the RIS for maximizing the minimum user rate.","Finally, numerical results are presented to verify the correctness of analytical conclusions and draw insights."],"url":"http://arxiv.org/abs/2403.09058v1","category":"cs.IT"}
{"created":"2024-03-14 02:55:37","title":"A Continued Pretrained LLM Approach for Automatic Medical Note Generation","abstract":"LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.","sentences":["LLMs are revolutionizing NLP tasks.","However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios.","We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing.","Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\\% accuracy and matches its performance in summarizing medical conversations into SOAP notes.","Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness."],"url":"http://arxiv.org/abs/2403.09057v1","category":"cs.CL"}
{"created":"2024-03-14 02:55:06","title":"Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines","abstract":"On modern industrial assembly lines, many intelligent algorithms have been developed to replace or supervise workers. However, we found that there were bottlenecks in both training datasets and real-time performance when deploying algorithms on actual assembly line. Therefore, we developed a promising strategy for expanding industrial datasets, which utilized large models with strong generalization abilities to achieve efficient, high-quality, and large-scale dataset expansion, solving the problem of insufficient and low-quality industrial datasets. We also applied this strategy to video action recognition. We proposed a method of converting hand action recognition problems into hand skeletal trajectory classification problems, which solved the real-time performance problem of industrial algorithms. In the \"hand movements during wire insertion\" scenarios on the actual assembly line, the accuracy of hand action recognition reached 98.8\\%. We conducted detailed experimental analysis to demonstrate the effectiveness and superiority of the method, and deployed the entire process on Midea's actual assembly line.","sentences":["On modern industrial assembly lines, many intelligent algorithms have been developed to replace or supervise workers.","However, we found that there were bottlenecks in both training datasets and real-time performance when deploying algorithms on actual assembly line.","Therefore, we developed a promising strategy for expanding industrial datasets, which utilized large models with strong generalization abilities to achieve efficient, high-quality, and large-scale dataset expansion, solving the problem of insufficient and low-quality industrial datasets.","We also applied this strategy to video action recognition.","We proposed a method of converting hand action recognition problems into hand skeletal trajectory classification problems, which solved the real-time performance problem of industrial algorithms.","In the \"hand movements during wire insertion\" scenarios on the actual assembly line, the accuracy of hand action recognition reached 98.8\\%.","We conducted detailed experimental analysis to demonstrate the effectiveness and superiority of the method, and deployed the entire process on Midea's actual assembly line."],"url":"http://arxiv.org/abs/2403.09056v1","category":"cs.CV"}
{"created":"2024-03-14 02:42:42","title":"Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference","abstract":"Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.","sentences":["Transformers have emerged as the underpinning architecture for Large Language Models (LLMs).","In generative language models, the inference process involves two primary phases: prompt processing and token generation.","Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache.","This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units.","This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   ","This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization.","Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as \"key\" tokens.","Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function.","This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy.","We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms.","Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts.","Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy."],"url":"http://arxiv.org/abs/2403.09054v1","category":"cs.LG"}
{"created":"2024-03-14 02:42:19","title":"Towards a theory of model distillation","abstract":"Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.","sentences":["Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15].","Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   ","To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84].","As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity."],"url":"http://arxiv.org/abs/2403.09053v1","category":"cs.LG"}
{"created":"2024-03-14 02:26:10","title":"Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs","abstract":"Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes. The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection. To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively. Then STRIPE incorporates separate spatial and temporal memory networks, which capture and store prototypes of normal patterns, thereby preserving the uniqueness of spatial and temporal normality. After that, through a mutual attention mechanism, these stored patterns are then retrieved and integrated with encoded graph embeddings. Finally, the integrated features are fed into the decoder to reconstruct the graph streams which serve as the proxy task for anomaly detection. This comprehensive approach not only minimizes reconstruction errors but also refines the model by emphasizing the compactness and distinctiveness of the embeddings in relation to the nearest memory prototypes. Through extensive testing, STRIPE has demonstrated a superior capability to discern anomalies by effectively leveraging the distinct spatial and temporal dynamics of dynamic graphs, significantly outperforming existing methodologies, with an average improvement of 15.39% on AUC values.","sentences":["Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes.","The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing.","However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection.","To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE).","Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively.","Then STRIPE incorporates separate spatial and temporal memory networks, which capture and store prototypes of normal patterns, thereby preserving the uniqueness of spatial and temporal normality.","After that, through a mutual attention mechanism, these stored patterns are then retrieved and integrated with encoded graph embeddings.","Finally, the integrated features are fed into the decoder to reconstruct the graph streams which serve as the proxy task for anomaly detection.","This comprehensive approach not only minimizes reconstruction errors but also refines the model by emphasizing the compactness and distinctiveness of the embeddings in relation to the nearest memory prototypes.","Through extensive testing, STRIPE has demonstrated a superior capability to discern anomalies by effectively leveraging the distinct spatial and temporal dynamics of dynamic graphs, significantly outperforming existing methodologies, with an average improvement of 15.39% on AUC values."],"url":"http://arxiv.org/abs/2403.09039v1","category":"cs.LG"}
{"created":"2024-03-14 01:51:35","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences","abstract":"Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning. Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model. Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence. Our code and data are available at https://github.com/martin-wey/CodeUltraFeedback.","sentences":["Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs.","By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment.","In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback.","We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback.","We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences.","Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning.","Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model.","Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence.","Our code and data are available at https://github.com/martin-wey/CodeUltraFeedback."],"url":"http://arxiv.org/abs/2403.09032v1","category":"cs.SE"}
{"created":"2024-03-14 01:46:30","title":"An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals","abstract":"This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals. A convolutional LSTM model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation. To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information. The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability. On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status. The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential to enhance the reliability and efficiency of wind power generation.","sentences":["This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals.","A convolutional LSTM model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation.","To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information.","The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability.","On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status.","The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential to enhance the reliability and efficiency of wind power generation."],"url":"http://arxiv.org/abs/2403.09030v1","category":"cs.SD"}
{"created":"2024-03-14 01:40:40","title":"Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset","abstract":"Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.","sentences":["Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML.","Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored.","We posit that this is mainly due to the absence of a suitable, high-quality dataset.","This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots.","We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code.","To accelerate the research in this area, we open-source WebSight."],"url":"http://arxiv.org/abs/2403.09029v1","category":"cs.HC"}
{"created":"2024-03-14 01:39:40","title":"VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework","abstract":"With the emergence of large language models (LLMs) and vision foundation models, how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world visual perception remains an open question. In this paper, we introduce VisionGPT to consolidate and automate the integration of state-of-the-art foundation models, thereby facilitating vision-language understanding and the development of vision-oriented AI. VisionGPT builds upon a generalized multimodal framework that distinguishes itself through three key features: (1) utilizing LLMs (e.g., LLaMA-2) as the pivot to break down users' requests into detailed action proposals to call suitable foundation models; (2) integrating multi-source outputs from foundation models automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text-conditioned image understanding/generation/editing and visual question answering. This paper outlines the architecture and capabilities of VisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, and generalization, and performance. Our code and models will be made publicly available. Keywords: VisionGPT, Open-world visual perception, Vision-language understanding, Large language model, and Foundation model","sentences":["With the emergence of large language models (LLMs) and vision foundation models, how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world visual perception remains an open question.","In this paper, we introduce VisionGPT to consolidate and automate the integration of state-of-the-art foundation models, thereby facilitating vision-language understanding and the development of vision-oriented AI.","VisionGPT builds upon a generalized multimodal framework that distinguishes itself through three key features: (1) utilizing LLMs (e.g., LLaMA-2) as the pivot to break down users' requests into detailed action proposals to call suitable foundation models; (2) integrating multi-source outputs from foundation models automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text-conditioned image understanding/generation/editing and visual question answering.","This paper outlines the architecture and capabilities of VisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, and generalization, and performance.","Our code and models will be made publicly available.","Keywords: VisionGPT, Open-world visual perception, Vision-language understanding, Large language model, and Foundation model"],"url":"http://arxiv.org/abs/2403.09027v1","category":"cs.CV"}
{"created":"2024-03-14 01:28:13","title":"Semiparametric Token-Sequence Co-Supervision","abstract":"In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametric sequence embedding space, a new space established by another language model.","sentences":["In this work, we introduce a semiparametric token-sequence co-supervision training method.","It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space.","The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding.","Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently.","Analysis suggests that this co-supervision encourages a broader generalization capability across the model.","Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametric sequence embedding space, a new space established by another language model."],"url":"http://arxiv.org/abs/2403.09024v1","category":"cs.CL"}
{"created":"2024-03-14 01:24:19","title":"Quantum Annealing Approach for the Optimal Real-time Traffic Control using QUBO","abstract":"Traffic congestion is one of the major issues in urban areas, particularly when traffic loads exceed the roads capacity, resulting in higher petrol consumption and carbon emissions as well as delays and stress for road users. In Asia, the traffic situation can be further deteriorated by road sharing of scooters. How to control the traffic flow to mitigate the congestion has been one of the central issues in transportation research. In this study, we employ a quantum annealing approach to optimize the traffic signals control at a real-life intersection with mixed traffic flows of vehicles and scooters. Considering traffic flow is a continuous and emerging phenomenon, we used quadratic unconstrained binary optimization (QUBO) formalism for traffic optimization, which has a natural equivalence to the Ising model and can be solved efficiently on the quantum annealers, quantum computers or digital annealers. In this article, we first applied the QUBO traffic optimization to artificially generated traffic for a simple intersection, and then we used real-time traffic data to simulate a real Dongda-Keyuan intersection with dedicated cars and scooter lanes, as well as mixed scooter and car lanes. We introduced two types of traffic light control systems for traffic optimization C-QUBO and QUBO. Our rigorous QUBO optimizations show that C-QUBO and QUBO outperform the commonly used fixed cycle method, with QUBO outperforming C-QUBO in some instances. It has been found that QUBO optimization significantly relieves traffic congestion for the unbalanced traffic volume. Furthermore, we found that dynamic changes in traffic light signal duration greatly reduce traffic congestion.","sentences":["Traffic congestion is one of the major issues in urban areas, particularly when traffic loads exceed the roads capacity, resulting in higher petrol consumption and carbon emissions as well as delays and stress for road users.","In Asia, the traffic situation can be further deteriorated by road sharing of scooters.","How to control the traffic flow to mitigate the congestion has been one of the central issues in transportation research.","In this study, we employ a quantum annealing approach to optimize the traffic signals control at a real-life intersection with mixed traffic flows of vehicles and scooters.","Considering traffic flow is a continuous and emerging phenomenon, we used quadratic unconstrained binary optimization (QUBO) formalism for traffic optimization, which has a natural equivalence to the Ising model and can be solved efficiently on the quantum annealers, quantum computers or digital annealers.","In this article, we first applied the QUBO traffic optimization to artificially generated traffic for a simple intersection, and then we used real-time traffic data to simulate a real Dongda-Keyuan intersection with dedicated cars and scooter lanes, as well as mixed scooter and car lanes.","We introduced two types of traffic light control systems for traffic optimization C-QUBO and QUBO.","Our rigorous QUBO optimizations show that C-QUBO and QUBO outperform the commonly used fixed cycle method, with QUBO outperforming C-QUBO in some instances.","It has been found that QUBO optimization significantly relieves traffic congestion for the unbalanced traffic volume.","Furthermore, we found that dynamic changes in traffic light signal duration greatly reduce traffic congestion."],"url":"http://arxiv.org/abs/2403.09023v1","category":"quant-ph"}
{"created":"2024-03-14 00:45:24","title":"AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic","abstract":"The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities, privacy, and offensive language. By introducing AraTrust, we aim to promote collaborative efforts to create safer and more trustworthy LLMs for Arabic users. We evaluated a set of LLMs against our benchmark to assess its trustworthiness. GPT-4 showed to be the most trustworthy regarding Arabic language.","sentences":["The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI.","Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks.","Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic.","In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic.","AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities, privacy, and offensive language.","By introducing AraTrust, we aim to promote collaborative efforts to create safer and more trustworthy LLMs for Arabic users.","We evaluated a set of LLMs against our benchmark to assess its trustworthiness.","GPT-4 showed to be the most trustworthy regarding Arabic language."],"url":"http://arxiv.org/abs/2403.09017v1","category":"cs.CL"}
{"created":"2024-03-14 00:30:17","title":"Urban mapping in Dar es Salaam using AJIVE","abstract":"Mapping deprivation in urban areas is important, for example for identifying areas of greatest need and planning interventions. Traditional ways of obtaining deprivation estimates are based on either census or household survey data, which in many areas is unavailable or difficult to collect. However, there has been a huge rise in the amount of new, non-traditional forms of data, such as satellite imagery and cell-phone call-record data, which may contain information useful for identifying deprivation. We use Angle-Based Joint and Individual Variation Explained (AJIVE) to jointly model satellite imagery data, cell-phone data, and survey data for the city of Dar es Salaam, Tanzania. We first identify interpretable low-dimensional structure from the imagery and cell-phone data, and find that we can use these to identify deprivation. We then consider what is gained from further incorporating the more traditional and costly survey data. We also introduce a scalar measure of deprivation as a response variable to be predicted, and consider various approaches to multiview regression, including using AJIVE scores as predictors.","sentences":["Mapping deprivation in urban areas is important, for example for identifying areas of greatest need and planning interventions.","Traditional ways of obtaining deprivation estimates are based on either census or household survey data, which in many areas is unavailable or difficult to collect.","However, there has been a huge rise in the amount of new, non-traditional forms of data, such as satellite imagery and cell-phone call-record data, which may contain information useful for identifying deprivation.","We use Angle-Based Joint and Individual Variation Explained (AJIVE) to jointly model satellite imagery data, cell-phone data, and survey data for the city of Dar es Salaam, Tanzania.","We first identify interpretable low-dimensional structure from the imagery and cell-phone data, and find that we can use these to identify deprivation.","We then consider what is gained from further incorporating the more traditional and costly survey data.","We also introduce a scalar measure of deprivation as a response variable to be predicted, and consider various approaches to multiview regression, including using AJIVE scores as predictors."],"url":"http://arxiv.org/abs/2403.09014v1","category":"stat.AP"}
{"created":"2024-03-13 23:37:02","title":"Building-block flow model for computational fluids","abstract":"We introduce a closure model for computational fluid dynamics, referred to as the Building-block Flow Model (BFM). The foundation of the model rests on the premise that a finite collection of simple flows encapsulates the essential physics necessary to predict more complex scenarios. The BFM is implemented using artificial neural networks and introduces five unique advancements within the framework of large-eddy simulation: (1) It is designed to predict multiple flow regimes (wall turbulence under zero, favorable, adverse mean-pressure-gradient, and separation); (2) It unifies the closure model at solid boundaries (i.e., the wall model) and the rest of the flow (i.e., the subgrid-scale model) into a single entity; (3) It ensures consistency with numerical schemes and gridding strategy by accounting for numerical errors; (4) It is directly applicable to arbitrary complex geometries; (5) It can be scaled up to model additional flow physics in the future if needed (e.g., shockwaves and laminar-to-turbulent transition). The BFM is utilized to predict key quantities of interest in turbulent channel flows, a Gaussian bump, and an aircraft in a landing configuration. In all cases, the BFM demonstrates similar or superior capabilities in terms of accuracy and computational efficiency compared to previous state-of-the-art closure models.","sentences":["We introduce a closure model for computational fluid dynamics, referred to as the Building-block Flow Model (BFM).","The foundation of the model rests on the premise that a finite collection of simple flows encapsulates the essential physics necessary to predict more complex scenarios.","The BFM is implemented using artificial neural networks and introduces five unique advancements within the framework of large-eddy simulation: (1) It is designed to predict multiple flow regimes (wall turbulence under zero, favorable, adverse mean-pressure-gradient, and separation); (2) It unifies the closure model at solid boundaries (i.e., the wall model) and the rest of the flow (i.e., the subgrid-scale model) into a single entity; (3) It ensures consistency with numerical schemes and gridding strategy by accounting for numerical errors; (4) It is directly applicable to arbitrary complex geometries; (5) It can be scaled up to model additional flow physics in the future if needed (e.g., shockwaves and laminar-to-turbulent transition).","The BFM is utilized to predict key quantities of interest in turbulent channel flows, a Gaussian bump, and an aircraft in a landing configuration.","In all cases, the BFM demonstrates similar or superior capabilities in terms of accuracy and computational efficiency compared to previous state-of-the-art closure models."],"url":"http://arxiv.org/abs/2403.09000v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 23:23:51","title":"Improved bass model using sales proportional average for one condition of mono peak curves","abstract":"\"This study provides a modified Bass model to deal with trend curves for basic issues of relevance to individuals from all over the world, for which we collected 16 data sets from 2004 to 2022 and that are available on Google servers as \"google trends\". It was discovered that the Bass model did not forecast well for curves that have a mono peak with a sharp decrease to some level then have semi-stable with small decrement sales for a long time, thus a new parameter based on r1 and r2 (ratios of average sales) was introduced, which improved the model's prediction ability and provided better results. The model was also applied to a data set taken from the Kaggle website about a subscriber digital product offering for financial services that include newsletters, webinars, and investment recommendations. The data contain 508932 data points about the products sold during 2016-2022. Compared to the traditional Bass model, the modified model showed better results in dealing with this condition, as the expected curve shape was closer to real sales, and the sum of squares error (SSE) value was reduced to a ratio ranging between (36.35-79.3%). Therefore, the improved model can be relied upon in these conditions.\"","sentences":["\"This study provides a modified Bass model to deal with trend curves for basic issues of relevance to individuals from all over the world, for which we collected 16 data sets from 2004 to 2022 and that are available on Google servers as \"google trends\".","It was discovered that the Bass model did not forecast well for curves that have a mono peak with a sharp decrease to some level then have semi-stable with small decrement sales for a long time, thus a new parameter based on r1 and r2 (ratios of average sales) was introduced, which improved the model's prediction ability and provided better results.","The model was also applied to a data set taken from the Kaggle website about a subscriber digital product offering for financial services that include newsletters, webinars, and investment recommendations.","The data contain 508932 data points about the products sold during 2016-2022.","Compared to the traditional Bass model, the modified model showed better results in dealing with this condition, as the expected curve shape was closer to real sales, and the sum of squares error (SSE) value was reduced to a ratio ranging between (36.35-79.3%).","Therefore, the improved model can be relied upon in these conditions.\""],"url":"http://arxiv.org/abs/2403.08993v1","category":"cs.CE"}
{"created":"2024-03-13 23:14:49","title":"Security Assumptions in Dispersive-Optics QKD","abstract":"Quantum key distribution (QKD) seeks to provide a method of generating cryptographically-secure keys between remote parties while guaranteeing unconditional security. Implementations of high-dimensional QKD using dispersive-optics (DO-QKD) have been proposed to allow for multiple secure bits to be transmitted per photon while remaining cost-effective and scalable using existing telecommunication technology [1]. In the recent literature, there have been a number of experimental realizations of DO-QKD systems [2-6], with security analysis based on the treatment in Ref. [1]. Here we demonstrate that in the case of finite dispersion, the model assumed for the eavesdropper's attack in Ref. [1] is non-optimal for the eavesdropper, which leads to a significant overestimation of the secure key rate between parties. We consider an alternative attack model that Alice and Bob find indistinguishable from the Ref. [1] model, as long as they are restricted to making the measurements typical in DO-QKD. We provide concrete examples where a significant gap exists between the Holevo information, and therefore the secret key rate, predicted by the two models. We further analyze the experiment in Ref. [2] as an example of a case where secure key is predicted according to the Ref. [1] model, but where in fact there is zero secure key rate when considering the full set of collective attacks that an eavesdropper may perform.","sentences":["Quantum key distribution (QKD) seeks to provide a method of generating cryptographically-secure keys between remote parties while guaranteeing unconditional security.","Implementations of high-dimensional QKD using dispersive-optics (DO-QKD) have been proposed to allow for multiple secure bits to be transmitted per photon while remaining cost-effective and scalable using existing telecommunication technology [1].","In the recent literature, there have been a number of experimental realizations of DO-QKD systems [2-6], with security analysis based on the treatment in Ref.","[1].","Here we demonstrate that in the case of finite dispersion, the model assumed for the eavesdropper's attack in Ref.","[1] is non-optimal for the eavesdropper, which leads to a significant overestimation of the secure key rate between parties.","We consider an alternative attack model that Alice and Bob find indistinguishable from the Ref.","[1] model, as long as they are restricted to making the measurements typical in DO-QKD.","We provide concrete examples where a significant gap exists between the Holevo information, and therefore the secret key rate, predicted by the two models.","We further analyze the experiment in Ref.","[2] as an example of a case where secure key is predicted according to the Ref.","[1] model, but where in fact there is zero secure key rate when considering the full set of collective attacks that an eavesdropper may perform."],"url":"http://arxiv.org/abs/2403.08992v1","category":"quant-ph"}
{"created":"2024-03-13 22:19:06","title":"Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its Experimental Evaluation","abstract":"Safe road-crossing by self-driving vehicles is a crucial problem to address in smart-cities. In this paper, we introduce a multi-sensor fusion approach to support road-crossing decisions in a system composed by an autonomous wheelchair and a flying drone featuring a robust sensory system made of diverse and redundant components. To that aim, we designed an analytical danger function based on explainable physical conditions evaluated by single sensors, including those using machine learning and artificial vision. As a proof-of-concept, we provide an experimental evaluation in a laboratory environment, showing the advantages of using multiple sensors, which can improve decision accuracy and effectively support safety assessment. We made the dataset available to the scientific community for further experimentation. The work has been developed in the context of an European project named REXASI-PRO, which aims to develop trustworthy artificial intelligence for social navigation of people with reduced mobility.","sentences":["Safe road-crossing by self-driving vehicles is a crucial problem to address in smart-cities.","In this paper, we introduce a multi-sensor fusion approach to support road-crossing decisions in a system composed by an autonomous wheelchair and a flying drone featuring a robust sensory system made of diverse and redundant components.","To that aim, we designed an analytical danger function based on explainable physical conditions evaluated by single sensors, including those using machine learning and artificial vision.","As a proof-of-concept, we provide an experimental evaluation in a laboratory environment, showing the advantages of using multiple sensors, which can improve decision accuracy and effectively support safety assessment.","We made the dataset available to the scientific community for further experimentation.","The work has been developed in the context of an European project named REXASI-PRO, which aims to develop trustworthy artificial intelligence for social navigation of people with reduced mobility."],"url":"http://arxiv.org/abs/2403.08984v1","category":"cs.RO"}
{"created":"2024-03-13 21:43:24","title":"Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields","abstract":"Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reconstruction with arbitrary resolution yet compact storage, and versatility across anatomical sites and tree complexities.","sentences":["Anatomical trees play a central role in clinical diagnosis and treatment planning.","However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry.","Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency.","Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently.","We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs.","We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution.","Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reconstruction with arbitrary resolution yet compact storage, and versatility across anatomical sites and tree complexities."],"url":"http://arxiv.org/abs/2403.08974v1","category":"cs.CV"}
{"created":"2024-03-13 21:34:36","title":"Measurements and modeling of induced flow in collective vertical migration","abstract":"Hydrodynamic interactions among swimming or flying organisms can lead to complex flows on the scale of the group. These emergent fluid dynamics are often more complex than a linear superposition of individual organism flows, especially at intermediate Reynolds numbers. This paper presents an approach to estimate the flow induced by multiple swimmer wakes in proximity using an analytical model that conserves mass and momentum in the aggregation. This analytical model was informed by and validated with empirical measurements of induced vertical migrations of brine shrimp, $\\textit{Artemia salina}$. The response of individual swimmers to ambient background flow and light intensity was evaluated. In addition, the time-resolved three-dimensional spatial configuration of the swimmers was measured using a recently developed laser scanning system. Computational experiments using the analytical model found that the induced flow at the front of the aggregation was insensitive to the presence of downstream swimmers, with the induced flow reaching an asymptote beyond a threshold aggregation length. Closer swimmer spacing led to higher induced flow, in some cases leading to model predictions of induced flow exceeding swimmer speeds required to maintain a stable spatial configuration. This result was reconciled by comparing two different models for the near-wake of each swimmer. Our results demonstrate that aggregation-scale flows result from a complex, yet predictable interplay amongst organism-scale wake structure, swimmer spacing and configuration, and aggregation size.","sentences":["Hydrodynamic interactions among swimming or flying organisms can lead to complex flows on the scale of the group.","These emergent fluid dynamics are often more complex than a linear superposition of individual organism flows, especially at intermediate Reynolds numbers.","This paper presents an approach to estimate the flow induced by multiple swimmer wakes in proximity using an analytical model that conserves mass and momentum in the aggregation.","This analytical model was informed by and validated with empirical measurements of induced vertical migrations of brine shrimp, $\\textit{Artemia salina}$.","The response of individual swimmers to ambient background flow and light intensity was evaluated.","In addition, the time-resolved three-dimensional spatial configuration of the swimmers was measured using a recently developed laser scanning system.","Computational experiments using the analytical model found that the induced flow at the front of the aggregation was insensitive to the presence of downstream swimmers, with the induced flow reaching an asymptote beyond a threshold aggregation length.","Closer swimmer spacing led to higher induced flow, in some cases leading to model predictions of induced flow exceeding swimmer speeds required to maintain a stable spatial configuration.","This result was reconciled by comparing two different models for the near-wake of each swimmer.","Our results demonstrate that aggregation-scale flows result from a complex, yet predictable interplay amongst organism-scale wake structure, swimmer spacing and configuration, and aggregation size."],"url":"http://arxiv.org/abs/2403.08973v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 21:30:01","title":"The Full-scale Assembly Simulation Testbed (FAST) Dataset","abstract":"In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.","sentences":["In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains.","One constraint for this research area is the dearth of open datasets.","In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST).","This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR.","In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers."],"url":"http://arxiv.org/abs/2403.08969v1","category":"cs.HC"}
{"created":"2024-03-13 21:19:12","title":"PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning","abstract":"In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions. However, aligning WSIs with diagnostic captions presents a significant challenge. This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model. To overcome these obstacles, we present PathM3, a multimodal, multi-task, multiple instance learning (MIL) framework for WSI classification and captioning. PathM3 adapts a query-based transformer to effectively align WSIs with diagnostic captions. Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers the correlations among instances. Furthermore, our PathM3 overcomes data scarcity in WSI-level captions by leveraging limited WSI diagnostic caption data in the manner of multi-task joint learning. Extensive experiments with improved classification accuracy and caption generation demonstrate the effectiveness of our method on both WSI classification and captioning task.","sentences":["In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions.","However, aligning WSIs with diagnostic captions presents a significant challenge.","This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model.","To overcome these obstacles, we present PathM3, a multimodal, multi-task, multiple instance learning (MIL) framework for WSI classification and captioning.","PathM3 adapts a query-based transformer to effectively align WSIs with diagnostic captions.","Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers the correlations among instances.","Furthermore, our PathM3 overcomes data scarcity in WSI-level captions by leveraging limited WSI diagnostic caption data in the manner of multi-task joint learning.","Extensive experiments with improved classification accuracy and caption generation demonstrate the effectiveness of our method on both WSI classification and captioning task."],"url":"http://arxiv.org/abs/2403.08967v1","category":"cs.CV"}
{"created":"2024-03-13 21:05:34","title":"Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring","abstract":"The aim of this paper is to evaluate the use of D-CNN (Deep Convolutional Neural Networks) algorithms to classify pig body conditions in normal or not normal conditions, with a focus on characteristics that are observed in sanitary monitoring, and were used six different algorithms to do this task. The study focused on five pig characteristics, being these caudophagy, ear hematoma, scratches on the body, redness, and natural stains (brown or black). The results of the study showed that D-CNN was effective in classifying deviations in pig body morphologies related to skin characteristics. The evaluation was conducted by analyzing the performance metrics Precision, Recall, and F-score, as well as the statistical analyses ANOVA and the Scott-Knott test. The contribution of this article is characterized by the proposal of using D-CNN networks for morphological classification in pigs, with a focus on characteristics identified in sanitary monitoring. Among the best results, the average Precision metric of 80.6\\% to classify caudophagy was achieved for the InceptionResNetV2 network, indicating the potential use of this technology for the proposed task. Additionally, a new image database was created, containing various pig's distinct body characteristics, which can serve as data for future research.","sentences":["The aim of this paper is to evaluate the use of D-CNN (Deep Convolutional Neural Networks) algorithms to classify pig body conditions in normal or not normal conditions, with a focus on characteristics that are observed in sanitary monitoring, and were used six different algorithms to do this task.","The study focused on five pig characteristics, being these caudophagy, ear hematoma, scratches on the body, redness, and natural stains (brown or black).","The results of the study showed that D-CNN was effective in classifying deviations in pig body morphologies related to skin characteristics.","The evaluation was conducted by analyzing the performance metrics Precision, Recall, and F-score, as well as the statistical analyses ANOVA and the Scott-Knott test.","The contribution of this article is characterized by the proposal of using D-CNN networks for morphological classification in pigs, with a focus on characteristics identified in sanitary monitoring.","Among the best results, the average Precision metric of 80.6\\% to classify caudophagy was achieved for the InceptionResNetV2 network, indicating the potential use of this technology for the proposed task.","Additionally, a new image database was created, containing various pig's distinct body characteristics, which can serve as data for future research."],"url":"http://arxiv.org/abs/2403.08962v1","category":"cs.CV"}
{"created":"2024-03-13 20:51:21","title":"AI coach for badminton","abstract":"In the competitive realm of sports, optimal performance necessitates rigorous management of nutrition and physical conditioning. Specifically, in badminton, the agility and precision required make it an ideal candidate for motion analysis through video analytics. This study leverages advanced neural network methodologies to dissect video footage of badminton matches, aiming to extract detailed insights into player kinetics and biomechanics. Through the analysis of stroke mechanics, including hand-hip coordination, leg positioning, and the execution angles of strokes, the research aims to derive predictive models that can suggest improvements in stance, technique, and muscle orientation. These recommendations are designed to mitigate erroneous techniques, reduce the risk of joint fatigue, and enhance overall performance. Utilizing a vast array of data available online, this research correlates players' physical attributes with their in-game movements to identify muscle activation patterns during play. The goal is to offer personalized training and nutrition strategies that align with the specific biomechanical demands of badminton, thereby facilitating targeted performance enhancements.","sentences":["In the competitive realm of sports, optimal performance necessitates rigorous management of nutrition and physical conditioning.","Specifically, in badminton, the agility and precision required make it an ideal candidate for motion analysis through video analytics.","This study leverages advanced neural network methodologies to dissect video footage of badminton matches, aiming to extract detailed insights into player kinetics and biomechanics.","Through the analysis of stroke mechanics, including hand-hip coordination, leg positioning, and the execution angles of strokes, the research aims to derive predictive models that can suggest improvements in stance, technique, and muscle orientation.","These recommendations are designed to mitigate erroneous techniques, reduce the risk of joint fatigue, and enhance overall performance.","Utilizing a vast array of data available online, this research correlates players' physical attributes with their in-game movements to identify muscle activation patterns during play.","The goal is to offer personalized training and nutrition strategies that align with the specific biomechanical demands of badminton, thereby facilitating targeted performance enhancements."],"url":"http://arxiv.org/abs/2403.08956v1","category":"cs.HC"}
{"created":"2024-03-13 20:50:49","title":"Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis","abstract":"Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\\mathcal{O}(\\epsilon^{-2})$ to reach an $\\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts. Our theoretical analysis demonstrates that risk-sensitive REINFORCE can have a reduced number of iterations required for convergence. This leads to improved iteration complexity, as employing the exponential utility does not entail additional computation per iteration. We characterize the conditions under which risk-sensitive algorithms can achieve better iteration complexity. Our simulation results also validate that risk-averse cases can converge and stabilize more quickly after approximately half of the episodes compared to their risk-neutral counterparts.","sentences":["Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments.","However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness.","Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored.","In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function.","We obtain an iteration complexity of $\\mathcal{O}(\\epsilon^{-2})$ to reach an $\\epsilon$-approximate first-order stationary point (FOSP).","We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts.","Our theoretical analysis demonstrates that risk-sensitive REINFORCE can have a reduced number of iterations required for convergence.","This leads to improved iteration complexity, as employing the exponential utility does not entail additional computation per iteration.","We characterize the conditions under which risk-sensitive algorithms can achieve better iteration complexity.","Our simulation results also validate that risk-averse cases can converge and stabilize more quickly after approximately half of the episodes compared to their risk-neutral counterparts."],"url":"http://arxiv.org/abs/2403.08955v1","category":"cs.LG"}
{"created":"2024-03-13 20:38:48","title":"Characterisation of analogue Monolithic Active Pixel Sensor test structures implemented in a 65 nm CMOS imaging process","abstract":"Analogue test structures were fabricated using the Tower Partners Semiconductor Co. CMOS 65 nm ISC process. The purpose was to characterise and qualify this process and to optimise the sensor for the next generation of Monolithic Active Pixels Sensors for high-energy physics. The technology was explored in several variants which differed by: doping levels, pixel geometries and pixel pitches (10-25 $\\mu$m). These variants have been tested following exposure to varying levels of irradiation up to 3 MGy and $10^{16}$ 1 MeV n$_\\text{eq}$ cm$^{-2}$. Here the results from prototypes that feature direct analogue output of a 4$\\times$4 pixel matrix are reported, allowing the systematic and detailed study of charge collection properties. Measurements were taken both using $^{55}$Fe X-ray sources and in beam tests using minimum ionizing particles. The results not only demonstrate the feasibility of using this technology for particle detection but also serve as a reference for future applications and optimisations.","sentences":["Analogue test structures were fabricated using the Tower Partners Semiconductor Co.","CMOS 65 nm ISC process.","The purpose was to characterise and qualify this process and to optimise the sensor for the next generation of Monolithic Active Pixels Sensors for high-energy physics.","The technology was explored in several variants which differed by: doping levels, pixel geometries and pixel pitches (10-25 $\\mu$m).","These variants have been tested following exposure to varying levels of irradiation up to 3 MGy and $10^{16}$ 1 MeV n$_\\text{eq}$","cm$^{-2}$. Here the results from prototypes that feature direct analogue output of a 4$\\times$4 pixel matrix are reported, allowing the systematic and detailed study of charge collection properties.","Measurements were taken both using $^{55}$Fe X-ray sources and in beam tests using minimum ionizing particles.","The results not only demonstrate the feasibility of using this technology for particle detection but also serve as a reference for future applications and optimisations."],"url":"http://arxiv.org/abs/2403.08952v1","category":"physics.ins-det"}
{"created":"2024-03-13 20:32:32","title":"Exploring Prompt Engineering Practices in the Enterprise","abstract":"Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made. We discuss design implications and future directions based on these prompt engineering practices.","sentences":["Interaction with Large Language Models (LLMs) is primarily carried out via prompting.","A prompt is a natural language instruction designed to elicit certain behaviour or output from a model.","In theory, natural language prompts enable non-experts to interact with and leverage LLMs.","However, for complex tasks and tasks with specific requirements, prompt design is not trivial.","Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal.","We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering.","To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made.","We discuss design implications and future directions based on these prompt engineering practices."],"url":"http://arxiv.org/abs/2403.08950v1","category":"cs.HC"}
{"created":"2024-03-13 20:27:35","title":"Model-free Resilient Controller Design based on Incentive Feedback Stackelberg Game and Q-learning","abstract":"In the swift evolution of Cyber-Physical Systems (CPSs) within intelligent environments, especially in the industrial domain shaped by Industry 4.0, the surge in development brings forth unprecedented security challenges. This paper explores the intricate security issues of Industrial CPSs (ICPSs), with a specific focus on the unique threats presented by intelligent attackers capable of directly compromising the controller, thereby posing a direct risk to physical security. Within the framework of hierarchical control and incentive feedback Stackelberg game, we design a resilient leading controller (leader) that is adaptive to a compromised following controller (follower) such that the compromised follower acts cooperatively with the leader, aligning its strategies with the leader's objective to achieve a team-optimal solution. First, we provide sufficient conditions for the existence of an incentive Stackelberg solution when system dynamics are known. Then, we propose a Q-learning-based Approximate Dynamic Programming (ADP) approach, and corresponding algorithms for the online resolution of the incentive Stackelberg solution without requiring prior knowledge of system dynamics. Last but not least, we prove the convergence of our approach to the optimum.","sentences":["In the swift evolution of Cyber-Physical Systems (CPSs) within intelligent environments, especially in the industrial domain shaped by Industry 4.0, the surge in development brings forth unprecedented security challenges.","This paper explores the intricate security issues of Industrial CPSs (ICPSs), with a specific focus on the unique threats presented by intelligent attackers capable of directly compromising the controller, thereby posing a direct risk to physical security.","Within the framework of hierarchical control and incentive feedback Stackelberg game, we design a resilient leading controller (leader) that is adaptive to a compromised following controller (follower) such that the compromised follower acts cooperatively with the leader, aligning its strategies with the leader's objective to achieve a team-optimal solution.","First, we provide sufficient conditions for the existence of an incentive Stackelberg solution when system dynamics are known.","Then, we propose a Q-learning-based Approximate Dynamic Programming (ADP) approach, and corresponding algorithms for the online resolution of the incentive Stackelberg solution without requiring prior knowledge of system dynamics.","Last but not least, we prove the convergence of our approach to the optimum."],"url":"http://arxiv.org/abs/2403.08948v1","category":"eess.SY"}
{"created":"2024-03-13 20:21:20","title":"Language-based game theory in the age of artificial intelligence","abstract":"Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology, and artificial intelligence. Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function. However, the exact factors influencing strategy choices remain elusive. While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions. This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions. We propose sentiment analysis as a fundamental tool for this shift and take an initial step by analyzing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions. Our meta-analysis shows that sentiment analysis can explain human behaviour beyond economic outcomes. We discuss future research directions. We hope this work sets the stage for a novel game theoretical approach that emphasizes the importance of language in human decisions.","sentences":["Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology, and artificial intelligence.","Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function.","However, the exact factors influencing strategy choices remain elusive.","While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions.","This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions.","We propose sentiment analysis as a fundamental tool for this shift and take an initial step by analyzing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions.","Our meta-analysis shows that sentiment analysis can explain human behaviour beyond economic outcomes.","We discuss future research directions.","We hope this work sets the stage for a novel game theoretical approach that emphasizes the importance of language in human decisions."],"url":"http://arxiv.org/abs/2403.08944v1","category":"cs.GT"}
{"created":"2024-03-13 20:16:16","title":"A Virtual Environment for Collaborative Inspection in Additive Manufacturing","abstract":"Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal geometry enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen's heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.","sentences":["Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries.","A defining feature for many AM parts is the complex internal geometry enabled by the printing process.","However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces.","Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team.","In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts.","Geographically separated experts can virtually inspect and jointly discuss data.","It also supports VR and non-VR users, who can be spectators in the VR environment.","Various features for data exploration and inspection are developed and enhanced via real-time synchronization.","We followed usability and interface verification guidelines using Nielsen's heuristics approach.","Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback.","Results reveal potential benefits, applicability, and current limitations.","The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings."],"url":"http://arxiv.org/abs/2403.08940v1","category":"cs.HC"}
{"created":"2024-03-13 20:12:01","title":"Bugs in Large Language Models Generated Code","abstract":"Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.","sentences":["Large Language Models (LLMs) for code have gained significant attention recently.","They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation.","Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community.","Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs.","This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration.","The bug patterns are presented in the form of a taxonomy.","The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers.","The surveyed participants generally asserted the significance and prevalence of the bug patterns.","Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code.","This study sheds light on the distinctive characteristics of LLM-generated code."],"url":"http://arxiv.org/abs/2403.08937v1","category":"cs.SE"}
{"created":"2024-03-13 20:11:20","title":"Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning","abstract":"Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to cooperate, namely personalized expert-guided MARL (PegMARL). This algorithm utilizes two discriminators: the first provides incentives based on the alignment of policy behavior with demonstrations, and the second regulates incentives based on whether the behavior leads to the desired objective. We evaluate PegMARL using personalized demonstrations in both discrete and continuous environments. The results demonstrate that PegMARL learns near-optimal policies even when provided with suboptimal demonstrations, and outperforms state-of-the-art MARL algorithms in solving coordinated tasks. We also showcase PegMARL's capability to leverage joint demonstrations in the StarCraft scenario and converge effectively even with demonstrations from non-co-trained policies.","sentences":["Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space.","While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations.","In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team.","These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts.","To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to cooperate, namely personalized expert-guided MARL (PegMARL).","This algorithm utilizes two discriminators: the first provides incentives based on the alignment of policy behavior with demonstrations, and the second regulates incentives based on whether the behavior leads to the desired objective.","We evaluate PegMARL using personalized demonstrations in both discrete and continuous environments.","The results demonstrate that PegMARL learns near-optimal policies even when provided with suboptimal demonstrations, and outperforms state-of-the-art MARL algorithms in solving coordinated tasks.","We also showcase PegMARL's capability to leverage joint demonstrations in the StarCraft scenario and converge effectively even with demonstrations from non-co-trained policies."],"url":"http://arxiv.org/abs/2403.08936v1","category":"cs.MA"}
{"created":"2024-03-13 19:56:30","title":"Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images","abstract":"Creating high-quality and realistic images is now possible thanks to the impressive advancements in image generation. A description in natural language of your desired output is all you need to obtain breathtaking results. However, as the use of generative models grows, so do concerns about the propagation of malicious content and misinformation. Consequently, the research community is actively working on the development of novel fake detection techniques, primarily focusing on low-level features and possible fingerprints left by generative models during the image generation process. In a different vein, in our work, we leverage human semantic knowledge to investigate the possibility of being included in frameworks of fake image detection. To achieve this, we collect a novel dataset of partially manipulated images using diffusion models and conduct an eye-tracking experiment to record the eye movements of different observers while viewing real and fake stimuli. A preliminary statistical analysis is conducted to explore the distinctive patterns in how humans perceive genuine and altered images. Statistical findings reveal that, when perceiving counterfeit samples, humans tend to focus on more confined regions of the image, in contrast to the more dispersed observational pattern observed when viewing genuine images. Our dataset is publicly available at: https://github.com/aimagelab/unveiling-the-truth.","sentences":["Creating high-quality and realistic images is now possible thanks to the impressive advancements in image generation.","A description in natural language of your desired output is all you need to obtain breathtaking results.","However, as the use of generative models grows, so do concerns about the propagation of malicious content and misinformation.","Consequently, the research community is actively working on the development of novel fake detection techniques, primarily focusing on low-level features and possible fingerprints left by generative models during the image generation process.","In a different vein, in our work, we leverage human semantic knowledge to investigate the possibility of being included in frameworks of fake image detection.","To achieve this, we collect a novel dataset of partially manipulated images using diffusion models and conduct an eye-tracking experiment to record the eye movements of different observers while viewing real and fake stimuli.","A preliminary statistical analysis is conducted to explore the distinctive patterns in how humans perceive genuine and altered images.","Statistical findings reveal that, when perceiving counterfeit samples, humans tend to focus on more confined regions of the image, in contrast to the more dispersed observational pattern observed when viewing genuine images.","Our dataset is publicly available at: https://github.com/aimagelab/unveiling-the-truth."],"url":"http://arxiv.org/abs/2403.08933v1","category":"cs.CV"}
{"created":"2024-03-13 19:43:26","title":"How Much Can Reconfigurable Intelligent Surfaces Augment Sky Visibility: A Stochastic Geometry Approach","abstract":"This paper uses the theory of point processes and stochastic geometry to quantify the sky visibility experienced by users located in an urban environment. The general idea is to represent the buildings of this environment as a stationary marked point process, where the points represent the building locations and the marks their heights. The point process framework is first used to characterize the distribution of the blockage angle, which limits the visibility of a typical user into the sky due to the obstruction by buildings. In the context of communications, this distribution is useful when users try to connect to the nodes of an aerial or non-terrestrial network in a Line-of-Sight way. Within this context, the point process framework can also be used to investigate the gain of connectivity obtained thanks to Reconfigurable Intelligent Surfaces. Assuming that such surfaces are installed on the top of buildings to extend the user's sky visibility, this point process approach allows one to quantify the gain in visibility and hence the gain in connectivity obtained by the typical user. The distributional properties of visibility-related metrics are cross-validated by comparison to simulation results.","sentences":["This paper uses the theory of point processes and stochastic geometry to quantify the sky visibility experienced by users located in an urban environment.","The general idea is to represent the buildings of this environment as a stationary marked point process, where the points represent the building locations and the marks their heights.","The point process framework is first used to characterize the distribution of the blockage angle, which limits the visibility of a typical user into the sky due to the obstruction by buildings.","In the context of communications, this distribution is useful when users try to connect to the nodes of an aerial or non-terrestrial network in a Line-of-Sight way.","Within this context, the point process framework can also be used to investigate the gain of connectivity obtained thanks to Reconfigurable Intelligent Surfaces.","Assuming that such surfaces are installed on the top of buildings to extend the user's sky visibility, this point process approach allows one to quantify the gain in visibility and hence the gain in connectivity obtained by the typical user.","The distributional properties of visibility-related metrics are cross-validated by comparison to simulation results."],"url":"http://arxiv.org/abs/2403.08930v1","category":"math.PR"}
{"created":"2024-03-13 19:36:03","title":"Neuromorphic force-control in an industrial task: validating energy and latency benefits","abstract":"As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements. Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture. Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks. Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion. We trained a spiking neural network (SNN) to perform force-torque feedback control using a reinforcement learning approach in simulation. We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At inference time we show latency competitive with current CPU/GPU architectures, two orders of magnitude less energy usage in comparison to traditional low-energy edge-hardware. We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots.","sentences":["As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements.","Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture.","Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks.","Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion.","We trained a spiking neural network (SNN) to perform force-torque feedback control using a reinforcement learning approach in simulation.","We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm.","At inference time we show latency competitive with current CPU/GPU architectures, two orders of magnitude less energy usage in comparison to traditional low-energy edge-hardware.","We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots."],"url":"http://arxiv.org/abs/2403.08928v1","category":"cs.RO"}
{"created":"2024-03-13 19:32:17","title":"Electrochemical Communication in Bacterial Biofilms: A Study on Potassium Stimulation and Signal Transmission","abstract":"Electrochemical communication is a mechanism that enables intercellular interaction among bacteria within communities. Bacteria achieves synchronization and coordinates collective actions at the population level through the utilization of electrochemical signals. In this work, we investigate the response of bacterial biofilms to artificial potassium concentration stimulation. We introduce signal inputs at a specific location within the biofilm and observe their transmission to other regions, facilitated by intermediary cells that amplify and relay the signal. We analyze the output signals when biofilm regions are subjected to different input signal types and explore their impact on biofilm growth. Furthermore, we investigate how the temporal gap between input pulses influences output signal characteristics, demonstrating that an appropriate gap yields distinct and well-defined output signals. Our research sheds light on the potential of bacterial biofilms as communication nodes in electrochemical communication networks.","sentences":["Electrochemical communication is a mechanism that enables intercellular interaction among bacteria within communities.","Bacteria achieves synchronization and coordinates collective actions at the population level through the utilization of electrochemical signals.","In this work, we investigate the response of bacterial biofilms to artificial potassium concentration stimulation.","We introduce signal inputs at a specific location within the biofilm and observe their transmission to other regions, facilitated by intermediary cells that amplify and relay the signal.","We analyze the output signals when biofilm regions are subjected to different input signal types and explore their impact on biofilm growth.","Furthermore, we investigate how the temporal gap between input pulses influences output signal characteristics, demonstrating that an appropriate gap yields distinct and well-defined output signals.","Our research sheds light on the potential of bacterial biofilms as communication nodes in electrochemical communication networks."],"url":"http://arxiv.org/abs/2403.08926v1","category":"cs.IT"}
{"created":"2024-03-13 19:11:58","title":"Cross-Modal Learning of Housing Quality in Amsterdam","abstract":"In our research we test data and models for the recognition of housing quality in the city of Amsterdam from ground-level and aerial imagery. For ground-level images we compare Google StreetView (GSV) to Flickr images. Our results show that GSV predicts the most accurate building quality scores, approximately 30% better than using only aerial images. However, we find that through careful filtering and by using the right pre-trained model, Flickr image features combined with aerial image features are able to halve the performance gap to GSV features from 30% to 15%. Our results indicate that there are viable alternatives to GSV for liveability factor prediction, which is encouraging as GSV images are more difficult to acquire and not always available.","sentences":["In our research we test data and models for the recognition of housing quality in the city of Amsterdam from ground-level and aerial imagery.","For ground-level images we compare Google StreetView (GSV) to Flickr images.","Our results show that GSV predicts the most accurate building quality scores, approximately 30% better than using only aerial images.","However, we find that through careful filtering and by using the right pre-trained model, Flickr image features combined with aerial image features are able to halve the performance gap to GSV features from 30% to 15%.","Our results indicate that there are viable alternatives to GSV for liveability factor prediction, which is encouraging as GSV images are more difficult to acquire and not always available."],"url":"http://arxiv.org/abs/2403.08915v1","category":"cs.CV"}
{"created":"2024-03-13 19:00:36","title":"Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning","abstract":"There is a growing interest in the application of Reinforcement Learning (RL) techniques to AI planning with the aim to come up with general policies. Typically, the mapping of the transition model of AI planning to the state transition system of a Markov Decision Process is established by assuming a one-to-one correspondence of the respective action spaces. In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning. Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models. The main objective of this article is thus to pave the way towards a redefinition of the RL action space in a manner that is more closely aligned with the planning perspective.","sentences":["There is a growing interest in the application of Reinforcement Learning (RL) techniques to AI planning with the aim to come up with general policies.","Typically, the mapping of the transition model of AI planning to the state transition system of a Markov Decision Process is established by assuming a one-to-one correspondence of the respective action spaces.","In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning.","Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models.","The main objective of this article is thus to pave the way towards a redefinition of the RL action space in a manner that is more closely aligned with the planning perspective."],"url":"http://arxiv.org/abs/2403.08910v1","category":"cs.AI"}
{"created":"2024-03-13 18:54:27","title":"Strategizing against Q-learners: A Control-theoretical Approach","abstract":"In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games. We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm. To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically.","sentences":["In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games.","We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm.","To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system.","We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically."],"url":"http://arxiv.org/abs/2403.08906v1","category":"cs.GT"}
{"created":"2024-03-13 18:20:40","title":"Moving Towards Automated Interstellar Boundary Explorer Data Selection with LOTUS","abstract":"The Interstellar Boundary Explorer (IBEX) satellite collects data on energetic neutral atoms (ENAs) that provide insight into the heliosphere, the region surrounding our solar system and separating it from interstellar space. IBEX collects information on these particles and on extraneous ``background'' particles. While IBEX records how and when the different particles are observed, it does not distinguish between heliospheric ENA particles and incidental background particles. To address this issue, all IBEX data has historically been manually labeled as ``good'' ENA data, or ``bad'' background data. This manual culling process is incredibly time-intensive and contingent on subjective, manually-induced decision thresholds. In this paper, we develop a three-stage automated culling process, called LOTUS, that uses random forests to expedite and standardize the labelling process. In Stage 1, LOTUS uses random forests to obtain probabilities of observing true ENA particles on a per-observation basis. In Stage 2, LOTUS aggregates these probabilities to obtain predictions within small windows of time. In Stage 3, LOTUS refines these predictions. We compare the labels generated by LOTUS to those manually generated by the subject matter expert. We use various metrics to demonstrate that LOTUS is a useful automated process for supplementing and standardizing the manual culling process.","sentences":["The Interstellar Boundary Explorer (IBEX) satellite collects data on energetic neutral atoms (ENAs) that provide insight into the heliosphere, the region surrounding our solar system and separating it from interstellar space.","IBEX collects information on these particles and on extraneous ``background'' particles.","While IBEX records how and when the different particles are observed, it does not distinguish between heliospheric ENA particles and incidental background particles.","To address this issue, all IBEX data has historically been manually labeled as ``good'' ENA data, or ``bad'' background data.","This manual culling process is incredibly time-intensive and contingent on subjective, manually-induced decision thresholds.","In this paper, we develop a three-stage automated culling process, called LOTUS, that uses random forests to expedite and standardize the labelling process.","In Stage 1, LOTUS uses random forests to obtain probabilities of observing true ENA particles on a per-observation basis.","In Stage 2, LOTUS aggregates these probabilities to obtain predictions within small windows of time.","In Stage 3, LOTUS refines these predictions.","We compare the labels generated by LOTUS to those manually generated by the subject matter expert.","We use various metrics to demonstrate that LOTUS is a useful automated process for supplementing and standardizing the manual culling process."],"url":"http://arxiv.org/abs/2403.08891v1","category":"stat.AP"}
{"created":"2024-03-13 18:16:54","title":"Federated Data Model","abstract":"In artificial intelligence (AI), especially deep learning, data diversity and volume play a pivotal role in model development. However, training a robust deep learning model often faces challenges due to data privacy, regulations, and the difficulty of sharing data between different locations, especially for medical applications. To address this, we developed a method called the Federated Data Model (FDM). This method uses diffusion models to learn the characteristics of data at one site and then creates synthetic data that can be used at another site without sharing the actual data. We tested this approach with a medical image segmentation task, focusing on cardiac magnetic resonance images from different hospitals. Our results show that models trained with this method perform well both on the data they were originally trained on and on data from other sites. This approach offers a promising way to train accurate and privacy-respecting AI models across different locations.","sentences":["In artificial intelligence (AI), especially deep learning, data diversity and volume play a pivotal role in model development.","However, training a robust deep learning model often faces challenges due to data privacy, regulations, and the difficulty of sharing data between different locations, especially for medical applications.","To address this, we developed a method called the Federated Data Model (FDM).","This method uses diffusion models to learn the characteristics of data at one site and then creates synthetic data that can be used at another site without sharing the actual data.","We tested this approach with a medical image segmentation task, focusing on cardiac magnetic resonance images from different hospitals.","Our results show that models trained with this method perform well both on the data they were originally trained on and on data from other sites.","This approach offers a promising way to train accurate and privacy-respecting AI models across different locations."],"url":"http://arxiv.org/abs/2403.08887v1","category":"cs.CV"}
{"created":"2024-03-13 18:12:53","title":"SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net","abstract":"We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics and shows great temporal consistency.","sentences":["We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data.","It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements.","The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything.","To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP).","This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior.","Volumetric semantics is computed by a 3D U-Net.","We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency.","We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches.","The SLCF-Net excels in all SSC metrics and shows great temporal consistency."],"url":"http://arxiv.org/abs/2403.08885v1","category":"cs.CV"}
{"created":"2024-03-13 18:11:17","title":"Cultural evolution in populations of Large Language Models","abstract":"Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake. Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution. We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed. The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence.","sentences":["Research in cultural evolution aims at providing causal explanations for the change of culture over time.","Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods.","While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models.","This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms.","We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap.","On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake.","Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution.","We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed.","The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence."],"url":"http://arxiv.org/abs/2403.08882v1","category":"cs.MA"}
{"created":"2024-03-13 18:06:43","title":"REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values","abstract":"Feature selection is a crucial step in building machine learning models. This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets. Several additional model performance characteristics such as fairness and robustness are of importance for model development. As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence. When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch. In this paper, we introduce the problem of feature \\emph{reselection}, so that features can be selected with respect to secondary model performance characteristics efficiently even after a feature selection process has been done with respect to a primary objective. To address this problem, we propose REFRESH, a method to reselect features so that additional constraints that are desirable towards model performance can be achieved without having to train several new models. REFRESH's underlying algorithm is a novel technique using SHAP values and correlation analysis that can approximate for the predictions of a model without having to train these models. Empirical evaluations on three datasets, including a large-scale loan defaulting dataset show that REFRESH can help find alternate models with better model characteristics efficiently. We also discuss the need for reselection and REFRESH based on regulation desiderata.","sentences":["Feature selection is a crucial step in building machine learning models.","This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets.","Several additional model performance characteristics such as fairness and robustness are of importance for model development.","As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence.","When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch.","In this paper, we introduce the problem of feature \\emph{reselection}, so that features can be selected with respect to secondary model performance characteristics efficiently even after a feature selection process has been done with respect to a primary objective.","To address this problem, we propose REFRESH, a method to reselect features so that additional constraints that are desirable towards model performance can be achieved without having to train several new models.","REFRESH's underlying algorithm is a novel technique using SHAP values and correlation analysis that can approximate for the predictions of a model without having to train these models.","Empirical evaluations on three datasets, including a large-scale loan defaulting dataset show that REFRESH can help find alternate models with better model characteristics efficiently.","We also discuss the need for reselection and REFRESH based on regulation desiderata."],"url":"http://arxiv.org/abs/2403.08880v1","category":"cs.LG"}
{"created":"2024-03-13 18:05:16","title":"Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning","abstract":"The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives. Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives. In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable. We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward. We test our algorithm in an ITS environment with edge cloud computing. Empirical results show that the algorithm is quick to adapt to new environments and performs better in all individual and system metrics compared to the state-of-the-art benchmark. Our algorithm also addresses various practical concerns with its modularized and asynchronous online training method. In addition to the cloud simulation, we test our algorithm on a single-board computer and show that it can make inference in 6 milliseconds.","sentences":["The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives.","Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives.","In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable.","We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward.","We test our algorithm in an ITS environment with edge cloud computing.","Empirical results show that the algorithm is quick to adapt to new environments and performs better in all individual and system metrics compared to the state-of-the-art benchmark.","Our algorithm also addresses various practical concerns with its modularized and asynchronous online training method.","In addition to the cloud simulation, we test our algorithm on a single-board computer and show that it can make inference in 6 milliseconds."],"url":"http://arxiv.org/abs/2403.08879v1","category":"cs.LG"}
{"created":"2024-03-13 18:00:03","title":"HST astrometry of the closest Brown Dwarfs -- II. Improved parameters and constraints on a third body","abstract":"Located at less than 2pc away, Luhman16AB (WISE.J104915.57-531906.1) is the closest pair of brown dwarfs and third closest `stellar' system to Earth. An exoplanet candidate in the Luhman16 binary system was reported in 2017 based on a weak astrometric signature in the analysis of 12 HST epochs. An additional epoch collected in 2018 and re-analysis of the data with more advanced methods further increased the significance level of the candidate, consistent with a Neptune-mass exoplanet orbiting one of the Luhman16 brown dwarf components. We report the joint analysis of these previous data together with two new astrometric HST epochs we obtained to confirm or disprove this astrometric signature. Our new analysis rules out presence of a planet orbiting one component of the Luhman16AB system for masses M > 1.5 M_Nep (Neptune masses) and periods between 400 and 5000 days. However, the presence of third bodies with masses M < 3 M_Nep and periods between 2 and 400 days (~1.1yrs) can not be excluded. Our measurements make significant improvements to the characterization of this sub-stellar binary, including its mass-ratio 0.8305+/-0.0006, individual component masses 35.4+/-0.2 M_Jup and 29.4+/-0.2 M_Jup (Jupiter masses), and parallax distance 1.9960pc +/- 50AU. Comparison of the masses and luminosities of Luhman16AB to several evolutionary models shows persistent discrepancies in the ages of the two components, but strengthens the case that this system is a member of the 510+/-95 Myr Oceanus Moving Group.","sentences":["Located at less than 2pc away, Luhman16AB (WISE.J104915.57-531906.1) is the closest pair of brown dwarfs and third closest `stellar' system to Earth.","An exoplanet candidate in the Luhman16 binary system was reported in 2017 based on a weak astrometric signature in the analysis of 12 HST epochs.","An additional epoch collected in 2018 and re-analysis of the data with more advanced methods further increased the significance level of the candidate, consistent with a Neptune-mass exoplanet orbiting one of the Luhman16 brown dwarf components.","We report the joint analysis of these previous data together with two new astrometric HST epochs we obtained to confirm or disprove this astrometric signature.","Our new analysis rules out presence of a planet orbiting one component of the Luhman16AB system for masses M > 1.5 M_Nep (Neptune masses) and periods between 400 and 5000 days.","However, the presence of third bodies with masses M < 3 M_Nep and periods between 2 and 400 days (~1.1yrs) can not be excluded.","Our measurements make significant improvements to the characterization of this sub-stellar binary, including its mass-ratio 0.8305+/-0.0006, individual component masses 35.4+/-0.2 M_Jup and 29.4+/-0.2 M_Jup (Jupiter masses), and parallax distance 1.9960pc +/-","50AU.","Comparison of the masses and luminosities of Luhman16AB to several evolutionary models shows persistent discrepancies in the ages of the two components, but strengthens the case that this system is a member of the 510+/-95 Myr Oceanus Moving Group."],"url":"http://arxiv.org/abs/2403.08865v1","category":"astro-ph.EP"}
{"created":"2024-03-13 16:30:57","title":"Bifurcated Attention for Single-Context Large-Batch Sampling","abstract":"In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation without substantially increasing latency, enhancing performance when integrated with postprocessing techniques such as reranking.","sentences":["In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts.","This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths.","Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process.","This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO.","Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length.","The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation without substantially increasing latency, enhancing performance when integrated with postprocessing techniques such as reranking."],"url":"http://arxiv.org/abs/2403.08845v1","category":"cs.LG"}
{"created":"2024-03-13 15:54:49","title":"AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models","abstract":"AcademiaOS is a first attempt to automate grounded theory development in qualitative research with large language models. Using recent large language models' language understanding, generation, and reasoning capabilities, AcademiaOS codes curated qualitative raw data such as interview transcripts and develops themes and dimensions to further develop a grounded theoretical model, affording novel insights. A user study (n=19) suggests that the system finds acceptance in the academic community and exhibits the potential to augment humans in qualitative research. AcademiaOS has been made open-source for others to build upon and adapt to their use cases.","sentences":["AcademiaOS is a first attempt to automate grounded theory development in qualitative research with large language models.","Using recent large language models' language understanding, generation, and reasoning capabilities, AcademiaOS codes curated qualitative raw data such as interview transcripts and develops themes and dimensions to further develop a grounded theoretical model, affording novel insights.","A user study (n=19) suggests that the system finds acceptance in the academic community and exhibits the potential to augment humans in qualitative research.","AcademiaOS has been made open-source for others to build upon and adapt to their use cases."],"url":"http://arxiv.org/abs/2403.08844v1","category":"cs.HC"}
{"created":"2024-03-13 14:45:54","title":"Fuzzy Fault Trees Formalized","abstract":"Fault tree analysis is a vital method of assessing safety risks. It helps to identify potential causes of accidents, assess their likelihood and severity, and suggest preventive measures. Quantitative analysis of fault trees is often done via the dependability metrics that compute the system's failure behaviour over time. However, the lack of precise data is a major obstacle to quantitative analysis, and so to reliability analysis. Fuzzy logic is a popular framework for dealing with ambiguous values and has applications in many domains. A number of fuzzy approaches have been proposed to fault tree analysis, but -- to the best of our knowledge -- none of them provide rigorous definitions or algorithms for computing fuzzy unreliability values. In this paper, we define a rigorous framework for fuzzy unreliability values. In addition, we provide a bottom-up algorithm to efficiently calculate fuzzy reliability for a system. The algorithm incorporates the concept of $\\alpha$-cuts method. That is, performing binary algebraic operations on intervals on horizontally discretised $\\alpha$-cut representations of fuzzy numbers. The method preserves the nonlinearity of fuzzy unreliability. Finally, we illustrate the results obtained from two case studies.","sentences":["Fault tree analysis is a vital method of assessing safety risks.","It helps to identify potential causes of accidents, assess their likelihood and severity, and suggest preventive measures.","Quantitative analysis of fault trees is often done via the dependability metrics that compute the system's failure behaviour over time.","However, the lack of precise data is a major obstacle to quantitative analysis, and so to reliability analysis.","Fuzzy logic is a popular framework for dealing with ambiguous values and has applications in many domains.","A number of fuzzy approaches have been proposed to fault tree analysis, but -- to the best of our knowledge -- none of them provide rigorous definitions or algorithms for computing fuzzy unreliability values.","In this paper, we define a rigorous framework for fuzzy unreliability values.","In addition, we provide a bottom-up algorithm to efficiently calculate fuzzy reliability for a system.","The algorithm incorporates the concept of $\\alpha$-cuts method.","That is, performing binary algebraic operations on intervals on horizontally discretised $\\alpha$-cut representations of fuzzy numbers.","The method preserves the nonlinearity of fuzzy unreliability.","Finally, we illustrate the results obtained from two case studies."],"url":"http://arxiv.org/abs/2403.08843v1","category":"cs.AI"}
{"created":"2024-03-14 17:58:09","title":"From the Conformal Anomaly to the Virasoro Algebra","abstract":"The conformal anomaly and the Virasoro algebra are fundamental aspects of 2D conformal field theory and conformally covariant models in planar random geometry. In this article, we explicitly derive the Virasoro algebra from an axiomatization of the conformal anomaly in terms of real determinant lines, one-dimensional vector spaces associated to Riemann surfaces with analytically parametrized boundary components. Here, analytical orientation-preserving diffeomorphisms and deformations of the circle naturally act on the boundary components. We introduce a sewing operation on the real determinant lines over the semigroup of annuli, which then induces central extensions of the diffeomorphism group, as well as of the complex deformations.   Our main theorem shows that on the one hand, the Lie algebra cocycle associated to the central extension of diffeomorphisms is trivial, while on the other hand, the one associated to the central extension of complex deformations is nontrivial, yielding the imaginary part of the Gel'fand-Fuks cocycle. The proof uses concrete computations, which we hope to be accessible to a wide audience. We also show an explicit relation to loop Loewner energy, anticipating the real determinant lines to be pertinent to semi-classical limits of random curves, as well as to Kahler geometry and geometric quantization of moduli spaces of Riemann surfaces. The conformal anomaly and real determinant line bundles are expected to be universal, following a classification of modular functors.","sentences":["The conformal anomaly and the Virasoro algebra are fundamental aspects of 2D conformal field theory and conformally covariant models in planar random geometry.","In this article, we explicitly derive the Virasoro algebra from an axiomatization of the conformal anomaly in terms of real determinant lines, one-dimensional vector spaces associated to Riemann surfaces with analytically parametrized boundary components.","Here, analytical orientation-preserving diffeomorphisms and deformations of the circle naturally act on the boundary components.","We introduce a sewing operation on the real determinant lines over the semigroup of annuli, which then induces central extensions of the diffeomorphism group, as well as of the complex deformations.   ","Our main theorem shows that on the one hand, the Lie algebra cocycle associated to the central extension of diffeomorphisms is trivial, while on the other hand, the one associated to the central extension of complex deformations is nontrivial, yielding the imaginary part of the Gel'fand-Fuks cocycle.","The proof uses concrete computations, which we hope to be accessible to a wide audience.","We also show an explicit relation to loop Loewner energy, anticipating the real determinant lines to be pertinent to semi-classical limits of random curves, as well as to Kahler geometry and geometric quantization of moduli spaces of Riemann surfaces.","The conformal anomaly and real determinant line bundles are expected to be universal, following a classification of modular functors."],"url":"http://arxiv.org/abs/2403.09628v1","category":"math-ph"}
{"created":"2024-03-14 17:51:38","title":"Compute-first optical detection for noise-resilient visual perception","abstract":"In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information. In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection. Here, we propose a concept of optical signal processing before detection to address this issue. We demonstrate that spatially redistributing optical signals through a properly designed linear transformer can enhance the detection noise resilience of visual perception tasks, as benchmarked with the MNIST classification. Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementation in an incoherent imaging system. This compute-first detection scheme can pave the way for advancing infrared machine vision technologies widely used for industrial and defense applications.","sentences":["In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information.","In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection.","Here, we propose a concept of optical signal processing before detection to address this issue.","We demonstrate that spatially redistributing optical signals through a properly designed linear transformer can enhance the detection noise resilience of visual perception tasks, as benchmarked with the MNIST classification.","Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementation in an incoherent imaging system.","This compute-first detection scheme can pave the way for advancing infrared machine vision technologies widely used for industrial and defense applications."],"url":"http://arxiv.org/abs/2403.09612v1","category":"physics.optics"}
{"created":"2024-03-14 17:48:30","title":"pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication","abstract":"Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.","sentences":["Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF).","These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process.","However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models.","We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF.","In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills.","We implemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and desktop conditions.","Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam.","We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity."],"url":"http://arxiv.org/abs/2403.09607v1","category":"cs.HC"}
{"created":"2024-03-14 17:36:16","title":"Scalable Autonomous Drone Flight in the Forest with Visual-Inertial SLAM and Dense Submaps Built without LiDAR","abstract":"Forestry constitutes a key element for a sustainable future, while it is supremely challenging to introduce digital processes to improve efficiency. The main limitation is the difficulty of obtaining accurate maps at high temporal and spatial resolution as a basis for informed forestry decision-making, due to the vast area forests extend over and the sheer number of trees. To address this challenge, we present an autonomous Micro Aerial Vehicle (MAV) system which purely relies on cost-effective and light-weight passive visual and inertial sensors to perform under-canopy autonomous navigation. We leverage visual-inertial simultaneous localization and mapping (VI-SLAM) for accurate MAV state estimates and couple it with a volumetric occupancy submapping system to achieve a scalable mapping framework which can be directly used for path planning. As opposed to a monolithic map, submaps inherently deal with inevitable drift and corrections from VI-SLAM, since they move with pose estimates as they are updated. To ensure the safety of the MAV during navigation, we also propose a novel reference trajectory anchoring scheme that moves and deforms the reference trajectory the MAV is tracking upon state updates from the VI-SLAM system in a consistent way, even upon large changes in state estimates due to loop-closures. We thoroughly validate our system in both real and simulated forest environments with high tree densities in excess of 400 trees per hectare and at speeds up to 3 m/s - while not encountering a single collision or system failure. To the best of our knowledge this is the first system which achieves this level of performance in such unstructured environment using low-cost passive visual sensors and fully on-board computation including VI-SLAM.","sentences":["Forestry constitutes a key element for a sustainable future, while it is supremely challenging to introduce digital processes to improve efficiency.","The main limitation is the difficulty of obtaining accurate maps at high temporal and spatial resolution as a basis for informed forestry decision-making, due to the vast area forests extend over and the sheer number of trees.","To address this challenge, we present an autonomous Micro Aerial Vehicle (MAV) system which purely relies on cost-effective and light-weight passive visual and inertial sensors to perform under-canopy autonomous navigation.","We leverage visual-inertial simultaneous localization and mapping (VI-SLAM) for accurate MAV state estimates and couple it with a volumetric occupancy submapping system to achieve a scalable mapping framework which can be directly used for path planning.","As opposed to a monolithic map, submaps inherently deal with inevitable drift and corrections from VI-SLAM, since they move with pose estimates as they are updated.","To ensure the safety of the MAV during navigation, we also propose a novel reference trajectory anchoring scheme that moves and deforms the reference trajectory the MAV is tracking upon state updates from the VI-SLAM system in a consistent way, even upon large changes in state estimates due to loop-closures.","We thoroughly validate our system in both real and simulated forest environments with high tree densities in excess of 400 trees per hectare and at speeds up to 3 m/s - while not encountering a single collision or system failure.","To the best of our knowledge this is the first system which achieves this level of performance in such unstructured environment using low-cost passive visual sensors and fully on-board computation including VI-SLAM."],"url":"http://arxiv.org/abs/2403.09596v1","category":"cs.RO"}
{"created":"2024-03-14 17:35:57","title":"A comprehensive study of orbital evolution of LMC X-4: Existence of a second derivative of the orbital period","abstract":"We report here results from pulse arrival time delay analysis of the eclipsing high mass X-ray binary pulsar LMC X-4 using observations made with the Rossi X-ray Timing Explorer, XMM-Newton, NuSTAR and AstroSat. Combining the orbital parameters determined from these observations with the historical measurements dating back to 1998, we have extended the $T_{\\pi/2}$ epoch history of LMC X-4 by about 4600 binary orbits spanning about 18 years. We also report mid-eclipse time measurements ($T_{ecl}$) using data obtained from wide-field X-ray monitors of MAXI-GSC and Swift-BAT. Combining the new $T_{\\pi/2}$ and $T_{ecl}$ estimates with all the previously reported values, we have significantly improved the orbital evolution measurement, which indicates that the orbital period is evolving at a time scale ($P_{\\rm orb}/\\dot{P}_{\\rm orb}$ ) of about 0.8 Myr. For the first time in an accreting X-ray pulsar system, we confirm the existence of a second derivative of the orbital period, having an evolution time scale ($\\dot{P}_{orb}/\\ddot{P}_{orb}$) of about 55 yr. Detection of a second derivative of the orbital period in LMC X-4 makes its orbital evolution timescale more uncertain, which may also be true for other HMXBs. Independent solutions for the orbital evolution measurement using the mid-eclipse data and the pulse timing data are consistent with each other, and help us put an upper limit of 0.009 on the eccentricity of the binary system.","sentences":["We report here results from pulse arrival time delay analysis of the eclipsing high mass X-ray binary pulsar LMC X-4 using observations made with the Rossi X-ray Timing Explorer, XMM-Newton, NuSTAR and AstroSat.","Combining the orbital parameters determined from these observations with the historical measurements dating back to 1998, we have extended the $T_{\\pi/2}$ epoch history of LMC X-4 by about 4600 binary orbits spanning about 18 years.","We also report mid-eclipse time measurements ($T_{ecl}$) using data obtained from wide-field X-ray monitors of MAXI-GSC and Swift-BAT.","Combining the new $T_{\\pi/2}$ and $T_{ecl}$ estimates with all the previously reported values, we have significantly improved the orbital evolution measurement, which indicates that the orbital period is evolving at a time scale ($P_{\\rm orb}/\\dot{P}_{\\rm orb}$ ) of about 0.8 Myr.","For the first time in an accreting X-ray pulsar system, we confirm the existence of a second derivative of the orbital period, having an evolution time scale ($\\dot{P}_{orb}/\\ddot{P}_{orb}$) of about 55 yr. Detection of a second derivative of the orbital period in LMC X-4 makes its orbital evolution timescale more uncertain, which may also be true for other HMXBs.","Independent solutions for the orbital evolution measurement using the mid-eclipse data and the pulse timing data are consistent with each other, and help us put an upper limit of 0.009 on the eccentricity of the binary system."],"url":"http://arxiv.org/abs/2403.09595v1","category":"astro-ph.HE"}
{"created":"2024-03-14 17:26:00","title":"Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation","abstract":"Many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making. Such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to. There exists a need to create novel data stream regression methods that can handle these scenarios. We present a database-inspired datastream regression model that (a) uses inspiration from R*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions. The R*-tree-inspired approach also makes the algorithm amenable to integration with database systems. Our experiments demonstrate that the ability of this method to discard data produces a significant order-of-magnitude improvement in latency and training time when evaluated against the most accurate state-of-the-art algorithms, while the R*-tree-inspired granulation technique provides competitively accurate predictions","sentences":["Many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making.","Such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to.","There exists a need to create novel data stream regression methods that can handle these scenarios.","We present a database-inspired datastream regression model that (a) uses inspiration from R*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions.","The R*-tree-inspired approach also makes the algorithm amenable to integration with database systems.","Our experiments demonstrate that the ability of this method to discard data produces a significant order-of-magnitude improvement in latency and training time when evaluated against the most accurate state-of-the-art algorithms, while the R*-tree-inspired granulation technique provides competitively accurate predictions"],"url":"http://arxiv.org/abs/2403.09588v1","category":"cs.LG"}
{"created":"2024-03-14 17:09:22","title":"Angle estimation using mmWave RSS measurements with enhanced multipath information","abstract":"mmWave communication has come up as the unexplored spectrum for 5G services. With new standards for 5G NR positioning, more off-the-shelf platforms and algorithms are needed to perform indoor positioning. An object can be accurately positioned in a room either by using an angle and a delay estimate or two angle estimates or three delay estimates. We propose an algorithm to jointly estimate the angle of arrival (AoA) and angle of departure (AoD), based only on the received signal strength (RSS). We use mm-FLEX, an experimentation platform developed by IMDEA Networks Institute that can perform real-time signal processing for experimental validation of our proposed algorithm. Codebook-based beampatterns are used with a uniquely placed multi-antenna array setup to enhance the reception of multipath components and we obtain an AoA estimate per receiver thereby overcoming the line-of-sight (LoS) limitation of RSS-based localization systems. We further validate the results from measurements by emulating the setup with a simple ray-tracing approach.","sentences":["mmWave communication has come up as the unexplored spectrum for 5G services.","With new standards for 5G NR positioning, more off-the-shelf platforms and algorithms are needed to perform indoor positioning.","An object can be accurately positioned in a room either by using an angle and a delay estimate or two angle estimates or three delay estimates.","We propose an algorithm to jointly estimate the angle of arrival (AoA) and angle of departure (AoD), based only on the received signal strength (RSS).","We use mm-FLEX, an experimentation platform developed by IMDEA Networks Institute that can perform real-time signal processing for experimental validation of our proposed algorithm.","Codebook-based beampatterns are used with a uniquely placed multi-antenna array setup to enhance the reception of multipath components and we obtain an AoA estimate per receiver thereby overcoming the line-of-sight (LoS) limitation of RSS-based localization systems.","We further validate the results from measurements by emulating the setup with a simple ray-tracing approach."],"url":"http://arxiv.org/abs/2403.09575v1","category":"eess.SP"}
{"created":"2024-03-14 17:03:11","title":"Learning High-Order Control Barrier Functions for Safety-Critical Control with Gaussian Processes","abstract":"Control barrier functions (CBFs) have recently introduced a systematic tool to ensure system safety by establishing set invariance. When combined with a nominal control strategy, they form a safety-critical control mechanism. However, the effectiveness of CBFs is closely tied to the system model. In practice, model uncertainty can compromise safety guarantees and may lead to conservative safety constraints, or conversely, allow the system to operate in unsafe regions. In this paper, we use Gaussian processes to mitigate the adverse effects of uncertainty on high-order CBFs (HOCBFs). A properly structured covariance function enables us to convert the chance constraints of HOCBFs into a second-order cone constraint. This results in a convex constrained optimization as a safety filter. We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility. The effectiveness of the proposed strategy is validated through two numerical results.","sentences":["Control barrier functions (CBFs) have recently introduced a systematic tool to ensure system safety by establishing set invariance.","When combined with a nominal control strategy, they form a safety-critical control mechanism.","However, the effectiveness of CBFs is closely tied to the system model.","In practice, model uncertainty can compromise safety guarantees and may lead to conservative safety constraints, or conversely, allow the system to operate in unsafe regions.","In this paper, we use Gaussian processes to mitigate the adverse effects of uncertainty on high-order CBFs (HOCBFs).","A properly structured covariance function enables us to convert the chance constraints of HOCBFs into a second-order cone constraint.","This results in a convex constrained optimization as a safety filter.","We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility.","The effectiveness of the proposed strategy is validated through two numerical results."],"url":"http://arxiv.org/abs/2403.09573v1","category":"eess.SY"}
{"created":"2024-03-14 16:56:56","title":"PaperBot: Learning to Design Real-World Tools Using Paper","abstract":"Paper is a cheap, recyclable, and clean material that is often used to make practical tools. Traditional tool design either relies on simulation or physical analysis, which is often inaccurate and time-consuming. In this paper, we propose PaperBot, an approach that directly learns to design and use a tool in the real world using paper without human intervention. We demonstrated the effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning to fold and throw paper airplanes for maximum travel distance 2. learning to cut paper into grippers that exert maximum gripping force. We present a self-supervised learning framework that learns to perform a sequence of folding, cutting, and dynamic manipulation actions in order to optimize the design and use of a tool. We deploy our system to a real-world two-arm robotic system to solve challenging design tasks that involve aerodynamics (paper airplane) and friction (paper gripper) that are impossible to simulate accurately.","sentences":["Paper is a cheap, recyclable, and clean material that is often used to make practical tools.","Traditional tool design either relies on simulation or physical analysis, which is often inaccurate and time-consuming.","In this paper, we propose PaperBot, an approach that directly learns to design and use a tool in the real world using paper without human intervention.","We demonstrated the effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning to fold and throw paper airplanes for maximum travel distance 2.","learning to cut paper into grippers that exert maximum gripping force.","We present a self-supervised learning framework that learns to perform a sequence of folding, cutting, and dynamic manipulation actions in order to optimize the design and use of a tool.","We deploy our system to a real-world two-arm robotic system to solve challenging design tasks that involve aerodynamics (paper airplane) and friction (paper gripper) that are impossible to simulate accurately."],"url":"http://arxiv.org/abs/2403.09566v1","category":"cs.RO"}
{"created":"2024-03-14 16:53:12","title":"Characterization of Polarimetric Properties in Various Brain Tumor Types Using Wide-Field Imaging Mueller Polarimetry","abstract":"Neuro-oncological surgery is the primary brain cancer treatment, yet it faces challenges with gliomas due to their invasiveness and the need to preserve neurological function. Hence, radical resection is often unfeasible, highlighting the importance of precise tumor margin delineation to prevent neurological deficits and improve prognosis. Imaging Mueller polarimetry, an effective modality in various organ tissues, seems a promising approach for tumor delineation in neurosurgery. To further assess its use, we characterized the polarimetric properties by analysing 45 polarimetric measurements of 27 fresh brain tumor samples, including different tumor types with a strong focus on gliomas. Our study integrates a wide-field imaging Mueller polarimetric system and a novel neuropathology protocol, correlating polarimetric and histological data for accurate tissue identification. An image processing pipeline facilitated the alignment and overlay of polarimetric images and histological masks. Variations in depolarization values were observed for grey and white matter of brain tumor tissue, while differences in linear retardance were seen only within white matter of brain tumor tissue. Notably, we identified pronounced optical axis azimuth randomization within tumor regions. This study lays the foundation for machine learning-based brain tumor segmentation algorithms using polarimetric data, facilitating intraoperative diagnosis and decision making.","sentences":["Neuro-oncological surgery is the primary brain cancer treatment, yet it faces challenges with gliomas due to their invasiveness and the need to preserve neurological function.","Hence, radical resection is often unfeasible, highlighting the importance of precise tumor margin delineation to prevent neurological deficits and improve prognosis.","Imaging Mueller polarimetry, an effective modality in various organ tissues, seems a promising approach for tumor delineation in neurosurgery.","To further assess its use, we characterized the polarimetric properties by analysing 45 polarimetric measurements of 27 fresh brain tumor samples, including different tumor types with a strong focus on gliomas.","Our study integrates a wide-field imaging Mueller polarimetric system and a novel neuropathology protocol, correlating polarimetric and histological data for accurate tissue identification.","An image processing pipeline facilitated the alignment and overlay of polarimetric images and histological masks.","Variations in depolarization values were observed for grey and white matter of brain tumor tissue, while differences in linear retardance were seen only within white matter of brain tumor tissue.","Notably, we identified pronounced optical axis azimuth randomization within tumor regions.","This study lays the foundation for machine learning-based brain tumor segmentation algorithms using polarimetric data, facilitating intraoperative diagnosis and decision making."],"url":"http://arxiv.org/abs/2403.09561v1","category":"physics.med-ph"}
{"created":"2024-03-14 16:43:33","title":"Memoryless concretization relation","abstract":"We introduce the concept of memoryless concretization relation (MCR) to describe abstraction within the context of controller synthesis. This relation is a specific instance of alternating simulation relation (ASR), where it is possible to simplify the controller architecture. In the case of ASR, the concretized controller needs to simulate the concurrent evolution of two systems, the original and abstract systems, while for MCR, the designed controllers only need knowledge of the current concrete state. We demonstrate that the distinction between ASR and MCR becomes significant only when a non-deterministic quantizer is involved, such as in cases where the state space discretization consists of overlapping cells. We also show that any abstraction of a system that alternatingly simulates a system can be completed to satisfy MCR at the expense of increasing the non-determinism in the abstraction. We clarify the difference between the MCR and the feedback refinement relation (FRR), showing in particular that the former allows for non-constant controllers within cells. This provides greater flexibility in constructing a practical abstraction, for instance, by reducing non-determinism in the abstraction. Finally, we prove that this relation is not only sufficient, but also necessary, for ensuring the above properties.","sentences":["We introduce the concept of memoryless concretization relation (MCR) to describe abstraction within the context of controller synthesis.","This relation is a specific instance of alternating simulation relation (ASR), where it is possible to simplify the controller architecture.","In the case of ASR, the concretized controller needs to simulate the concurrent evolution of two systems, the original and abstract systems, while for MCR, the designed controllers only need knowledge of the current concrete state.","We demonstrate that the distinction between ASR and MCR becomes significant only when a non-deterministic quantizer is involved, such as in cases where the state space discretization consists of overlapping cells.","We also show that any abstraction of a system that alternatingly simulates a system can be completed to satisfy MCR at the expense of increasing the non-determinism in the abstraction.","We clarify the difference between the MCR and the feedback refinement relation (FRR), showing in particular that the former allows for non-constant controllers within cells.","This provides greater flexibility in constructing a practical abstraction, for instance, by reducing non-determinism in the abstraction.","Finally, we prove that this relation is not only sufficient, but also necessary, for ensuring the above properties."],"url":"http://arxiv.org/abs/2403.09556v1","category":"math.DS"}
{"created":"2024-03-14 16:41:08","title":"A targeted radio pulsar survey of redback candidates with MeerKAT","abstract":"Redbacks are millisecond pulsar binaries with low mass, irradiated companions. These systems have a rich phenomenology that can be used to probe binary evolution models, pulsar wind physics, and the neutron star mass distribution. A number of high-confidence redback candidates have been identified through searches for variable optical and X-ray sources within the localisation regions of unidentified but pulsar-like Fermi-LAT gamma-ray sources. However, these candidates remain unconfirmed until pulsations are detected. As part of the TRAPUM project, we searched for radio pulsations from six of these redback candidates with MeerKAT. We discovered three new radio millisecond pulsars, PSRs J0838$-$2527, J0955$-$3947 and J2333$-$5526, confirming their redback nature. PSR J0838$-$2827 remained undetected for two years after our discovery despite repeated observations, likely due to evaporated material absorbing the radio emission for long periods of time. While, to our knowledge, this system has not undergone a transition to an accreting state, the disappearance, likely caused by extreme eclipses, illustrates the transient nature of spider pulsars and the heavy selection bias in uncovering their radio population. Radio timing enabled the detection of gamma-ray pulsations from all three pulsars, from which we obtained 15-year timing solutions. All of these sources exhibit complex orbital period variations consistent with gravitational quadrupole moment variations in the companion stars. These timing solutions also constrain the binary mass ratios, allowing us to narrow down the pulsar masses. We find that PSR J2333$-$5526 may have a neutron star mass in excess of 2 M$_{\\odot}$.","sentences":["Redbacks are millisecond pulsar binaries with low mass, irradiated companions.","These systems have a rich phenomenology that can be used to probe binary evolution models, pulsar wind physics, and the neutron star mass distribution.","A number of high-confidence redback candidates have been identified through searches for variable optical and X-ray sources within the localisation regions of unidentified but pulsar-like Fermi-LAT gamma-ray sources.","However, these candidates remain unconfirmed until pulsations are detected.","As part of the TRAPUM project, we searched for radio pulsations from six of these redback candidates with MeerKAT.","We discovered three new radio millisecond pulsars, PSRs J0838$-$2527, J0955$-$3947 and J2333$-$5526, confirming their redback nature.","PSR J0838$-$2827 remained undetected for two years after our discovery despite repeated observations, likely due to evaporated material absorbing the radio emission for long periods of time.","While, to our knowledge, this system has not undergone a transition to an accreting state, the disappearance, likely caused by extreme eclipses, illustrates the transient nature of spider pulsars and the heavy selection bias in uncovering their radio population.","Radio timing enabled the detection of gamma-ray pulsations from all three pulsars, from which we obtained 15-year timing solutions.","All of these sources exhibit complex orbital period variations consistent with gravitational quadrupole moment variations in the companion stars.","These timing solutions also constrain the binary mass ratios, allowing us to narrow down the pulsar masses.","We find that PSR J2333$-$5526 may have a neutron star mass in excess of 2 M$_{\\odot}$."],"url":"http://arxiv.org/abs/2403.09553v1","category":"astro-ph.HE"}
{"created":"2024-03-14 16:29:56","title":"Competing Interactions in Strongly Driven Multi-Level Systems","abstract":"We experimentally study the level mixing, splitting and repulsion of an optically driven atomic multi-level system under two competing interactions. The strength of the optical coupling is increased until it surpasses the atomic hyperfine interaction responsible for mixing the magnetic substates. Due to the multi-level character of the coupled state space, the level shifts exhibit complex behavior reminiscent of the Paschen-Back effect. Our results show that multi-level effects can have significant influence for strong external drive, differing from a simple model of effective non-interacting two-level systems. These results highlight the relevance of imperfections of the light polarization or initial state preparation in strongly optically driven systems.","sentences":["We experimentally study the level mixing, splitting and repulsion of an optically driven atomic multi-level system under two competing interactions.","The strength of the optical coupling is increased until it surpasses the atomic hyperfine interaction responsible for mixing the magnetic substates.","Due to the multi-level character of the coupled state space, the level shifts exhibit complex behavior reminiscent of the Paschen-Back effect.","Our results show that multi-level effects can have significant influence for strong external drive, differing from a simple model of effective non-interacting two-level systems.","These results highlight the relevance of imperfections of the light polarization or initial state preparation in strongly optically driven systems."],"url":"http://arxiv.org/abs/2403.09542v1","category":"quant-ph"}
{"created":"2024-03-14 16:17:11","title":"Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems","abstract":"The recent application of Federated Learning algorithms in IOT and Wireless vehicular networks have given rise to newer cyber threats in the mobile environment which hitherto were not present in traditional fixed networks. These threats arise due to the intrinsic nature of wireless transmission medium and other inherent characteristics of mobile networks such as high-node mobility and rapidly changing topology. This paper investigates the robustness of Vehicular AttestedFL defense strategies against falsified information attacks by tracking the behavior. We show that the defense strategies are capable of detecting and eliminating malicious nodes in the wireless mobile setting of the future smart road networks.","sentences":["The recent application of Federated Learning algorithms in IOT and Wireless vehicular networks have given rise to newer cyber threats in the mobile environment which hitherto were not present in traditional fixed networks.","These threats arise due to the intrinsic nature of wireless transmission medium and other inherent characteristics of mobile networks such as high-node mobility and rapidly changing topology.","This paper investigates the robustness of Vehicular AttestedFL defense strategies against falsified information attacks by tracking the behavior.","We show that the defense strategies are capable of detecting and eliminating malicious nodes in the wireless mobile setting of the future smart road networks."],"url":"http://arxiv.org/abs/2403.09531v1","category":"eess.SY"}
{"created":"2024-03-14 16:10:37","title":"Decay of Correlations via induced Weak Gibbs Markov maps for non-H\u00f6lder observables","abstract":"We extend the results of [Ullah, A., Vilarinho, H,.: Statistical properties of dynamical systems via induced weak Gibbs Markov maps. arXiv:2311.17531 (2023)] by considering larger classes of observables. More precisely, we obtain estimates on the decay of correlations, Central Limit Theorem and Large Deviations for dynamical systems having an induced weak Gibbs Markov map, for larger classes of observables with weaker regularity than H\\\"{o}lder.","sentences":["We extend the results of [Ullah, A., Vilarinho, H,.:","Statistical properties of dynamical systems via induced weak Gibbs Markov maps.","arXiv:2311.17531 (2023)] by considering larger classes of observables.","More precisely, we obtain estimates on the decay of correlations, Central Limit Theorem and Large Deviations for dynamical systems having an induced weak Gibbs Markov map, for larger classes of observables with weaker regularity than H\\\"{o}lder."],"url":"http://arxiv.org/abs/2403.09528v1","category":"math.DS"}
{"created":"2024-03-14 16:10:21","title":"An Extreme Ultra-Compact X-ray Binary in a Globular Cluster: Multi-Wavelength Observations of RZ2109 Explored in a Triple System Framework","abstract":"The globular cluster ultraluminous X-ray source, RZ2109, is a complex and unique system which has been detected at X-ray, ultra-violet, and optical wavelengths. Based on almost 20 years of Chandra and XMM-Newton observations, the X-ray luminosity exhibits order-of-magnitude variability, with the peak flux lasting on the order of a few hours. We perform robust time series analysis on the archival X-ray observations and find that this variability is periodic on a timescale of 1.3 $\\pm 0.04$ days. The source also demonstrates broad [OIII] 5007 Angstrom emission, which has been observed since 2004, suggesting a white dwarf donor and therefore an ultra-compact X-ray binary. We present new spectra from 2020 and 2022, marking eighteen years of observed [OIII] emission from this source. Meanwhile, we find that the globular cluster counterpart is unusually bright in the NUV/UVW2 band. Finally, we discuss RZ2109 in the context of the eccentric Kozai Lidov mechanism and show that the observed 1.3 day periodicity can be used to place constraints on the tertiary configuration, ranging from 20 minutes (for a 0.1 ${\\rm M}_\\odot$ companion) to approximately 95 minutes (for a 1 ${\\rm M}_\\odot$ companion), if the eccentric Kozai Lidov mechanism is at the origin of the periodic variability.","sentences":["The globular cluster ultraluminous X-ray source, RZ2109, is a complex and unique system which has been detected at X-ray, ultra-violet, and optical wavelengths.","Based on almost 20 years of Chandra and XMM-Newton observations, the X-ray luminosity exhibits order-of-magnitude variability, with the peak flux lasting on the order of a few hours.","We perform robust time series analysis on the archival X-ray observations and find that this variability is periodic on a timescale of 1.3 $\\pm 0.04$ days.","The source also demonstrates broad","[OIII] 5007","Angstrom emission, which has been observed since 2004, suggesting a white dwarf donor and therefore an ultra-compact X-ray binary.","We present new spectra from 2020 and 2022, marking eighteen years of observed [OIII] emission from this source.","Meanwhile, we find that the globular cluster counterpart is unusually bright in the NUV/UVW2 band.","Finally, we discuss RZ2109 in the context of the eccentric Kozai Lidov mechanism and show that the observed 1.3 day periodicity can be used to place constraints on the tertiary configuration, ranging from 20 minutes (for a 0.1 ${\\rm M}_\\odot$ companion) to approximately 95 minutes (for a 1 ${\\rm M}_\\odot$ companion), if the eccentric Kozai Lidov mechanism is at the origin of the periodic variability."],"url":"http://arxiv.org/abs/2403.09525v1","category":"astro-ph.HE"}
{"created":"2024-03-14 16:03:58","title":"Smallest gaps between eigenvalues of real Gaussian matrices","abstract":"We consider an $n\\times n$ matrix of independent real Gaussian random variables and determine the asymptotic distribution of the smallest gaps between complex eigenvalues.","sentences":["We consider an $n\\times n$ matrix of independent real Gaussian random variables and determine the asymptotic distribution of the smallest gaps between complex eigenvalues."],"url":"http://arxiv.org/abs/2403.09521v1","category":"math.PR"}
{"created":"2024-03-14 16:01:26","title":"Observation of quantum thermalization restricted to Hilbert space fragments","abstract":"Quantum thermalization occurs in a broad class of systems from elementary particles to complex materials. Out-of-equilibrium quantum systems have long been understood to either thermalize or retain memory of their initial states, but not both. Here we achieve the first simultaneous demonstration of thermalization and memory in a quantum system. Using a Rydberg atom array, we observe quantum thermalization restricted to Hilbert space fragments, where the thermalized system retains characteristics of the initial configuration. Intriguingly, states from different subspaces do not thermalize with each other even when they have the same energy. Our work challenges established ideas of quantum thermalization while experimentally resolving the longstanding tension between thermalization and memory. These results may be applied to control entanglement dynamics in quantum processors and quantum sensors.","sentences":["Quantum thermalization occurs in a broad class of systems from elementary particles to complex materials.","Out-of-equilibrium quantum systems have long been understood to either thermalize or retain memory of their initial states, but not both.","Here we achieve the first simultaneous demonstration of thermalization and memory in a quantum system.","Using a Rydberg atom array, we observe quantum thermalization restricted to Hilbert space fragments, where the thermalized system retains characteristics of the initial configuration.","Intriguingly, states from different subspaces do not thermalize with each other even when they have the same energy.","Our work challenges established ideas of quantum thermalization while experimentally resolving the longstanding tension between thermalization and memory.","These results may be applied to control entanglement dynamics in quantum processors and quantum sensors."],"url":"http://arxiv.org/abs/2403.09517v1","category":"quant-ph"}
{"created":"2024-03-14 15:56:02","title":"On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study","abstract":"Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions. This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD). System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry. However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels. This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability. This can be seen as a maintainability challenge in continuous development and deployment (DevOps). In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compared to assess their applicability to the distributed development of complex AI-enabled systems like AD. Further, an approach to overcome the challenges of using STPA in a multi-level design context is proposed. By conducting an interview study with automotive industry experts for the development of AD, the challenges are validated and the effectiveness of the proposed approach is evaluated.","sentences":["Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions.","This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD).","System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry.","However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels.","This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability.","This can be seen as a maintainability challenge in continuous development and deployment (DevOps).","In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compared to assess their applicability to the distributed development of complex AI-enabled systems like AD.","Further, an approach to overcome the challenges of using STPA in a multi-level design context is proposed.","By conducting an interview study with automotive industry experts for the development of AD, the challenges are validated and the effectiveness of the proposed approach is evaluated."],"url":"http://arxiv.org/abs/2403.09509v1","category":"cs.SE"}
{"created":"2024-03-14 15:48:07","title":"Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems","abstract":"Learning models or control policies from data has become a powerful tool to improve the performance of uncertain systems. While a strong focus has been placed on increasing the amount and quality of data to improve performance, data can never fully eliminate uncertainty, making feedback necessary to ensure stability and performance. We show that the control frequency at which the input is recalculated is a crucial design parameter, yet it has hardly been considered before. We address this gap by combining probabilistic model learning and sampled-data control. We use Gaussian processes (GPs) to learn a continuous-time model and compute a corresponding discrete-time controller. The result is an uncertain sampled-data control system, for which we derive robust stability conditions. We formulate semidefinite programs to compute the minimum control frequency required for stability and to optimize performance. As a result, our approach enables us to study the effect of both control frequency and data on stability and closed-loop performance. We show in numerical simulations of a quadrotor that performance can be improved by increasing either the amount of data or the control frequency, and that we can trade off one for the other. For example, by increasing the control frequency by 33%, we can reduce the number of data points by half while still achieving similar performance.","sentences":["Learning models or control policies from data has become a powerful tool to improve the performance of uncertain systems.","While a strong focus has been placed on increasing the amount and quality of data to improve performance, data can never fully eliminate uncertainty, making feedback necessary to ensure stability and performance.","We show that the control frequency at which the input is recalculated is a crucial design parameter, yet it has hardly been considered before.","We address this gap by combining probabilistic model learning and sampled-data control.","We use Gaussian processes (GPs) to learn a continuous-time model and compute a corresponding discrete-time controller.","The result is an uncertain sampled-data control system, for which we derive robust stability conditions.","We formulate semidefinite programs to compute the minimum control frequency required for stability and to optimize performance.","As a result, our approach enables us to study the effect of both control frequency and data on stability and closed-loop performance.","We show in numerical simulations of a quadrotor that performance can be improved by increasing either the amount of data or the control frequency, and that we can trade off one for the other.","For example, by increasing the control frequency by 33%, we can reduce the number of data points by half while still achieving similar performance."],"url":"http://arxiv.org/abs/2403.09504v1","category":"eess.SY"}
{"created":"2024-03-14 15:18:28","title":"Sunyaev-Zeldovich Signals from $L^*$ Galaxies: Observations, Analytics, and Simulations","abstract":"We analyze measurements of the thermal Sunyaev-Zeldovich (tSZ) effect arising in the circumgalactic medium (CGM) of $L^*$ galaxies, reported by Bregman et al. 2022 and Das et al. 2023. In our analysis we use the Faerman et al. 2017 and Faerman et al. 2020 CGM models, a new power-law model (PLM), and the TNG100 simulation. For a given $M_{\\rm vir}$, our PLM has four parameters; the fraction, $f_{\\rm hCGM}$, of the halo baryon mass in hot CGM gas, the ratio, $\\phi_T$, of the actual gas temperature at the virial radius to the virial temperature, and the power-law indicies, $a_{P,{\\rm th}}$ and $a_n$ for the thermal electron pressure and the hydrogen nucleon density. The B+22 Compton-$y$ profile implies steep electron pressure slopes ($a_{P,{\\rm th}}\\simeq 2$). For isothermal conditions the temperature is at least $1.1\\times 10^6$ K, with a hot CGM gas mass of up to $3.5\\times 10^{11}$ M$_\\odot$ for a virial mass of $2.75\\times 10^{12}$ M$_\\odot$. However, if isothermal the gas must be expanding out of the halos. An isentropic equation of state is favored for which hydrostatic equilibrium is possible. The B+22 and D+23 results are consistent with each other and with recent (0.5-2 keV) CGM X-ray observations by Zhang et al. 2024 of Milky Way mass systems. For $M_{\\rm vir}\\simeq 3\\times 10^{12}$ M$_\\odot$, the scaled Compton pressure integrals, $E(z)^{-2/3}Y_{500}/M_{\\rm vir,12}^{5/3}$, lie in the narrow range, $2.5\\times 10^{-4}$ to $5.0\\times 10^{-4}$ kpc$^2$, for all three sets of observations. TNG100 underpredicts the tSZ parameters by factors $\\sim 0.5$ dex for the $L^*$ galaxies, suggesting that the feedback strengths and CGM gas losses are overestimated in the simulated halos at these mass scales.","sentences":["We analyze measurements of the thermal Sunyaev-Zeldovich (tSZ) effect arising in the circumgalactic medium (CGM) of $L^*$ galaxies, reported by Bregman et al. 2022 and Das et al. 2023.","In our analysis we use the Faerman et al. 2017 and Faerman et al. 2020","CGM models, a new power-law model (PLM), and the TNG100 simulation.","For a given $M_{\\rm vir}$, our PLM has four parameters; the fraction, $f_{\\rm hCGM}$, of the halo baryon mass in hot CGM gas, the ratio, $\\phi_T$, of the actual gas temperature at the virial radius to the virial temperature, and the power-law indicies, $a_{P,{\\rm th}}$ and $a_n$ for the thermal electron pressure and the hydrogen nucleon density.","The B+22 Compton-$y$ profile implies steep electron pressure slopes ($a_{P,{\\rm th}}\\simeq 2$).","For isothermal conditions the temperature is at least $1.1\\times 10^6$ K, with a hot CGM gas mass of up to $3.5\\times 10^{11}$ M$_\\odot$ for a virial mass of $2.75\\times 10^{12}$ M$_\\odot$.","However, if isothermal the gas must be expanding out of the halos.","An isentropic equation of state is favored for which hydrostatic equilibrium is possible.","The B+22 and D+23 results are consistent with each other and with recent (0.5-2 keV) CGM X-ray observations by Zhang et al. 2024 of Milky Way mass systems.","For $M_{\\rm vir}\\simeq 3\\times 10^{12}$ M$_\\odot$, the scaled Compton pressure integrals, $E(z)^{-2/3}Y_{500}/M_{\\rm vir,12}^{5/3}$, lie in the narrow range, $2.5\\times 10^{-4}$ to $5.0\\times 10^{-4}$ kpc$^2$, for all three sets of observations.","TNG100 underpredicts the tSZ parameters by factors $\\sim 0.5$ dex for the $L^*$ galaxies, suggesting that the feedback strengths and CGM gas losses are overestimated in the simulated halos at these mass scales."],"url":"http://arxiv.org/abs/2403.09476v1","category":"astro-ph.GA"}
{"created":"2024-03-14 15:17:56","title":"Covert Communication for Untrusted UAV-Assisted Wireless Systems","abstract":"Wireless systems are of paramount importance for providing ubiquitous data transmission for smart cities. However, due to the broadcasting and openness of wireless channels, such systems face potential security challenges. UAV-assisted covert communication is a supporting technology for improving covert performances and has become a hot issue in the research of wireless communication security. This paper investigates the performance of joint covert and security communication in a tow-hop UAV-assisted wireless system, where a source transmits the covert message to a destination with the help of an untrusted UAV. We first design a transmission scheme such that use UAVs to assist in covert communications while ensuring the security of covert messages. Then, we develop a theoretical model to derive the expressions for the detection error probability of the warden and the covert and security rate, and the maximum covert and security rate is optimized by power control under a given covertness and security requirements. Finally, numerical results are provided to illustrate our theoretical analysis and the performance of covert and security communication in such systems.","sentences":["Wireless systems are of paramount importance for providing ubiquitous data transmission for smart cities.","However, due to the broadcasting and openness of wireless channels, such systems face potential security challenges.","UAV-assisted covert communication is a supporting technology for improving covert performances and has become a hot issue in the research of wireless communication security.","This paper investigates the performance of joint covert and security communication in a tow-hop UAV-assisted wireless system, where a source transmits the covert message to a destination with the help of an untrusted UAV.","We first design a transmission scheme such that use UAVs to assist in covert communications while ensuring the security of covert messages.","Then, we develop a theoretical model to derive the expressions for the detection error probability of the warden and the covert and security rate, and the maximum covert and security rate is optimized by power control under a given covertness and security requirements.","Finally, numerical results are provided to illustrate our theoretical analysis and the performance of covert and security communication in such systems."],"url":"http://arxiv.org/abs/2403.09475v1","category":"cs.CR"}
{"created":"2024-03-14 15:00:46","title":"Exact matrix product state representations for a type of scale-invariant states","abstract":"Exact matrix product state representations for a type of scale-invariant states are presented, which describe highly degenerate ground states arising from spontaneous symmetry breaking with type-B Goldstone modes in one-dimensional quantum many-body systems. As a possible application, such a representation offers a convenient but powerful means for evaluating the norms of highly degenerate ground states. This in turn allows us to perform a universal finite system-size scaling analysis of the entanglement entropy. Moreover, this approach vividly explains why the entanglement entropy does not depend on what types of the boundary conditions are adopted, either periodic boundary conditions or open boundary conditions. Illustrative examples include the ${\\rm SU}(2)$ spin-$s$ Heisenberg ferromagnetic model, the ${\\rm SU}(2s+1)$ ferromagnetic model, and the staggered ${\\rm SU}(3)$ spin-1 ferromagnetic biquadratic model.","sentences":["Exact matrix product state representations for a type of scale-invariant states are presented, which describe highly degenerate ground states arising from spontaneous symmetry breaking with type-B Goldstone modes in one-dimensional quantum many-body systems.","As a possible application, such a representation offers a convenient but powerful means for evaluating the norms of highly degenerate ground states.","This in turn allows us to perform a universal finite system-size scaling analysis of the entanglement entropy.","Moreover, this approach vividly explains why the entanglement entropy does not depend on what types of the boundary conditions are adopted, either periodic boundary conditions or open boundary conditions.","Illustrative examples include the ${\\rm SU}(2)$ spin-$s$ Heisenberg ferromagnetic model, the ${\\rm SU}(2s+1)$ ferromagnetic model, and the staggered ${\\rm SU}(3)$ spin-1 ferromagnetic biquadratic model."],"url":"http://arxiv.org/abs/2403.09458v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 14:53:54","title":"The Neural-SRP method for positional sound source localization","abstract":"Steered Response Power (SRP) is a widely used method for the task of sound source localization using microphone arrays, showing satisfactory localization performance on many practical scenarios. However, its performance is diminished under highly reverberant environments. Although Deep Neural Networks (DNNs) have been previously proposed to overcome this limitation, most are trained for a specific number of microphones with fixed spatial coordinates. This restricts their practical application on scenarios frequently observed in wireless acoustic sensor networks, where each application has an ad-hoc microphone topology. We propose Neural-SRP, a DNN which combines the flexibility of SRP with the performance gains of DNNs. We train our network using simulated data and transfer learning, and evaluate our approach on recorded and simulated data. Results verify that Neural-SRP's localization performance significantly outperforms the baselines.","sentences":["Steered Response Power (SRP) is a widely used method for the task of sound source localization using microphone arrays, showing satisfactory localization performance on many practical scenarios.","However, its performance is diminished under highly reverberant environments.","Although Deep Neural Networks (DNNs) have been previously proposed to overcome this limitation, most are trained for a specific number of microphones with fixed spatial coordinates.","This restricts their practical application on scenarios frequently observed in wireless acoustic sensor networks, where each application has an ad-hoc microphone topology.","We propose Neural-SRP, a DNN which combines the flexibility of SRP with the performance gains of DNNs.","We train our network using simulated data and transfer learning, and evaluate our approach on recorded and simulated data.","Results verify that Neural-SRP's localization performance significantly outperforms the baselines."],"url":"http://arxiv.org/abs/2403.09455v1","category":"cs.SD"}
{"created":"2024-03-14 14:49:40","title":"M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment","abstract":"This paper introduces the M&M model, a novel multimodal-multitask learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA). M&M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs. A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for synchronized multitasking. Another notable feature is the model's three specialized branches, each tailored to a specific cognitive load label, enabling nuanced, task-specific analysis. While it shows modest performance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a promising framework for integrated multimodal processing. This work paves the way for future enhancements in multimodal-multitask learning systems, emphasizing the fusion of diverse data types for complex task handling.","sentences":["This paper introduces the M&M model, a novel multimodal-multitask learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA).","M&M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs.","A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for synchronized multitasking.","Another notable feature is the model's three specialized branches, each tailored to a specific cognitive load label, enabling nuanced, task-specific analysis.","While it shows modest performance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a promising framework for integrated multimodal processing.","This work paves the way for future enhancements in multimodal-multitask learning systems, emphasizing the fusion of diverse data types for complex task handling."],"url":"http://arxiv.org/abs/2403.09451v1","category":"cs.CV"}
{"created":"2024-03-14 14:42:30","title":"Benchmarking Distributed Coordination Systems: A Survey and Analysis","abstract":"Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability. However, due to lack of a standard benchmarking tool for distributed coordination services, coordination service developers/researchers either use a NoSQL standard benchmark and omit evaluating consistency, distribution, and fault-tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services. In this paper, we analyze and compare known and widely used distributed coordination services, their evaluations, and the tools used to benchmark those systems. We identify important requirements of distributed coordination service benchmarking, like the metrics and parameters that need to be evaluated and their evaluation setups and tools.","sentences":["Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability.","However, due to lack of a standard benchmarking tool for distributed coordination services, coordination service developers/researchers either use a NoSQL standard benchmark and omit evaluating consistency, distribution, and fault-tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services.","In this paper, we analyze and compare known and widely used distributed coordination services, their evaluations, and the tools used to benchmark those systems.","We identify important requirements of distributed coordination service benchmarking, like the metrics and parameters that need to be evaluated and their evaluation setups and tools."],"url":"http://arxiv.org/abs/2403.09445v1","category":"cs.DC"}
{"created":"2024-03-14 14:30:31","title":"Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting","abstract":"Current human pose estimation systems focus on retrieving an accurate 3D global estimate of a single person. Therefore, this paper presents one of the first 3D multi-person human pose estimation systems that is able to work in real-time and is also able to handle basic forms of occlusion. First, we adjust an off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use with a 360$^\\circ$ panoramic camera and mmWave radar sensors. We then introduce several contributions, including camera and radar calibrations, and the improved matching of people within the image and radar space. The system addresses both the depth and scale ambiguity problems by employing a lightweight 2D-3D pose lifting algorithm that is able to work in real-time while exhibiting accurate performance in both indoor and outdoor environments which offers both an affordable and scalable solution. Notably, our system's time complexity remains nearly constant irrespective of the number of detected individuals, achieving a frame rate of approximately 7-8 fps on a laptop with a commercial-grade GPU.","sentences":["Current human pose estimation systems focus on retrieving an accurate 3D global estimate of a single person.","Therefore, this paper presents one of the first 3D multi-person human pose estimation systems that is able to work in real-time and is also able to handle basic forms of occlusion.","First, we adjust an off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use with a 360$^\\circ$ panoramic camera and mmWave radar sensors.","We then introduce several contributions, including camera and radar calibrations, and the improved matching of people within the image and radar space.","The system addresses both the depth and scale ambiguity problems by employing a lightweight 2D-3D pose lifting algorithm that is able to work in real-time while exhibiting accurate performance in both indoor and outdoor environments which offers both an affordable and scalable solution.","Notably, our system's time complexity remains nearly constant irrespective of the number of detected individuals, achieving a frame rate of approximately 7-8 fps on a laptop with a commercial-grade GPU."],"url":"http://arxiv.org/abs/2403.09437v1","category":"cs.CV"}
{"created":"2024-03-14 14:23:02","title":"Field-orientation-dependent magnetic phases in GdRu$_2$Si$_2$ probed with muon-spin spectroscopy","abstract":"Centrosymmetric GdRu$_2$Si$_2$ exhibits a variety of multi-Q magnetic states as a function of temperature and applied magnetic field, including a square skyrmion-lattice phase. The material's behavior is strongly dependent on the direction of the applied field, with different phase diagrams resulting for fields applied parallel or perpendicular to the crystallographic $c$ axis. Here, we present the results of muon-spin relaxation ($\\mu^+$SR) measurements on single crystals of GdRu$_2$Si$_2$. Our analysis is based on the computation of muon stopping sites and consideration of zero-point motion effects, allowing direct comparison with the underlying spin textures in the material. Using transverse-field $\\mu^+$SR with fields applied along either the [001] or [100] crystallographic directions, we distinguish between the magnetic phases in this system via their distinct muon response, providing additional evidence for the skyrmion and meron-lattice phases, while also suggesting the existence of RKKY-driven muon hyperfine coupling. Zero-field $\\mu^+$SR provides clear evidence for a transition between two distinct magnetically-ordered phases at 39 K.","sentences":["Centrosymmetric GdRu$_2$Si$_2$ exhibits a variety of multi-Q magnetic states as a function of temperature and applied magnetic field, including a square skyrmion-lattice phase.","The material's behavior is strongly dependent on the direction of the applied field, with different phase diagrams resulting for fields applied parallel or perpendicular to the crystallographic $c$ axis.","Here, we present the results of muon-spin relaxation ($\\mu^+$SR) measurements on single crystals of GdRu$_2$Si$_2$. Our analysis is based on the computation of muon stopping sites and consideration of zero-point motion effects, allowing direct comparison with the underlying spin textures in the material.","Using transverse-field $\\mu^+$SR with fields applied along either the [001] or [100] crystallographic directions, we distinguish between the magnetic phases in this system via their distinct muon response, providing additional evidence for the skyrmion and meron-lattice phases, while also suggesting the existence of RKKY-driven muon hyperfine coupling.","Zero-field $\\mu^+$SR provides clear evidence for a transition between two distinct magnetically-ordered phases at 39 K."],"url":"http://arxiv.org/abs/2403.09431v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 14:17:01","title":"Low-coercive-field ferroelectric hafnia with mobile domain walls","abstract":"The high coercive field ($\\mathcal{E}_c$) of hafnia-based ferroelectrics presents a major obstacle to their applications. The ferroelectric switching mechanisms in hafnia that dictate $\\mathcal{E}_c$, especially those related to nucleation-and-growth at the domain wall (DW), have remained elusive. Through deep-learning-assisted multiscale simulations, we determine the finite-temperature thermodynamics and switching mechanisms for diverse types of 180$^\\circ$ DWs, revealing a complex, stress-sensitive mobility landscape. The propagation velocities for mobile DW types under various thermal conditions can be characterized with a single creep equation, featuring a creep exponent of 2. This unconventional critical exponent results from the nucleation of a half-unit-cell-thin, elliptically-shaped critical nucleus. Our multiscale approach not only reproduces the experimental thickness ($d$) scaling, $\\mathcal{E}_c\\propto d^{-\\frac{2}{3}}$, but also predicts that $\\mathcal{E}_c$ of HfO$_2$ can be engineered to $\\approx$0.1 MV/cm, even lower than perovskite ferroelectrics. The theoretical lower bound of $\\mathcal{E}_c$ afforded by ferroelectric hafnia offers opportunities to realize power-efficient, high-fidelity ferroelectric nanoelectronics.","sentences":["The high coercive field ($\\mathcal{E}_c$) of hafnia-based ferroelectrics presents a major obstacle to their applications.","The ferroelectric switching mechanisms in hafnia that dictate $\\mathcal{E}_c$, especially those related to nucleation-and-growth at the domain wall (DW), have remained elusive.","Through deep-learning-assisted multiscale simulations, we determine the finite-temperature thermodynamics and switching mechanisms for diverse types of 180$^\\circ$ DWs, revealing a complex, stress-sensitive mobility landscape.","The propagation velocities for mobile DW types under various thermal conditions can be characterized with a single creep equation, featuring a creep exponent of 2.","This unconventional critical exponent results from the nucleation of a half-unit-cell-thin, elliptically-shaped critical nucleus.","Our multiscale approach not only reproduces the experimental thickness ($d$) scaling, $\\mathcal{E}_c\\propto d^{-\\frac{2}{3}}$, but also predicts that $\\mathcal{E}_c$ of HfO$_2$ can be engineered to $\\approx$0.1 MV/cm, even lower than perovskite ferroelectrics.","The theoretical lower bound of $\\mathcal{E}_c$ afforded by ferroelectric hafnia offers opportunities to realize power-efficient, high-fidelity ferroelectric nanoelectronics."],"url":"http://arxiv.org/abs/2403.09426v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 14:15:14","title":"Inelastic neutron scattering and muon spin relaxation investigations of the deuterated Kondo lattices CeNiSnD$ _x $","abstract":"CeNiSn is a Kondo semimetal where a gap opens at low temperatures due to hybridization between 4$f$ and conduction electrons, but a full insulating state fails to develop. Upon the insertion of hydrogen, long range magnetic order is induced. Here we report zero-field muon-spin relaxation and inelastic neutron scattering measurements of polycrystalline samples of the deuterides CeNiSnD$_x$ ($x$=1.0, 1.8). The muon-spin relaxation results confirm magnetic ordering in the whole sample of CeNiSnD below around 4.7 K, while inelastic neutron scattering reveals two well-defined crystalline-electric field (CEF) excitations at around 13 meV and 34 meV in CeNiSnD, and 5 meV and 27 meV for CeNiSnD$_{1.8}$. These results suggest that hydrogenation leads to the localization of the Ce-4$f$ electrons, giving rise to long-range magnetic order. We propose CEF level schemes for both systems, which predict a ground state moment of 0.96$\\mu_{\\rm B}$/Ce within the $ab$-plane for CeNiSnD$_{1.8}$ and a saturated moment of 1.26$\\mu_{\\rm B}$/Ce along the easy $c$ axis for CeNiSnD, that account for the observed magnetic properties.","sentences":["CeNiSn is a Kondo semimetal where a gap opens at low temperatures due to hybridization between 4$f$ and conduction electrons, but a full insulating state fails to develop.","Upon the insertion of hydrogen, long range magnetic order is induced.","Here we report zero-field muon-spin relaxation and inelastic neutron scattering measurements of polycrystalline samples of the deuterides CeNiSnD$_x$ ($x$=1.0, 1.8).","The muon-spin relaxation results confirm magnetic ordering in the whole sample of CeNiSnD below around 4.7 K, while inelastic neutron scattering reveals two well-defined crystalline-electric field (CEF) excitations at around 13 meV and 34 meV in CeNiSnD, and 5 meV and 27 meV for CeNiSnD$_{1.8}$.","These results suggest that hydrogenation leads to the localization of the Ce-4$f$ electrons, giving rise to long-range magnetic order.","We propose CEF level schemes for both systems, which predict a ground state moment of 0.96$\\mu_{\\rm B}$/Ce within the $ab$-plane for CeNiSnD$_{1.8}$ and a saturated moment of 1.26$\\mu_{\\rm B}$/Ce along the easy $c$ axis for CeNiSnD, that account for the observed magnetic properties."],"url":"http://arxiv.org/abs/2403.09424v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 14:08:59","title":"RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban Scenes","abstract":"The task of separating dynamic objects from static environments using NeRFs has been widely studied in recent years. However, capturing large-scale scenes still poses a challenge due to their complex geometric structures and unconstrained dynamics. Without the help of 3D motion cues, previous methods often require simplified setups with slow camera motion and only a few/single dynamic actors, leading to suboptimal solutions in most urban setups. To overcome such limitations, we present RoDUS, a pipeline for decomposing static and dynamic elements in urban scenes, with thoughtfully separated NeRF models for moving and non-moving components. Our approach utilizes a robust kernel-based initialization coupled with 4D semantic information to selectively guide the learning process. This strategy enables accurate capturing of the dynamics in the scene, resulting in reduced artifacts caused by NeRF on background reconstruction, all by using self-supervision. Notably, experimental evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of our method in decomposing challenging urban scenes into precise static and dynamic components.","sentences":["The task of separating dynamic objects from static environments using NeRFs has been widely studied in recent years.","However, capturing large-scale scenes still poses a challenge due to their complex geometric structures and unconstrained dynamics.","Without the help of 3D motion cues, previous methods often require simplified setups with slow camera motion and only a few/single dynamic actors, leading to suboptimal solutions in most urban setups.","To overcome such limitations, we present RoDUS, a pipeline for decomposing static and dynamic elements in urban scenes, with thoughtfully separated NeRF models for moving and non-moving components.","Our approach utilizes a robust kernel-based initialization coupled with 4D semantic information to selectively guide the learning process.","This strategy enables accurate capturing of the dynamics in the scene, resulting in reduced artifacts caused by NeRF on background reconstruction, all by using self-supervision.","Notably, experimental evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of our method in decomposing challenging urban scenes into precise static and dynamic components."],"url":"http://arxiv.org/abs/2403.09419v1","category":"cs.CV"}
{"created":"2024-03-14 14:01:19","title":"Binomial sums and Mellin asymptotics with explicit error bounds: a case study","abstract":"Making use of a newly developed package in the computer algebra system SageMath, we show how to perform a full asymptotic analysis by means of the Mellin transform with explicit error bounds. As an application of the method, we answer a question of B\\'ona and DeJonge on 132-avoiding permutations with a unique longest increasing subsequence that can be translated into an inequality for a certain binomial sum.","sentences":["Making use of a newly developed package in the computer algebra system SageMath, we show how to perform a full asymptotic analysis by means of the Mellin transform with explicit error bounds.","As an application of the method, we answer a question of B\\'ona and DeJonge on 132-avoiding permutations with a unique longest increasing subsequence that can be translated into an inequality for a certain binomial sum."],"url":"http://arxiv.org/abs/2403.09408v1","category":"math.CO"}
{"created":"2024-03-14 13:56:44","title":"Difference of solutions for the inversion problem of ultra-elliptic integrals","abstract":"Let $V$ be a hyperelliptic curve of genus 2 defined by $Y^2=f(X)$, where $f(X)$ is a polynomial of degree 5. The sigma function associated with $V$ is a holomorphic function on $\\mathbb{C}^2$. For a point $P$ on $V$, we consider the problem to express the $X$-coordinate of $P$ in terms of the image of $P$ under the Abel-Jacobi map. Two meromorphic functions $f_2$ and $g_2$ on $\\mathbb{C}^2$ which give solutions of this problem are known. Since $f_2$ and $g_2$ coincide on the zero set of the sigma function, it is expected that $f_2-g_2$ can be divided by the sigma function. In this paper, we decompose $f_2-g_2$ into a product of the sigma function and a meromorphic function explicitly.","sentences":["Let $V$ be a hyperelliptic curve of genus 2 defined by $Y^2=f(X)$, where $f(X)$ is a polynomial of degree 5.","The sigma function associated with $V$ is a holomorphic function on $\\mathbb{C}^2$. For a point $P$ on $V$, we consider the problem to express the $X$-coordinate of $P$ in terms of the image of $P$ under the Abel-Jacobi map.","Two meromorphic functions $f_2$ and $g_2$ on $\\mathbb{C}^2$ which give solutions of this problem are known.","Since $f_2$ and $g_2$ coincide on the zero set of the sigma function, it is expected that $f_2-g_2$ can be divided by the sigma function.","In this paper, we decompose $f_2-g_2$ into a product of the sigma function and a meromorphic function explicitly."],"url":"http://arxiv.org/abs/2403.09406v1","category":"math.CV"}
{"created":"2024-03-14 13:52:48","title":"Comparative Microscopic Study of Entropies and their Production","abstract":"We study the time evolution of eleven microscopic entropy definitions (of Boltzmann-surface, Gibbs-volume, canonical, coarse-grained-observational, entanglement and diagonal type) and three microscopic temperature definitions (based on Boltzmann, Gibbs or canonical entropy). This is done for the archetypal nonequilibrium setup of two systems exchanging energy, modeled here with random matrix theory, based on numerical integration of the Schroedinger equation. We consider three types of pure initial states (local energy eigenstates, decorrelated and entangled microcanonical states) and three classes of systems: (A) two normal systems, (B) a normal and a negative temperature system and (C) a normal and a negative heat capacity system.   We find: (1) All types of initial states give rise to the same macroscopic dynamics. (2) Entanglement and diagonal entropy sensitively depend on the microstate, in contrast to all other entropies. (3) For class B and C, Gibbs-volume entropies can violate the second law and the associated temperature becomes meaningless. (4) For class C, Boltzmann-surface entropies can violate the second law and the associated temperature becomes meaningless. (5) Canonical entropy has a tendency to remain almost constant. (6) For a Haar random initial state, entanglement or diagonal entropy behave similar or identical to coarse-grained-observational entropy.","sentences":["We study the time evolution of eleven microscopic entropy definitions (of Boltzmann-surface, Gibbs-volume, canonical, coarse-grained-observational, entanglement and diagonal type) and three microscopic temperature definitions (based on Boltzmann, Gibbs or canonical entropy).","This is done for the archetypal nonequilibrium setup of two systems exchanging energy, modeled here with random matrix theory, based on numerical integration of the Schroedinger equation.","We consider three types of pure initial states (local energy eigenstates, decorrelated and entangled microcanonical states) and three classes of systems: (A) two normal systems, (B) a normal and a negative temperature system and (C) a normal and a negative heat capacity system.   ","We find: (1) All types of initial states give rise to the same macroscopic dynamics.","(2) Entanglement and diagonal entropy sensitively depend on the microstate, in contrast to all other entropies.","(3) For class B and C, Gibbs-volume entropies can violate the second law and the associated temperature becomes meaningless.","(4) For class C, Boltzmann-surface entropies can violate the second law and the associated temperature becomes meaningless.","(5) Canonical entropy has a tendency to remain almost constant.","(6) For a Haar random initial state, entanglement or diagonal entropy behave similar or identical to coarse-grained-observational entropy."],"url":"http://arxiv.org/abs/2403.09403v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 13:52:41","title":"An Extensible Framework for Architecture-Based Data Flow Analysis for Information Security","abstract":"The growing interconnection between software systems increases the need for security already at design time. Security-related properties like confidentiality are often analyzed based on data flow diagrams (DFDs). However, manually analyzing DFDs of large software systems is bothersome and error-prone, and adjusting an already deployed software is costly. Additionally, closed analysis ecosystems limit the reuse of modeled information and impede comprehensive statements about a system's security. In this paper, we present an open and extensible framework for data flow analysis. The central element of our framework is our new implementation of a well-validated data-flow-based analysis approach. The framework is compatible with DFDs and can also extract data flows from the Palladio architectural description language. We showcase the extensibility with multiple model and analysis extensions. Our evaluation indicates that we can analyze similar scenarios while achieving higher scalability compared to previous implementations.","sentences":["The growing interconnection between software systems increases the need for security already at design time.","Security-related properties like confidentiality are often analyzed based on data flow diagrams (DFDs).","However, manually analyzing DFDs of large software systems is bothersome and error-prone, and adjusting an already deployed software is costly.","Additionally, closed analysis ecosystems limit the reuse of modeled information and impede comprehensive statements about a system's security.","In this paper, we present an open and extensible framework for data flow analysis.","The central element of our framework is our new implementation of a well-validated data-flow-based analysis approach.","The framework is compatible with DFDs and can also extract data flows from the Palladio architectural description language.","We showcase the extensibility with multiple model and analysis extensions.","Our evaluation indicates that we can analyze similar scenarios while achieving higher scalability compared to previous implementations."],"url":"http://arxiv.org/abs/2403.09402v1","category":"cs.SE"}
{"created":"2024-03-14 13:49:26","title":"A device for studying elementary plasticity fluctuations in granular media","abstract":"In this manuscript, we describe a scientific device specifically designed for the study of the plasticity fluctuations preceding the fracture of granular media. Biaxial tests on model granular media are performed using a commercial uniaxial loading system. Strain field fluctuations are measured using a method based on the interference of coherent light scattered by the sample. We show that such a device enables discrete plasticity events to be unambiguously evidenced. Moreover, those discrete plasticity fluctuations depend only on the imposed strain, and not on the strain rate.","sentences":["In this manuscript, we describe a scientific device specifically designed for the study of the plasticity fluctuations preceding the fracture of granular media.","Biaxial tests on model granular media are performed using a commercial uniaxial loading system.","Strain field fluctuations are measured using a method based on the interference of coherent light scattered by the sample.","We show that such a device enables discrete plasticity events to be unambiguously evidenced.","Moreover, those discrete plasticity fluctuations depend only on the imposed strain, and not on the strain rate."],"url":"http://arxiv.org/abs/2403.09396v1","category":"cond-mat.soft"}
{"created":"2024-03-14 13:40:26","title":"Learning to optimize with convergence guarantees using nonlinear system theory","abstract":"The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensuring convergence by design while learning to optimize.","sentences":["The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes.","Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones.","The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms.","In this paper, we fill this gap by harnessing nonlinear system theory.","Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions.","Notably, our framework is directly compatible with automatic differentiation tools, ensuring convergence by design while learning to optimize."],"url":"http://arxiv.org/abs/2403.09389v1","category":"eess.SY"}
{"created":"2024-03-14 13:38:54","title":"High-energy Neutrinos from Outflows Powered by Kicked Remnants of Binary Black Hole Mergers in AGN Accretion Disks","abstract":"Merging of stellar-mass binary black holes (BBH) could take place within the accretion disk of active galactic nuclei (AGN). The resulting BH remnant is likely to accrete the disk gas at a super-Eddington rate, launching a fast, quasi-spherical outflow (wind). Particles will be accelerated by shocks driven by the wind, subsequently interacting with the shocked disk gas or radiation field through hadronic processes and resulting in the production of high-energy neutrinos and potential electromagnetic (EM) emissions. This study delves into the intricate evolution of the shock driven by the remnant BH wind within AGN disks. Subsequently, we calculated the production of neutrinos and the expected detection numbers for a single event, along with their contributions to the overall diffuse neutrino background. Our analysis, considering various scenarios, reveals considerable neutrino production and possible detection by IceCube for nearby events. The contribution of the remnant BH winds on the diffuse neutrino background is minor due to the low event rate density, but it can be improved to some extent for some optimistic parameters. We also propose that there could be two neutrino/EM bursts, one originating from the premerger BBH wind and the other from the remnant BH wind, with the latter typically having a time gap to the GW event of around tens of days. When combined with the anticipated gravitational waves (GW) emitted during the BBH merger, such a system emerges as a promising candidate for joint observations involving neutrinos, GWs, and EM signals.","sentences":["Merging of stellar-mass binary black holes (BBH) could take place within the accretion disk of active galactic nuclei (AGN).","The resulting BH remnant is likely to accrete the disk gas at a super-Eddington rate, launching a fast, quasi-spherical outflow (wind).","Particles will be accelerated by shocks driven by the wind, subsequently interacting with the shocked disk gas or radiation field through hadronic processes and resulting in the production of high-energy neutrinos and potential electromagnetic (EM) emissions.","This study delves into the intricate evolution of the shock driven by the remnant BH wind within AGN disks.","Subsequently, we calculated the production of neutrinos and the expected detection numbers for a single event, along with their contributions to the overall diffuse neutrino background.","Our analysis, considering various scenarios, reveals considerable neutrino production and possible detection by IceCube for nearby events.","The contribution of the remnant BH winds on the diffuse neutrino background is minor due to the low event rate density, but it can be improved to some extent for some optimistic parameters.","We also propose that there could be two neutrino/EM bursts, one originating from the premerger BBH wind and the other from the remnant BH wind, with the latter typically having a time gap to the GW event of around tens of days.","When combined with the anticipated gravitational waves (GW) emitted during the BBH merger, such a system emerges as a promising candidate for joint observations involving neutrinos, GWs, and EM signals."],"url":"http://arxiv.org/abs/2403.09387v1","category":"astro-ph.HE"}
{"created":"2024-03-14 13:36:01","title":"Exploring the Interplay of Intrinsic Fluctuation and Complexity in Intracellular Calcium Dynamics","abstract":"The concentration of intracellular calcium ion (Ca$^{2+}$) exhibits complex oscillations, including bursting and chaos, as observed experimentally. These dynamics are influenced by inherent fluctuations within cells, which serve as crucial determinants in cellular decision-making processes and fate determination. In this study, we systematically explore the interplay between intrinsic fluctuation and the complexity of intracellular cytosolic Ca$^{2+}$ dynamics using complexity measures such as permutation entropy (PE) and statistical complexity (SC). Using the chemical Langevin equation, we simulate the stochastic dynamics of cytosolic Ca$^{2+}$. Our findings reveal that PE and SC effectively characterize the diverse, dynamic states of cytosolic Ca$^{2+}$ and illustrate their interactions with intrinsic fluctuation. PE analysis elucidates that the chaotic state is more sensitive to intrinsic fluctuation than the other periodic states. Furthermore, we identify distinct states of cytosolic Ca$^{2+}$ occupying specific locations within the theoretical bounds of the complexity-entropy causality plane. These locations indicate varying complexity and information content as intrinsic fluctuation varies. When adjusting the permutation order, the SC for the different states exhibits peaks in an intermediate range of intrinsic fluctuation values. Additionally, we identify scale-free or self-similar patterns in this intermediate range, which are further corroborated by multifractal detrended fluctuation analysis. These high-complexity states likely correspond to optimal Ca$^{2+}$ dynamics with biological significance, revealing rich and complex dynamics shaped by the interplay of intrinsic fluctuation and complexity. Our investigation enhances our understanding of how intrinsic fluctuation modulates the complexity of intracellular Ca$^{2+}$ dynamics that play crucial roles in biological cells.","sentences":["The concentration of intracellular calcium ion (Ca$^{2+}$) exhibits complex oscillations, including bursting and chaos, as observed experimentally.","These dynamics are influenced by inherent fluctuations within cells, which serve as crucial determinants in cellular decision-making processes and fate determination.","In this study, we systematically explore the interplay between intrinsic fluctuation and the complexity of intracellular cytosolic Ca$^{2+}$ dynamics using complexity measures such as permutation entropy (PE) and statistical complexity (SC).","Using the chemical Langevin equation, we simulate the stochastic dynamics of cytosolic Ca$^{2+}$.","Our findings reveal that PE and SC effectively characterize the diverse, dynamic states of cytosolic Ca$^{2+}$ and illustrate their interactions with intrinsic fluctuation.","PE analysis elucidates that the chaotic state is more sensitive to intrinsic fluctuation than the other periodic states.","Furthermore, we identify distinct states of cytosolic Ca$^{2+}$ occupying specific locations within the theoretical bounds of the complexity-entropy causality plane.","These locations indicate varying complexity and information content as intrinsic fluctuation varies.","When adjusting the permutation order, the SC for the different states exhibits peaks in an intermediate range of intrinsic fluctuation values.","Additionally, we identify scale-free or self-similar patterns in this intermediate range, which are further corroborated by multifractal detrended fluctuation analysis.","These high-complexity states likely correspond to optimal Ca$^{2+}$ dynamics with biological significance, revealing rich and complex dynamics shaped by the interplay of intrinsic fluctuation and complexity.","Our investigation enhances our understanding of how intrinsic fluctuation modulates the complexity of intracellular Ca$^{2+}$ dynamics that play crucial roles in biological cells."],"url":"http://arxiv.org/abs/2403.09386v1","category":"nlin.AO"}
{"created":"2024-03-14 13:34:53","title":"Anomalous thermal transport and high thermoelectric performance of Cu-based vanadate CuVO3","abstract":"Thermoelectric (TE) conversion technology, capable of transforming heat into electricity, is critical for sustainable energy solutions. Many promising TE materials contain rare or toxic elements, so the development of cost-effective and eco-friendly high-performance TE materials is highly urgent. Herein, we explore the thermal transport and TE properties of transition metal vanadate CuVO3 by using first-principles calculation. On the basis of unified theory of heat conduction, we uncover the hierarchical thermal transport feature in CuVO3, where wave-like tunneling makes a significant contribution to the lattice thermal conductivity (\\k{appa}l) and result in the anomalously weak temperature dependence of \\k{appa}l. This is primarily attributable to the complex phononic band structure caused by the heterogeneity of Cu-O and V-O bonds. Simultaneously, we report a high power factor of 5.45 mW K-2 m-1 realized in hole-doped CuVO3, which arises from a high electrical conductivity and a large Seebeck coefficient enabled by the multiple valleys and large electronic density of states near the valence band edge. Impressively, the low \\k{appa}l and the high power factor make p-typed CuVO3 have ZT of up to 1.39, with the excellent average ZT above 1.0 from 300 to 600 K, which is superior to most reported Cu-based TE materials. Our findings suggest that CuVO3 compound is promising candidate for energy conversion applications in innovative TE devices.","sentences":["Thermoelectric (TE) conversion technology, capable of transforming heat into electricity, is critical for sustainable energy solutions.","Many promising TE materials contain rare or toxic elements, so the development of cost-effective and eco-friendly high-performance TE materials is highly urgent.","Herein, we explore the thermal transport and TE properties of transition metal vanadate CuVO3 by using first-principles calculation.","On the basis of unified theory of heat conduction, we uncover the hierarchical thermal transport feature in CuVO3, where wave-like tunneling makes a significant contribution to the lattice thermal conductivity (\\k{appa}l) and result in the anomalously weak temperature dependence of \\k{appa}l.","This is primarily attributable to the complex phononic band structure caused by the heterogeneity of Cu-O and V-O bonds.","Simultaneously, we report a high power factor of 5.45 mW K-2 m-1 realized in hole-doped CuVO3, which arises from a high electrical conductivity and a large Seebeck coefficient enabled by the multiple valleys and large electronic density of states near the valence band edge.","Impressively, the low \\k{appa}l and the high power factor make p-typed CuVO3 have ZT of up to 1.39, with the excellent average ZT above 1.0 from 300 to 600 K, which is superior to most reported Cu-based TE materials.","Our findings suggest that CuVO3 compound is promising candidate for energy conversion applications in innovative TE devices."],"url":"http://arxiv.org/abs/2403.09384v1","category":"physics.comp-ph"}
{"created":"2024-03-14 13:34:30","title":"Pantypes: Diverse Representatives for Self-Explainable Models","abstract":"Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions. Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness. In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and fairness.","sentences":["Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems.","These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects.","While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions.","Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness.","In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects.","We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and fairness."],"url":"http://arxiv.org/abs/2403.09383v1","category":"stat.ML"}
{"created":"2024-03-14 13:31:56","title":"Impact of Synthetic Images on Morphing Attack Detection Using a Siamese Network","abstract":"This paper evaluated the impact of synthetic images on Morphing Attack Detection (MAD) using a Siamese network with a semi-hard-loss function. Intra and cross-dataset evaluations were performed to measure synthetic image generalisation capabilities using a cross-dataset for evaluation. Three different pre-trained networks were used as feature extractors from traditional MobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained on EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in comparison with SOTA. Conversely, worse performances were reached when the system was trained only with synthetic images. A mixed approach (synthetic + digital) database may help to improve MAD and reduce the error rate. This fact shows that we still need to keep going with our efforts to include synthetic images in the training process.","sentences":["This paper evaluated the impact of synthetic images on Morphing Attack Detection (MAD) using a Siamese network with a semi-hard-loss function.","Intra and cross-dataset evaluations were performed to measure synthetic image generalisation capabilities using a cross-dataset for evaluation.","Three different pre-trained networks were used as feature extractors from traditional MobileNetV2, MobileNetV3 and EfficientNetB0.","Our results show that MAD trained on EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in comparison with SOTA.","Conversely, worse performances were reached when the system was trained only with synthetic images.","A mixed approach (synthetic + digital) database may help to improve MAD and reduce the error rate.","This fact shows that we still need to keep going with our efforts to include synthetic images in the training process."],"url":"http://arxiv.org/abs/2403.09380v1","category":"cs.CV"}
{"created":"2024-03-14 13:21:50","title":"Solvability of the Inverse Optimal Control problem based on the minimum principle","abstract":"In this paper, the solvability of the Inverse Optimal Control (IOC) problem based on two existing minimum principal methods, is analysed. The aim of this work is to answer the question regarding what kinds of trajectories, that is depending on the initial conditions of the closed-loop system and system dynamics, of the original optimal control problem, will result in the recovery of the true weights of the reward function for both the soft and the hard-constrained methods [1], [2]. Analytical conditions are provided which allow to verify if a trajectory is sufficiently conditioned, that is, holds sufficient information to recover the true weights of an optimal control problem. It was found that the open-loop system of the original optimal problem has a stronger influence on the solvability of the Inverse Optimal Control problem for the hard-constrained method as compared to the soft-constrained method. These analytical results were validated via simulation.","sentences":["In this paper, the solvability of the Inverse Optimal Control (IOC) problem based on two existing minimum principal methods, is analysed.","The aim of this work is to answer the question regarding what kinds of trajectories, that is depending on the initial conditions of the closed-loop system and system dynamics, of the original optimal control problem, will result in the recovery of the true weights of the reward function for both the soft and the hard-constrained methods [1], [2].","Analytical conditions are provided which allow to verify if a trajectory is sufficiently conditioned, that is, holds sufficient information to recover the true weights of an optimal control problem.","It was found that the open-loop system of the original optimal problem has a stronger influence on the solvability of the Inverse Optimal Control problem for the hard-constrained method as compared to the soft-constrained method.","These analytical results were validated via simulation."],"url":"http://arxiv.org/abs/2403.09375v1","category":"math.OC"}
{"created":"2024-03-14 13:19:48","title":"On function spaces for radial functions","abstract":"This paper is concerned with complex Banach-space valued functions of the form $$ \\hat{f}_k(r\\cos\\theta,r\\sin\\theta,z)=\\mathrm{e}^{\\mathrm{i} k \\theta}f_k(r,z), \\qquad r \\in [0,\\infty), \\theta \\in \\mathbb{T}^1, z \\in \\mathbb{R}, $$ for some $k \\in \\mathbb{Z}$. It is demonstrated how classical and Sobolev spaces for the radial function $f_k$ can be constructed in a natural fashion from the corresponding standard function spaces for $\\hat{f}_k$. A theory of radial distributions is derived in the same spirit. Finally, a new class of \\textit{Hankel spaces} for the case $f_k=f_k(r)$ is introduced. These spaces are the radial counterparts of the familiar Bessel-potential spaces for functions defined on $\\mathbb{R}^d$. The paper concludes with an application of the theory to the Dirichlet boundary-value problem for Poisson's equation in a cylindrical domain.","sentences":["This paper is concerned with complex Banach-space valued functions of the form $$ \\hat{f}_k(r\\cos\\theta,r\\sin\\theta,z)=\\mathrm{e}^{\\mathrm{i} k \\theta}f_k(r,z), \\qquad r \\in [0,\\infty), \\theta \\in \\mathbb{T}^1, z \\in \\mathbb{R}, $$ for some $k \\in \\mathbb{Z}$. It is demonstrated how classical and Sobolev spaces for the radial function $f_k$ can be constructed in a natural fashion from the corresponding standard function spaces for $\\hat{f}_k$. A theory of radial distributions is derived in the same spirit.","Finally, a new class of \\textit{Hankel spaces} for the case $f_k=f_k(r)$ is introduced.","These spaces are the radial counterparts of the familiar Bessel-potential spaces for functions defined on $\\mathbb{R}^d$. The paper concludes with an application of the theory to the Dirichlet boundary-value problem for Poisson's equation in a cylindrical domain."],"url":"http://arxiv.org/abs/2403.09372v1","category":"math.FA"}
{"created":"2024-03-14 13:04:40","title":"Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer","abstract":"Integrated data and energy transfer (IDET) has been of fundamental importance for providing both wireless data transfer (WDT) and wireless energy transfer (WET) services towards low-power devices. Fluid antenna (FA) is capable of exploiting the huge spatial diversity of the wireless channel to enhance the receive signal strength, which is more suitable for the tiny-size low-power devices having the IDET requirements. In this letter, a multiuser FA assisted IDET system is studied and the weighted energy harvesting power at energy receivers (ERs) is maximized by jointly optimizing the port selection and transmit beamforming design under imperfect channel state information (CSI), while the signal-to-interference-plus-noise ratio (SINR) constraint for each data receiver (DR) is satisfied. An efficient algorithm is proposed to obtain the suboptimal solutions for the non-convex problem. Simulation results evaluate the performance of the FA-IDET system, while also demonstrate that FA outperforms the multi-input-multi-output (MIMO) counterpart in terms of the IDET performance, as long as the port number is large enough.","sentences":["Integrated data and energy transfer (IDET) has been of fundamental importance for providing both wireless data transfer (WDT) and wireless energy transfer (WET) services towards low-power devices.","Fluid antenna (FA) is capable of exploiting the huge spatial diversity of the wireless channel to enhance the receive signal strength, which is more suitable for the tiny-size low-power devices having the IDET requirements.","In this letter, a multiuser FA assisted IDET system is studied and the weighted energy harvesting power at energy receivers (ERs) is maximized by jointly optimizing the port selection and transmit beamforming design under imperfect channel state information (CSI), while the signal-to-interference-plus-noise ratio (SINR) constraint for each data receiver (DR) is satisfied.","An efficient algorithm is proposed to obtain the suboptimal solutions for the non-convex problem.","Simulation results evaluate the performance of the FA-IDET system, while also demonstrate that FA outperforms the multi-input-multi-output (MIMO) counterpart in terms of the IDET performance, as long as the port number is large enough."],"url":"http://arxiv.org/abs/2403.09357v1","category":"cs.IT"}
{"created":"2024-03-14 12:57:59","title":"REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography","abstract":"Significant research efforts have been dedicated to designing cryptographic algorithms that are quantum-resistant. The motivation is clear: robust quantum computers, once available, will render current cryptographic standards vulnerable. Thus, we need new Post-Quantum Cryptography (PQC) algorithms, and, due to the inherent complexity of such algorithms, there is also a demand to accelerate them in hardware. In this paper, we show that PQC hardware accelerators can be backdoored by two different adversaries located in the chip supply chain. We propose REPQC, a sophisticated reverse engineering algorithm that can be employed to confidently identify hashing operations (i.e., Keccak) within the PQC accelerator - the location of which serves as an anchor for finding secret information to be leaked. Armed with REPQC, an adversary proceeds to insert malicious logic in the form of a stealthy Hardware Trojan Horse (HTH). Using Dilithium as a study case, our results demonstrate that HTHs that increase the accelerator's layout density by as little as 0.1\\% can be inserted without any impact on the performance of the circuit and with a marginal increase in power consumption. An essential aspect is that the entire reverse engineering in REPQC is automated, and so is the HTH insertion that follows it, empowering adversaries to explore multiple HTH designs and identify the most suitable one.","sentences":["Significant research efforts have been dedicated to designing cryptographic algorithms that are quantum-resistant.","The motivation is clear: robust quantum computers, once available, will render current cryptographic standards vulnerable.","Thus, we need new Post-Quantum Cryptography (PQC) algorithms, and, due to the inherent complexity of such algorithms, there is also a demand to accelerate them in hardware.","In this paper, we show that PQC hardware accelerators can be backdoored by two different adversaries located in the chip supply chain.","We propose REPQC, a sophisticated reverse engineering algorithm that can be employed to confidently identify hashing operations (i.e., Keccak) within the PQC accelerator - the location of which serves as an anchor for finding secret information to be leaked.","Armed with REPQC, an adversary proceeds to insert malicious logic in the form of a stealthy Hardware Trojan Horse (HTH).","Using Dilithium as a study case, our results demonstrate that HTHs that increase the accelerator's layout density by as little as 0.1\\% can be inserted without any impact on the performance of the circuit and with a marginal increase in power consumption.","An essential aspect is that the entire reverse engineering in REPQC is automated, and so is the HTH insertion that follows it, empowering adversaries to explore multiple HTH designs and identify the most suitable one."],"url":"http://arxiv.org/abs/2403.09352v1","category":"cs.CR"}
{"created":"2024-03-14 12:52:30","title":"Pinched theorem and the reverse Yau's inequalities for compact K\u00e4hler-Einstein manifolds","abstract":"For a compact K\\\"{a}hler-Einstein manifold $M$ of dimension $n\\ge 2$, we explicitly write the expression $-c_1^n(M)+\\frac{2(n+1)}{n}c_2(M)c_1^{n-2}(M)$ in the form of certain integral on the holomorphic sectional curvature and its average at a fixed point in $M$ using the invariant theory. As applications, we get a reverse Yau's inequality and improve the classical $\\frac{1}{4}$-pinched theorem and negative $\\frac{1}{4}$-pinched theorem for compact K\\\"{a}hler-Einstein manifolds to smaller pinching constant depending only on the dimension and the first Chern class of $M$. If $M$ is not with positive or negative holomorphic sectional curvature, then there exists a point $x\\in M$ such that the average of the holomorphic sectional curvature at $x$ vanishes. In particular, we characterise the $2$-dimensional complex torus by certain curvature condition. Moreover, we confirm Yau's conjecture for positive holomorphic sectional curvature and Siu-Yang's conjecture for negative holomorphic sectional curvature even for higher dimensions if the absolute value of the holomorphic sectional curvature is small enough. Finally, using the reverse Yau's inequality, we can judge if a projective manifold doesn't carry any hermitian metric with negative holomorphic sectional curvature.","sentences":["For a compact K\\\"{a}hler-Einstein manifold $M$ of dimension $n\\ge 2$, we explicitly write the expression $-c_1^n(M)+\\frac{2(n+1)}{n}c_2(M)c_1^{n-2}(M)$ in the form of certain integral on the holomorphic sectional curvature and its average at a fixed point in $M$ using the invariant theory.","As applications, we get a reverse Yau's inequality and improve the classical $\\frac{1}{4}$-pinched theorem and negative $\\frac{1}{4}$-pinched theorem for compact K\\\"{a}hler-Einstein manifolds to smaller pinching constant depending only on the dimension and the first Chern class of $M$. If $M$ is not with positive or negative holomorphic sectional curvature, then there exists a point $x\\in M$ such that the average of the holomorphic sectional curvature at $x$ vanishes.","In particular, we characterise the $2$-dimensional complex torus by certain curvature condition.","Moreover, we confirm Yau's conjecture for positive holomorphic sectional curvature and Siu-Yang's conjecture for negative holomorphic sectional curvature even for higher dimensions if the absolute value of the holomorphic sectional curvature is small enough.","Finally, using the reverse Yau's inequality, we can judge if a projective manifold doesn't carry any hermitian metric with negative holomorphic sectional curvature."],"url":"http://arxiv.org/abs/2403.09348v1","category":"math.DG"}
{"created":"2024-03-14 12:51:58","title":"BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences","abstract":"Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence processing. The experimental results under different length settings demonstrate that BurstAttention offers significant advantages for processing long sequences compared with these competitive baselines, reducing 40% communication overheads and achieving 2 X speedup during training 32K sequence length on 8 X A100.","sentences":["Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences.","One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs).","However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones.","In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels.","In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence processing.","The experimental results under different length settings demonstrate that BurstAttention offers significant advantages for processing long sequences compared with these competitive baselines, reducing 40% communication overheads and achieving 2 X speedup during training 32K sequence length on 8 X A100."],"url":"http://arxiv.org/abs/2403.09347v1","category":"cs.DC"}
{"created":"2024-03-14 12:49:49","title":"Classical-Quantum correspondence in Lindblad evolution","abstract":"We show that for the Lindblad evolution defined using (at most) quadratically growing classical Hamiltonians and (at most) linearly growing classical jump functions (quantized into jump operators assumed to satisfy certain ellipticity conditions and modeling interaction with a larger system), the evolution of a quantum observable remains close to the classical Fokker--Planck evolution in the Hilbert--Schmidt norm for times vastly exceeding the Ehrenfest time (the limit of such agreement with no jump operators). The time scale is the same as two recent papers by Hern\\'andez--Ranard--Riedel but the statement and methods are different.","sentences":["We show that for the Lindblad evolution defined using (at most) quadratically growing classical Hamiltonians and (at most) linearly growing classical jump functions (quantized into jump operators assumed to satisfy certain ellipticity conditions and modeling interaction with a larger system), the evolution of a quantum observable remains close to the classical Fokker--Planck evolution in the Hilbert--Schmidt norm for times vastly exceeding the Ehrenfest time (the limit of such agreement with no jump operators).","The time scale is the same as two recent papers by Hern\\'andez--Ranard--Riedel but the statement and methods are different."],"url":"http://arxiv.org/abs/2403.09345v1","category":"math-ph"}
{"created":"2024-03-14 12:40:50","title":"Influence of Dimensionality of Carbon-based Additives on Thermoelectric Transport Parameters in Polymer Electrolytes","abstract":"This paper investigates the thermoelectric properties of solid polymer electrolytes (SPE) containing lithium bis(trifluoromethanesulfonyl)imide (LiTFSI) and sodium bis(trifluoromethanesulfonyl)imide (NaTFSI) salts, along with carbon-based additives of various dimensionalities. Increasing salt concentration leads to higher Seebeck coefficients as a result of the increasing number of free charge carriers and additional, superimposed effects by ion-ion and ion-polymer interactions. NaTFSI-based electrolytes exhibit negative Seebeck coefficients (up to $S = -1.5\\,\\mathrm{mV\\,K^{-1}}$), indicating dominant mobility of $\\mathrm{TFSI^-}$ ions. Quasi-one-dimensional carbon nanotubes (CNTs) increase the Seebeck coefficient by a factor of 3. Planar, two-dimensional graphite flakes (GF) moderately enhance it, affecting $\\mathrm{Na^+}$ and $\\mathrm{TFSI^-}$ ion mobilities and electronic conductivity. Bulky, three-dimensional carbon black (CB) additives induce a unique behavior where the sign of the Seebeck coefficient changes with temperature, presumably due to interaction with $\\mathrm{TFSI^-}$ ions within the CB structure. Changes in activation energy and Vogel temperature with salt concentration suggest structural and mechanical modifications in the polymer matrix. The choice of carbon-based additives and salt concentration significantly influences the thermoelectric properties of SPEs thermoelectric properties, providing insights into their potential for thermoelectric applications. Sodium-based electrolytes emerge as promising, sustainable alternatives to lithium-based systems, aligning with sustainable energy research demands.","sentences":["This paper investigates the thermoelectric properties of solid polymer electrolytes (SPE) containing lithium bis(trifluoromethanesulfonyl)imide (LiTFSI) and sodium bis(trifluoromethanesulfonyl)imide (NaTFSI) salts, along with carbon-based additives of various dimensionalities.","Increasing salt concentration leads to higher Seebeck coefficients as a result of the increasing number of free charge carriers and additional, superimposed effects by ion-ion and ion-polymer interactions.","NaTFSI-based electrolytes exhibit negative Seebeck coefficients (up to $S = -1.5\\,\\mathrm{mV\\,K^{-1}}$), indicating dominant mobility of $\\mathrm{TFSI^-}$ ions.","Quasi-one-dimensional carbon nanotubes (CNTs) increase the Seebeck coefficient by a factor of 3.","Planar, two-dimensional graphite flakes (GF) moderately enhance it, affecting $\\mathrm{Na^+}$ and $\\mathrm{TFSI^-}$ ion mobilities and electronic conductivity.","Bulky, three-dimensional carbon black (CB) additives induce a unique behavior where the sign of the Seebeck coefficient changes with temperature, presumably due to interaction with $\\mathrm{TFSI^-}$ ions within the CB structure.","Changes in activation energy and Vogel temperature with salt concentration suggest structural and mechanical modifications in the polymer matrix.","The choice of carbon-based additives and salt concentration significantly influences the thermoelectric properties of SPEs thermoelectric properties, providing insights into their potential for thermoelectric applications.","Sodium-based electrolytes emerge as promising, sustainable alternatives to lithium-based systems, aligning with sustainable energy research demands."],"url":"http://arxiv.org/abs/2403.09340v1","category":"physics.app-ph"}
{"created":"2024-03-14 12:32:50","title":"Field test of mode-pairing quantum key distribution","abstract":"Quantum key distribution is a cornerstone of quantum technology, offering information-theoretical secure keys for remote parties. With many quantum communication networks established globally, the mode-pairing protocol stands out for its efficacy over inter-city distances using simple setups, emerging as a promising solution. In this study, we employ the mode-pairing scheme into existing inter-city fiber links, conducting field tests across distances ranging from tens to about a hundred kilometers. Our system achieves a key rate of $1.217$ kbit/s in a $195.85$ km symmetric link and $3.089$ kbit/s in a $127.92$ km asymmetric link without global phase locking. The results demonstrate that the mode-pairing protocol can achieve key rates comparable to those of a single quantum link between two trusted nodes on the Beijing-Shanghai backbone line, effectively reducing the need for half of the trusted nodes. These field tests confirm the mode-pairing scheme's adaptability, efficiency, and practicality, positioning it as a highly suitable protocol for quantum networks.","sentences":["Quantum key distribution is a cornerstone of quantum technology, offering information-theoretical secure keys for remote parties.","With many quantum communication networks established globally, the mode-pairing protocol stands out for its efficacy over inter-city distances using simple setups, emerging as a promising solution.","In this study, we employ the mode-pairing scheme into existing inter-city fiber links, conducting field tests across distances ranging from tens to about a hundred kilometers.","Our system achieves a key rate of $1.217$ kbit/s in a $195.85$ km symmetric link and $3.089$ kbit/s in a $127.92$ km asymmetric link without global phase locking.","The results demonstrate that the mode-pairing protocol can achieve key rates comparable to those of a single quantum link between two trusted nodes on the Beijing-Shanghai backbone line, effectively reducing the need for half of the trusted nodes.","These field tests confirm the mode-pairing scheme's adaptability, efficiency, and practicality, positioning it as a highly suitable protocol for quantum networks."],"url":"http://arxiv.org/abs/2403.09339v1","category":"quant-ph"}
{"created":"2024-03-14 12:20:59","title":"A secular solar system resonance that disrupts the dominant cycle in Earth's orbital eccentricity (g2-g5): Implications for astrochronology","abstract":"The planets' gravitational interaction causes rhythmic changes in Earth's orbital parameters (also called Milankovi\\'c cycles), which have powerful applications in geology and astrochronology. For instance, the primary astronomical eccentricity cycle due to the secular frequency term (g2-g5) (~405 kyr in the recent past) utilized in deep-time analyses is dominated by Venus' and Jupiter's orbits, aka long eccentricity cycle. The widely accepted and long-held view is that (g2-g5) was practically stable in the past and may hence be used as a \"metronome\" to reconstruct accurate ages and chronologies. However, using state-of-the-art integrations of the solar system, we show here that (g2-g5) can become unstable over long time scales, without major changes in, or destabilization of, planetary orbits. The (g2-g5) disruption is due to the secular resonance $\\sigma_{12}$ = (g1 - g2) + (s1 - s2), a major contributor to solar system chaos. We demonstrate that entering/exiting the $\\sigma_{12}$ resonance is a common phenomenon on long time scales, occurring in ~40% of our solutions. During $\\sigma_{12}$-resonance episodes, (g2-g5) is very weak or absent and Earth's orbital eccentricity and climate-forcing spectrum are unrecognizable compared to the recent past. Our results have fundamental implications for geology and astrochronology, as well as climate forcing because the paradigm that the longest Milankovi\\'c cycle dominates Earth's astronomical forcing, is stable, and has a period of ~405 kyr requires revision.","sentences":["The planets' gravitational interaction causes rhythmic changes in Earth's orbital parameters (also called Milankovi\\'c cycles), which have powerful applications in geology and astrochronology.","For instance, the primary astronomical eccentricity cycle due to the secular frequency term (g2-g5) (~405 kyr in the recent past) utilized in deep-time analyses is dominated by Venus' and Jupiter's orbits, aka long eccentricity cycle.","The widely accepted and long-held view is that (g2-g5) was practically stable in the past and may hence be used as a \"metronome\" to reconstruct accurate ages and chronologies.","However, using state-of-the-art integrations of the solar system, we show here that (g2-g5) can become unstable over long time scales, without major changes in, or destabilization of, planetary orbits.","The (g2-g5) disruption is due to the secular resonance $\\sigma_{12}$ = (g1 - g2)","+","(s1 - s2), a major contributor to solar system chaos.","We demonstrate that entering/exiting the $\\sigma_{12}$ resonance is a common phenomenon on long time scales, occurring in ~40% of our solutions.","During $\\sigma_{12}$-resonance episodes, (g2-g5) is very weak or absent and Earth's orbital eccentricity and climate-forcing spectrum are unrecognizable compared to the recent past.","Our results have fundamental implications for geology and astrochronology, as well as climate forcing because the paradigm that the longest Milankovi\\'c cycle dominates Earth's astronomical forcing, is stable, and has a period of ~405 kyr requires revision."],"url":"http://arxiv.org/abs/2403.09332v1","category":"astro-ph.EP"}
{"created":"2024-03-14 12:17:07","title":"Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening","abstract":"Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning. Deep learning has seen great success in overcoming the limitations of traditional methods. However, these inverse problems rarely come with ground truth data, highlighting the importance of unsupervised learning from partial and noisy measurements alone. We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems. This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other unsupervised methods in the literature. Code at https://andrewwango.github.io/perspective-equivariant-imaging","sentences":["Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning.","Deep learning has seen great success in overcoming the limitations of traditional methods.","However, these inverse problems rarely come with ground truth data, highlighting the importance of unsupervised learning from partial and noisy measurements alone.","We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems.","This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other unsupervised methods in the literature.","Code at https://andrewwango.github.io/perspective-equivariant-imaging"],"url":"http://arxiv.org/abs/2403.09327v1","category":"cs.CV"}
{"created":"2024-03-14 12:00:37","title":"Large deviations of one-hidden-layer neural networks","abstract":"We study large deviations in the context of stochastic gradient descent for one-hidden-layer neural networks with quadratic loss. We derive a quenched large deviation principle, where we condition on an initial weight measure, and an annealed large deviation principle for the empirical weight evolution during training when letting the number of neurons and the number of training iterations simultaneously tend to infinity. The weight evolution is treated as an interacting dynamic particle system. The distinctive aspect compared to prior work on interacting particle systems lies in the discrete particle updates, simultaneously with a growing number of particles.","sentences":["We study large deviations in the context of stochastic gradient descent for one-hidden-layer neural networks with quadratic loss.","We derive a quenched large deviation principle, where we condition on an initial weight measure, and an annealed large deviation principle for the empirical weight evolution during training when letting the number of neurons and the number of training iterations simultaneously tend to infinity.","The weight evolution is treated as an interacting dynamic particle system.","The distinctive aspect compared to prior work on interacting particle systems lies in the discrete particle updates, simultaneously with a growing number of particles."],"url":"http://arxiv.org/abs/2403.09310v1","category":"math.PR"}
{"created":"2024-03-14 11:56:13","title":"Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback","abstract":"For mobile robots, navigating cluttered or dynamic environments often necessitates non-prehensile manipulation, particularly when faced with objects that are too large, irregular, or fragile to grasp. The unpredictable behavior and varying physical properties of these objects significantly complicate manipulation tasks. To address this challenge, this manuscript proposes a novel Reactive Pushing Strategy. This strategy allows a mobile robot to dynamically adjust its base movements in real-time to achieve successful pushing maneuvers towards a target location. Notably, our strategy adapts the robot motion based on changes in contact location obtained through the tactile sensor covering the base, avoiding dependence on object-related assumptions and its modeled behavior. The effectiveness of the Reactive Pushing Strategy was initially evaluated in the simulation environment, where it significantly outperformed the compared baseline approaches. Following this, we validated the proposed strategy through real-world experiments, demonstrating the robot capability to push objects to the target points located in the entire vicinity of the robot. In both simulation and real-world experiments, the object-specific properties (shape, mass, friction, inertia) were altered along with the changes in target locations to assess the robustness of the proposed method comprehensively.","sentences":["For mobile robots, navigating cluttered or dynamic environments often necessitates non-prehensile manipulation, particularly when faced with objects that are too large, irregular, or fragile to grasp.","The unpredictable behavior and varying physical properties of these objects significantly complicate manipulation tasks.","To address this challenge, this manuscript proposes a novel Reactive Pushing Strategy.","This strategy allows a mobile robot to dynamically adjust its base movements in real-time to achieve successful pushing maneuvers towards a target location.","Notably, our strategy adapts the robot motion based on changes in contact location obtained through the tactile sensor covering the base, avoiding dependence on object-related assumptions and its modeled behavior.","The effectiveness of the Reactive Pushing Strategy was initially evaluated in the simulation environment, where it significantly outperformed the compared baseline approaches.","Following this, we validated the proposed strategy through real-world experiments, demonstrating the robot capability to push objects to the target points located in the entire vicinity of the robot.","In both simulation and real-world experiments, the object-specific properties (shape, mass, friction, inertia) were altered along with the changes in target locations to assess the robustness of the proposed method comprehensively."],"url":"http://arxiv.org/abs/2403.09305v1","category":"cs.RO"}
{"created":"2024-03-14 11:54:25","title":"The Existential Closedness and Zilber-Pink Conjectures","abstract":"In this paper we survey the history of, and recent developments on, two major conjectures originating in Zilber's model-theoretic work on complex exponentiation -- Existential Closedness and Zilber-Pink. The main focus is on the modular versions of these conjectures and specifically on novel variants incorporating the derivatives of modular functions. The functional analogues of all the conjectures that we consider are theorems which are presented too. The paper also contains some new results and conjectures.","sentences":["In this paper we survey the history of, and recent developments on, two major conjectures originating in Zilber's model-theoretic work on complex exponentiation -- Existential Closedness and Zilber-Pink.","The main focus is on the modular versions of these conjectures and specifically on novel variants incorporating the derivatives of modular functions.","The functional analogues of all the conjectures that we consider are theorems which are presented too.","The paper also contains some new results and conjectures."],"url":"http://arxiv.org/abs/2403.09304v1","category":"math.LO"}
{"created":"2024-03-14 11:46:25","title":"Recursive Causal Discovery","abstract":"Causal discovery, i.e., learning the causal graph from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests. The worst-case performances of these methods nearly match the lower bound. In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation. A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency. Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms. This package is designed for practitioners and researchers interested in applying these methods in practical scenarios. The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com.","sentences":["Causal discovery, i.e., learning the causal graph from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains.","Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting.","This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a).","These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery.","Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively.","This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests.","The worst-case performances of these methods nearly match the lower bound.","In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation.","A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency.","Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms.","This package is designed for practitioners and researchers interested in applying these methods in practical scenarios.","The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com."],"url":"http://arxiv.org/abs/2403.09300v1","category":"cs.LG"}
{"created":"2024-03-14 11:37:02","title":"More than words: Advancements and challenges in speech recognition for singing","abstract":"This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions.","sentences":["This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition.","Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference.","We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription.","I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field.","My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions."],"url":"http://arxiv.org/abs/2403.09298v1","category":"cs.SD"}
{"created":"2024-03-14 11:03:22","title":"Whittle Index Based User Association in Dense Millimeter Wave Networks","abstract":"We address the problem of user association in a dense millimeter wave (mmWave) network, in which each arriving user brings a file containing a random number of packets and each time slot is divided into multiple mini-slots. This problem is an instance of the restless multi-armed bandit problem, and is provably hard to solve. Using a technique introduced by Whittle, we relax the hard per-stage constraint that each arriving user must be associated with exactly one mmWave base station (mBS) to a long-term constraint and then use the Lagrangian multiplier technique to convert the problem into an unconstrained problem. This decouples the process governing the system into separate Markov Decision Processes at different mBSs. We prove that the problem is Whittle indexable, present a scheme for computing the Whittle indices of different mBSs, and propose an association scheme under which, each arriving user is associated with the mBS with the smallest value of the Whittle index. Using extensive simulations, we show that the proposed Whittle index based scheme outperforms several user association schemes proposed in prior work in terms of various performance metrics such as average cost, delay, throughput, and Jain's fairness index.","sentences":["We address the problem of user association in a dense millimeter wave (mmWave) network, in which each arriving user brings a file containing a random number of packets and each time slot is divided into multiple mini-slots.","This problem is an instance of the restless multi-armed bandit problem, and is provably hard to solve.","Using a technique introduced by Whittle, we relax the hard per-stage constraint that each arriving user must be associated with exactly one mmWave base station (mBS) to a long-term constraint and then use the Lagrangian multiplier technique to convert the problem into an unconstrained problem.","This decouples the process governing the system into separate Markov Decision Processes at different mBSs.","We prove that the problem is Whittle indexable, present a scheme for computing the Whittle indices of different mBSs, and propose an association scheme under which, each arriving user is associated with the mBS with the smallest value of the Whittle index.","Using extensive simulations, we show that the proposed Whittle index based scheme outperforms several user association schemes proposed in prior work in terms of various performance metrics such as average cost, delay, throughput, and Jain's fairness index."],"url":"http://arxiv.org/abs/2403.09279v1","category":"cs.NI"}
{"created":"2024-03-14 10:57:27","title":"Rational Gluing in Edge Replacement Systems","abstract":"In this paper, we prove the rationality of the gluing relation of edge replacement systems, which were introduced for studying rearrangement groups of fractals. More precisely, we describe an algorithmic procedure for building a finite state automaton that recognizes pairs or equivalent sequences that are glued in the fractal. This fits in recent interest towards the rationality of gluing relations on totally disconnected compact metrizable spaces.","sentences":["In this paper, we prove the rationality of the gluing relation of edge replacement systems, which were introduced for studying rearrangement groups of fractals.","More precisely, we describe an algorithmic procedure for building a finite state automaton that recognizes pairs or equivalent sequences that are glued in the fractal.","This fits in recent interest towards the rationality of gluing relations on totally disconnected compact metrizable spaces."],"url":"http://arxiv.org/abs/2403.09276v1","category":"math.GR"}
{"created":"2024-03-14 10:38:34","title":"A noncommutative maximal inequality for Fej\u00e9r means on totally disconnected non-abelian groups","abstract":"In this paper, we explore Fourier analysis for noncommutative $L_p$ space-valued functions on $G$, where $G$ is a totally disconnected non-abelian compact group. By additionally assuming that the value of these functions remains invariant within each conjugacy class, we establish a noncommutative maximal inequality for Fej\\'er means utilizing the associated character system of $G$. This is an operator-valued version of the classical result due to G\\'at. We follow essentially the classical sketch, but due to the noncommutativity, many classical arguments have to be revised. Notably, compared to the classical results. the bounds of our estimates are explicity calculated.","sentences":["In this paper, we explore Fourier analysis for noncommutative $L_p$ space-valued functions on $G$, where $G$ is a totally disconnected non-abelian compact group.","By additionally assuming that the value of these functions remains invariant within each conjugacy class, we establish a noncommutative maximal inequality for Fej\\'er means utilizing the associated character system of $G$.","This is an operator-valued version of the classical result due to G\\'at.","We follow essentially the classical sketch, but due to the noncommutativity, many classical arguments have to be revised.","Notably, compared to the classical results.","the bounds of our estimates are explicity calculated."],"url":"http://arxiv.org/abs/2403.09263v1","category":"math.FA"}
{"created":"2024-03-14 10:30:58","title":"Near-Field EM-Based Multistatic Radar Range Estimation","abstract":"Radar targets are traditionally modelled as point target reflectors, even in the near-field region. Yet, for radar systems operating at high carrier frequencies and small distances, traditional radar propagation models do not accurately model the scatterer responses. In this paper, a novel electromagnetic-based model is thus developed for the multistatic radar detection of a rectangular plate reflector in the near-field region. This model is applied to an automotive scenario, in which a linear antenna array is spread out at the front of a vehicle, and performs a radar measurement of the distance to the back of the vehicle ahead. Based on the developed received signal model, the maximum likelihood estimator of the range is designed. By exploiting the near-field target model, this estimator is shown to provide a significant gain with respect to traditional range estimators. The impact of the system and scenario parameters, i.e. the carrier frequency, bandwidth and distance to the target, is furthermore evaluated. This analysis shows that the radar resolution in the near-field regime is improved at high carrier frequencies, while saturating to the traditional bandwidth-dependent resolution in the far-field region.","sentences":["Radar targets are traditionally modelled as point target reflectors, even in the near-field region.","Yet, for radar systems operating at high carrier frequencies and small distances, traditional radar propagation models do not accurately model the scatterer responses.","In this paper, a novel electromagnetic-based model is thus developed for the multistatic radar detection of a rectangular plate reflector in the near-field region.","This model is applied to an automotive scenario, in which a linear antenna array is spread out at the front of a vehicle, and performs a radar measurement of the distance to the back of the vehicle ahead.","Based on the developed received signal model, the maximum likelihood estimator of the range is designed.","By exploiting the near-field target model, this estimator is shown to provide a significant gain with respect to traditional range estimators.","The impact of the system and scenario parameters, i.e. the carrier frequency, bandwidth and distance to the target, is furthermore evaluated.","This analysis shows that the radar resolution in the near-field regime is improved at high carrier frequencies, while saturating to the traditional bandwidth-dependent resolution in the far-field region."],"url":"http://arxiv.org/abs/2403.09258v1","category":"eess.SP"}
{"created":"2024-03-14 10:27:38","title":"A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning","abstract":"Robot-assisted surgery has advantages compared to conventional laparoscopic procedures, e.g., precise movement of the surgical instruments, improved dexterity, and high-resolution visualization of the surgical field. However, mechanical tissue properties may provide additional information, e.g., on the location of lesions or vessels. While elastographic imaging has been proposed, it is not readily available as an online modality during robot-assisted surgery. We propose modifying a da~Vinci surgical instrument to realize optical coherence elastography (OCE) for quantitative elasticity estimation. The modified da~Vinci instrument is equipped with piezoelectric elements for shear wave excitation and we employ fast optical coherence tomography (OCT) imaging to track propagating wave fields, which are directly related to biomechanical tissue properties. All high-voltage components are mounted at the proximal end outside the patient. We demonstrate that external excitation at the instrument shaft can effectively stimulate shear waves, even when considering damping. Comparing conventional and deep learning-based signal processing, resulting in mean absolute errors of 19.27 kPa and 6.29 kPa, respectively. These results illustrate that precise quantitative elasticity estimates can be obtained. We also demonstrate quantitative elasticity estimation on ex-vivo tissue samples of heart, liver and stomach, and show that the measurements can be used to distinguish soft and stiff tissue types.","sentences":["Robot-assisted surgery has advantages compared to conventional laparoscopic procedures, e.g., precise movement of the surgical instruments, improved dexterity, and high-resolution visualization of the surgical field.","However, mechanical tissue properties may provide additional information, e.g., on the location of lesions or vessels.","While elastographic imaging has been proposed, it is not readily available as an online modality during robot-assisted surgery.","We propose modifying a da~Vinci surgical instrument to realize optical coherence elastography (OCE) for quantitative elasticity estimation.","The modified da~Vinci instrument is equipped with piezoelectric elements for shear wave excitation and we employ fast optical coherence tomography (OCT) imaging to track propagating wave fields, which are directly related to biomechanical tissue properties.","All high-voltage components are mounted at the proximal end outside the patient.","We demonstrate that external excitation at the instrument shaft can effectively stimulate shear waves, even when considering damping.","Comparing conventional and deep learning-based signal processing, resulting in mean absolute errors of 19.27 kPa and 6.29 kPa, respectively.","These results illustrate that precise quantitative elasticity estimates can be obtained.","We also demonstrate quantitative elasticity estimation on ex-vivo tissue samples of heart, liver and stomach, and show that the measurements can be used to distinguish soft and stiff tissue types."],"url":"http://arxiv.org/abs/2403.09256v1","category":"eess.IV"}
{"created":"2024-03-14 10:27:14","title":"Quantum analog of Landau-Lifshitz-Gilbert dynamics","abstract":"The Landau-Lifshitz-Gilbert (LLG) and Landau-Lifshitz (LL) equations play an essential role for describing the dynamics of magnetization in solids. While a quantum analog of the LL dynamics has been proposed in [Phys. Rev. Lett. 110, 147201 (2013)], the corresponding quantum version of LLG remains unknown. Here, we propose such a quantum LLG equation that inherently conserves purity of the quantum state. We examine the quantum LLG dynamics of a dimer consisting of two interacting spin-1/2 particles. Our analysis reveals that, in the case of ferromagnetic coupling, the evolution of initially uncorrelated spins mirrors the classical LLG dynamics. However, in the antiferromagnetic scenario, we observe pronounced deviations from classical behavior, underscoring the unique dynamics of becoming a spinless state, which is non-locally correlated. Moreover, when considering spins that are initially correlated, our study uncovers an unusual form of transient quantum correlation dynamics, which differ significantly from what is typically seen in open quantum systems.","sentences":["The Landau-Lifshitz-Gilbert (LLG) and Landau-Lifshitz (LL) equations play an essential role for describing the dynamics of magnetization in solids.","While a quantum analog of the LL dynamics has been proposed in [Phys. Rev. Lett.","110, 147201 (2013)], the corresponding quantum version of LLG remains unknown.","Here, we propose such a quantum LLG equation that inherently conserves purity of the quantum state.","We examine the quantum LLG dynamics of a dimer consisting of two interacting spin-1/2 particles.","Our analysis reveals that, in the case of ferromagnetic coupling, the evolution of initially uncorrelated spins mirrors the classical LLG dynamics.","However, in the antiferromagnetic scenario, we observe pronounced deviations from classical behavior, underscoring the unique dynamics of becoming a spinless state, which is non-locally correlated.","Moreover, when considering spins that are initially correlated, our study uncovers an unusual form of transient quantum correlation dynamics, which differ significantly from what is typically seen in open quantum systems."],"url":"http://arxiv.org/abs/2403.09255v1","category":"quant-ph"}
{"created":"2024-03-14 10:10:54","title":"Experimental study of dynamic wetting behavior through curved microchannels with automated image analysis","abstract":"Preventing fluid penetration poses a challenging reliability concern in the context of power electronics, which is usually caused by unforeseen microfractures along the sealing joints. A better and more reliable product design heavily depends on the understanding of the dynamic wetting processes happening inside these complex microfractures, i.e. microchannels. A novel automated image processing procedure is proposed in this work for analyzing the moving interface and the dynamic contact angle in microchannels. In particular, the developed method is advantageous for experiments involving non-transparent samples, where extracting the fluid interface geometry poses a significant challenge. The developed method is validated with theoretical values and manual measurements and exhibits high accuracy. The implementation is made publicly available. The developed method is validated and applied to experimental investigations of forced wetting with two working fluids (water and 50 wt% glycerin/water mixture) in four distinct microchannels characterized by different dimensions and curvature. The comparison between the experimental results and molecular kinetic theory (MKT) reveals that the dynamic wetting behavior can be described well by MKT, even in highly curved microchannels. The dynamic wetting behavior shows a strong dependency on the channel geometry and curvature.","sentences":["Preventing fluid penetration poses a challenging reliability concern in the context of power electronics, which is usually caused by unforeseen microfractures along the sealing joints.","A better and more reliable product design heavily depends on the understanding of the dynamic wetting processes happening inside these complex microfractures, i.e. microchannels.","A novel automated image processing procedure is proposed in this work for analyzing the moving interface and the dynamic contact angle in microchannels.","In particular, the developed method is advantageous for experiments involving non-transparent samples, where extracting the fluid interface geometry poses a significant challenge.","The developed method is validated with theoretical values and manual measurements and exhibits high accuracy.","The implementation is made publicly available.","The developed method is validated and applied to experimental investigations of forced wetting with two working fluids (water and 50 wt% glycerin/water mixture) in four distinct microchannels characterized by different dimensions and curvature.","The comparison between the experimental results and molecular kinetic theory (MKT) reveals that the dynamic wetting behavior can be described well by MKT, even in highly curved microchannels.","The dynamic wetting behavior shows a strong dependency on the channel geometry and curvature."],"url":"http://arxiv.org/abs/2403.09246v1","category":"physics.flu-dyn"}
{"created":"2024-03-14 09:57:15","title":"D-YOLO a robust framework for object detection in adverse weather conditions","abstract":"Adverse weather conditions including haze, snow and rain lead to decline in image qualities, which often causes a decline in performance for deep-learning based detection networks. Most existing approaches attempts to rectify hazy images before performing object detection, which increases the complexity of the network and may result in the loss in latent information. To better integrate image restoration and object detection tasks, we designed a double-route network with an attention feature fusion module, taking both hazy and dehazed features into consideration. We also proposed a subnetwork to provide haze-free features to the detection network. Specifically, our D-YOLO improves the performance of the detection network by minimizing the distance between the clear feature extraction subnetwork and detection network. Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates better performance compared to the state-of-the-art methods. It is a robust detection framework for bridging the gap between low-level dehazing and high-level detection.","sentences":["Adverse weather conditions including haze, snow and rain lead to decline in image qualities, which often causes a decline in performance for deep-learning based detection networks.","Most existing approaches attempts to rectify hazy images before performing object detection, which increases the complexity of the network and may result in the loss in latent information.","To better integrate image restoration and object detection tasks, we designed a double-route network with an attention feature fusion module, taking both hazy and dehazed features into consideration.","We also proposed a subnetwork to provide haze-free features to the detection network.","Specifically, our D-YOLO improves the performance of the detection network by minimizing the distance between the clear feature extraction subnetwork and detection network.","Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates better performance compared to the state-of-the-art methods.","It is a robust detection framework for bridging the gap between low-level dehazing and high-level detection."],"url":"http://arxiv.org/abs/2403.09233v1","category":"cs.CV"}
{"created":"2024-03-14 09:39:18","title":"A Robust Semantic Communication System for Image","abstract":"Semantic communications have gained significant attention as a promising approach to address the transmission bottleneck, especially with the continuous development of 6G techniques. Distinct from the well investigated physical channel impairments, this paper focuses on semantic impairments in image, particularly those arising from adversarial perturbations. Specifically, we propose a novel metric for quantifying the intensity of semantic impairment and develop a semantic impairment dataset. Furthermore, we introduce a deep learning enabled semantic communication system, termed as DeepSC-RI, to enhance the robustness of image transmission, which incorporates a multi-scale semantic extractor with a dual-branch architecture for extracting semantics with varying granularity, thereby improving the robustness of the system. The fine-grained branch incorporates a semantic importance evaluation module to identify and prioritize crucial semantics, while the coarse-grained branch adopts a hierarchical approach for capturing the robust semantics. These two streams of semantics are seamlessly integrated via an advanced cross-attention-based semantic fusion module. Experimental results demonstrate the superior performance of DeepSC-RI under various levels of semantic impairment intensity.","sentences":["Semantic communications have gained significant attention as a promising approach to address the transmission bottleneck, especially with the continuous development of 6G techniques.","Distinct from the well investigated physical channel impairments, this paper focuses on semantic impairments in image, particularly those arising from adversarial perturbations.","Specifically, we propose a novel metric for quantifying the intensity of semantic impairment and develop a semantic impairment dataset.","Furthermore, we introduce a deep learning enabled semantic communication system, termed as DeepSC-RI, to enhance the robustness of image transmission, which incorporates a multi-scale semantic extractor with a dual-branch architecture for extracting semantics with varying granularity, thereby improving the robustness of the system.","The fine-grained branch incorporates a semantic importance evaluation module to identify and prioritize crucial semantics, while the coarse-grained branch adopts a hierarchical approach for capturing the robust semantics.","These two streams of semantics are seamlessly integrated via an advanced cross-attention-based semantic fusion module.","Experimental results demonstrate the superior performance of DeepSC-RI under various levels of semantic impairment intensity."],"url":"http://arxiv.org/abs/2403.09222v1","category":"eess.SP"}
{"created":"2024-03-14 09:22:30","title":"MINDS: The JWST MIRI Mid-INfrared Disk Survey","abstract":"The study of protoplanetary disks has become increasingly important with the Kepler satellite finding that exoplanets are ubiquitous around stars in our galaxy and the discovery of enormous diversity in planetary system architectures and planet properties. High-resolution near-IR and ALMA images show strong evidence for ongoing planet formation in young disks. The JWST MIRI mid-INfrared Disk Survey (MINDS) aims to (1) investigate the chemical inventory in the terrestrial planet-forming zone across stellar spectral type, (2) follow the gas evolution into the disk dispersal stage, and (3) study the structure of protoplanetary and debris disks in the thermal mid-IR. The MINDS survey will thus build a bridge between the chemical inventory of disks and the properties of exoplanets. The survey comprises 52 targets (Herbig Ae stars, T Tauri stars, very low-mass stars and young debris disks). We primarily obtain MIRI/MRS spectra with high S/N (~100-500) covering the complete wavelength range from 4.9 to 27.9 {\\mu}m. For a handful of selected targets we also obtain NIRSpec IFU high resolution spectroscopy (2.87-5.27 {\\mu}m). We will search for signposts of planet formation in thermal emission of micron-sized dust - information complementary to near-IR scattered light emission from small dust grains and emission from large dust in the submillimeter wavelength domain. We will also study the spatial structure of disks in three key systems that have shown signposts for planet formation, TW Hya and HD 169142 using the MIRI coronagraph at 15.5 {\\mu}m and 10.65 {\\mu}m respectively and PDS70 using NIRCam imaging in the 1.87 {\\mu}m narrow and the 4.8 {\\mu}m medium band filter. ...","sentences":["The study of protoplanetary disks has become increasingly important with the Kepler satellite finding that exoplanets are ubiquitous around stars in our galaxy and the discovery of enormous diversity in planetary system architectures and planet properties.","High-resolution near-IR and ALMA images show strong evidence for ongoing planet formation in young disks.","The JWST MIRI mid-INfrared Disk Survey (MINDS) aims to (1) investigate the chemical inventory in the terrestrial planet-forming zone across stellar spectral type, (2) follow the gas evolution into the disk dispersal stage, and (3) study the structure of protoplanetary and debris disks in the thermal mid-IR.","The MINDS survey will thus build a bridge between the chemical inventory of disks and the properties of exoplanets.","The survey comprises 52 targets (Herbig Ae stars, T Tauri stars, very low-mass stars and young debris disks).","We primarily obtain MIRI/MRS spectra with high S/N (~100-500) covering the complete wavelength range from 4.9 to 27.9 {\\mu}m.","For a handful of selected targets we also obtain NIRSpec IFU high resolution spectroscopy (2.87-5.27 {\\mu}m).","We will search for signposts of planet formation in thermal emission of micron-sized dust - information complementary to near-IR scattered light emission from small dust grains and emission from large dust in the submillimeter wavelength domain.","We will also study the spatial structure of disks in three key systems that have shown signposts for planet formation, TW Hya and HD 169142 using the MIRI coronagraph at 15.5 {\\mu}m and 10.65 {\\mu}m respectively and PDS70 using NIRCam imaging in the 1.87 {\\mu}m narrow and the 4.8 {\\mu}m medium band filter. ..."],"url":"http://arxiv.org/abs/2403.09210v1","category":"astro-ph.EP"}
{"created":"2024-03-14 09:22:16","title":"Older adults' safety and security online: A post-pandemic exploration of attitudes and behaviors","abstract":"Older adults' growing use of the internet and related technologies, further accelerated by the COVID-19 pandemic, has prompted not only a critical examination of their behaviors and attitudes about online threats but also a greater understanding of the roles of specific characteristics within this population group. Based on survey data and using descriptive and inferential statistics, this empirical study delves into this matter. The behaviors and attitudes of a group of older adults aged 60 years and older (n=275) regarding different dimensions of online safety and cybersecurity are investigated. The results show that older adults report a discernible degree of concern about the security of their personal information. Despite the varied precautions taken, most of them do not know where to report online threats. What is more, regarding key demographics, the study found some significant differences in terms of gender and age group, but not disability status. This implies that older adults do not seem to constitute a homogeneous group when it comes to attitudes and behaviors regarding safety and security online. The study concludes that support systems should include older adults in the development of protective measures and acknowledge their diversity. The implications of the results are discussed and some directions for future research are proposed.","sentences":["Older adults' growing use of the internet and related technologies, further accelerated by the COVID-19 pandemic, has prompted not only a critical examination of their behaviors and attitudes about online threats but also a greater understanding of the roles of specific characteristics within this population group.","Based on survey data and using descriptive and inferential statistics, this empirical study delves into this matter.","The behaviors and attitudes of a group of older adults aged 60 years and older (n=275) regarding different dimensions of online safety and cybersecurity are investigated.","The results show that older adults report a discernible degree of concern about the security of their personal information.","Despite the varied precautions taken, most of them do not know where to report online threats.","What is more, regarding key demographics, the study found some significant differences in terms of gender and age group, but not disability status.","This implies that older adults do not seem to constitute a homogeneous group when it comes to attitudes and behaviors regarding safety and security online.","The study concludes that support systems should include older adults in the development of protective measures and acknowledge their diversity.","The implications of the results are discussed and some directions for future research are proposed."],"url":"http://arxiv.org/abs/2403.09208v1","category":"cs.CY"}
{"created":"2024-03-14 09:19:35","title":"Microscopic Study on Superexchange Dynamics of Composite Spin-1 Bosons","abstract":"We report on an experimental simulation of the spin-1 Heisenberg model with composite bosons in a one-dimensional chain based on the two-component Bose-Hubbard model. Exploiting our site-and spin-resolved quantum gas microscope, we observed faster superexchange dynamics of the spin-1 system compared to its spin-1/2 counterpart, which is attributed to the enhancement effect of multi-bosons. We further probed the non-equilibrium spin dynamics driven by the superexchange and single-ion anisotropy terms, unveiling the linear expansion of the spin-spin correlations, which is limited by the Lieb-Robinson bound. Based on the superexchange process, we prepared and verified the entangled qutrits pairs with these composite spin-1 bosons, potentially being applied in qutrit-based quantum information processing.","sentences":["We report on an experimental simulation of the spin-1 Heisenberg model with composite bosons in a one-dimensional chain based on the two-component Bose-Hubbard model.","Exploiting our site-and spin-resolved quantum gas microscope, we observed faster superexchange dynamics of the spin-1 system compared to its spin-1/2 counterpart, which is attributed to the enhancement effect of multi-bosons.","We further probed the non-equilibrium spin dynamics driven by the superexchange and single-ion anisotropy terms, unveiling the linear expansion of the spin-spin correlations, which is limited by the Lieb-Robinson bound.","Based on the superexchange process, we prepared and verified the entangled qutrits pairs with these composite spin-1 bosons, potentially being applied in qutrit-based quantum information processing."],"url":"http://arxiv.org/abs/2403.09205v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-14 09:11:23","title":"Sparse Data Structures for Efficient State-to-State Kinetic Simulations","abstract":"Higher-fidelity entry simulations can be enabled by integrating finer thermo-chemistry models into compressible flow physics. One such class of models are State-to-State (StS) kinetics, which explicitly track species populations among quantum energy levels. StS models can represent thermo-chemical non-equilibrium effects that are hardly captured by standard multi-temperature models. However, the associated increase in computational cost is dramatic. For implicit solution techniques that rely on standard block-sparse representations of the Jacobian, both the spatial complexity and the temporal complexity grow quadratically with respect to the number of quantum levels represented. We introduce a more efficient way to represent the Jacobian arising in first-order implicit simulations for compressible flow physics coupled with StS models. The key idea is to recognize that the density of local blocks of the Jacobian comes from rank-one updates that can be managed separately. This leads to a new Jacobian structure, consisting of a fully-sparse matrix and block-wise rank-one updates, whose overall complexity grows linearly with the number of quantum levels. This structure also brings forth a potentially faster variation of the block-Jacobi preconditioning algorithm by leveraging the Sherman-Morrison-Woodbury inversion formula.","sentences":["Higher-fidelity entry simulations can be enabled by integrating finer thermo-chemistry models into compressible flow physics.","One such class of models are State-to-State (StS) kinetics, which explicitly track species populations among quantum energy levels.","StS models can represent thermo-chemical non-equilibrium effects that are hardly captured by standard multi-temperature models.","However, the associated increase in computational cost is dramatic.","For implicit solution techniques that rely on standard block-sparse representations of the Jacobian, both the spatial complexity and the temporal complexity grow quadratically with respect to the number of quantum levels represented.","We introduce a more efficient way to represent the Jacobian arising in first-order implicit simulations for compressible flow physics coupled with StS models.","The key idea is to recognize that the density of local blocks of the Jacobian comes from rank-one updates that can be managed separately.","This leads to a new Jacobian structure, consisting of a fully-sparse matrix and block-wise rank-one updates, whose overall complexity grows linearly with the number of quantum levels.","This structure also brings forth a potentially faster variation of the block-Jacobi preconditioning algorithm by leveraging the Sherman-Morrison-Woodbury inversion formula."],"url":"http://arxiv.org/abs/2403.09198v1","category":"physics.comp-ph"}
{"created":"2024-03-14 09:05:55","title":"Superintegrable systems on conformal surfaces","abstract":"We reconsider non-degenerate second order superintegrable systems in dimension two as geometric structures on conformal surfaces. This extends a formalism developed by the authors, initially introduced for (pseudo-)Riemannian manifolds of dimension three and higher. The governing equations of non-degenerate second order superintegrability in dimension two are structurally significantly different from those valid in higher dimensions. Specifically, we find conformally covariant structural equations, allowing one to classify the (conformal classes of) non-degenerate second order superintegrable systems on conformal surfaces geometrically. We then specialise to second order properly superintegrable systems on surfaces with a (pseudo-)Riemannian metric and obtain structural equations in accordance with the known equations for Euclidean space. We finally give a single explicit set of purely algebraic equations defining the variety parametrising such systems on all constant curvature surfaces.","sentences":["We reconsider non-degenerate second order superintegrable systems in dimension two as geometric structures on conformal surfaces.","This extends a formalism developed by the authors, initially introduced for (pseudo-)Riemannian manifolds of dimension three and higher.","The governing equations of non-degenerate second order superintegrability in dimension two are structurally significantly different from those valid in higher dimensions.","Specifically, we find conformally covariant structural equations, allowing one to classify the (conformal classes of) non-degenerate second order superintegrable systems on conformal surfaces geometrically.","We then specialise to second order properly superintegrable systems on surfaces with a (pseudo-)Riemannian metric and obtain structural equations in accordance with the known equations for Euclidean space.","We finally give a single explicit set of purely algebraic equations defining the variety parametrising such systems on all constant curvature surfaces."],"url":"http://arxiv.org/abs/2403.09191v1","category":"math.DG"}
{"created":"2024-03-14 09:04:30","title":"Dynamics of Droplets Moving on Lubricated Polymer Brushes","abstract":"Understanding the dynamics of drops on polymer-coated surfaces is crucial for optimizing applications such as self-cleaning materials or microfluidic devices. While the static and dynamic properties of deposited drops have been well characterised, a microscopic understanding of the underlying dynamics is missing. In particular, it is unclear how drop dynamics depends on the amount of uncrosslinked chains in the brush, because experimental techniques fail to quantify those. Here we use coarse-grained simulations to study droplets moving on a lubricated polymer brush substrate under the influence of an external body force. The simulation model is based on the many body dissipative particle dynamics (mDPD) method and designed to mimic a system of water droplets on polydimethylsiloxane (PDMS) brushes with chemically identical PDMS lubricant. In agreement with experiments, we find a sublinear power law dependence between the external force $F$ and the droplet velocity $v$, $F \\propto v^\\alpha$ with $\\alpha <1$; however, the exponents differ ($\\alpha \\sim 0.6-0.7$ in simulations versus $\\alpha \\sim 0.25$ in experiments). With increasing velocity, the droplets elongate and the receding contact angle decreases, whereas the advancing contact angle remains roughly constant. Analyzing the flow profiles inside the droplet reveals that the droplets do not slide, but roll, with vanishing slip at the substrate surface. Surprisingly, adding lubricant has very little effect on the effective friction force between the droplet and the substrate, even though it has a pronounced effect on the size and structure of the wetting ridge, especially above the cloaking transition.","sentences":["Understanding the dynamics of drops on polymer-coated surfaces is crucial for optimizing applications such as self-cleaning materials or microfluidic devices.","While the static and dynamic properties of deposited drops have been well characterised, a microscopic understanding of the underlying dynamics is missing.","In particular, it is unclear how drop dynamics depends on the amount of uncrosslinked chains in the brush, because experimental techniques fail to quantify those.","Here we use coarse-grained simulations to study droplets moving on a lubricated polymer brush substrate under the influence of an external body force.","The simulation model is based on the many body dissipative particle dynamics (mDPD) method and designed to mimic a system of water droplets on polydimethylsiloxane (PDMS) brushes with chemically identical PDMS lubricant.","In agreement with experiments, we find a sublinear power law dependence between the external force $F$ and the droplet velocity $v$, $F \\propto v^\\alpha$ with $\\alpha <1$; however, the exponents differ ($\\alpha \\sim 0.6-0.7$ in simulations versus $\\alpha \\sim 0.25$ in experiments).","With increasing velocity, the droplets elongate and the receding contact angle decreases, whereas the advancing contact angle remains roughly constant.","Analyzing the flow profiles inside the droplet reveals that the droplets do not slide, but roll, with vanishing slip at the substrate surface.","Surprisingly, adding lubricant has very little effect on the effective friction force between the droplet and the substrate, even though it has a pronounced effect on the size and structure of the wetting ridge, especially above the cloaking transition."],"url":"http://arxiv.org/abs/2403.09189v1","category":"cond-mat.soft"}
{"created":"2024-03-14 09:03:51","title":"Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study","abstract":"Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data. However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models. A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation. The transformed data is expected to facilitate the gradient calculation and finetuned process in a DL training process. The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS. The BPL layer was placed at the beginning of the DL model. The tunable parameters in the layer were learnable projected axes that were the bases of a new representation space. The layer rotated these bases when its parameters were updated. When the number of the bases was the same as the original dimension, the increasing percentage of the F1 scores was 8.56%. Furthermore, when the number was set as 768 (the original dimension was 490), the increasing percentage of the F1 score was 11.49%. The layer not only maintained the model performance and even constructed a better representation space in analyzing sparse datasets.","sentences":["Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data.","However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence.","These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models.","A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation.","The transformed data is expected to facilitate the gradient calculation and finetuned process in a DL training process.","The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS.","The BPL layer was placed at the beginning of the DL model.","The tunable parameters in the layer were learnable projected axes that were the bases of a new representation space.","The layer rotated these bases when its parameters were updated.","When the number of the bases was the same as the original dimension, the increasing percentage of the F1 scores was 8.56%.","Furthermore, when the number was set as 768 (the original dimension was 490), the increasing percentage of the F1 score was 11.49%.","The layer not only maintained the model performance and even constructed a better representation space in analyzing sparse datasets."],"url":"http://arxiv.org/abs/2403.09188v1","category":"cs.LG"}
{"created":"2024-03-14 08:49:18","title":"On the dynamical Mordell-Lang conjecture in positive characteristic","abstract":"We prove that the dynamical Mordell-Lang conjecture in positive characteristic holds for bounded-degree self-maps of projective varieties. The key ingredient of the proof is a Mordell-Lang-type result for arbitrary algebraic groups over algebraically closed fields of positive characteristic, which is also interesting on its own. Moreover, we propose a geometric version of dynamical Mordell-Lang conjecture in positive characteristic.","sentences":["We prove that the dynamical Mordell-Lang conjecture in positive characteristic holds for bounded-degree self-maps of projective varieties.","The key ingredient of the proof is a Mordell-Lang-type result for arbitrary algebraic groups over algebraically closed fields of positive characteristic, which is also interesting on its own.","Moreover, we propose a geometric version of dynamical Mordell-Lang conjecture in positive characteristic."],"url":"http://arxiv.org/abs/2403.09181v1","category":"math.DS"}
{"created":"2024-03-14 08:47:32","title":"Online and Offline Evaluation in Search Clarification","abstract":"The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement.","sentences":["The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness.","To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation).","However, the relationship between online and offline evaluations has been debated in information retrieval.","This study aims to investigate how this discordance holds in search clarification.","We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement."],"url":"http://arxiv.org/abs/2403.09180v1","category":"cs.IR"}
{"created":"2024-03-14 08:44:43","title":"High-order numerical integration on regular embedded surfaces","abstract":"We present a high-order surface quadrature (HOSQ) for accurately approximating regular surface integrals on closed surfaces. The initial step of our approach rests on exploiting square-squeezing--a homeomorphic bilinear square-simplex transformation, re-parametrizing any surface triangulation to a quadrilateral mesh. For each resulting quadrilateral domain we interpolate the geometry by tensor polynomials in Chebyshev--Lobatto grids. Posterior the tensor-product Clenshaw-Curtis quadrature is applied to compute the resulting integral. We demonstrate efficiency, fast runtime performance, high-order accuracy, and robustness for complex geometries.","sentences":["We present a high-order surface quadrature (HOSQ) for accurately approximating regular surface integrals on closed surfaces.","The initial step of our approach rests on exploiting square-squeezing--a homeomorphic bilinear square-simplex transformation, re-parametrizing any surface triangulation to a quadrilateral mesh.","For each resulting quadrilateral domain we interpolate the geometry by tensor polynomials in Chebyshev--Lobatto grids.","Posterior the tensor-product Clenshaw-Curtis quadrature is applied to compute the resulting integral.","We demonstrate efficiency, fast runtime performance, high-order accuracy, and robustness for complex geometries."],"url":"http://arxiv.org/abs/2403.09178v1","category":"math.NA"}
{"created":"2024-03-14 08:44:15","title":"Cellular-enabled Collaborative Robots Planning and Operations for Search-and-Rescue Scenarios","abstract":"Mission-critical operations, particularly in the context of Search-and-Rescue (SAR) and emergency response situations, demand optimal performance and efficiency from every component involved to maximize the success probability of such operations. In these settings, cellular-enabled collaborative robotic systems have emerged as invaluable assets, assisting first responders in several tasks, ranging from victim localization to hazardous area exploration. However, a critical limitation in the deployment of cellular-enabled collaborative robots in SAR missions is their energy budget, primarily supplied by batteries, which directly impacts their task execution and mobility. This paper tackles this problem, and proposes a search-and-rescue framework for cellular-enabled collaborative robots use cases that, taking as input the area size to be explored, the robots fleet size, their energy profile, exploration rate required and target response time, finds the minimum number of robots able to meet the SAR mission goals and the path they should follow to explore the area. Our results, i) show that first responders can rely on a SAR cellular-enabled robotics framework when planning mission-critical operations to take informed decisions with limited resources, and, ii) illustrate the number of robots versus explored area and response time trade-off depending on the type of robot: wheeled vs quadruped.","sentences":["Mission-critical operations, particularly in the context of Search-and-Rescue (SAR) and emergency response situations, demand optimal performance and efficiency from every component involved to maximize the success probability of such operations.","In these settings, cellular-enabled collaborative robotic systems have emerged as invaluable assets, assisting first responders in several tasks, ranging from victim localization to hazardous area exploration.","However, a critical limitation in the deployment of cellular-enabled collaborative robots in SAR missions is their energy budget, primarily supplied by batteries, which directly impacts their task execution and mobility.","This paper tackles this problem, and proposes a search-and-rescue framework for cellular-enabled collaborative robots use cases that, taking as input the area size to be explored, the robots fleet size, their energy profile, exploration rate required and target response time, finds the minimum number of robots able to meet the SAR mission goals and the path they should follow to explore the area.","Our results, i) show that first responders can rely on a SAR cellular-enabled robotics framework when planning mission-critical operations to take informed decisions with limited resources, and, ii) illustrate the number of robots versus explored area and response time trade-off depending on the type of robot: wheeled vs quadruped."],"url":"http://arxiv.org/abs/2403.09177v1","category":"cs.RO"}
{"created":"2024-03-14 08:40:30","title":"Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy","abstract":"Quantum computing has attracted significant attention in areas such as cryptography, cybersecurity, and drug discovery. Due to the advantage of parallel processing, quantum computing can speed up the response to complex challenges and the processing of large-scale datasets. However, since quantum computing usually requires sensitive datasets, privacy breaches have become a vital concern. Differential privacy (DP) is a promising privacy-preserving method in classical computing and has been extended to the quantum domain in recent years. In this survey, we categorize the existing literature based on whether internal inherent noise or external artificial noise is used as a source to achieve DP in quantum computing. We explore how these approaches are applied at different stages of a quantum algorithm (i.e., state preparation, quantum circuit, and quantum measurement). We also discuss challenges and future directions for DP in quantum computing. By summarizing recent advancements, we hope to provide a comprehensive, up-to-date overview for researchers venturing into this field.","sentences":["Quantum computing has attracted significant attention in areas such as cryptography, cybersecurity, and drug discovery.","Due to the advantage of parallel processing, quantum computing can speed up the response to complex challenges and the processing of large-scale datasets.","However, since quantum computing usually requires sensitive datasets, privacy breaches have become a vital concern.","Differential privacy (DP) is a promising privacy-preserving method in classical computing and has been extended to the quantum domain in recent years.","In this survey, we categorize the existing literature based on whether internal inherent noise or external artificial noise is used as a source to achieve DP in quantum computing.","We explore how these approaches are applied at different stages of a quantum algorithm (i.e., state preparation, quantum circuit, and quantum measurement).","We also discuss challenges and future directions for DP in quantum computing.","By summarizing recent advancements, we hope to provide a comprehensive, up-to-date overview for researchers venturing into this field."],"url":"http://arxiv.org/abs/2403.09173v1","category":"quant-ph"}
{"created":"2024-03-14 08:26:32","title":"Verification of Bell Nonlocality by Violating Quantum Monogamy Relations","abstract":"Quantum nonlocality as a witness of entanglement plays a crucial role in various fields. Existing quantum monogamy relations rule out the possibility of simultaneous violations of any Bell inequalities with partial statistics generated from one Bell experiment on any multipartite entanglement or post-quantum sources. In this paper, we report an efficient method to construct multipartite Bell test based on any Bell inequalities. We demonstrate that violating these monogamy relations can dynamically witness simultaneous Bell nonlocalities of partial systems. We conduct a tripartite experiment to verify quantum nonlocalities by violating a tripartite monogamy relation using a maximally entangled two-photon state.","sentences":["Quantum nonlocality as a witness of entanglement plays a crucial role in various fields.","Existing quantum monogamy relations rule out the possibility of simultaneous violations of any Bell inequalities with partial statistics generated from one Bell experiment on any multipartite entanglement or post-quantum sources.","In this paper, we report an efficient method to construct multipartite Bell test based on any Bell inequalities.","We demonstrate that violating these monogamy relations can dynamically witness simultaneous Bell nonlocalities of partial systems.","We conduct a tripartite experiment to verify quantum nonlocalities by violating a tripartite monogamy relation using a maximally entangled two-photon state."],"url":"http://arxiv.org/abs/2403.09166v1","category":"quant-ph"}
{"created":"2024-03-14 08:19:41","title":"Caveat Lector: Large Language Models in Legal Practice","abstract":"The current fascination with large language models, or LLMs, derives from the fact that many users lack the expertise to evaluate the quality of the generated text. LLMs may therefore appear more capable than they actually are. The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance. Who would not trust perfect legalese? Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of LLMs in legal practice. Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform complex reasoning tasks. Trained to model language on the basis of stochastic word predictions, LLMs cannot distinguish fact from fiction. Their knowledge of the law is limited to word strings memorized in their parameters. It is also incomplete and largely incorrect. LLMs operate at the level of word distributions, not at the level of verified facts. The resulting propensity to hallucinate, to produce statements that are incorrect but appear helpful and relevant, is alarming in high-risk areas like legal services. At present, lawyers should beware of relying on text generated by LLMs.","sentences":["The current fascination with large language models, or LLMs, derives from the fact that many users lack the expertise to evaluate the quality of the generated text.","LLMs may therefore appear more capable than they actually are.","The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance.","Who would not trust perfect legalese?","Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of LLMs in legal practice.","Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks.","Notwithstanding their unprecedented ability to generate text, LLMs do not understand text.","Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform complex reasoning tasks.","Trained to model language on the basis of stochastic word predictions, LLMs cannot distinguish fact from fiction.","Their knowledge of the law is limited to word strings memorized in their parameters.","It is also incomplete and largely incorrect.","LLMs operate at the level of word distributions, not at the level of verified facts.","The resulting propensity to hallucinate, to produce statements that are incorrect but appear helpful and relevant, is alarming in high-risk areas like legal services.","At present, lawyers should beware of relying on text generated by LLMs."],"url":"http://arxiv.org/abs/2403.09163v1","category":"cs.CL"}
{"created":"2024-03-14 08:17:53","title":"Compactness of quantics tensor train representations of local imaginary-time propagators","abstract":"Space-time dependence of imaginary-time propagators, vital for \\textit{ab initio} and many-body calculations based on quantum field theories, has been revealed to be compressible using Quantum Tensor Trains (QTTs) [Phys. Rev. X {\\bf 13}, 021015 (2023)]. However, the impact of system parameters, like temperature, on data size remains underexplored. This paper provides a comprehensive numerical analysis of the compactness of local imaginary-time propagators in QTT for one-time/-frequency objects and two-time/-frequency objects, considering truncation in terms of the Frobenius and maximum norms. We employ random pole models to study worst-case scenarios. The numerical analysis reveals several key findings. For one-time/-frequency objects and two-time/-frequency objects, the bond dimensions saturate at low temperatures, especially for truncation in terms of the Frobenius norm. We provide counting-number arguments for the saturation of bond dimensions for the one-time/-frequency objects, while the origin of this saturation for two-time/-frequency objects remains to be clarified. This paper's findings highlight the critical need for further research on the selection of truncation methods, tolerance levels, and the choice between imaginary-time and imaginary-frequency representations in practical applications.","sentences":["Space-time dependence of imaginary-time propagators, vital for \\textit{ab initio} and many-body calculations based on quantum field theories, has been revealed to be compressible using Quantum Tensor Trains (QTTs)","[Phys. Rev. X {\\bf 13}, 021015 (2023)].","However, the impact of system parameters, like temperature, on data size remains underexplored.","This paper provides a comprehensive numerical analysis of the compactness of local imaginary-time propagators in QTT for one-time/-frequency objects and two-time/-frequency objects, considering truncation in terms of the Frobenius and maximum norms.","We employ random pole models to study worst-case scenarios.","The numerical analysis reveals several key findings.","For one-time/-frequency objects and two-time/-frequency objects, the bond dimensions saturate at low temperatures, especially for truncation in terms of the Frobenius norm.","We provide counting-number arguments for the saturation of bond dimensions for the one-time/-frequency objects, while the origin of this saturation for two-time/-frequency objects remains to be clarified.","This paper's findings highlight the critical need for further research on the selection of truncation methods, tolerance levels, and the choice between imaginary-time and imaginary-frequency representations in practical applications."],"url":"http://arxiv.org/abs/2403.09161v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 08:16:50","title":"Efficient Lexicographic Optimization for Prioritized Robot Control and Planning","abstract":"In this work, we present several tools for efficient sequential hierarchical least-squares programming (S-HLSP) for lexicographical optimization tailored to robot control and planning. As its main step, S-HLSP relies on approximations of the original non-linear hierarchical least-squares programming (NL-HLSP) to a hierarchical least-squares programming (HLSP) by the hierarchical Newton's method or the hierarchical Gauss-Newton algorithm. We present a threshold adaptation strategy for appropriate switches between the two. This ensures optimality of infeasible constraints, promotes numerical stability when solving the HLSP's and enhances optimality of lower priority levels by avoiding regularized local minima. We introduce the solver $\\mathcal{N}$ADM$_2$, an alternating direction method of multipliers for HLSP based on nullspace projections of active constraints. The required basis of nullspace of the active constraints is provided by a computationally efficient turnback algorithm for system dynamics discretized by the Euler method. It is based on an upper bound on the bandwidth of linearly independent column subsets within the linearized constraint matrices. Importantly, an expensive initial rank-revealing matrix factorization is unnecessary. We show how the high sparsity of the basis in the fully-actuated case can be preserved in the under-actuated case. $\\mathcal{N}$ADM$_2$ consistently shows faster computations times than competing off-the-shelf solvers on NL-HLSP composed of test-functions and whole-body trajectory optimization for fully-actuated and under-actuated robotic systems. We demonstrate how the inherently lower accuracy solutions of the alternating direction method of multipliers can be used to warm-start the non-linear solver for efficient computation of high accuracy solutions to non-linear hierarchical least-squares programs.","sentences":["In this work, we present several tools for efficient sequential hierarchical least-squares programming (S-HLSP) for lexicographical optimization tailored to robot control and planning.","As its main step, S-HLSP relies on approximations of the original non-linear hierarchical least-squares programming (NL-HLSP) to a hierarchical least-squares programming (HLSP) by the hierarchical Newton's method or the hierarchical Gauss-Newton algorithm.","We present a threshold adaptation strategy for appropriate switches between the two.","This ensures optimality of infeasible constraints, promotes numerical stability when solving the HLSP's and enhances optimality of lower priority levels by avoiding regularized local minima.","We introduce the solver $\\mathcal{N}$ADM$_2$, an alternating direction method of multipliers for HLSP based on nullspace projections of active constraints.","The required basis of nullspace of the active constraints is provided by a computationally efficient turnback algorithm for system dynamics discretized by the Euler method.","It is based on an upper bound on the bandwidth of linearly independent column subsets within the linearized constraint matrices.","Importantly, an expensive initial rank-revealing matrix factorization is unnecessary.","We show how the high sparsity of the basis in the fully-actuated case can be preserved in the under-actuated case.","$\\mathcal{N}$ADM$_2$ consistently shows faster computations times than competing off-the-shelf solvers on NL-HLSP composed of test-functions and whole-body trajectory optimization for fully-actuated and under-actuated robotic systems.","We demonstrate how the inherently lower accuracy solutions of the alternating direction method of multipliers can be used to warm-start the non-linear solver for efficient computation of high accuracy solutions to non-linear hierarchical least-squares programs."],"url":"http://arxiv.org/abs/2403.09160v1","category":"cs.RO"}
{"created":"2024-03-14 08:12:39","title":"VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation","abstract":"In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated. However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for Transformers. Recently, State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. Inspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual State Space (VSS) Block is introduced to capture extensive contextual information, the Semantics and Detail Infusion (SDI) is introduced to augment the infusion of low-level and high-level features. We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB and ETIS-LaribPolypDB public datasets. The results indicate that VM-UNetV2 exhibits competitive performance in medical image segmentation tasks. Our code is available at https://github.com/nobodyplayer1/VM-UNetV2.","sentences":["In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated.","However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully.","On the other hand, the quadratic computational complexity poses a challenge for Transformers.","Recently, State Space Models (SSMs), such as Mamba, have been recognized as a promising method.","They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity.","Inspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual State Space (VSS) Block is introduced to capture extensive contextual information, the Semantics and Detail Infusion (SDI) is introduced to augment the infusion of low-level and high-level features.","We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB and ETIS-LaribPolypDB public datasets.","The results indicate that VM-UNetV2 exhibits competitive performance in medical image segmentation tasks.","Our code is available at https://github.com/nobodyplayer1/VM-UNetV2."],"url":"http://arxiv.org/abs/2403.09157v1","category":"eess.IV"}
{"created":"2024-03-14 08:05:36","title":"Energy-gap modulation and majorization in three-level quantum Otto engine","abstract":"A three-level quantum system having two energy gaps presents a nontrivial working medium for a quantum heat engine. Our focus lies in understanding the constraints on the ability to modulate these gaps relative to the changes in probability distributions at the two given heat reservoirs. It is seen that an Otto engine in the quasistatic limit is feasible if at least one energy gap shrinks during the first quantum adiabatic stage. We analyze operating conditions under different variations of the gaps, revealing that a definite majorization relation between the hot and cold distributions serves as a sufficient criterion for the engine when both gaps are shrinking. Further, Otto efficiency is enhanced in case majorization holds. On the other hand, majorization becomes a necessary condition when only one of the gaps is shrinking. In the special case where one gap remains fixed, majorization is both necessary and sufficient for engine operation. For an $n$-level system, we note that a well defined change in energy gaps aligns with the majorization relation, thus characterizing the operation of the engine.","sentences":["A three-level quantum system having two energy gaps presents a nontrivial working medium for a quantum heat engine.","Our focus lies in understanding the constraints on the ability to modulate these gaps relative to the changes in probability distributions at the two given heat reservoirs.","It is seen that an Otto engine in the quasistatic limit is feasible if at least one energy gap shrinks during the first quantum adiabatic stage.","We analyze operating conditions under different variations of the gaps, revealing that a definite majorization relation between the hot and cold distributions serves as a sufficient criterion for the engine when both gaps are shrinking.","Further, Otto efficiency is enhanced in case majorization holds.","On the other hand, majorization becomes a necessary condition when only one of the gaps is shrinking.","In the special case where one gap remains fixed, majorization is both necessary and sufficient for engine operation.","For an $n$-level system, we note that a well defined change in energy gaps aligns with the majorization relation, thus characterizing the operation of the engine."],"url":"http://arxiv.org/abs/2403.09154v1","category":"quant-ph"}
{"created":"2024-03-14 08:04:46","title":"Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks","abstract":"In the rapidly advancing field of federated learning (FL), ensuring efficient FL task delegation while incentivising FL client participation poses significant challenges, especially in wireless networks where FL participants' coverage is limited. Existing Contract Theory-based methods are designed under the assumption that there is only one FL server in the system (i.e., the monopoly market assumption), which in unrealistic in practice. To address this limitation, we propose Fairness-Aware Multi-Server FL task delegation approach (FAMuS), a novel framework based on Contract Theory and Lyapunov optimization to jointly address these intricate issues facing wireless multi-server FL networks (WMSFLN). Within a given WMSFLN, a task requester products multiple FL tasks and delegate them to FL servers which coordinate the training processes. To ensure fair treatment of FL servers, FAMuS establishes virtual queues to track their previous access to FL tasks, updating them in relation to the resulting FL model performance. The objective is to minimize the time-averaged cost in a WMSFLN, while ensuring all queues remain stable. This is particularly challenging given the incomplete information regarding FL clients' participation cost and the unpredictable nature of the WMSFLN state, which depends on the locations of the mobile clients. Extensive experiments comparing FAMuS against five state-of-the-art approaches based on two real-world datasets demonstrate that it achieves 6.91% higher test accuracy, 27.34% lower cost, and 0.63% higher fairness on average than the best-performing baseline.","sentences":["In the rapidly advancing field of federated learning (FL), ensuring efficient FL task delegation while incentivising FL client participation poses significant challenges, especially in wireless networks where FL participants' coverage is limited.","Existing Contract Theory-based methods are designed under the assumption that there is only one FL server in the system (i.e., the monopoly market assumption), which in unrealistic in practice.","To address this limitation, we propose Fairness-Aware Multi-Server FL task delegation approach (FAMuS), a novel framework based on Contract Theory and Lyapunov optimization to jointly address these intricate issues facing wireless multi-server FL networks (WMSFLN).","Within a given WMSFLN, a task requester products multiple FL tasks and delegate them to FL servers which coordinate the training processes.","To ensure fair treatment of FL servers, FAMuS establishes virtual queues to track their previous access to FL tasks, updating them in relation to the resulting FL model performance.","The objective is to minimize the time-averaged cost in a WMSFLN, while ensuring all queues remain stable.","This is particularly challenging given the incomplete information regarding FL clients' participation cost and the unpredictable nature of the WMSFLN state, which depends on the locations of the mobile clients.","Extensive experiments comparing FAMuS against five state-of-the-art approaches based on two real-world datasets demonstrate that it achieves 6.91% higher test accuracy, 27.34% lower cost, and 0.63% higher fairness on average than the best-performing baseline."],"url":"http://arxiv.org/abs/2403.09153v1","category":"eess.SY"}
{"created":"2024-03-14 08:00:37","title":"Line geometry of pairs of second-order Hamiltonian operators and quasilinear systems","abstract":"We show that a pair formed by a second-order homogeneous Hamiltonian structures in $N$ components and the associated system of conservation laws is in bijective correspondence with an alternating three-form on a $N+2$ vector space. We use this result to characterise these pairs up to $N=4$. We also show that the three-form provides $N+2$ linear equations in the Pl\\\"ucker coordinates which define the associated line congruence.","sentences":["We show that a pair formed by a second-order homogeneous Hamiltonian structures in $N$ components and the associated system of conservation laws is in bijective correspondence with an alternating three-form on a $N+2$ vector space.","We use this result to characterise these pairs up to $N=4$. We also show that the three-form provides $N+2$ linear equations in the Pl\\\"ucker coordinates which define the associated line congruence."],"url":"http://arxiv.org/abs/2403.09152v1","category":"math-ph"}
{"created":"2024-03-14 07:59:51","title":"MPC without Terminal Ingredients Tailored to the SEIR Compartmental Epidemic Model","abstract":"We consider the SEIR epidemic model subject to state and input constraints (a cap on the proportion of infectuous individuals and limits on the allowed social distancing and quarantining measures, respectively). We present a model predictive control (MPC) formulation tailored to this system without terminal conditions in the recursively solved finite-horizon optimal control problem. We rigorously show recursive feasibility and asymptotic stability of the disease-free equilibrium w.r.t. the MPC closed loop for suitably designed quadratic running cost and a sufficiently long prediction horizon (forecast window). Moreover, we establish the viability kernel (a.k.a. the admissible set) as domain of attraction provided that the infection numbers that are not too small at the beginning, which corresponds to infection numbers noticeable by, e.g., policy makers or the general public.","sentences":["We consider the SEIR epidemic model subject to state and input constraints (a cap on the proportion of infectuous individuals and limits on the allowed social distancing and quarantining measures, respectively).","We present a model predictive control (MPC) formulation tailored to this system without terminal conditions in the recursively solved finite-horizon optimal control problem.","We rigorously show recursive feasibility and asymptotic stability of the disease-free equilibrium w.r.t.","the MPC closed loop for suitably designed quadratic running cost and a sufficiently long prediction horizon (forecast window).","Moreover, we establish the viability kernel (a.k.a.","the admissible set) as domain of attraction provided that the infection numbers that are not too small at the beginning, which corresponds to infection numbers noticeable by, e.g., policy makers or the general public."],"url":"http://arxiv.org/abs/2403.09151v1","category":"math.OC"}
{"created":"2024-03-14 07:59:35","title":"Detecting the N\u00e9el vector of altermagnet by attaching a topological insulator and crystalline valley-edge insulator","abstract":"In order to detect the N\\'{e}el vector of an altermagnet, we investigate topological phases in a bilayer system composed of an altermagnet and a two-dimensional topological insulator described by the Bernevig-Hughes-Zhang model. A topological phase transition occurs from a first-order topological insulator to a trivial insulator at a certain critical altermagnetization if the N\\'{e}el vector of altermagnet is along the $x$ axis or the $y$ axis. It is intriguing that valley-protected edge states emerge along the N\\'{e}el vector in this trivial insulator, which are as stable as the topological edge states. We name it a crystalline valley-edge insulator. On the other hand, the system turns out to be a second-order topological insulator when the N\\'{e}el vector is along the $z$ axis. The tunneling conductance has a strong dependence on the N\\'{e}el vector. In addition, the band gap depends on the N\\'{e}el vector, which is measurable by optical absorption. Hence, it is possible experimentally to detect the N\\'{e}el vector by measuring tunneling conductance and optical absorption.","sentences":["In order to detect the N\\'{e}el vector of an altermagnet, we investigate topological phases in a bilayer system composed of an altermagnet and a two-dimensional topological insulator described by the Bernevig-Hughes-Zhang model.","A topological phase transition occurs from a first-order topological insulator to a trivial insulator at a certain critical altermagnetization if the N\\'{e}el vector of altermagnet is along the $x$ axis or the $y$ axis.","It is intriguing that valley-protected edge states emerge along the N\\'{e}el vector in this trivial insulator, which are as stable as the topological edge states.","We name it a crystalline valley-edge insulator.","On the other hand, the system turns out to be a second-order topological insulator when the N\\'{e}el vector is along the $z$ axis.","The tunneling conductance has a strong dependence on the N\\'{e}el vector.","In addition, the band gap depends on the N\\'{e}el vector, which is measurable by optical absorption.","Hence, it is possible experimentally to detect the N\\'{e}el vector by measuring tunneling conductance and optical absorption."],"url":"http://arxiv.org/abs/2403.09150v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-14 07:58:42","title":"$2$-Periodic complexes over regular local rings","abstract":"Let $(A,\\mathfrak{m})$ be a regular local ring of dimension $d \\geq 1$. Let $\\mathcal{D}^2_{fg}(A)$ denote the derived category of $2$-periodic complexes with finitely generated cohomology modules. Let $\\mathcal{K}^2(\\proj A) $ denote the homotopy category of $2$-periodic complexes of finitely generated free $A$-modules.   We show the natural map $\\mathcal{K}^2(\\ proj \\ A) \\longrightarrow \\mathcal{D}^2(A)$ is an equivalence of categories. When $A$ is complete we show that $\\mathcal{K}^2_f(\\ proj \\ A)$ ($2$-periodic complexes with finite length cohomology) is Krull-Schmidt with Auslander-Reiten (AR) triangles. We also compute the AR-quiver of $\\mathcal{K}^2_f(\\ proj \\ A)$ when $\\ dim \\ A = 1$.","sentences":["Let $(A,\\mathfrak{m})$ be a regular local ring of dimension $d \\geq 1$. Let $\\mathcal{D}^2_{fg}(A)$ denote the derived category of $2$-periodic complexes with finitely generated cohomology modules.","Let $\\mathcal{K}^2(\\proj A) $ denote the homotopy category of $2$-periodic complexes of finitely generated free $A$-modules.   ","We show the natural map $\\mathcal{K}^2(\\ proj \\ A)","\\longrightarrow \\mathcal{D}^2(A)$ is an equivalence of categories.","When $A$ is complete we show that $\\mathcal{K}^2_f(\\ proj \\ A)$ ($2$-periodic complexes with finite length cohomology) is Krull-Schmidt with Auslander-Reiten (AR) triangles.","We also compute the AR-quiver of $\\mathcal{K}^2_f(\\ proj \\ A)$ when $\\ dim \\ A = 1$."],"url":"http://arxiv.org/abs/2403.09149v1","category":"math.AC"}
{"created":"2024-03-14 07:47:02","title":"Complexity Classification of Complex-Weighted Counting Acyclic Constraint Satisfaction Problems","abstract":"We study the computational complexity of counting constraint satisfaction problems (#CSPs) whose constraints assign complex numbers to Boolean inputs when the corresponding constraint hypergraphs are acyclic. These problems are called acyclic #CSPs or succinctly, #ACSPs. We wish to determine the computational complexity of all such #ACSPs when arbitrary unary constraints are freely available. Depending on whether we further allow or disallow the free use of the specific constraint XOR (binary disequality), we present two complexity classifications of the #ACSPs according to the types of constraints used for the problems. When XOR is freely available, we first obtain a complete dichotomy classification. On the contrary, when XOR is not available for free, we then obtain a trichotomy classification. To deal with an acyclic nature of constraints in those classifications, we develop a new technical tool called acyclic-T-constructibility or AT-constructibility, and we exploit it to analyze a complexity upper bound of each #ACSPs.","sentences":["We study the computational complexity of counting constraint satisfaction problems (#CSPs) whose constraints assign complex numbers to Boolean inputs when the corresponding constraint hypergraphs are acyclic.","These problems are called acyclic #CSPs or succinctly, #ACSPs.","We wish to determine the computational complexity of all such #ACSPs when arbitrary unary constraints are freely available.","Depending on whether we further allow or disallow the free use of the specific constraint XOR (binary disequality), we present two complexity classifications of the #ACSPs according to the types of constraints used for the problems.","When XOR is freely available, we first obtain a complete dichotomy classification.","On the contrary, when XOR is not available for free, we then obtain a trichotomy classification.","To deal with an acyclic nature of constraints in those classifications, we develop a new technical tool called acyclic-T-constructibility or AT-constructibility, and we exploit it to analyze a complexity upper bound of each #ACSPs."],"url":"http://arxiv.org/abs/2403.09145v1","category":"cs.CC"}
{"created":"2024-03-14 07:22:52","title":"Non-distributive relatives of ETL and NFL","abstract":"In this paper, we devise non-distributive relatives of Exactly True Logic (ETL) by Pietz and Riveccio and its dual (NFL) Non-Falsity Logic by Shramko, Zaitsev and Belikov. We consider two pre-orders which are algebraic counterparts of the ETL's and NFL's entailment relations on the De Morgan lattice $\\mathbf{4}$. We generalise these pre-orders and determine which distributive properties that hold on $\\mathbf{4}$ are not forced by either of the pre-orders. We then construct relatives of ETL and NFL but lack such distributive properties. For these logics, we also devise a truth table semantics which uses non-distributive lattice $\\mathbf{M3}$ as their lattice of truth values. We also provide analytic tableaux systems that work with sequents of the form $\\phi\\vdash\\chi$. We also prove the correctness and completeness results for these proof systems and provide a neat generalisation for non-distributive ETL- and NFL-like logics built over a certain family of non-distributive modular lattices.","sentences":["In this paper, we devise non-distributive relatives of Exactly True Logic (ETL) by Pietz and Riveccio and its dual (NFL) Non-Falsity Logic by Shramko, Zaitsev and Belikov.","We consider two pre-orders which are algebraic counterparts of the ETL's and NFL's entailment relations on the De Morgan lattice $\\mathbf{4}$. We generalise these pre-orders and determine which distributive properties that hold on $\\mathbf{4}$ are not forced by either of the pre-orders.","We then construct relatives of ETL and NFL but lack such distributive properties.","For these logics, we also devise a truth table semantics which uses non-distributive lattice $\\mathbf{M3}$ as their lattice of truth values.","We also provide analytic tableaux systems that work with sequents of the form $\\phi\\vdash\\chi$. We also prove the correctness and completeness results for these proof systems and provide a neat generalisation for non-distributive ETL- and NFL-like logics built over a certain family of non-distributive modular lattices."],"url":"http://arxiv.org/abs/2403.09137v1","category":"math.LO"}
{"created":"2024-03-14 07:21:46","title":"Biophysics Informed Pathological Regularisation for Brain Tumour Segmentation","abstract":"Recent advancements in deep learning have significantly improved brain tumour segmentation techniques; however, the results still lack confidence and robustness as they solely consider image data without biophysical priors or pathological information. Integrating biophysics-informed regularisation is one effective way to change this situation, as it provides an prior regularisation for automated end-to-end learning. In this paper, we propose a novel approach that designs brain tumour growth Partial Differential Equation (PDE) models as a regularisation with deep learning, operational with any network model. Our method introduces tumour growth PDE models directly into the segmentation process, improving accuracy and robustness, especially in data-scarce scenarios. This system estimates tumour cell density using a periodic activation function. By effectively integrating this estimation with biophysical models, we achieve a better capture of tumour characteristics. This approach not only aligns the segmentation closer to actual biological behaviour but also strengthens the model's performance under limited data conditions. We demonstrate the effectiveness of our framework through extensive experiments on the BraTS 2023 dataset, showcasing significant improvements in both precision and reliability of tumour segmentation.","sentences":["Recent advancements in deep learning have significantly improved brain tumour segmentation techniques; however, the results still lack confidence and robustness as they solely consider image data without biophysical priors or pathological information.","Integrating biophysics-informed regularisation is one effective way to change this situation, as it provides an prior regularisation for automated end-to-end learning.","In this paper, we propose a novel approach that designs brain tumour growth Partial Differential Equation (PDE) models as a regularisation with deep learning, operational with any network model.","Our method introduces tumour growth PDE models directly into the segmentation process, improving accuracy and robustness, especially in data-scarce scenarios.","This system estimates tumour cell density using a periodic activation function.","By effectively integrating this estimation with biophysical models, we achieve a better capture of tumour characteristics.","This approach not only aligns the segmentation closer to actual biological behaviour but also strengthens the model's performance under limited data conditions.","We demonstrate the effectiveness of our framework through extensive experiments on the BraTS 2023 dataset, showcasing significant improvements in both precision and reliability of tumour segmentation."],"url":"http://arxiv.org/abs/2403.09136v1","category":"eess.IV"}
{"created":"2024-03-14 07:05:10","title":"Local Enumeration and Majority Lower Bounds","abstract":"Depth-3 circuit lower bounds and $k$-SAT algorithms are intimately related; the state-of-the-art $\\Sigma^k_3$-circuit lower bound and the $k$-SAT algorithm are based on the same combinatorial theorem. In this paper we define a problem which reveals new interactions between the two. Define Enum($k$, $t$) problem as: given an $n$-variable $k$-CNF and an initial assignment $\\alpha$, output all satisfying assignments at Hamming distance $t$ from $\\alpha$, assuming that there are no satisfying assignments of Hamming distance less than $t$ from $\\alpha$. Observe that: an upper bound $b(n, k, t)$ on the complexity of Enum($k$, $t$) implies:   - Depth-3 circuits: Any $\\Sigma^k_3$ circuit computing the Majority function has size at least $\\binom{n}{\\frac{n}{2}}/b(n, k, \\frac{n}{2})$.   - $k$-SAT: There exists an algorithm solving $k$-SAT in time $O(\\sum_{t = 1}^{n/2}b(n, k, t))$.   A simple construction shows that $b(n, k, \\frac{n}{2}) \\ge 2^{(1 - O(\\log(k)/k))n}$. Thus, matching upper bounds would imply a $\\Sigma^k_3$-circuit lower bound of $2^{\\Omega(\\log(k)n/k)}$ and a $k$-SAT upper bound of $2^{(1 - \\Omega(\\log(k)/k))n}$. The former yields an unrestricted depth-3 lower bound of $2^{\\omega(\\sqrt{n})}$ solving a long standing open problem, and the latter breaks the Super Strong Exponential Time Hypothesis.   In this paper, we propose a randomized algorithm for Enum($k$, $t$) and introduce new ideas to analyze it. We demonstrate the power of our ideas by considering the first non-trivial instance of the problem, i.e., Enum($3$, $\\frac{n}{2}$). We show that the expected running time of our algorithm is $1.598^n$, substantially improving on the trivial bound of $3^{n/2} \\simeq 1.732^n$. This already improves $\\Sigma^3_3$ lower bounds for Majority function to $1.251^n$. The previous bound was $1.154^n$ which follows from the work of H{\\aa}stad, Jukna, and Pudl\\'ak (Comput. Complex.'95).","sentences":["Depth-3 circuit lower bounds and $k$-SAT algorithms are intimately related; the state-of-the-art $\\Sigma^k_3$-circuit lower bound and the $k$-SAT algorithm are based on the same combinatorial theorem.","In this paper we define a problem which reveals new interactions between the two.","Define Enum($k$, $t$) problem as: given an $n$-variable $k$-CNF and an initial assignment $\\alpha$, output all satisfying assignments at Hamming distance $t$ from $\\alpha$, assuming that there are no satisfying assignments of Hamming distance less than $t$ from $\\alpha$. Observe that: an upper bound $b(n, k, t)$ on the complexity of Enum($k$, $t$) implies:   - Depth-3 circuits: Any $\\Sigma^k_3$ circuit computing the Majority function has size at least $\\binom{n}{\\frac{n}{2}}/b(n, k, \\frac{n}{2})$.   - $k$-SAT: There exists an algorithm solving $k$-SAT in time $O(\\sum_{t = 1}^{n/2}b(n, k, t))$.   A simple construction shows that $b(n, k, \\frac{n}{2}) \\ge","2^{(1 - O(\\log(k)/k))n}$.","Thus, matching upper bounds would imply a $\\Sigma^k_3$-circuit lower bound of $2^{\\Omega(\\log(k)n/k)}$ and a $k$-SAT upper bound of $2^{(1 - \\Omega(\\log(k)/k))n}$. The former yields an unrestricted depth-3 lower bound of $2^{\\omega(\\sqrt{n})}$ solving a long standing open problem, and the latter breaks the Super Strong Exponential Time Hypothesis.   ","In this paper, we propose a randomized algorithm for Enum($k$, $t$) and introduce new ideas to analyze it.","We demonstrate the power of our ideas by considering the first non-trivial instance of the problem, i.e., Enum($3$, $\\frac{n}{2}$).","We show that the expected running time of our algorithm is $1.598^n$, substantially improving on the trivial bound of $3^{n/2} \\simeq 1.732^n$.","This already improves $\\Sigma^3_3$ lower bounds for Majority function to $1.251^n$. The previous bound was $1.154^n$ which follows from the work of H{\\aa}stad, Jukna, and Pudl\\'ak (Comput.","Complex.'95)."],"url":"http://arxiv.org/abs/2403.09134v1","category":"cs.CC"}
{"created":"2024-03-14 06:57:04","title":"Quantitative Reducibility of $C^k$ Quasi-Periodic Cocycles","abstract":"This paper establishes an extreme $C^k$ reducibility theorem of quasi-periodic $SL(2, \\mathbb{R})$ cocycles in the local perturbative region, revealing both the essence of Eliasson [Commun.Math.Phys.1992] and Hou-You [Invent.Math.2022] in respectively the non-resonant and resonant cases. By paralleling further the reducibility process with the almost reducibility, we are able to acquire the least initial regularity as well as the least loss of regularity for the whole KAM iterations. This, in return, makes various spectral applications of quasi-periodic Schr\\\"odinger operators wide open.","sentences":["This paper establishes an extreme $C^k$ reducibility theorem of quasi-periodic $SL(2, \\mathbb{R})$ cocycles in the local perturbative region, revealing both the essence of Eliasson","[Commun.","Math.Phys.1992] and Hou-You [Invent.","Math.2022] in respectively the non-resonant and resonant cases.","By paralleling further the reducibility process with the almost reducibility, we are able to acquire the least initial regularity as well as the least loss of regularity for the whole KAM iterations.","This, in return, makes various spectral applications of quasi-periodic Schr\\\"odinger operators wide open."],"url":"http://arxiv.org/abs/2403.09132v1","category":"math.DS"}
{"created":"2024-03-14 06:29:17","title":"All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System","abstract":"Pricing is an important issue in mobile edge computing. How to appropriately determine the bid of end user (EU) is an incentive factor for edge cloud (EC) to offer service. In this letter, we propose an equilibrium pricing scheme based on the all-pay auction model in end-to-end collaboration environment, wherein all EUs can acquire the service at a lower price than the own value of the required resource. In addition, we propose a set allocation algorithm to divide all the bidders into different sets according to the price, and the EUs in each set get the service, which averts the case of getting no service due to the low price. Extensive simulation results demonstrate that the proposed scheme can effectively maximize the total profit of the edge offloading system, and guarantee all EUs can access the service.","sentences":["Pricing is an important issue in mobile edge computing.","How to appropriately determine the bid of end user (EU) is an incentive factor for edge cloud (EC) to offer service.","In this letter, we propose an equilibrium pricing scheme based on the all-pay auction model in end-to-end collaboration environment, wherein all EUs can acquire the service at a lower price than the own value of the required resource.","In addition, we propose a set allocation algorithm to divide all the bidders into different sets according to the price, and the EUs in each set get the service, which averts the case of getting no service due to the low price.","Extensive simulation results demonstrate that the proposed scheme can effectively maximize the total profit of the edge offloading system, and guarantee all EUs can access the service."],"url":"http://arxiv.org/abs/2403.09129v1","category":"cs.GT"}
{"created":"2024-03-14 06:25:37","title":"Optimal Pinning Control for Synchronization over Temporal Networks","abstract":"In this paper, we address the finite time synchronization of a network of dynamical systems with time-varying interactions modeled using temporal networks. We synchronize a few nodes initially using external control inputs. These nodes are termed as pinning nodes. The other nodes are synchronized by interacting with the pinning nodes and with each other. We first provide sufficient conditions for the network to be synchronized. Then we formulate an optimization problem to minimize the number of pinning nodes for synchronizing the entire network. Finally, we address the problem of maximizing the number of synchronized nodes when there are constraints on the number of nodes that could be pinned. We show that this problem belongs to the class of NP-hard problems and propose a greedy heuristic. We illustrate the results using numerical simulations.","sentences":["In this paper, we address the finite time synchronization of a network of dynamical systems with time-varying interactions modeled using temporal networks.","We synchronize a few nodes initially using external control inputs.","These nodes are termed as pinning nodes.","The other nodes are synchronized by interacting with the pinning nodes and with each other.","We first provide sufficient conditions for the network to be synchronized.","Then we formulate an optimization problem to minimize the number of pinning nodes for synchronizing the entire network.","Finally, we address the problem of maximizing the number of synchronized nodes when there are constraints on the number of nodes that could be pinned.","We show that this problem belongs to the class of NP-hard problems and propose a greedy heuristic.","We illustrate the results using numerical simulations."],"url":"http://arxiv.org/abs/2403.09127v1","category":"eess.SY"}
{"created":"2024-03-14 06:20:35","title":"Femtoscopic study of the $\u039b\u03b1$ interaction","abstract":"We examine the $\\Lambda$-${}^4\\mathrm{He}$ ($\\alpha$) momentum correlation in high-energy collisions to elucidate the interaction between Lambdas ($\\Lambda$) and nucleons ($N$). We compare phenomenological $\\Lambda\\alpha$ potentials with different strength at short range. In addition to the conventional Gaussian type potentials, we construct the $\\Lambda\\alpha$ potentials by substituting the nucleon density distribution of $\\alpha$ in the Skyrme-type $\\Lambda$ potentials. We find that the dependence on the employed potential models is visible in the correlation functions from a small size source. This indicates that the $\\Lambda\\alpha$ momentum correlation could constrain the property of the $\\Lambda N$ interaction at high densities, which is expected to play an essential role in dense nuclear matter. Also, we verify that the Lednicky-Lyuboshits formula can yield erroneous results for a small source size with a potential which has large interaction range, like the $\\Lambda\\alpha$ system.","sentences":["We examine the $\\Lambda$-${}^4\\mathrm{He}$ ($\\alpha$) momentum correlation in high-energy collisions to elucidate the interaction between Lambdas ($\\Lambda$) and nucleons ($N$).","We compare phenomenological $\\Lambda\\alpha$ potentials with different strength at short range.","In addition to the conventional Gaussian type potentials, we construct the $\\Lambda\\alpha$ potentials by substituting the nucleon density distribution of $\\alpha$ in the Skyrme-type $\\Lambda$ potentials.","We find that the dependence on the employed potential models is visible in the correlation functions from a small size source.","This indicates that the $\\Lambda\\alpha$ momentum correlation could constrain the property of the $\\Lambda N$ interaction at high densities, which is expected to play an essential role in dense nuclear matter.","Also, we verify that the Lednicky-Lyuboshits formula can yield erroneous results for a small source size with a potential which has large interaction range, like the $\\Lambda\\alpha$ system."],"url":"http://arxiv.org/abs/2403.09126v1","category":"nucl-th"}
{"created":"2024-03-14 06:14:07","title":"Optimal Top-Two Method for Best Arm Identification and Fluid Analysis","abstract":"Top-$2$ methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\\beta$, and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified $\\delta >0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\\delta \\rightarrow 0$ by computationally demanding plug-in methods. The above top 2 algorithm for any $\\beta \\in (0,1)$ has sample complexity within a constant of the lower bound. However, determining the optimal $\\beta$ that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchored at a threshold. If it exceeds the threshold then the algorithm samples the empirical best arm. Otherwise, it samples the challenger arm. We show that the proposed algorithm is optimal as $\\delta \\rightarrow 0$. Our analysis relies on identifying a limiting fluid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm. We rely on the implicit function theorem to show existence and uniqueness of these fluid ode's and to show that the proposed algorithm remains close to the ode solution.","sentences":["Top-$2$ methods have become popular in solving the best arm identification (BAI) problem.","The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\\beta$, and pulls the best challenger arm otherwise.","The probability of incorrect selection is guaranteed to lie below a specified $\\delta >0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\\delta \\rightarrow 0$ by computationally demanding plug-in methods.","The above top 2 algorithm for any $\\beta \\in (0,1)$ has sample complexity within a constant of the lower bound.","However, determining the optimal $\\beta$ that matches the lower bound has proven difficult.","In this paper, we address this and propose an optimal top-2 type algorithm.","We consider a function of allocations anchored at a threshold.","If it exceeds the threshold then the algorithm samples the empirical best arm.","Otherwise, it samples the challenger arm.","We show that the proposed algorithm is optimal as $\\delta \\rightarrow 0$.","Our analysis relies on identifying a limiting fluid dynamics of allocations that satisfy a series of ordinary differential equations pasted together and that describe the asymptotic path followed by our algorithm.","We rely on the implicit function theorem to show existence and uniqueness of these fluid ode's and to show that the proposed algorithm remains close to the ode solution."],"url":"http://arxiv.org/abs/2403.09123v1","category":"cs.LG"}
{"created":"2024-03-14 06:10:51","title":"OutlineSpark: Igniting AI-powered Presentation Slides Creation from Computational Notebooks through Outlines","abstract":"Computational notebooks are widely utilized for exploration and analysis. However, creating slides to communicate analysis results from these notebooks is quite tedious and time-consuming. Researchers have proposed automatic systems for generating slides from notebooks, which, however, often do not consider the process of users conceiving and organizing their messages from massive code cells. Those systems ask users to go directly into the slide creation process, which causes potentially ill-structured slides and burdens in further refinement. Inspired by the common and widely recommended slide creation practice: drafting outlines first and then adding concrete content, we introduce OutlineSpark, an AI-powered slide creation tool that generates slides from a slide outline written by the user. The tool automatically retrieves relevant notebook cells based on the outlines and converts them into slide content. We evaluated OutlineSpark with 12 users. Both the quantitative and qualitative feedback from the participants verify its effectiveness and usability.","sentences":["Computational notebooks are widely utilized for exploration and analysis.","However, creating slides to communicate analysis results from these notebooks is quite tedious and time-consuming.","Researchers have proposed automatic systems for generating slides from notebooks, which, however, often do not consider the process of users conceiving and organizing their messages from massive code cells.","Those systems ask users to go directly into the slide creation process, which causes potentially ill-structured slides and burdens in further refinement.","Inspired by the common and widely recommended slide creation practice: drafting outlines first and then adding concrete content, we introduce OutlineSpark, an AI-powered slide creation tool that generates slides from a slide outline written by the user.","The tool automatically retrieves relevant notebook cells based on the outlines and converts them into slide content.","We evaluated OutlineSpark with 12 users.","Both the quantitative and qualitative feedback from the participants verify its effectiveness and usability."],"url":"http://arxiv.org/abs/2403.09121v1","category":"cs.HC"}
{"created":"2024-03-14 06:01:26","title":"Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss","abstract":"In this paper, we present a~generalisation of proof simulation procedures for Frege systems by Bonet and Buss to some logics for which the deduction theorem does not hold. In particular, we study the case of finite-valued \\L{}ukasiewicz logics. To this end, we provide proof systems that augment Avron's Frege system for \\L{}ukasiewicz three-valued logic with nested and general versions of the disjunction elimination rule, respectively. For these systems we provide upper bounds on speed-ups w.r.t.\\ both the number of steps in proofs and the length of proofs. We also consider Tamminga's natural deduction and Avron's hypersequent calculus for 3-valued \\L{}ukasiewicz logic and generalise our results considering the disjunction elimination rule to all finite-valued \\L{}ukasiewicz logics.","sentences":["In this paper, we present a~generalisation of proof simulation procedures for Frege systems by Bonet and Buss to some logics for which the deduction theorem does not hold.","In particular, we study the case of finite-valued \\L{}ukasiewicz logics.","To this end, we provide proof systems that augment Avron's Frege system for \\L{}ukasiewicz three-valued logic with nested and general versions of the disjunction elimination rule, respectively.","For these systems we provide upper bounds on speed-ups w.r.t.\\ both the number of steps in proofs and the length of proofs.","We also consider Tamminga's natural deduction and Avron's hypersequent calculus for 3-valued \\L{}ukasiewicz logic and generalise our results considering the disjunction elimination rule to all finite-valued \\L{}ukasiewicz logics."],"url":"http://arxiv.org/abs/2403.09119v1","category":"math.LO"}
{"created":"2024-03-14 06:00:42","title":"Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network","abstract":"This study introduces a robust solution for the detection of Distributed Denial of Service (DDoS) attacks in Internet of Things (IoT) systems, leveraging the capabilities of Graph Convolutional Networks (GCN). By conceptualizing IoT devices as nodes within a graph structure, we present a detection mechanism capable of operating efficiently even in lossy network environments. We introduce various graph topologies for modeling IoT networks and evaluate them for detecting tunable futuristic DDoS attacks. By studying different levels of network connection loss and various attack situations, we demonstrate that the correlation-based hybrid graph structure is effective in spotting DDoS attacks, substantiating its good performance even in lossy network scenarios. The results indicate a remarkable performance of the GCN-based DDoS detection model with an F1 score of up to 91%. Furthermore, we observe at most a 2% drop in F1-score in environments with up to 50% connection loss. The findings from this study highlight the advantages of utilizing GCN for the security of IoT systems which benefit from high detection accuracy while being resilient to connection disruption.","sentences":["This study introduces a robust solution for the detection of Distributed Denial of Service (DDoS) attacks in Internet of Things (IoT) systems, leveraging the capabilities of Graph Convolutional Networks (GCN).","By conceptualizing IoT devices as nodes within a graph structure, we present a detection mechanism capable of operating efficiently even in lossy network environments.","We introduce various graph topologies for modeling IoT networks and evaluate them for detecting tunable futuristic DDoS attacks.","By studying different levels of network connection loss and various attack situations, we demonstrate that the correlation-based hybrid graph structure is effective in spotting DDoS attacks, substantiating its good performance even in lossy network scenarios.","The results indicate a remarkable performance of the GCN-based DDoS detection model with an F1 score of up to 91%.","Furthermore, we observe at most a 2% drop in F1-score in environments with up to 50% connection loss.","The findings from this study highlight the advantages of utilizing GCN for the security of IoT systems which benefit from high detection accuracy while being resilient to connection disruption."],"url":"http://arxiv.org/abs/2403.09118v1","category":"cs.CR"}
{"created":"2024-03-14 05:40:23","title":"Randomized Principal Component Analysis for Hyperspectral Image Classification","abstract":"The high-dimensional feature space of the hyperspectral imagery poses major challenges to the processing and analysis of the hyperspectral data sets. In such a case, dimensionality reduction is necessary to decrease the computational complexity. The random projections open up new ways of dimensionality reduction, especially for large data sets. In this paper, the principal component analysis (PCA) and randomized principal component analysis (R-PCA) for the classification of hyperspectral images using support vector machines (SVM) and light gradient boosting machines (LightGBM) have been investigated. In this experimental research, the number of features was reduced to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and Pavia University). The experimental results demonstrated that PCA outperformed R-PCA for SVM for both datasets, but received close accuracy values for LightGBM. The highest classification accuracies were obtained as 0.9925 and 0.9639 by LightGBM with original features for the Pavia University and Indian Pines, respectively.","sentences":["The high-dimensional feature space of the hyperspectral imagery poses major challenges to the processing and analysis of the hyperspectral data sets.","In such a case, dimensionality reduction is necessary to decrease the computational complexity.","The random projections open up new ways of dimensionality reduction, especially for large data sets.","In this paper, the principal component analysis (PCA) and randomized principal component analysis (R-PCA) for the classification of hyperspectral images using support vector machines (SVM) and light gradient boosting machines (LightGBM) have been investigated.","In this experimental research, the number of features was reduced to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and Pavia University).","The experimental results demonstrated that PCA outperformed R-PCA for SVM for both datasets, but received close accuracy values for LightGBM.","The highest classification accuracies were obtained as 0.9925 and 0.9639 by LightGBM with original features for the Pavia University and Indian Pines, respectively."],"url":"http://arxiv.org/abs/2403.09117v1","category":"eess.IV"}
{"created":"2024-03-14 05:37:33","title":"Frustrated Quantum Magnetism on Complex Networks: What Sets the Total Spin","abstract":"Consider equal antiferromagnetic Heisenberg interactions between qubits sitting at the nodes of a complex, nonbipartite network. We ask the question: How does the network topology determine the net magnetization of the ground state and to what extent is it tunable? By examining various network families with tunable properties, we demonstrate that (i) graph heterogeneity, i.e., spread in the number of neighbors, is essential for a nonzero total spin, and (ii) other than the average number of neighbors, the key structure governing the total spin is the presence of (disassortative) hubs, as opposed to the level of frustration. We also show how to construct simple networks where the magnetization can be tuned over its entire range across both abrupt and continuous transitions, which may be realizable on existing platforms. Our findings pose a number of fundamental questions and strongly motivate wider exploration of quantum many-body phenomena beyond regular lattices.","sentences":["Consider equal antiferromagnetic Heisenberg interactions between qubits sitting at the nodes of a complex, nonbipartite network.","We ask the question: How does the network topology determine the net magnetization of the ground state and to what extent is it tunable?","By examining various network families with tunable properties, we demonstrate that (i) graph heterogeneity, i.e., spread in the number of neighbors, is essential for a nonzero total spin, and (ii) other than the average number of neighbors, the key structure governing the total spin is the presence of (disassortative) hubs, as opposed to the level of frustration.","We also show how to construct simple networks where the magnetization can be tuned over its entire range across both abrupt and continuous transitions, which may be realizable on existing platforms.","Our findings pose a number of fundamental questions and strongly motivate wider exploration of quantum many-body phenomena beyond regular lattices."],"url":"http://arxiv.org/abs/2403.09116v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-14 05:19:34","title":"Partitioning Distribution Networks for Integrated Electrification Planning","abstract":"In many developing countries, access to electricity remains a significant challenge. Electrification planners in these countries often have to make important decisions on the mode of electrification and the planning of electrical networks for those without access, while under resource constraints. An integrated approach to electrification planning in which traditional grid electrification is complemented off-the-grid technologies such as off-grid microgrids and stand-alone systems can enable the economic provision of electricity access in these regions. This integrated planning approach can be facilitated by determining the least-cost mode of electrification - i.e by electric grid extension or off-grid systems - for non-electrified consumers in a region under analysis, while considering technical, economic and environmental constraints. Computational clustering methods the identification of consumer clusters (either as clusters of off-grid microgrids, stand-alone systems or grid-extension projects) can be undertaken using computational clustering methods. This paper presents a novel computational approach to achieve this purpose. This methodology involves exploiting the grid network that connects all consumers, by greedily partitioning the network to identify clusters of consumers to be electrified by grid-extension and off-grid microgrid systems. Using test cases and sensitivity analyses, we implement and benchmark this top-down approach with those obtained from a bottom-up clustering methodology used by the Reference Electrification Model, a model obtainable in literature. Results presented show that the alternative top-down methodology proposed can compare favorably, in terms of global electrification costs, with a bottom-up approach to rural electrification planning.","sentences":["In many developing countries, access to electricity remains a significant challenge.","Electrification planners in these countries often have to make important decisions on the mode of electrification and the planning of electrical networks for those without access, while under resource constraints.","An integrated approach to electrification planning in which traditional grid electrification is complemented off-the-grid technologies such as off-grid microgrids and stand-alone systems can enable the economic provision of electricity access in these regions.","This integrated planning approach can be facilitated by determining the least-cost mode of electrification - i.e by electric grid extension or off-grid systems - for non-electrified consumers in a region under analysis, while considering technical, economic and environmental constraints.","Computational clustering methods the identification of consumer clusters (either as clusters of off-grid microgrids, stand-alone systems or grid-extension projects) can be undertaken using computational clustering methods.","This paper presents a novel computational approach to achieve this purpose.","This methodology involves exploiting the grid network that connects all consumers, by greedily partitioning the network to identify clusters of consumers to be electrified by grid-extension and off-grid microgrid systems.","Using test cases and sensitivity analyses, we implement and benchmark this top-down approach with those obtained from a bottom-up clustering methodology used by the Reference Electrification Model, a model obtainable in literature.","Results presented show that the alternative top-down methodology proposed can compare favorably, in terms of global electrification costs, with a bottom-up approach to rural electrification planning."],"url":"http://arxiv.org/abs/2403.09111v1","category":"eess.SY"}
{"created":"2024-03-14 05:17:39","title":"SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning","abstract":"Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow. However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications. In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems. Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime. In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpretable, and trustworthy representations of the dynamics model, reward function, and control policy. We demonstrate the effectiveness of our approaches on benchmark control environments and challenging fluids problems. SINDy-RL achieves comparable performance to state-of-the-art DRL algorithms using significantly fewer interactions in the environment and results in an interpretable control policy orders of magnitude smaller than a deep neural network policy.","sentences":["Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow.","However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications.","In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems.","Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime.","In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpretable, and trustworthy representations of the dynamics model, reward function, and control policy.","We demonstrate the effectiveness of our approaches on benchmark control environments and challenging fluids problems.","SINDy-RL achieves comparable performance to state-of-the-art DRL algorithms using significantly fewer interactions in the environment and results in an interpretable control policy orders of magnitude smaller than a deep neural network policy."],"url":"http://arxiv.org/abs/2403.09110v1","category":"cs.LG"}
{"created":"2024-03-14 05:09:22","title":"Temperature and Tautomeric Effects in High-Resolution Oxygen 1s X-ray Photoelectron Spectroscopy of Purines and Pyrimidines","abstract":"Purines and pyrimidines, crucial building blocks in biological systems, have attracted significant interest across molecular physics, biochemistry, pharmacology, and chemistry. Extensive spectroscopies have been employed for characterization, while the temperature and potential tautomeric effects can complicate the interpretation of underlying physics and chemistry. Here, we conducted first-principles simulations to analyze the vibrationally-resolved O1s X-ray photoelectron spectra of 6 common biomolecules at different temperatures, comprising 3 purine (xanthine, caffeine, and hypoxanthine) and 3 pyrimidine (thymine, 5F-uracil, and uracil) derivatives, and the tautomeric effect of hypoxanthine at varying temperatures. Using both time-independent (TI) and time-dependent (TD) methods under the Franck-Condon approximation, we obtained theoretical spectra that exhibited excellent agreement with experiments. Our analysis of these systems, all featuring carbonyl oxygens, unveiled distinctive characteristics of oxygen in N-CO-N (O2) compared to that within a N-CO-C structure (O1), showcasing higher O1s binding energy and total vibrational reorganization energy. We observed small differences in the zero-point vibration energies between the core-ionized and ground states, indicating a weak Duschinsky rotation effect. We consistently found that O1s ionization resulted in elongation of the O*=C bond length. The TI method facilitated the assignment of experimental spectra to different atoms or tautomers, where the atom-specific vibronic profiles of all 6 molecules exhibited similarity, with the 0-2 transitions dominating. TD enabled a more comprehensive exploration of the temperature effect, and the tautomeric effect of hypoxanthine by incorporating the Boltzmann population ratios of tautomers. We observed significant temperature dependence in the vibronic features present in these spectra.","sentences":["Purines and pyrimidines, crucial building blocks in biological systems, have attracted significant interest across molecular physics, biochemistry, pharmacology, and chemistry.","Extensive spectroscopies have been employed for characterization, while the temperature and potential tautomeric effects can complicate the interpretation of underlying physics and chemistry.","Here, we conducted first-principles simulations to analyze the vibrationally-resolved O1s X-ray photoelectron spectra of 6 common biomolecules at different temperatures, comprising 3 purine (xanthine, caffeine, and hypoxanthine) and 3 pyrimidine (thymine, 5F-uracil, and uracil) derivatives, and the tautomeric effect of hypoxanthine at varying temperatures.","Using both time-independent (TI) and time-dependent (TD) methods under the Franck-Condon approximation, we obtained theoretical spectra that exhibited excellent agreement with experiments.","Our analysis of these systems, all featuring carbonyl oxygens, unveiled distinctive characteristics of oxygen in N-CO-N (O2) compared to that within a N-CO-C structure (O1), showcasing higher O1s binding energy and total vibrational reorganization energy.","We observed small differences in the zero-point vibration energies between the core-ionized and ground states, indicating a weak Duschinsky rotation effect.","We consistently found that O1s ionization resulted in elongation of the O*=C bond length.","The TI method facilitated the assignment of experimental spectra to different atoms or tautomers, where the atom-specific vibronic profiles of all 6 molecules exhibited similarity, with the 0-2 transitions dominating.","TD enabled a more comprehensive exploration of the temperature effect, and the tautomeric effect of hypoxanthine by incorporating the Boltzmann population ratios of tautomers.","We observed significant temperature dependence in the vibronic features present in these spectra."],"url":"http://arxiv.org/abs/2403.09109v1","category":"physics.chem-ph"}
{"created":"2024-03-14 05:01:31","title":"CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification","abstract":"Capsule Neural Networks (CapsNets) is a novel architecture that utilizes vector-wise representations formed by multiple neurons. Specifically, the Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic routing mechanism to train capsules and acquire translation-equivariance properties, enhancing its robustness compared to traditional Convolutional Neural Networks (CNNs). Echocardiograms, which capture moving images of the heart, present unique challenges for traditional image classification methods. In this paper, we explore the potential of DR-CapsNets and propose CardioCaps, a novel attention-based DR-CapsNet architecture for class-imbalanced echocardiogram classification. CardioCaps comprises two key components: a weighted margin loss incorporating a regression auxiliary loss and an attention mechanism. First, the weighted margin loss prioritizes positive cases, supplemented by an auxiliary loss function based on the Ejection Fraction (EF) regression task, a crucial measure of cardiac function. This approach enhances the model's resilience in the face of class imbalance. Second, recognizing the quadratic complexity of dynamic routing leading to training inefficiencies, we adopt the attention mechanism as a more computationally efficient alternative. Our results demonstrate that CardioCaps surpasses traditional machine learning baseline methods, including Logistic Regression, Random Forest, and XGBoost with sampling methods and a class weight matrix. Furthermore, CardioCaps outperforms other deep learning baseline methods such as CNNs, ResNets, U-Nets, and ViTs, as well as advanced CapsNets methods such as EM-CapsNets and Efficient-CapsNets. Notably, our model demonstrates robustness to class imbalance, achieving high precision even in datasets with a substantial proportion of negative cases.","sentences":["Capsule Neural Networks (CapsNets) is a novel architecture that utilizes vector-wise representations formed by multiple neurons.","Specifically, the Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic routing mechanism to train capsules and acquire translation-equivariance properties, enhancing its robustness compared to traditional Convolutional Neural Networks (CNNs).","Echocardiograms, which capture moving images of the heart, present unique challenges for traditional image classification methods.","In this paper, we explore the potential of DR-CapsNets and propose CardioCaps, a novel attention-based DR-CapsNet architecture for class-imbalanced echocardiogram classification.","CardioCaps comprises two key components: a weighted margin loss incorporating a regression auxiliary loss and an attention mechanism.","First, the weighted margin loss prioritizes positive cases, supplemented by an auxiliary loss function based on the Ejection Fraction (EF) regression task, a crucial measure of cardiac function.","This approach enhances the model's resilience in the face of class imbalance.","Second, recognizing the quadratic complexity of dynamic routing leading to training inefficiencies, we adopt the attention mechanism as a more computationally efficient alternative.","Our results demonstrate that CardioCaps surpasses traditional machine learning baseline methods, including Logistic Regression, Random Forest, and XGBoost with sampling methods and a class weight matrix.","Furthermore, CardioCaps outperforms other deep learning baseline methods such as CNNs, ResNets, U-Nets, and ViTs, as well as advanced CapsNets methods such as EM-CapsNets and Efficient-CapsNets.","Notably, our model demonstrates robustness to class imbalance, achieving high precision even in datasets with a substantial proportion of negative cases."],"url":"http://arxiv.org/abs/2403.09108v1","category":"cs.CV"}
{"created":"2024-03-14 04:51:17","title":"Alternant Hydrocarbon Diradicals as Optically Addressable Molecular Qubits","abstract":"High-spin molecules allow for bottom-up qubit design and are promising platforms for magnetic sensing and quantum information science. Optical addressability of molecular electron spins has also been proposed in first-row transition metal complexes via optically-detected magnetic resonance (ODMR) mechanisms analogous to the diamond-NV colour centre. However, significantly less progress has been made on the front of metal-free molecules, which can deliver lower costs and milder environmental impacts. At present, most luminescent open-shell organic molecules are {\\pi}-diradicals, but such systems often suffer from poor ground-state open-shell characters necessary to realise a stable molecular qubit. In this work, we propose the use of alternancy symmetry to selectively minimise radical-radical interactions in the ground state, generating {\\pi}-systems with high diradical characters. We call them m-dimers, referencing the need to covalently link two benzylic radicals at their meta carbon atoms for the desired symmetry. Through a detailed electronic structure analysis, we find that the excited states of alternant hydrocarbon m-diradicals contain important symmetries that can be used to construct ODMR mechanisms. The molecular parameters are set in the context of a tris(2,4,6-trichlorophenyl)methyl (TTM) radical dimer covalently tethered at the meta position, demonstrating the feasibility of realising a molecular colour centre with alternant {\\pi}-diradicals.","sentences":["High-spin molecules allow for bottom-up qubit design and are promising platforms for magnetic sensing and quantum information science.","Optical addressability of molecular electron spins has also been proposed in first-row transition metal complexes via optically-detected magnetic resonance (ODMR) mechanisms analogous to the diamond-NV colour centre.","However, significantly less progress has been made on the front of metal-free molecules, which can deliver lower costs and milder environmental impacts.","At present, most luminescent open-shell organic molecules are {\\pi}-diradicals, but such systems often suffer from poor ground-state open-shell characters necessary to realise a stable molecular qubit.","In this work, we propose the use of alternancy symmetry to selectively minimise radical-radical interactions in the ground state, generating {\\pi}-systems with high diradical characters.","We call them m-dimers, referencing the need to covalently link two benzylic radicals at their meta carbon atoms for the desired symmetry.","Through a detailed electronic structure analysis, we find that the excited states of alternant hydrocarbon m-diradicals contain important symmetries that can be used to construct ODMR mechanisms.","The molecular parameters are set in the context of a tris(2,4,6-trichlorophenyl)methyl (TTM) radical dimer covalently tethered at the meta position, demonstrating the feasibility of realising a molecular colour centre with alternant {\\pi}-diradicals."],"url":"http://arxiv.org/abs/2403.09102v1","category":"physics.chem-ph"}
{"created":"2024-03-14 04:48:06","title":"Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning","abstract":"Systemic amyloidosis is a group of diseases characterized by the deposition of misfolded proteins in various organs and tissues, leading to progressive organ dysfunction and failure. Congo red stain is the gold standard chemical stain for the visualization of amyloid deposits in tissue sections, as it forms complexes with the misfolded proteins and shows a birefringence pattern under polarized light microscopy. However, Congo red staining is tedious and costly to perform, and prone to false diagnoses due to variations in the amount of amyloid, staining quality and expert interpretation through manual examination of tissue under a polarization microscope. Here, we report the first demonstration of virtual birefringence imaging and virtual Congo red staining of label-free human tissue to show that a single trained neural network can rapidly transform autofluorescence images of label-free tissue sections into brightfield and polarized light microscopy equivalent images, matching the histochemically stained versions of the same samples. We demonstrate the efficacy of our method with blind testing and pathologist evaluations on cardiac tissue where the virtually stained images agreed well with the histochemically stained ground truth images. Our virtually stained polarization and brightfield images highlight amyloid birefringence patterns in a consistent, reproducible manner while mitigating diagnostic challenges due to variations in the quality of chemical staining and manual imaging processes as part of the clinical workflow.","sentences":["Systemic amyloidosis is a group of diseases characterized by the deposition of misfolded proteins in various organs and tissues, leading to progressive organ dysfunction and failure.","Congo red stain is the gold standard chemical stain for the visualization of amyloid deposits in tissue sections, as it forms complexes with the misfolded proteins and shows a birefringence pattern under polarized light microscopy.","However, Congo red staining is tedious and costly to perform, and prone to false diagnoses due to variations in the amount of amyloid, staining quality and expert interpretation through manual examination of tissue under a polarization microscope.","Here, we report the first demonstration of virtual birefringence imaging and virtual Congo red staining of label-free human tissue to show that a single trained neural network can rapidly transform autofluorescence images of label-free tissue sections into brightfield and polarized light microscopy equivalent images, matching the histochemically stained versions of the same samples.","We demonstrate the efficacy of our method with blind testing and pathologist evaluations on cardiac tissue where the virtually stained images agreed well with the histochemically stained ground truth images.","Our virtually stained polarization and brightfield images highlight amyloid birefringence patterns in a consistent, reproducible manner while mitigating diagnostic challenges due to variations in the quality of chemical staining and manual imaging processes as part of the clinical workflow."],"url":"http://arxiv.org/abs/2403.09100v1","category":"physics.med-ph"}
{"created":"2024-03-14 17:56:14","title":"Score-Guided Diffusion for 3D Human Recovery","abstract":"We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for solving inverse problems for 3D human pose and shape reconstruction. These inverse problems involve fitting a human body model to image observations, traditionally solved through optimization techniques. ScoreHMR mimics model fitting approaches, but alignment with the image observation is achieved through score guidance in the latent space of a diffusion model. The diffusion model is trained to capture the conditional distribution of the human model parameters given an input image. By guiding its denoising process with a task-specific score, ScoreHMR effectively solves inverse problems for various applications without the need for retraining the task-agnostic diffusion model. We evaluate our approach on three settings/applications. These are: (i) single-frame model fitting; (ii) reconstruction from multiple uncalibrated views; (iii) reconstructing humans in video sequences. ScoreHMR consistently outperforms all optimization baselines on popular benchmarks across all settings. We make our code and models available at the https://statho.github.io/ScoreHMR.","sentences":["We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for solving inverse problems for 3D human pose and shape reconstruction.","These inverse problems involve fitting a human body model to image observations, traditionally solved through optimization techniques.","ScoreHMR mimics model fitting approaches, but alignment with the image observation is achieved through score guidance in the latent space of a diffusion model.","The diffusion model is trained to capture the conditional distribution of the human model parameters given an input image.","By guiding its denoising process with a task-specific score, ScoreHMR effectively solves inverse problems for various applications without the need for retraining the task-agnostic diffusion model.","We evaluate our approach on three settings/applications.","These are: (i) single-frame model fitting; (ii) reconstruction from multiple uncalibrated views; (iii) reconstructing humans in video sequences.","ScoreHMR consistently outperforms all optimization baselines on popular benchmarks across all settings.","We make our code and models available at the https://statho.github.io/ScoreHMR."],"url":"http://arxiv.org/abs/2403.09623v1","category":"cs.CV"}
{"created":"2024-03-14 17:17:16","title":"High-dimensional expansion and soficity of groups","abstract":"For $d \\geq 4$ and $p$ a sufficiently large prime, we construct a lattice $\\Gamma \\leq {\\rm PSp}_{2d}(\\mathbb Q_p),$ such that its universal central extension cannot be sofic if $\\Gamma$ satisfies some weak form of stability in permutations. In the proof, we make use of high-dimensional expansion phenomena and, extending results of Lubotzky, we construct new examples of cosystolic expanders over arbitrary finite abelian groups.","sentences":["For $d \\geq 4$ and $p$ a sufficiently large prime, we construct a lattice $\\Gamma \\leq {\\rm PSp}_{2d}(\\mathbb Q_p),$ such that its universal central extension cannot be sofic if $\\Gamma$ satisfies some weak form of stability in permutations.","In the proof, we make use of high-dimensional expansion phenomena and, extending results of Lubotzky, we construct new examples of cosystolic expanders over arbitrary finite abelian groups."],"url":"http://arxiv.org/abs/2403.09582v1","category":"math.GR"}
{"created":"2024-03-14 17:11:49","title":"The NeRFect Match: Exploring NeRF Features for Visual Localization","abstract":"In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks.","sentences":["In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization.","Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module.","We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization.","To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions.","This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations.","Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis.","Our evaluation of NeRFMatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks."],"url":"http://arxiv.org/abs/2403.09577v1","category":"cs.CV"}
{"created":"2024-03-14 16:39:23","title":"\"Are You Really Sure?\" Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making","abstract":"In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI. This paper approaches this problem from a human-centered perspective, \"human self-confidence calibration\". We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence. In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness. Then in our second study, We propose three calibration mechanisms and compare their effects on humans' self-confidence and user experience. Subsequently, our third study investigates the effects of self-confidence calibration on AI-assisted decision-making. Results show that calibrating human self-confidence enhances human-AI team performance and encourages more rational reliance on AI (in some aspects) compared to uncalibrated baselines. Finally, we discuss our main findings and provide implications for designing future AI-assisted decision-making interfaces.","sentences":["In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI.","This paper approaches this problem from a human-centered perspective, \"human self-confidence calibration\".","We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence.","In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness.","Then in our second study, We propose three calibration mechanisms and compare their effects on humans' self-confidence and user experience.","Subsequently, our third study investigates the effects of self-confidence calibration on AI-assisted decision-making.","Results show that calibrating human self-confidence enhances human-AI team performance and encourages more rational reliance on AI (in some aspects) compared to uncalibrated baselines.","Finally, we discuss our main findings and provide implications for designing future AI-assisted decision-making interfaces."],"url":"http://arxiv.org/abs/2403.09552v1","category":"cs.HC"}
{"created":"2024-03-14 16:39:11","title":"WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity","abstract":"Weakly supervised surgical instrument segmentation with only instrument presence labels has been rarely explored in surgical domain. To mitigate the highly under-constrained challenges, we extend a two-stage weakly supervised segmentation paradigm with temporal attributes from two perspectives. From a temporal equivariance perspective, we propose a prototype-based temporal equivariance regulation loss to enhance pixel-wise consistency between adjacent features. From a semantic continuity perspective, we propose a class-aware temporal semantic continuity loss to constrain the semantic consistency between a global view of target frame and local non-discriminative regions of adjacent reference frame. To the best of our knowledge, WeakSurg is the first instrument-presence-only weakly supervised segmentation architecture to take temporal information into account for surgical scenarios. Extensive experiments are validated on Cholec80, an open benchmark for phase and instrument recognition. We annotate instance-wise instrument labels with fixed time-steps which are double checked by a clinician with 3-years experience. Our results show that WeakSurg compares favorably with state-of-the-art methods not only on semantic segmentation metrics but also on instance segmentation metrics.","sentences":["Weakly supervised surgical instrument segmentation with only instrument presence labels has been rarely explored in surgical domain.","To mitigate the highly under-constrained challenges, we extend a two-stage weakly supervised segmentation paradigm with temporal attributes from two perspectives.","From a temporal equivariance perspective, we propose a prototype-based temporal equivariance regulation loss to enhance pixel-wise consistency between adjacent features.","From a semantic continuity perspective, we propose a class-aware temporal semantic continuity loss to constrain the semantic consistency between a global view of target frame and local non-discriminative regions of adjacent reference frame.","To the best of our knowledge, WeakSurg is the first instrument-presence-only weakly supervised segmentation architecture to take temporal information into account for surgical scenarios.","Extensive experiments are validated on Cholec80, an open benchmark for phase and instrument recognition.","We annotate instance-wise instrument labels with fixed time-steps which are double checked by a clinician with 3-years experience.","Our results show that WeakSurg compares favorably with state-of-the-art methods not only on semantic segmentation metrics but also on instance segmentation metrics."],"url":"http://arxiv.org/abs/2403.09551v1","category":"cs.CV"}
{"created":"2024-03-14 16:34:30","title":"de Leeuw representations of functionals on Lipschitz spaces","abstract":"Let $\\mathrm{Lip}_0(M)$ be the space of Lipschitz functions on a complete metric space $(M,d)$ that vanish at a point $0\\in M$. We investigate its dual $\\mathrm{Lip}_0(M)^*$ using the de Leeuw transform, which allows representing each functional on $\\mathrm{Lip}_0(M)$ as a (non-unique) measure on $\\beta\\widetilde{M}$, where $\\widetilde{M}$ is the space of pairs $(x,y)\\in M\\times M$, $x\\neq y$. We distinguish a set of points of $\\beta\\widetilde{M}$ that are \"away from infinity\", which can be assigned coordinates belonging to the Lipschitz realcompactification $M^{\\mathcal{R}}$ of $M$. We define a natural metric $\\bar{d}$ on $M^{\\mathcal{R}}$ extending $d$ and we show that optimal (i.e. positive and norm-minimal) de Leeuw representations of well-behaved functionals are characterised by $\\bar{d}$-cyclical monotonicity of their support, extending known results for functionals in $\\mathcal{F}(M)$, the predual of $\\mathrm{Lip}_0(M)$. We also extend the Kantorovich-Rubinstein theorem to normal Hausdorff spaces, in particular to $M^{\\mathcal{R}}$, and use this to characterise measure-induced and majorisable functionals in $\\mathrm{Lip}_0(M)^*$ as those admitting optimal representations with additional finiteness properties. Finally, we use de Leeuw representations to define a natural L-projection of $\\mathrm{Lip}_0(M)^*$ onto $\\mathcal{F}(M)$ under some conditions on $M$.","sentences":["Let $\\mathrm{Lip}_0(M)$ be the space of Lipschitz functions on a complete metric space $(M,d)$ that vanish at a point $0\\in M$. We investigate its dual $\\mathrm{Lip}_0(M)^*$ using the de Leeuw transform, which allows representing each functional on $\\mathrm{Lip}_0(M)$ as a (non-unique) measure on $\\beta\\widetilde{M}$, where $\\widetilde{M}$ is the space of pairs $(x,y)\\in M\\times M$, $x\\neq y$. We distinguish a set of points of $\\beta\\widetilde{M}$ that are \"away from infinity\", which can be assigned coordinates belonging to the Lipschitz realcompactification $M^{\\mathcal{R}}$ of $M$. We define a natural metric $\\bar{d}$ on $M^{\\mathcal{R}}$ extending $d$ and we show that optimal (i.e. positive and norm-minimal) de Leeuw representations of well-behaved functionals are characterised by $\\bar{d}$-cyclical monotonicity of their support, extending known results for functionals in $\\mathcal{F}(M)$, the predual of $\\mathrm{Lip}_0(M)$. We also extend the Kantorovich-Rubinstein theorem to normal Hausdorff spaces, in particular to $M^{\\mathcal{R}}$, and use this to characterise measure-induced and majorisable functionals in $\\mathrm{Lip}_0(M)^*$ as those admitting optimal representations with additional finiteness properties.","Finally, we use de Leeuw representations to define a natural L-projection of $\\mathrm{Lip}_0(M)^*$ onto $\\mathcal{F}(M)$ under some conditions on $M$."],"url":"http://arxiv.org/abs/2403.09546v1","category":"math.FA"}
{"created":"2024-03-14 16:32:29","title":"Sequential Contracts","abstract":"We study the principal-agent setting, where a principal delegates the execution of a costly project to an agent. In the classical model, the agent chooses an action among a set of available actions. Every action is associated with some cost, and leads to a stochastic outcome for the project. The agent's action is hidden from the principal, who only observes the outcome. The principal incentivizes the agent through a payment scheme (a contract) that maps outcomes to payments, with the objective of finding the optimal contract - the contract maximizing the principal's expected utility.   In this work, we introduce a sequential variant of the model, capturing many real-life settings, where the agent engages in multiple attempts, incurring the sum of costs of the actions taken and being compensated for the best realized outcome. We study the contract design problem in this new setting. We first observe that the agent's problem - finding the sequential set of actions that maximizes his utility for a given contract - is equivalent to the well-known Pandora's Box problem. With this insight at hand, we provide algorithms and hardness results for the (principal's) contract design problem, under both independent and correlated actions. For independent actions, we show that the optimal linear contract can be computed in polynomial time. Furthermore, this result extends to the optimal arbitrary contract when the number of outcomes is a constant. For correlated actions we find that approximating the optimal contract within any constant ratio is NP-hard.","sentences":["We study the principal-agent setting, where a principal delegates the execution of a costly project to an agent.","In the classical model, the agent chooses an action among a set of available actions.","Every action is associated with some cost, and leads to a stochastic outcome for the project.","The agent's action is hidden from the principal, who only observes the outcome.","The principal incentivizes the agent through a payment scheme (a contract) that maps outcomes to payments, with the objective of finding the optimal contract - the contract maximizing the principal's expected utility.   ","In this work, we introduce a sequential variant of the model, capturing many real-life settings, where the agent engages in multiple attempts, incurring the sum of costs of the actions taken and being compensated for the best realized outcome.","We study the contract design problem in this new setting.","We first observe that the agent's problem - finding the sequential set of actions that maximizes his utility for a given contract - is equivalent to the well-known Pandora's Box problem.","With this insight at hand, we provide algorithms and hardness results for the (principal's) contract design problem, under both independent and correlated actions.","For independent actions, we show that the optimal linear contract can be computed in polynomial time.","Furthermore, this result extends to the optimal arbitrary contract when the number of outcomes is a constant.","For correlated actions we find that approximating the optimal contract within any constant ratio is NP-hard."],"url":"http://arxiv.org/abs/2403.09545v1","category":"cs.GT"}
{"created":"2024-03-14 16:31:18","title":"Effect of external characteristics of a virtual human being during the use of a computer-assisted therapy tool","abstract":"Identification within media, whether with real or fictional characters, significantly impacts users, shaping their behavior and enriching their social and emotional experiences. Immersive media, like video games, utilize virtual entities such as agents, avatars, or NPCs to connect users with virtual worlds, fostering a heightened sense of immersion and identification. However, challenges arise in visually representing these entities, with design decisions crucial for enhancing user interaction. Recent research highlights the potential of user-defined design, or customization, which goes beyond mere visual resemblance to the user. Understanding how identification with virtual avatars influences user experiences, especially in psychological interventions, is pivotal. In a study exploring this, 22 participants created virtual agents either similar or dissimilar to themselves, which then addressed their dysfunctional thoughts. Results indicate that similarity between users and virtual agents not only boosts identification but also positively impacts emotions and motivation, enhancing interest and enjoyment. This study sheds light on the significance of customization and identification, particularly in computer-assisted therapy tools, underscoring the importance of visual design for optimizing user experiences.","sentences":["Identification within media, whether with real or fictional characters, significantly impacts users, shaping their behavior and enriching their social and emotional experiences.","Immersive media, like video games, utilize virtual entities such as agents, avatars, or NPCs to connect users with virtual worlds, fostering a heightened sense of immersion and identification.","However, challenges arise in visually representing these entities, with design decisions crucial for enhancing user interaction.","Recent research highlights the potential of user-defined design, or customization, which goes beyond mere visual resemblance to the user.","Understanding how identification with virtual avatars influences user experiences, especially in psychological interventions, is pivotal.","In a study exploring this, 22 participants created virtual agents either similar or dissimilar to themselves, which then addressed their dysfunctional thoughts.","Results indicate that similarity between users and virtual agents not only boosts identification but also positively impacts emotions and motivation, enhancing interest and enjoyment.","This study sheds light on the significance of customization and identification, particularly in computer-assisted therapy tools, underscoring the importance of visual design for optimizing user experiences."],"url":"http://arxiv.org/abs/2403.09544v1","category":"cs.HC"}
{"created":"2024-03-14 16:21:32","title":"Robust SGLD algorithm for solving non-convex distributionally robust optimisation problems","abstract":"In this paper we develop a Stochastic Gradient Langevin Dynamics (SGLD) algorithm tailored for solving a certain class of non-convex distributionally robust optimisation problems. By deriving non-asymptotic convergence bounds, we build an algorithm which for any prescribed accuracy $\\varepsilon>0$ outputs an estimator whose expected excess risk is at most $\\varepsilon$. As a concrete application, we employ our robust SGLD algorithm to solve the (regularised) distributionally robust Mean-CVaR portfolio optimisation problem using real financial data. We empirically demonstrate that the trading strategy obtained by our robust SGLD algorithm outperforms the trading strategy obtained when solving the corresponding non-robust Mean-CVaR portfolio optimisation problem using, e.g., a classical SGLD algorithm. This highlights the practical relevance of incorporating model uncertainty when optimising portfolios in real financial markets.","sentences":["In this paper we develop a Stochastic Gradient Langevin Dynamics (SGLD) algorithm tailored for solving a certain class of non-convex distributionally robust optimisation problems.","By deriving non-asymptotic convergence bounds, we build an algorithm which for any prescribed accuracy $\\varepsilon>0$ outputs an estimator whose expected excess risk is at most $\\varepsilon$. As a concrete application, we employ our robust SGLD algorithm to solve the (regularised) distributionally robust Mean-CVaR portfolio optimisation problem using real financial data.","We empirically demonstrate that the trading strategy obtained by our robust SGLD algorithm outperforms the trading strategy obtained when solving the corresponding non-robust Mean-CVaR portfolio optimisation problem using, e.g., a classical SGLD algorithm.","This highlights the practical relevance of incorporating model uncertainty when optimising portfolios in real financial markets."],"url":"http://arxiv.org/abs/2403.09532v1","category":"math.OC"}
{"created":"2024-03-14 16:02:01","title":"About Berge-F\u00fcredi's conjecture on the chromatic index of hypergraphs","abstract":"We show that the chromatic index of a hypergraph $\\mathcal{H}$ satisfies Berge-F\\\"uredi conjectured bound $\\mathrm{q}(\\mathcal{H})\\le \\Delta([\\mathcal{H}]_2)+1$ under certain hypotheses on the antirank $\\mathrm{ar}(\\mathcal{H})$ or on the maximum degree $\\Delta(\\mathcal{H})$. This provides sharp information in connection with Erd\\H{o}s-Faber-Lov\\'asz Conjecture which deals with the coloring of a family of cliques that intersect pairwise in at most one vertex.","sentences":["We show that the chromatic index of a hypergraph $\\mathcal{H}$ satisfies Berge-F\\\"uredi conjectured bound $\\mathrm{q}(\\mathcal{H})\\le \\Delta([\\mathcal{H}]_2)+1$ under certain hypotheses on the antirank $\\mathrm{ar}(\\mathcal{H})$ or on the maximum degree $\\Delta(\\mathcal{H})$. This provides sharp information in connection with Erd\\H{o}s-Faber-Lov\\'asz Conjecture which deals with the coloring of a family of cliques that intersect pairwise in at most one vertex."],"url":"http://arxiv.org/abs/2403.09518v1","category":"math.CO"}
{"created":"2024-03-14 15:54:29","title":"Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase","abstract":"Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. However, compared to code defect detection, code revert prediction has been rarely studied in previous research. Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features. Different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection. We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.","sentences":["Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development.","This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes.","However, compared to code defect detection, code revert prediction has been rarely studied in previous research.","Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts.","Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase.","To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import graph with code features.","Different strategies to address anomalies and data imbalance have been implemented including graph neural networks with imbalance classification and anomaly detection.","We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem."],"url":"http://arxiv.org/abs/2403.09507v1","category":"cs.SE"}
{"created":"2024-03-14 15:35:07","title":"Anomaly Detection by Adapting a pre-trained Vision Language Model","abstract":"Recently, large vision and language models have shown their success when adapting them to many downstream tasks. In this paper, we present a unified framework named CLIP-ADA for Anomaly Detection by Adapting a pre-trained CLIP model. To this end, we make two important improvements: 1) To acquire unified anomaly detection across industrial images of multiple categories, we introduce the learnable prompt and propose to associate it with abnormal patterns through self-supervised learning. 2) To fully exploit the representation power of CLIP, we introduce an anomaly region refinement strategy to refine the localization quality. During testing, the anomalies are localized by directly calculating the similarity between the representation of the learnable prompt and the image. Comprehensive experiments demonstrate the superiority of our framework, e.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and VisA for anomaly detection and localization. In addition, the proposed method also achieves encouraging performance with marginal training data, which is more challenging.","sentences":["Recently, large vision and language models have shown their success when adapting them to many downstream tasks.","In this paper, we present a unified framework named CLIP-ADA for Anomaly Detection by Adapting a pre-trained CLIP model.","To this end, we make two important improvements: 1) To acquire unified anomaly detection across industrial images of multiple categories, we introduce the learnable prompt and propose to associate it with abnormal patterns through self-supervised learning.","2) To fully exploit the representation power of CLIP, we introduce an anomaly region refinement strategy to refine the localization quality.","During testing, the anomalies are localized by directly calculating the similarity between the representation of the learnable prompt and the image.","Comprehensive experiments demonstrate the superiority of our framework, e.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and VisA for anomaly detection and localization.","In addition, the proposed method also achieves encouraging performance with marginal training data, which is more challenging."],"url":"http://arxiv.org/abs/2403.09493v1","category":"cs.CV"}
{"created":"2024-03-14 14:46:59","title":"Twenty ways to estimate the Log Gaussian Cox Process model with point and aggregated case data: the rts2 package for R","abstract":"The R package rts2 provides data manipulation and model fitting tools for Log Gaussian Cox Process (LGCP) models. LGCP models are a key method for disease and other types of surveillance, and provide a means of predicting risk across an area of interest based on spatially-referenced and time-stamped case data. However, these models can be difficult to specify and computationally demanding to estimate. For many surveillance scenarios we require results in near real-time using routinely available data to guide and direct policy responses, or due to limited availability of computational resources. There are limited software implementations available for this real-time context with reliable predictions and quantification of uncertainty. The rts2 package provides a range of modern Gaussian process approximations and model fitting methods to fit the LGCP, including estimation of covariance parameters, using both Bayesian and stochastic Maximum Likelihood methods. The package provides a suite of data manipulation tools. We also provide a novel implementation to estimate the LGCP when case data are aggregated to an irregular grid such as census tract areas.","sentences":["The R package rts2 provides data manipulation and model fitting tools for Log Gaussian Cox Process (LGCP) models.","LGCP models are a key method for disease and other types of surveillance, and provide a means of predicting risk across an area of interest based on spatially-referenced and time-stamped case data.","However, these models can be difficult to specify and computationally demanding to estimate.","For many surveillance scenarios we require results in near real-time using routinely available data to guide and direct policy responses, or due to limited availability of computational resources.","There are limited software implementations available for this real-time context with reliable predictions and quantification of uncertainty.","The rts2 package provides a range of modern Gaussian process approximations and model fitting methods to fit the LGCP, including estimation of covariance parameters, using both Bayesian and stochastic Maximum Likelihood methods.","The package provides a suite of data manipulation tools.","We also provide a novel implementation to estimate the LGCP when case data are aggregated to an irregular grid such as census tract areas."],"url":"http://arxiv.org/abs/2403.09448v1","category":"stat.CO"}
{"created":"2024-03-14 14:45:32","title":"Charting Censorship Resilience & Global Internet Reachability: A Quantitative Approach","abstract":"Internet censorship and global Internet reachability are prevalent topics of today's Internet. Nonetheless, the impact of network topology and Internet architecture to these aspects of the Internet is under-explored. With the goal of informing policy discussions with an objective basis, we present an approach for evaluating both censorship resilience and global Internet reachability using quantitative network metrics, which are applicable to current BGP/IP networks and also to alternative Internet network architectures. We devise and instantiate the metric on the network topology of multiple countries, comparing the BGP/IP network, an overlay network using a waypoint mechanism for circumventing undesired nodes, and the path-aware Internet architecture SCION. The novelty of the approach resides in providing a metric enabling the analysis of these aspects of the Internet at the routing level, taking into account the innate properties of the routing protocol and architecture. We demonstrate that the Internet topology matters, and strongly influences both censorship resilience and reachability to the global Internet. Finally, we argue that access to multiple paths accompanied with path-awareness could enable a higher level of censorship resilience compared to the current Internet, and reduce the centralization of Internet routing.","sentences":["Internet censorship and global Internet reachability are prevalent topics of today's Internet.","Nonetheless, the impact of network topology and Internet architecture to these aspects of the Internet is under-explored.","With the goal of informing policy discussions with an objective basis, we present an approach for evaluating both censorship resilience and global Internet reachability using quantitative network metrics, which are applicable to current BGP/IP networks and also to alternative Internet network architectures.","We devise and instantiate the metric on the network topology of multiple countries, comparing the BGP/IP network, an overlay network using a waypoint mechanism for circumventing undesired nodes, and the path-aware Internet architecture SCION.","The novelty of the approach resides in providing a metric enabling the analysis of these aspects of the Internet at the routing level, taking into account the innate properties of the routing protocol and architecture.","We demonstrate that the Internet topology matters, and strongly influences both censorship resilience and reachability to the global Internet.","Finally, we argue that access to multiple paths accompanied with path-awareness could enable a higher level of censorship resilience compared to the current Internet, and reduce the centralization of Internet routing."],"url":"http://arxiv.org/abs/2403.09447v1","category":"cs.NI"}
{"created":"2024-03-14 14:34:25","title":"Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency","abstract":"As deep learning (DL) models are increasingly being integrated into our everyday lives, ensuring their safety by making them robust against adversarial attacks has become increasingly critical. DL models have been found to be susceptible to adversarial attacks which can be achieved by introducing small, targeted perturbations to disrupt the input data. Adversarial training has been presented as a mitigation strategy which can result in more robust models. This adversarial robustness comes with additional computational costs required to design adversarial attacks during training. The two objectives -- adversarial robustness and computational efficiency -- then appear to be in conflict of each other. In this work, we explore the effects of two different model compression methods -- structured weight pruning and quantization -- on adversarial robustness. We specifically explore the effects of fine-tuning on compressed models, and present the trade-off between standard fine-tuning and adversarial fine-tuning. Our results show that compression does not inherently lead to loss in model robustness and adversarial fine-tuning of a compressed model can yield large improvement to the robustness performance of models. We present experiments on two benchmark datasets showing that adversarial fine-tuning of compressed models can achieve robustness performance comparable to adversarially trained models, while also improving computational efficiency.","sentences":["As deep learning (DL) models are increasingly being integrated into our everyday lives, ensuring their safety by making them robust against adversarial attacks has become increasingly critical.","DL models have been found to be susceptible to adversarial attacks which can be achieved by introducing small, targeted perturbations to disrupt the input data.","Adversarial training has been presented as a mitigation strategy which can result in more robust models.","This adversarial robustness comes with additional computational costs required to design adversarial attacks during training.","The two objectives -- adversarial robustness and computational efficiency -- then appear to be in conflict of each other.","In this work, we explore the effects of two different model compression methods -- structured weight pruning and quantization -- on adversarial robustness.","We specifically explore the effects of fine-tuning on compressed models, and present the trade-off between standard fine-tuning and adversarial fine-tuning.","Our results show that compression does not inherently lead to loss in model robustness and adversarial fine-tuning of a compressed model can yield large improvement to the robustness performance of models.","We present experiments on two benchmark datasets showing that adversarial fine-tuning of compressed models can achieve robustness performance comparable to adversarially trained models, while also improving computational efficiency."],"url":"http://arxiv.org/abs/2403.09441v1","category":"cs.LG"}
{"created":"2024-03-14 14:23:23","title":"Efficient Transferability Assessment for Selection of Pre-trained Detectors","abstract":"Large-scale pre-training followed by downstream fine-tuning is an effective solution for transferring deep-learning-based models. Since finetuning all possible pre-trained models is computational costly, we aim to predict the transferability performance of these pre-trained models in a computational efficient manner. Different from previous work that seek out suitable models for downstream classification and segmentation tasks, this paper studies the efficient transferability assessment of pre-trained object detectors. To this end, we build up a detector transferability benchmark which contains a large and diverse zoo of pre-trained detectors with various architectures, source datasets and training schemes. Given this zoo, we adopt 7 target datasets from 5 diverse domains as the downstream target tasks for evaluation. Further, we propose to assess classification and regression sub-tasks simultaneously in a unified framework. Additionally, we design a complementary metric for evaluating tasks with varying objects. Experimental results demonstrate that our method outperforms other state-of-the-art approaches in assessing transferability under different target domains while efficiently reducing wall-clock time 32$\\times$ and requires a mere 5.2\\% memory footprint compared to brute-force fine-tuning of all pre-trained detectors.","sentences":["Large-scale pre-training followed by downstream fine-tuning is an effective solution for transferring deep-learning-based models.","Since finetuning all possible pre-trained models is computational costly, we aim to predict the transferability performance of these pre-trained models in a computational efficient manner.","Different from previous work that seek out suitable models for downstream classification and segmentation tasks, this paper studies the efficient transferability assessment of pre-trained object detectors.","To this end, we build up a detector transferability benchmark which contains a large and diverse zoo of pre-trained detectors with various architectures, source datasets and training schemes.","Given this zoo, we adopt 7 target datasets from 5 diverse domains as the downstream target tasks for evaluation.","Further, we propose to assess classification and regression sub-tasks simultaneously in a unified framework.","Additionally, we design a complementary metric for evaluating tasks with varying objects.","Experimental results demonstrate that our method outperforms other state-of-the-art approaches in assessing transferability under different target domains while efficiently reducing wall-clock time 32$\\times$ and requires a mere 5.2\\% memory footprint compared to brute-force fine-tuning of all pre-trained detectors."],"url":"http://arxiv.org/abs/2403.09432v1","category":"cs.CV"}
{"created":"2024-03-14 14:20:22","title":"Variational Inference with Sequential Sample-Average Approximations","abstract":"We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations. VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.","sentences":["We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations.","VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region.","This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost.","We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates."],"url":"http://arxiv.org/abs/2403.09429v1","category":"stat.ML"}
{"created":"2024-03-14 13:50:03","title":"Detecting the third family of compact stars with normalizing flows","abstract":"We explore the anomaly detection framework based on Normalizing Flows (NF) models introduced in \\cite{PhysRevC.106.065802} to detect the presence of a large (destabilising) dense matter phase transition in neutron star (NS) observations of masses and radii, and relate the feasibility of detection with parameters of the underlying mass-radius sequence, which is a functional of the dense matter equation of state. Once trained on simulated data featuring continuous $M(R)$ solutions (i.e., no phase transitions), NF is used to determine the likelihood of a first-order phase transition in a given set of $M(R)$ observations featuring a discontinuity, i.e., perform the anomaly detection. Different mock test sets, featuring two branch solutions in the $M(R)$ diagram, were parameterized by the NS mass at which the phase transition occurs, $M_c$, and the radius difference between the heaviest hadronic star and lightest hybrid star, $\\Delta R$. We analyze the impact of these parameters on the NF performance in detecting the presence of a first-order phase transition. Among the results, we report that given a set of 15 stars with radius uncertainty of $0.2$ km, a detection of a two-branch solution is possible with 95\\% accuracy if $\\Delta R > 0.4$ km.","sentences":["We explore the anomaly detection framework based on Normalizing Flows (NF) models introduced in \\cite{PhysRevC.106.065802} to detect the presence of a large (destabilising) dense matter phase transition in neutron star (NS) observations of masses and radii, and relate the feasibility of detection with parameters of the underlying mass-radius sequence, which is a functional of the dense matter equation of state.","Once trained on simulated data featuring continuous $M(R)$ solutions (i.e., no phase transitions), NF is used to determine the likelihood of a first-order phase transition in a given set of $M(R)$ observations featuring a discontinuity, i.e., perform the anomaly detection.","Different mock test sets, featuring two branch solutions in the $M(R)$ diagram, were parameterized by the NS mass at which the phase transition occurs, $M_c$, and the radius difference between the heaviest hadronic star and lightest hybrid star, $\\Delta R$.","We analyze the impact of these parameters on the NF performance in detecting the presence of a first-order phase transition.","Among the results, we report that given a set of 15 stars with radius uncertainty of $0.2$ km, a detection of a two-branch solution is possible with 95\\% accuracy if $\\Delta R > 0.4$ km."],"url":"http://arxiv.org/abs/2403.09398v1","category":"nucl-th"}
{"created":"2024-03-14 13:43:22","title":"Curvature of the chiral phase transition line from the magnetic equation of state of (2+1)-flavor QCD","abstract":"We analyze the dependence of the chiral phase transition temperature on baryon number and strangeness chemical potentials by calculating the leading order curvature coefficients in the light and strange quark flavor basis as well as in the conserved charge ($B, S$) basis. Making use of scaling properties of the magnetic equation of state (MEoS) and including diagonal as well as off-diagonal contributions in the expansion of the energy-like scaling variable that enters the parametrization of the MEoS, allows to explore the variation of $T_c(\\mu_B,\\mu_S) = T_c ( 1 - (\\kappa_2^B \\hat{\\mu}_B^2 + \\kappa_2^S \\hat{\\mu}_S^2 + 2\\kappa_{11}^{BS} \\hat{\\mu}_B \\hat{\\mu}_S))$ along different lines in the $(\\mu_B,\\mu_S)$ plane. On lattices with fixed cut-off in units of temperature, $aT=1/8$, we find $\\kappa_2^B=0.015(1)$, $\\kappa_2^S=0.0124(5)$ and $\\kappa_{11}^{BS}=-0.0050(7)$. We show that the chemical potential dependence along the line of vanishing strangeness chemical potential is about 10\\% larger than along the strangeness neutral line. The latter differs only by about $3\\%$ from the curvature on a line of vanishing strange quark chemical potential, $\\mu_s=0$. We also show that close to the chiral limit the strange quark mass contributes like an energy-like variable in scaling relations for pseudo-critical temperatures. The chiral phase transition temperature decreases with decreasing strange quark mass, $T_c(m_s)= T_c(m_s^{\\rm phy}) (1 - 0.097(2) (m_s-m_s^{\\rm phys})/m_s^{\\rm phy}+{\\cal O}((\\Delta m_s)^2)$.","sentences":["We analyze the dependence of the chiral phase transition temperature on baryon number and strangeness chemical potentials by calculating the leading order curvature coefficients in the light and strange quark flavor basis as well as in the conserved charge ($B, S$) basis.","Making use of scaling properties of the magnetic equation of state (MEoS) and including diagonal as well as off-diagonal contributions in the expansion of the energy-like scaling variable that enters the parametrization of the MEoS, allows to explore the variation of $T_c(\\mu_B,\\mu_S)","= T_c","( 1 - (\\kappa_2^B \\hat{\\mu}_B^2 + \\kappa_2^S \\hat{\\mu}_S^2 + 2\\kappa_{11}^{BS} \\hat{\\mu}_B \\hat{\\mu}_S))$ along different lines in the $(\\mu_B,\\mu_S)$ plane.","On lattices with fixed cut-off in units of temperature, $aT=1/8$, we find $\\kappa_2^B=0.015(1)$, $\\kappa_2^S=0.0124(5)$ and $\\kappa_{11}^{BS}=-0.0050(7)$. We show that the chemical potential dependence along the line of vanishing strangeness chemical potential is about 10\\% larger than along the strangeness neutral line.","The latter differs only by about $3\\%$ from the curvature on a line of vanishing strange quark chemical potential, $\\mu_s=0$. We also show that close to the chiral limit the strange quark mass contributes like an energy-like variable in scaling relations for pseudo-critical temperatures.","The chiral phase transition temperature decreases with decreasing strange quark mass, $T_c(m_s)= T_c(m_s^{\\rm phy}) (1 - 0.097(2) (m_s-m_s^{\\rm phys})/m_s^{\\rm phy}+{\\cal O}((\\Delta m_s)^2)$."],"url":"http://arxiv.org/abs/2403.09390v1","category":"hep-lat"}
{"created":"2024-03-14 13:21:48","title":"Machine Learning Processes as Sources of Ambiguity: Insights from AI Art","abstract":"Ongoing efforts to turn Machine Learning (ML) into a design material have encountered limited success. This paper examines the burgeoning area of AI art to understand how artists incorporate ML in their creative work. Drawing upon related HCI theories, we investigate how artists create ambiguity by analyzing nine AI artworks that use computer vision and image synthesis. Our analysis shows that, in addition to the established types of ambiguity, artists worked closely with the ML process (dataset curation, model training, and application) and developed various techniques to evoke the ambiguity of processes. Our finding indicates that the current conceptualization of ML as a design material needs to reframe the ML process as design elements, instead of technical details. Finally, this paper offers reflections on commonly held assumptions in HCI about ML uncertainty, dependability, and explainability, and advocates to supplement the artifact-centered design perspective of ML with a process-centered one.","sentences":["Ongoing efforts to turn Machine Learning (ML) into a design material have encountered limited success.","This paper examines the burgeoning area of AI art to understand how artists incorporate ML in their creative work.","Drawing upon related HCI theories, we investigate how artists create ambiguity by analyzing nine AI artworks that use computer vision and image synthesis.","Our analysis shows that, in addition to the established types of ambiguity, artists worked closely with the ML process (dataset curation, model training, and application) and developed various techniques to evoke the ambiguity of processes.","Our finding indicates that the current conceptualization of ML as a design material needs to reframe the ML process as design elements, instead of technical details.","Finally, this paper offers reflections on commonly held assumptions in HCI about ML uncertainty, dependability, and explainability, and advocates to supplement the artifact-centered design perspective of ML with a process-centered one."],"url":"http://arxiv.org/abs/2403.09374v1","category":"cs.HC"}
{"created":"2024-03-14 12:53:58","title":"A Bayes Factor Framework for Unified Parameter Estimation and Hypothesis Testing","abstract":"The Bayes factor, the data-based updating factor of the prior to posterior odds of two hypotheses, is a natural measure of statistical evidence for one hypothesis over the other. We show how Bayes factors can also be used for parameter estimation. The key idea is to consider the Bayes factor as a function of the parameter value under the null hypothesis. This 'Bayes factor function' is inverted to obtain point estimates ('maximum evidence estimates') and interval estimates ('support intervals'), similar to how P-value functions are inverted to obtain point estimates and confidence intervals. This provides data analysts with a unified inference framework as Bayes factors (for any tested parameter value), support intervals (at any level), and point estimates can be easily read off from a plot of the Bayes factor function. This approach shares similarities but is also distinct from conventional Bayesian and frequentist approaches: It uses the Bayesian evidence calculus, but without synthesizing data and prior, and it defines statistical evidence in terms of (integrated) likelihood ratios, but also includes a natural way for dealing with nuisance parameters. Applications to real-world examples illustrate how our framework is of practical value for making make quantitative inferences.","sentences":["The Bayes factor, the data-based updating factor of the prior to posterior odds of two hypotheses, is a natural measure of statistical evidence for one hypothesis over the other.","We show how Bayes factors can also be used for parameter estimation.","The key idea is to consider the Bayes factor as a function of the parameter value under the null hypothesis.","This 'Bayes factor function' is inverted to obtain point estimates ('maximum evidence estimates') and interval estimates ('support intervals'), similar to how P-value functions are inverted to obtain point estimates and confidence intervals.","This provides data analysts with a unified inference framework as Bayes factors (for any tested parameter value), support intervals (at any level), and point estimates can be easily read off from a plot of the Bayes factor function.","This approach shares similarities but is also distinct from conventional Bayesian and frequentist approaches: It uses the Bayesian evidence calculus, but without synthesizing data and prior, and it defines statistical evidence in terms of (integrated) likelihood ratios, but also includes a natural way for dealing with nuisance parameters.","Applications to real-world examples illustrate how our framework is of practical value for making make quantitative inferences."],"url":"http://arxiv.org/abs/2403.09350v1","category":"stat.ME"}
{"created":"2024-03-14 12:09:36","title":"A Hierarchical Fused Quantum Fuzzy Neural Network for Image Classification","abstract":"Neural network is a powerful learning paradigm for data feature learning in the era of big data. However, most neural network models are deterministic models that ignore the uncertainty of data. Fuzzy neural networks are proposed to address this problem. FDNN is a hierarchical deep neural network that derives information from both fuzzy and neural representations, the representations are then fused to form representation to be classified. FDNN perform well on uncertain data classification tasks. In this paper, we proposed a novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from classical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership functions in fuzzy neural network. We conducted simulated experiment on two types of datasets (Dirty-MNIST and 15-Scene), the results show that the proposed model can outperform several existing methods. In addition, we demonstrate the robustness of the proposed quantum circuit.","sentences":["Neural network is a powerful learning paradigm for data feature learning in the era of big data.","However, most neural network models are deterministic models that ignore the uncertainty of data.","Fuzzy neural networks are proposed to address this problem.","FDNN is a hierarchical deep neural network that derives information from both fuzzy and neural representations, the representations are then fused to form representation to be classified.","FDNN perform well on uncertain data classification tasks.","In this paper, we proposed a novel hierarchical fused quantum fuzzy neural network (HQFNN).","Different from classical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership functions in fuzzy neural network.","We conducted simulated experiment on two types of datasets (Dirty-MNIST and 15-Scene), the results show that the proposed model can outperform several existing methods.","In addition, we demonstrate the robustness of the proposed quantum circuit."],"url":"http://arxiv.org/abs/2403.09318v1","category":"quant-ph"}
{"created":"2024-03-14 12:08:34","title":"A consistent explanation for the unusual initial mass function and star formation rate in the Central Molecular Zone (CMZ)","abstract":"We examine various physical processes that may explain the shallow high-mass slope of the IMF as well as the low SFR in star-forming molecular clouds (MCs) in the Central Molecular Zone (CMZ). We show that the strong tidal field and the tidal shear experienced by the CMZ have opposite effects on the collapse of density fluctuations and nearly compensate, but in any case have a negligible impact and can not explain these unusual properties. Similarly, we show that the intense magnetic field in the CMZ provides a negligible pressure support and, for the high densities at play should not modify the probability density function (PDF) of the turbulent gas flow in the clouds, thus affecting negligibly the slope of the IMF. However, we show that, in contrast to MCs in the Galactic disk, the ones in the CMZ experience only one single episode of turbulence injection at large scale, most likely due dominantly to bar gas inflow. Indeed, their rather short lifetime, due to their high mean densities, is similar to one typical turbulence crossing time. Consequently, according to the Hennebelle-Chabrier theory of star formation, within this 'single turbulence episode' scenario, the cloud experiences one single field of turbulence induced density fluctuations, leading eventually to gravitationally unstable prestellar cores. As shown in Hennebelle & Chabrier (2013}, this yields a flatter IMF than usual and leads to the correct observed slope for the CMZ star-forming clouds. Similarly, this single large scale turbulence event within the cloud lifetime yields a 5 to 6 lower SFR than under usual MW cloud conditions, again in agreement with the observed values. Therefore, we suggest that this 'single large scale turbulence injection' episode can explain both the shallow IMF high-mass slope and low SFR of clouds in the CMZ.","sentences":["We examine various physical processes that may explain the shallow high-mass slope of the IMF as well as the low SFR in star-forming molecular clouds (MCs) in the Central Molecular Zone (CMZ).","We show that the strong tidal field and the tidal shear experienced by the CMZ have opposite effects on the collapse of density fluctuations and nearly compensate, but in any case have a negligible impact and can not explain these unusual properties.","Similarly, we show that the intense magnetic field in the CMZ provides a negligible pressure support and, for the high densities at play should not modify the probability density function (PDF) of the turbulent gas flow in the clouds, thus affecting negligibly the slope of the IMF.","However, we show that, in contrast to MCs in the Galactic disk, the ones in the CMZ experience only one single episode of turbulence injection at large scale, most likely due dominantly to bar gas inflow.","Indeed, their rather short lifetime, due to their high mean densities, is similar to one typical turbulence crossing time.","Consequently, according to the Hennebelle-Chabrier theory of star formation, within this 'single turbulence episode' scenario, the cloud experiences one single field of turbulence induced density fluctuations, leading eventually to gravitationally unstable prestellar cores.","As shown in Hennebelle & Chabrier (2013}, this yields a flatter IMF than usual and leads to the correct observed slope for the CMZ star-forming clouds.","Similarly, this single large scale turbulence event within the cloud lifetime yields a 5 to 6 lower SFR than under usual MW cloud conditions, again in agreement with the observed values.","Therefore, we suggest that this 'single large scale turbulence injection' episode can explain both the shallow IMF high-mass slope and low SFR of clouds in the CMZ."],"url":"http://arxiv.org/abs/2403.09316v1","category":"astro-ph.GA"}
{"created":"2024-03-14 11:51:01","title":"Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective","abstract":"Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the information entropy of latent vectors. Experiments on four datasets with two image modalities validate the effectiveness of our theory. To the best of our knowledge, this is the first effort to theoretically clarify the principles and design philosophy of AE for anomaly detection. Code will be available upon acceptance.","sentences":["Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases.","Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field.","They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors.","However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound.","This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection.","By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the information entropy of latent vectors.","Experiments on four datasets with two image modalities validate the effectiveness of our theory.","To the best of our knowledge, this is the first effort to theoretically clarify the principles and design philosophy of AE for anomaly detection.","Code will be available upon acceptance."],"url":"http://arxiv.org/abs/2403.09303v1","category":"cs.LG"}
{"created":"2024-03-14 11:10:06","title":"Regularity and trend to equilibrium for a non-local advection-diffusion model of active particles","abstract":"We establish regularity and, under suitable assumptions, convergence to stationary states for weak solutions of a parabolic equation with a non-linear non-local drift term; this equation was derived from a model of active Brownian particles with repulsive interactions in a previous work, which incorporates advection-diffusion processes both in particle position and orientation. We apply De Giorgi's method and differentiate the equation with respect to the time variable iteratively to show that weak solutions become smooth away from the initial time. This strategy requires that we obtain improved integrability estimates in order to cater for the presence of the non-local drift. The instantaneous smoothing effect observed for weak solutions is shown to also hold for very weak solutions arising from distributional initial data; the proof of this result relies on a uniqueness theorem in the style of M.~Pierre for low-regularity solutions. The convergence to stationary states is proved under a smallness assumption on the drift term.","sentences":["We establish regularity and, under suitable assumptions, convergence to stationary states for weak solutions of a parabolic equation with a non-linear non-local drift term; this equation was derived from a model of active Brownian particles with repulsive interactions in a previous work, which incorporates advection-diffusion processes both in particle position and orientation.","We apply De Giorgi's method and differentiate the equation with respect to the time variable iteratively to show that weak solutions become smooth away from the initial time.","This strategy requires that we obtain improved integrability estimates in order to cater for the presence of the non-local drift.","The instantaneous smoothing effect observed for weak solutions is shown to also hold for very weak solutions arising from distributional initial data; the proof of this result relies on a uniqueness theorem in the style of M.~Pierre for low-regularity solutions.","The convergence to stationary states is proved under a smallness assumption on the drift term."],"url":"http://arxiv.org/abs/2403.09282v1","category":"math.AP"}
{"created":"2024-03-14 10:50:24","title":"Identifying Galaxy Cluster Mergers with Deep Neural Networks using Idealized Compton-y and X-ray maps","abstract":"We present a novel approach to identify galaxy clusters that are undergoing a merger using a deep learning approach. This paper uses massive galaxy clusters spanning $0 \\leq z \\leq 2$ from \\textsc{The Three Hundred} project, a suite of hydrodynamic re-simulations of 324 large galaxy clusters. Mock, idealised Compton-{\\it y} and X-ray maps were constructed for the sample, capturing them out to a radius of $2R_{200}$. The idealised nature of these maps mean they do not consider observational effects such as foreground or background astrophysical objects, any spatial resolution limits or restriction on X-ray energy bands. Half of the maps belong to a merging population as defined by a mass increase $\\Delta${\\it M/M} $\\geq$ 0.75, and the other half serve as a control, relaxed population. We employ a convolutional neural network architecture and train the model to classify clusters into one of the groups. A best-performing model was able to correctly distinguish between the two populations with a balanced accuracy (BA) and recall of 0.77, ROC-AUC of 0.85, PR-AUC of 0.55 and $F_{1}$ score of 0.53. Using a multichannel model relative to a single channel model, we obtain a 3\\% improvement in BA score, and a 6\\% improvement in $F_{1}$ score. We use a saliency interpretation approach to discern the regions most important to each classification decision. By analysing radially binned saliency values we find a preference to utilise regions out to larger distances for mergers with respect to non-mergers, greater than $\\sim1.2 R_{200}$ and $\\sim0.7 R_{200}$ for SZ and X-ray respectively.","sentences":["We present a novel approach to identify galaxy clusters that are undergoing a merger using a deep learning approach.","This paper uses massive galaxy clusters spanning $0 \\leq z \\leq 2$ from \\textsc{The Three Hundred} project, a suite of hydrodynamic re-simulations of 324 large galaxy clusters.","Mock, idealised Compton-{\\it y} and X-ray maps were constructed for the sample, capturing them out to a radius of $2R_{200}$. The idealised nature of these maps mean they do not consider observational effects such as foreground or background astrophysical objects, any spatial resolution limits or restriction on X-ray energy bands.","Half of the maps belong to a merging population as defined by a mass increase $\\Delta${\\it M/M} $\\geq$ 0.75, and the other half serve as a control, relaxed population.","We employ a convolutional neural network architecture and train the model to classify clusters into one of the groups.","A best-performing model was able to correctly distinguish between the two populations with a balanced accuracy (BA) and recall of 0.77, ROC-AUC of 0.85, PR-AUC of 0.55 and $F_{1}$ score of 0.53.","Using a multichannel model relative to a single channel model, we obtain a 3\\% improvement in BA score, and a 6\\% improvement in $F_{1}$ score.","We use a saliency interpretation approach to discern the regions most important to each classification decision.","By analysing radially binned saliency values we find a preference to utilise regions out to larger distances for mergers with respect to non-mergers, greater than $\\sim1.2 R_{200}$ and $\\sim0.7 R_{200}$ for SZ and X-ray respectively."],"url":"http://arxiv.org/abs/2403.09273v1","category":"astro-ph.GA"}
{"created":"2024-03-14 10:44:10","title":"Deep Limit Order Book Forecasting","abstract":"We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting complete transactions. This work offers academics and practitioners an avenue to make informed and robust decisions on the application of deep learning techniques, their scope and limitations, effectively exploiting emergent statistical properties of the Limit Order Book.","sentences":["We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange.","In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities.","Our results are twofold.","We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals.","We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context.","As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting complete transactions.","This work offers academics and practitioners an avenue to make informed and robust decisions on the application of deep learning techniques, their scope and limitations, effectively exploiting emergent statistical properties of the Limit Order Book."],"url":"http://arxiv.org/abs/2403.09267v1","category":"q-fin.TR"}
{"created":"2024-03-14 10:33:28","title":"To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation","abstract":"Active learning (AL) techniques reduce labeling costs for training neural machine translation (NMT) models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines uncertainty and diversity for sentence selection. HUDS computes uncertainty scores for unlabeled sentences and subsequently stratifies them. It then clusters sentence embeddings within each stratum using k-MEANS and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then used to select the top instances for annotation in each AL iteration. Experiments on multi-domain German-English datasets demonstrate the better performance of HUDS over other strong AL baselines. We analyze the sentence selection with HUDS and show that it prioritizes diverse instances having high model uncertainty for annotation in early AL iterations.","sentences":["Active learning (AL) techniques reduce labeling costs for training neural machine translation (NMT) models by selecting smaller representative subsets from unlabeled data for annotation.","Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty.","Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances.","To bridge this gap, we propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines uncertainty and diversity for sentence selection.","HUDS computes uncertainty scores for unlabeled sentences and subsequently stratifies them.","It then clusters sentence embeddings within each stratum using k-MEANS and computes diversity scores by distance to the centroid.","A weighted hybrid score that combines uncertainty and diversity is then used to select the top instances for annotation in each AL iteration.","Experiments on multi-domain German-English datasets demonstrate the better performance of HUDS over other strong AL baselines.","We analyze the sentence selection with HUDS and show that it prioritizes diverse instances having high model uncertainty for annotation in early AL iterations."],"url":"http://arxiv.org/abs/2403.09259v1","category":"cs.CL"}
{"created":"2024-03-14 10:17:17","title":"Magnetotransport properties in van-der-Waals \\textit{\\textbf{R}}Te$_{3}$ (\\textit{\\textbf{R}} = La, Ce, Tb)","abstract":"Rare-earth tritellurides are van-der-Waals antiferromagnets which have been attracting attention as materials not onlywith high mobility, but also with various states such as superconductivity under high pressure, incommensurate charge-density-wave (CDW) phase, andmultiple antiferromagnetic phases.In this work, we performed longitudinal resistivity and Hall resistivity measurements simultaneously in exfoliated $R$Te$_3$ ($R=$La, Ce, Tb) thin film devices,in order to investigate the influence of magnetic ordering on transport properties in van-der-Waals magnetic materials.We have obtained carrier mobility and concentration using a two-band model, and have observed an increase in carrier mobility in the antiferromagnets CeTe$_3$ and TbTe$_3$ due to the magnetic transition.Especially in CeTe$_3$, the carrier concentration has changed drastically below the magnetic transition temperature,suggesting the interaction between the CDW and antiferromagnetic phases.In addition, the analysis of the Shubnikov-de Haas oscillations in CeTe$_3$ supports the possibility of Fermi surface modulation by magnetic ordering.This research will pave the way not only for spintronic devicesthat take advantage of high mobility, but also for the study of the correlation between CDW and magnetism states in low-dimensional materials.","sentences":["Rare-earth tritellurides are van-der-Waals antiferromagnets which have been attracting attention as materials not onlywith high mobility, but also with various states such as superconductivity under high pressure, incommensurate charge-density-wave (CDW) phase, andmultiple antiferromagnetic phases.","In this work, we performed longitudinal resistivity and Hall resistivity measurements simultaneously in exfoliated $R$Te$_3$ ($R=$La, Ce, Tb) thin film devices,in order to investigate the influence of magnetic ordering on transport properties in van-der-Waals magnetic materials.","We have obtained carrier mobility and concentration using a two-band model, and have observed an increase in carrier mobility in the antiferromagnets CeTe$_3$ and TbTe$_3$ due to the magnetic transition.","Especially in CeTe$_3$, the carrier concentration has changed drastically below the magnetic transition temperature,suggesting the interaction between the CDW and antiferromagnetic phases.","In addition, the analysis of the Shubnikov-de Haas oscillations in CeTe$_3$ supports the possibility of Fermi surface modulation by magnetic ordering.","This research will pave the way not only for spintronic devicesthat take advantage of high mobility, but also for the study of the correlation between CDW and magnetism states in low-dimensional materials."],"url":"http://arxiv.org/abs/2403.09250v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 10:05:06","title":"S-flat cotorsion pair","abstract":"Let $R$ be a commutative ring, and let $S$ be a multiplicative subset of $R$. In this paper, we investigate the notion of $S$-cotorsion modules. An $R$-module $C$ is called $S$-cotorsion if $\\text{Ext}^{1}_{R}(F,C) = 0$ for every $S$-flat $R$-module $F$. Among other results, we establish that the pair $(S\\mathcal{F}, S\\mathcal{C})$, where $S\\mathcal{F}$ denotes the class of all $S$-flat $R$-modules and $S\\mathcal{C}$ denotes the class of all $S$-cotorsion modules, forms a hereditary perfect cotorsion pair. As applications, we provide characterizations of $S$-perfect rings in terms of $S$-cotorsion modules. We conclude the paper with results on $S\\mathcal{F}$-preenvelopes. Namely, we prove that if every module has an $S\\mathcal{F}$-preenvelope, then $R$ is $S$-coherent. Furthermore, we establish the converse under the condition that $R_S$ is a finitely presented $R$-module.","sentences":["Let $R$ be a commutative ring, and let $S$ be a multiplicative subset of $R$. In this paper, we investigate the notion of $S$-cotorsion modules.","An $R$-module $C$ is called $S$-cotorsion if $\\text{Ext}^{1}_{R}(F,C) = 0$ for every $S$-flat $R$-module $F$. Among other results, we establish that the pair $(S\\mathcal{F}, S\\mathcal{C})$, where $S\\mathcal{F}$ denotes the class of all $S$-flat $R$-modules and $S\\mathcal{C}$ denotes the class of all $S$-cotorsion modules, forms a hereditary perfect cotorsion pair.","As applications, we provide characterizations of $S$-perfect rings in terms of $S$-cotorsion modules.","We conclude the paper with results on $S\\mathcal{F}$-preenvelopes.","Namely, we prove that if every module has an $S\\mathcal{F}$-preenvelope, then $R$ is $S$-coherent.","Furthermore, we establish the converse under the condition that $R_S$ is a finitely presented $R$-module."],"url":"http://arxiv.org/abs/2403.09242v1","category":"math.AC"}
{"created":"2024-03-14 10:01:47","title":"Quantum effects in the H-bond symmetrization and in the thermodynamic properties of high pressure ice","abstract":"We investigate the structural and thermodynamic properties of high-pressure ice by incorporating quantum anharmonicity at a non-perturbative level. Quantum fluctuations reduce the critical pressure of the phase transition between phase VIII (with asymmetric H-bonds) and phase X (with symmetric H-bonds) by 65 GPa from its classical value of 116 GPa at 0K. Moreover, quantum effects make it temperature-independent over a wide temperature range (0K-300K), in agreement with experimental estimates obtained through vibrational spectroscopy and in striking contrast to the strong temperature dependence found in the classical approximation. The equation of state shows fingerprints of the transition in accordance with experimental evidence. Additionally, we demonstrate that, within our approach, proton disorder in phase VII has a negligible impact on the occurrence of phase X. Finally, we reproduce with high accuracy the 10 GPa isotope shift due to the hydrogen-to-deuterium substitution.","sentences":["We investigate the structural and thermodynamic properties of high-pressure ice by incorporating quantum anharmonicity at a non-perturbative level.","Quantum fluctuations reduce the critical pressure of the phase transition between phase VIII (with asymmetric H-bonds) and phase X (with symmetric H-bonds) by 65 GPa from its classical value of 116 GPa at 0K.","Moreover, quantum effects make it temperature-independent over a wide temperature range (0K-300K), in agreement with experimental estimates obtained through vibrational spectroscopy and in striking contrast to the strong temperature dependence found in the classical approximation.","The equation of state shows fingerprints of the transition in accordance with experimental evidence.","Additionally, we demonstrate that, within our approach, proton disorder in phase VII has a negligible impact on the occurrence of phase X. Finally, we reproduce with high accuracy the 10 GPa isotope shift due to the hydrogen-to-deuterium substitution."],"url":"http://arxiv.org/abs/2403.09238v1","category":"cond-mat.other"}
{"created":"2024-03-14 09:48:48","title":"Uncertainty Quantification for cross-subject Motor Imagery classification","abstract":"Uncertainty Quantification aims to determine when the prediction from a Machine Learning model is likely to be wrong. Computer Vision research has explored methods for determining epistemic uncertainty (also known as model uncertainty), which should correspond with generalisation error. These methods theoretically allow to predict misclassifications due to inter-subject variability. We applied a variety of Uncertainty Quantification methods to predict misclassifications for a Motor Imagery Brain Computer Interface. Deep Ensembles performed best, both in terms of classification performance and cross-subject Uncertainty Quantification performance. However, we found that standard CNNs with Softmax output performed better than some of the more advanced methods.","sentences":["Uncertainty Quantification aims to determine when the prediction from a Machine Learning model is likely to be wrong.","Computer Vision research has explored methods for determining epistemic uncertainty (also known as model uncertainty), which should correspond with generalisation error.","These methods theoretically allow to predict misclassifications due to inter-subject variability.","We applied a variety of Uncertainty Quantification methods to predict misclassifications for a Motor Imagery Brain Computer Interface.","Deep Ensembles performed best, both in terms of classification performance and cross-subject Uncertainty Quantification performance.","However, we found that standard CNNs with Softmax output performed better than some of the more advanced methods."],"url":"http://arxiv.org/abs/2403.09228v1","category":"cs.LG"}
{"created":"2024-03-14 09:31:20","title":"Unlocking the Potential of Open Government Data: Exploring the Strategic, Technical, and Application Perspectives of High-Value Datasets Opening in Taiwan","abstract":"Today, data has an unprecedented value as it forms the basis for data-driven decision-making, including serving as an input for AI models, where the latter is highly dependent on the availability of the data. However, availability of data in an open data format creates a little added value, where the value of these data, i.e., their relevance to the real needs of the end user, is key. This is where the concept of high-value dataset (HVD) comes into play, which has become popular in recent years. Defining and opening HVD is an ongoing process consisting of a set of interrelated steps, the implementation of which may vary from one country or region to another. Therefore, there has recently been a call to conduct research in a country or region setting considered to be of greatest national value. So far, only a few studies have been conducted at the regional or national level, most of which consider only one step of the process, such as identifying HVD or measuring their impact. With this study, we answer this call and examine the national case of Taiwan by exploring the entire lifecycle of HVD opening. The aim of the paper is to understand and evaluate the lifecycle of high-value dataset publishing in one of the world's leading producers of information and communication technology (ICT) products - Taiwan. To do this, we conduct a qualitative study with exploratory interviews with representatives from government agencies in Taiwan responsible for HVD opening, exploring HVD opening lifecycle. As such, we examine (1) strategic aspects related to the HVD determination process, (2) technical aspects, and (3) application aspects.","sentences":["Today, data has an unprecedented value as it forms the basis for data-driven decision-making, including serving as an input for AI models, where the latter is highly dependent on the availability of the data.","However, availability of data in an open data format creates a little added value, where the value of these data, i.e., their relevance to the real needs of the end user, is key.","This is where the concept of high-value dataset (HVD) comes into play, which has become popular in recent years.","Defining and opening HVD is an ongoing process consisting of a set of interrelated steps, the implementation of which may vary from one country or region to another.","Therefore, there has recently been a call to conduct research in a country or region setting considered to be of greatest national value.","So far, only a few studies have been conducted at the regional or national level, most of which consider only one step of the process, such as identifying HVD or measuring their impact.","With this study, we answer this call and examine the national case of Taiwan by exploring the entire lifecycle of HVD opening.","The aim of the paper is to understand and evaluate the lifecycle of high-value dataset publishing in one of the world's leading producers of information and communication technology (ICT) products - Taiwan.","To do this, we conduct a qualitative study with exploratory interviews with representatives from government agencies in Taiwan responsible for HVD opening, exploring HVD opening lifecycle.","As such, we examine (1) strategic aspects related to the HVD determination process, (2) technical aspects, and (3) application aspects."],"url":"http://arxiv.org/abs/2403.09216v1","category":"cs.CY"}
{"created":"2024-03-14 09:28:26","title":"Efficient size-prescribed $k$-core search","abstract":"$k$-core is a subgraph where every node has at least $k$ neighbors within the subgraph. The $k$-core subgraphs has been employed in large platforms like Network Repository to comprehend the underlying structures and dynamics of the network. Existing studies have primarily focused on finding $k$-core groups without considering their size, despite the relevance of solution sizes in many real-world scenarios. This paper addresses this gap by introducing the size-prescribed $k$-core search (SPCS) problem, where the goal is to find a subgraph of a specified size that has the highest possible core number. We propose two algorithms, namely the {\\it TSizeKcore-BU} and the {\\it TSizeKcore-TD}, to identify cohesive subgraphs that satisfy both the $k$-core requirement and the size constraint. Our experimental results demonstrate the superiority of our approach in terms of solution quality and efficiency. The {\\it TSizeKcore-BU} algorithm proves to be highly efficient in finding size-prescribed $k$-core subgraphs on large datasets, making it a favorable choice for such scenarios. On the other hand, the {\\it TSizeKcore-TD} algorithm is better suited for small datasets where running time is less critical.","sentences":["$k$-core is a subgraph where every node has at least $k$ neighbors within the subgraph.","The $k$-core subgraphs has been employed in large platforms like Network Repository to comprehend the underlying structures and dynamics of the network.","Existing studies have primarily focused on finding $k$-core groups without considering their size, despite the relevance of solution sizes in many real-world scenarios.","This paper addresses this gap by introducing the size-prescribed $k$-core search (SPCS) problem, where the goal is to find a subgraph of a specified size that has the highest possible core number.","We propose two algorithms, namely the {\\it TSizeKcore-BU} and the {\\it TSizeKcore-TD}, to identify cohesive subgraphs that satisfy both the $k$-core requirement and the size constraint.","Our experimental results demonstrate the superiority of our approach in terms of solution quality and efficiency.","The {\\it TSizeKcore-BU} algorithm proves to be highly efficient in finding size-prescribed $k$-core subgraphs on large datasets, making it a favorable choice for such scenarios.","On the other hand, the {\\it TSizeKcore-TD} algorithm is better suited for small datasets where running time is less critical."],"url":"http://arxiv.org/abs/2403.09214v1","category":"cs.DS"}
{"created":"2024-03-14 09:06:49","title":"PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation","abstract":"Recently, the scale of transformers has grown rapidly, which introduces considerable challenges in terms of training overhead and inference efficiency in the scope of task adaptation. Existing works, namely Parameter-Efficient Fine-Tuning (PEFT) and model compression, have separately investigated the challenges. However, PEFT cannot guarantee the inference efficiency of the original backbone, especially for large-scale models. Model compression requires significant training costs for structure searching and re-training. Consequently, a simple combination of them cannot guarantee accomplishing both training efficiency and inference efficiency with minimal costs. In this paper, we propose a novel Parallel Yielding Re-Activation (PYRA) method for such a challenge of training-inference efficient task adaptation. PYRA first utilizes parallel yielding adaptive weights to comprehensively perceive the data distribution in downstream tasks. A re-activation strategy for token modulation is then applied for tokens to be merged, leading to calibrated token features. Extensive experiments demonstrate that PYRA outperforms all competing methods under both low compression rate and high compression rate, demonstrating its effectiveness and superiority in maintaining both training efficiency and inference efficiency for large-scale foundation models. Our code will be released to the public.","sentences":["Recently, the scale of transformers has grown rapidly, which introduces considerable challenges in terms of training overhead and inference efficiency in the scope of task adaptation.","Existing works, namely Parameter-Efficient Fine-Tuning (PEFT) and model compression, have separately investigated the challenges.","However, PEFT cannot guarantee the inference efficiency of the original backbone, especially for large-scale models.","Model compression requires significant training costs for structure searching and re-training.","Consequently, a simple combination of them cannot guarantee accomplishing both training efficiency and inference efficiency with minimal costs.","In this paper, we propose a novel Parallel Yielding Re-Activation (PYRA) method for such a challenge of training-inference efficient task adaptation.","PYRA first utilizes parallel yielding adaptive weights to comprehensively perceive the data distribution in downstream tasks.","A re-activation strategy for token modulation is then applied for tokens to be merged, leading to calibrated token features.","Extensive experiments demonstrate that PYRA outperforms all competing methods under both low compression rate and high compression rate, demonstrating its effectiveness and superiority in maintaining both training efficiency and inference efficiency for large-scale foundation models.","Our code will be released to the public."],"url":"http://arxiv.org/abs/2403.09192v1","category":"cs.CV"}
{"created":"2024-03-14 08:30:25","title":"Analysis of singular subspaces under random perturbations","abstract":"We present a comprehensive analysis of singular vector and singular subspace perturbations in the context of the signal plus random Gaussian noise matrix model. Assuming a low-rank signal matrix, we extend the Wedin-Davis-Kahan theorem in a fully generalized manner, applicable to any unitarily invariant matrix norm, extending previous results of O'Rourke, Vu and the author. We also obtain the fine-grained results, which encompass the $\\ell_\\infty$ analysis of singular vectors, the $\\ell_{2, \\infty}$ analysis of singular subspaces, as well as the exploration of linear and bilinear functions related to the singular vectors. Moreover, we explore the practical implications of these findings, in the context of the Gaussian mixture model and the submatrix localization problem.","sentences":["We present a comprehensive analysis of singular vector and singular subspace perturbations in the context of the signal plus random Gaussian noise matrix model.","Assuming a low-rank signal matrix, we extend the Wedin-Davis-Kahan theorem in a fully generalized manner, applicable to any unitarily invariant matrix norm, extending previous results of O'Rourke, Vu and the author.","We also obtain the fine-grained results, which encompass the $\\ell_\\infty$ analysis of singular vectors, the $\\ell_{2, \\infty}$ analysis of singular subspaces, as well as the exploration of linear and bilinear functions related to the singular vectors.","Moreover, we explore the practical implications of these findings, in the context of the Gaussian mixture model and the submatrix localization problem."],"url":"http://arxiv.org/abs/2403.09170v1","category":"math.ST"}
{"created":"2024-03-14 08:27:59","title":"Novel Boundary Conditions for the Ricci Flow","abstract":"If we want to deform a compact Riemannian manifold with boundary using Ricci flow, we first need to decide on appropriate boundary conditions. We would like these conditions to reflect the geometric nature of the flow and allow for a variety of initial data. Importantly, the conditions should be compatible with the expected evolution of Einstein metrics. We propose it is natural to choose those conditions, for which the first variation of certain functionals, such as the Einstein-Hilbert action and Perelmans lambda-functional, does not admit a boundary term. We provide a proof of the short term existence of solutions of the initial boundary value problem, under these conditions.","sentences":["If we want to deform a compact Riemannian manifold with boundary using Ricci flow, we first need to decide on appropriate boundary conditions.","We would like these conditions to reflect the geometric nature of the flow and allow for a variety of initial data.","Importantly, the conditions should be compatible with the expected evolution of Einstein metrics.","We propose it is natural to choose those conditions, for which the first variation of certain functionals, such as the Einstein-Hilbert action and Perelmans lambda-functional, does not admit a boundary term.","We provide a proof of the short term existence of solutions of the initial boundary value problem, under these conditions."],"url":"http://arxiv.org/abs/2403.09169v1","category":"math.DG"}
{"created":"2024-03-14 07:43:41","title":"Probing Gauge-Higgs Unification models at the ILC with di-quark forward-backward asymmetry at center-of-mass energies above the $Z$ mass","abstract":"The International Linear Collider (ILC) will allow the precise study of $e^{-}e^{+}\\rightarrow q\\bar{q}$ interactions at different center-of-mass energies from the $Z$-pole to 1 TeV. In this paper, we discuss the experimental prospects for measuring differential observables in $e^{-}e^{+}\\rightarrow b\\bar{b}$ and $e^{-}e^{+}\\rightarrow c\\bar{c}$ at the ILC baseline energies, 250 and 500 GeV. The study is based on full simulation and reconstruction of the International Large Detector (ILD) concept. Two gauge-Higgs unification models predicting new high-mass resonances beyond the Standard Model are discussed. These models predict sizable deviations of the forward-backward observables at the ILC running above the $Z$ mass and with longitudinally polarized electron and positron beams. The ability of the ILC to probe these models via high-precision measurements of the forward-backward asymmetry is discussed. Alternative scenarios at other energies and beam polarization schemes are also discussed, extrapolating the estimated uncertainties from the two baseline scenarios.","sentences":["The International Linear Collider (ILC) will allow the precise study of $e^{-}e^{+}\\rightarrow q\\bar{q}$ interactions at different center-of-mass energies from the $Z$-pole to 1 TeV. In this paper, we discuss the experimental prospects for measuring differential observables in $e^{-}e^{+}\\rightarrow b\\bar{b}$ and $e^{-}e^{+}\\rightarrow c\\bar{c}$ at the ILC baseline energies, 250 and 500 GeV.","The study is based on full simulation and reconstruction of the International Large Detector (ILD) concept.","Two gauge-Higgs unification models predicting new high-mass resonances beyond the Standard Model are discussed.","These models predict sizable deviations of the forward-backward observables at the ILC running above the $Z$ mass and with longitudinally polarized electron and positron beams.","The ability of the ILC to probe these models via high-precision measurements of the forward-backward asymmetry is discussed.","Alternative scenarios at other energies and beam polarization schemes are also discussed, extrapolating the estimated uncertainties from the two baseline scenarios."],"url":"http://arxiv.org/abs/2403.09144v1","category":"hep-ph"}
{"created":"2024-03-14 07:42:12","title":"A New Split Algorithm for 3D Gaussian Splatting","abstract":"3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and geometry generation. Progress has been rapid. However, due to their mixed scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like effect near the surface. At the same time, 3D Gaussian splatting models tend to flatten large untextured regions, yielding a very sparse point cloud. These problems are caused by the non-uniform nature of 3D Gaussian splatting models, so in this paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more uniform and surface-bounded 3D Gaussian splatting model. Our algorithm splits an $N$-dimensional Gaussian into two N-dimensional Gaussians. It ensures consistency of mathematical characteristics and similarity of appearance, allowing resulting 3D Gaussian splatting models to be more uniform and a better fit to the underlying surface, and thus more suitable for explicit editing, point cloud extraction and other tasks. Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form solution, making it readily applicable to any 3D Gaussian model.","sentences":["3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and geometry generation.","Progress has been rapid.","However, due to their mixed scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like effect near the surface.","At the same time, 3D Gaussian splatting models tend to flatten large untextured regions, yielding a very sparse point cloud.","These problems are caused by the non-uniform nature of 3D Gaussian splatting models, so in this paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more uniform and surface-bounded 3D Gaussian splatting model.","Our algorithm splits an $N$-dimensional Gaussian into two N-dimensional Gaussians.","It ensures consistency of mathematical characteristics and similarity of appearance, allowing resulting 3D Gaussian splatting models to be more uniform and a better fit to the underlying surface, and thus more suitable for explicit editing, point cloud extraction and other tasks.","Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form solution, making it readily applicable to any 3D Gaussian model."],"url":"http://arxiv.org/abs/2403.09143v1","category":"cs.GR"}
{"created":"2024-03-14 07:06:57","title":"Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models","abstract":"Research demonstrates that the proactivity of in-vehicle conversational assistants (IVCAs) can help to reduce distractions and enhance driving safety, better meeting users' cognitive needs. However, existing IVCAs struggle with user intent recognition and context awareness, which leads to suboptimal proactive interactions. Large language models (LLMs) have shown potential for generalizing to various tasks with prompts, but their application in IVCAs and exploration of proactive interaction remain under-explored. These raise questions about how LLMs improve proactive interactions for IVCAs and influence user perception. To investigate these questions systematically, we establish a framework with five proactivity levels across two dimensions-assumption and autonomy-for IVCAs. According to the framework, we propose a \"Rewrite + ReAct + Reflect\" strategy, aiming to empower LLMs to fulfill the specific demands of each proactivity level when interacting with users. Both feasibility and subjective experiments are conducted. The LLM outperforms the state-of-the-art model in success rate and achieves satisfactory results for each proactivity level. Subjective experiments with 40 participants validate the effectiveness of our framework and show the proactive level with strong assumptions and user confirmation is most appropriate.","sentences":["Research demonstrates that the proactivity of in-vehicle conversational assistants (IVCAs) can help to reduce distractions and enhance driving safety, better meeting users' cognitive needs.","However, existing IVCAs struggle with user intent recognition and context awareness, which leads to suboptimal proactive interactions.","Large language models (LLMs) have shown potential for generalizing to various tasks with prompts, but their application in IVCAs and exploration of proactive interaction remain under-explored.","These raise questions about how LLMs improve proactive interactions for IVCAs and influence user perception.","To investigate these questions systematically, we establish a framework with five proactivity levels across two dimensions-assumption and autonomy-for IVCAs.","According to the framework, we propose a \"Rewrite + ReAct + Reflect\" strategy, aiming to empower LLMs to fulfill the specific demands of each proactivity level when interacting with users.","Both feasibility and subjective experiments are conducted.","The LLM outperforms the state-of-the-art model in success rate and achieves satisfactory results for each proactivity level.","Subjective experiments with 40 participants validate the effectiveness of our framework and show the proactive level with strong assumptions and user confirmation is most appropriate."],"url":"http://arxiv.org/abs/2403.09135v1","category":"cs.HC"}
{"created":"2024-03-14 06:57:16","title":"A Low-Rank ADMM Splitting Approach for Semidefinite Programming","abstract":"We introduce a new first-order method for solving general semidefinite programming problems, based on the alternating direction method of multipliers (ADMM) and a matrix-splitting technique. Our algorithm has an advantage over the Burer-Monteiro approach as it only involves much easier quadratically regularized subproblems in each iteration. For a linear objective, the subproblems are well-conditioned quadratic programs that can be efficiently solved by the standard conjugate gradient method. We show that the ADMM algorithm achieves sublinear or linear convergence rates to the KKT solutions under different conditions. Building on this theoretical development, we present LoRADS, a new solver for linear SDP based on the \\underline{\\bf Lo}w-\\underline{\\bf R}ank \\underline{\\bf AD}MM \\underline{\\bf S}plitting approach. LoRADS incorporates several strategies that significantly increase its efficiency. Firstly, it initiates with a warm-start phase that uses the Burer-Monteiro approach. Moreover, motivated by the SDP low-rank theory (So et al. 2008), LoRADS chooses an initial rank of logarithmic order and then employs a dynamic approach to increase the rank. Numerical experiments indicate that LoRADS exhibits promising performance on large-scale SDP problems. A noteworthy achievement of LoRADS is its successful solving of a matrix completion problem with $15,694,167$ constraints and a matrix variable of size $40,000 \\times 40,000$ in $351$ seconds.","sentences":["We introduce a new first-order method for solving general semidefinite programming problems, based on the alternating direction method of multipliers (ADMM) and a matrix-splitting technique.","Our algorithm has an advantage over the Burer-Monteiro approach as it only involves much easier quadratically regularized subproblems in each iteration.","For a linear objective, the subproblems are well-conditioned quadratic programs that can be efficiently solved by the standard conjugate gradient method.","We show that the ADMM algorithm achieves sublinear or linear convergence rates to the KKT solutions under different conditions.","Building on this theoretical development, we present LoRADS, a new solver for linear SDP based on the \\underline{\\bf Lo}w-\\underline{\\bf R}ank \\underline{\\bf AD}MM \\underline{\\bf S}plitting approach.","LoRADS incorporates several strategies that significantly increase its efficiency.","Firstly, it initiates with a warm-start phase that uses the Burer-Monteiro approach.","Moreover, motivated by the SDP low-rank theory (So et al. 2008), LoRADS chooses an initial rank of logarithmic order and then employs a dynamic approach to increase the rank.","Numerical experiments indicate that LoRADS exhibits promising performance on large-scale SDP problems.","A noteworthy achievement of LoRADS is its successful solving of a matrix completion problem with $15,694,167$ constraints and a matrix variable of size $40,000 \\times 40,000$ in $351$ seconds."],"url":"http://arxiv.org/abs/2403.09133v1","category":"math.OC"}
{"created":"2024-03-14 06:16:21","title":"Single Domain Generalization for Crowd Counting","abstract":"Current image-based crowd counting widely employs density map regression due to its promising results. However, the method often suffers from severe performance degradation when tested on data from unseen scenarios. To address this so-called \"domain shift\" problem, we investigate single domain generalization (SDG) for crowd counting. The existing SDG approaches are mainly for classification and segmentation, and can hardly be extended to our case due to its regression nature and label ambiguity (i.e., ambiguous pixel-level ground truths). We propose MPCount, a novel SDG approach effective even for narrow source distribution. Reconstructing diverse features for density map regression with a single memory bank, MPCount retains only domain-invariant representations using a content error mask and attention consistency loss. It further introduces patch-wise classification as an auxiliary task to boost the robustness of density prediction to achieve highly accurate labels. Through extensive experiments on different datasets, MPCount is shown to significantly improve counting accuracy compared to the state of the art under diverse scenarios unobserved in the training data of narrow source distribution. Code is available at https://github.com/Shimmer93/MPCount.","sentences":["Current image-based crowd counting widely employs density map regression due to its promising results.","However, the method often suffers from severe performance degradation when tested on data from unseen scenarios.","To address this so-called \"domain shift\" problem, we investigate single domain generalization (SDG) for crowd counting.","The existing SDG approaches are mainly for classification and segmentation, and can hardly be extended to our case due to its regression nature and label ambiguity (i.e., ambiguous pixel-level ground truths).","We propose MPCount, a novel SDG approach effective even for narrow source distribution.","Reconstructing diverse features for density map regression with a single memory bank, MPCount retains only domain-invariant representations using a content error mask and attention consistency loss.","It further introduces patch-wise classification as an auxiliary task to boost the robustness of density prediction to achieve highly accurate labels.","Through extensive experiments on different datasets, MPCount is shown to significantly improve counting accuracy compared to the state of the art under diverse scenarios unobserved in the training data of narrow source distribution.","Code is available at https://github.com/Shimmer93/MPCount."],"url":"http://arxiv.org/abs/2403.09124v1","category":"cs.CV"}
{"created":"2024-03-14 04:54:25","title":"On the Evidence for Molecular Outflows in High-redshift Dusty Star-forming Galaxies","abstract":"Galactic-scale outflows of molecular gas from star-forming galaxies constitute the most direct evidence for regulation of star formation. In the early universe ($ z > 4 $), such outflows have recently been inferred from gravitationally-lensed dusty star-forming galaxies (DSFGs) based on ubiquitous detections of OH absorption extending to more blueshifted velocities than [CII] or CO emission in spatially-integrated spectra. Because these lines are redshifted to sub-mm wavelengths, such measurements require careful corrections for atmospheric absorption lines, and a proper accounting of sometimes large variations in measurement uncertainties over these lines. Taking these factors into consideration, we re-analyze OH and [CII] data taken with ALMA for the five sources where such data is available, of which four were categorised as exhibiting outflows. Based on their spatially-integrated spectra alone, we find statistically significant ($ \\geq 3 \\sigma $) OH absorption more blueshifted than [CII] emission in only one source. By contrast, searching channel maps for signals diluted below the detection threshold in spatially-integrated spectra, we find evidence for a separate kinematic component in OH absorption in all five sources in the form of: (i) more blueshifted OH absorption than [CII] emission and/or (ii) a component in OH absorption exhibiting a different spatio-kinematic pattern than [CII] emission, the latter presumably tracing gas in a rotating disc. Providing a more complete and accurate assessment of molecular outflows in gravitationally-lensed DSFGs, we suggest methods to better assess the precision of corrections for atmospheric absorption and to more accurately measure the source continuum in future observations.","sentences":["Galactic-scale outflows of molecular gas from star-forming galaxies constitute the most direct evidence for regulation of star formation.","In the early universe ($ z > 4 $), such outflows have recently been inferred from gravitationally-lensed dusty star-forming galaxies (DSFGs) based on ubiquitous detections of OH absorption extending to more blueshifted velocities than [CII] or CO emission in spatially-integrated spectra.","Because these lines are redshifted to sub-mm wavelengths, such measurements require careful corrections for atmospheric absorption lines, and a proper accounting of sometimes large variations in measurement uncertainties over these lines.","Taking these factors into consideration, we re-analyze OH and [CII] data taken with ALMA for the five sources where such data is available, of which four were categorised as exhibiting outflows.","Based on their spatially-integrated spectra alone, we find statistically significant ($ \\geq 3 \\sigma $) OH absorption more blueshifted than [CII] emission in only one source.","By contrast, searching channel maps for signals diluted below the detection threshold in spatially-integrated spectra, we find evidence for a separate kinematic component in OH absorption in all five sources in the form of: (i) more blueshifted OH absorption than [CII] emission and/or (ii) a component in OH absorption exhibiting a different spatio-kinematic pattern than [CII] emission, the latter presumably tracing gas in a rotating disc.","Providing a more complete and accurate assessment of molecular outflows in gravitationally-lensed DSFGs, we suggest methods to better assess the precision of corrections for atmospheric absorption and to more accurately measure the source continuum in future observations."],"url":"http://arxiv.org/abs/2403.09104v1","category":"astro-ph.GA"}
{"created":"2024-03-14 04:46:29","title":"Quantum emitters in van der Waals \u03b1-MoO3","abstract":"Quantum emitters in solid-state materials are highly promising building blocks for quantum information processing and communication science. Recently, single-photon emission from van der Waals materials has been reported in transition metal dichalcogenides and hexagonal boron nitride, exhibiting the potential to realize photonic quantum technologies in two-dimensional materials. Here, we report the observation of single-photon generation from exfoliated and thermally annealed single crystals of van der Waals {\\alpha}-MoO3. The second-order correlation function measurement displays a clear photon antibunching, while the luminescence intensity exceeds 100 kcounts/s and remains stable under laser excitation. Also, the zero-phonon lines of these emitters are distributed in a spectrally narrow energy range. The theoretical calculation suggests that an oxygen vacancy defect is a possible candidate for the observed emitters. Together with photostability and brightness, quantum emitters in {\\alpha}-MoO3 provide a new avenue to realize photon-based quantum information science in van der Waals materials.","sentences":["Quantum emitters in solid-state materials are highly promising building blocks for quantum information processing and communication science.","Recently, single-photon emission from van der Waals materials has been reported in transition metal dichalcogenides and hexagonal boron nitride, exhibiting the potential to realize photonic quantum technologies in two-dimensional materials.","Here, we report the observation of single-photon generation from exfoliated and thermally annealed single crystals of van der Waals {\\alpha}-MoO3.","The second-order correlation function measurement displays a clear photon antibunching, while the luminescence intensity exceeds 100 kcounts/s and remains stable under laser excitation.","Also, the zero-phonon lines of these emitters are distributed in a spectrally narrow energy range.","The theoretical calculation suggests that an oxygen vacancy defect is a possible candidate for the observed emitters.","Together with photostability and brightness, quantum emitters in {\\alpha}-MoO3 provide a new avenue to realize photon-based quantum information science in van der Waals materials."],"url":"http://arxiv.org/abs/2403.09099v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 04:41:30","title":"Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction","abstract":"In recent years, the fusion of high spatial resolution multispectral image (HR-MSI) and low spatial resolution hyperspectral image (LR-HSI) has been recognized as an effective method for HSI super-resolution (HSI-SR). However, both HSI and MSI may be acquired under extreme conditions such as night or poorly illuminating scenarios, which may cause different exposure levels, thereby seriously downgrading the yielded HSISR. In contrast to most existing methods based on respective low-light enhancements (LLIE) of MSI and HSI followed by their fusion, a deep Unfolding HSI Super-Resolution with Automatic Exposure Correction (UHSR-AEC) is proposed, that can effectively generate a high-quality fused HSI-SR (in texture and features) even under very imbalanced exposures, thanks to the correlation between LLIE and HSI-SR taken into account. Extensive experiments are provided to demonstrate the state-of-the-art overall performance of the proposed UHSR-AEC, including comparison with some benchmark peer methods.","sentences":["In recent years, the fusion of high spatial resolution multispectral image (HR-MSI) and low spatial resolution hyperspectral image (LR-HSI) has been recognized as an effective method for HSI super-resolution (HSI-SR).","However, both HSI and MSI may be acquired under extreme conditions such as night or poorly illuminating scenarios, which may cause different exposure levels, thereby seriously downgrading the yielded HSISR.","In contrast to most existing methods based on respective low-light enhancements (LLIE) of MSI and HSI followed by their fusion, a deep Unfolding HSI Super-Resolution with Automatic Exposure Correction (UHSR-AEC) is proposed, that can effectively generate a high-quality fused HSI-SR (in texture and features) even under very imbalanced exposures, thanks to the correlation between LLIE and HSI-SR taken into account.","Extensive experiments are provided to demonstrate the state-of-the-art overall performance of the proposed UHSR-AEC, including comparison with some benchmark peer methods."],"url":"http://arxiv.org/abs/2403.09096v1","category":"eess.IV"}
{"created":"2024-03-14 04:27:32","title":"Spherical amoebae and a spherical logarithm map","abstract":"Let $G$ be a connected reductive algebraic group over $\\mathbb{C}$ with a maximal compact subgroup $K$. Let $G/H$ be a (quasi-affine) spherical homogeneous space. In the first part of the paper, following Akhiezer's definition of spherical functions, we introduce a $K$-invariant map $sLog_{\\Gamma, t}: G/H \\to \\mathbb{R}^s$ which depends on a choice of a finite set $\\Gamma$ of dominant weights and $s = |\\Gamma|$. We call $sLog_{\\Gamma, t}$ a spherical logarithm map. We show that when $\\Gamma$ generates the highest weight monoid of $G/H$, the image of the spherical logarithm map parametrizes $K$-orbits in $G/H$. This idea of using the spherical functions to understand the geometry of the space $K \\backslash G/H$ of $K$-orbits in $G/H$ can be viewed as a generalization of the classical Cartan decomposition. In the second part of the paper, we define the spherical amoeba (depending on $\\Gamma$ and $t$) of a subvariety $Y$ of $G/H$ as $sLog_{\\Gamma, t}(Y)$, and we ask for conditions under which the image of a subvariety $Y \\subset G/H$ under $sLog_{\\Gamma, t}$ converges, as $t \\to 0$, in the sense of Kuratowski to its spherical tropicalization as defined by Tevelev and Vogiannou. We prove a partial result toward answering this question, which shows in particular that the valuation cone is always contained in the Kuratowski limit of the spherical amoebae of $G/H$. We also show that the limit of the spherical amoebae of $G/H$ is equal to its valuation cone in a number of interesting examples, including when $G/H$ is horospherical, and in the case when $G/H$ is the space of hyperbolic triangles.","sentences":["Let $G$ be a connected reductive algebraic group over $\\mathbb{C}$ with a maximal compact subgroup $K$. Let $G/H$ be a (quasi-affine) spherical homogeneous space.","In the first part of the paper, following Akhiezer's definition of spherical functions, we introduce a $K$-invariant map $sLog_{\\Gamma, t}: G/H \\to \\mathbb{R}^s$ which depends on a choice of a finite set $\\Gamma$ of dominant weights and $s = |\\Gamma|$.","We call $sLog_{\\Gamma, t}$ a spherical logarithm map.","We show that when $\\Gamma$ generates the highest weight monoid of $G/H$, the image of the spherical logarithm map parametrizes $K$-orbits in $G/H$. This idea of using the spherical functions to understand the geometry of the space $K \\backslash G/H$ of $K$-orbits in $G/H$ can be viewed as a generalization of the classical Cartan decomposition.","In the second part of the paper, we define the spherical amoeba (depending on $\\Gamma$ and $t$) of a subvariety $Y$ of $G/H$ as $sLog_{\\Gamma, t}(Y)$, and we ask for conditions under which the image of a subvariety $Y \\subset G/H$ under $sLog_{\\Gamma, t}$ converges, as $t \\to 0$, in the sense of Kuratowski to its spherical tropicalization as defined by Tevelev and Vogiannou.","We prove a partial result toward answering this question, which shows in particular that the valuation cone is always contained in the Kuratowski limit of the spherical amoebae of $G/H$. We also show that the limit of the spherical amoebae of $G/H$ is equal to its valuation cone in a number of interesting examples, including when $G/H$ is horospherical, and in the case when $G/H$ is the space of hyperbolic triangles."],"url":"http://arxiv.org/abs/2403.09091v1","category":"math.AG"}
{"created":"2024-03-14 03:55:42","title":"Sparse maximum likelihood estimation for regression models","abstract":"For regression model selection under the maximum likelihood framework, we study the likelihood ratio confidence region for the regression parameter vector of a full regression model. We show that, when the confidence level increases with the sample size at a certain speed, with probability tending to one, the confidence region contains only vectors representing models having all active variables, including the parameter vector of the true model. This result leads to a consistent model selection criterion with a sparse maximum likelihood interpretation and certain advantages over popular information criteria. It also provides a large-sample characterization of models of maximum likelihood at different model sizes which shows that, for selection consistency, it suffices to consider only this small set of models.","sentences":["For regression model selection under the maximum likelihood framework, we study the likelihood ratio confidence region for the regression parameter vector of a full regression model.","We show that, when the confidence level increases with the sample size at a certain speed, with probability tending to one, the confidence region contains only vectors representing models having all active variables, including the parameter vector of the true model.","This result leads to a consistent model selection criterion with a sparse maximum likelihood interpretation and certain advantages over popular information criteria.","It also provides a large-sample characterization of models of maximum likelihood at different model sizes which shows that, for selection consistency, it suffices to consider only this small set of models."],"url":"http://arxiv.org/abs/2403.09081v1","category":"math.ST"}
{"created":"2024-03-14 03:52:33","title":"PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF Priors","abstract":"Autonomous vehicles rely extensively on perception systems to navigate and interpret their surroundings. Despite significant advancements in these systems recently, challenges persist under conditions like occlusion, extreme lighting, or in unfamiliar urban areas. Unlike these systems, humans do not solely depend on immediate observations to perceive the environment. In navigating new cities, humans gradually develop a preliminary mental map to supplement real-time perception during subsequent visits. Inspired by this human approach, we introduce a novel framework, Pre-Sight, that leverages past traversals to construct static prior memories, enhancing online perception in later navigations. Our method involves optimizing a city-scale neural radiance field with data from previous journeys to generate neural priors. These priors, rich in semantic and geometric details, are derived without manual annotations and can seamlessly augment various state-of-the-art perception models, improving their efficacy with minimal additional computational cost. Experimental results on the nuScenes dataset demonstrate the framework's high compatibility with diverse online perception models. Specifically, it shows remarkable improvements in HD-map construction and occupancy prediction tasks, highlighting its potential as a new perception framework for autonomous driving systems. Our code will be released at https://github.com/yuantianyuan01/PreSight.","sentences":["Autonomous vehicles rely extensively on perception systems to navigate and interpret their surroundings.","Despite significant advancements in these systems recently, challenges persist under conditions like occlusion, extreme lighting, or in unfamiliar urban areas.","Unlike these systems, humans do not solely depend on immediate observations to perceive the environment.","In navigating new cities, humans gradually develop a preliminary mental map to supplement real-time perception during subsequent visits.","Inspired by this human approach, we introduce a novel framework, Pre-Sight, that leverages past traversals to construct static prior memories, enhancing online perception in later navigations.","Our method involves optimizing a city-scale neural radiance field with data from previous journeys to generate neural priors.","These priors, rich in semantic and geometric details, are derived without manual annotations and can seamlessly augment various state-of-the-art perception models, improving their efficacy with minimal additional computational cost.","Experimental results on the nuScenes dataset demonstrate the framework's high compatibility with diverse online perception models.","Specifically, it shows remarkable improvements in HD-map construction and occupancy prediction tasks, highlighting its potential as a new perception framework for autonomous driving systems.","Our code will be released at https://github.com/yuantianyuan01/PreSight."],"url":"http://arxiv.org/abs/2403.09079v1","category":"cs.CV"}
{"created":"2024-03-14 03:28:17","title":"Long time dynamics for helical vortex filament in Euler flows","abstract":"We consider the three-dimensional incompressible Euler equation \\begin{equation*}\\left\\{\\begin{aligned} &\\partial_t \\Omega+U \\cdot \\nabla \\Omega-\\Omega\\cdot \\nabla U=0 \\\\ &\\Omega(x,0)=\\Omega_0(x) \\end{aligned}\\right. \\end{equation*} under the assumption that $\\Omega^z$ is helical and in the absence of vorticity stretching. Assuming that the initial vorticity $\\Omega_0$ is primarily concentrated within an $\\epsilon$ neighborhood of a helix $\\Gamma_0$, we prove that its solution $\\Omega(\\cdot,t)$ remain concentrated near a helix $\\Gamma(t)$ for any $t \\in [0,T)$, where $\\Gamma(t)$ can be interpreted as $\\Gamma_0$ rotating around the $x_3$ axis with a speed $V=C\\log \\frac{1}{\\epsilon}+O(1)$. It should be emphasized that the dynamics for the helical vortex filament are exhibited on the time interval $[0,T)$, which is longer than $\\left[0, \\frac{T}{\\log\\frac{1}{\\epsilon}}\\right)$.","sentences":["We consider the three-dimensional incompressible Euler equation \\begin{equation*}\\left\\{\\begin{aligned} &\\partial_t \\Omega+U \\cdot \\nabla \\Omega-\\Omega\\cdot \\nabla U=0 \\\\ &\\Omega(x,0)=\\Omega_0(x) \\end{aligned}\\right.","\\end{equation*} under the assumption that $\\Omega^z$ is helical and in the absence of vorticity stretching.","Assuming that the initial vorticity $\\Omega_0$ is primarily concentrated within an $\\epsilon$ neighborhood of a helix $\\Gamma_0$, we prove that its solution $\\Omega(\\cdot,t)$ remain concentrated near a helix $\\Gamma(t)$ for any $t \\in [0,T)$, where $\\Gamma(t)$ can be interpreted as $\\Gamma_0$ rotating around the $x_3$ axis with a speed $V=C\\log \\frac{1}{\\epsilon}+O(1)$. It should be emphasized that the dynamics for the helical vortex filament are exhibited on the time interval $[0,T)$, which is longer than $\\left[0, \\frac{T}{\\log\\frac{1}{\\epsilon}}\\right)$."],"url":"http://arxiv.org/abs/2403.09071v1","category":"math.AP"}
{"created":"2024-03-14 02:38:09","title":"CLOAF: CoLlisiOn-Aware Human Flow","abstract":"Even the best current algorithms for estimating body 3D shape and pose yield results that include body self-intersections. In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still imposing body shape constraints. We show that, unlike earlier approaches to addressing this issue, ours completely eliminates the self-intersections without compromising the accuracy of the reconstructions. Being differentiable, CLOAF can be used to fine-tune pose and shape estimation baselines to improve their overall performance and eliminate self-intersections in their predictions. Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion field induced by the user. CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior.","sentences":["Even the best current algorithms for estimating body 3D shape and pose yield results that include body self-intersections.","In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still imposing body shape constraints.","We show that, unlike earlier approaches to addressing this issue, ours completely eliminates the self-intersections without compromising the accuracy of the reconstructions.","Being differentiable, CLOAF can be used to fine-tune pose and shape estimation baselines to improve their overall performance and eliminate self-intersections in their predictions.","Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion field induced by the user.","CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior."],"url":"http://arxiv.org/abs/2403.09050v1","category":"cs.CV"}
{"created":"2024-03-14 02:37:31","title":"Joule-Thomson cooling of CO2 injected into aquifer under heat exchange with adjacent formations by Newtons law- 1D exact solution","abstract":"This paper discusses axi-symmetric flow during CO2 injection into a non-adiabatic reservoir accounting for Joule-Thomson cooling and steady-state heat exchange between the reservoir and the adjacent layers by Newtons law. An exact solution for this 1D problem is derived and a new method for model validation by comparison with quasi 2D analytical heat-conductivity solution is developed. The temperature profile obtained by the analytical solution shows a temperature decrease to a minimum value, followed by a sharp increase to initial reservoir temperature. The analytical model exhibits stabilization of the temperature profile and cooled zone. When accounting for heat gain from adjacent layers, the minimum temperature is higher compared to the case with no heat exchange. The analytical model can be used to evaluate the risks of hydrate formation and/or rock integrity during CO2 storage in depleted reservoirs.","sentences":["This paper discusses axi-symmetric flow during CO2 injection into a non-adiabatic reservoir accounting for Joule-Thomson cooling and steady-state heat exchange between the reservoir and the adjacent layers by Newtons law.","An exact solution for this 1D problem is derived and a new method for model validation by comparison with quasi 2D analytical heat-conductivity solution is developed.","The temperature profile obtained by the analytical solution shows a temperature decrease to a minimum value, followed by a sharp increase to initial reservoir temperature.","The analytical model exhibits stabilization of the temperature profile and cooled zone.","When accounting for heat gain from adjacent layers, the minimum temperature is higher compared to the case with no heat exchange.","The analytical model can be used to evaluate the risks of hydrate formation and/or rock integrity during CO2 storage in depleted reservoirs."],"url":"http://arxiv.org/abs/2403.09049v1","category":"physics.geo-ph"}
{"created":"2024-03-14 02:29:51","title":"Entangled vs. Separable Choice","abstract":"We study joint probabilistic choice rules that describe the behavior of two decision makers, each facing a possibly different menu. These choice rules are separable when they can be factored into autonomous choices from each individual solely correlated through their individual probabilistic choice rules. Despite recent interest in studying such rules, a complete characterization of the restrictions on them remains an open question. A reasonable conjecture is that such restrictions on separable joint choice can be factored into individual choice restrictions. We name these restrictions separable and show that this conjecture is true if and only if the probabilistic choice rule of at least one decision maker uniquely identifies the distribution over deterministic choice rules. Otherwise, entangled choice rules exist that satisfy separable restrictions yet are not separable. The possibility of entangled choice complicates the characterization of separable choice since one needs to augment the separable restrictions with the new emerging ones.","sentences":["We study joint probabilistic choice rules that describe the behavior of two decision makers, each facing a possibly different menu.","These choice rules are separable when they can be factored into autonomous choices from each individual solely correlated through their individual probabilistic choice rules.","Despite recent interest in studying such rules, a complete characterization of the restrictions on them remains an open question.","A reasonable conjecture is that such restrictions on separable joint choice can be factored into individual choice restrictions.","We name these restrictions separable and show that this conjecture is true if and only if the probabilistic choice rule of at least one decision maker uniquely identifies the distribution over deterministic choice rules.","Otherwise, entangled choice rules exist that satisfy separable restrictions yet are not separable.","The possibility of entangled choice complicates the characterization of separable choice since one needs to augment the separable restrictions with the new emerging ones."],"url":"http://arxiv.org/abs/2403.09045v1","category":"econ.GN"}
{"created":"2024-03-14 02:29:36","title":"Relationship between General MP and DPP for the Stochastic Recursive Optimal Control Problem With Jumps: Viscosity Solution Framework","abstract":"This paper is concerned with the relationship between general maximum principle and dynamic programming principle for the stochastic recursive optimal control problem with jumps, where the control domain is not necessarily convex. Relations among the adjoint processes, the generalized Hamiltonian function and the value function are proved, under the assumption of a smooth value function and within the framework of viscosity solutions, respectively. Some examples are given to illustrate the theoretical results.","sentences":["This paper is concerned with the relationship between general maximum principle and dynamic programming principle for the stochastic recursive optimal control problem with jumps, where the control domain is not necessarily convex.","Relations among the adjoint processes, the generalized Hamiltonian function and the value function are proved, under the assumption of a smooth value function and within the framework of viscosity solutions, respectively.","Some examples are given to illustrate the theoretical results."],"url":"http://arxiv.org/abs/2403.09044v1","category":"math.OC"}
{"created":"2024-03-14 02:28:21","title":"Recurrent Events Modeling Based on a Reflected Brownian Motion with Application to Hypoglycemia","abstract":"Patients with type 2 diabetes need to closely monitor blood sugar levels as their routine diabetes self-management. Although many treatment agents aim to tightly control blood sugar, hypoglycemia often stands as an adverse event. In practice, patients can observe hypoglycemic events more easily than hyperglycemic events due to the perception of neurogenic symptoms. We propose to model each patient's observed hypoglycemic event as a lower-boundary crossing event for a reflected Brownian motion with an upper reflection barrier. The lower-boundary is set by clinical standards. To capture patient heterogeneity and within-patient dependence, covariates and a patient level frailty are incorporated into the volatility and the upper reflection barrier. This framework provides quantification for the underlying glucose level variability, patients heterogeneity, and risk factors' impact on glucose. We make inferences based on a Bayesian framework using Markov chain Monte Carlo. Two model comparison criteria, the Deviance Information Criterion and the Logarithm of the Pseudo-Marginal Likelihood, are used for model selection. The methodology is validated in simulation studies. In analyzing a dataset from the diabetic patients in the DURABLE trial, our model provides adequate fit, generates data similar to the observed data, and offers insights that could be missed by other models.","sentences":["Patients with type 2 diabetes need to closely monitor blood sugar levels as their routine diabetes self-management.","Although many treatment agents aim to tightly control blood sugar, hypoglycemia often stands as an adverse event.","In practice, patients can observe hypoglycemic events more easily than hyperglycemic events due to the perception of neurogenic symptoms.","We propose to model each patient's observed hypoglycemic event as a lower-boundary crossing event for a reflected Brownian motion with an upper reflection barrier.","The lower-boundary is set by clinical standards.","To capture patient heterogeneity and within-patient dependence, covariates and a patient level frailty are incorporated into the volatility and the upper reflection barrier.","This framework provides quantification for the underlying glucose level variability, patients heterogeneity, and risk factors' impact on glucose.","We make inferences based on a Bayesian framework using Markov chain Monte Carlo.","Two model comparison criteria, the Deviance Information Criterion and the Logarithm of the Pseudo-Marginal Likelihood, are used for model selection.","The methodology is validated in simulation studies.","In analyzing a dataset from the diabetic patients in the DURABLE trial, our model provides adequate fit, generates data similar to the observed data, and offers insights that could be missed by other models."],"url":"http://arxiv.org/abs/2403.09042v1","category":"stat.ME"}
{"created":"2024-03-14 02:25:35","title":"The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?","abstract":"Large vision-language models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against multi-modal jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating potential bias in the existing datasets. Second, we observe performance improvement by utilizing the first logit distributions on three additional tasks, including indicting uncertainty in math solving, mitigating hallucination, and image classification. Last, with the same training data, simply finetuning LVLMs improve models' performance but is still inferior to linear probing on these tasks.","sentences":["Large vision-language models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions.","This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs.","We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against multi-modal jailbreaking attack, and identifying deceptive questions.","Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation.","Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content.","In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating potential bias in the existing datasets.","Second, we observe performance improvement by utilizing the first logit distributions on three additional tasks, including indicting uncertainty in math solving, mitigating hallucination, and image classification.","Last, with the same training data, simply finetuning LVLMs improve models' performance but is still inferior to linear probing on these tasks."],"url":"http://arxiv.org/abs/2403.09037v1","category":"cs.CV"}
{"created":"2024-03-14 02:11:16","title":"rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours","abstract":"Remote photoplethysmography (rPPG) technique extracts blood volume pulse (BVP) signals from subtle pixel changes in video frames. This study introduces rFaceNet, an advanced rPPG method that enhances the extraction of facial BVP signals with a focus on facial contours. rFaceNet integrates identity-specific facial contour information and eliminates redundant data. It efficiently extracts facial contours from temporally normalized frame inputs through a Temporal Compressor Unit (TCU) and steers the model focus to relevant facial regions by using the Cross-Task Feature Combiner (CTFC). Through elaborate training, the quality and interpretability of facial physiological signals extracted by rFaceNet are greatly improved compared to previous methods. Moreover, our novel approach demonstrates superior performance than SOTA methods in various heart rate estimation benchmarks.","sentences":["Remote photoplethysmography (rPPG) technique extracts blood volume pulse (BVP) signals from subtle pixel changes in video frames.","This study introduces rFaceNet, an advanced rPPG method that enhances the extraction of facial BVP signals with a focus on facial contours.","rFaceNet integrates identity-specific facial contour information and eliminates redundant data.","It efficiently extracts facial contours from temporally normalized frame inputs through a Temporal Compressor Unit (TCU) and steers the model focus to relevant facial regions by using the Cross-Task Feature Combiner (CTFC).","Through elaborate training, the quality and interpretability of facial physiological signals extracted by rFaceNet are greatly improved compared to previous methods.","Moreover, our novel approach demonstrates superior performance than SOTA methods in various heart rate estimation benchmarks."],"url":"http://arxiv.org/abs/2403.09034v1","category":"cs.CV"}
{"created":"2024-03-14 01:46:56","title":"Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization","abstract":"Current spectral compressed sensing methods via Hankel matrix completion employ symmetric factorization to demonstrate the low-rank property of the Hankel matrix. However, previous non-convex gradient methods only utilize asymmetric factorization to achieve spectral compressed sensing. In this paper, we propose a novel nonconvex projected gradient descent method for spectral compressed sensing via symmetric factorization named Symmetric Hankel Projected Gradient Descent (SHGD), which updates only one matrix and avoids a balancing regularization term. SHGD reduces about half of the computation and storage costs compared to the prior gradient method based on asymmetric factorization. {Besides, the symmetric factorization employed in our work is completely novel to the prior low-rank factorization model, introducing a new factorization ambiguity under complex orthogonal transformation}. Novel distance metrics are designed for our factorization method and a linear convergence guarantee to the desired signal is established with $O(r^2\\log(n))$ observations. Numerical simulations demonstrate the superior performance of the proposed SHGD method in phase transitions and computation efficiency compared to state-of-the-art methods.","sentences":["Current spectral compressed sensing methods via Hankel matrix completion employ symmetric factorization to demonstrate the low-rank property of the Hankel matrix.","However, previous non-convex gradient methods only utilize asymmetric factorization to achieve spectral compressed sensing.","In this paper, we propose a novel nonconvex projected gradient descent method for spectral compressed sensing via symmetric factorization named Symmetric Hankel Projected Gradient Descent (SHGD), which updates only one matrix and avoids a balancing regularization term.","SHGD reduces about half of the computation and storage costs compared to the prior gradient method based on asymmetric factorization.","{Besides, the symmetric factorization employed in our work is completely novel to the prior low-rank factorization model, introducing a new factorization ambiguity under complex orthogonal transformation}.","Novel distance metrics are designed for our factorization method and a linear convergence guarantee to the desired signal is established with $O(r^2\\log(n))$ observations.","Numerical simulations demonstrate the superior performance of the proposed SHGD method in phase transitions and computation efficiency compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.09031v1","category":"cs.IR"}
{"created":"2024-03-13 23:30:26","title":"Ventilation and Temperature Control for Energy-efficient and Healthy Buildings: A Differentiable PDE Approach","abstract":"In this paper, we introduce a novel framework for building learning and control, focusing on ventilation and thermal management to enhance energy efficiency. We validate the performance of the proposed framework in system model learning via two case studies: a synthetic study focusing on the joint learning of temperature and CO2 fields, and an application to a real-world dataset for CO2 field learning. For building control, we demonstrate that the proposed framework can optimize the control actions and significantly reduce the energy cost while maintaining a comfort and healthy indoor environment. When compared to existing traditional methods, an optimization-based method with ODE models and reinforcement learning, our approach can significantly reduce the energy consumption while guarantees all the safety-critical air quality and control constraints. Promising future research directions involve validating and improving the proposed PDE models through accurate estimation of airflow fields within indoor environments. Additionally, incorporating uncertainty modeling into the PDE framework for HVAC control presents an opportunity to enhance the efficiency and reliability of building HVAC system management.","sentences":["In this paper, we introduce a novel framework for building learning and control, focusing on ventilation and thermal management to enhance energy efficiency.","We validate the performance of the proposed framework in system model learning via two case studies: a synthetic study focusing on the joint learning of temperature and CO2 fields, and an application to a real-world dataset for CO2 field learning.","For building control, we demonstrate that the proposed framework can optimize the control actions and significantly reduce the energy cost while maintaining a comfort and healthy indoor environment.","When compared to existing traditional methods, an optimization-based method with ODE models and reinforcement learning, our approach can significantly reduce the energy consumption while guarantees all the safety-critical air quality and control constraints.","Promising future research directions involve validating and improving the proposed PDE models through accurate estimation of airflow fields within indoor environments.","Additionally, incorporating uncertainty modeling into the PDE framework for HVAC control presents an opportunity to enhance the efficiency and reliability of building HVAC system management."],"url":"http://arxiv.org/abs/2403.08996v1","category":"eess.SY"}
{"created":"2024-03-13 23:07:46","title":"Pseudo-differential operators on Homogeneous vector bundles over compact homogeneous manifolds","abstract":"In this work, we introduce a global theory of subelliptic pseudo-differential operators on arbitrary homogeneous vector bundles over orientable compact homogeneous manifolds. We will show that a global pseudo-differential calculus can be associated to the operators acting on any pair of homogeneous vector-bundles with base space $M,$ if the compact Lie group $G$ that acts on $M=G/K$ is endowed with a (Riemannian or) sub-Riemannian structure. This is always possible if we choose on $G$ a sub-Laplacian associated to a H\\\"ormander system of vector-fields or we fix the Laplace-Beltrami operator on $G$. We begin with developing a global subelliptic symbolic calculus for vector-valued pseudo-differential operators on $G$ and then, we show that this vector-valued calculus induces a pseudo-differential calculus on homogeneous vector bundles, which, among other things, is stable under the action of the complex functional calculus. We prove global versions of the Calder\\'on-Vaillancourt theorem, Fefferman theorem and also, of the G\\r{a}rding inequality. We present applications of the obtained G\\r{a}rding inequality to the wellposedness of evolution problems. We characterise the classes of pseudo-differential operators on homogeneous vector bundles in the sense of H\\\"ormander (which are defined by using local coordinate systems) in terms of their global symbols. Finally, using this formalism, we compute the global symbol of the exterior derivative, its adjoint, and the symbol of the Dirac operator on the vector bundle of differential forms. We hope that this work will provide a solid foundation for further research using the global quantisation of operators on (vector-bundles over) compact homogeneous manifolds.","sentences":["In this work, we introduce a global theory of subelliptic pseudo-differential operators on arbitrary homogeneous vector bundles over orientable compact homogeneous manifolds.","We will show that a global pseudo-differential calculus can be associated to the operators acting on any pair of homogeneous vector-bundles with base space $M,$ if the compact Lie group $G$ that acts on $M=G/K$ is endowed with a (Riemannian or) sub-Riemannian structure.","This is always possible if we choose on $G$ a sub-Laplacian associated to a H\\\"ormander system of vector-fields or we fix the Laplace-Beltrami operator on $G$. We begin with developing a global subelliptic symbolic calculus for vector-valued pseudo-differential operators on $G$ and then, we show that this vector-valued calculus induces a pseudo-differential calculus on homogeneous vector bundles, which, among other things, is stable under the action of the complex functional calculus.","We prove global versions of the Calder\\'on-Vaillancourt theorem, Fefferman theorem and also, of the G\\r{a}rding inequality.","We present applications of the obtained G\\r{a}rding inequality to the wellposedness of evolution problems.","We characterise the classes of pseudo-differential operators on homogeneous vector bundles in the sense of H\\\"ormander (which are defined by using local coordinate systems) in terms of their global symbols.","Finally, using this formalism, we compute the global symbol of the exterior derivative, its adjoint, and the symbol of the Dirac operator on the vector bundle of differential forms.","We hope that this work will provide a solid foundation for further research using the global quantisation of operators on (vector-bundles over) compact homogeneous manifolds."],"url":"http://arxiv.org/abs/2403.08990v1","category":"math.AP"}
{"created":"2024-03-13 22:06:44","title":"7T MRI Synthesization from 3T Acquisitions","abstract":"Supervised deep learning techniques can be used to generate synthetic 7T MRIs from 3T MRI inputs. This image enhancement process leverages the advantages of ultra-high-field MRI to improve the signal-to-noise and contrast-to-noise ratios of 3T acquisitions. In this paper, we introduce multiple novel 7T synthesization algorithms based on custom-designed variants of the V-Net convolutional neural network. We demonstrate that the V-Net based model has superior performance in enhancing both single-site and multi-site MRI datasets compared to the existing benchmark model. When trained on 3T-7T MRI pairs from 8 subjects with mild Traumatic Brain Injury (TBI), our model achieves state-of-the-art 7T synthesization performance. Compared to previous works, synthetic 7T images generated from our pipeline also display superior enhancement of pathological tissue. Additionally, we implement and test a data augmentation scheme for training models that are robust to variations in the input distribution. This allows synthetic 7T models to accommodate intra-scanner and inter-scanner variability in multisite datasets. On a harmonized dataset consisting of 18 3T-7T MRI pairs from two institutions, including both healthy subjects and those with mild TBI, our model maintains its performance and can generalize to 3T MRI inputs with lower resolution. Our findings demonstrate the promise of V-Net based models for MRI enhancement and offer a preliminary probe into improving the generalizability of synthetic 7T models with data augmentation.","sentences":["Supervised deep learning techniques can be used to generate synthetic 7T MRIs from 3T MRI inputs.","This image enhancement process leverages the advantages of ultra-high-field MRI to improve the signal-to-noise and contrast-to-noise ratios of 3T acquisitions.","In this paper, we introduce multiple novel 7T synthesization algorithms based on custom-designed variants of the V-Net convolutional neural network.","We demonstrate that the V-Net based model has superior performance in enhancing both single-site and multi-site MRI datasets compared to the existing benchmark model.","When trained on 3T-7T MRI pairs from 8 subjects with mild Traumatic Brain Injury (TBI), our model achieves state-of-the-art 7T synthesization performance.","Compared to previous works, synthetic 7T images generated from our pipeline also display superior enhancement of pathological tissue.","Additionally, we implement and test a data augmentation scheme for training models that are robust to variations in the input distribution.","This allows synthetic 7T models to accommodate intra-scanner and inter-scanner variability in multisite datasets.","On a harmonized dataset consisting of 18 3T-7T MRI pairs from two institutions, including both healthy subjects and those with mild TBI, our model maintains its performance and can generalize to 3T MRI inputs with lower resolution.","Our findings demonstrate the promise of V-Net based models for MRI enhancement and offer a preliminary probe into improving the generalizability of synthetic 7T models with data augmentation."],"url":"http://arxiv.org/abs/2403.08979v1","category":"eess.IV"}
{"created":"2024-03-13 22:06:03","title":"AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents","abstract":"The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential decision-making benchmarks.","sentences":["The primary limitation of large language models (LLMs) is their restricted understanding of the world.","This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge.","In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences.","Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines.","Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable.","As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process.","We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential decision-making benchmarks."],"url":"http://arxiv.org/abs/2403.08978v1","category":"cs.CL"}
{"created":"2024-03-13 21:12:24","title":"Managing Distributional Ambiguity in Stochastic Optimization through a Statistical Upper Bound Framework","abstract":"Stochastic optimization is often hampered by distributional ambiguity, where critical probability distributions are poorly characterized or unknown. Addressing this challenge, we introduce a new framework that targets the minimization of a statistical upper bound for the expected value of uncertain objectives, facilitating more statistically robust decision-making. Central to our approach is the Average Percentile Upper Bound (APUB), a novel construct that simultaneously delivers a statistically rigorous upper bound for the population mean and a meaningful risk metric for the sample mean. The integration of APUB into stochastic optimization not only fortifies the process against distributional ambiguity but also reinforces key data-driven decision-making attributes, such as reliability, consistency, and comprehensibility. Notably, APUB-enriched optimization problems feature tractability, with particular advantages in two-stage stochastic optimization with random recourse. Empirical demonstrations on two-stage product mix and multi-product newsvendor benchmark problems reveal the benefit of the APUB optimization framework, in comparison with conventional techniques such as sample average approximation and distributionally robust optimization.","sentences":["Stochastic optimization is often hampered by distributional ambiguity, where critical probability distributions are poorly characterized or unknown.","Addressing this challenge, we introduce a new framework that targets the minimization of a statistical upper bound for the expected value of uncertain objectives, facilitating more statistically robust decision-making.","Central to our approach is the Average Percentile Upper Bound (APUB), a novel construct that simultaneously delivers a statistically rigorous upper bound for the population mean and a meaningful risk metric for the sample mean.","The integration of APUB into stochastic optimization not only fortifies the process against distributional ambiguity but also reinforces key data-driven decision-making attributes, such as reliability, consistency, and comprehensibility.","Notably, APUB-enriched optimization problems feature tractability, with particular advantages in two-stage stochastic optimization with random recourse.","Empirical demonstrations on two-stage product mix and multi-product newsvendor benchmark problems reveal the benefit of the APUB optimization framework, in comparison with conventional techniques such as sample average approximation and distributionally robust optimization."],"url":"http://arxiv.org/abs/2403.08966v1","category":"math.OC"}
{"created":"2024-03-13 21:08:52","title":"Hyperelasticity of Blood Clots: Bridging the Gap between Microscopic and Continuum Scales","abstract":"The biomechanical properties of blood clots, which are dictated by their compositions and micro-structures, play a critical role in determining their fates, occlusion, persistency, or embolization in the human circulatory system. While numerous constitutive models have emerged to describe the biomechanics of blood clots, the majority of these models have primarily focused on the macroscopic deformation of the clots and the resultant strain-stress correlations without depicting the microscopic contributions from their structural components, such as fibrin fibers, fibrin network and red blood cells. This work addresses the gap in current scientific understanding by quantifying how changes in the microstructure of blood clots affect its mechanical responses under different external stresses. We leverage our previous published work to develop a hyperelastic potential model for blood clots, which incorporates six distinct strain-energy components to describe the alignment of fibers, the entropic and enthalpic stretching of fibrin fibers, the buckling of these fibers, clot densification, and clot jamming.These strain-energy components are represented by a combination of simple harmonic oscillators, one-sided harmonic potentials, and a Gaussian potential. The proposed model, which is C0, C1, and C2 continuous with a total of 13 parameters, has been validated against three data sets: fibrin clot in tension, blood clot in compression, and blood clots in shear, demonstrating its robustness. Subsequent simulations of a microscopic blood clot model are performed to uncover mechanistic correlations for a majority of the hyperelastic potential's stiffness/strain parameters. Our results show that only one proposed term concerning fiber buckling needs further refinement, while the remaining five strain-energy terms appear to describe precisely what they were intended to.","sentences":["The biomechanical properties of blood clots, which are dictated by their compositions and micro-structures, play a critical role in determining their fates, occlusion, persistency, or embolization in the human circulatory system.","While numerous constitutive models have emerged to describe the biomechanics of blood clots, the majority of these models have primarily focused on the macroscopic deformation of the clots and the resultant strain-stress correlations without depicting the microscopic contributions from their structural components, such as fibrin fibers, fibrin network and red blood cells.","This work addresses the gap in current scientific understanding by quantifying how changes in the microstructure of blood clots affect its mechanical responses under different external stresses.","We leverage our previous published work to develop a hyperelastic potential model for blood clots, which incorporates six distinct strain-energy components to describe the alignment of fibers, the entropic and enthalpic stretching of fibrin fibers, the buckling of these fibers, clot densification, and clot jamming.","These strain-energy components are represented by a combination of simple harmonic oscillators, one-sided harmonic potentials, and a Gaussian potential.","The proposed model, which is C0, C1, and C2 continuous with a total of 13 parameters, has been validated against three data sets: fibrin clot in tension, blood clot in compression, and blood clots in shear, demonstrating its robustness.","Subsequent simulations of a microscopic blood clot model are performed to uncover mechanistic correlations for a majority of the hyperelastic potential's stiffness/strain parameters.","Our results show that only one proposed term concerning fiber buckling needs further refinement, while the remaining five strain-energy terms appear to describe precisely what they were intended to."],"url":"http://arxiv.org/abs/2403.08964v1","category":"physics.bio-ph"}
{"created":"2024-03-13 20:46:22","title":"Morphological instability at topological defects in a three-dimensional vertex model for spherical epithelia","abstract":"Epithelial monolayers are a central building block of complex organisms. Topological defects have emerged as important elements for single cell behavior in flat epithelia. Here we theoretically study such defects in a three-dimensional vertex model for spherical epithelia like cysts or intestinal organoids. We find that they lead to the same generic morphological instability to an icosahedral shape as it is known from spherical elastic shells like virus capsids, polymerized vesicles or buckyballs. We derive analytical expressions for the effective stretching and bending moduli as a function of the parameters of the vertex model, in excellent agreement with computer simulations. These equations accurately predict both the buckling of a flat epithelial monolayer under uniaxial compression and the faceting transition around the topological defects in spherical epithelia. We further show that localized apico-basal tension asymmetries allow them to reduce the transition threshold to small system sizes.","sentences":["Epithelial monolayers are a central building block of complex organisms.","Topological defects have emerged as important elements for single cell behavior in flat epithelia.","Here we theoretically study such defects in a three-dimensional vertex model for spherical epithelia like cysts or intestinal organoids.","We find that they lead to the same generic morphological instability to an icosahedral shape as it is known from spherical elastic shells like virus capsids, polymerized vesicles or buckyballs.","We derive analytical expressions for the effective stretching and bending moduli as a function of the parameters of the vertex model, in excellent agreement with computer simulations.","These equations accurately predict both the buckling of a flat epithelial monolayer under uniaxial compression and the faceting transition around the topological defects in spherical epithelia.","We further show that localized apico-basal tension asymmetries allow them to reduce the transition threshold to small system sizes."],"url":"http://arxiv.org/abs/2403.08954v1","category":"physics.bio-ph"}
{"created":"2024-03-13 20:40:30","title":"On the Intersection of Two Conics","abstract":"Finding the intersection of two conics is a commonly occurring problem. For example, it occurs when identifying patterns of craters on the lunar surface, detecting the orientation of a face from a single image, or estimating the attitude of a camera from 2D-to-3D point correspondences. Regardless of the application, the study of this classical problem presents a number of delightful geometric results.   In most of the cases, the intersection points are computed by finding the degenerate conic consisting of two lines passing through the common points. Once a linear combination of the two conic matrices has been constructed, the solution of an eigenvalue problem provides four possible degenerate conics, of which only one coincides with the sought pair of lines. Then, the method proceeds by finding the intersection between one of the conics and the two lines. Other approaches make use of different methods, such as Gr\\\"obner bases or geometric algebra.   Conic intersection, however, may be solved more intuitively with a convenient change of coordinates. In this work, we will consider two such coordinate changes. In the first approach, one of the conics is transformed into a parabola, which reduces the intersection problem to finding the solution of a quartic. In the second approach, we instead use the concept of self-polar triangles - which, amazingly, reduces the conic intersection problem to the solution of a simple quadratic equation.","sentences":["Finding the intersection of two conics is a commonly occurring problem.","For example, it occurs when identifying patterns of craters on the lunar surface, detecting the orientation of a face from a single image, or estimating the attitude of a camera from 2D-to-3D point correspondences.","Regardless of the application, the study of this classical problem presents a number of delightful geometric results.   ","In most of the cases, the intersection points are computed by finding the degenerate conic consisting of two lines passing through the common points.","Once a linear combination of the two conic matrices has been constructed, the solution of an eigenvalue problem provides four possible degenerate conics, of which only one coincides with the sought pair of lines.","Then, the method proceeds by finding the intersection between one of the conics and the two lines.","Other approaches make use of different methods, such as Gr\\\"obner bases or geometric algebra.   ","Conic intersection, however, may be solved more intuitively with a convenient change of coordinates.","In this work, we will consider two such coordinate changes.","In the first approach, one of the conics is transformed into a parabola, which reduces the intersection problem to finding the solution of a quartic.","In the second approach, we instead use the concept of self-polar triangles - which, amazingly, reduces the conic intersection problem to the solution of a simple quadratic equation."],"url":"http://arxiv.org/abs/2403.08953v1","category":"math.AG"}
{"created":"2024-03-13 20:38:09","title":"The inviscid limit for long time statistics of the one-dimensional stochastic Ginzburg-Landau equation","abstract":"We consider the long time statistics of a one-dimensional stochastic Ginzburg-Landau equation with cubic nonlinearity while being subjected to random perturbations via an additive Gaussian noise. Under the assumption that sufficiently many directions of the phase space are stochastically forced, we find that the dynamics is attractive toward the unique invariant probability measure with a polynomial rate that is independent of the vanishing viscosity. This relies on a coupling technique exploiting a Foias-Prodi argument specifically tailored to the system. Then, in the inviscid regime, we show that the sequence of invariant measures converges toward the invariant measure of the stochastic Schr\\\"odinger equation in a suitable Wasserstein distance. Together with the uniform polynomial mixing, we obtain the validity of the inviscid limit for the solutions on the infinite time horizon with a log log rate.","sentences":["We consider the long time statistics of a one-dimensional stochastic Ginzburg-Landau equation with cubic nonlinearity while being subjected to random perturbations via an additive Gaussian noise.","Under the assumption that sufficiently many directions of the phase space are stochastically forced, we find that the dynamics is attractive toward the unique invariant probability measure with a polynomial rate that is independent of the vanishing viscosity.","This relies on a coupling technique exploiting a Foias-Prodi argument specifically tailored to the system.","Then, in the inviscid regime, we show that the sequence of invariant measures converges toward the invariant measure of the stochastic Schr\\\"odinger equation in a suitable Wasserstein distance.","Together with the uniform polynomial mixing, we obtain the validity of the inviscid limit for the solutions on the infinite time horizon with a log log rate."],"url":"http://arxiv.org/abs/2403.08951v1","category":"math.PR"}
{"created":"2024-03-13 20:13:25","title":"FogGuard: guarding YOLO against fog using perceptual loss","abstract":"In this paper, we present a novel fog-aware object detection network called FogGuard, designed to address the challenges posed by foggy weather conditions. Autonomous driving systems heavily rely on accurate object detection algorithms, but adverse weather conditions can significantly impact the reliability of deep neural networks (DNNs).   Existing approaches fall into two main categories, 1) image enhancement such as IA-YOLO 2) domain adaptation based approaches. Image enhancement based techniques attempt to generate fog-free image. However, retrieving a fogless image from a foggy image is a much harder problem than detecting objects in a foggy image. Domain-adaptation based approaches, on the other hand, do not make use of labelled datasets in the target domain. Both categories of approaches are attempting to solve a harder version of the problem. Our approach builds over fine-tuning on the   Our framework is specifically designed to compensate for foggy conditions present in the scene, ensuring robust performance even. We adopt YOLOv3 as the baseline object detection algorithm and introduce a novel Teacher-Student Perceptual loss, to high accuracy object detection in foggy images.   Through extensive evaluations on common datasets such as PASCAL VOC and RTTS, we demonstrate the improvement in performance achieved by our network. We demonstrate that FogGuard achieves 69.43\\% mAP, as compared to 57.78\\% for YOLOv3 on the RTTS dataset.   Furthermore, we show that while our training method increases time complexity, it does not introduce any additional overhead during inference compared to the regular YOLO network.","sentences":["In this paper, we present a novel fog-aware object detection network called FogGuard, designed to address the challenges posed by foggy weather conditions.","Autonomous driving systems heavily rely on accurate object detection algorithms, but adverse weather conditions can significantly impact the reliability of deep neural networks (DNNs).   ","Existing approaches fall into two main categories, 1) image enhancement such as IA-YOLO 2) domain adaptation based approaches.","Image enhancement based techniques attempt to generate fog-free image.","However, retrieving a fogless image from a foggy image is a much harder problem than detecting objects in a foggy image.","Domain-adaptation based approaches, on the other hand, do not make use of labelled datasets in the target domain.","Both categories of approaches are attempting to solve a harder version of the problem.","Our approach builds over fine-tuning on the   Our framework is specifically designed to compensate for foggy conditions present in the scene, ensuring robust performance even.","We adopt YOLOv3 as the baseline object detection algorithm and introduce a novel Teacher-Student Perceptual loss, to high accuracy object detection in foggy images.   ","Through extensive evaluations on common datasets such as PASCAL VOC and RTTS, we demonstrate the improvement in performance achieved by our network.","We demonstrate that FogGuard achieves 69.43\\% mAP, as compared to 57.78\\% for YOLOv3 on the RTTS dataset.   ","Furthermore, we show that while our training method increases time complexity, it does not introduce any additional overhead during inference compared to the regular YOLO network."],"url":"http://arxiv.org/abs/2403.08939v1","category":"cs.CV"}
{"created":"2024-03-13 20:12:03","title":"A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator","abstract":"We consider learning an unknown target function $f_*$ using kernel ridge regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\\leq n$, where $u_i \\in U$ is a covariate vector and $y_i = f_* (u_i) +\\varepsilon_i \\in \\mathbb{R}$. A recent string of work has empirically shown that the test error of KRR can be well approximated by a closed-form estimate derived from an `equivalent' sequence model that only depends on the spectrum of the kernel operator. However, a theoretical justification for this equivalence has so far relied either on restrictive assumptions -- such as subgaussian independent eigenfunctions -- , or asymptotic derivations for specific kernels in high dimensions.   In this paper, we prove that this equivalence holds for a general class of problems satisfying some spectral and concentration properties on the kernel eigendecomposition. Specifically, we establish in this setting a non-asymptotic deterministic approximation for the test error of KRR -- with explicit non-asymptotic bounds -- that only depends on the eigenvalues and the target function alignment to the eigenvectors of the kernel. Our proofs rely on a careful derivation of deterministic equivalents for random matrix functionals in the dimension free regime pioneered by Cheng and Montanari (2022).   We apply this setting to several classical examples and show an excellent agreement between theoretical predictions and numerical simulations. These results rely on having access to the eigendecomposition of the kernel operator. Alternatively, we prove that, under this same setting, the generalized cross-validation (GCV) estimator concentrates on the test error uniformly over a range of ridge regularization parameter that includes zero (the interpolating solution). As a consequence, the GCV estimator can be used to estimate from data the test error and optimal regularization parameter for KRR.","sentences":["We consider learning an unknown target function $f_*$ using kernel ridge regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\\leq n$, where $u_i \\in U$ is a covariate vector and $y_i = f_*","(u_i) +\\varepsilon_i \\in \\mathbb{R}$. A recent string of work has empirically shown that the test error of KRR can be well approximated by a closed-form estimate derived from an `equivalent' sequence model that only depends on the spectrum of the kernel operator.","However, a theoretical justification for this equivalence has so far relied either on restrictive assumptions -- such as subgaussian independent eigenfunctions -- , or asymptotic derivations for specific kernels in high dimensions.   ","In this paper, we prove that this equivalence holds for a general class of problems satisfying some spectral and concentration properties on the kernel eigendecomposition.","Specifically, we establish in this setting a non-asymptotic deterministic approximation for the test error of KRR -- with explicit non-asymptotic bounds -- that only depends on the eigenvalues and the target function alignment to the eigenvectors of the kernel.","Our proofs rely on a careful derivation of deterministic equivalents for random matrix functionals in the dimension free regime pioneered by Cheng and Montanari (2022).   ","We apply this setting to several classical examples and show an excellent agreement between theoretical predictions and numerical simulations.","These results rely on having access to the eigendecomposition of the kernel operator.","Alternatively, we prove that, under this same setting, the generalized cross-validation (GCV) estimator concentrates on the test error uniformly over a range of ridge regularization parameter that includes zero (the interpolating solution).","As a consequence, the GCV estimator can be used to estimate from data the test error and optimal regularization parameter for KRR."],"url":"http://arxiv.org/abs/2403.08938v1","category":"stat.ML"}
{"created":"2024-03-13 20:05:09","title":"Formalizing Date Arithmetic and Statically Detecting Ambiguities for the Law","abstract":"Legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time. Unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users. In this work, we aim to provide a solid foundation for date arithmetic working on days, months and years. We first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the F* proof assistant. Building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations. We finally integrate our approach in the Catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the Catala implementation of the French housing benefits, leading to the discovery of several date-related ambiguities.","sentences":["Legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time.","Unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users.","In this work, we aim to provide a solid foundation for date arithmetic working on days, months and years.","We first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the F* proof assistant.","Building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations.","We finally integrate our approach in the Catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the Catala implementation of the French housing benefits, leading to the discovery of several date-related ambiguities."],"url":"http://arxiv.org/abs/2403.08935v1","category":"cs.PL"}
{"created":"2024-03-13 20:02:35","title":"The indoor agriculture industry: a promising player in demand response services","abstract":"Demand response (DR) programs currently cover about 2\\% of the average annual global demand, which is far from contributing to the International Energy Agency's ``Net Zero by 2050'' roadmap's 20\\% target. While aggregation of many small flexible loads such as individual households can help reaching this target, increasing the participation of industries that are major electricity consumers is certainly a way forward. The indoor agriculture sector currently experiences a significant growth to partake in the sustainable production of high-quality food world-wide. As energy-related costs, up to 40\\% of the total expenses, may preclude full maturity of this industry, DR participation can result in a win-win situation. Indeed, the agriculture system must transform and become a sustainable source of food for an increasing number of people worldwide under the constraints of preservation of soils and water, carbon footprint, and energy efficiency. We considered the case of the Russian Federation where indoor farming is burgeoning and already represents a load of several thousand megawatts. To show the viability of the indoor farming industry participation in implicit and explicit DR programs, we built a physical model of a vertical farm inside a phytotron with complete control of environmental parameters including ambient temperature, relative humidity, CO$_2$ concentration, and photosynthetic photon flux density. This phytotron was used as a model greenhouse. We grew different varieties of leafy plants under simulated DR conditions and control conditions on the same setup. Our results show that the indoor farming dedicated to greens can participate in DR without adversely affecting plant production and that this presents an economic advantage.","sentences":["Demand response (DR) programs currently cover about 2\\% of the average annual global demand, which is far from contributing to the International Energy Agency's ``Net Zero by 2050'' roadmap's 20\\% target.","While aggregation of many small flexible loads such as individual households can help reaching this target, increasing the participation of industries that are major electricity consumers is certainly a way forward.","The indoor agriculture sector currently experiences a significant growth to partake in the sustainable production of high-quality food world-wide.","As energy-related costs, up to 40\\% of the total expenses, may preclude full maturity of this industry, DR participation can result in a win-win situation.","Indeed, the agriculture system must transform and become a sustainable source of food for an increasing number of people worldwide under the constraints of preservation of soils and water, carbon footprint, and energy efficiency.","We considered the case of the Russian Federation where indoor farming is burgeoning and already represents a load of several thousand megawatts.","To show the viability of the indoor farming industry participation in implicit and explicit DR programs, we built a physical model of a vertical farm inside a phytotron with complete control of environmental parameters including ambient temperature, relative humidity, CO$_2$ concentration, and photosynthetic photon flux density.","This phytotron was used as a model greenhouse.","We grew different varieties of leafy plants under simulated DR conditions and control conditions on the same setup.","Our results show that the indoor farming dedicated to greens can participate in DR without adversely affecting plant production and that this presents an economic advantage."],"url":"http://arxiv.org/abs/2403.08934v1","category":"physics.soc-ph"}
{"created":"2024-03-13 19:37:04","title":"Two-sided Assortment Optimization: Adaptivity Gaps and Approximation Algorithms","abstract":"To address the challenge of choice congestion in matching markets, in this work, we introduce a two-sided assortment optimization framework under general choice preferences. The goal in this problem is to maximize the expected number of matches by deciding which assortments are displayed to the agents and the order in which they are shown. In this context, we identify several classes of policies that platforms can use in their design. Our goals are: (1) to measure the value that one class of policies has over another one, and (2) to approximately solve the optimization problem itself for a given class. For (1), we define the adaptivity gap as the worst-case ratio between the optimal values of two different policy classes. First, we show that the gap between the class of policies that statically show assortments to one-side first and the class of policies that adaptively show assortments to one-side first is exactly $1-1/e$. Second, we show that the gap between the latter class of policies and the fully adaptive class of policies that show assortments to agents one by one is exactly $1/2$. We also note that the worst policies are those who simultaneously show assortments to all the agents, in fact, we show that their adaptivity gap even with respect to one-sided static policies can be arbitrarily small. For (2), we first show that there exists a polynomial time policy that achieves a $1/4$ approximation factor within the class of policies that adaptively show assortments to agents one by one. Finally, when agents' preferences are governed by multinomial-logit models, we show that a 0.066 approximation factor can be obtained within the class of policies that show assortments to all agents at once.","sentences":["To address the challenge of choice congestion in matching markets, in this work, we introduce a two-sided assortment optimization framework under general choice preferences.","The goal in this problem is to maximize the expected number of matches by deciding which assortments are displayed to the agents and the order in which they are shown.","In this context, we identify several classes of policies that platforms can use in their design.","Our goals are: (1) to measure the value that one class of policies has over another one, and (2) to approximately solve the optimization problem itself for a given class.","For (1), we define the adaptivity gap as the worst-case ratio between the optimal values of two different policy classes.","First, we show that the gap between the class of policies that statically show assortments to one-side first and the class of policies that adaptively show assortments to one-side first is exactly $1-1/e$. Second, we show that the gap between the latter class of policies and the fully adaptive class of policies that show assortments to agents one by one is exactly $1/2$. We also note that the worst policies are those who simultaneously show assortments to all the agents, in fact, we show that their adaptivity gap even with respect to one-sided static policies can be arbitrarily small.","For (2), we first show that there exists a polynomial time policy that achieves a $1/4$ approximation factor within the class of policies that adaptively show assortments to agents one by one.","Finally, when agents' preferences are governed by multinomial-logit models, we show that a 0.066 approximation factor can be obtained within the class of policies that show assortments to all agents at once."],"url":"http://arxiv.org/abs/2403.08929v1","category":"math.OC"}
{"created":"2024-03-13 19:33:27","title":"Principal stratification with U-statistics under principal ignorability","abstract":"Principal stratification is a popular framework for causal inference in the presence of an intermediate outcome. While the principal average treatment effects have traditionally been the default target of inference, it may not be sufficient when the interest lies in the relative favorability of one potential outcome over the other within the principal stratum. We thus introduce the principal generalized causal effect estimands, which extend the principal average causal effects to accommodate nonlinear contrast functions. Under principal ignorability, we expand the theoretical results in Jiang et. al. (2022) to a much wider class of causal estimands in the presence of a binary intermediate variable. We develop identification formulas and derive the efficient influence functions of the generalized estimands for principal stratification analyses. These efficient influence functions motivate a set of multiply robust estimators and lay the ground for obtaining efficient debiased machine learning estimators via cross-fitting based on $U$-statistics. The proposed methods are illustrated through simulations and the analysis of a data example.","sentences":["Principal stratification is a popular framework for causal inference in the presence of an intermediate outcome.","While the principal average treatment effects have traditionally been the default target of inference, it may not be sufficient when the interest lies in the relative favorability of one potential outcome over the other within the principal stratum.","We thus introduce the principal generalized causal effect estimands, which extend the principal average causal effects to accommodate nonlinear contrast functions.","Under principal ignorability, we expand the theoretical results in Jiang et.","al. (2022) to a much wider class of causal estimands in the presence of a binary intermediate variable.","We develop identification formulas and derive the efficient influence functions of the generalized estimands for principal stratification analyses.","These efficient influence functions motivate a set of multiply robust estimators and lay the ground for obtaining efficient debiased machine learning estimators via cross-fitting based on $U$-statistics.","The proposed methods are illustrated through simulations and the analysis of a data example."],"url":"http://arxiv.org/abs/2403.08927v1","category":"stat.ME"}
{"created":"2024-03-13 19:29:17","title":"Large Steklov eigenvalues under volume constraints","abstract":"In this note we establish an expression for the Steklov spectrum of warped products in terms of auxiliary Steklov problems for drift Laplacians with weight induced by the warping factor. As an application, we show that a compact manifold with connected boundary diffeomorphic to a product admits a family of Riemannian metrics which coincide on the boundary, have fixed volume and arbitrarily large first non-zero Steklov eigenvalue. These are the first examples of Riemannian metrics with these properties on three-dimensional manifolds.","sentences":["In this note we establish an expression for the Steklov spectrum of warped products in terms of auxiliary Steklov problems for drift Laplacians with weight induced by the warping factor.","As an application, we show that a compact manifold with connected boundary diffeomorphic to a product admits a family of Riemannian metrics which coincide on the boundary, have fixed volume and arbitrarily large first non-zero Steklov eigenvalue.","These are the first examples of Riemannian metrics with these properties on three-dimensional manifolds."],"url":"http://arxiv.org/abs/2403.08925v1","category":"math.DG"}
{"created":"2024-03-13 18:46:50","title":"Bury Me Here --The New Genre of Narrative Design Game Based on Immersive Storytelling","abstract":"Virtual reality games always provide the player with the most verisimilitude experience. With the advancement of VR hardware, it may become mainstream how people feel and attach to a virtual world. The paper discusses a possible solution to finding a better balance between the two classical genres of VR games, sensory stimulation and storytelling. To this end, we designed a game named \"Bury Me Here,\" in which players can find an emotional bond between the game protagonist and themselves. The game includes four sections, the departure from the hometown, the travel on the train, the work in the office, and the life in the penthouse. At the game's end, the protagonist returns to his country yard and spends the rest of his life there. All the sections are designed to tell a stranger's life story to the player, making them experience someone else's life path and bonding an emotional connection between the player and the protagonist through storytelling. Results show that the game provides an immersive visual experience and has emotive sparks echo in players' minds.","sentences":["Virtual reality games always provide the player with the most verisimilitude experience.","With the advancement of VR hardware, it may become mainstream how people feel and attach to a virtual world.","The paper discusses a possible solution to finding a better balance between the two classical genres of VR games, sensory stimulation and storytelling.","To this end, we designed a game named \"Bury Me Here,\" in which players can find an emotional bond between the game protagonist and themselves.","The game includes four sections, the departure from the hometown, the travel on the train, the work in the office, and the life in the penthouse.","At the game's end, the protagonist returns to his country yard and spends the rest of his life there.","All the sections are designed to tell a stranger's life story to the player, making them experience someone else's life path and bonding an emotional connection between the player and the protagonist through storytelling.","Results show that the game provides an immersive visual experience and has emotive sparks echo in players' minds."],"url":"http://arxiv.org/abs/2403.08903v1","category":"cs.HC"}
{"created":"2024-03-13 18:45:51","title":"A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty","abstract":"The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and efficient strategy for balancing the trade-off between model complexity, accuracy, and prediction uncertainty. The effectiveness of OPAL-surrogate is demonstrated through two modeling problems, including the deformation of porous materials for building insulation and turbulent combustion flow for the ablation of solid fuels within hybrid rocket motors.","sentences":["The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making.","This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters.","The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty.","Leveraging these principles, OPAL-surrogate introduces a systematic and efficient strategy for balancing the trade-off between model complexity, accuracy, and prediction uncertainty.","The effectiveness of OPAL-surrogate is demonstrated through two modeling problems, including the deformation of porous materials for building insulation and turbulent combustion flow for the ablation of solid fuels within hybrid rocket motors."],"url":"http://arxiv.org/abs/2403.08901v1","category":"cs.CE"}
{"created":"2024-03-13 18:45:15","title":"Handoffs in User-Centric Cell-Free MIMO Networks: A POMDP Framework","abstract":"We study the problem of managing handoffs (HOs) in user-centric cell-free massive MIMO (UC-mMIMO) networks. Motivated by the importance of controlling the number of HOs and by the correlation between efficient HO decisions and the temporal evolution of the channel conditions, we formulate a partially observable Markov decision process (POMDP) with the state space representing the discrete versions of the large-scale fading and the action space representing the association decisions of the user with the access points (APs). We develop a novel algorithm that employs this model to derive a HO policy for a mobile user based on current and future rewards. To alleviate the high complexity of our POMDP, we follow a divide-and-conquer approach by breaking down the POMDP formulation into sub-problems, each solved separately. Then, the policy and the candidate pool of APs for the sub-problem that produced the best total expected reward are used to perform HOs within a specific time horizon. We then introduce modifications to our algorithm to decrease the number of HOs. The results show that half of the number of HOs in the UC-mMIMO networks can be eliminated. Namely, our novel solution can control the number of HOs while maintaining a rate guarantee, where a 47%-70% reduction of the cumulative number of HOs is observed in networks with a density of 125 APs per km2. Most importantly, our results show that a POMDP-based HO scheme is promising to control HOs.","sentences":["We study the problem of managing handoffs (HOs) in user-centric cell-free massive MIMO (UC-mMIMO) networks.","Motivated by the importance of controlling the number of HOs and by the correlation between efficient HO decisions and the temporal evolution of the channel conditions, we formulate a partially observable Markov decision process (POMDP) with the state space representing the discrete versions of the large-scale fading and the action space representing the association decisions of the user with the access points (APs).","We develop a novel algorithm that employs this model to derive a HO policy for a mobile user based on current and future rewards.","To alleviate the high complexity of our POMDP, we follow a divide-and-conquer approach by breaking down the POMDP formulation into sub-problems, each solved separately.","Then, the policy and the candidate pool of APs for the sub-problem that produced the best total expected reward are used to perform HOs within a specific time horizon.","We then introduce modifications to our algorithm to decrease the number of HOs.","The results show that half of the number of HOs in the UC-mMIMO networks can be eliminated.","Namely, our novel solution can control the number of HOs while maintaining a rate guarantee, where a 47%-70% reduction of the cumulative number of HOs is observed in networks with a density of 125 APs per km2.","Most importantly, our results show that a POMDP-based HO scheme is promising to control HOs."],"url":"http://arxiv.org/abs/2403.08900v1","category":"cs.IT"}
{"created":"2024-03-13 18:37:16","title":"One-Shot Averaging for Distributed TD($\u03bb$) Under Markov Sampling","abstract":"We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.","sentences":["We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent.","We show that in this setting, we can achieve a linear speedup for TD($\\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough.","Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\\lambda$) with Markov sampling independently and only average their results after the final step.","This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work."],"url":"http://arxiv.org/abs/2403.08896v1","category":"cs.LG"}
{"created":"2024-03-13 18:36:44","title":"General Multipoles and Their Implications for Dark Matter Inference","abstract":"The flux ratios of strongly lensed quasars have previously been used to infer the properties of dark matter. In these analyses it is crucial to separate the effect of the main lensing galaxy and the low-mass dark matter halo population. In this work, we investigate flux-ratio perturbations resulting from general third- and fourth-order multipole perturbations to the main lensing galaxy's mass profile. We simulate four lens systems, each with a different lensing configuration, without multipoles. The simulated flux ratios are perturbed by 10-40 per cent by a population of low-mass haloes consistent with CDM and, in one case, also a satellite galaxy. This level of perturbation is comparable to the magnitude of flux-ratio anomalies in real data that has been previously analyzed. We then attempt to fit the simulated systems using multipoles instead of low-mass haloes. We find that multipoles with amplitudes of 0.01 or less can produce flux-ratio perturbations in excess of 40 per cent. In all cases, third- or fourth-order multipoles can individually reduce the magnitude of, if not eliminate, flux-ratio anomalies. When both multipole orders are jointly included, all simulated flux ratios can be fit to within the observational uncertainty. Our results indicate that low-mass haloes and multipoles are highly degenerate when modelling quadruply-imaged quasars based just on image positions and flux ratios. In the presence of this degeneracy, flux-ratio anomalies in lensed quasars alone cannot be used to place strong constraints on the properties of dark matter without additional information that can inform our priors.","sentences":["The flux ratios of strongly lensed quasars have previously been used to infer the properties of dark matter.","In these analyses it is crucial to separate the effect of the main lensing galaxy and the low-mass dark matter halo population.","In this work, we investigate flux-ratio perturbations resulting from general third- and fourth-order multipole perturbations to the main lensing galaxy's mass profile.","We simulate four lens systems, each with a different lensing configuration, without multipoles.","The simulated flux ratios are perturbed by 10-40 per cent by a population of low-mass haloes consistent with CDM and, in one case, also a satellite galaxy.","This level of perturbation is comparable to the magnitude of flux-ratio anomalies in real data that has been previously analyzed.","We then attempt to fit the simulated systems using multipoles instead of low-mass haloes.","We find that multipoles with amplitudes of 0.01 or less can produce flux-ratio perturbations in excess of 40 per cent.","In all cases, third- or fourth-order multipoles can individually reduce the magnitude of, if not eliminate, flux-ratio anomalies.","When both multipole orders are jointly included, all simulated flux ratios can be fit to within the observational uncertainty.","Our results indicate that low-mass haloes and multipoles are highly degenerate when modelling quadruply-imaged quasars based just on image positions and flux ratios.","In the presence of this degeneracy, flux-ratio anomalies in lensed quasars alone cannot be used to place strong constraints on the properties of dark matter without additional information that can inform our priors."],"url":"http://arxiv.org/abs/2403.08895v1","category":"astro-ph.CO"}
{"created":"2024-03-13 18:20:24","title":"From \"um\" to \"yeah\": Producing, predicting, and regulating information flow in human conversation","abstract":"Conversation demands attention. Speakers must call words to mind, listeners must make sense of them, and both together must negotiate this flow of information, all in fractions of a second. We used large language models to study how this works in a large-scale dataset of English-language conversation, the CANDOR corpus. We provide a new estimate of the information density of unstructured conversation, of approximately 13 bits/second, and find significant effects associated with the cognitive load of both retrieving, and presenting, that information. We also reveal a role for backchannels -- the brief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the production of novelty: the lead-up to a backchannel is associated with declining information rate, while speech downstream rebounds to previous rates. Our results provide new insights into long-standing theories of how we respond to fluctuating demands on cognitive resources, and how we negotiate those demands in partnership with others.","sentences":["Conversation demands attention.","Speakers must call words to mind, listeners must make sense of them, and both together must negotiate this flow of information, all in fractions of a second.","We used large language models to study how this works in a large-scale dataset of English-language conversation, the CANDOR corpus.","We provide a new estimate of the information density of unstructured conversation, of approximately 13 bits/second, and find significant effects associated with the cognitive load of both retrieving, and presenting, that information.","We also reveal a role for backchannels -- the brief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the production of novelty: the lead-up to a backchannel is associated with declining information rate, while speech downstream rebounds to previous rates.","Our results provide new insights into long-standing theories of how we respond to fluctuating demands on cognitive resources, and how we negotiate those demands in partnership with others."],"url":"http://arxiv.org/abs/2403.08890v1","category":"cs.CL"}
{"created":"2024-03-13 18:17:18","title":"Lifting Galois representations via Kummer flags","abstract":"Let $\\Gamma$ be either i) the absolute Galois group of a local field $F$, or ii) the topological fundamental group of a closed connected orientable surface of genus $g$. In case i), assume that $\\mu_{p^2} \\subset F$. We give an elementary and unified proof that every representation $\\rho_1: \\Gamma \\to \\mathbf{GL}_d(\\mathbb{F}_p)$ lifts to a representation $\\rho_2: \\Gamma \\to \\mathbf{GL}_d(\\mathbb{Z}/p^2)$. [In case i), it is understood these are continuous.] The actual statement is much stronger: for $r \\geq 1$, under \"suitable\" assumptions, triangular representations $\\rho_r: \\Gamma \\to \\mathbf{B}_d(\\mathbb{Z}/p^r)$ lift to $\\rho_{r+1}: \\Gamma \\to \\mathbf{B}_d(\\mathbb{Z}/p^{r+1})$, in the strongest possible step-by-step sense. Here \"suitable\" is made precise by the concept of $\\textit{Kummer flag}$. An essential aspect of this work, is to identify the common properties of groups i) and ii), that suffice to ensure the existence of such lifts.","sentences":["Let $\\Gamma$ be either i) the absolute Galois group of a local field $F$, or ii) the topological fundamental group of a closed connected orientable surface of genus $g$.","In case i), assume that $\\mu_{p^2} \\subset F$. We give an elementary and unified proof that every representation $\\rho_1: \\Gamma \\to \\mathbf{GL}_d(\\mathbb{F}_p)$ lifts to a representation $\\rho_2: \\Gamma \\to \\mathbf{GL}_d(\\mathbb{Z}/p^2)$.","[In case i), it is understood these are continuous.]","The actual statement is much stronger: for $r \\geq 1$, under \"suitable\" assumptions, triangular representations $\\rho_r: \\Gamma \\to \\mathbf{B}_d(\\mathbb{Z}/p^r)$ lift to $\\rho_{r+1}: \\Gamma \\to \\mathbf{B}_d(\\mathbb{Z}/p^{r+1})$, in the strongest possible step-by-step sense.","Here \"suitable\" is made precise by the concept of $\\textit{Kummer flag}$. An essential aspect of this work, is to identify the common properties of groups i) and ii), that suffice to ensure the existence of such lifts."],"url":"http://arxiv.org/abs/2403.08888v1","category":"math.NT"}
{"created":"2024-03-13 18:12:42","title":"A note on spectral properties of random $S$-adic systems","abstract":"The paper is concerned with random $S$-adic systems arising from an i.i.d.\\ sequence of unimodular substitutions. Using equidistribution results of Benoist and Quint, we show in Theorem 3.3 that, under some natural assumptions, if the Lyapunov exponent of the spectral cocycle is strictly less that 1/2 of the Lyapunov exponent of the random walk on $SL(2,\\mathbb{R})$ driven by the sequence of substitution matrices, then almost surely the spectrum of the $S$-adic $\\mathbb{Z}$-action is singular with respect to any (fixed in advance) continuous measure.","sentences":["The paper is concerned with random $S$-adic systems arising from an i.i.d.\\ sequence of unimodular substitutions.","Using equidistribution results of Benoist and Quint, we show in Theorem 3.3 that, under some natural assumptions, if the Lyapunov exponent of the spectral cocycle is strictly less that 1/2 of the Lyapunov exponent of the random walk on $SL(2,\\mathbb{R})$ driven by the sequence of substitution matrices, then almost surely the spectrum of the $S$-adic $\\mathbb{Z}$-action is singular with respect to any (fixed in advance) continuous measure."],"url":"http://arxiv.org/abs/2403.08884v1","category":"math.DS"}
{"created":"2024-03-13 18:00:02","title":"Worldtube excision method for intermediate-mass-ratio inspirals: self-consistent evolution in a scalar-charge model","abstract":"This is a third installment in a program to develop a method for alleviating the scale disparity in binary black hole simulations with mass ratios in the intermediate astrophysical range, where simulation cost is prohibitive while purely perturbative methods may not be adequate. The method is based on excising a \"worldtube\" around the smaller object, much larger than the object itself, replacing it with an analytical model that approximates a tidally deformed black hole. Previously (arXiv:2304.05329) we have tested the idea in a toy model of a scalar charge in a fixed circular geodesic orbit around a Schwarzschild black hole, solving for the massless Klein-Gordon field in 3+1 dimensions on the SpECTRE platform. Here we take the significant further step of allowing the orbit to evolve radiatively, in a self-consistent manner, under the effect of back-reaction from the scalar field. We compute the inspiral orbit and the emitted scalar-field waveform, showing a good agreement with perturbative calculations in the adiabatic approximation. We also demonstrate how our simulations accurately resolve post-adiabatic effects (for which we do not have perturbative results). In this work we focus on quasi-circular inspirals. Our implementation will shortly be publicly accessible in the SpECTRE numerical relativity code.","sentences":["This is a third installment in a program to develop a method for alleviating the scale disparity in binary black hole simulations with mass ratios in the intermediate astrophysical range, where simulation cost is prohibitive while purely perturbative methods may not be adequate.","The method is based on excising a \"worldtube\" around the smaller object, much larger than the object itself, replacing it with an analytical model that approximates a tidally deformed black hole.","Previously (arXiv:2304.05329) we have tested the idea in a toy model of a scalar charge in a fixed circular geodesic orbit around a Schwarzschild black hole, solving for the massless Klein-Gordon field in 3+1 dimensions on the SpECTRE platform.","Here we take the significant further step of allowing the orbit to evolve radiatively, in a self-consistent manner, under the effect of back-reaction from the scalar field.","We compute the inspiral orbit and the emitted scalar-field waveform, showing a good agreement with perturbative calculations in the adiabatic approximation.","We also demonstrate how our simulations accurately resolve post-adiabatic effects (for which we do not have perturbative results).","In this work we focus on quasi-circular inspirals.","Our implementation will shortly be publicly accessible in the SpECTRE numerical relativity code."],"url":"http://arxiv.org/abs/2403.08864v1","category":"gr-qc"}
{"created":"2024-03-14 17:57:03","title":"Physically motivated improvements of Variational Quantum Eigensolvers","abstract":"The Adaptive Derivative-Assembled Pseudo-Trotter Variational Quantum Eigensolver (ADAPT-VQE) has emerged as a pivotal promising approach for electronic structure challenges in quantum chemistry with noisy quantum devices. Nevertheless, to surmount existing technological constraints, this study endeavors to enhance ADAPT-VQE's efficacy. Leveraging insights from electronic structure theory, we concentrate on optimizing state preparation without added computational burden and guiding ansatz expansion to yield more concise wavefunctions with expedited convergence toward exact solutions. These advancements culminate in shallower circuits and, as demonstrated, reduced measurement requirements. This research delineates these enhancements and assesses their performance across mono, di, and tridimensional arrangements of H4 models, as well as in the water molecule. Ultimately, this work attests to the viability of physically-motivated strategies in fortifying ADAPT-VQE's efficiency, marking a significant stride in quantum chemistry simulations.","sentences":["The Adaptive Derivative-Assembled Pseudo-Trotter Variational Quantum Eigensolver (ADAPT-VQE) has emerged as a pivotal promising approach for electronic structure challenges in quantum chemistry with noisy quantum devices.","Nevertheless, to surmount existing technological constraints, this study endeavors to enhance ADAPT-VQE's efficacy.","Leveraging insights from electronic structure theory, we concentrate on optimizing state preparation without added computational burden and guiding ansatz expansion to yield more concise wavefunctions with expedited convergence toward exact solutions.","These advancements culminate in shallower circuits and, as demonstrated, reduced measurement requirements.","This research delineates these enhancements and assesses their performance across mono, di, and tridimensional arrangements of H4 models, as well as in the water molecule.","Ultimately, this work attests to the viability of physically-motivated strategies in fortifying ADAPT-VQE's efficiency, marking a significant stride in quantum chemistry simulations."],"url":"http://arxiv.org/abs/2403.09624v1","category":"quant-ph"}
{"created":"2024-03-14 17:13:37","title":"uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures","abstract":"Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that \\modelname achieves 4-6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE","sentences":["Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks.","Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs.","Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs.","To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures.","Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics.","To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces.","Experiments in low/few-shot settings demonstrate that \\modelname achieves 4-6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE"],"url":"http://arxiv.org/abs/2403.09579v1","category":"cs.SD"}
{"created":"2024-03-14 15:55:53","title":"SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition","abstract":"Skeleton-based action recognition, which classifies human actions based on the coordinates of joints and their connectivity within skeleton data, is widely utilized in various scenarios. While Graph Convolutional Networks (GCNs) have been proposed for skeleton data represented as graphs, they suffer from limited receptive fields constrained by joint connectivity. To address this limitation, recent advancements have introduced transformer-based methods. However, capturing correlations between all joints in all frames requires substantial memory resources. To alleviate this, we propose a novel approach called Skeletal-Temporal Transformer (SkateFormer) that partitions joints and frames based on different types of skeletal-temporal relation (Skate-Type) and performs skeletal-temporal self-attention (Skate-MSA) within each partition. We categorize the key skeletal-temporal relations for action recognition into a total of four distinct types. These types combine (i) two skeletal relation types based on physically neighboring and distant joints, and (ii) two temporal relation types based on neighboring and distant frames. Through this partition-specific attention strategy, our SkateFormer can selectively focus on key joints and frames crucial for action recognition in an action-adaptive manner with efficient computation. Extensive experiments on various benchmark datasets validate that our SkateFormer outperforms recent state-of-the-art methods.","sentences":["Skeleton-based action recognition, which classifies human actions based on the coordinates of joints and their connectivity within skeleton data, is widely utilized in various scenarios.","While Graph Convolutional Networks (GCNs) have been proposed for skeleton data represented as graphs, they suffer from limited receptive fields constrained by joint connectivity.","To address this limitation, recent advancements have introduced transformer-based methods.","However, capturing correlations between all joints in all frames requires substantial memory resources.","To alleviate this, we propose a novel approach called Skeletal-Temporal Transformer (SkateFormer) that partitions joints and frames based on different types of skeletal-temporal relation (Skate-Type) and performs skeletal-temporal self-attention (Skate-MSA) within each partition.","We categorize the key skeletal-temporal relations for action recognition into a total of four distinct types.","These types combine (i) two skeletal relation types based on physically neighboring and distant joints, and (ii) two temporal relation types based on neighboring and distant frames.","Through this partition-specific attention strategy, our SkateFormer can selectively focus on key joints and frames crucial for action recognition in an action-adaptive manner with efficient computation.","Extensive experiments on various benchmark datasets validate that our SkateFormer outperforms recent state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.09508v1","category":"cs.CV"}
{"created":"2024-03-14 15:45:02","title":"Shrinkage for Extreme Partial Least-Squares","abstract":"This work focuses on dimension-reduction techniques for modelling conditional extreme values. Specifically, we investigate the idea that extreme values of a response variable can be explained by nonlinear functions derived from linear projections of an input random vector. In this context, the estimation of projection directions is examined, as approached by the Extreme Partial Least Squares (EPLS) method--an adaptation of the original Partial Least Squares (PLS) method tailored to the extreme-value framework. Further, a novel interpretation of EPLS directions as maximum likelihood estimators is introduced, utilizing the von Mises-Fisher distribution applied to hyperballs. The dimension reduction process is enhanced through the Bayesian paradigm, enabling the incorporation of prior information into the projection direction estimation. The maximum a posteriori estimator is derived in two specific cases, elucidating it as a regularization or shrinkage of the EPLS estimator. We also establish its asymptotic behavior as the sample size approaches infinity. A simulation data study is conducted in order to assess the practical utility of our proposed method. This clearly demonstrates its effectiveness even in moderate data problems within high-dimensional settings. Furthermore, we provide an illustrative example of the method's applicability using French farm income data, highlighting its efficacy in real-world scenarios.","sentences":["This work focuses on dimension-reduction techniques for modelling conditional extreme values.","Specifically, we investigate the idea that extreme values of a response variable can be explained by nonlinear functions derived from linear projections of an input random vector.","In this context, the estimation of projection directions is examined, as approached by the Extreme Partial Least Squares (EPLS) method--an adaptation of the original Partial Least Squares (PLS) method tailored to the extreme-value framework.","Further, a novel interpretation of EPLS directions as maximum likelihood estimators is introduced, utilizing the von Mises-Fisher distribution applied to hyperballs.","The dimension reduction process is enhanced through the Bayesian paradigm, enabling the incorporation of prior information into the projection direction estimation.","The maximum a posteriori estimator is derived in two specific cases, elucidating it as a regularization or shrinkage of the EPLS estimator.","We also establish its asymptotic behavior as the sample size approaches infinity.","A simulation data study is conducted in order to assess the practical utility of our proposed method.","This clearly demonstrates its effectiveness even in moderate data problems within high-dimensional settings.","Furthermore, we provide an illustrative example of the method's applicability using French farm income data, highlighting its efficacy in real-world scenarios."],"url":"http://arxiv.org/abs/2403.09503v1","category":"stat.ME"}
{"created":"2024-03-14 15:42:31","title":"Faceptor: A Generalist Model for Face Perception","abstract":"With the comprehensive research conducted on various face analysis tasks, there is a growing interest among researchers to develop a unified approach to face perception. Existing methods mainly discuss unified representation and training, which lack task extensibility and application efficiency. To tackle this issue, we focus on the unified model structure, exploring a face generalist model. As an intuitive design, Naive Faceptor enables tasks with the same output shape and granularity to share the structural design of the standardized output head, achieving improved task extensibility. Furthermore, Faceptor is proposed to adopt a well-designed single-encoder dual-decoder architecture, allowing task-specific queries to represent new-coming semantics. This design enhances the unification of model structure while improving application efficiency in terms of storage overhead. Additionally, we introduce Layer-Attention into Faceptor, enabling the model to adaptively select features from optimal layers to perform the desired tasks. Through joint training on 13 face perception datasets, Faceptor achieves exceptional performance in facial landmark localization, face parsing, age estimation, expression recognition, binary attribute classification, and face recognition, achieving or surpassing specialized methods in most tasks. Our training framework can also be applied to auxiliary supervised learning, significantly improving performance in data-sparse tasks such as age estimation and expression recognition. The code and models will be made publicly available at https://github.com/lxq1000/Faceptor.","sentences":["With the comprehensive research conducted on various face analysis tasks, there is a growing interest among researchers to develop a unified approach to face perception.","Existing methods mainly discuss unified representation and training, which lack task extensibility and application efficiency.","To tackle this issue, we focus on the unified model structure, exploring a face generalist model.","As an intuitive design, Naive Faceptor enables tasks with the same output shape and granularity to share the structural design of the standardized output head, achieving improved task extensibility.","Furthermore, Faceptor is proposed to adopt a well-designed single-encoder dual-decoder architecture, allowing task-specific queries to represent new-coming semantics.","This design enhances the unification of model structure while improving application efficiency in terms of storage overhead.","Additionally, we introduce Layer-Attention into Faceptor, enabling the model to adaptively select features from optimal layers to perform the desired tasks.","Through joint training on 13 face perception datasets, Faceptor achieves exceptional performance in facial landmark localization, face parsing, age estimation, expression recognition, binary attribute classification, and face recognition, achieving or surpassing specialized methods in most tasks.","Our training framework can also be applied to auxiliary supervised learning, significantly improving performance in data-sparse tasks such as age estimation and expression recognition.","The code and models will be made publicly available at https://github.com/lxq1000/Faceptor."],"url":"http://arxiv.org/abs/2403.09500v1","category":"cs.CV"}
{"created":"2024-03-14 15:02:24","title":"Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots","abstract":"This paper investigates the development and optimization of control algorithms for mobile robotics, with a keen focus on their implementation in Field-Programmable Gate Arrays (FPGAs). It delves into both classical control approaches such as PID and modern techniques including deep learning, addressing their application in sectors ranging from industrial automation to medical care. The study highlights the practical challenges and advancements in embedding these algorithms into FPGAs, which offer significant benefits for mobile robotics due to their high-speed processing and parallel computation capabilities. Through an analysis of various control strategies, the paper showcases the improvements in robot performance, particularly in navigation and obstacle avoidance. It emphasizes the critical role of FPGAs in enhancing the efficiency and adaptability of control algorithms in dynamic environments. Additionally, the research discusses the difficulties in benchmarking and evaluating the performance of these algorithms in real-world applications, suggesting a need for standardized evaluation criteria. The contribution of this work lies in its comprehensive examination of control algorithms' potential in FPGA-based mobile robotics, offering insights into future research directions for improving robotic autonomy and operational efficiency.","sentences":["This paper investigates the development and optimization of control algorithms for mobile robotics, with a keen focus on their implementation in Field-Programmable Gate Arrays (FPGAs).","It delves into both classical control approaches such as PID and modern techniques including deep learning, addressing their application in sectors ranging from industrial automation to medical care.","The study highlights the practical challenges and advancements in embedding these algorithms into FPGAs, which offer significant benefits for mobile robotics due to their high-speed processing and parallel computation capabilities.","Through an analysis of various control strategies, the paper showcases the improvements in robot performance, particularly in navigation and obstacle avoidance.","It emphasizes the critical role of FPGAs in enhancing the efficiency and adaptability of control algorithms in dynamic environments.","Additionally, the research discusses the difficulties in benchmarking and evaluating the performance of these algorithms in real-world applications, suggesting a need for standardized evaluation criteria.","The contribution of this work lies in its comprehensive examination of control algorithms' potential in FPGA-based mobile robotics, offering insights into future research directions for improving robotic autonomy and operational efficiency."],"url":"http://arxiv.org/abs/2403.09459v1","category":"cs.RO"}
{"created":"2024-03-14 13:35:41","title":"PeV-Scale SUSY and Cosmic Strings from F-term Hybrid Inflation","abstract":"We consider F-term hybrid inflation (FHI) and SUSY breaking in the context of a B-L extension of MSSM which largely respects a global U(1) R symmetry. The hidden sector Kaehler manifold enjoys an enhanced SU(1,1)/U(1) symmetry with the scalar curvature determined by the achievement of a SUSY-breaking de Sitter vacuum without ugly tuning. FHI turns out to be consistent with data, provided that the magnitude of the emergent soft tadpole term is confined in the range (1.2-100) TeV, and it is accompanied with the production of B-L cosmic strings. If these are metastable they interpret the present observations from PTA experiments on the stochastic background of gravitational waves with dimensionless tension Gmu~(1-9.2)x10^-8. The mu parameter of MSSM arises by appropriately adapting the Giudice-Masiero mechanism and facilitates the out-of-equilibrium decay of the R saxion at a reheat temperature lower than about 71 GeV. Due to the prolonged matter dominated era the gravitational wave signal is suppressed at high frequencies. The SUSY mass scale turns out to lie in the PeV region.","sentences":["We consider F-term hybrid inflation (FHI) and SUSY breaking in the context of a B-L extension of MSSM which largely respects a global U(1) R symmetry.","The hidden sector Kaehler manifold enjoys an enhanced SU(1,1)/U(1) symmetry with the scalar curvature determined by the achievement of a SUSY-breaking de Sitter vacuum without ugly tuning.","FHI turns out to be consistent with data, provided that the magnitude of the emergent soft tadpole term is confined in the range (1.2-100) TeV, and it is accompanied with the production of B-L cosmic strings.","If these are metastable they interpret the present observations from PTA experiments on the stochastic background of gravitational waves with dimensionless tension Gmu~(1-9.2)x10^-8.","The mu parameter of MSSM arises by appropriately adapting the Giudice-Masiero mechanism and facilitates the out-of-equilibrium decay of the R saxion at a reheat temperature lower than about 71 GeV. Due to the prolonged matter dominated era the gravitational wave signal is suppressed at high frequencies.","The SUSY mass scale turns out to lie in the PeV region."],"url":"http://arxiv.org/abs/2403.09385v1","category":"hep-ph"}
{"created":"2024-03-14 13:27:42","title":"Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks","abstract":"Mainstream parameter-efficient fine-tuning (PEFT) methods, such as LoRA or Adapter, project a model's hidden states to a lower dimension, allowing pre-trained models to adapt to new data through this low-rank bottleneck. However, PEFT tasks involving multiple modalities, like vision-language (VL) tasks, require not only adaptation to new data but also learning the relationship between different modalities. Targeting at VL PEFT tasks, we propose a family of operations, called routing functions, to enhance VL alignment in the low-rank bottlenecks. The routing functions adopt linear operations and do not introduce new trainable parameters. In-depth analyses are conducted to study their behavior. In various VL PEFT settings, the routing functions significantly improve performance of the original PEFT methods, achieving over 20% improvement on VQAv2 ($\\text{RoBERTa}_{\\text{large}}$+ViT-L/16) and 30% on COCO Captioning (GPT2-medium+ViT-L/16). Also when fine-tuning a pre-trained multimodal model such as CLIP-BART, we observe smaller but consistent improvements across a range of VL PEFT tasks.","sentences":["Mainstream parameter-efficient fine-tuning (PEFT) methods, such as LoRA or Adapter, project a model's hidden states to a lower dimension, allowing pre-trained models to adapt to new data through this low-rank bottleneck.","However, PEFT tasks involving multiple modalities, like vision-language (VL) tasks, require not only adaptation to new data but also learning the relationship between different modalities.","Targeting at VL PEFT tasks, we propose a family of operations, called routing functions, to enhance VL alignment in the low-rank bottlenecks.","The routing functions adopt linear operations and do not introduce new trainable parameters.","In-depth analyses are conducted to study their behavior.","In various VL PEFT settings, the routing functions significantly improve performance of the original PEFT methods, achieving over 20% improvement on VQAv2 ($\\text{RoBERTa}_{\\text{large}}$+ViT-L/16) and 30% on COCO Captioning (GPT2-medium+ViT-L/16).","Also when fine-tuning a pre-trained multimodal model such as CLIP-BART, we observe smaller but consistent improvements across a range of VL PEFT tasks."],"url":"http://arxiv.org/abs/2403.09377v1","category":"cs.CV"}
{"created":"2024-03-14 13:04:57","title":"Bandwidth-Effective DRAM Cache for GPUs with Storage-Class Memory","abstract":"We propose overcoming the memory capacity limitation of GPUs with high-capacity Storage-Class Memory (SCM) and DRAM cache. By significantly increasing the memory capacity with SCM, the GPU can capture a larger fraction of the memory footprint than HBM for workloads that oversubscribe memory, achieving high speedups. However, the DRAM cache needs to be carefully designed to address the latency and BW limitations of the SCM while minimizing cost overhead and considering GPU's characteristics. Because the massive number of GPU threads can thrash the DRAM cache, we first propose an SCM-aware DRAM cache bypass policy for GPUs that considers the multi-dimensional characteristics of memory accesses by GPUs with SCM to bypass DRAM for data with low performance utility. In addition, to reduce DRAM cache probes and increase effective DRAM BW with minimal cost, we propose a Configurable Tag Cache (CTC) that repurposes part of the L2 cache to cache DRAM cacheline tags. The L2 capacity used for the CTC can be adjusted by users for adaptability. Furthermore, to minimize DRAM cache probe traffic from CTC misses, our Aggregated Metadata-In-Last-column (AMIL) DRAM cache organization co-locates all DRAM cacheline tags in a single column within a row. The AMIL also retains the full ECC protection, unlike prior DRAM cache's Tag-And-Data (TAD) organization. Additionally, we propose SCM throttling to curtail power and exploiting SCM's SLC/MLC modes to adapt to workload's memory footprint. While our techniques can be used for different DRAM and SCM devices, we focus on a Heterogeneous Memory Stack (HMS) organization that stacks SCM dies on top of DRAM dies for high performance. Compared to HBM, HMS improves performance by up to 12.5x (2.9x overall) and reduces energy by up to 89.3% (48.1% overall). Compared to prior works, we reduce DRAM cache probe and SCM write traffic by 91-93% and 57-75%, respectively.","sentences":["We propose overcoming the memory capacity limitation of GPUs with high-capacity Storage-Class Memory (SCM) and DRAM cache.","By significantly increasing the memory capacity with SCM, the GPU can capture a larger fraction of the memory footprint than HBM for workloads that oversubscribe memory, achieving high speedups.","However, the DRAM cache needs to be carefully designed to address the latency and BW limitations of the SCM while minimizing cost overhead and considering GPU's characteristics.","Because the massive number of GPU threads can thrash the DRAM cache, we first propose an SCM-aware DRAM cache bypass policy for GPUs that considers the multi-dimensional characteristics of memory accesses by GPUs with SCM to bypass DRAM for data with low performance utility.","In addition, to reduce DRAM cache probes and increase effective DRAM BW with minimal cost, we propose a Configurable Tag Cache (CTC) that repurposes part of the L2 cache to cache DRAM cacheline tags.","The L2 capacity used for the CTC can be adjusted by users for adaptability.","Furthermore, to minimize DRAM cache probe traffic from CTC misses, our Aggregated Metadata-In-Last-column (AMIL) DRAM cache organization co-locates all DRAM cacheline tags in a single column within a row.","The AMIL also retains the full ECC protection, unlike prior DRAM cache's Tag-And-Data (TAD) organization.","Additionally, we propose SCM throttling to curtail power and exploiting SCM's SLC/MLC modes to adapt to workload's memory footprint.","While our techniques can be used for different DRAM and SCM devices, we focus on a Heterogeneous Memory Stack (HMS) organization that stacks SCM dies on top of DRAM dies for high performance.","Compared to HBM, HMS improves performance by up to 12.5x (2.9x overall) and reduces energy by up to 89.3% (48.1% overall).","Compared to prior works, we reduce DRAM cache probe and SCM write traffic by 91-93% and 57-75%, respectively."],"url":"http://arxiv.org/abs/2403.09358v1","category":"cs.AR"}
{"created":"2024-03-14 11:27:44","title":"Long-time weak convergence analysis of a semi-discrete scheme for stochastic Maxwell equations","abstract":"It is known from the monograph [1, Chapter 5] that the weak convergence analysis of numerical schemes for stochastic Maxwell equations is an unsolved problem. This paper aims to fill the gap by establishing the long-time weak convergence analysis of the semi-implicit Euler scheme for stochastic Maxwell equations. Based on analyzing the regularity of transformed Kolmogorov equation associated to stochastic Maxwell equations and constructing a proper continuous adapted auxiliary process for the semi-implicit scheme, we present the long-time weak convergence analysis for this scheme and prove that the weak convergence order is one, which is twice the strong convergence order. As applications of this result, we obtain the convergence order of the numerical invariant measure, the strong law of large numbers and central limit theorem related to the numerical solution, and the error estimate of the multi-level Monte Carlo estimator. As far as we know, this is the first result on the weak convergence order for stochastic Maxwell equations.","sentences":["It is known from the monograph [1, Chapter 5] that the weak convergence analysis of numerical schemes for stochastic Maxwell equations is an unsolved problem.","This paper aims to fill the gap by establishing the long-time weak convergence analysis of the semi-implicit Euler scheme for stochastic Maxwell equations.","Based on analyzing the regularity of transformed Kolmogorov equation associated to stochastic Maxwell equations and constructing a proper continuous adapted auxiliary process for the semi-implicit scheme, we present the long-time weak convergence analysis for this scheme and prove that the weak convergence order is one, which is twice the strong convergence order.","As applications of this result, we obtain the convergence order of the numerical invariant measure, the strong law of large numbers and central limit theorem related to the numerical solution, and the error estimate of the multi-level Monte Carlo estimator.","As far as we know, this is the first result on the weak convergence order for stochastic Maxwell equations."],"url":"http://arxiv.org/abs/2403.09293v1","category":"math.NA"}
{"created":"2024-03-14 11:18:18","title":"Optical Calibration of Holographic Acoustic Tweezers","abstract":"Recently, acoustic tweezers based on an array of ultrasonic transducers have been reported taking inspiration from holographic optical tweezers. In the latter technique, the calibration of the optical trap is an essential procedure to obtain the trap stiffnesses. On the contrary, in the case of acoustic tweezers the calibration of the acoustic forces is seldom carried out. To cover this gap, in this work, we adapt the calibration protocols employed in optical tweezers to acoustic tweezers based on arrays of ultrasonic transducers. We measure trap stiffnesses in the mN/m range that are consistent with theoretical estimates obtained by calculations of the acoustic radiation forces based on the Gorkov potential. This work gives a common framework to the optical and acoustic manipulation communities, paving the way to a consistent calibration of hybrid acoustooptical setups.","sentences":["Recently, acoustic tweezers based on an array of ultrasonic transducers have been reported taking inspiration from holographic optical tweezers.","In the latter technique, the calibration of the optical trap is an essential procedure to obtain the trap stiffnesses.","On the contrary, in the case of acoustic tweezers the calibration of the acoustic forces is seldom carried out.","To cover this gap, in this work, we adapt the calibration protocols employed in optical tweezers to acoustic tweezers based on arrays of ultrasonic transducers.","We measure trap stiffnesses in the mN/m range that are consistent with theoretical estimates obtained by calculations of the acoustic radiation forces based on the Gorkov potential.","This work gives a common framework to the optical and acoustic manipulation communities, paving the way to a consistent calibration of hybrid acoustooptical setups."],"url":"http://arxiv.org/abs/2403.09286v1","category":"physics.optics"}
{"created":"2024-03-14 10:30:43","title":"WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images","abstract":"The Segment Anything Model (SAM) marks a significant advancement in segmentation models, offering powerful zero-shot capabilities and dynamic prompting. However, existing medical SAMs are not suitable for the multi-scale nature of whole-slide images (WSIs), restricting their effectiveness. To resolve this drawback, we present WSI-SAM, enhancing SAM with precise object segmentation capabilities for histopathology images using multi-resolution patches, while preserving its original prompt-driven design, efficiency, and zero-shot adaptability. To fully exploit pretrained knowledge while minimizing training overhead, we keep SAM frozen, only introducing minimal additional parameters and computation. In particular, we introduce High-Resolution (HR) token, Low-Resolution (LR) token and dual mask decoder. This decoder integrates the original SAM mask decoder with a lightweight fusion module that integrates features at multiple scales. Instead of predicting a mask independently, we integrate HR and LR token at intermediate layer to jointly learn features of the same object across multiple resolutions. Experiments show that our WSI-SAM outperforms state-of-the-art SAM and its variants. In particular, our model outperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ (DCIS) segmentation tasks and breast cancer metastasis segmentation task (CAMELYON16 dataset). The code will be available at https://github.com/HongLiuuuuu/WSI-SAM.","sentences":["The Segment Anything Model (SAM) marks a significant advancement in segmentation models, offering powerful zero-shot capabilities and dynamic prompting.","However, existing medical SAMs are not suitable for the multi-scale nature of whole-slide images (WSIs), restricting their effectiveness.","To resolve this drawback, we present WSI-SAM, enhancing SAM with precise object segmentation capabilities for histopathology images using multi-resolution patches, while preserving its original prompt-driven design, efficiency, and zero-shot adaptability.","To fully exploit pretrained knowledge while minimizing training overhead, we keep SAM frozen, only introducing minimal additional parameters and computation.","In particular, we introduce High-Resolution (HR) token, Low-Resolution (LR) token and dual mask decoder.","This decoder integrates the original SAM mask decoder with a lightweight fusion module that integrates features at multiple scales.","Instead of predicting a mask independently, we integrate HR and LR token at intermediate layer to jointly learn features of the same object across multiple resolutions.","Experiments show that our WSI-SAM outperforms state-of-the-art SAM and its variants.","In particular, our model outperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ (DCIS) segmentation tasks and breast cancer metastasis segmentation task (CAMELYON16 dataset).","The code will be available at https://github.com/HongLiuuuuu/WSI-SAM."],"url":"http://arxiv.org/abs/2403.09257v1","category":"cs.CV"}
{"created":"2024-03-14 09:21:25","title":"TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks","abstract":"In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA","sentences":["In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks.","As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA.","It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks.","Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning.","We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning.","All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA"],"url":"http://arxiv.org/abs/2403.09207v1","category":"cs.CL"}
{"created":"2024-03-14 06:40:34","title":"Viral Load Inference in Non-Adaptive Pooled Testing","abstract":"Medical diagnostic testing can be made significantly more efficient using pooled testing protocols. These typically require a sparse infection signal and use either binary or real-valued entries of O(1). However, existing methods do not allow for inferring viral loads which span many orders of magnitude. We develop a message passing algorithm coupled with a PCR (Polymerase Chain Reaction) specific noise function to allow accurate inference of realistic viral load signals. This work is in the non-adaptive setting and could open the possibility of efficient screening where viral load determination is clinically important.","sentences":["Medical diagnostic testing can be made significantly more efficient using pooled testing protocols.","These typically require a sparse infection signal and use either binary or real-valued entries of O(1).","However, existing methods do not allow for inferring viral loads which span many orders of magnitude.","We develop a message passing algorithm coupled with a PCR (Polymerase Chain Reaction) specific noise function to allow accurate inference of realistic viral load signals.","This work is in the non-adaptive setting and could open the possibility of efficient screening where viral load determination is clinically important."],"url":"http://arxiv.org/abs/2403.09130v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 04:26:00","title":"Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization","abstract":"Gradient Descent Ascent (GDA) methods for min-max optimization problems typically produce oscillatory behavior that can lead to instability, e.g., in bilinear settings. To address this problem, we introduce a dissipation term into the GDA updates to dampen these oscillations. The proposed Dissipative GDA (DGDA) method can be seen as performing standard GDA on a state-augmented and regularized saddle function that does not strictly introduce additional convexity/concavity. We theoretically show the linear convergence of DGDA in the bilinear and strongly convex-strongly concave settings and assess its performance by comparing DGDA with other methods such as GDA, Extra-Gradient (EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these methods, achieving superior convergence rates. We support our claims with two numerical examples that showcase DGDA's effectiveness in solving saddle point problems.","sentences":["Gradient Descent Ascent (GDA) methods for min-max optimization problems typically produce oscillatory behavior that can lead to instability, e.g., in bilinear settings.","To address this problem, we introduce a dissipation term into the GDA updates to dampen these oscillations.","The proposed Dissipative GDA (DGDA) method can be seen as performing standard GDA on a state-augmented and regularized saddle function that does not strictly introduce additional convexity/concavity.","We theoretically show the linear convergence of DGDA in the bilinear and strongly convex-strongly concave settings and assess its performance by comparing DGDA with other methods such as GDA, Extra-Gradient (EG), and Optimistic GDA.","Our findings demonstrate that DGDA surpasses these methods, achieving superior convergence rates.","We support our claims with two numerical examples that showcase DGDA's effectiveness in solving saddle point problems."],"url":"http://arxiv.org/abs/2403.09090v1","category":"math.OC"}
{"created":"2024-03-14 03:26:08","title":"Analytical Heterogeneous Die-to-Die 3D Placement with Macros","abstract":"This paper presents an innovative approach to 3D mixed-size placement in heterogeneous face-to-face (F2F) bonded 3D ICs. We propose an analytical framework that utilizes a dedicated density model and a bistratal wirelength model, effectively handling macros and standard cells in a 3D solution space. A novel 3D preconditioner is developed to resolve the topological and physical gap between macros and standard cells. Additionally, we propose a mixed-integer linear programming (MILP) formulation for macro rotation to optimize wirelength. Our framework is implemented with full-scale GPU acceleration, leveraging an adaptive 3D density accumulation algorithm and an incremental wirelength gradient algorithm. Experimental results on ICCAD 2023 contest benchmarks demonstrate that our framework can achieve 5.9% quality score improvement compared to the first-place winner with 4.0x runtime speedup.","sentences":["This paper presents an innovative approach to 3D mixed-size placement in heterogeneous face-to-face (F2F) bonded 3D ICs.","We propose an analytical framework that utilizes a dedicated density model and a bistratal wirelength model, effectively handling macros and standard cells in a 3D solution space.","A novel 3D preconditioner is developed to resolve the topological and physical gap between macros and standard cells.","Additionally, we propose a mixed-integer linear programming (MILP) formulation for macro rotation to optimize wirelength.","Our framework is implemented with full-scale GPU acceleration, leveraging an adaptive 3D density accumulation algorithm and an incremental wirelength gradient algorithm.","Experimental results on ICCAD 2023 contest benchmarks demonstrate that our framework can achieve 5.9% quality score improvement compared to the first-place winner with 4.0x runtime speedup."],"url":"http://arxiv.org/abs/2403.09070v1","category":"cs.AR"}
{"created":"2024-03-14 03:18:52","title":"Confidence-Aware Safe and Stable Control of Control-Affine Systems","abstract":"Designing control inputs that satisfy safety requirements is crucial in safety-critical nonlinear control, and this task becomes particularly challenging when full-state measurements are unavailable. In this work, we address the problem of synthesizing safe and stable control for control-affine systems via output feedback (using an observer) while reducing the estimation error of the observer. To achieve this, we adapt control Lyapunov function (CLF) and control barrier function (CBF) techniques to the output feedback setting. Building upon the existing CLF-CBF-QP (Quadratic Program) and CBF-QP frameworks, we formulate two confidence-aware optimization problems and establish the Lipschitz continuity of the obtained solutions. To validate our approach, we conduct simulation studies on two illustrative examples. The simulation studies indicate both improvements in the observer's estimation accuracy and the fulfillment of safety and control requirements.","sentences":["Designing control inputs that satisfy safety requirements is crucial in safety-critical nonlinear control, and this task becomes particularly challenging when full-state measurements are unavailable.","In this work, we address the problem of synthesizing safe and stable control for control-affine systems via output feedback (using an observer) while reducing the estimation error of the observer.","To achieve this, we adapt control","Lyapunov function (CLF) and control barrier function (CBF) techniques to the output feedback setting.","Building upon the existing CLF-CBF-QP (Quadratic Program) and CBF-QP frameworks, we formulate two confidence-aware optimization problems and establish the Lipschitz continuity of the obtained solutions.","To validate our approach, we conduct simulation studies on two illustrative examples.","The simulation studies indicate both improvements in the observer's estimation accuracy and the fulfillment of safety and control requirements."],"url":"http://arxiv.org/abs/2403.09067v1","category":"eess.SY"}
{"created":"2024-03-14 01:54:29","title":"Adaptivity is not helpful for Pauli channel learning","abstract":"This note shows that adaptive strategies do not offer additional advantages for learning and testing Pauli channels with entangled input. First, the tight query complexity of learning Pauli channels with entangled input is established for the general norm $l_p$. In particular, the complexities for the $l_{1}$, $l_2$ and $l_\\infty$ norms are improved or matched compared to previous results using entanglement in the literature. We also settle the query complexity to test if Pauli channels are white noise sources across $l_p$. Additionally, we demonstrate that the query complexity of estimating the noise level of a Pauli channel, characterized by the entropy of its error distribution and the count of non-zero probabilities, is $\\Theta(4^n/n)$. Further, $\\Theta(4^n/n)$ queries are sufficient to estimate the diamond norm between two Pauli channels.","sentences":["This note shows that adaptive strategies do not offer additional advantages for learning and testing Pauli channels with entangled input.","First, the tight query complexity of learning Pauli channels with entangled input is established for the general norm $l_p$. In particular, the complexities for the $l_{1}$, $l_2$ and $l_\\infty$ norms are improved or matched compared to previous results using entanglement in the literature.","We also settle the query complexity to test if Pauli channels are white noise sources across $l_p$. Additionally, we demonstrate that the query complexity of estimating the noise level of a Pauli channel, characterized by the entropy of its error distribution and the count of non-zero probabilities, is $\\Theta(4^n/n)$. Further, $\\Theta(4^n/n)$ queries are sufficient to estimate the diamond norm between two Pauli channels."],"url":"http://arxiv.org/abs/2403.09033v1","category":"quant-ph"}
{"created":"2024-03-14 01:39:12","title":"FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices","abstract":"This paper introduces FlexNN, a Flexible Neural Network accelerator, which adopts agile design principles to enable versatile dataflows, enhancing energy efficiency. Unlike conventional convolutional neural network accelerator architectures that adhere to fixed dataflows (such as input, weight, output, or row stationary) for transferring activations and weights between storage and compute units, our design revolutionizes by enabling adaptable dataflows of any type through software configurable descriptors. Considering that data movement costs considerably outweigh compute costs from an energy perspective, the flexibility in dataflow allows us to optimize the movement per layer for minimal data transfer and energy consumption, a capability unattainable in fixed dataflow architectures. To further enhance throughput and reduce energy consumption in the FlexNN architecture, we propose a novel sparsity-based acceleration logic that utilizes fine-grained sparsity in both the activation and weight tensors to bypass redundant computations, thus optimizing the convolution engine within the hardware accelerator. Extensive experimental results underscore a significant enhancement in the performance and energy efficiency of FlexNN relative to existing DNN accelerators.","sentences":["This paper introduces FlexNN, a Flexible Neural Network accelerator, which adopts agile design principles to enable versatile dataflows, enhancing energy efficiency.","Unlike conventional convolutional neural network accelerator architectures that adhere to fixed dataflows (such as input, weight, output, or row stationary) for transferring activations and weights between storage and compute units, our design revolutionizes by enabling adaptable dataflows of any type through software configurable descriptors.","Considering that data movement costs considerably outweigh compute costs from an energy perspective, the flexibility in dataflow allows us to optimize the movement per layer for minimal data transfer and energy consumption, a capability unattainable in fixed dataflow architectures.","To further enhance throughput and reduce energy consumption in the FlexNN architecture, we propose a novel sparsity-based acceleration logic that utilizes fine-grained sparsity in both the activation and weight tensors to bypass redundant computations, thus optimizing the convolution engine within the hardware accelerator.","Extensive experimental results underscore a significant enhancement in the performance and energy efficiency of FlexNN relative to existing DNN accelerators."],"url":"http://arxiv.org/abs/2403.09026v1","category":"cs.AR"}
{"created":"2024-03-14 01:30:28","title":"VDNA-PR: Using General Dataset Representations for Robust Sequential Visual Place Recognition","abstract":"This paper adapts a general dataset representation technique to produce robust Visual Place Recognition (VPR) descriptors, crucial to enable real-world mobile robot localisation. Two parallel lines of work on VPR have shown, on one side, that general-purpose off-the-shelf feature representations can provide robustness to domain shifts, and, on the other, that fused information from sequences of images improves performance. In our recent work on measuring domain gaps between image datasets, we proposed a Visual Distribution of Neuron Activations (VDNA) representation to represent datasets of images. This representation can naturally handle image sequences and provides a general and granular feature representation derived from a general-purpose model. Moreover, our representation is based on tracking neuron activation values over the list of images to represent and is not limited to a particular neural network layer, therefore having access to high- and low-level concepts. This work shows how VDNAs can be used for VPR by learning a very lightweight and simple encoder to generate task-specific descriptors. Our experiments show that our representation can allow for better robustness than current solutions to serious domain shifts away from the training data distribution, such as to indoor environments and aerial imagery.","sentences":["This paper adapts a general dataset representation technique to produce robust Visual Place Recognition (VPR) descriptors, crucial to enable real-world mobile robot localisation.","Two parallel lines of work on VPR have shown, on one side, that general-purpose off-the-shelf feature representations can provide robustness to domain shifts, and, on the other, that fused information from sequences of images improves performance.","In our recent work on measuring domain gaps between image datasets, we proposed a Visual Distribution of Neuron Activations (VDNA) representation to represent datasets of images.","This representation can naturally handle image sequences and provides a general and granular feature representation derived from a general-purpose model.","Moreover, our representation is based on tracking neuron activation values over the list of images to represent and is not limited to a particular neural network layer, therefore having access to high- and low-level concepts.","This work shows how VDNAs can be used for VPR by learning a very lightweight and simple encoder to generate task-specific descriptors.","Our experiments show that our representation can allow for better robustness than current solutions to serious domain shifts away from the training data distribution, such as to indoor environments and aerial imagery."],"url":"http://arxiv.org/abs/2403.09025v1","category":"cs.CV"}
{"created":"2024-03-14 00:55:57","title":"Unified description of electronic orderings and cross correlations by complete multipole representation","abstract":"We overview recent developments of electronic orderings and associated cross correlations in condensed matter physics based on a complete set of multipole representations (electric, magnetic, electric toroidal, and magnetic toroidal multipoles) with distinct space-time inversion symmetries. By means of the symmetry-adapted complete basis set in any physical Hilbert space including atomic and cluster degrees of freedom, it provides a unified description of electronic orderings, deformations of band structures, cross correlations, and transport properties in solids. We review that a multipole representation is a powerful tool to attain a comprehensive understanding of various electronic properties and physical phenomena observed in materials. We also demonstrate how to identify active multipoles and expected physical phenomena in real materials with several examples. Our review would serve as a solid foundation for future studies toward observations and identifications of unknown electronic phases and their related physical phenomena.","sentences":["We overview recent developments of electronic orderings and associated cross correlations in condensed matter physics based on a complete set of multipole representations (electric, magnetic, electric toroidal, and magnetic toroidal multipoles) with distinct space-time inversion symmetries.","By means of the symmetry-adapted complete basis set in any physical Hilbert space including atomic and cluster degrees of freedom, it provides a unified description of electronic orderings, deformations of band structures, cross correlations, and transport properties in solids.","We review that a multipole representation is a powerful tool to attain a comprehensive understanding of various electronic properties and physical phenomena observed in materials.","We also demonstrate how to identify active multipoles and expected physical phenomena in real materials with several examples.","Our review would serve as a solid foundation for future studies toward observations and identifications of unknown electronic phases and their related physical phenomena."],"url":"http://arxiv.org/abs/2403.09019v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 00:17:34","title":"Lightning-fast adaptive immune receptor similarity search by symmetric deletion lookup","abstract":"An individual's adaptive immune receptor (AIR) repertoire records immune history due to the exquisite antigen specificity of AIRs. Reading this record requires computational approaches for inferring receptor function from sequence, as the diversity of possible receptor-antigen pairs vastly outstrips experimental knowledge. Identification of AIRs with similar sequence and thus putatively similar function is a common performance bottleneck in these approaches. Here, we benchmark the time complexity of five different algorithmic approaches to radius-based search for Levenshtein neighbors. We show that a symmetric deletion lookup approach, originally proposed for spell-checking, is particularly scalable. We then introduce XTNeighbor, a variant of this algorithm that can be massively parallelized on GPUs. For one million input sequences, XTNeighbor identifies all sequence neighbors that differ by up to two edits in seconds on commodity hardware, orders of magnitude faster than existing approaches. We also demonstrate how symmetric deletion lookup can speed up search with more complex sequence-similarity metrics such as TCRdist. Our contribution is poised to greatly speed up existing analysis pipelines and enable processing of large-scale immunosequencing data without downsampling.","sentences":["An individual's adaptive immune receptor (AIR) repertoire records immune history due to the exquisite antigen specificity of AIRs.","Reading this record requires computational approaches for inferring receptor function from sequence, as the diversity of possible receptor-antigen pairs vastly outstrips experimental knowledge.","Identification of AIRs with similar sequence and thus putatively similar function is a common performance bottleneck in these approaches.","Here, we benchmark the time complexity of five different algorithmic approaches to radius-based search for Levenshtein neighbors.","We show that a symmetric deletion lookup approach, originally proposed for spell-checking, is particularly scalable.","We then introduce XTNeighbor, a variant of this algorithm that can be massively parallelized on GPUs.","For one million input sequences, XTNeighbor identifies all sequence neighbors that differ by up to two edits in seconds on commodity hardware, orders of magnitude faster than existing approaches.","We also demonstrate how symmetric deletion lookup can speed up search with more complex sequence-similarity metrics such as TCRdist.","Our contribution is poised to greatly speed up existing analysis pipelines and enable processing of large-scale immunosequencing data without downsampling."],"url":"http://arxiv.org/abs/2403.09010v1","category":"q-bio.QM"}
{"created":"2024-03-14 00:08:54","title":"A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents","abstract":"This paper presents a novel approach for resilient distributed consensus in multiagent networks when dealing with adversarial agents imprecision in states observed by normal agents. Traditional resilient distributed consensus algorithms often presume that agents have exact knowledge of their neighbors' states, which is unrealistic in practical scenarios. We show that such existing methods are inadequate when agents only have access to imprecise states of their neighbors. To overcome this challenge, we adapt a geometric approach and model an agent's state by an `imprecision region' rather than a point in $\\mathbb{R}^d$. From a given set of imprecision regions, we first present an efficient way to compute a region that is guaranteed to lie in the convex hull of true, albeit unknown, states of agents. We call this region the \\emph{invariant hull} of imprecision regions and provide its geometric characterization. Next, we use these invariant hulls to identify a \\emph{safe point} for each normal agent. The safe point of an agent lies within the convex hull of its \\emph{normal} neighbors' states and hence is used by the agent to update it's state. This leads to the aggregation of normal agents' states to safe points inside the convex hull of their initial states, or an approximation of consensus. We also illustrate our results through simulations. Our contributions enhance the robustness of resilient distributed consensus algorithms by accommodating state imprecision without compromising resilience against adversarial agents.","sentences":["This paper presents a novel approach for resilient distributed consensus in multiagent networks when dealing with adversarial agents imprecision in states observed by normal agents.","Traditional resilient distributed consensus algorithms often presume that agents have exact knowledge of their neighbors' states, which is unrealistic in practical scenarios.","We show that such existing methods are inadequate when agents only have access to imprecise states of their neighbors.","To overcome this challenge, we adapt a geometric approach and model an agent's state by an `imprecision region' rather than a point in $\\mathbb{R}^d$. From a given set of imprecision regions, we first present an efficient way to compute a region that is guaranteed to lie in the convex hull of true, albeit unknown, states of agents.","We call this region the \\emph{invariant hull} of imprecision regions and provide its geometric characterization.","Next, we use these invariant hulls to identify a \\emph{safe point} for each normal agent.","The safe point of an agent lies within the convex hull of its \\emph{normal} neighbors' states and hence is used by the agent to update it's state.","This leads to the aggregation of normal agents' states to safe points inside the convex hull of their initial states, or an approximation of consensus.","We also illustrate our results through simulations.","Our contributions enhance the robustness of resilient distributed consensus algorithms by accommodating state imprecision without compromising resilience against adversarial agents."],"url":"http://arxiv.org/abs/2403.09009v1","category":"eess.SY"}
{"created":"2024-03-13 23:32:14","title":"Frequency-dependent entanglement advantage in spin-network Quantum Reservoir Computing","abstract":"We study the performance of an Ising spin network for quantum reservoir computing (QRC) in linear and non-linear memory tasks. We investigate the extent to which quantumness enhances performance by monitoring the behaviour of quantum entanglement, which we quantify by means of the partial transpose of the density matrix. In the most general case where the effects of dissipation are incorporated, our results indicate that the strength of the entanglement advantage depends on the frequency of the input signal; the benefit of entanglement is greater the more rapidly fluctuating the signal, whereas a low-frequency input lends itself better to a non-entangled reservoir. This suggests that the extent to which quantumness is beneficial is dependent on the difficulty of the memory task.","sentences":["We study the performance of an Ising spin network for quantum reservoir computing (QRC) in linear and non-linear memory tasks.","We investigate the extent to which quantumness enhances performance by monitoring the behaviour of quantum entanglement, which we quantify by means of the partial transpose of the density matrix.","In the most general case where the effects of dissipation are incorporated, our results indicate that the strength of the entanglement advantage depends on the frequency of the input signal; the benefit of entanglement is greater the more rapidly fluctuating the signal, whereas a low-frequency input lends itself better to a non-entangled reservoir.","This suggests that the extent to which quantumness is beneficial is dependent on the difficulty of the memory task."],"url":"http://arxiv.org/abs/2403.08998v1","category":"quant-ph"}
{"created":"2024-03-13 21:52:39","title":"Spectral inequalities for Schr\u00f6dinger equations with various potentials","abstract":"We study the spectral inequalities of Schr\\\"odinger operator in the whole space for different potentials, which can be power growth or continuously vanishing at infinity. The spectral inequalities quantitatively depends on the density of the sensor sets with positive measure, growth rate of the potentials and spectrum (or eigenvalues). One important component in the poof is the adaptation of propagation of smallness argument for gradients in \\cite{LM18}. As an application, we apply the spectral inequalities to obtain quantitative observability inequalities for heat equations.","sentences":["We study the spectral inequalities of Schr\\\"odinger operator in the whole space for different potentials, which can be power growth or continuously vanishing at infinity.","The spectral inequalities quantitatively depends on the density of the sensor sets with positive measure, growth rate of the potentials and spectrum (or eigenvalues).","One important component in the poof is the adaptation of propagation of smallness argument for gradients in \\cite{LM18}.","As an application, we apply the spectral inequalities to obtain quantitative observability inequalities for heat equations."],"url":"http://arxiv.org/abs/2403.08975v1","category":"math.AP"}
{"created":"2024-03-13 21:30:01","title":"Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling","abstract":"Recent studies have demonstrated that the ability of dense retrieval models to generalize to target domains with different distributions is limited, which contrasts with the results obtained with interaction-based models. Prior attempts to mitigate this challenge involved leveraging adversarial learning and query generation approaches, but both approaches nevertheless resulted in limited improvements. In this paper, we propose to combine the query-generation approach with a self-supervision approach in which pseudo-relevance labels are automatically generated on the target domain. To accomplish this, a T5-3B model is utilized for pseudo-positive labeling, and meticulous hard negatives are chosen. We also apply this strategy on conversational dense retrieval model for conversational search. A similar pseudo-labeling approach is used, but with the addition of a query-rewriting module to rewrite conversational queries for subsequent labeling. This proposed approach enables a model's domain adaptation with real queries and documents from the target dataset. Experiments on standard dense retrieval and conversational dense retrieval models both demonstrate improvements on baseline models when they are fine-tuned on the pseudo-relevance labeled data.","sentences":["Recent studies have demonstrated that the ability of dense retrieval models to generalize to target domains with different distributions is limited, which contrasts with the results obtained with interaction-based models.","Prior attempts to mitigate this challenge involved leveraging adversarial learning and query generation approaches, but both approaches nevertheless resulted in limited improvements.","In this paper, we propose to combine the query-generation approach with a self-supervision approach in which pseudo-relevance labels are automatically generated on the target domain.","To accomplish this, a T5-3B model is utilized for pseudo-positive labeling, and meticulous hard negatives are chosen.","We also apply this strategy on conversational dense retrieval model for conversational search.","A similar pseudo-labeling approach is used, but with the addition of a query-rewriting module to rewrite conversational queries for subsequent labeling.","This proposed approach enables a model's domain adaptation with real queries and documents from the target dataset.","Experiments on standard dense retrieval and conversational dense retrieval models both demonstrate improvements on baseline models when they are fine-tuned on the pseudo-relevance labeled data."],"url":"http://arxiv.org/abs/2403.08970v1","category":"cs.IR"}
{"created":"2024-03-13 19:16:10","title":"Rollover Prevention for Mobile Robots with Control Barrier Functions: Differentiator-Based Adaptation and Projection-to-State Safety","abstract":"This paper develops rollover prevention guarantees for mobile robots using control barrier function (CBF) theory, and demonstrates these formal results experimentally. To this end, we consider a safety measure based on the zero moment point to provide conditions on the control input through the lens of CBFs. However, these conditions depend on time-varying and noisy parameters. To address this, we present a differentiator-based safety-critical controller that estimates these parameters and pairs Input-to-State Stable (ISS) differentiator dynamics with CBFs to achieve rigorous guarantees of safety. Additionally, to ensure safety in the presence of disturbance, we utilize a time-varying extension of Projection-to-State Safety (PSSf). The effectiveness of the proposed method is demonstrated through experiments on a tracked robot with a rollover potential on steep slopes.","sentences":["This paper develops rollover prevention guarantees for mobile robots using control barrier function (CBF) theory, and demonstrates these formal results experimentally.","To this end, we consider a safety measure based on the zero moment point to provide conditions on the control input through the lens of CBFs.","However, these conditions depend on time-varying and noisy parameters.","To address this, we present a differentiator-based safety-critical controller that estimates these parameters and pairs Input-to-State Stable (ISS) differentiator dynamics with CBFs to achieve rigorous guarantees of safety.","Additionally, to ensure safety in the presence of disturbance, we utilize a time-varying extension of Projection-to-State Safety (PSSf).","The effectiveness of the proposed method is demonstrated through experiments on a tracked robot with a rollover potential on steep slopes."],"url":"http://arxiv.org/abs/2403.08916v1","category":"eess.SY"}
{"created":"2024-03-13 05:22:39","title":"TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation","abstract":"Zero-shot navigation is a critical challenge in Vision-Language Navigation (VLN) tasks, where the ability to adapt to unfamiliar instructions and to act in unknown environments is essential. Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities. Large Language Models (LLMs), with their extensive knowledge and emergent reasoning abilities, present a potential pathway for achieving zero-shot navigation. This paper presents a VLN agent based on LLMs, exploring approaches to the zero-shot navigation problem. To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent to scrutinize perceptual information and autonomously query key clues within the environment through an introduced question-answering module, thereby aligning instructions with specific perceptual data. The navigation agent's perceptual abilities are enhanced through the TINA framework, while the explicit thought and query processes also improve the navigational procedure's explainability and transparency. We evaluate the performance of our method on the Room-to-Room dataset. The experiment results indicate that our approach improves the navigation performance of LLM-based agents. Our approach also outperformed some supervised learning-based methods, highlighting its efficacy in zero-shot navigation.","sentences":["Zero-shot navigation is a critical challenge in Vision-Language Navigation (VLN) tasks, where the ability to adapt to unfamiliar instructions and to act in unknown environments is essential.","Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities.","Large Language Models (LLMs), with their extensive knowledge and emergent reasoning abilities, present a potential pathway for achieving zero-shot navigation.","This paper presents a VLN agent based on LLMs, exploring approaches to the zero-shot navigation problem.","To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework.","TINA enables the agent to scrutinize perceptual information and autonomously query key clues within the environment through an introduced question-answering module, thereby aligning instructions with specific perceptual data.","The navigation agent's perceptual abilities are enhanced through the TINA framework, while the explicit thought and query processes also improve the navigational procedure's explainability and transparency.","We evaluate the performance of our method on the Room-to-Room dataset.","The experiment results indicate that our approach improves the navigation performance of LLM-based agents.","Our approach also outperformed some supervised learning-based methods, highlighting its efficacy in zero-shot navigation."],"url":"http://arxiv.org/abs/2403.08833v1","category":"cs.CV"}
{"created":"2024-03-14 17:51:54","title":"Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training","abstract":"We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.","sentences":["We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence.","Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again.","The behavior emerges and becomes more robust as the architecture scales up its number of parameters.","Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments."],"url":"http://arxiv.org/abs/2403.09613v1","category":"cs.LG"}
{"created":"2024-03-14 17:12:17","title":"Commutation principles for nonsmooth variational problems on Euclidean Jordan algebras","abstract":"The commutation principle proved by Ram\\'irez, Seeger, and Sossa (SIAM J Optim 23:687-694, 2013) in the setting of Euclidean Jordan algebras says that for a Fr\\'echet differentiable function $\\Theta$ and a spectral function $F$, any local minimizer or maximizer $a$ of $\\Theta+F$ over a spectral set $\\mathcal{E}$ operator commutes with the gradient of $\\Theta$ at $a$. In this paper, we improve this commutation principle by allowing $\\Theta$ to be nonsmooth with mild regularity assumptions over it. For example, for the case of local minimizer, we show that $a$ operator commutes with some element of the limiting (Mordukhovich) subdifferential of $\\Theta$ at $a$ provided that $\\Theta$ is subdifferentially regular at $a$ satisfying a qualification condition. For the case of local maximizer, we prove that $a$ operator commutes with each element of the (Fenchel) subdifferential of $\\Theta$ at $a$ whenever this subdifferential is nonempty. As an application, we characterize the local optimizers of shifted strictly convex spectral functions and norms over automorphism invariant sets.","sentences":["The commutation principle proved by Ram\\'irez, Seeger, and Sossa (SIAM J Optim 23:687-694, 2013) in the setting of Euclidean Jordan algebras says that for a Fr\\'echet differentiable function $\\Theta$ and a spectral function $F$, any local minimizer or maximizer $a$ of $\\Theta+F$ over a spectral set $\\mathcal{E}$ operator commutes with the gradient of $\\Theta$ at $a$. In this paper, we improve this commutation principle by allowing $\\Theta$ to be nonsmooth with mild regularity assumptions over it.","For example, for the case of local minimizer, we show that $a$ operator commutes with some element of the limiting (Mordukhovich) subdifferential of $\\Theta$ at $a$ provided that $\\Theta$ is subdifferentially regular at $a$ satisfying a qualification condition.","For the case of local maximizer, we prove that $a$ operator commutes with each element of the (Fenchel) subdifferential of $\\Theta$ at $a$ whenever this subdifferential is nonempty.","As an application, we characterize the local optimizers of shifted strictly convex spectral functions and norms over automorphism invariant sets."],"url":"http://arxiv.org/abs/2403.09578v1","category":"math.OC"}
{"created":"2024-03-14 16:55:04","title":"Comments on the quantitative uniqueness of continuation for evolution equations","abstract":"We establish near-optimal quantitative uniqueness of continuation for solutions of evolution equations vanishing on the lateral boundary. These results were obtained simply by combining existing observability inequalities and energy estimates.","sentences":["We establish near-optimal quantitative uniqueness of continuation for solutions of evolution equations vanishing on the lateral boundary.","These results were obtained simply by combining existing observability inequalities and energy estimates."],"url":"http://arxiv.org/abs/2403.09564v1","category":"math.AP"}
{"created":"2024-03-14 16:10:17","title":"Physics-Informed Neural Network for Volumetric Sound field Reconstruction of Speech Signals","abstract":"Recent developments in acoustic signal processing have seen the integration of deep learning methodologies, alongside the continued prominence of classical wave expansion-based approaches, particularly in sound field reconstruction. Physics-Informed Neural Networks (PINNs) have emerged as a novel framework, bridging the gap between data-driven and model-based techniques for addressing physical phenomena governed by partial differential equations. This paper introduces a PINN-based approach for the recovery of arbitrary volumetric acoustic fields. The network incorporates the wave equation to impose a regularization on signal reconstruction in the time domain. This methodology enables the network to learn the underlying physics of sound propagation and allows for the complete characterization of the sound field based on a limited set of observations. The proposed method's efficacy is validated through experiments involving speech signals in a real-world environment, considering varying numbers of available measurements. Moreover, a comparative analysis is undertaken against state-of-the-art frequency-domain and time-domain reconstruction methods from existing literature, highlighting the increased accuracy across the various measurement configurations.","sentences":["Recent developments in acoustic signal processing have seen the integration of deep learning methodologies, alongside the continued prominence of classical wave expansion-based approaches, particularly in sound field reconstruction.","Physics-Informed Neural Networks (PINNs) have emerged as a novel framework, bridging the gap between data-driven and model-based techniques for addressing physical phenomena governed by partial differential equations.","This paper introduces a PINN-based approach for the recovery of arbitrary volumetric acoustic fields.","The network incorporates the wave equation to impose a regularization on signal reconstruction in the time domain.","This methodology enables the network to learn the underlying physics of sound propagation and allows for the complete characterization of the sound field based on a limited set of observations.","The proposed method's efficacy is validated through experiments involving speech signals in a real-world environment, considering varying numbers of available measurements.","Moreover, a comparative analysis is undertaken against state-of-the-art frequency-domain and time-domain reconstruction methods from existing literature, highlighting the increased accuracy across the various measurement configurations."],"url":"http://arxiv.org/abs/2403.09524v1","category":"eess.AS"}
{"created":"2024-03-14 15:52:13","title":"Efficient Convolutional Forward Modeling and Sparse Coding in Multichannel Imaging","abstract":"This study considers the Block-Toeplitz structural properties inherent in traditional multichannel forward model matrices, using Full Matrix Capture (FMC) in ultrasonic testing as a case study. We propose an analytical convolutional forward model that transforms reflectivity maps into FMC data. Our findings demonstrate that the convolutional model excels over its matrix-based counterpart in terms of computational efficiency and storage requirements. This accelerated forward modeling approach holds significant potential for various inverse problems, notably enhancing Sparse Signal Recovery (SSR) within the context LASSO regression, which facilitates efficient Convolutional Sparse Coding (CSC) algorithms. Additionally, we explore the integration of Convolutional Neural Networks (CNNs) for the forward model, employing deep unfolding to implement the Learned Block Convolutional ISTA (BC-LISTA).","sentences":["This study considers the Block-Toeplitz structural properties inherent in traditional multichannel forward model matrices, using Full Matrix Capture (FMC) in ultrasonic testing as a case study.","We propose an analytical convolutional forward model that transforms reflectivity maps into FMC data.","Our findings demonstrate that the convolutional model excels over its matrix-based counterpart in terms of computational efficiency and storage requirements.","This accelerated forward modeling approach holds significant potential for various inverse problems, notably enhancing Sparse Signal Recovery (SSR) within the context LASSO regression, which facilitates efficient Convolutional Sparse Coding (CSC) algorithms.","Additionally, we explore the integration of Convolutional Neural Networks (CNNs) for the forward model, employing deep unfolding to implement the Learned Block Convolutional ISTA (BC-LISTA)."],"url":"http://arxiv.org/abs/2403.09505v1","category":"eess.SP"}
{"created":"2024-03-14 15:19:19","title":"VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields","abstract":"Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, while in larger ones, it is bounded by the utilized ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic and infrared sensors is highly effective when dealing with sparse data and low view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the mapping capabilities and increases the training speed by 46% compared to Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for cost-effective local mapping in mobile robotics, with potential applications in safety and navigation tasks. The code can be found at https://github.com/ethz-asl/virus nerf.","sentences":["Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations.","Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras.","We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields.","Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching.","Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage.","Notably, in small environments, its accuracy aligns with that of LiDAR measurements, while in larger ones, it is bounded by the utilized ultrasonic sensors.","An in-depth ablation study reveals that adding ultrasonic and infrared sensors is highly effective when dealing with sparse data and low view variation.","Further, the proposed occupancy grid of VIRUS-NeRF improves the mapping capabilities and increases the training speed by 46% compared to Instant-NGP.","Overall, VIRUS-NeRF presents a promising approach for cost-effective local mapping in mobile robotics, with potential applications in safety and navigation tasks.","The code can be found at https://github.com/ethz-asl/virus nerf."],"url":"http://arxiv.org/abs/2403.09477v1","category":"cs.RO"}
{"created":"2024-03-14 15:05:23","title":"Mild solutions to semilinear rough partial differential equations","abstract":"We provide an existence and uniqueness result for mild solutions to rough partial differential equations in the framework of the semigroup approach. Applications to stochastic partial differential equations driven by infinite dimensional Wiener processes and infinite dimensional fractional Brownian motion are presented as well.","sentences":["We provide an existence and uniqueness result for mild solutions to rough partial differential equations in the framework of the semigroup approach.","Applications to stochastic partial differential equations driven by infinite dimensional Wiener processes and infinite dimensional fractional Brownian motion are presented as well."],"url":"http://arxiv.org/abs/2403.09466v1","category":"math.PR"}
{"created":"2024-03-14 15:03:09","title":"Deep-learning-assisted optical communication with discretized state space of structural light","abstract":"The rich structure of the transverse spatial mode of structural light has facilitated its applications in quantum information and optical communication. The Laguerre-Gaussian (LG) modes, with azimuthal and radial indexes, consist of a complete orthogonal basis to describe the transverse spatial mode of light. The azimuthal index is often endowed with the orbital angular momentum (OAM), a high dimensional degree of freedom. The advent of OAM in optical science marks a pivotal advancement, surpassing traditional optical techniques in light manipulation for advanced data encoding and signal transmission. Here, we present a scheme that utilizes the advanced deep learning technique for LG modes recognition. By discretizing the state space of the LG modes, a neural network model is trained to classify the given samples. A proof-of-principle experiment is performed to show that our scheme requires less samples for model training, while increasing the channel capacity within limited OAM number. We further apply our scheme to an image transmission task, demonstrating the ability to encode large data with low OAM number. Our work opens a new avenue for high capacity optical communication based on structural light.","sentences":["The rich structure of the transverse spatial mode of structural light has facilitated its applications in quantum information and optical communication.","The Laguerre-Gaussian (LG) modes, with azimuthal and radial indexes, consist of a complete orthogonal basis to describe the transverse spatial mode of light.","The azimuthal index is often endowed with the orbital angular momentum (OAM), a high dimensional degree of freedom.","The advent of OAM in optical science marks a pivotal advancement, surpassing traditional optical techniques in light manipulation for advanced data encoding and signal transmission.","Here, we present a scheme that utilizes the advanced deep learning technique for LG modes recognition.","By discretizing the state space of the LG modes, a neural network model is trained to classify the given samples.","A proof-of-principle experiment is performed to show that our scheme requires less samples for model training, while increasing the channel capacity within limited OAM number.","We further apply our scheme to an image transmission task, demonstrating the ability to encode large data with low OAM number.","Our work opens a new avenue for high capacity optical communication based on structural light."],"url":"http://arxiv.org/abs/2403.09462v1","category":"physics.optics"}
{"created":"2024-03-14 13:33:14","title":"The convexity of a planar domain via properties of solutions to the modified Helmholtz equation","abstract":"A new characterization of convexity of a planar domain is obtained. Its derivation involves two classical facts: the Varadhan's formula, expressing the distance function with respect to the domain's boundary via real-valued solutions of the modified Helmholtz equation, and the convexity of a planar domain in which the distance function is superharmonic","sentences":["A new characterization of convexity of a planar domain is obtained.","Its derivation involves two classical facts: the Varadhan's formula, expressing the distance function with respect to the domain's boundary via real-valued solutions of the modified Helmholtz equation, and the convexity of a planar domain in which the distance function is superharmonic"],"url":"http://arxiv.org/abs/2403.09382v1","category":"math.AP"}
{"created":"2024-03-14 13:15:46","title":"DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification","abstract":"Recent advancements in remote sensing (RS) technologies have shown their potential in accurately classifying local climate zones (LCZs). However, traditional scene-level methods using convolutional neural networks (CNNs) often struggle to integrate prior knowledge of ground objects effectively. Moreover, commonly utilized data sources like Sentinel-2 encounter difficulties in capturing detailed ground object information. To tackle these challenges, we propose a data fusion method that integrates ground object priors extracted from high-resolution Google imagery with Sentinel-2 multispectral imagery. The proposed method introduces a novel Dual-stream Fusion framework for LCZ classification (DF4LCZ), integrating instance-based location features from Google imagery with the scene-level spatial-spectral features extracted from Sentinel-2 imagery. The framework incorporates a Graph Convolutional Network (GCN) module empowered by the Segment Anything Model (SAM) to enhance feature extraction from Google imagery. Simultaneously, the framework employs a 3D-CNN architecture to learn the spectral-spatial features of Sentinel-2 imagery. Experiments are conducted on a multi-source remote sensing image dataset specifically designed for LCZ classification, validating the effectiveness of the proposed DF4LCZ. The related code and dataset are available at https://github.com/ctrlovefly/DF4LCZ.","sentences":["Recent advancements in remote sensing (RS) technologies have shown their potential in accurately classifying local climate zones (LCZs).","However, traditional scene-level methods using convolutional neural networks (CNNs) often struggle to integrate prior knowledge of ground objects effectively.","Moreover, commonly utilized data sources like Sentinel-2 encounter difficulties in capturing detailed ground object information.","To tackle these challenges, we propose a data fusion method that integrates ground object priors extracted from high-resolution Google imagery with Sentinel-2 multispectral imagery.","The proposed method introduces a novel Dual-stream Fusion framework for LCZ classification (DF4LCZ), integrating instance-based location features from Google imagery with the scene-level spatial-spectral features extracted from Sentinel-2 imagery.","The framework incorporates a Graph Convolutional Network (GCN) module empowered by the Segment Anything Model (SAM) to enhance feature extraction from Google imagery.","Simultaneously, the framework employs a 3D-CNN architecture to learn the spectral-spatial features of Sentinel-2 imagery.","Experiments are conducted on a multi-source remote sensing image dataset specifically designed for LCZ classification, validating the effectiveness of the proposed DF4LCZ.","The related code and dataset are available at https://github.com/ctrlovefly/DF4LCZ."],"url":"http://arxiv.org/abs/2403.09367v1","category":"cs.CV"}
{"created":"2024-03-14 13:15:06","title":"Existence and concentration of normalized solutions for $p$-Laplacian equations with logarithmic nonlinearity","abstract":"We investigate the existence and concentration of normalized solutions for a $p$-Laplacian problem with logarithmic nonlinearity of type \\[   \\left\\{   \\begin{array}{ll}   \\displaystyle -\\varepsilon^p\\Delta_p u+V(x)|u|^{p-2}u=\\lambda |u|^{p-2}u+|u|^{p-2}u\\log|u|^p ~\\text{in}~\\mathbb R^N,\\newline   \\displaystyle \\int_{\\mathbb R^N}|u|^pdx=a^p\\varepsilon^N,   \\end{array}   \\right. \\] where $a,\\varepsilon> 0$, $\\lambda\\in\\mathbb R$ is known as the Lagrange multiplier, $\\Delta_p\\cdot =\\text{div} (|\\nabla \\cdot|^{p-2}\\nabla \\cdot)$ denotes the usual $p$-Laplacian operator with $2\\leq p < N$ and $V \\in \\mathcal{C}^0(\\mathbb R^N)$ is the potential which satisfies some suitable assumptions. We prove that the number of positive solutions depends on the profile of $V$ and each solution concentrates around its corresponding global minimum point of $V$ in the semiclassical limit when $\\varepsilon\\to0^+$ using variational method.   Moreover, we also get the existence of normalized solutions for some logarithmic $p$-Laplacian equations involving mass-supercritical nonlinearities.","sentences":["We investigate the existence and concentration of normalized solutions for a $p$-Laplacian problem with logarithmic nonlinearity of type \\[   \\left\\{   \\begin{array}{ll}   \\displaystyle -\\varepsilon^p\\Delta_p u+V(x)|u|^{p-2}u=\\lambda |u|^{p-2}u+|u|^{p-2}u\\log|u|^p ~\\text{in}~\\mathbb R^N,\\newline   \\displaystyle \\int_{\\mathbb R^N}|u|^pdx=a^p\\varepsilon^N,   \\end{array}   \\right.","\\] where $a,\\varepsilon> 0$, $\\lambda\\in\\mathbb R$ is known as the Lagrange multiplier, $\\Delta_p\\cdot =\\text{div} (|\\nabla \\cdot|^{p-2}\\nabla \\cdot)$ denotes the usual $p$-Laplacian operator with $2\\leq p < N$ and $V \\in \\mathcal{C}^0(\\mathbb R^N)$ is the potential which satisfies some suitable assumptions.","We prove that the number of positive solutions depends on the profile of $V$ and each solution concentrates around its corresponding global minimum point of $V$ in the semiclassical limit when $\\varepsilon\\to0^+$ using variational method.   ","Moreover, we also get the existence of normalized solutions for some logarithmic $p$-Laplacian equations involving mass-supercritical nonlinearities."],"url":"http://arxiv.org/abs/2403.09366v1","category":"math.AP"}
{"created":"2024-03-14 13:02:49","title":"Very weak solutions of the Dirichlet problem for 2-Hessian equation","abstract":"For any $\\alpha $ small, we construct infinitely many $C^{1,\\alpha}$ very weak solutions to the 2-Hessian equation with prescribed boundary value. The proof relies on the convex integration method and cut-off technique.","sentences":["For any $\\alpha $ small, we construct infinitely many $C^{1,\\alpha}$ very weak solutions to the 2-Hessian equation with prescribed boundary value.","The proof relies on the convex integration method and cut-off technique."],"url":"http://arxiv.org/abs/2403.09356v1","category":"math.AP"}
{"created":"2024-03-14 12:12:20","title":"Hydrodynamic behavior near dynamical criticality of a facilitated conservative lattice gas","abstract":"We investigate a $2d$-conservative lattice gas exhibiting a dynamical active-absorbing phase transition with critical density $\\rho_c$. We derive the hydrodynamic equation for this model, showing that all critical exponents governing the large scale behavior near criticality can be obtained from two independent ones. We show that as the supercritical density approaches criticality, distinct length scales naturally appear. Remarkably, this behavior is different from the subcritical one. Numerical simulations support the critical relations and the scale separation.","sentences":["We investigate a $2d$-conservative lattice gas exhibiting a dynamical active-absorbing phase transition with critical density $\\rho_c$. We derive the hydrodynamic equation for this model, showing that all critical exponents governing the large scale behavior near criticality can be obtained from two independent ones.","We show that as the supercritical density approaches criticality, distinct length scales naturally appear.","Remarkably, this behavior is different from the subcritical one.","Numerical simulations support the critical relations and the scale separation."],"url":"http://arxiv.org/abs/2403.09324v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-14 12:09:40","title":"Anomalous quantum scattering and transport of electrons with Mexican-hat dispersion induced by electrical potential","abstract":"We theoretically study the quantum scattering and transport of electrons with Mexican-hat dispersion through both step and rectangular potential barriers by using the transfer matrix method. Owing to the torus-like iso-energy lines of the Mexican-hat dispersion, we observe the presence of double reflections and double transmissions in both two different barrier scenarios, i.e., the normal reflection (NR), retro-reflection (RR), normal transmission (NT), and specular transmission (ST).For the step potential with electrons incident from the large wavevector, the transmission is primarily governed by NT with nearly negligible ST, while the reflection is dominant by RR (NR) within (outside) the critical angle. Additionally, for electrons incident from the small wavevector, the NT can be reduced to zero by adjusting the barrier, resulting in a significant enhancement of ST and RR. For the rectangular barrier, the transmission and reflection spectra resemble those of the step barrier, but there are two kinds of resonant tunneling which can lead to perfect NT or ST. There exists a negative differential conductance (NDC) effect in the conductance spectrum. The conductance and the peak-to-valley ratio of the NDC effect can be effectively controlled by adjusting the height and width of the barrier as well as the incident energy. Our results provide a deeper understanding of the electron states governed by the Mexican-hat dispersion.","sentences":["We theoretically study the quantum scattering and transport of electrons with Mexican-hat dispersion through both step and rectangular potential barriers by using the transfer matrix method.","Owing to the torus-like iso-energy lines of the Mexican-hat dispersion, we observe the presence of double reflections and double transmissions in both two different barrier scenarios, i.e., the normal reflection (NR), retro-reflection (RR), normal transmission (NT), and specular transmission (ST).For the step potential with electrons incident from the large wavevector, the transmission is primarily governed by NT with nearly negligible ST, while the reflection is dominant by RR (NR) within (outside) the critical angle.","Additionally, for electrons incident from the small wavevector, the NT can be reduced to zero by adjusting the barrier, resulting in a significant enhancement of ST and RR.","For the rectangular barrier, the transmission and reflection spectra resemble those of the step barrier, but there are two kinds of resonant tunneling which can lead to perfect NT or ST.","There exists a negative differential conductance (NDC) effect in the conductance spectrum.","The conductance and the peak-to-valley ratio of the NDC effect can be effectively controlled by adjusting the height and width of the barrier as well as the incident energy.","Our results provide a deeper understanding of the electron states governed by the Mexican-hat dispersion."],"url":"http://arxiv.org/abs/2403.09319v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-14 12:05:25","title":"Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations","abstract":"Accurate identification of breast masses is crucial in diagnosing breast cancer; however, it can be challenging due to their small size and being camouflaged in surrounding normal glands. Worse still, it is also expensive in clinical practice to obtain adequate pixel-wise annotations for training deep neural networks. To overcome these two difficulties with one stone, we propose a semi- and weakly-supervised learning framework for mass segmentation that utilizes limited strongly-labeled samples and sufficient weakly-labeled samples to achieve satisfactory performance. The framework consists of an auxiliary branch to exclude lesion-irrelevant background areas, a segmentation branch for final prediction, and a spatial prompting module to integrate the complementary information of the two branches. We further disentangle encoded obscure features into lesion-related and others to boost performance. Experiments on CBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.","sentences":["Accurate identification of breast masses is crucial in diagnosing breast cancer; however, it can be challenging due to their small size and being camouflaged in surrounding normal glands.","Worse still, it is also expensive in clinical practice to obtain adequate pixel-wise annotations for training deep neural networks.","To overcome these two difficulties with one stone, we propose a semi- and weakly-supervised learning framework for mass segmentation that utilizes limited strongly-labeled samples and sufficient weakly-labeled samples to achieve satisfactory performance.","The framework consists of an auxiliary branch to exclude lesion-irrelevant background areas, a segmentation branch for final prediction, and a spatial prompting module to integrate the complementary information of the two branches.","We further disentangle encoded obscure features into lesion-related and others to boost performance.","Experiments on CBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2403.09315v1","category":"cs.CV"}
{"created":"2024-03-14 12:05:18","title":"Onsager vortex clusters on a sphere","abstract":"We study Onsager vortex clustered states in a shell-shaped superfluid containing a large number of quantum vortices. In the incompressible limit and at low temperatures, the relevant problem can be boiled down to the statistical mechanics of neutral point vortices confined on a sphere. We analyze rotation free vortex clustered states within the mean field theory in the microcanonical ensemble. We find that the sandwich state, which involves the separating of vortices with opposite circulation and the clustering of vortices with same circulation around the poles and the equator, is the maximum entropy vortex distribution, subject to zero angular momentum constraint. The dipole momentum vanishes for the sandwich state and the quadrupole tensor serves as an order parameter to characterize the vortex cluster structure. For given finite angular momentum, the equilibrium vortex distribution forms a dipole structure, i.e., vortices with opposite sign are separated and are accumulated around the south and north pole, respectively. The conditions for the onset of clustering, and the exponents associated with the quadrupole moment and the dipole moment as functions of energy, are obtained within the mean field theory. At large energies, we obtain asymptotically exact vortex density distributions using the stereographic projection method, which give rise the parameter bounds for the vortex clustered states. The analytical predictions are in excellent agreement with microcanonical Monte Carlo simulations.","sentences":["We study Onsager vortex clustered states in a shell-shaped superfluid containing a large number of quantum vortices.","In the incompressible limit and at low temperatures, the relevant problem can be boiled down to the statistical mechanics of neutral point vortices confined on a sphere.","We analyze rotation free vortex clustered states within the mean field theory in the microcanonical ensemble.","We find that the sandwich state, which involves the separating of vortices with opposite circulation and the clustering of vortices with same circulation around the poles and the equator, is the maximum entropy vortex distribution, subject to zero angular momentum constraint.","The dipole momentum vanishes for the sandwich state and the quadrupole tensor serves as an order parameter to characterize the vortex cluster structure.","For given finite angular momentum, the equilibrium vortex distribution forms a dipole structure, i.e., vortices with opposite sign are separated and are accumulated around the south and north pole, respectively.","The conditions for the onset of clustering, and the exponents associated with the quadrupole moment and the dipole moment as functions of energy, are obtained within the mean field theory.","At large energies, we obtain asymptotically exact vortex density distributions using the stereographic projection method, which give rise the parameter bounds for the vortex clustered states.","The analytical predictions are in excellent agreement with microcanonical Monte Carlo simulations."],"url":"http://arxiv.org/abs/2403.09314v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-14 11:49:43","title":"StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images","abstract":"Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images. We propose a new approach, StainFuser, which treats this problem as a style transfer task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components. With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural style transfer for high-quality transformations. Trained on this data, StainFuser outperforms current state-of-the-art GAN and handcrafted methods in terms of the quality of normalized images. Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a test time augmentation method on the challenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel Whole Slide Images (WSIs) and demonstrate improved performance in terms of computational efficiency, image quality and consistency across tiles over current methods.","sentences":["Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images.","We propose a new approach, StainFuser, which treats this problem as a style transfer task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components.","With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural style transfer for high-quality transformations.","Trained on this data, StainFuser outperforms current state-of-the-art GAN and handcrafted methods in terms of the quality of normalized images.","Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a test time augmentation method on the challenging CoNIC dataset.","Finally, we apply StainFuser on multi-gigapixel Whole Slide Images (WSIs) and demonstrate improved performance in terms of computational efficiency, image quality and consistency across tiles over current methods."],"url":"http://arxiv.org/abs/2403.09302v1","category":"eess.IV"}
{"created":"2024-03-14 09:32:25","title":"Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning","abstract":"Social media platforms have become one of the main channels where people disseminate and acquire information, of which the reliability is severely threatened by rumors widespread in the network. Existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users. In this paper, we introduce a novel rumor mitigation paradigm, where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors, countering misinformation with low business cost and user awareness. A knowledge-informed agent embodying rumor propagation mechanisms is developed, which intervenes the social network with a graph neural network for capturing information flow in the social media platforms and a policy network for selecting links. Experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors, substantially reducing the affected populations by over 25%. Codes for this paper are released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation.","sentences":["Social media platforms have become one of the main channels where people disseminate and acquire information, of which the reliability is severely threatened by rumors widespread in the network.","Existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users.","In this paper, we introduce a novel rumor mitigation paradigm, where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors, countering misinformation with low business cost and user awareness.","A knowledge-informed agent embodying rumor propagation mechanisms is developed, which intervenes the social network with a graph neural network for capturing information flow in the social media platforms and a policy network for selecting links.","Experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors, substantially reducing the affected populations by over 25%.","Codes for this paper are released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation."],"url":"http://arxiv.org/abs/2403.09217v1","category":"cs.SI"}
{"created":"2024-03-14 09:15:16","title":"Some congruences of calibers of real quadratic fields","abstract":"In this paper, the congruence equations for caliber and m-caliber in various discriminants are proven. Additionally, We also obtained the lengths of the periods of several continued fractions as corollaries.","sentences":["In this paper, the congruence equations for caliber and m-caliber in various discriminants are proven.","Additionally, We also obtained the lengths of the periods of several continued fractions as corollaries."],"url":"http://arxiv.org/abs/2403.09202v1","category":"math.NT"}
{"created":"2024-03-14 09:14:12","title":"Rectified deep neural networks overcome the curse of dimensionality in the numerical approximation of gradient-dependent semilinear heat equations","abstract":"Numerical experiments indicate that deep learning algorithms overcome the curse of dimensionality when approximating solutions of semilinear PDEs. For certain linear PDEs and semilinear PDEs with gradient-independent nonlinearities this has also been proved mathematically, i.e., it has been shown that the number of parameters of the approximating DNN increases at most polynomially in both the PDE dimension $d\\in \\mathbb{N}$ and the reciprocal of the prescribed accuracy $\\epsilon\\in (0,1)$. The main contribution of this paper is to rigorously prove for the first time that deep neural networks can also overcome the curse dimensionality in the approximation of a certain class of nonlinear PDEs with gradient-dependent nonlinearities.","sentences":["Numerical experiments indicate that deep learning algorithms overcome the curse of dimensionality when approximating solutions of semilinear PDEs.","For certain linear PDEs and semilinear PDEs with gradient-independent nonlinearities this has also been proved mathematically, i.e., it has been shown that the number of parameters of the approximating DNN increases at most polynomially in both the PDE dimension $d\\in \\mathbb{N}$ and the reciprocal of the prescribed accuracy $\\epsilon\\in (0,1)$.","The main contribution of this paper is to rigorously prove for the first time that deep neural networks can also overcome the curse dimensionality in the approximation of a certain class of nonlinear PDEs with gradient-dependent nonlinearities."],"url":"http://arxiv.org/abs/2403.09200v1","category":"math.NA"}
{"created":"2024-03-14 06:07:59","title":"On the Miyaoka-Yau inequality for manifolds with nef anti-canonical line bundle","abstract":"Based on the recent work of K.~Zhang, we discuss the Miyaoka-Yau type inequality for projective manifolds with nef anti-canonical line bundle, assuming the lower bound of the delta-invariant introduced by Fujita and Odaka.","sentences":["Based on the recent work of K.~Zhang, we discuss the Miyaoka-Yau type inequality for projective manifolds with nef anti-canonical line bundle, assuming the lower bound of the delta-invariant introduced by Fujita and Odaka."],"url":"http://arxiv.org/abs/2403.09120v1","category":"math.DG"}
{"created":"2024-03-14 05:00:20","title":"Effects of neck and nuclear orientations on the mass drift in heavy ion collisions","abstract":"We clarified that the fusion hindrance in heavy ion collisions is caused by the expansion of the neck bridge at the early stage of collision [Phys. Rev. C 108, 014612 (2023)]; however, our discussion was limited to the trajectory analysis. To get a reliable fusion cross section, it is important to understand the fusion process connecting with multinucleon transfer and also the process depending on the target orientation in detail. Especially, the effects of target orientation on the multinucleon transfer process have not been discussed so far in our model. First, we investigate precisely the start time of the neck expansion relevant to the mass transfer. The main aims of this paper are to discuss the mass drift in the collision with the different target orientations within the dynamical approach in the reaction 32S + 232Th. The orientation effects are incorporated within the framework of the Langevin equation. The start time of the neck expansion was presumed to be 10 zs by analysis in several entrance channels. By taking account of the target nuclear orientation, a strong mass-angle correlation was obtained which is compatible with the experimental data. Not only ``delayed relaxation'' of the neck but also the nuclear orientation effects have an important role in the strong correlation between fragment mass and its emitting angle. The mass evolution toward mass symmetry is slower than the standard mass drift mode assuming an exponential-type function. Particularly, the mass drift of the tip collision follows the slow mass drift mode assuming a Fermi-type function rather than an exponential-type function, which is related to the different features of the maximum neck cross-sectional area in the sticking process.","sentences":["We clarified that the fusion hindrance in heavy ion collisions is caused by the expansion of the neck bridge at the early stage of collision","[Phys. Rev. C 108, 014612 (2023)]; however, our discussion was limited to the trajectory analysis.","To get a reliable fusion cross section, it is important to understand the fusion process connecting with multinucleon transfer and also the process depending on the target orientation in detail.","Especially, the effects of target orientation on the multinucleon transfer process have not been discussed so far in our model.","First, we investigate precisely the start time of the neck expansion relevant to the mass transfer.","The main aims of this paper are to discuss the mass drift in the collision with the different target orientations within the dynamical approach in the reaction 32S + 232Th.","The orientation effects are incorporated within the framework of the Langevin equation.","The start time of the neck expansion was presumed to be 10 zs by analysis in several entrance channels.","By taking account of the target nuclear orientation, a strong mass-angle correlation was obtained which is compatible with the experimental data.","Not only ``delayed relaxation'' of the neck but also the nuclear orientation effects have an important role in the strong correlation between fragment mass and its emitting angle.","The mass evolution toward mass symmetry is slower than the standard mass drift mode assuming an exponential-type function.","Particularly, the mass drift of the tip collision follows the slow mass drift mode assuming a Fermi-type function rather than an exponential-type function, which is related to the different features of the maximum neck cross-sectional area in the sticking process."],"url":"http://arxiv.org/abs/2403.09106v1","category":"nucl-th"}
{"created":"2024-03-14 04:56:13","title":"Effect of torsion in long-baseline neutrino oscillation experiments","abstract":"In this work we investigate the effect of curved spacetime on neutrino oscillation. In a curved spacetime, the effect of curvature on fermionic fields is represented by spin connection. The spin connection consists of a non-universal ``contorsion\" part which is expressed in terms of vector and axial current density of fermions. The contraction of contorsion part with the tetrad fields, which connects the internal flat space metric and the spacetime metric, is called torsion. In a scenario where neutrino travels through background of fermionic matter at ordinary densities in a curved spacetime, the Hamiltonian of neutrino oscillation gets modified by the torsional coupling constants $\\lambda_{21}^{\\prime}$ and $\\lambda_{31}^{\\prime}$. The aim of this work is to study the effect of $\\lambda_{21}^{\\prime}$ and $\\lambda_{31}^{\\prime}$ in DUNE and P2SO. In our study we, (i) discuss the effect of torsional coupling constants on the neutrino oscillation probabilities, (ii) estimate the capability of P2SO and DUNE to put bounds on these parameters and (iii) study how the physics sensitivities get modified in presence of torsion.","sentences":["In this work we investigate the effect of curved spacetime on neutrino oscillation.","In a curved spacetime, the effect of curvature on fermionic fields is represented by spin connection.","The spin connection consists of a non-universal ``contorsion\" part which is expressed in terms of vector and axial current density of fermions.","The contraction of contorsion part with the tetrad fields, which connects the internal flat space metric and the spacetime metric, is called torsion.","In a scenario where neutrino travels through background of fermionic matter at ordinary densities in a curved spacetime, the Hamiltonian of neutrino oscillation gets modified by the torsional coupling constants $\\lambda_{21}^{\\prime}$ and $\\lambda_{31}^{\\prime}$. The aim of this work is to study the effect of $\\lambda_{21}^{\\prime}$ and $\\lambda_{31}^{\\prime}$ in DUNE and P2SO.","In our study we, (i) discuss the effect of torsional coupling constants on the neutrino oscillation probabilities, (ii) estimate the capability of P2SO and DUNE to put bounds on these parameters and (iii) study how the physics sensitivities get modified in presence of torsion."],"url":"http://arxiv.org/abs/2403.09105v1","category":"hep-ph"}
{"created":"2024-03-14 04:48:31","title":"Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement","abstract":"Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks. However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirical results demonstrate that our method can simultaneously boost the standard accuracy and robust performance across multiple benchmark datasets, attack types, and architectures. In addition, we also provide a set of analyses from the perspectives of information theory to dive into our method and suggest the importance of soft labels for robust generalization.","sentences":["Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks.","However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves.","In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm.","As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT.","Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers.","Empirical results demonstrate that our method can simultaneously boost the standard accuracy and robust performance across multiple benchmark datasets, attack types, and architectures.","In addition, we also provide a set of analyses from the perspectives of information theory to dive into our method and suggest the importance of soft labels for robust generalization."],"url":"http://arxiv.org/abs/2403.09101v1","category":"cs.LG"}
{"created":"2024-03-14 04:39:14","title":"Exploring Hilbert-Space Fragmentation on a Superconducting Processor","abstract":"Isolated interacting quantum systems generally thermalize, yet there are several counterexamples for the breakdown of ergodicity, such as many-body localization and quantum scars. Recently, ergodicity breaking has been observed in systems subjected to linear potentials, termed Stark many-body localization. This phenomenon is closely associated with Hilbert-space fragmentation, characterized by a strong dependence of dynamics on initial conditions. Here, we experimentally explore initial-state dependent dynamics using a ladder-type superconducting processor with up to 24 qubits, which enables precise control of the qubit frequency and initial state preparation. In systems with linear potentials, we observe distinct non-equilibrium dynamics for initial states with the same quantum numbers and energy, but with varying domain wall numbers. This distinction becomes increasingly pronounced as the system size grows, in contrast with disordered interacting systems. Our results provide convincing experimental evidence of the fragmentation in Stark systems, enriching our understanding of the weak breakdown of ergodicity.","sentences":["Isolated interacting quantum systems generally thermalize, yet there are several counterexamples for the breakdown of ergodicity, such as many-body localization and quantum scars.","Recently, ergodicity breaking has been observed in systems subjected to linear potentials, termed Stark many-body localization.","This phenomenon is closely associated with Hilbert-space fragmentation, characterized by a strong dependence of dynamics on initial conditions.","Here, we experimentally explore initial-state dependent dynamics using a ladder-type superconducting processor with up to 24 qubits, which enables precise control of the qubit frequency and initial state preparation.","In systems with linear potentials, we observe distinct non-equilibrium dynamics for initial states with the same quantum numbers and energy, but with varying domain wall numbers.","This distinction becomes increasingly pronounced as the system size grows, in contrast with disordered interacting systems.","Our results provide convincing experimental evidence of the fragmentation in Stark systems, enriching our understanding of the weak breakdown of ergodicity."],"url":"http://arxiv.org/abs/2403.09095v1","category":"quant-ph"}
{"created":"2024-03-14 04:25:39","title":"Gravitational Wave Probe of Gravitational Dark Matter from Preheating","abstract":"Following our previous studies on gravitational dark matter (GDM) production in the early Universe of preheating,we forecast high-frequency gravitational wave (GW) spectra as the indirect probe of such GDM. We use proper lattice simulations to handle the resonance,and to solve the GW equation of motion with resonance induced scalar field excitations as the source term.Our numerical results show that the Higgs scalar excitations in the Higgs preheating model give rise to magnitudes of GW energy density spectra of order $10^{-10}$ at frequencies $10-10^{3}$ MHz, whereas the inflaton fluctuation excitations in the inflaton self-resonant preheating model yield magnitudes of GW energy density spectrum up to $10^{-9}~(10^{-11})$ at frequencies near $30~(2)$ MHz for the index $n=4~(6)$.","sentences":["Following our previous studies on gravitational dark matter (GDM) production in the early Universe of preheating,we forecast high-frequency gravitational wave (GW) spectra as the indirect probe of such GDM.","We use proper lattice simulations to handle the resonance,and to solve the GW equation of motion with resonance induced scalar field excitations as the source term.","Our numerical results show that the Higgs scalar excitations in the Higgs preheating model give rise to magnitudes of GW energy density spectra of order $10^{-10}$ at frequencies $10-10^{3}$ MHz, whereas the inflaton fluctuation excitations in the inflaton self-resonant preheating model yield magnitudes of GW energy density spectrum up to $10^{-9}~(10^{-11})$ at frequencies near $30~(2)$ MHz for the index $n=4~(6)$."],"url":"http://arxiv.org/abs/2403.09089v1","category":"hep-ph"}
{"created":"2024-03-14 04:10:13","title":"Temporal Signal Processing with Nonlocal Optical Metasurfaces","abstract":"Nonlocal metasurfaces have recently enabled an ultra-compact, low-power and high-speed platform to perform analog image processing. While several computational tasks have been demonstrated based on this platform, most of the previous studies have focused only on spatial operations, such as spatial differentiation and edge detection. Here, we demonstrate that metasurfaces with temporal nonlocalities - that is, with a tailored dispersive response - can be used to implement time-domain signal processing in deeply subwavelength footprints. In particular, we show a passive metasurface performing first-order differentiation of an input signal with high-fidelity and high-efficiency. We also demonstrate that this approach is prone to scalability and cascaded computation. Our work paves the way to a new generation of ultra-compact, passive devices for all-optical computation, with applications in neural networks and neuromorphic computing.","sentences":["Nonlocal metasurfaces have recently enabled an ultra-compact, low-power and high-speed platform to perform analog image processing.","While several computational tasks have been demonstrated based on this platform, most of the previous studies have focused only on spatial operations, such as spatial differentiation and edge detection.","Here, we demonstrate that metasurfaces with temporal nonlocalities - that is, with a tailored dispersive response - can be used to implement time-domain signal processing in deeply subwavelength footprints.","In particular, we show a passive metasurface performing first-order differentiation of an input signal with high-fidelity and high-efficiency.","We also demonstrate that this approach is prone to scalability and cascaded computation.","Our work paves the way to a new generation of ultra-compact, passive devices for all-optical computation, with applications in neural networks and neuromorphic computing."],"url":"http://arxiv.org/abs/2403.09087v1","category":"physics.optics"}
{"created":"2024-03-14 03:43:04","title":"Klein-Gordon theory in noncommutative phase space","abstract":"We extend the three-dimensional noncommutative relations of the positions and momenta operators to those in the four dimension. Using the Bopp shift technique, we give the Heisenberg representation of these noncommutative algebras and endow the noncommutative parameters associated with the Planck constant, Planck length and cosmological constant. As an analog with the electromagnetic gauge potential, the noncommutative effect can be interpreted as an effective gauge field, which depends on the Plank constant and cosmological constant. Based on these noncommutative relations, we give the Klein-Gordon (KG) equation and its corresponding current continuity equation in the noncommutative phase space including the canonical and Hamiltonian forms and their novel properties beyond the conventional KG equation. We analyze the symmetries of the KG equations and some observables such as velocity and force of free particles in the noncommutative phase space. We give the perturbation solution of the KG equation.","sentences":["We extend the three-dimensional noncommutative relations of the positions and momenta operators to those in the four dimension.","Using the Bopp shift technique, we give the Heisenberg representation of these noncommutative algebras and endow the noncommutative parameters associated with the Planck constant, Planck length and cosmological constant.","As an analog with the electromagnetic gauge potential, the noncommutative effect can be interpreted as an effective gauge field, which depends on the Plank constant and cosmological constant.","Based on these noncommutative relations, we give the Klein-Gordon (KG) equation and its corresponding current continuity equation in the noncommutative phase space including the canonical and Hamiltonian forms and their novel properties beyond the conventional KG equation.","We analyze the symmetries of the KG equations and some observables such as velocity and force of free particles in the noncommutative phase space.","We give the perturbation solution of the KG equation."],"url":"http://arxiv.org/abs/2403.09075v1","category":"hep-th"}
{"created":"2024-03-14 03:39:35","title":"Local first integrals for stochastic differential equations","abstract":"Poincar\\'{e}'s classical results [H. Poincar\\'{e}, Sur l'int\\'{e}gration des \\'{e}quations diff\\'{e}rentielles du premier order et du premier degr\\'{e} I and II, Rend. Circ. Mat. Palermo 5 (1891) 161-191; 11 (1897) 193-239] first provide a link between the existence of analytic first integrals and the resonant relations for analytic dynamical systems. In this paper, we show that by appropriately selecting the definition of the stochastic local first integrals, we are able to obtain the stochastic version of Poincar\\'e non-integrability theorem. More specifically, we introduce two definitions of local first integrals for stochastic differential equations (SDEs) in the sense of probability one and expectation, respectively. We present the necessary conditions for the existence of functionally independent analytic or rational first integrals of SDEs via the resonances. We also show that for given integrable ordinary differential equations with some nondegeneracy conditions, there exists a linear stochastic perturbation such that the corresponding disturbed SDEs have no any analytic first integrals. Some examples are given to illustrate our results.","sentences":["Poincar\\'{e}'s classical results [H. Poincar\\'{e}, Sur l'int\\'{e}gration des \\'{e}quations diff\\'{e}rentielles du premier order et du premier degr\\'{e} I and II, Rend.","Circ.","Mat. Palermo 5 (1891) 161-191; 11 (1897) 193-239] first provide a link between the existence of analytic first integrals and the resonant relations for analytic dynamical systems.","In this paper, we show that by appropriately selecting the definition of the stochastic local first integrals, we are able to obtain the stochastic version of Poincar\\'e non-integrability theorem.","More specifically, we introduce two definitions of local first integrals for stochastic differential equations (SDEs) in the sense of probability one and expectation, respectively.","We present the necessary conditions for the existence of functionally independent analytic or rational first integrals of SDEs via the resonances.","We also show that for given integrable ordinary differential equations with some nondegeneracy conditions, there exists a linear stochastic perturbation such that the corresponding disturbed SDEs have no any analytic first integrals.","Some examples are given to illustrate our results."],"url":"http://arxiv.org/abs/2403.09074v1","category":"math.DG"}
{"created":"2024-03-14 03:19:31","title":"Self-enhanced mobility enables vortex pattern formation in living matter","abstract":"Emergence of regular spatial patterns is a hallmark in living matter ranging from subcellular organelles to developing embryos and to ecosystems. Mechanisms for the formation of ordered spatial patterns in biology often require chemical signaling that coordinates cellular behavior and differentiation. Here we discovered a novel route to large-scale regular pattern formation in living matter mediated by purely physical interactions. We found that dense bacterial living matter spontaneously developed an ordered lattice of mesoscale, fast-spinning vortices each consisting of ~10^4-10^5 motile cells; these mesoscale vortices were arranged in space over centimeter scale with apparent hexagonal order, while individual cells in the vortices moved in coordinated directions with strong polar and vortical order. Single-cell tracking and numerical simulations suggest that the phenomenon is enabled by self-enhanced mobility of individual cells in the system. Our findings demonstrate a simple physical mechanism for self-organized pattern formation in living systems and more generally, in other active matter systems near the boundary of fluidlike and solidlike behaviors.","sentences":["Emergence of regular spatial patterns is a hallmark in living matter ranging from subcellular organelles to developing embryos and to ecosystems.","Mechanisms for the formation of ordered spatial patterns in biology often require chemical signaling that coordinates cellular behavior and differentiation.","Here we discovered a novel route to large-scale regular pattern formation in living matter mediated by purely physical interactions.","We found that dense bacterial living matter spontaneously developed an ordered lattice of mesoscale, fast-spinning vortices each consisting of ~10^4-10^5 motile cells; these mesoscale vortices were arranged in space over centimeter scale with apparent hexagonal order, while individual cells in the vortices moved in coordinated directions with strong polar and vortical order.","Single-cell tracking and numerical simulations suggest that the phenomenon is enabled by self-enhanced mobility of individual cells in the system.","Our findings demonstrate a simple physical mechanism for self-organized pattern formation in living systems and more generally, in other active matter systems near the boundary of fluidlike and solidlike behaviors."],"url":"http://arxiv.org/abs/2403.09068v1","category":"physics.bio-ph"}
{"created":"2024-03-14 03:10:16","title":"Exact theory of the finite-temperature spectral function of Fermi polarons with multiple particle-hole excitations: Diagrammatic theory versus Chevy ansatz","abstract":"By using both diagrammatic theory and Chevy ansatz approach, we derive an exact set of equations, which determines the spectral function of Fermi polarons with multiple particle-hole excitations at nonzero temperature. In the diagrammatic theory, we find out the complete series of Feynman diagrams for the multi-particle vertex functions, when the unregularized contact interaction strength becomes infinitesimal, a typical situation occurring in two- or three- dimensional free space. The latter Chevy ansatz approach is more widely applicable, allowing a nonzero interaction strength. We clarify the equivalence of the two approaches for an infinitesimal interaction strength and show that the variational coefficients in the Chevy ansatz are precisely the on-shell multi-particle vertex functions divided by an excitation energy. Truncated to a particular order of particle-hole excitations, our exact set of equations can be used to numerically calculate the finite-temperature polaron spectral function, once the numerical singularities in the equations are appropriately treated. As a concrete example, we calculate the finite-temperature spectral function of Fermi polarons in one-dimensional lattices, taking into account all the two-particle-hole excitations. We show that the inclusion of two-particle-hole excitations quantitatively improve the predictions on the polaron spectral function. Our results provide a useful way to solve the challenge problem of accurately predicting the finite-temperature spectral function of Fermi polarons in three-dimensional free space. In addition, our clarification of the complete set of Feynman diagrams for the multi-particle polaron vertex functions may inspire the development of more accurate diagrammatic theories of population-imbalanced strongly interacting Fermi gases, beyond the conventional many-body $T$-matrix approximation.","sentences":["By using both diagrammatic theory and Chevy ansatz approach, we derive an exact set of equations, which determines the spectral function of Fermi polarons with multiple particle-hole excitations at nonzero temperature.","In the diagrammatic theory, we find out the complete series of Feynman diagrams for the multi-particle vertex functions, when the unregularized contact interaction strength becomes infinitesimal, a typical situation occurring in two- or three- dimensional free space.","The latter Chevy ansatz approach is more widely applicable, allowing a nonzero interaction strength.","We clarify the equivalence of the two approaches for an infinitesimal interaction strength and show that the variational coefficients in the Chevy ansatz are precisely the on-shell multi-particle vertex functions divided by an excitation energy.","Truncated to a particular order of particle-hole excitations, our exact set of equations can be used to numerically calculate the finite-temperature polaron spectral function, once the numerical singularities in the equations are appropriately treated.","As a concrete example, we calculate the finite-temperature spectral function of Fermi polarons in one-dimensional lattices, taking into account all the two-particle-hole excitations.","We show that the inclusion of two-particle-hole excitations quantitatively improve the predictions on the polaron spectral function.","Our results provide a useful way to solve the challenge problem of accurately predicting the finite-temperature spectral function of Fermi polarons in three-dimensional free space.","In addition, our clarification of the complete set of Feynman diagrams for the multi-particle polaron vertex functions may inspire the development of more accurate diagrammatic theories of population-imbalanced strongly interacting Fermi gases, beyond the conventional many-body $T$-matrix approximation."],"url":"http://arxiv.org/abs/2403.09064v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-14 02:40:20","title":"Enhanced ClNO$_2$ formation at the interface of sea-salt aerosol","abstract":"The reactive uptake of $\\mathrm{N_2O_5}$ on sea-spray aerosol plays a key role in regulating NO$_\\mathrm{x}$ concentration in the troposphere. Despite numerous field and laboratory studies, a microscopic understanding of its heterogeneous reactivity remains unclear. Here, we use molecular simulation and theory to elucidate the chlorination of $\\mathrm{N_2O_5}$ to form ClNO$_2$, the primary reactive channel within sea-spray aerosol. We find the formation of ClNO$_2$ is markedly enhanced at the air-water interface due to the stabilization of the charge-delocalized transition state, as evident from the formulation of bimolecular rate theory in heterogeneous environments. We explore the consequences of the enhanced interfacial reactivity in the uptake of $\\mathrm{N_2O_5}$ using numerical solutions of molecular reaction-diffusion equations as well as their analytical approximations. Our results suggest that the current interpretation of aerosol branching ratios needs to be revisited.","sentences":["The reactive uptake of $\\mathrm{N_2O_5}$ on sea-spray aerosol plays a key role in regulating NO$_\\mathrm{x}$ concentration in the troposphere.","Despite numerous field and laboratory studies, a microscopic understanding of its heterogeneous reactivity remains unclear.","Here, we use molecular simulation and theory to elucidate the chlorination of $\\mathrm{N_2O_5}$ to form ClNO$_2$, the primary reactive channel within sea-spray aerosol.","We find the formation of ClNO$_2$ is markedly enhanced at the air-water interface due to the stabilization of the charge-delocalized transition state, as evident from the formulation of bimolecular rate theory in heterogeneous environments.","We explore the consequences of the enhanced interfacial reactivity in the uptake of $\\mathrm{N_2O_5}$ using numerical solutions of molecular reaction-diffusion equations as well as their analytical approximations.","Our results suggest that the current interpretation of aerosol branching ratios needs to be revisited."],"url":"http://arxiv.org/abs/2403.09052v1","category":"physics.chem-ph"}
{"created":"2024-03-14 02:11:38","title":"DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers","abstract":"Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources. Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy. In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers' diversity, adversarial selector-classifiers training to ensure synergistic interactions thereby maximizing their complementarity, and heterogeneous feature aggregation to improve the capacity of classifiers. We further propose a network slicing technique to alleviate the extra memory overhead incurred by feature aggregation. We deploy DiTMoS on the Neucleo STM32F767ZI board and evaluate it based on three time-series datasets for human activity recognition, keywords spotting, and emotion recognition, respectively. The experiment results manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement compared to the best baseline; (b) network slicing almost completely eliminates the memory overhead incurred by feature aggregation with a marginal increase of latency.","sentences":["Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources.","Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy.","In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy.","Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification.","DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound.","To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers' diversity, adversarial selector-classifiers training to ensure synergistic interactions thereby maximizing their complementarity, and heterogeneous feature aggregation to improve the capacity of classifiers.","We further propose a network slicing technique to alleviate the extra memory overhead incurred by feature aggregation.","We deploy DiTMoS on the Neucleo STM32F767ZI board and evaluate it based on three time-series datasets for human activity recognition, keywords spotting, and emotion recognition, respectively.","The experiment results manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement compared to the best baseline; (b) network slicing almost completely eliminates the memory overhead incurred by feature aggregation with a marginal increase of latency."],"url":"http://arxiv.org/abs/2403.09035v1","category":"cs.LG"}
{"created":"2024-03-13 23:55:08","title":"Condensate-Induced Inflation from Primordial Gravitational Waves in String-Inspired Chern-Simons Gravity","abstract":"In this work, we elaborate further on a cosmological model of inflation that characterises Chern-Simons (CS) gravity models inspired from string theory. Such models are known to belong to the class of the so-called String-Inspired Running Vacuum Cosmologies. In particular, by applying methods of dynamical systems, commonly used in scalar-field cosmology, we examine in detail, for the first time, the passage from a pre-inflationary era dominated by a stiff-axion-matter equation of state, characteristic of the model, to inflation of Running Vacuum Model (RVM) type. By a careful discussion of the formation of the condensate of the CS gravitational anomaly term, induced by populations of primordial gravitational waves at the end of the stiff-axion-matter era, we show that an effectively linear axion-monodromy potential arises. This eventually causes the transition from the matter to the RVM inflation. By taking into account terms that have previously been ignored in the relevant literature of weak-graviton quantisation, we show that the effect of such terms is to diminish the value of the condensate by half, remaining however in the same order of magnitude. This, in turn, implies that the qualitative conclusions of previous works on the subject remain valid. Moreover, on assuming the approximate cosmic-time independence of the gravitational-CS condensate, we also provide an estimate of the number of sources of the primordial gravitational waves, upon the requirement of respecting the transplanckian conjecture.","sentences":["In this work, we elaborate further on a cosmological model of inflation that characterises Chern-Simons (CS) gravity models inspired from string theory.","Such models are known to belong to the class of the so-called String-Inspired Running Vacuum Cosmologies.","In particular, by applying methods of dynamical systems, commonly used in scalar-field cosmology, we examine in detail, for the first time, the passage from a pre-inflationary era dominated by a stiff-axion-matter equation of state, characteristic of the model, to inflation of Running Vacuum Model (RVM) type.","By a careful discussion of the formation of the condensate of the CS gravitational anomaly term, induced by populations of primordial gravitational waves at the end of the stiff-axion-matter era, we show that an effectively linear axion-monodromy potential arises.","This eventually causes the transition from the matter to the RVM inflation.","By taking into account terms that have previously been ignored in the relevant literature of weak-graviton quantisation, we show that the effect of such terms is to diminish the value of the condensate by half, remaining however in the same order of magnitude.","This, in turn, implies that the qualitative conclusions of previous works on the subject remain valid.","Moreover, on assuming the approximate cosmic-time independence of the gravitational-CS condensate, we also provide an estimate of the number of sources of the primordial gravitational waves, upon the requirement of respecting the transplanckian conjecture."],"url":"http://arxiv.org/abs/2403.09005v1","category":"gr-qc"}
{"created":"2024-03-13 23:50:32","title":"Meta-Learning-Based Fronthaul Compression for Cloud Radio Access Networks","abstract":"This paper investigates the fronthaul compression problem in a user-centric cloud radio access network, in which single-antenna users are served by a central processor (CP) cooperatively via a cluster of remote radio heads (RRHs). To satisfy the fronthaul capacity constraint, this paper proposes a transform-compress-forward scheme, which consists of well-designed transformation matrices and uniform quantizers. The transformation matrices perform dimension reduction in the uplink and dimension expansion in the downlink. To reduce the communication overhead for designing the transformation matrices, this paper further proposes a deep learning framework to first learn a suboptimal transformation matrix at each RRH based on the local channel state information (CSI), and then to refine it iteratively. To facilitate the refinement process, we propose an efficient signaling scheme that only requires the transmission of low-dimensional effective CSI and its gradient between the CP and RRH, and further, a meta-learning based gated recurrent unit network to reduce the number of signaling transmission rounds. For the sum-rate maximization problem, simulation results show that the proposed two-stage neural network can perform close to the fully cooperative global CSI based benchmark with significantly reduced communication overhead for both the uplink and the downlink. Moreover, using the first stage alone can already outperform the existing local CSI based benchmark.","sentences":["This paper investigates the fronthaul compression problem in a user-centric cloud radio access network, in which single-antenna users are served by a central processor (CP) cooperatively via a cluster of remote radio heads (RRHs).","To satisfy the fronthaul capacity constraint, this paper proposes a transform-compress-forward scheme, which consists of well-designed transformation matrices and uniform quantizers.","The transformation matrices perform dimension reduction in the uplink and dimension expansion in the downlink.","To reduce the communication overhead for designing the transformation matrices, this paper further proposes a deep learning framework to first learn a suboptimal transformation matrix at each RRH based on the local channel state information (CSI), and then to refine it iteratively.","To facilitate the refinement process, we propose an efficient signaling scheme that only requires the transmission of low-dimensional effective CSI and its gradient between the CP and RRH, and further, a meta-learning based gated recurrent unit network to reduce the number of signaling transmission rounds.","For the sum-rate maximization problem, simulation results show that the proposed two-stage neural network can perform close to the fully cooperative global CSI based benchmark with significantly reduced communication overhead for both the uplink and the downlink.","Moreover, using the first stage alone can already outperform the existing local CSI based benchmark."],"url":"http://arxiv.org/abs/2403.09004v1","category":"cs.IT"}
{"created":"2024-03-13 23:39:18","title":"Solving Partial Differential Equations Using Artificial Neural Networks","abstract":"Partial differential equations have a wide range of applications in modeling multiple physical, biological, or social phenomena. Therefore, we need to approximate the solutions of these equations in computationally feasible terms. Nowadays, among the most popular numerical methods for solving partial differential equations in engineering, we encounter the finite difference and finite element methods. An alternative numerical method that has recently gained popularity for numerically solving partial differential equations is the use of artificial neural networks.   Artificial neural networks, or neural networks for short, are mathematical structures with universal approximation properties. In addition, thanks to the extraordinary computational development of the last decade, neural networks have become accessible and powerful numerical methods for engineers and researchers. For example, imaging and language processing are applications of neural networks today that show sublime performance inconceivable years ago.   This dissertation contributes to the numerical solution of partial differential equations using neural networks with the following two-fold objective: investigate the behavior of neural networks as approximators of solutions of partial differential equations and propose neural-network-based methods for frameworks that are hardly addressable via traditional numerical methods.   As novel neural-network-based proposals, we first present a method inspired by the finite element method when applying mesh refinements to solve parametric problems. Secondly, we propose a general residual minimization scheme based on a generalized version of the Ritz method. Finally, we develop a memory-based strategy to overcome a usual numerical integration limitation when using neural networks to solve partial differential equations.","sentences":["Partial differential equations have a wide range of applications in modeling multiple physical, biological, or social phenomena.","Therefore, we need to approximate the solutions of these equations in computationally feasible terms.","Nowadays, among the most popular numerical methods for solving partial differential equations in engineering, we encounter the finite difference and finite element methods.","An alternative numerical method that has recently gained popularity for numerically solving partial differential equations is the use of artificial neural networks.   ","Artificial neural networks, or neural networks for short, are mathematical structures with universal approximation properties.","In addition, thanks to the extraordinary computational development of the last decade, neural networks have become accessible and powerful numerical methods for engineers and researchers.","For example, imaging and language processing are applications of neural networks today that show sublime performance inconceivable years ago.   ","This dissertation contributes to the numerical solution of partial differential equations using neural networks with the following two-fold objective: investigate the behavior of neural networks as approximators of solutions of partial differential equations and propose neural-network-based methods for frameworks that are hardly addressable via traditional numerical methods.   ","As novel neural-network-based proposals, we first present a method inspired by the finite element method when applying mesh refinements to solve parametric problems.","Secondly, we propose a general residual minimization scheme based on a generalized version of the Ritz method.","Finally, we develop a memory-based strategy to overcome a usual numerical integration limitation when using neural networks to solve partial differential equations."],"url":"http://arxiv.org/abs/2403.09001v1","category":"math.NA"}
{"created":"2024-03-13 22:12:43","title":"Observation and differential cross section measurement of neutral current DIS events with an empty hemisphere in the Breit frame","abstract":"The Breit frame provides a natural frame to analyze lepton-proton scattering events. In this reference frame, the parton model hard interactions between a quark and an exchanged boson defines the coordinate system such that the struck quark is back-scattered along the virtual photon momentum direction. In Quantum Chromodynamics (QCD), higher order perturbative or non-perturbative effects can change this picture drastically. As Bjorken-$x$ decreases below one half, a rather peculiar event signature is predicted with increasing probability, where no radiation is present in one of the two Breit-frame hemispheres and all emissions are to be found in the other hemisphere. At higher orders in $\\alpha_s$ or in the presence of soft QCD effects, predictions of the rate of these events are far from trivial, and that motivates measurements with real data. We report on the first observation of the empty current hemisphere events in electron-proton collisions at the HERA collider using data recorded with the H1 detector at a center-of-mass energy of 319 GeV. The fraction of inclusive neutral-current DIS events with an empty hemisphere is found to be $0.0112 \\pm 3.9\\,\\%_\\text{stat} \\pm 4.5\\,\\%_\\text{syst} \\pm 1.6\\,\\%_\\text{mod}$ in the selected kinematic region of $150< Q^2<1500$ GeV$^2$ and inelasticity $0.14< y<0.7$. The data sample corresponds to an integrated luminosity of 351.1 pb$^{-1}$, sufficient to enable differential cross section measurements of these events. The results show an enhanced discriminating power at lower Bjorken-$x$ among different Monte Carlo event generator predictions.","sentences":["The Breit frame provides a natural frame to analyze lepton-proton scattering events.","In this reference frame, the parton model hard interactions between a quark and an exchanged boson defines the coordinate system such that the struck quark is back-scattered along the virtual photon momentum direction.","In Quantum Chromodynamics (QCD), higher order perturbative or non-perturbative effects can change this picture drastically.","As Bjorken-$x$ decreases below one half, a rather peculiar event signature is predicted with increasing probability, where no radiation is present in one of the two Breit-frame hemispheres and all emissions are to be found in the other hemisphere.","At higher orders in $\\alpha_s$ or in the presence of soft QCD effects, predictions of the rate of these events are far from trivial, and that motivates measurements with real data.","We report on the first observation of the empty current hemisphere events in electron-proton collisions at the HERA collider using data recorded with the H1 detector at a center-of-mass energy of 319 GeV.","The fraction of inclusive neutral-current DIS events with an empty hemisphere is found to be $0.0112 \\pm 3.9\\,\\%_\\text{stat} \\pm 4.5\\,\\%_\\text{syst} \\pm 1.6\\,\\%_\\text{mod}$ in the selected kinematic region of $150< Q^2<1500$ GeV$^2$ and inelasticity $0.14< y<0.7$.","The data sample corresponds to an integrated luminosity of 351.1 pb$^{-1}$, sufficient to enable differential cross section measurements of these events.","The results show an enhanced discriminating power at lower Bjorken-$x$ among different Monte Carlo event generator predictions."],"url":"http://arxiv.org/abs/2403.08982v1","category":"hep-ex"}
{"created":"2024-03-13 22:11:44","title":"A Game Theoretic Approach to Sustainizability Over Sets and its application to a multi-species population model","abstract":"In this work, a theorem is first proved which presents a game theoretic formulation of a necessary and sufficient sustainizability over a set condition for a general system described by ordinary differential equations (ODEs). Then, two additional theorems are proved for the n-species Gause-Lotka-Volterra (GLV) population model, establishing necessary and sufficient sustainability and sustainizability conditions over rectangular sets. Three case studies on the May-Leonard, 3-species, GLV model are then presented to illustrate the power of the above Theorems. In two of these case studies (2 and 3), it is shown that a particular instance of the 3-species, GLV model is unsustainable but sustainizable through allowable action.","sentences":["In this work, a theorem is first proved which presents a game theoretic formulation of a necessary and sufficient sustainizability over a set condition for a general system described by ordinary differential equations (ODEs).","Then, two additional theorems are proved for the n-species Gause-Lotka-Volterra (GLV) population model, establishing necessary and sufficient sustainability and sustainizability conditions over rectangular sets.","Three case studies on the May-Leonard, 3-species, GLV model are then presented to illustrate the power of the above Theorems.","In two of these case studies (2 and 3), it is shown that a particular instance of the 3-species, GLV model is unsustainable but sustainizable through allowable action."],"url":"http://arxiv.org/abs/2403.08981v1","category":"math.OC"}
{"created":"2024-03-13 22:10:42","title":"Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications","abstract":"With more scientific fields relying on neural networks (NNs) to process data incoming at extreme throughputs and latencies, it is crucial to develop NNs with all their parameters stored on-chip. In many of these applications, there is not enough time to go off-chip and retrieve weights. Even more so, off-chip memory such as DRAM does not have the bandwidth required to process these NNs as fast as the data is being produced (e.g., every 25 ns). As such, these extreme latency and bandwidth requirements have architectural implications for the hardware intended to run these NNs: 1) all NN parameters must fit on-chip, and 2) codesigning custom/reconfigurable logic is often required to meet these latency and bandwidth constraints. In our work, we show that many scientific NN applications must run fully on chip, in the extreme case requiring a custom chip to meet such stringent constraints.","sentences":["With more scientific fields relying on neural networks (NNs) to process data incoming at extreme throughputs and latencies, it is crucial to develop NNs with all their parameters stored on-chip.","In many of these applications, there is not enough time to go off-chip and retrieve weights.","Even more so, off-chip memory such as DRAM does not have the bandwidth required to process these NNs as fast as the data is being produced (e.g., every 25 ns).","As such, these extreme latency and bandwidth requirements have architectural implications for the hardware intended to run these NNs: 1) all NN parameters must fit on-chip, and 2) codesigning custom/reconfigurable logic is often required to meet these latency and bandwidth constraints.","In our work, we show that many scientific NN applications must run fully on chip, in the extreme case requiring a custom chip to meet such stringent constraints."],"url":"http://arxiv.org/abs/2403.08980v1","category":"cs.LG"}
{"created":"2024-03-13 21:34:22","title":"A comparative analysis of transient finite-strain coupled diffusion-deformation theories for hydrogels","abstract":"This work presents a comparative review and classification between some well-known thermodynamically consistent models of hydrogel behavior in a large deformation setting, specifically focusing on solvent absorption/desorption and its impact on mechanical deformation and network swelling. The proposed discussion addresses formulation aspects, general mathematical classification of the governing equations, and numerical implementation issues based on the finite element method. The theories are presented in a unified framework demonstrating that, despite not being evident in some cases, all of them follow equivalent thermodynamic arguments. A detailed numerical analysis is carried out where Taylor-Hood elements are employed in the spatial discretization to satisfy the inf-sup condition and to prevent spurious numerical oscillations. The resulting discrete problems are solved using the FEniCS platform through consistent variational formulations, employing both monolithic and staggered approaches. We conduct benchmark tests on various hydrogel structures, demonstrating that major differences arise from the chosen volumetric response of the hydrogel. The significance of this choice is frequently underestimated in the state-of-the-art literature but has been shown to have substantial implications on the resulting hydrogel behavior.","sentences":["This work presents a comparative review and classification between some well-known thermodynamically consistent models of hydrogel behavior in a large deformation setting, specifically focusing on solvent absorption/desorption and its impact on mechanical deformation and network swelling.","The proposed discussion addresses formulation aspects, general mathematical classification of the governing equations, and numerical implementation issues based on the finite element method.","The theories are presented in a unified framework demonstrating that, despite not being evident in some cases, all of them follow equivalent thermodynamic arguments.","A detailed numerical analysis is carried out where Taylor-Hood elements are employed in the spatial discretization to satisfy the inf-sup condition and to prevent spurious numerical oscillations.","The resulting discrete problems are solved using the FEniCS platform through consistent variational formulations, employing both monolithic and staggered approaches.","We conduct benchmark tests on various hydrogel structures, demonstrating that major differences arise from the chosen volumetric response of the hydrogel.","The significance of this choice is frequently underestimated in the state-of-the-art literature but has been shown to have substantial implications on the resulting hydrogel behavior."],"url":"http://arxiv.org/abs/2403.08972v1","category":"cs.CE"}
{"created":"2024-03-13 21:21:53","title":"Model order reduction for transient coupled diffusion-deformation of hydrogels","abstract":"This study introduces a reduced-order model (ROM) for analyzing the transient diffusion-deformation of hydrogels. The full-order model (FOM) describing hydrogel transient behavior consists of a coupled system of partial differential equations in which chemical potential and displacements are coupled. This system is formulated in a monolithic fashion and solved using the Finite Element Method (FEM). The ROM employs proper orthogonal decomposition as a model order reduction approach. We test the ROM performance through benchmark tests on hydrogel swelling behavior and a case study simulating co-axial printing. Finally, we embed the ROM into an optimization problem to identify the model material parameters of the coupled problem using full-field data. We verify that the ROM can predict hydrogels' diffusion-deformation evolution and material properties, significantly reducing computation time compared to the FOM. The results demonstrate the ROM's accuracy and computational efficiency. This work paths the way towards advanced practical applications of ROMs, e.g., in the context of feedback error control in hydrogel 3D printing.","sentences":["This study introduces a reduced-order model (ROM) for analyzing the transient diffusion-deformation of hydrogels.","The full-order model (FOM) describing hydrogel transient behavior consists of a coupled system of partial differential equations in which chemical potential and displacements are coupled.","This system is formulated in a monolithic fashion and solved using the Finite Element Method (FEM).","The ROM employs proper orthogonal decomposition as a model order reduction approach.","We test the ROM performance through benchmark tests on hydrogel swelling behavior and a case study simulating co-axial printing.","Finally, we embed the ROM into an optimization problem to identify the model material parameters of the coupled problem using full-field data.","We verify that the ROM can predict hydrogels' diffusion-deformation evolution and material properties, significantly reducing computation time compared to the FOM.","The results demonstrate the ROM's accuracy and computational efficiency.","This work paths the way towards advanced practical applications of ROMs, e.g., in the context of feedback error control in hydrogel 3D printing."],"url":"http://arxiv.org/abs/2403.08968v1","category":"cs.CE"}
{"created":"2024-03-13 21:11:58","title":"Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory","abstract":"The study of the Two-Body and Circular Restricted Three-Body Problems in the field of aerospace engineering and sciences is deeply important because they help describe the motion of both celestial and artificial satellites. With the growing demand for satellites and satellite formation flying, fast and efficient control of these systems is becoming ever more important. Global linearization of these systems allows engineers to employ methods of control in order to achieve these desired results. We propose a data-driven framework for simultaneous system identification and global linearization of both the Two-Body Problem and Circular Restricted Three-Body Problem via deep learning-based Koopman Theory, i.e., a framework that can identify the underlying dynamics and globally linearize it into a linear time-invariant (LTI) system. The linear Koopman operator is discovered through purely data-driven training of a Deep Neural Network with a custom architecture. This paper displays the ability of the Koopman operator to generalize to various other Two-Body systems without the need for retraining. We also demonstrate the capability of the same architecture to be utilized to accurately learn a Koopman operator that approximates the Circular Restricted Three-Body Problem.","sentences":["The study of the Two-Body and Circular Restricted Three-Body Problems in the field of aerospace engineering and sciences is deeply important because they help describe the motion of both celestial and artificial satellites.","With the growing demand for satellites and satellite formation flying, fast and efficient control of these systems is becoming ever more important.","Global linearization of these systems allows engineers to employ methods of control in order to achieve these desired results.","We propose a data-driven framework for simultaneous system identification and global linearization of both the Two-Body Problem and Circular Restricted Three-Body Problem via deep learning-based Koopman Theory, i.e., a framework that can identify the underlying dynamics and globally linearize it into a linear time-invariant (LTI) system.","The linear Koopman operator is discovered through purely data-driven training of a Deep Neural Network with a custom architecture.","This paper displays the ability of the Koopman operator to generalize to various other Two-Body systems without the need for retraining.","We also demonstrate the capability of the same architecture to be utilized to accurately learn a Koopman operator that approximates the Circular Restricted Three-Body Problem."],"url":"http://arxiv.org/abs/2403.08965v1","category":"math-ph"}
{"created":"2024-03-13 21:04:03","title":"Gromoll--Meyer's actions and the geometry of (exotic) spacetimes","abstract":"Since the advent of new pairwise non-diffeomorphic structures on smooth manifolds, it has been questioned whether two topologically identical manifolds could admit different geometries. Not surprisingly, physicists have wondered whether a smooth structure assumption different from some classical known models could produce different physical meanings. In this paper, we inaugurate a very computational manner to produce physical models on classical and exotic spheres that can be built equivariantly, such as the classical Gromoll--Meyer exotic spheres. As first applications, we produce Lorentzian metrics on homeomorphic but not diffeomorphic manifolds that enjoy the same physical properties, such as geodesic completeness, positive Ricci curvature, and compatible time orientation. These constructions can be pulled back to higher models, such as exotic ten spheres bounding spin manifolds, to be approached in forthcoming papers.","sentences":["Since the advent of new pairwise non-diffeomorphic structures on smooth manifolds, it has been questioned whether two topologically identical manifolds could admit different geometries.","Not surprisingly, physicists have wondered whether a smooth structure assumption different from some classical known models could produce different physical meanings.","In this paper, we inaugurate a very computational manner to produce physical models on classical and exotic spheres that can be built equivariantly, such as the classical Gromoll--Meyer exotic spheres.","As first applications, we produce Lorentzian metrics on homeomorphic but not diffeomorphic manifolds that enjoy the same physical properties, such as geodesic completeness, positive Ricci curvature, and compatible time orientation.","These constructions can be pulled back to higher models, such as exotic ten spheres bounding spin manifolds, to be approached in forthcoming papers."],"url":"http://arxiv.org/abs/2403.08960v1","category":"math.DG"}
{"created":"2024-03-13 20:28:40","title":"Studying time-like proton form factors using vortex state $p\\bar{p}$ annihilation","abstract":"Vortex states of particles - non-plane-wave solutions of the corresponding wave equation with a helicoidal wave front - open new opportunities for particle physics, unavailable in plane wave scattering. Here we demonstrate that $p\\bar p$ annihilation with a vortex proton and antiproton provides access to the phase of proton electromagnetic form factors even in unpolarized scattering.","sentences":["Vortex states of particles - non-plane-wave solutions of the corresponding wave equation with a helicoidal wave front - open new opportunities for particle physics, unavailable in plane wave scattering.","Here we demonstrate that $p\\bar p$ annihilation with a vortex proton and antiproton provides access to the phase of proton electromagnetic form factors even in unpolarized scattering."],"url":"http://arxiv.org/abs/2403.08949v1","category":"hep-ph"}
{"created":"2024-03-13 20:16:46","title":"Collision-Free Platooning of Mobile Robots through a Set-Theoretic Predictive Control Approach","abstract":"This paper proposes a control solution to achieve collision-free platooning control of input-constrained mobile robots. The platooning policy is based on a leader-follower approach where the leader tracks a reference trajectory while followers track the leader's pose with an inter-agent delay. First, the leader and the follower kinematic models are feedback linearized and the platoon's error dynamics and input constraints characterized. Then, a set-theoretic model predictive control strategy is proposed to address the platooning trajectory tracking control problem. An ad-hoc collision avoidance policy is also proposed to guarantee collision avoidance amongst the agents. Finally, the effectiveness of the proposed control architecture is validated through experiments performed on a formation of Khepera IV differential drive robots","sentences":["This paper proposes a control solution to achieve collision-free platooning control of input-constrained mobile robots.","The platooning policy is based on a leader-follower approach where the leader tracks a reference trajectory while followers track the leader's pose with an inter-agent delay.","First, the leader and the follower kinematic models are feedback linearized and the platoon's error dynamics and input constraints characterized.","Then, a set-theoretic model predictive control strategy is proposed to address the platooning trajectory tracking control problem.","An ad-hoc collision avoidance policy is also proposed to guarantee collision avoidance amongst the agents.","Finally, the effectiveness of the proposed control architecture is validated through experiments performed on a formation of Khepera IV differential drive robots"],"url":"http://arxiv.org/abs/2403.08942v1","category":"cs.RO"}
{"created":"2024-03-13 19:19:19","title":"Efficiently Computing Similarities to Private Datasets","abstract":"Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \\subset \\mathbb{R}^d$, output a differentially private (DP) data structure which approximates $\\sum_{x \\in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\\|x-y\\|_2^2/\\sigma^2}$ (also known as DP kernel density estimation), or a distance function such as $f(x,y) = \\|x-y\\|_2$, among others.   Our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. The unifying approach behind our results is leveraging `low-dimensional structures' present in the specific functions $f$ that we study, using tools such as provable dimensionality reduction, approximation theory, and one-dimensional decomposition of the functions. Our algorithms empirically exhibit improved query times and accuracy over prior state of the art. We also present an application to DP classification. Our experiments demonstrate that the simple methodology of classifying based on average similarity is orders of magnitude faster than prior DP-SGD based approaches for comparable accuracy.","sentences":["Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data.","We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \\subset \\mathbb{R}^d$, output a differentially private (DP) data structure which approximates $\\sum_{x \\in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\\|x-y\\|_2^2/\\sigma^2}$ (also known as DP kernel density estimation), or a distance function such as $f(x,y) = \\|x-y\\|_2$, among others.   ","Our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions.","The unifying approach behind our results is leveraging `low-dimensional structures' present in the specific functions $f$ that we study, using tools such as provable dimensionality reduction, approximation theory, and one-dimensional decomposition of the functions.","Our algorithms empirically exhibit improved query times and accuracy over prior state of the art.","We also present an application to DP classification.","Our experiments demonstrate that the simple methodology of classifying based on average similarity is orders of magnitude faster than prior DP-SGD based approaches for comparable accuracy."],"url":"http://arxiv.org/abs/2403.08917v1","category":"cs.CR"}
{"created":"2024-03-13 19:09:45","title":"Error analysis in large area multi-Raman pulse atom interferometry due to undesired spontaneous decay","abstract":"Despite the fact that atom interferometry has been a successful application of quantum sensing, a major topic of interest is the further improvement of the sensitivity of these devices. In particular, the area enclosed by the interferometer (which controls the sensitivity) can be increased by providing a larger momentum kick to the atom cloud, increasing the extent of the momentum axis. One such atom optics technique involves increasing the number of central $\\pi-$Raman pulses. This technique, while providing the prerequisite additional momentum boost, also causes the atom to remain in the intermediate high energy state for longer periods of time. This additional length of time is often neglected in many treatments due to the adiabatic elimination of the higher energy state enabled by the large optical detuning. The increased time in the intermediate high energy state results in a higher probability of undesired spontaneous decay and a loss of quantum information, thereby adding error to the atom interferometer. In this work, we consider an open quantum system using the Lindblad master equation to devise a model for the atomic state dynamics that includes the undesired spontaneous decay from the intermediate high energy state. We formulate an error figure of merit to analyze limitations of an atom interferometer configured for acceleration measurements. Our theoretical results show the error figure of merit will be dominated by a $N_{R}^{-2}$ scaling factor for low numbers of $\\pi-$Raman pulses, but will be dominated by a monotonic increase in error for high number of $\\pi-$Raman pulses. We determined the number of $\\pi$-Raman pulses that accomplishes maximal momentum transfer with a the minimal error, depending on major system parameters.","sentences":["Despite the fact that atom interferometry has been a successful application of quantum sensing, a major topic of interest is the further improvement of the sensitivity of these devices.","In particular, the area enclosed by the interferometer (which controls the sensitivity) can be increased by providing a larger momentum kick to the atom cloud, increasing the extent of the momentum axis.","One such atom optics technique involves increasing the number of central $\\pi-$Raman pulses.","This technique, while providing the prerequisite additional momentum boost, also causes the atom to remain in the intermediate high energy state for longer periods of time.","This additional length of time is often neglected in many treatments due to the adiabatic elimination of the higher energy state enabled by the large optical detuning.","The increased time in the intermediate high energy state results in a higher probability of undesired spontaneous decay and a loss of quantum information, thereby adding error to the atom interferometer.","In this work, we consider an open quantum system using the Lindblad master equation to devise a model for the atomic state dynamics that includes the undesired spontaneous decay from the intermediate high energy state.","We formulate an error figure of merit to analyze limitations of an atom interferometer configured for acceleration measurements.","Our theoretical results show the error figure of merit will be dominated by a $N_{R}^{-2}$ scaling factor for low numbers of $\\pi-$Raman pulses, but will be dominated by a monotonic increase in error for high number of $\\pi-$Raman pulses.","We determined the number of $\\pi$-Raman pulses that accomplishes maximal momentum transfer with a the minimal error, depending on major system parameters."],"url":"http://arxiv.org/abs/2403.08913v1","category":"quant-ph"}
{"created":"2024-03-13 18:42:55","title":"Robust a posteriori error control for the Allen-Cahn equation with variable mobility","abstract":"In this work, we derive a $\\gamma$-robust a posteriori error estimator for finite element approximations of the Allen-Cahn equation with variable non-degenerate mobility. The estimator utilizes spectral estimates for the linearized steady part of the differential operator as well as a conditional stability estimate based on a weighted sum of Bregman distances, based on the energy and a functional related to the mobility. A suitable reconstruction of the numerical solution in the stability estimate leads to a fully computable estimator.","sentences":["In this work, we derive a $\\gamma$-robust a posteriori error estimator for finite element approximations of the Allen-Cahn equation with variable non-degenerate mobility.","The estimator utilizes spectral estimates for the linearized steady part of the differential operator as well as a conditional stability estimate based on a weighted sum of Bregman distances, based on the energy and a functional related to the mobility.","A suitable reconstruction of the numerical solution in the stability estimate leads to a fully computable estimator."],"url":"http://arxiv.org/abs/2403.08898v1","category":"math.NA"}
{"created":"2024-03-13 18:37:20","title":"Noninvasive identification of carbon-based black pigments with pump-probe microscopy","abstract":"Carbon-based black pigments, a widely used class of pigments, are difficult to differentiate with the noninvasive techniques currently used in cultural heritage science. We utilize pump-probe microscopy to distinguish four common carbon-based black pigments as pure pigments, as two-component black pigment mixtures, and as a mixture of a black and a colorful pigment. This work also demonstrates that even nominally homogeneous pigments present remarkable, and useful, heterogeneity in pump-probe microscopy.","sentences":["Carbon-based black pigments, a widely used class of pigments, are difficult to differentiate with the noninvasive techniques currently used in cultural heritage science.","We utilize pump-probe microscopy to distinguish four common carbon-based black pigments as pure pigments, as two-component black pigment mixtures, and as a mixture of a black and a colorful pigment.","This work also demonstrates that even nominally homogeneous pigments present remarkable, and useful, heterogeneity in pump-probe microscopy."],"url":"http://arxiv.org/abs/2403.08897v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 18:18:33","title":"Weyl superconductivity and quasiperiodic Majorana arcs in quasicrystals","abstract":"Weyl superconductivity is a topological phase in three-dimensional crystals in which the Weyl equation describes quasiparticle excitation near band-touching points in momentum space called Weyl nodes. For quasicrystals which lack translational symmetry, a theory of Weyl superconductivity has not been established, in spite of recent extensive studies on quasicrystalline topological phases. Here, we demonstrate the occurrence of quasicrystalline Weyl superconductivity by extending the definition of Weyl superconductivity to periodically stacked, two-dimensional superconducting quasicrystals. We identify quasicrystalline Weyl nodes -- topologically protected point nodes in one-dimensional momentum space corresponding to the stacking direction -- in terms of a topological invariant given by a change in the Bott index in quasicrystalline layers. We find that these Weyl nodes exist in pairs and that Majorana zero-energy modes protected by the nonzero Bott index between a pair of quasicrystalline Weyl nodes appear on surfaces. These Majorana zero modes form an infinite number of arcs in momentum space, densely and quasiperiodically distributed as a function of momentum in the direction of surfaces within each quasicrystalline layer. In Ammann-Beenker (Penrose) quasicrystals, the quasiperiodicity of Majorana arcs is governed by the silver (golden) ratio associated with the quasicrystalline structure.","sentences":["Weyl superconductivity is a topological phase in three-dimensional crystals in which the Weyl equation describes quasiparticle excitation near band-touching points in momentum space called Weyl nodes.","For quasicrystals which lack translational symmetry, a theory of Weyl superconductivity has not been established, in spite of recent extensive studies on quasicrystalline topological phases.","Here, we demonstrate the occurrence of quasicrystalline Weyl superconductivity by extending the definition of Weyl superconductivity to periodically stacked, two-dimensional superconducting quasicrystals.","We identify quasicrystalline Weyl nodes -- topologically protected point nodes in one-dimensional momentum space corresponding to the stacking direction -- in terms of a topological invariant given by a change in the Bott index in quasicrystalline layers.","We find that these Weyl nodes exist in pairs and that Majorana zero-energy modes protected by the nonzero Bott index between a pair of quasicrystalline Weyl nodes appear on surfaces.","These Majorana zero modes form an infinite number of arcs in momentum space, densely and quasiperiodically distributed as a function of momentum in the direction of surfaces within each quasicrystalline layer.","In Ammann-Beenker (Penrose) quasicrystals, the quasiperiodicity of Majorana arcs is governed by the silver (golden) ratio associated with the quasicrystalline structure."],"url":"http://arxiv.org/abs/2403.08889v1","category":"cond-mat.supr-con"}
{"created":"2024-03-13 18:00:02","title":"An atlas of resolved spectral features in the transmission spectrum of WASP-189 b with MAROON-X","abstract":"Exoplanets in the ultra-hot Jupiter regime provide an excellent laboratory for testing the impact of stellar irradiation on the dynamics and chemical composition of gas giant atmospheres. In this study, we observed two transits of the ultra-hot Jupiter WASP-189 b with MAROON-X/Gemini-North to probe its high-altitude atmospheric layers, using strong absorption lines. We derived posterior probability distributions for the planetary and stellar parameters by calculating the stellar spectrum behind the planet at every orbital phase during the transit. This was used to correct the Rossiter-McLaughlin imprint on the transmission spectra. Using differential transmission spectroscopy, we detect strong absorption lines of Ca+, Ba+, Na, H$\\alpha$, Mg, Fe, and Fe+, providing an unprecedented and detailed view of the atmospheric chemical composition. Ca+ absorption is particularly well suited for analysis through time-resolved narrow-band spectroscopy, owing to its transition lines formed in high-altitude layers. The spectral absorption lines show no significant blueshifts that would indicate high-altitude day-to-night winds, and further analysis is needed to investigate the implications for atmospheric dynamics. These high signal-to-noise observations provide a benchmark data set for testing high-resolution retrievals and the assumptions of atmospheric models. We also simulate observations of WASP-189 b with ANDES/ELT, and show that ANDES will be highly sensitive to the individual absorption lines of a myriad of elements and molecules, including TiO and CO.","sentences":["Exoplanets in the ultra-hot Jupiter regime provide an excellent laboratory for testing the impact of stellar irradiation on the dynamics and chemical composition of gas giant atmospheres.","In this study, we observed two transits of the ultra-hot Jupiter WASP-189 b with MAROON-X/Gemini-North to probe its high-altitude atmospheric layers, using strong absorption lines.","We derived posterior probability distributions for the planetary and stellar parameters by calculating the stellar spectrum behind the planet at every orbital phase during the transit.","This was used to correct the Rossiter-McLaughlin imprint on the transmission spectra.","Using differential transmission spectroscopy, we detect strong absorption lines of Ca+, Ba+, Na, H$\\alpha$, Mg, Fe, and Fe+, providing an unprecedented and detailed view of the atmospheric chemical composition.","Ca+ absorption is particularly well suited for analysis through time-resolved narrow-band spectroscopy, owing to its transition lines formed in high-altitude layers.","The spectral absorption lines show no significant blueshifts that would indicate high-altitude day-to-night winds, and further analysis is needed to investigate the implications for atmospheric dynamics.","These high signal-to-noise observations provide a benchmark data set for testing high-resolution retrievals and the assumptions of atmospheric models.","We also simulate observations of WASP-189 b with ANDES/ELT, and show that ANDES will be highly sensitive to the individual absorption lines of a myriad of elements and molecules, including TiO and CO."],"url":"http://arxiv.org/abs/2403.08863v1","category":"astro-ph.EP"}
{"created":"2024-03-13 18:00:01","title":"Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling","abstract":"Many machine learning applications involve learning a latent representation of data, which is often high-dimensional and difficult to directly interpret. In this work, we propose \"Moment Pooling\", a natural extension of Deep Sets networks which drastically decrease latent space dimensionality of these networks while maintaining or even improving performance. Moment Pooling generalizes the summation in Deep Sets to arbitrary multivariate moments, which enables the model to achieve a much higher effective latent dimensionality for a fixed latent dimension. We demonstrate Moment Pooling on the collider physics task of quark/gluon jet classification by extending Energy Flow Networks (EFNs) to Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1 perform similarly to ordinary EFNs with higher latent dimension. This small latent dimension allows for the internal representation to be directly visualized and interpreted, which in turn enables the learned internal jet representation to be extracted in closed form.","sentences":["Many machine learning applications involve learning a latent representation of data, which is often high-dimensional and difficult to directly interpret.","In this work, we propose \"Moment Pooling\", a natural extension of Deep Sets networks which drastically decrease latent space dimensionality of these networks while maintaining or even improving performance.","Moment Pooling generalizes the summation in Deep Sets to arbitrary multivariate moments, which enables the model to achieve a much higher effective latent dimensionality for a fixed latent dimension.","We demonstrate Moment Pooling on the collider physics task of quark/gluon jet classification by extending Energy Flow Networks (EFNs) to Moment EFNs.","We find that Moment EFNs with latent dimensions as small as 1 perform similarly to ordinary EFNs with higher latent dimension.","This small latent dimension allows for the internal representation to be directly visualized and interpreted, which in turn enables the learned internal jet representation to be extracted in closed form."],"url":"http://arxiv.org/abs/2403.08854v1","category":"hep-ph"}
{"created":"2024-03-13 18:00:00","title":"PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models","abstract":"We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a given observation). Our study demonstrates the potential for using generalist foundation models rather than task-specific models for interacting with astronomical data by leveraging text as an interface.","sentences":["We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model.","The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs).","Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a given observation).","Our study demonstrates the potential for using generalist foundation models rather than task-specific models for interacting with astronomical data by leveraging text as an interface."],"url":"http://arxiv.org/abs/2403.08851v1","category":"astro-ph.IM"}
{"created":"2024-03-14 17:59:59","title":"GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding","abstract":"Self-supervised 3D representation learning aims to learn effective representations from large-scale unlabeled point clouds. Most existing approaches adopt point discrimination as the pretext task, which assigns matched points in two distinct views as positive pairs and unmatched points as negative pairs. However, this approach often results in semantically identical points having dissimilar representations, leading to a high number of false negatives and introducing a \"semantic conflict\" problem. To address this issue, we propose GroupContrast, a novel approach that combines segment grouping and semantic-aware contrastive learning. Segment grouping partitions points into semantically meaningful regions, which enhances semantic coherence and provides semantic guidance for the subsequent contrastive representation learning. Semantic-aware contrastive learning augments the semantic information extracted from segment grouping and helps to alleviate the issue of \"semantic conflict\". We conducted extensive experiments on multiple 3D scene understanding tasks. The results demonstrate that GroupContrast learns semantically meaningful representations and achieves promising transfer learning performance.","sentences":["Self-supervised 3D representation learning aims to learn effective representations from large-scale unlabeled point clouds.","Most existing approaches adopt point discrimination as the pretext task, which assigns matched points in two distinct views as positive pairs and unmatched points as negative pairs.","However, this approach often results in semantically identical points having dissimilar representations, leading to a high number of false negatives and introducing a \"semantic conflict\" problem.","To address this issue, we propose GroupContrast, a novel approach that combines segment grouping and semantic-aware contrastive learning.","Segment grouping partitions points into semantically meaningful regions, which enhances semantic coherence and provides semantic guidance for the subsequent contrastive representation learning.","Semantic-aware contrastive learning augments the semantic information extracted from segment grouping and helps to alleviate the issue of \"semantic conflict\".","We conducted extensive experiments on multiple 3D scene understanding tasks.","The results demonstrate that GroupContrast learns semantically meaningful representations and achieves promising transfer learning performance."],"url":"http://arxiv.org/abs/2403.09639v1","category":"cs.CV"}
{"created":"2024-03-14 17:51:32","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","abstract":"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.","sentences":["In this work, we discuss building performant Multimodal Large Language Models (MLLMs).","In particular, we study the importance of various architecture components and data choices.","Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons.","For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results.","Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance.","By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks.","Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting."],"url":"http://arxiv.org/abs/2403.09611v1","category":"cs.CV"}
{"created":"2024-03-14 17:50:51","title":"Signal Recovery with Proximal Comixtures","abstract":"In variational signal processing and machine learning problems, loss functions and linear operators are typically aggregated as an average of composite terms. We propose an alternative formulation using proximal comixtures, an operation that combines functions and linear operators in such a way that the proximity operator of the resulting function is computable explicitly. The benefits of comixture formulations are illustrated through image recovery and machine learning applications.","sentences":["In variational signal processing and machine learning problems, loss functions and linear operators are typically aggregated as an average of composite terms.","We propose an alternative formulation using proximal comixtures, an operation that combines functions and linear operators in such a way that the proximity operator of the resulting function is computable explicitly.","The benefits of comixture formulations are illustrated through image recovery and machine learning applications."],"url":"http://arxiv.org/abs/2403.09610v1","category":"math.OC"}
{"created":"2024-03-14 17:45:24","title":"Extremal graphical modeling with latent variables","abstract":"Extremal graphical models encode the conditional independence structure of multivariate extremes and provide a powerful tool for quantifying the risk of rare events. Prior work on learning these graphs from data has focused on the setting where all relevant variables are observed. For the popular class of H\\\"usler-Reiss models, we propose the \\texttt{eglatent} method, a tractable convex program for learning extremal graphical models in the presence of latent variables. Our approach decomposes the H\\\"usler-Reiss precision matrix into a sparse component encoding the graphical structure among the observed variables after conditioning on the latent variables, and a low-rank component encoding the effect of a few latent variables on the observed variables. We provide finite-sample guarantees of \\texttt{eglatent} and show that it consistently recovers the conditional graph as well as the number of latent variables. We highlight the improved performances of our approach on synthetic and real data.","sentences":["Extremal graphical models encode the conditional independence structure of multivariate extremes and provide a powerful tool for quantifying the risk of rare events.","Prior work on learning these graphs from data has focused on the setting where all relevant variables are observed.","For the popular class of H\\\"usler-Reiss models, we propose the \\texttt{eglatent} method, a tractable convex program for learning extremal graphical models in the presence of latent variables.","Our approach decomposes the H\\\"usler-Reiss precision matrix into a sparse component encoding the graphical structure among the observed variables after conditioning on the latent variables, and a low-rank component encoding the effect of a few latent variables on the observed variables.","We provide finite-sample guarantees of \\texttt{eglatent} and show that it consistently recovers the conditional graph as well as the number of latent variables.","We highlight the improved performances of our approach on synthetic and real data."],"url":"http://arxiv.org/abs/2403.09604v1","category":"stat.ME"}
{"created":"2024-03-14 17:39:14","title":"Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds","abstract":"Multi-label imbalanced classification poses a significant challenge in machine learning, particularly evident in bioacoustics where animal sounds often co-occur, and certain sounds are much less frequent than others. This paper focuses on the specific case of classifying anuran species sounds using the dataset AnuraSet, that contains both class imbalance and multi-label examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a framework that leverages mixing regularization methods Mixup, Manifold Mixup, and MultiMix. Experimental results show that these methods, individually, may lead to suboptimal results; however, when applied randomly, with one selected at each training iteration, they prove effective in addressing the mentioned challenges, particularly for rare classes with few occurrences. Further analysis reveals that Mix2 is also proficient in classifying sounds across various levels of class co-occurrences.","sentences":["Multi-label imbalanced classification poses a significant challenge in machine learning, particularly evident in bioacoustics where animal sounds often co-occur, and certain sounds are much less frequent than others.","This paper focuses on the specific case of classifying anuran species sounds using the dataset AnuraSet, that contains both class imbalance and multi-label examples.","To address these challenges, we introduce Mixture of Mixups (Mix2), a framework that leverages mixing regularization methods Mixup, Manifold Mixup, and MultiMix.","Experimental results show that these methods, individually, may lead to suboptimal results; however, when applied randomly, with one selected at each training iteration, they prove effective in addressing the mentioned challenges, particularly for rare classes with few occurrences.","Further analysis reveals that Mix2 is also proficient in classifying sounds across various levels of class co-occurrences."],"url":"http://arxiv.org/abs/2403.09598v1","category":"cs.SD"}
{"created":"2024-03-14 17:18:15","title":"ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models","abstract":"In image-based robot manipulation tasks with large observation and action spaces, reinforcement learning struggles with low sample efficiency, slow training speed, and uncertain convergence. As an alternative, large pre-trained foundation models have shown promise in robotic manipulation, particularly in zero-shot and few-shot applications. However, using these models directly is unreliable due to limited reasoning capabilities and challenges in understanding physical and spatial contexts. This paper introduces ExploRLLM, a novel approach that leverages the inductive bias of foundation models (e.g. Large Language Models) to guide exploration in reinforcement learning. We also exploit these foundation models to reformulate the action and observation spaces to enhance the training efficiency in reinforcement learning. Our experiments demonstrate that guided exploration enables much quicker convergence than training without it. Additionally, we validate that ExploRLLM outperforms vanilla foundation model baselines and that the policy trained in simulation can be applied in real-world settings without additional training.","sentences":["In image-based robot manipulation tasks with large observation and action spaces, reinforcement learning struggles with low sample efficiency, slow training speed, and uncertain convergence.","As an alternative, large pre-trained foundation models have shown promise in robotic manipulation, particularly in zero-shot and few-shot applications.","However, using these models directly is unreliable due to limited reasoning capabilities and challenges in understanding physical and spatial contexts.","This paper introduces ExploRLLM, a novel approach that leverages the inductive bias of foundation models (e.g. Large Language Models) to guide exploration in reinforcement learning.","We also exploit these foundation models to reformulate the action and observation spaces to enhance the training efficiency in reinforcement learning.","Our experiments demonstrate that guided exploration enables much quicker convergence than training without it.","Additionally, we validate that ExploRLLM outperforms vanilla foundation model baselines and that the policy trained in simulation can be applied in real-world settings without additional training."],"url":"http://arxiv.org/abs/2403.09583v1","category":"cs.RO"}
{"created":"2024-03-14 16:35:43","title":"Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability","abstract":"Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. The main objective of this study is to use state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and LightGBM to predict and diagnose breast cancer and to find the most effective metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study is the first to use these four boosting algorithms with Optuna, a library for hyperparameter optimization, and the SHAP method to improve the interpretability of our model, which can be used as a support to identify and predict breast cancer. We were able to improve AUC or recall for all the models and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more than 99.41\\% for all models.","sentences":["Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths.","However, it can be prevented by early detection and, consequently, early treatment.","Any development for detection or perdition this kind of cancer is important for a better healthy life.","Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric.","This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric.","Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases.","The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes.","The main objective of this study is to use state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and LightGBM to predict and diagnose breast cancer and to find the most effective metric regarding recall, ROC-AUC, and confusion matrix.","Furthermore, our study is the first to use these four boosting algorithms with Optuna, a library for hyperparameter optimization, and the SHAP method to improve the interpretability of our model, which can be used as a support to identify and predict breast cancer.","We were able to improve AUC or recall for all the models and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more than 99.41\\% for all models."],"url":"http://arxiv.org/abs/2403.09548v1","category":"cs.LG"}
{"created":"2024-03-14 16:35:39","title":"How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions","abstract":"Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build durations, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure. These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively.","sentences":["Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored.","Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches.","In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects).","Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects.","Our findings indicate that ML projects often require longer build durations, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects.","Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts.","Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure.","These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively."],"url":"http://arxiv.org/abs/2403.09547v1","category":"cs.SE"}
{"created":"2024-03-14 16:30:52","title":"Explorations in Texture Learning","abstract":"In this work, we investigate \\textit{texture learning}: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in CNNs and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases.","sentences":["In this work, we investigate \\textit{texture learning}: the identification of textures learned by object classification models, and the extent to which they rely on these textures.","We build texture-object associations that uncover new insights about the relationships between texture and object classes in CNNs and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present.","Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases."],"url":"http://arxiv.org/abs/2403.09543v1","category":"cs.CV"}
{"created":"2024-03-14 15:58:36","title":"Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information","abstract":"Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.","sentences":["Mitigating social biases typically requires identifying the social groups associated with each data sample.","In this paper, we present DAFair, a novel approach to address social bias in language models.","Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information.","Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations.","Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data.","Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches."],"url":"http://arxiv.org/abs/2403.09516v1","category":"cs.CL"}
{"created":"2024-03-14 15:30:25","title":"Hyper-CL: Conditioning Sentence Representations with Hypernetworks","abstract":"While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations. In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions. Evaluation on two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning sentence representations, showcasing its computational efficiency at the same time. We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms.","sentences":["While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives.","In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations.","In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers.","This enables the same sentence embeddings to be projected differently according to various conditions.","Evaluation on two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning sentence representations, showcasing its computational efficiency at the same time.","We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms."],"url":"http://arxiv.org/abs/2403.09490v1","category":"cs.CL"}
{"created":"2024-03-14 15:04:14","title":"Machine learning activation function parameterization of the occupation numbers for natural orbital functionals based on electron pairing approaches","abstract":"Within the framework of natural orbital functional theory, having a convenient representation of the optimization function becomes critical for the computational performance of the calculation. Recognizing this, we propose an innovative parametrization of the occupation numbers that takes advantage of the electron-pairing approach used in Piris natural orbital functionals, through the adoption of the softmax function, a pivotal component in modern deep-learning models. Our approach not only ensures adherence to the N-representability of the first-order reduced density matrix (1RDM) but also significantly enhances the computational efficiency of 1RDM functional theory calculations. The effectiveness of this alternative parameterization approach was assessed using the W4-17-MR molecular set, which demonstrated faster and robust convergence compared to previous implementations.","sentences":["Within the framework of natural orbital functional theory, having a convenient representation of the optimization function becomes critical for the computational performance of the calculation.","Recognizing this, we propose an innovative parametrization of the occupation numbers that takes advantage of the electron-pairing approach used in Piris natural orbital functionals, through the adoption of the softmax function, a pivotal component in modern deep-learning models.","Our approach not only ensures adherence to the N-representability of the first-order reduced density matrix (1RDM) but also significantly enhances the computational efficiency of 1RDM functional theory calculations.","The effectiveness of this alternative parameterization approach was assessed using the W4-17-MR molecular set, which demonstrated faster and robust convergence compared to previous implementations."],"url":"http://arxiv.org/abs/2403.09463v1","category":"physics.chem-ph"}
{"created":"2024-03-14 14:19:48","title":"Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity","abstract":"Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited sample size issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training samples. We propose to use retrieval-augmented in-context learning to address these two crucial issues by unleashing the potential of a transformer's in-context learning ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training samples, our work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a higher degree of sample efficiency and is empirically demonstrated to enhance the classification model's performance on both full- and missing-modality data in the low-data regime across various multimodal learning tasks. When only 1% of the training data are available, our proposed method demonstrates an average improvement of 6.1% over a recent strong baseline across various datasets and missing states. Notably, our method also reduces the performance gap between full-modality and missing-modality data compared with the baseline.","sentences":["Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare.","This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited sample size issues.","This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training samples.","We propose to use retrieval-augmented in-context learning to address these two crucial issues by unleashing the potential of a transformer's in-context learning ability.","Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training samples, our work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge.","The proposed data-dependent framework exhibits a higher degree of sample efficiency and is empirically demonstrated to enhance the classification model's performance on both full- and missing-modality data in the low-data regime across various multimodal learning tasks.","When only 1% of the training data are available, our proposed method demonstrates an average improvement of 6.1% over a recent strong baseline across various datasets and missing states.","Notably, our method also reduces the performance gap between full-modality and missing-modality data compared with the baseline."],"url":"http://arxiv.org/abs/2403.09428v1","category":"cs.LG"}
{"created":"2024-03-14 14:05:24","title":"Constrained and Vanishing Expressivity of Quantum Fourier Models","abstract":"In this work, we highlight an unforeseen behavior of the expressivity of Parameterized Quantum Circuits (PQC) for machine learning. A large class of these models, seen as Fourier Series which frequencies are derived from the encoding gates, were thought to have their Fourier coefficients mostly determined by the trainable gates. Here, we demonstrate a new correlation between the Fourier coefficients of the quantum model and its encoding gates. In addition, we display a phenomenon of vanishing expressivity in certain settings, where some Fourier coefficients vanish exponentially when the number of qubits grows. These two behaviors imply novel forms of constraints which limit the expressivity of PQCs, and therefore imply a new inductive bias for Quantum models. The key concept in this work is the notion of a frequency redundancy in the Fourier series spectrum, which determines its importance. Those theoretical behaviours are observed in numerical simulations.","sentences":["In this work, we highlight an unforeseen behavior of the expressivity of Parameterized Quantum Circuits (PQC) for machine learning.","A large class of these models, seen as Fourier Series which frequencies are derived from the encoding gates, were thought to have their Fourier coefficients mostly determined by the trainable gates.","Here, we demonstrate a new correlation between the Fourier coefficients of the quantum model and its encoding gates.","In addition, we display a phenomenon of vanishing expressivity in certain settings, where some Fourier coefficients vanish exponentially when the number of qubits grows.","These two behaviors imply novel forms of constraints which limit the expressivity of PQCs, and therefore imply a new inductive bias for Quantum models.","The key concept in this work is the notion of a frequency redundancy in the Fourier series spectrum, which determines its importance.","Those theoretical behaviours are observed in numerical simulations."],"url":"http://arxiv.org/abs/2403.09417v1","category":"quant-ph"}
{"created":"2024-03-14 14:04:37","title":"User Identification via Free Roaming Eye Tracking Data","abstract":"We present a new dataset of \"free roaming\" (FR) and \"targeted roaming\" (TR): a pool of 41 participants is asked to walk around a university campus (FR) or is asked to find a particular room within a library (TR). Eye movements are recorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz). On this dataset we investigate the accuracy of user identification using a previously known machine learning pipeline where a Radial Basis Function Network (RBFN) is used as classifier. Our highest accuracies are 87.3% for FR and 89.4% for TR. This should be compared to 95.3% which is the (corresponding) highest accuracy we are aware of (achieved in a laboratory setting using the \"RAN\" stimulus of the BioEye 2015 competition dataset). To the best of our knowledge, our results are the first that study user identification in a non laboratory setting; such settings are often more feasible than laboratory settings and may include further advantages. The minimum duration of each recording is 263s for FR and 154s for TR. Our best accuracies are obtained when restricting to 120s and 140s for FR and TR respectively, always cut from the end of the trajectories (both for the training and testing sessions). If we cut the same length from the beginning, then accuracies are 12.2% lower for FR and around 6.4% lower for TR. On the full trajectories accuracies are lower by 5% and 52% for FR and TR. We also investigate the impact of including higher order velocity derivatives (such as acceleration, jerk, or jounce).","sentences":["We present a new dataset of \"free roaming\" (FR) and \"targeted roaming\" (TR): a pool of 41 participants is asked to walk around a university campus (FR) or is asked to find a particular room within a library (TR).","Eye movements are recorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz).","On this dataset we investigate the accuracy of user identification using a previously known machine learning pipeline where a Radial Basis Function Network (RBFN) is used as classifier.","Our highest accuracies are 87.3% for FR and 89.4% for TR.","This should be compared to 95.3% which is the (corresponding) highest accuracy we are aware of (achieved in a laboratory setting using the \"RAN\" stimulus of the BioEye 2015 competition dataset).","To the best of our knowledge, our results are the first that study user identification in a non laboratory setting; such settings are often more feasible than laboratory settings and may include further advantages.","The minimum duration of each recording is 263s for FR and 154s for TR.","Our best accuracies are obtained when restricting to 120s and 140s for FR and TR respectively, always cut from the end of the trajectories (both for the training and testing sessions).","If we cut the same length from the beginning, then accuracies are 12.2% lower for FR and around 6.4% lower for TR.","On the full trajectories accuracies are lower by 5% and 52% for FR and TR.","We also investigate the impact of including higher order velocity derivatives (such as acceleration, jerk, or jounce)."],"url":"http://arxiv.org/abs/2403.09415v1","category":"cs.LG"}
{"created":"2024-03-14 12:12:17","title":"EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection","abstract":"Multimodal image fusion and object detection play a vital role in autonomous driving. Current joint learning methods have made significant progress in the multimodal fusion detection task combining the texture detail and objective semantic information. However, the tedious training steps have limited its applications to wider real-world industrial deployment. To address this limitation, we propose a novel end-to-end multimodal fusion detection algorithm, named EfficientMFD, to simplify models that exhibit decent performance with only one training step. Synchronous joint optimization is utilized in an end-to-end manner between two components, thus not being affected by the local optimal solution of the individual task. Besides, a comprehensive optimization is established in the gradient matrix between the shared parameters for both tasks. It can converge to an optimal point with fusion detection weights. We extensively test it on several public datasets, demonstrating superior performance on not only visually appealing fusion but also favorable detection performance (e.g., 6.6% mAP50:95) over other state-of-the-art approaches.","sentences":["Multimodal image fusion and object detection play a vital role in autonomous driving.","Current joint learning methods have made significant progress in the multimodal fusion detection task combining the texture detail and objective semantic information.","However, the tedious training steps have limited its applications to wider real-world industrial deployment.","To address this limitation, we propose a novel end-to-end multimodal fusion detection algorithm, named EfficientMFD, to simplify models that exhibit decent performance with only one training step.","Synchronous joint optimization is utilized in an end-to-end manner between two components, thus not being affected by the local optimal solution of the individual task.","Besides, a comprehensive optimization is established in the gradient matrix between the shared parameters for both tasks.","It can converge to an optimal point with fusion detection weights.","We extensively test it on several public datasets, demonstrating superior performance on not only visually appealing fusion but also favorable detection performance (e.g., 6.6% mAP50:95) over other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.09323v1","category":"cs.CV"}
{"created":"2024-03-14 11:59:32","title":"MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion","abstract":"Cluttered bin-picking environments are challenging for pose estimation models. Despite the impressive progress enabled by deep learning, single-view RGB pose estimation models perform poorly in cluttered dynamic environments. Imbuing the rich temporal information contained in the video of scenes has the potential to enhance models ability to deal with the adverse effects of occlusion and the dynamic nature of the environments. Moreover, joint object detection and pose estimation models are better suited to leverage the co-dependent nature of the tasks for improving the accuracy of both tasks. To this end, we propose attention-based temporal fusion for multi-object 6D pose estimation that accumulates information across multiple frames of a video sequence. Our MOTPose method takes a sequence of images as input and performs joint object detection and pose estimation for all objects in one forward pass. It learns to aggregate both object embeddings and object parameters over multiple time steps using cross-attention-based fusion modules. We evaluate our method on the physically-realistic cluttered bin-picking dataset SynPick and the YCB-Video dataset and demonstrate improved pose estimation accuracy as well as better object detection accuracy","sentences":["Cluttered bin-picking environments are challenging for pose estimation models.","Despite the impressive progress enabled by deep learning, single-view RGB pose estimation models perform poorly in cluttered dynamic environments.","Imbuing the rich temporal information contained in the video of scenes has the potential to enhance models ability to deal with the adverse effects of occlusion and the dynamic nature of the environments.","Moreover, joint object detection and pose estimation models are better suited to leverage the co-dependent nature of the tasks for improving the accuracy of both tasks.","To this end, we propose attention-based temporal fusion for multi-object 6D pose estimation that accumulates information across multiple frames of a video sequence.","Our MOTPose method takes a sequence of images as input and performs joint object detection and pose estimation for all objects in one forward pass.","It learns to aggregate both object embeddings and object parameters over multiple time steps using cross-attention-based fusion modules.","We evaluate our method on the physically-realistic cluttered bin-picking dataset SynPick and the YCB-Video dataset and demonstrate improved pose estimation accuracy as well as better object detection accuracy"],"url":"http://arxiv.org/abs/2403.09309v1","category":"cs.RO"}
{"created":"2024-03-14 11:29:47","title":"Anatomical Structure-Guided Medical Vision-Language Pre-training","abstract":"Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, i.e., local alignment lacks interpretability and clinical relevance, and the insufficient internal and external representation learning of image-report pairs. To address these issues, we propose an Anatomical Structure-Guided (ASG) framework. Specifically, we parse raw reports into triplets <anatomical region, finding, existence>, and fully utilize each element as supervision to enhance representation learning. For anatomical region, we design an automatic anatomical region-sentence alignment paradigm in collaboration with radiologists, considering them as the minimum semantic units to explore fine-grained local alignment. For finding and existence, we regard them as image tags, applying an image-tag recognition decoder to associate image features with their respective tags within each sample and constructing soft labels for contrastive learning to improve the semantic association of different image-report pairs. We evaluate the proposed ASG framework on two downstream tasks, including five public benchmarks. Experimental results demonstrate that our method outperforms the state-of-the-art methods.","sentences":["Learning medical visual representations through vision-language pre-training has reached remarkable progress.","Despite the promising performance, it still faces challenges, i.e., local alignment lacks interpretability and clinical relevance, and the insufficient internal and external representation learning of image-report pairs.","To address these issues, we propose an Anatomical Structure-Guided (ASG) framework.","Specifically, we parse raw reports into triplets <anatomical region, finding, existence>, and fully utilize each element as supervision to enhance representation learning.","For anatomical region, we design an automatic anatomical region-sentence alignment paradigm in collaboration with radiologists, considering them as the minimum semantic units to explore fine-grained local alignment.","For finding and existence, we regard them as image tags, applying an image-tag recognition decoder to associate image features with their respective tags within each sample and constructing soft labels for contrastive learning to improve the semantic association of different image-report pairs.","We evaluate the proposed ASG framework on two downstream tasks, including five public benchmarks.","Experimental results demonstrate that our method outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.09294v1","category":"cs.CV"}
{"created":"2024-03-14 11:12:10","title":"DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning","abstract":"Personalized federated learning becomes a hot research topic that can learn a personalized learning model for each client. Existing personalized federated learning models prefer to aggregate similar clients with similar data distribution to improve the performance of learning models. However, similaritybased personalized federated learning methods may exacerbate the class imbalanced problem. In this paper, we propose a novel Dynamic Affinity-based Personalized Federated Learning model (DA-PFL) to alleviate the class imbalanced problem during federated learning. Specifically, we build an affinity metric from a complementary perspective to guide which clients should be aggregated. Then we design a dynamic aggregation strategy to dynamically aggregate clients based on the affinity metric in each round to reduce the class imbalanced risk. Extensive experiments show that the proposed DA-PFL model can significantly improve the accuracy of each client in three real-world datasets with state-of-the-art comparison methods.","sentences":["Personalized federated learning becomes a hot research topic that can learn a personalized learning model for each client.","Existing personalized federated learning models prefer to aggregate similar clients with similar data distribution to improve the performance of learning models.","However, similaritybased personalized federated learning methods may exacerbate the class imbalanced problem.","In this paper, we propose a novel Dynamic Affinity-based Personalized Federated Learning model (DA-PFL) to alleviate the class imbalanced problem during federated learning.","Specifically, we build an affinity metric from a complementary perspective to guide which clients should be aggregated.","Then we design a dynamic aggregation strategy to dynamically aggregate clients based on the affinity metric in each round to reduce the class imbalanced risk.","Extensive experiments show that the proposed DA-PFL model can significantly improve the accuracy of each client in three real-world datasets with state-of-the-art comparison methods."],"url":"http://arxiv.org/abs/2403.09284v1","category":"cs.LG"}
{"created":"2024-03-14 08:27:34","title":"VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos","abstract":"The lengthy monologue-style online lectures cause learners to lose engagement easily. Designing lectures in a \"vicarious dialogue\" format can foster learners' cognitive activities more than monologue-style. However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors. We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of large language models (LLM) to transform a monologue lecture script into pedagogically meaningful dialogue. Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues. In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues. Our findings demonstrate the potential of LLMs to assist instructors with creating high-quality educational dialogues across various learning stages.","sentences":["The lengthy monologue-style online lectures cause learners to lose engagement easily.","Designing lectures in a \"vicarious dialogue\" format can foster learners' cognitive activities more than monologue-style.","However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors.","We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of large language models (LLM) to transform a monologue lecture script into pedagogically meaningful dialogue.","Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues.","In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues.","Our findings demonstrate the potential of LLMs to assist instructors with creating high-quality educational dialogues across various learning stages."],"url":"http://arxiv.org/abs/2403.09168v1","category":"cs.HC"}
{"created":"2024-03-14 08:18:59","title":"Unveiling the Generalization Power of Fine-Tuned Large Language Models","abstract":"While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood. This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability. Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.","sentences":["While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning.","However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood.","This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants.","Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs.","To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.","Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.","Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability.","Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs."],"url":"http://arxiv.org/abs/2403.09162v1","category":"cs.CL"}
{"created":"2024-03-14 07:38:22","title":"Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios","abstract":"A connectional brain template (CBT) is a holistic representation of a population of multi-view brain connectivity graphs, encoding shared patterns and normalizing typical variations across individuals. The federation of CBT learning allows for an inclusive estimation of the representative center of multi-domain brain connectivity datasets in a fully data-preserving manner. However, existing methods overlook the non-independent and identically distributed (non-IDD) issue stemming from multidomain brain connectivity heterogeneity, in which data domains are drawn from different hospitals and imaging modalities. To overcome this limitation, we unprecedentedly propose a metadata-driven federated learning framework, called MetaFedCBT, for cross-domain CBT learning. Given the data drawn from a specific domain (i.e., hospital), our model aims to learn metadata in a fully supervised manner by introducing a local client-based regressor network. The generated meta-data is forced to meet the statistical attributes (e.g., mean) of other domains, while preserving their privacy. Our supervised meta-data generation approach boosts the unsupervised learning of a more centered, representative, and holistic CBT of a particular brain state across diverse domains. As the federated learning progresses over multiple rounds, the learned metadata and associated generated connectivities are continuously updated to better approximate the target domain information. MetaFedCBT overcomes the non-IID issue of existing methods by generating informative brain connectivities for privacy-preserving holistic CBT learning with guidance using metadata. Extensive experiments on multi-view morphological brain networks of normal and patient subjects demonstrate that our MetaFedCBT is a superior federated CBT learning model and significantly advances the state-of-the-art performance.","sentences":["A connectional brain template (CBT) is a holistic representation of a population of multi-view brain connectivity graphs, encoding shared patterns and normalizing typical variations across individuals.","The federation of CBT learning allows for an inclusive estimation of the representative center of multi-domain brain connectivity datasets in a fully data-preserving manner.","However, existing methods overlook the non-independent and identically distributed (non-IDD) issue stemming from multidomain brain connectivity heterogeneity, in which data domains are drawn from different hospitals and imaging modalities.","To overcome this limitation, we unprecedentedly propose a metadata-driven federated learning framework, called MetaFedCBT, for cross-domain CBT learning.","Given the data drawn from a specific domain (i.e., hospital), our model aims to learn metadata in a fully supervised manner by introducing a local client-based regressor network.","The generated meta-data is forced to meet the statistical attributes (e.g., mean) of other domains, while preserving their privacy.","Our supervised meta-data generation approach boosts the unsupervised learning of a more centered, representative, and holistic CBT of a particular brain state across diverse domains.","As the federated learning progresses over multiple rounds, the learned metadata and associated generated connectivities are continuously updated to better approximate the target domain information.","MetaFedCBT overcomes the non-IID issue of existing methods by generating informative brain connectivities for privacy-preserving holistic CBT learning with guidance using metadata.","Extensive experiments on multi-view morphological brain networks of normal and patient subjects demonstrate that our MetaFedCBT is a superior federated CBT learning model and significantly advances the state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.09139v1","category":"cs.CV"}
{"created":"2024-03-14 06:26:34","title":"Rethinking Referring Object Removal","abstract":"Referring object removal refers to removing the specific object in an image referred by natural language expressions and filling the missing region with reasonable semantics. To address this task, we construct the ComCOCO, a synthetic dataset consisting of 136,495 referring expressions for 34,615 objects in 23,951 image pairs. Each pair contains an image with referring expressions and the ground truth after elimination. We further propose an end-to-end syntax-aware hybrid mapping network with an encoding-decoding structure. Linguistic features are hierarchically extracted at the syntactic level and fused in the downsampling process of visual features with multi-head attention. The feature-aligned pyramid network is leveraged to generate segmentation masks and replace internal pixels with region affinity learned from external semantics in high-level feature maps. Extensive experiments demonstrate that our model outperforms diffusion models and two-stage methods which process the segmentation and inpainting task separately by a significant margin.","sentences":["Referring object removal refers to removing the specific object in an image referred by natural language expressions and filling the missing region with reasonable semantics.","To address this task, we construct the ComCOCO, a synthetic dataset consisting of 136,495 referring expressions for 34,615 objects in 23,951 image pairs.","Each pair contains an image with referring expressions and the ground truth after elimination.","We further propose an end-to-end syntax-aware hybrid mapping network with an encoding-decoding structure.","Linguistic features are hierarchically extracted at the syntactic level and fused in the downsampling process of visual features with multi-head attention.","The feature-aligned pyramid network is leveraged to generate segmentation masks and replace internal pixels with region affinity learned from external semantics in high-level feature maps.","Extensive experiments demonstrate that our model outperforms diffusion models and two-stage methods which process the segmentation and inpainting task separately by a significant margin."],"url":"http://arxiv.org/abs/2403.09128v1","category":"cs.CV"}
{"created":"2024-03-14 05:00:29","title":"S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering","abstract":"Anchor-based large-scale multi-view clustering has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for clustering by exploring global correlations between anchor graphs or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor clustering (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates graph similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding features to ensure inter-view semantic consistency. Experimental results on six large-scale multi-view datasets demonstrate that S^2MVTC significantly outperforms state-of-the-art algorithms in terms of clustering performance and CPU execution time, especially when handling massive data. The code of S^2MVTC is publicly available at https://github.com/longzhen520/S2MVTC.","sentences":["Anchor-based large-scale multi-view clustering has attracted considerable attention for its effectiveness in handling massive datasets.","However, current methods mainly seek the consensus embedding feature for clustering by exploring global correlations between anchor graphs or projection matrices.","In this paper, we propose a simple yet efficient scalable multi-view tensor clustering (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views.","Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it.","Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates graph similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views.","Furthermore, consensus constraints are applied to embedding features to ensure inter-view semantic consistency.","Experimental results on six large-scale multi-view datasets demonstrate that S^2MVTC significantly outperforms state-of-the-art algorithms in terms of clustering performance and CPU execution time, especially when handling massive data.","The code of S^2MVTC is publicly available at https://github.com/longzhen520/S2MVTC."],"url":"http://arxiv.org/abs/2403.09107v1","category":"cs.LG"}
{"created":"2024-03-14 04:06:45","title":"Learning from straggler clients in federated learning","abstract":"How well do existing federated learning algorithms learn from client devices that return model updates with a significant time delay? Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled? We answer these questions by developing Monte Carlo simulations of client latency that are guided by real-world applications. We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients. To improve upon this situation, we experiment with modifications, including distillation regularization and exponential moving averages of model weights. Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and StackOverflow benchmark federated learning tasks demonstrate that our new algorithms outperform existing ones in terms of accuracy for straggler clients, while also providing better trade-offs between training time and total accuracy.","sentences":["How well do existing federated learning algorithms learn from client devices that return model updates with a significant time delay?","Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled?","We answer these questions by developing Monte Carlo simulations of client latency that are guided by real-world applications.","We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients.","To improve upon this situation, we experiment with modifications, including distillation regularization and exponential moving averages of model weights.","Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation and averaging, respectively.","Experiments with the EMNIST, CIFAR-100, and StackOverflow benchmark federated learning tasks demonstrate that our new algorithms outperform existing ones in terms of accuracy for straggler clients, while also providing better trade-offs between training time and total accuracy."],"url":"http://arxiv.org/abs/2403.09086v1","category":"cs.LG"}
{"created":"2024-03-14 03:49:36","title":"Information Extraction: An application to the domain of hyper-local financial data on developing countries","abstract":"Despite the need for financial data on company activities in developing countries for development research and economic analysis, such data does not exist. In this project, we develop and evaluate two Natural Language Processing (NLP) based techniques to address this issue. First, we curate a custom dataset specific to the domain of financial text data on developing countries and explore multiple approaches for information extraction. We then explore a text-to-text approach with the transformer-based T5 model with the goal of undertaking simultaneous NER and relation extraction. We find that this model is able to learn the custom text structure output data corresponding to the entities and their relations, resulting in an accuracy of 92.44\\%, a precision of 68.25\\% and a recall of 54.20\\% from our best T5 model on the combined task. Secondly, we explore an approach with sequential NER and relation extration. For the NER, we run pre-trained and fine-tuned models using SpaCy, and we develop a custom relation extraction model using SpaCy's Dependency Parser output and some heuristics to determine entity relationships \\cite{spacy}. We obtain an accuracy of 84.72\\%, a precision of 6.06\\% and a recall of 5.57\\% on this sequential task.","sentences":["Despite the need for financial data on company activities in developing countries for development research and economic analysis, such data does not exist.","In this project, we develop and evaluate two Natural Language Processing (NLP) based techniques to address this issue.","First, we curate a custom dataset specific to the domain of financial text data on developing countries and explore multiple approaches for information extraction.","We then explore a text-to-text approach with the transformer-based T5 model with the goal of undertaking simultaneous NER and relation extraction.","We find that this model is able to learn the custom text structure output data corresponding to the entities and their relations, resulting in an accuracy of 92.44\\%, a precision of 68.25\\% and a recall of 54.20\\% from our best T5 model on the combined task.","Secondly, we explore an approach with sequential NER and relation extration.","For the NER, we run pre-trained and fine-tuned models using SpaCy, and we develop a custom relation extraction model using SpaCy's Dependency Parser output and some heuristics to determine entity relationships \\cite{spacy}.","We obtain an accuracy of 84.72\\%, a precision of 6.06\\% and a recall of 5.57\\% on this sequential task."],"url":"http://arxiv.org/abs/2403.09077v1","category":"cs.CL"}
{"created":"2024-03-14 03:21:33","title":"Dyadic Interaction Modeling for Social Behavior Generation","abstract":"Human-human communication is like a delicate dance where listeners and speakers concurrently interact to maintain conversational dynamics. Hence, an effective model for generating listener nonverbal behaviors requires understanding the dyadic context and interaction. In this paper, we present an effective framework for creating 3D facial motions in dyadic interactions. Existing work consider a listener as a reactive agent with reflexive behaviors to the speaker's voice and facial motions. The heart of our framework is Dyadic Interaction Modeling (DIM), a pre-training approach that jointly models speakers' and listeners' motions through masking and contrastive learning to learn representations that capture the dyadic context. To enable the generation of non-deterministic behaviors, we encode both listener and speaker motions into discrete latent representations, through VQ-VAE. The pre-trained model is further fine-tuned for motion generation. Extensive experiments demonstrate the superiority of our framework in generating listener motions, establishing a new state-of-the-art according to the quantitative measures capturing the diversity and realism of generated motions. Qualitative results demonstrate the superior capabilities of the proposed approach in generating diverse and realistic expressions, eye blinks and head gestures.","sentences":["Human-human communication is like a delicate dance where listeners and speakers concurrently interact to maintain conversational dynamics.","Hence, an effective model for generating listener nonverbal behaviors requires understanding the dyadic context and interaction.","In this paper, we present an effective framework for creating 3D facial motions in dyadic interactions.","Existing work consider a listener as a reactive agent with reflexive behaviors to the speaker's voice and facial motions.","The heart of our framework is Dyadic Interaction Modeling (DIM), a pre-training approach that jointly models speakers' and listeners' motions through masking and contrastive learning to learn representations that capture the dyadic context.","To enable the generation of non-deterministic behaviors, we encode both listener and speaker motions into discrete latent representations, through VQ-VAE.","The pre-trained model is further fine-tuned for motion generation.","Extensive experiments demonstrate the superiority of our framework in generating listener motions, establishing a new state-of-the-art according to the quantitative measures capturing the diversity and realism of generated motions.","Qualitative results demonstrate the superior capabilities of the proposed approach in generating diverse and realistic expressions, eye blinks and head gestures."],"url":"http://arxiv.org/abs/2403.09069v1","category":"cs.CV"}
{"created":"2024-03-14 03:13:01","title":"Hyperparameters in Continual Learning: a Reality Check","abstract":"Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datasets but share the same CL scenario. In the Hyperparameter Tuning phase, each algorithm is iteratively trained with different hyperparameter values to find the optimal hyperparameter values. Subsequently, in the Evaluation phase, the optimal hyperparameter values is directly applied for training each algorithm, and their performance in the Evaluation phase serves as the criterion for evaluating them. Through experiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in class-incremental learning, we not only observed that the existing evaluation method fail to properly assess the CL capability of each algorithm but also observe that some recently proposed state-of-the-art algorithms, which reported superior performance, actually exhibit inferior performance compared to the previous algorithm.","sentences":["Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process.","To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential.","As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset.","Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm.","In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm.","Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases.","Those phases consist of different datasets but share the same CL scenario.","In the Hyperparameter Tuning phase, each algorithm is iteratively trained with different hyperparameter values to find the optimal hyperparameter values.","Subsequently, in the Evaluation phase, the optimal hyperparameter values is directly applied for training each algorithm, and their performance in the Evaluation phase serves as the criterion for evaluating them.","Through experiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in class-incremental learning, we not only observed that the existing evaluation method fail to properly assess the CL capability of each algorithm but also observe that some recently proposed state-of-the-art algorithms, which reported superior performance, actually exhibit inferior performance compared to the previous algorithm."],"url":"http://arxiv.org/abs/2403.09066v1","category":"cs.LG"}
{"created":"2024-03-14 02:36:16","title":"Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains","abstract":"Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and preserve local data privacy. The $\\alpha$-sparsity prototype loss aligns samples from underrepresented domains, enhancing intra-class similarity and reducing inter-class similarity. Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method's superiority over existing approaches.","sentences":["Federated learning (FL) allows collaborative machine learning training without sharing private data.","While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains.","Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization.","However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions.","To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\\alpha$-sparsity prototype loss.","The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and preserve local data privacy.","The $\\alpha$-sparsity prototype loss aligns samples from underrepresented domains, enhancing intra-class similarity and reducing inter-class similarity.","Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method's superiority over existing approaches."],"url":"http://arxiv.org/abs/2403.09048v1","category":"cs.LG"}
{"created":"2024-03-14 02:28:29","title":"How do Older Adults Set Up Voice Assistants? Lessons Learned from a Deployment Experience for Older Adults to Set Up Standalone Voice Assistants","abstract":"While standalone Voice Assistants (VAs) are promising to support older adults' daily routine and wellbeing management, onboarding and setting up these devices can be challenging. Although some older adults choose to seek assistance from technicians and adult children, easy set up processes that facilitate independent use are still critical, especially for those who do not have access to external resources. We aim to understand the older adults' experience while setting up commercially available voice-only and voice-first screen-based VAs. Rooted in participants observations and semi-structured interviews, we designed a within-subject study with 10 older adults using Amazon Echo Dot and Echo Show. We identified the values of the built-in touchscreen and the instruction documents, as well as the impact of form factors, and outline important directions to support older adult independence with VAs.","sentences":["While standalone Voice Assistants (VAs) are promising to support older adults' daily routine and wellbeing management, onboarding and setting up these devices can be challenging.","Although some older adults choose to seek assistance from technicians and adult children, easy set up processes that facilitate independent use are still critical, especially for those who do not have access to external resources.","We aim to understand the older adults' experience while setting up commercially available voice-only and voice-first screen-based VAs.","Rooted in participants observations and semi-structured interviews, we designed a within-subject study with 10 older adults using Amazon Echo Dot and Echo Show.","We identified the values of the built-in touchscreen and the instruction documents, as well as the impact of form factors, and outline important directions to support older adult independence with VAs."],"url":"http://arxiv.org/abs/2403.09043v1","category":"cs.HC"}
{"created":"2024-03-13 23:27:31","title":"NTIRE 2023 Image Shadow Removal Challenge Technical Report: Team IIM_TTI","abstract":"In this paper, we analyze and discuss ShadowFormer in preparation for the NTIRE2023 Shadow Removal Challenge [1], implementing five key improvements: image alignment, the introduction of a perceptual quality loss function, the semi-automatic annotation for shadow detection, joint learning of shadow detection and removal, and the introduction of new data augmentation techniques for shadow removal. Our method achieved scores of 0.196 (3rd out of 19) in LPIPS and 7.44 (3rd out of 19) in the Mean Opinion Score (MOS).","sentences":["In this paper, we analyze and discuss ShadowFormer in preparation for the NTIRE2023 Shadow Removal Challenge [1], implementing five key improvements: image alignment, the introduction of a perceptual quality loss function, the semi-automatic annotation for shadow detection, joint learning of shadow detection and removal, and the introduction of new data augmentation techniques for shadow removal.","Our method achieved scores of 0.196 (3rd out of 19) in LPIPS and 7.44 (3rd out of 19) in the Mean Opinion Score (MOS)."],"url":"http://arxiv.org/abs/2403.08995v1","category":"cs.CV"}
{"created":"2024-03-13 20:57:10","title":"scVGAE: A Novel Approach using ZINB-Based Variational Graph Autoencoder for Single-Cell RNA-Seq Imputation","abstract":"Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to study individual cellular distinctions and uncover unique cell characteristics. However, a significant technical challenge in scRNA-seq analysis is the occurrence of \"dropout\" events, where certain gene expressions cannot be detected. This issue is particularly pronounced in genes with low or sparse expression levels, impacting the precision and interpretability of the obtained data. To address this challenge, various imputation methods have been implemented to predict such missing values, aiming to enhance the analysis's accuracy and usefulness. A prevailing hypothesis posits that scRNA-seq data conforms to a zero-inflated negative binomial (ZINB) distribution. Consequently, methods have been developed to model the data according to this distribution. Recent trends in scRNA-seq analysis have seen the emergence of deep learning approaches. Some techniques, such as the variational autoencoder, incorporate the ZINB distribution as a model loss function. Graph-based methods like Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) have also gained attention as deep learning methodologies for scRNA-seq analysis. This study introduces scVGAE, an innovative approach integrating GCN into a variational autoencoder framework while utilizing a ZINB loss function. This integration presents a promising avenue for effectively addressing dropout events in scRNA-seq data, thereby enhancing the accuracy and reliability of downstream analyses. scVGAE outperforms other methods in cell clustering, with the best performance in 11 out of 14 datasets. Ablation study shows all components of scVGAE are necessary. scVGAE is implemented in Python and downloadable at https://github.com/inoue0426/scVGAE.","sentences":["Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to study individual cellular distinctions and uncover unique cell characteristics.","However, a significant technical challenge in scRNA-seq analysis is the occurrence of \"dropout\" events, where certain gene expressions cannot be detected.","This issue is particularly pronounced in genes with low or sparse expression levels, impacting the precision and interpretability of the obtained data.","To address this challenge, various imputation methods have been implemented to predict such missing values, aiming to enhance the analysis's accuracy and usefulness.","A prevailing hypothesis posits that scRNA-seq data conforms to a zero-inflated negative binomial (ZINB) distribution.","Consequently, methods have been developed to model the data according to this distribution.","Recent trends in scRNA-seq analysis have seen the emergence of deep learning approaches.","Some techniques, such as the variational autoencoder, incorporate the ZINB distribution as a model loss function.","Graph-based methods like Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) have also gained attention as deep learning methodologies for scRNA-seq analysis.","This study introduces scVGAE, an innovative approach integrating GCN into a variational autoencoder framework while utilizing a ZINB loss function.","This integration presents a promising avenue for effectively addressing dropout events in scRNA-seq data, thereby enhancing the accuracy and reliability of downstream analyses.","scVGAE outperforms other methods in cell clustering, with the best performance in 11 out of 14 datasets.","Ablation study shows all components of scVGAE are necessary.","scVGAE is implemented in Python and downloadable at https://github.com/inoue0426/scVGAE."],"url":"http://arxiv.org/abs/2403.08959v1","category":"q-bio.GN"}
{"created":"2024-03-13 20:26:50","title":"Robust COVID-19 Detection in CT Images with CLIP","abstract":"In the realm of medical imaging, particularly for COVID-19 detection, deep learning models face substantial challenges such as the necessity for extensive computational resources, the paucity of well-annotated datasets, and a significant amount of unlabeled data. In this work, we introduce the first lightweight detector designed to overcome these obstacles, leveraging a frozen CLIP image encoder and a trainable multilayer perception (MLP). Enhanced with Conditional Value at Risk (CVaR) for robustness and a loss landscape flattening strategy for improved generalization, our model is tailored for high efficacy in COVID-19 detection. Furthermore, we integrate a teacher-student framework to capitalize on the vast amounts of unlabeled data, enabling our model to achieve superior performance despite the inherent data limitations. Experimental results on the COV19-CT-DB dataset demonstrate the effectiveness of our approach, surpassing baseline by up to 10.6% in `macro' F1 score in supervised learning. The code is available at https://github.com/Purdue-M2/COVID-19_Detection_M2_PURDUE.","sentences":["In the realm of medical imaging, particularly for COVID-19 detection, deep learning models face substantial challenges such as the necessity for extensive computational resources, the paucity of well-annotated datasets, and a significant amount of unlabeled data.","In this work, we introduce the first lightweight detector designed to overcome these obstacles, leveraging a frozen CLIP image encoder and a trainable multilayer perception (MLP).","Enhanced with Conditional Value at Risk (CVaR) for robustness and a loss landscape flattening strategy for improved generalization, our model is tailored for high efficacy in COVID-19 detection.","Furthermore, we integrate a teacher-student framework to capitalize on the vast amounts of unlabeled data, enabling our model to achieve superior performance despite the inherent data limitations.","Experimental results on the COV19-CT-DB dataset demonstrate the effectiveness of our approach, surpassing baseline by up to 10.6% in `macro' F1 score in supervised learning.","The code is available at https://github.com/Purdue-M2/COVID-19_Detection_M2_PURDUE."],"url":"http://arxiv.org/abs/2403.08947v1","category":"eess.IV"}
{"created":"2024-03-13 20:25:27","title":"Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era","abstract":"Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the \"black box\" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems, and (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.","sentences":["Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models.","Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency.","This extension calls for a significant transformation in XAI methodologies because of two reasons.","First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities.","Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the \"black box\" to actively enhancing the productivity and applicability of LLMs in real-world settings.","Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI.","Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems, and (2) how LLMs can contribute to the advancement of XAI.","We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges.","We also provide case studies to demonstrate how to obtain and leverage explanations.","The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM."],"url":"http://arxiv.org/abs/2403.08946v1","category":"cs.LG"}
{"created":"2024-03-13 20:16:21","title":"Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders","abstract":"Inference for Variational Autoencoders (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data. The two components are learned jointly via a lower bound to the generative model's log marginal likelihood. In early phases of joint training, the inference model poorly approximates the latent code posteriors. Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model. As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model. Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative to joint training for speed. Here, we suggest an inference method that trains the generative and inference models independently. It approximates the posterior of the true model a priori; fixing this posterior approximation, we then maximize the lower bound relative to only the generative model. By conventional wisdom, this approach should rely on the true prior and likelihood of the true model to approximate its posterior (which are unknown). However, we show that we can compute a deterministic, model-agnostic posterior approximation (MAPA) of the true model's posterior. We then use MAPA to develop a proof-of-concept inference method. We present preliminary results on low-dimensional synthetic data that (1) MAPA captures the trend of the true posterior, and (2) our MAPA-based inference performs better density estimation with less computation than baselines. Lastly, we present a roadmap for scaling the MAPA-based inference method to high-dimensional data.","sentences":["Inference for Variational Autoencoders (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data.","The two components are learned jointly via a lower bound to the generative model's log marginal likelihood.","In early phases of joint training, the inference model poorly approximates the latent code posteriors.","Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model.","As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model.","Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative to joint training for speed.","Here, we suggest an inference method that trains the generative and inference models independently.","It approximates the posterior of the true model a priori; fixing this posterior approximation, we then maximize the lower bound relative to only the generative model.","By conventional wisdom, this approach should rely on the true prior and likelihood of the true model to approximate its posterior (which are unknown).","However, we show that we can compute a deterministic, model-agnostic posterior approximation (MAPA) of the true model's posterior.","We then use MAPA to develop a proof-of-concept inference method.","We present preliminary results on low-dimensional synthetic data that (1) MAPA captures the trend of the true posterior, and (2) our MAPA-based inference performs better density estimation with less computation than baselines.","Lastly, we present a roadmap for scaling the MAPA-based inference method to high-dimensional data."],"url":"http://arxiv.org/abs/2403.08941v1","category":"stat.ML"}
{"created":"2024-03-14 17:54:21","title":"Dynamically accelerating the power iteration with momentum","abstract":"In this paper, we propose, analyze and demonstrate a dynamic momentum method to accelerate power and inverse power iterations with minimal computational overhead. The method is appropriate for real, diagonalizable matrices, and does not require a priori spectral knowledge. We review and extend background results on previously developed static momentum accelerations for the power iteration through the connection between the momentum accelerated iteration and the standard power iteration applied to an augmented matrix. We show that the augmented matrix is defective for the optimal parameter choice. We then present our dynamic method which updates the momentum parameter at each iteration based on the Rayleigh quotient and two previous residuals. We present convergence and stability theory for the method by considering a power-like method consisting of multiplying an initial vector by a sequence of augmented matrices. We demonstrate the developed method on a number of benchmark problems, and see that it outperforms both the power iteration and often the static momentum acceleration with optimal parameter choice. Finally, we present and demonstrate an explicit extension of the algorithm to inverse power iterations.","sentences":["In this paper, we propose, analyze and demonstrate a dynamic momentum method to accelerate power and inverse power iterations with minimal computational overhead.","The method is appropriate for real, diagonalizable matrices, and does not require a priori spectral knowledge.","We review and extend background results on previously developed static momentum accelerations for the power iteration through the connection between the momentum accelerated iteration and the standard power iteration applied to an augmented matrix.","We show that the augmented matrix is defective for the optimal parameter choice.","We then present our dynamic method which updates the momentum parameter at each iteration based on the Rayleigh quotient and two previous residuals.","We present convergence and stability theory for the method by considering a power-like method consisting of multiplying an initial vector by a sequence of augmented matrices.","We demonstrate the developed method on a number of benchmark problems, and see that it outperforms both the power iteration and often the static momentum acceleration with optimal parameter choice.","Finally, we present and demonstrate an explicit extension of the algorithm to inverse power iterations."],"url":"http://arxiv.org/abs/2403.09618v1","category":"math.NA"}
{"created":"2024-03-14 16:02:27","title":"Efficient Tensor Networks for Control-Enhanced Quantum Metrology","abstract":"We propose efficient tensor network algorithms for optimizing control-enhanced sequential strategies in estimating a large number of quantum channels, with no ancilla or bounded ancilla. Our first approach allows for applying $N-1$ arbitrary interleaved control operations between the $N$ channels to estimate, and the second approach restricts all control operations to be identical, which could further facilitate simpler experimental demonstration. The numerical experiments show that our algorithm has a good performance in optimizing the metrological protocol for $N=100$ single-qubit and two-qubit channels. In particular, our algorithm identifies a strategy that can outperform the asymptotically optimal quantum error correction protocol when $N$ is finite but large.","sentences":["We propose efficient tensor network algorithms for optimizing control-enhanced sequential strategies in estimating a large number of quantum channels, with no ancilla or bounded ancilla.","Our first approach allows for applying $N-1$ arbitrary interleaved control operations between the $N$ channels to estimate, and the second approach restricts all control operations to be identical, which could further facilitate simpler experimental demonstration.","The numerical experiments show that our algorithm has a good performance in optimizing the metrological protocol for $N=100$ single-qubit and two-qubit channels.","In particular, our algorithm identifies a strategy that can outperform the asymptotically optimal quantum error correction protocol when $N$ is finite but large."],"url":"http://arxiv.org/abs/2403.09519v1","category":"quant-ph"}
{"created":"2024-03-14 15:58:00","title":"Quantum Fourier Transform using Dynamic Circuits","abstract":"In dynamic quantum circuits, classical information from mid-circuit measurements is fed forward during circuit execution. This emerging capability of quantum computers confers numerous advantages that can enable more efficient and powerful protocols by drastically reducing the resource requirements for certain core algorithmic primitives. In particular, in the case of the $n$-qubit quantum Fourier transform followed immediately by measurement, the scaling of resource requirements is reduced from $O(n^2)$ two-qubit gates in an all-to-all connectivity in the standard unitary formulation to $O(n)$ mid-circuit measurements in its dynamic counterpart without any connectivity constraints. Here, we demonstrate the advantage of dynamic quantum circuits for the quantum Fourier transform on IBM's superconducting quantum hardware with certified process fidelities of $>50\\%$ on up to $16$ qubits and $>1\\%$ on up to $37$ qubits, exceeding previous reports across all quantum computing platforms. These results are enabled by our contribution of an efficient method for certifying the process fidelity, as well as of a dynamical decoupling protocol for error suppression during mid-circuit measurements and feed-forward within a dynamic quantum circuit. Our results demonstrate the advantages of leveraging dynamic circuits in optimizing the compilation of quantum algorithms.","sentences":["In dynamic quantum circuits, classical information from mid-circuit measurements is fed forward during circuit execution.","This emerging capability of quantum computers confers numerous advantages that can enable more efficient and powerful protocols by drastically reducing the resource requirements for certain core algorithmic primitives.","In particular, in the case of the $n$-qubit quantum Fourier transform followed immediately by measurement, the scaling of resource requirements is reduced from $O(n^2)$ two-qubit gates in an all-to-all connectivity in the standard unitary formulation to $O(n)$ mid-circuit measurements in its dynamic counterpart without any connectivity constraints.","Here, we demonstrate the advantage of dynamic quantum circuits for the quantum Fourier transform on IBM's superconducting quantum hardware with certified process fidelities of $>50\\%$ on up to $16$ qubits and $>1\\%$ on up to $37$ qubits, exceeding previous reports across all quantum computing platforms.","These results are enabled by our contribution of an efficient method for certifying the process fidelity, as well as of a dynamical decoupling protocol for error suppression during mid-circuit measurements and feed-forward within a dynamic quantum circuit.","Our results demonstrate the advantages of leveraging dynamic circuits in optimizing the compilation of quantum algorithms."],"url":"http://arxiv.org/abs/2403.09514v1","category":"quant-ph"}
{"created":"2024-03-14 15:35:30","title":"Layer 2 be or Layer not 2 be: Scaling on Uniswap v3","abstract":"This paper studies the market structure impact of cheaper and faster chains on the Uniswap v3 Protocol. The Uniswap Protocol is the largest decentralized application on Ethereum by both gas and blockspace used, and user behaviors of the protocol are very sensitive to fluctuations in gas prices and market structure due to the economic factors of the Protocol. We focus on the chains where Uniswap v3 has the most activity, giving us the best comparison to Ethereum mainnet. Because of cheaper gas and lower block times, we find evidence that the majority of swaps get better gas-adjusted execution on these chains, liquidity providers are more capital efficient, and liquidity providers have increased fee returns from more arbitrage. We also present evidence that two second block times may be too long for optimal liquidity provider returns, compared to first come, first served. We argue that many of the current drawbacks with AMMs may be due to chain dynamics and are vastly improved with cheaper and faster transactions","sentences":["This paper studies the market structure impact of cheaper and faster chains on the Uniswap v3 Protocol.","The Uniswap Protocol is the largest decentralized application on Ethereum by both gas and blockspace used, and user behaviors of the protocol are very sensitive to fluctuations in gas prices and market structure due to the economic factors of the Protocol.","We focus on the chains where Uniswap v3 has the most activity, giving us the best comparison to Ethereum mainnet.","Because of cheaper gas and lower block times, we find evidence that the majority of swaps get better gas-adjusted execution on these chains, liquidity providers are more capital efficient, and liquidity providers have increased fee returns from more arbitrage.","We also present evidence that two second block times may be too long for optimal liquidity provider returns, compared to first come, first served.","We argue that many of the current drawbacks with AMMs may be due to chain dynamics and are vastly improved with cheaper and faster transactions"],"url":"http://arxiv.org/abs/2403.09494v1","category":"q-fin.TR"}
{"created":"2024-03-14 14:04:21","title":"Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting","abstract":"3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When trained with randomly initialized point clouds, 3DGS fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting), that successfully trains 3D Gaussians from random point clouds. We show the effectiveness of our strategy through quantitative and qualitative comparisons on multiple datasets, largely improving the performance in all settings. Our project page and code can be found at https://ku-cvlab.github.io/RAIN-GS.","sentences":["3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction.","However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods.","When trained with randomly initialized point clouds, 3DGS fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR.","Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting), that successfully trains 3D Gaussians from random point clouds.","We show the effectiveness of our strategy through quantitative and qualitative comparisons on multiple datasets, largely improving the performance in all settings.","Our project page and code can be found at https://ku-cvlab.github.io/RAIN-GS."],"url":"http://arxiv.org/abs/2403.09413v1","category":"cs.CV"}
{"created":"2024-03-14 12:18:17","title":"On ultrafilters in ZF models and indecomposable ultrafilters","abstract":"We use indecomposable ultrafilters to answer some questions of Hayut, Karagila paper \"Spectra of uniformity\". It is shown that the bound on the strength by T. Usuba \"A note on uniform ultrafilters in choiceless context\" is optimal.","sentences":["We use indecomposable ultrafilters to answer some questions of Hayut, Karagila paper \"Spectra of uniformity\".","It is shown that the bound on the strength by T. Usuba \"A note on uniform ultrafilters in choiceless context\" is optimal."],"url":"http://arxiv.org/abs/2403.09329v1","category":"math.LO"}
{"created":"2024-03-14 10:44:46","title":"Injection and capture of antiprotons in a Penning-Malmberg trap using a drift tube accelerator and degrader foil","abstract":"The Antiproton Decelerator (AD) at CERN provides antiproton bunches with a kinetic energy of 5.3 MeV. The Extra-Low ENergy Antiproton ring at CERN, commissioned at the AD in 2018, now supplies a bunch of electron-cooled antiprotons at a fixed energy of 100 keV. The MUSASHI antiproton trap was upgraded by replacing the radio-frequency quadrupole decelerator with a pulsed drift tube to re-accelerate antiprotons and optimize the injection energy into the degrader foils. By increasing the beam energy to 119 keV, a cooled antiproton accumulation efficiency of (26 +- 6)% was achieved.","sentences":["The Antiproton Decelerator (AD) at CERN provides antiproton bunches with a kinetic energy of 5.3 MeV.","The Extra-Low ENergy Antiproton ring at CERN, commissioned at the AD in 2018, now supplies a bunch of electron-cooled antiprotons at a fixed energy of 100 keV.","The MUSASHI antiproton trap was upgraded by replacing the radio-frequency quadrupole decelerator with a pulsed drift tube to re-accelerate antiprotons and optimize the injection energy into the degrader foils.","By increasing the beam energy to 119 keV, a cooled antiproton accumulation efficiency of (26 +- 6)% was achieved."],"url":"http://arxiv.org/abs/2403.09268v1","category":"physics.acc-ph"}
{"created":"2024-03-14 10:18:42","title":"Ahlfors regularity of continua that minimize maxitive set functions","abstract":"The primary objective of this paper is to establish the Ahlfors regularity of minimizers of set functions that satisfy a suitable maxitive condition on disjoint unions of sets. Our analysis focuses on minimizers within continua of the plane with finite one-dimensional Hausdorff measure. Through quantitative estimates, we prove that the length of a minimizer inside the ball centered at one of its points is comparable to the radius of the ball. By operating within an abstract framework, we are able to encompass a diverse range of entities, including spectral functionals defined in terms of the eigenvalues of elliptic operators, the inradius, and the maximum of the torsion function. These entities are of interest for several applications, such as structural engineering, urban planning, and quantum mechanics.","sentences":["The primary objective of this paper is to establish the Ahlfors regularity of minimizers of set functions that satisfy a suitable maxitive condition on disjoint unions of sets.","Our analysis focuses on minimizers within continua of the plane with finite one-dimensional Hausdorff measure.","Through quantitative estimates, we prove that the length of a minimizer inside the ball centered at one of its points is comparable to the radius of the ball.","By operating within an abstract framework, we are able to encompass a diverse range of entities, including spectral functionals defined in terms of the eigenvalues of elliptic operators, the inradius, and the maximum of the torsion function.","These entities are of interest for several applications, such as structural engineering, urban planning, and quantum mechanics."],"url":"http://arxiv.org/abs/2403.09251v1","category":"math.OC"}
{"created":"2024-03-14 09:28:16","title":"On Discrete Subproblems in Integer Optimal Control with Total Variation Regularization in Two Dimensions","abstract":"We analyze integer linear programs which we obtain after discretizing two-dimensional subproblems arising from a trust-region algorithm for mixed integer optimal control problems with total variation regularization. We discuss NP-hardness of the discretized problems and the connection to graph-based problems. We show that the underlying polyhedron exhibits structural restrictions in its vertices with regards to which variables can attain fractional values at the same time. Based on this property, we derive cutting planes by employing a relation to shortest-path and minimum bisection problems. We propose a branching rule and a primal heuristic which improves previously found feasible points. We validate the proposed tools with a numerical benchmark in a standard integer programming solver. We observe a significant speedup for medium-sized problems. Our results give hints for scaling towards larger instances in the future.","sentences":["We analyze integer linear programs which we obtain after discretizing two-dimensional subproblems arising from a trust-region algorithm for mixed integer optimal control problems with total variation regularization.","We discuss NP-hardness of the discretized problems and the connection to graph-based problems.","We show that the underlying polyhedron exhibits structural restrictions in its vertices with regards to which variables can attain fractional values at the same time.","Based on this property, we derive cutting planes by employing a relation to shortest-path and minimum bisection problems.","We propose a branching rule and a primal heuristic which improves previously found feasible points.","We validate the proposed tools with a numerical benchmark in a standard integer programming solver.","We observe a significant speedup for medium-sized problems.","Our results give hints for scaling towards larger instances in the future."],"url":"http://arxiv.org/abs/2403.09213v1","category":"math.OC"}
{"created":"2024-03-14 09:16:13","title":"Perspectives on physics-based one-dimensional modeling of lung physiology","abstract":"The need to understand how infection spreads to the deep lung was acutely realized during the Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2) pandemic. The challenge of modeling virus laden aerosol transport and deposition in the airways, coupled with mucus clearance, and infection kinetics, became evident. This perspective provides a consolidated view of coupled one-dimensional physics-based mathematical models to probe multifaceted aspects of lung physiology. Successes of 1D trumpet models in providing mechanistic insights into lung function and optimalities are reviewed while identifying limitations and future directions. Key non-dimensional numbers defining lung function are reported. The need to quantitatively map various pathologies on a physics-based parameter space of non-dimensional numbers (a virtual disease landscape) is noted with an eye on translating modeling to clinical practice. This could aid in disease diagnosis, get mechanistic insights into pathologies, and determine patient specific treatment plan. 1D modeling could be an important tool in developing novel measurement and analysis platforms that could be deployed at point-of-care.","sentences":["The need to understand how infection spreads to the deep lung was acutely realized during the Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2) pandemic.","The challenge of modeling virus laden aerosol transport and deposition in the airways, coupled with mucus clearance, and infection kinetics, became evident.","This perspective provides a consolidated view of coupled one-dimensional physics-based mathematical models to probe multifaceted aspects of lung physiology.","Successes of 1D trumpet models in providing mechanistic insights into lung function and optimalities are reviewed while identifying limitations and future directions.","Key non-dimensional numbers defining lung function are reported.","The need to quantitatively map various pathologies on a physics-based parameter space of non-dimensional numbers (a virtual disease landscape) is noted with an eye on translating modeling to clinical practice.","This could aid in disease diagnosis, get mechanistic insights into pathologies, and determine patient specific treatment plan.","1D modeling could be an important tool in developing novel measurement and analysis platforms that could be deployed at point-of-care."],"url":"http://arxiv.org/abs/2403.09203v1","category":"physics.bio-ph"}
{"created":"2024-03-14 08:06:27","title":"News Media as Suppliers of Narratives (and Information)","abstract":"We present a model of news media that shape consumer beliefs by providing information (signals about an exogenous state) and narratives (models of what determines outcomes). To amplify consumers' engagement, media maximize consumers' anticipatory utility. Focusing on a class of separable consumer preferences, we show that a monopolistic media platform facing homogenous consumers provides a false \"empowering\" narrative coupled with an optimistically biased signal. Consumer heterogeneity gives rise to a novel menu-design problem due to a \"data externality\" among consumers. The optimal menu features multiple narratives and creates polarized beliefs. These effects also arise in a competitive media market model.","sentences":["We present a model of news media that shape consumer beliefs by providing information (signals about an exogenous state) and narratives (models of what determines outcomes).","To amplify consumers' engagement, media maximize consumers' anticipatory utility.","Focusing on a class of separable consumer preferences, we show that a monopolistic media platform facing homogenous consumers provides a false \"empowering\" narrative coupled with an optimistically biased signal.","Consumer heterogeneity gives rise to a novel menu-design problem due to a \"data externality\" among consumers.","The optimal menu features multiple narratives and creates polarized beliefs.","These effects also arise in a competitive media market model."],"url":"http://arxiv.org/abs/2403.09155v1","category":"econ.TH"}
{"created":"2024-03-14 05:36:48","title":"Effects of Structural Variations to X-ray Absorption Spectra of g-C$_3$N$_4$: Insights from DFT and TDDFT Simulations","abstract":"X-ray absorption spectroscopy (XAS) is widely employed for structure characterization of graphitic carbon nitride (g-C$_3$N$_4$) and its composites. Nevertheless, even for pure g-C$_3$N$_4$, discrepancies in energy and profile exist across different experiments, which can be attributed to variations in structures arising from diverse synthesis conditions and calibration procedures. Here, we conducted a theoretical investigation on XAS of three representative g-C$_3$N$_4$ structures (planar, corrugated, and micro-corrugated) optimized with different strategies, to understand the structure-spectroscopy relation. Different methods were compared, including density functional theory (DFT) with the full (FCH) or equivalent (ECH) core-hole approximation, as well as the time-dependent DFT (TDDFT). FCH was responsible for getting accurate absolute absorption energy; while ECH and TDDFT aided in interpreting the spectra, through ECH-state canonical molecular orbitals (ECH-CMOs) and natural transition orbitals (NTOs), respectively. With each method, the spectra at the three structures show evident differences, which can be correlated to different individual experiments or in between. Our calculations explained the structural reason behind the spectral discrepancies among different experiments. Moreover, profiles predicted by these methods also displayed consistency, so their differences can be used as a reliable indicator of their accuracy. Both ECH-CMOs and NTO particle orbitals led to similar graphics, validating their applicability in interpreting the transitions. This work provides a comprehensive analysis of the structure-XAS relation for g-C$_3$N$_4$, provides concrete explanations for the spectral differences reported in various experiments, and offers insight for future structure dynamical and transient X-ray spectral analyses.","sentences":["X-ray absorption spectroscopy (XAS) is widely employed for structure characterization of graphitic carbon nitride (g-C$_3$N$_4$) and its composites.","Nevertheless, even for pure g-C$_3$N$_4$, discrepancies in energy and profile exist across different experiments, which can be attributed to variations in structures arising from diverse synthesis conditions and calibration procedures.","Here, we conducted a theoretical investigation on XAS of three representative g-C$_3$N$_4$ structures (planar, corrugated, and micro-corrugated) optimized with different strategies, to understand the structure-spectroscopy relation.","Different methods were compared, including density functional theory (DFT) with the full (FCH) or equivalent (ECH) core-hole approximation, as well as the time-dependent DFT (TDDFT).","FCH was responsible for getting accurate absolute absorption energy; while ECH and TDDFT aided in interpreting the spectra, through ECH-state canonical molecular orbitals (ECH-CMOs) and natural transition orbitals (NTOs), respectively.","With each method, the spectra at the three structures show evident differences, which can be correlated to different individual experiments or in between.","Our calculations explained the structural reason behind the spectral discrepancies among different experiments.","Moreover, profiles predicted by these methods also displayed consistency, so their differences can be used as a reliable indicator of their accuracy.","Both ECH-CMOs and NTO particle orbitals led to similar graphics, validating their applicability in interpreting the transitions.","This work provides a comprehensive analysis of the structure-XAS relation for g-C$_3$N$_4$, provides concrete explanations for the spectral differences reported in various experiments, and offers insight for future structure dynamical and transient X-ray spectral analyses."],"url":"http://arxiv.org/abs/2403.09115v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 05:19:47","title":"Spin-Orbit Coupled Insulators and Metals on the Verge of Kitaev Spin Liquids in Ilmenite Heterostructures","abstract":"Competition and cooperation between electron correlation and relativistic spin-orbit coupling give rise to diverse exotic quantum phenomena in solids. An illustrative example is spin-orbit entangled quantum liquids, which exhibit remarkable features such as topological orders and fractional excitations. The Kitaev honeycomb model realizes such interesting states, called the Kitaev spin liquids, but its experimental feasibility is still challenging. Here we theoretically investigate hexagonal heterostructures including a candidate for the Kitaev magnets, MgIrO$_3$, to actively manipulate the electronic and magnetic properties toward realizing the Kitaev spin liquids. For three different structure types of ilmenite bilayers MgIrO$_3$/$A$TiO$_3$ with $A$ = Mn, Fe, Co, and Ni, we obtain the optimized lattice structures, the electronic band structures, the stable magnetic orders, and the effective magnetic couplings. We find that the spin-orbital coupled bands characterized by the pseudospin $j_{\\rm eff}=$ 1/2 are retained in the MgIrO$_3$ layer for all the heterostructures, but the magnetic state and the band gap depend on the types of heterostructures as well as the $A$ atoms. In particular, one type becomes metallic irrespective of $A$, while the other two are mostly insulating. We show that the insulating cases provide spin-orbit coupled Mott insulating states with dominant Kitaev-type interactions, accompanied by different combinations of subdominant interactions depending on the heterostructural type and $A$, while the metallic cases realize spin-orbit coupled metals with various doping rates. Our results indicate that these hexagonal heterostructures are a good platform for engineering electronic and magnetic properties of the spin-orbital coupled correlated materials, including the possibility of Majorana Fermi surfaces and topological superconductivity.","sentences":["Competition and cooperation between electron correlation and relativistic spin-orbit coupling give rise to diverse exotic quantum phenomena in solids.","An illustrative example is spin-orbit entangled quantum liquids, which exhibit remarkable features such as topological orders and fractional excitations.","The Kitaev honeycomb model realizes such interesting states, called the Kitaev spin liquids, but its experimental feasibility is still challenging.","Here we theoretically investigate hexagonal heterostructures including a candidate for the Kitaev magnets, MgIrO$_3$, to actively manipulate the electronic and magnetic properties toward realizing the Kitaev spin liquids.","For three different structure types of ilmenite bilayers MgIrO$_3$/$A$TiO$_3$ with $A$ = Mn, Fe, Co, and Ni, we obtain the optimized lattice structures, the electronic band structures, the stable magnetic orders, and the effective magnetic couplings.","We find that the spin-orbital coupled bands characterized by the pseudospin $j_{\\rm eff}=$ 1/2 are retained in the MgIrO$_3$ layer for all the heterostructures, but the magnetic state and the band gap depend on the types of heterostructures as well as the $A$ atoms.","In particular, one type becomes metallic irrespective of $A$, while the other two are mostly insulating.","We show that the insulating cases provide spin-orbit coupled Mott insulating states with dominant Kitaev-type interactions, accompanied by different combinations of subdominant interactions depending on the heterostructural type and $A$, while the metallic cases realize spin-orbit coupled metals with various doping rates.","Our results indicate that these hexagonal heterostructures are a good platform for engineering electronic and magnetic properties of the spin-orbital coupled correlated materials, including the possibility of Majorana Fermi surfaces and topological superconductivity."],"url":"http://arxiv.org/abs/2403.09112v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 02:56:42","title":"Query Rewriting via Large Language Models","abstract":"Query rewriting is one of the most effective techniques for coping with poorly written queries before passing them down to the query optimizer. Manual rewriting is not scalable, as it is error-prone and requires deep expertise. Similarly, traditional query rewriting algorithms can only handle a small subset of queries: rule-based techniques do not generalize to new query patterns and synthesis-based techniques cannot handle complex queries. Fortunately, the rise of Large Language Models (LLMs), equipped with broad general knowledge and advanced reasoning capabilities, has created hopes for solving some of these previously open problems.   In this paper, we present GenRewrite, the first holistic system that leverages LLMs for query rewriting. We introduce the notion of Natural Language Rewrite Rules (NLR2s), and use them as hints to the LLM but also a means for transferring knowledge from rewriting one query to another, and thus becoming smarter and more effective over time. We present a novel counterexample-guided technique that iteratively corrects the syntactic and semantic errors in the rewritten query, significantly reducing the LLM costs and the manual effort required for verification. GenRewrite speeds up 22 out of 99 TPC queries (the most complex public benchmark) by more than 2x, which is 2.5x--3.2x higher coverage than state-of-the-art traditional query rewriting and 2.1x higher than the out-of-the-box LLM baseline.","sentences":["Query rewriting is one of the most effective techniques for coping with poorly written queries before passing them down to the query optimizer.","Manual rewriting is not scalable, as it is error-prone and requires deep expertise.","Similarly, traditional query rewriting algorithms can only handle a small subset of queries: rule-based techniques do not generalize to new query patterns and synthesis-based techniques cannot handle complex queries.","Fortunately, the rise of Large Language Models (LLMs), equipped with broad general knowledge and advanced reasoning capabilities, has created hopes for solving some of these previously open problems.   ","In this paper, we present GenRewrite, the first holistic system that leverages LLMs for query rewriting.","We introduce the notion of Natural Language Rewrite Rules (NLR2s), and use them as hints to the LLM but also a means for transferring knowledge from rewriting one query to another, and thus becoming smarter and more effective over time.","We present a novel counterexample-guided technique that iteratively corrects the syntactic and semantic errors in the rewritten query, significantly reducing the LLM costs and the manual effort required for verification.","GenRewrite speeds up 22 out of 99 TPC queries (the most complex public benchmark) by more than 2x, which is 2.5x--3.2x higher coverage than state-of-the-art traditional query rewriting and 2.1x higher than the out-of-the-box LLM baseline."],"url":"http://arxiv.org/abs/2403.09060v1","category":"cs.DB"}
{"created":"2024-03-14 02:27:55","title":"Microwave photonic transversal filters based on microcombs with feedback control","abstract":"Feedback control plays a crucial role in improving system accuracy and stability for a variety of scientific and engineering applications. Here, we theoretically and experimentally investigate the implementation of feedback control in microwave photonic (MWP) transversal filter systems based on optical microcomb sources, which offer advantages in achieving highly reconfigurable processing functions without requiring changes to hardware. We propose four different feedback control methods including (1) one stage spectral power reshaping, (2) one stage impulse response reshaping, (3) two stage spectral power reshaping, and (4) two stage synergic spectral power reshaping and impulse response reshaping. We experimentally implement these feedback control methods and compare their performance. The results show that the feedback control can significantly improve not only the accuracy of comb line shaping as well as temporal signal processing and spectral filtering, but also the systems long term stability. Finally, we discuss the current limitations and future prospects for optimizing feedback control in microcomb based MWP transversal filter systems implemented by both discrete components and integrated chips. Our results provide a comprehensive guide for the implementation of feedback control in microcomb based MWP filter systems in order to improve their performance for practical applications.","sentences":["Feedback control plays a crucial role in improving system accuracy and stability for a variety of scientific and engineering applications.","Here, we theoretically and experimentally investigate the implementation of feedback control in microwave photonic (MWP) transversal filter systems based on optical microcomb sources, which offer advantages in achieving highly reconfigurable processing functions without requiring changes to hardware.","We propose four different feedback control methods including (1) one stage spectral power reshaping, (2) one stage impulse response reshaping, (3) two stage spectral power reshaping, and (4) two stage synergic spectral power reshaping and impulse response reshaping.","We experimentally implement these feedback control methods and compare their performance.","The results show that the feedback control can significantly improve not only the accuracy of comb line shaping as well as temporal signal processing and spectral filtering, but also the systems long term stability.","Finally, we discuss the current limitations and future prospects for optimizing feedback control in microcomb based MWP transversal filter systems implemented by both discrete components and integrated chips.","Our results provide a comprehensive guide for the implementation of feedback control in microcomb based MWP filter systems in order to improve their performance for practical applications."],"url":"http://arxiv.org/abs/2403.09041v1","category":"physics.optics"}
{"created":"2024-03-14 02:26:31","title":"RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems","abstract":"Retrieval-augmented generation (RAG) greatly benefits language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA). Despite its potential, the power of RAG is highly dependent on its configuration, raising the question: What is the optimal RAG configuration? To answer this, we introduce the RAGGED framework to analyze and optimize RAG systems. On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that different models suit substantially varied RAG setups. While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use < 5 documents, despite often having a longer context window. RAGGED offers further insights into LMs' context utilization habits, where we find that encoder-decoder models rely more on contexts and are thus more sensitive to retrieval quality, while decoder-only models tend to rely on knowledge memorized during training.","sentences":["Retrieval-augmented generation (RAG) greatly benefits language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA).","Despite its potential, the power of RAG is highly dependent on its configuration, raising the question: What is the optimal RAG configuration?","To answer this, we introduce the RAGGED framework to analyze and optimize RAG systems.","On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures.","Through RAGGED, we uncover that different models suit substantially varied RAG setups.","While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use < 5 documents, despite often having a longer context window.","RAGGED offers further insights into LMs' context utilization habits, where we find that encoder-decoder models rely more on contexts and are thus more sensitive to retrieval quality, while decoder-only models tend to rely on knowledge memorized during training."],"url":"http://arxiv.org/abs/2403.09040v1","category":"cs.CL"}
{"created":"2024-03-14 02:21:01","title":"Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier","abstract":"In the real-world setting, data often follows a long-tailed distribution, where head classes contain significantly more training samples than tail classes. Consequently, models trained on such data tend to be biased toward head classes. The medium of this bias is imbalanced gradients, which include not only the ratio of scale between positive and negative gradients but also imbalanced gradients from different negative classes. Therefore, we propose the Gradient-Aware Logit Adjustment (GALA) loss, which adjusts the logits based on accumulated gradients to balance the optimization process. Additionally, We find that most of the solutions to long-tailed problems are still biased towards head classes in the end, and we propose a simple and post hoc prediction re-balancing strategy to further mitigate the basis toward head class. Extensive experiments are conducted on multiple popular long-tailed recognition benchmark datasets to evaluate the effectiveness of these two designs. Our approach achieves top-1 accuracy of 48.5\\%, 41.4\\%, and 73.3\\% on CIFAR100-LT, Places-LT, and iNaturalist, outperforming the state-of-the-art method GCL by a significant margin of 3.62\\%, 0.76\\% and 1.2\\%, respectively. Code is available at https://github.com/lt-project-repository/lt-project.","sentences":["In the real-world setting, data often follows a long-tailed distribution, where head classes contain significantly more training samples than tail classes.","Consequently, models trained on such data tend to be biased toward head classes.","The medium of this bias is imbalanced gradients, which include not only the ratio of scale between positive and negative gradients but also imbalanced gradients from different negative classes.","Therefore, we propose the Gradient-Aware Logit Adjustment (GALA) loss, which adjusts the logits based on accumulated gradients to balance the optimization process.","Additionally, We find that most of the solutions to long-tailed problems are still biased towards head classes in the end, and we propose a simple and post hoc prediction re-balancing strategy to further mitigate the basis toward head class.","Extensive experiments are conducted on multiple popular long-tailed recognition benchmark datasets to evaluate the effectiveness of these two designs.","Our approach achieves top-1 accuracy of 48.5\\%, 41.4\\%, and 73.3\\% on CIFAR100-LT, Places-LT, and iNaturalist, outperforming the state-of-the-art method GCL by a significant margin of 3.62\\%, 0.76\\% and 1.2\\%, respectively.","Code is available at https://github.com/lt-project-repository/lt-project."],"url":"http://arxiv.org/abs/2403.09036v1","category":"cs.CV"}
{"created":"2024-03-14 01:10:43","title":"Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting","abstract":"In this paper, we propose algorithms to minimize the energy consumption in millimeter wave/terahertz multi-user downlink communication systems. To ensure coverage in blockage-vulnerable high frequency systems, we consider cooperative rate-splitting (CRS) and transmission over multiple time blocks, where via CRS, multiple users cooperate to assist a blocked user. Moreover, we show that transmission over multiple time blocks provides benefits through smart resource allocation. We first propose a communication framework named improved distinct extraction-based CRS (iDeCRS) that utilizes the benefits of rate-splitting. With our transmission framework, we derive a performance benchmark assuming genie channel state information (CSI), i.e., the channels of the present and future time blocks are known, denoted as GENIE. Using the results from GENIE, we derive a novel efficiency constrained optimization (ECO) algorithm assuming instantaneous CSI. In addition, a simple but effective even data transmission (EDT) algorithm that promotes steady transmission along the time blocks is proposed. Simulation results show that ECO and EDT have satisfactory performances compared to GENIE. The results also show that ECO outperforms EDT when many users are cooperating, and vise versa.","sentences":["In this paper, we propose algorithms to minimize the energy consumption in millimeter wave/terahertz multi-user downlink communication systems.","To ensure coverage in blockage-vulnerable high frequency systems, we consider cooperative rate-splitting (CRS) and transmission over multiple time blocks, where via CRS, multiple users cooperate to assist a blocked user.","Moreover, we show that transmission over multiple time blocks provides benefits through smart resource allocation.","We first propose a communication framework named improved distinct extraction-based CRS (iDeCRS) that utilizes the benefits of rate-splitting.","With our transmission framework, we derive a performance benchmark assuming genie channel state information (CSI), i.e., the channels of the present and future time blocks are known, denoted as GENIE.","Using the results from GENIE, we derive a novel efficiency constrained optimization (ECO)","algorithm assuming instantaneous CSI.","In addition, a simple but effective even data transmission (EDT) algorithm that promotes steady transmission along the time blocks is proposed.","Simulation results show that ECO and EDT have satisfactory performances compared to GENIE.","The results also show that ECO outperforms EDT when many users are cooperating, and vise versa."],"url":"http://arxiv.org/abs/2403.09022v1","category":"cs.IT"}
{"created":"2024-03-14 00:06:30","title":"Fault Detection and Tolerant Control for Aero2 2D0F Two-rotor Helicopter","abstract":"Stability and satisfactory performance are critical control requirements for Unmanned Aerial Vehicle (UAV) applications. While conventional control systems for UAVs aim to ensure flight stability and safe operation while accomplishing tasks, UAVs may experience various flight faults that can degrade performance or, in severe cases, lead to instability. Unsatisfactory performance or instability of a UAV poses risks to lives, properties, and the flying environment. Therefore, it's essential to design a system capable of detecting faults, pinpointing their location, assessing their severity, and using this information to mitigate them, enabling the vehicle to continue operating satisfactorily. Despite the importance of analysing fault performance to select optimal fault detection and tolerance strategies, limited research has been conducted, especially with real systems. This study examines the performance of a 2-degree-of-freedom (2DOF) bi-rotor helicopter's control system in the presence of various actuator faults. Results from different fault conditions demonstrate that faults degrade the performance of conventional control systems on UAVs and introduce vibrations into the system, particularly when faults cause asymmetry or imbalance. However, additional experiments reveal that effective fault diagnosis and accommodation methods can help maintain satisfactory system performance despite the presence of faults.","sentences":["Stability and satisfactory performance are critical control requirements for Unmanned Aerial Vehicle (UAV) applications.","While conventional control systems for UAVs aim to ensure flight stability and safe operation while accomplishing tasks, UAVs may experience various flight faults that can degrade performance or, in severe cases, lead to instability.","Unsatisfactory performance or instability of a UAV poses risks to lives, properties, and the flying environment.","Therefore, it's essential to design a system capable of detecting faults, pinpointing their location, assessing their severity, and using this information to mitigate them, enabling the vehicle to continue operating satisfactorily.","Despite the importance of analysing fault performance to select optimal fault detection and tolerance strategies, limited research has been conducted, especially with real systems.","This study examines the performance of a 2-degree-of-freedom (2DOF) bi-rotor helicopter's control system in the presence of various actuator faults.","Results from different fault conditions demonstrate that faults degrade the performance of conventional control systems on UAVs and introduce vibrations into the system, particularly when faults cause asymmetry or imbalance.","However, additional experiments reveal that effective fault diagnosis and accommodation methods can help maintain satisfactory system performance despite the presence of faults."],"url":"http://arxiv.org/abs/2403.09008v1","category":"eess.SY"}
{"created":"2024-03-13 22:59:15","title":"Maximum Channel Coding Rate of Finite Block Length MIMO Faster-Than-Nyquist Signaling","abstract":"The pursuit of higher data rates and efficient spectrum utilization in modern communication technologies necessitates novel solutions. In order to provide insights into improving spectral efficiency and reducing latency, this study investigates the maximum channel coding rate (MCCR) of finite block length (FBL) multiple-input multiple-output (MIMO) faster-than-Nyquist (FTN) channels. By optimizing power allocation, we derive the system's MCCR expression. Simulation results are compared with the existing literature to reveal the benefits of FTN in FBL transmission.","sentences":["The pursuit of higher data rates and efficient spectrum utilization in modern communication technologies necessitates novel solutions.","In order to provide insights into improving spectral efficiency and reducing latency, this study investigates the maximum channel coding rate (MCCR) of finite block length (FBL) multiple-input multiple-output (MIMO) faster-than-Nyquist (FTN) channels.","By optimizing power allocation, we derive the system's MCCR expression.","Simulation results are compared with the existing literature to reveal the benefits of FTN in FBL transmission."],"url":"http://arxiv.org/abs/2403.08989v1","category":"cs.IT"}
{"created":"2024-03-13 22:29:03","title":"A Constrained Tracking Controller for Ramp and Sinusoidal Reference Signals using Robust Positive Invariance","abstract":"This paper proposes an output feedback controller capable of ensuring steady-state offset-free tracking for ramp and sinusoidal reference signals while ensuring local stability and state and input constraints fulfillment. The proposed solution is derived by jointly exploiting the internal model principle, polyhedral robust positively invariant arguments, and the Extended Farkas' Lemma. In particular, by considering a generic class of output feedback controller equipped with a feedforward term, a proportional effect, and a double integrator, we offline design the controller's gains by means of a single bilinear optimization problem. A peculiar feature of the proposed design is that the sets of all the admissible reference signals and the plant's initial conditions are also offline determined. Simulation results are provided to testify to the effectiveness of the proposed tracking controller and its capability to deal with both state and input constraints.","sentences":["This paper proposes an output feedback controller capable of ensuring steady-state offset-free tracking for ramp and sinusoidal reference signals while ensuring local stability and state and input constraints fulfillment.","The proposed solution is derived by jointly exploiting the internal model principle, polyhedral robust positively invariant arguments, and the Extended Farkas' Lemma.","In particular, by considering a generic class of output feedback controller equipped with a feedforward term, a proportional effect, and a double integrator, we offline design the controller's gains by means of a single bilinear optimization problem.","A peculiar feature of the proposed design is that the sets of all the admissible reference signals and the plant's initial conditions are also offline determined.","Simulation results are provided to testify to the effectiveness of the proposed tracking controller and its capability to deal with both state and input constraints."],"url":"http://arxiv.org/abs/2403.08987v1","category":"math.OC"}
{"created":"2024-03-13 22:19:41","title":"The minimal seed for transition to convective turbulence in heated pipe flow","abstract":"It is well known that buoyancy suppresses, and can even laminarise turbulence in upward heated pipe flow. Heat transfer seriously deteriorates in this case. Through a new DNS model, we confirm that the deteriorated heat transfer within convective turbulence is related to a lack of near-wall rolls, which leads to a weak mixing between the flow near the wall and centre of pipe. Having surveyed the fundamental properties of the system, we perform a nonlinear nonmodal stability analysis. it is found that, the minimal seed becomes thinner and closer to the wall, with increase of buoyancy number C. Most importantly, we show that the critical initial energy required to trigger shear-driven turbulence keeps increasing, implying that attempts to artificially trigger it may not be an efficient means to improve heat transfer at larger C. The new minimal seed, found at C=6, is localised in streamwise direction and is active in the centre of pipe. To find this branch of optimal, we took advantage of a window of linear stability. While the nonlinear optimal causes transition to convective turbulence directly at this and larger C, transition via the linear instability passes via a travelling wave or periodic orbit solutions. Detailed analysis of the periodic solution reveals three stages: growth of the unstable eigenfunction, the formation of streaks, and the decay of streaks due to suppression of the instability. Flow visualization at C up to 10 also show similar features, suggesting that convective turbulence is sustained by these three typical processes.","sentences":["It is well known that buoyancy suppresses, and can even laminarise turbulence in upward heated pipe flow.","Heat transfer seriously deteriorates in this case.","Through a new DNS model, we confirm that the deteriorated heat transfer within convective turbulence is related to a lack of near-wall rolls, which leads to a weak mixing between the flow near the wall and centre of pipe.","Having surveyed the fundamental properties of the system, we perform a nonlinear nonmodal stability analysis.","it is found that, the minimal seed becomes thinner and closer to the wall, with increase of buoyancy number C. Most importantly, we show that the critical initial energy required to trigger shear-driven turbulence keeps increasing, implying that attempts to artificially trigger it may not be an efficient means to improve heat transfer at larger C. The new minimal seed, found at C=6, is localised in streamwise direction and is active in the centre of pipe.","To find this branch of optimal, we took advantage of a window of linear stability.","While the nonlinear optimal causes transition to convective turbulence directly at this and larger C, transition via the linear instability passes via a travelling wave or periodic orbit solutions.","Detailed analysis of the periodic solution reveals three stages: growth of the unstable eigenfunction, the formation of streaks, and the decay of streaks due to suppression of the instability.","Flow visualization at C up to 10 also show similar features, suggesting that convective turbulence is sustained by these three typical processes."],"url":"http://arxiv.org/abs/2403.08985v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 22:14:14","title":"Approximating Small Sparse Cuts","abstract":"We study polynomial-time approximation algorithms for (edge/vertex) Sparsest Cut and Small Set Expansion in terms of $k$, the number of edges or vertices cut in the optimal solution. Our main results are $\\mathcal{O}(\\text{polylog}\\, k)$-approximation algorithms for various versions in this setting.   Our techniques involve an extension of the notion of sample sets (Feige and Mahdian STOC'06), originally developed for small balanced cuts, to sparse cuts in general. We then show how to combine this notion of sample sets with two algorithms, one based on an existing framework of LP rounding and another new algorithm based on the cut-matching game, to get such approximation algorithms. Our cut-matching game algorithm can be viewed as a local version of the cut-matching game by Khandekar, Khot, Orecchia and Vishnoi and certifies an expansion of every vertex set of size $s$ in $\\mathcal{O}(\\log s)$ rounds. These techniques may be of independent interest.   As corollaries of our results, we also obtain an $\\mathcal{O}(\\log opt)$-approximation for min-max graph partitioning, where $opt$ is the min-max value of the optimal cut, and improve the bound on the size of multicut mimicking networks computable in polynomial time.","sentences":["We study polynomial-time approximation algorithms for (edge/vertex) Sparsest Cut and Small Set Expansion in terms of $k$, the number of edges or vertices cut in the optimal solution.","Our main results are $\\mathcal{O}(\\text{polylog}\\, k)$-approximation algorithms for various versions in this setting.   ","Our techniques involve an extension of the notion of sample sets (Feige and Mahdian STOC'06), originally developed for small balanced cuts, to sparse cuts in general.","We then show how to combine this notion of sample sets with two algorithms, one based on an existing framework of LP rounding and another new algorithm based on the cut-matching game, to get such approximation algorithms.","Our cut-matching game algorithm can be viewed as a local version of the cut-matching game by Khandekar, Khot, Orecchia and Vishnoi and certifies an expansion of every vertex set of size $s$ in $\\mathcal{O}(\\log s)$ rounds.","These techniques may be of independent interest.   ","As corollaries of our results, we also obtain an $\\mathcal{O}(\\log opt)$-approximation for min-max graph partitioning, where $opt$ is the min-max value of the optimal cut, and improve the bound on the size of multicut mimicking networks computable in polynomial time."],"url":"http://arxiv.org/abs/2403.08983v1","category":"cs.DS"}
{"created":"2024-03-13 20:53:41","title":"Necessary conditions for turnpike property for generalized linear-quadratic problems","abstract":"In this paper, we develop several necessary conditions of turnpike property for generalizaid linear-quadratic (LQ) optimal control problem in infinite dimensional setting. The term 'generalized' here means that both quadratic and linear terms are considered in the running cost. The turnpike property reflects the fact that over a sufficiently large time horizon, the optimal trajectories and optimal controls stay for most of the time close to a steady state of the system. We show that the turnpike property is strongly connected to certain system theoretical properties of the control system. We provide suitable conditions to characterize the turnpike property in terms of the detectability and stabilizability of the system. Subsequently, we show the equivalence between the exponential turnpike property for generalized LQ and LQ optimal control problems.","sentences":["In this paper, we develop several necessary conditions of turnpike property for generalizaid linear-quadratic (LQ) optimal control problem in infinite dimensional setting.","The term 'generalized' here means that both quadratic and linear terms are considered in the running cost.","The turnpike property reflects the fact that over a sufficiently large time horizon, the optimal trajectories and optimal controls stay for most of the time close to a steady state of the system.","We show that the turnpike property is strongly connected to certain system theoretical properties of the control system.","We provide suitable conditions to characterize the turnpike property in terms of the detectability and stabilizability of the system.","Subsequently, we show the equivalence between the exponential turnpike property for generalized LQ and LQ optimal control problems."],"url":"http://arxiv.org/abs/2403.08958v1","category":"math.OC"}
{"created":"2024-03-13 19:23:59","title":"Multi-product Hamiltonian simulation with explicit commutator scaling","abstract":"The well-conditioned multi-product formula (MPF), proposed by [Low, Kliuchnikov, and Wiebe, 2019], is a simple high-order time-independent Hamiltonian simulation algorithm that implements a linear combination of standard product formulas of low order. While the MPF aims to simultaneously exploit commutator scaling among Hamiltonians and achieve near-optimal time and precision dependence, its lack of a rigorous error bound on the nested commutators renders its practical advantage ambiguous. In this work, we conduct a rigorous complexity analysis of the well-conditioned MPF, demonstrating explicit commutator scaling and near-optimal time and precision dependence at the same time. Using our improved complexity analysis, we present several applications of practical interest where the MPF based on a second-order product formula can achieve a polynomial speedup in both system size and evolution time, as well as an exponential speedup in precision, compared to second-order and even higher-order product formulas. Compared to post-Trotter methods, the MPF based on a second-order product formula can achieve polynomially better scaling in system size, with only poly-logarithmic overhead in evolution time and precision.","sentences":["The well-conditioned multi-product formula (MPF), proposed by [Low, Kliuchnikov, and Wiebe, 2019], is a simple high-order time-independent Hamiltonian simulation algorithm that implements a linear combination of standard product formulas of low order.","While the MPF aims to simultaneously exploit commutator scaling among Hamiltonians and achieve near-optimal time and precision dependence, its lack of a rigorous error bound on the nested commutators renders its practical advantage ambiguous.","In this work, we conduct a rigorous complexity analysis of the well-conditioned MPF, demonstrating explicit commutator scaling and near-optimal time and precision dependence at the same time.","Using our improved complexity analysis, we present several applications of practical interest where the MPF based on a second-order product formula can achieve a polynomial speedup in both system size and evolution time, as well as an exponential speedup in precision, compared to second-order and even higher-order product formulas.","Compared to post-Trotter methods, the MPF based on a second-order product formula can achieve polynomially better scaling in system size, with only poly-logarithmic overhead in evolution time and precision."],"url":"http://arxiv.org/abs/2403.08922v1","category":"quant-ph"}
{"created":"2024-03-13 18:45:01","title":"Compton scattering from superstrings","abstract":"We propose a candidate Compton amplitude which is valid for any (integer) quantum spin and free from any spurious poles. We consider the cases of electromagnetism and gravity. We obtain such amplitudes by calculating the corresponding ones from superstring theory involving states on the leading Regge trajectory. To extract the associated field-theory amplitudes a few considerations in the form of simple physical constraints are required, such as: Soft momentum transfer, compactification of polarizations and consistent factorization in the physical channels. We believe the present exploration will be significantly relevant for the physics of compact binary systems with spin.","sentences":["We propose a candidate Compton amplitude which is valid for any (integer) quantum spin and free from any spurious poles.","We consider the cases of electromagnetism and gravity.","We obtain such amplitudes by calculating the corresponding ones from superstring theory involving states on the leading Regge trajectory.","To extract the associated field-theory amplitudes a few considerations in the form of simple physical constraints are required, such as: Soft momentum transfer, compactification of polarizations and consistent factorization in the physical channels.","We believe the present exploration will be significantly relevant for the physics of compact binary systems with spin."],"url":"http://arxiv.org/abs/2403.08899v1","category":"hep-th"}
{"created":"2024-03-13 18:35:51","title":"Interpolatory model order reduction of large-scale dynamical systems with root mean squared error measures","abstract":"The root mean squared error is an important measure used in a variety of applications such as structural dynamics and acoustics to model averaged deviations from standard behavior. For large-scale systems, simulations of this quantity quickly become computationally prohibitive. Classical model order reduction techniques attempt to resolve this issue via the construction of surrogate models that emulate the root mean squared error measure using an intermediate linear system. However, this approach requires a potentially large number of linear outputs, which can be disadvantageous in the design of reduced-order models. In this work, we consider directly the root mean squared error as the quantity of interest using the concept of quadratic-output models and propose several new model reduction techniques for the construction of appropriate surrogates. We test the proposed methods on a model for the vibrational response of a plate with tuned vibration absorbers.","sentences":["The root mean squared error is an important measure used in a variety of applications such as structural dynamics and acoustics to model averaged deviations from standard behavior.","For large-scale systems, simulations of this quantity quickly become computationally prohibitive.","Classical model order reduction techniques attempt to resolve this issue via the construction of surrogate models that emulate the root mean squared error measure using an intermediate linear system.","However, this approach requires a potentially large number of linear outputs, which can be disadvantageous in the design of reduced-order models.","In this work, we consider directly the root mean squared error as the quantity of interest using the concept of quadratic-output models and propose several new model reduction techniques for the construction of appropriate surrogates.","We test the proposed methods on a model for the vibrational response of a plate with tuned vibration absorbers."],"url":"http://arxiv.org/abs/2403.08894v1","category":"math.NA"}
{"created":"2024-03-13 16:35:07","title":"Valuation of Power Purchase Agreements for Corporate Renewable Energy Procurement","abstract":"Corporate renewable power purchase agreements (PPAs) are long-term contracts that enable companies to source renewable energy without having to develop and operate their own capacities. Typically, producers and consumers agree on a fixed per-unit price at which power is purchased. The value of the PPA to the buyer depends on the so called capture price defined as the difference between this fixed price and the market value of the produced volume during the duration of the contract. To model the capture price, practitioners often use either fundamental or statistical approaches to model future market prices, which both have their inherent limitations. We propose a new approach that blends the logic of fundamental electricity market models with statistical learning techniques. In particular, we use regularized inverse optimization in a quadratic fundamental bottom-up model of the power market to estimate the marginal costs of different technologies as a parametric function of exogenous factors. We compare the out-of-sample performance in forecasting the capture price using market data from three European countries and demonstrate that our approach outperforms established statistical learning benchmarks. We then discuss the case of a photovoltaic plant in Spain to illustrate how to use the model to value a PPA from the buyer's perspective.","sentences":["Corporate renewable power purchase agreements (PPAs) are long-term contracts that enable companies to source renewable energy without having to develop and operate their own capacities.","Typically, producers and consumers agree on a fixed per-unit price at which power is purchased.","The value of the PPA to the buyer depends on the so called capture price defined as the difference between this fixed price and the market value of the produced volume during the duration of the contract.","To model the capture price, practitioners often use either fundamental or statistical approaches to model future market prices, which both have their inherent limitations.","We propose a new approach that blends the logic of fundamental electricity market models with statistical learning techniques.","In particular, we use regularized inverse optimization in a quadratic fundamental bottom-up model of the power market to estimate the marginal costs of different technologies as a parametric function of exogenous factors.","We compare the out-of-sample performance in forecasting the capture price using market data from three European countries and demonstrate that our approach outperforms established statistical learning benchmarks.","We then discuss the case of a photovoltaic plant in Spain to illustrate how to use the model to value a PPA from the buyer's perspective."],"url":"http://arxiv.org/abs/2403.08846v1","category":"econ.GN"}
{"created":"2024-03-13 12:08:27","title":"Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows","abstract":"Large Neighborhood Search (LNS) is a universal approach that is broadly applicable and has proven to be highly efficient in practice for solving optimization problems. We propose to integrate machine learning (ML) into LNS to assist in deciding which parts of the solution should be destroyed and repaired in each iteration of LNS. We refer to our new approach as Learning-Enhanced Neighborhood Selection (LENS for short). Our approach is universally applicable, i.e., it can be applied to any LNS algorithm to amplify the workings of the destroy algorithm. In this paper, we demonstrate the potential of LENS on the fundamental Vehicle Routing Problem with Time Windows (VRPTW). We implemented an LNS algorithm for VRPTW and collected data on generated novel training instances derived from well-known, extensively utilized benchmark datasets. We trained our LENS approach with this data and compared the experimental results of our approach with two benchmark algorithms: a random neighborhood selection method to show that LENS learns to make informed choices and an oracle neighborhood selection method to demonstrate the potential of our LENS approach. With LENS, we obtain results that significantly improve the quality of the solutions.","sentences":["Large Neighborhood Search (LNS) is a universal approach that is broadly applicable and has proven to be highly efficient in practice for solving optimization problems.","We propose to integrate machine learning (ML) into LNS to assist in deciding which parts of the solution should be destroyed and repaired in each iteration of LNS.","We refer to our new approach as Learning-Enhanced Neighborhood Selection (LENS for short).","Our approach is universally applicable, i.e., it can be applied to any LNS algorithm to amplify the workings of the destroy algorithm.","In this paper, we demonstrate the potential of LENS on the fundamental Vehicle Routing Problem with Time Windows (VRPTW).","We implemented an LNS algorithm for VRPTW and collected data on generated novel training instances derived from well-known, extensively utilized benchmark datasets.","We trained our LENS approach with this data and compared the experimental results of our approach with two benchmark algorithms: a random neighborhood selection method to show that LENS learns to make informed choices and an oracle neighborhood selection method to demonstrate the potential of our LENS approach.","With LENS, we obtain results that significantly improve the quality of the solutions."],"url":"http://arxiv.org/abs/2403.08839v1","category":"cs.LG"}
{"created":"2024-03-13 12:05:02","title":"Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation","abstract":"Vessel trajectory clustering, which aims to find similar trajectory patterns, has been widely leveraged in overwater applications. Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors. They aim for high-quality clustering and conduct clustering on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution. To resolve this problem, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence. Then, it predicts evolution at each timestamp of the sequence based on the representations. By applying predictive clustering and latent encoding, PC-HiV improves clustering and predictions simultaneously. Experiments on real AIS datasets demonstrate PC-HiV's superiority over existing methods, showcasing its effectiveness in capturing behavioral evolution discrepancies between vessel types (tramp vs. liner) and within emission control areas. Results show that our method outperforms NN-Kmeans and Robust DAA by 3.9% and 6.4% of the purity score.","sentences":["Vessel trajectory clustering, which aims to find similar trajectory patterns, has been widely leveraged in overwater applications.","Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors.","They aim for high-quality clustering and conduct clustering on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution.","To resolve this problem, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV).","PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence.","Then, it predicts evolution at each timestamp of the sequence based on the representations.","By applying predictive clustering and latent encoding, PC-HiV improves clustering and predictions simultaneously.","Experiments on real AIS datasets demonstrate PC-HiV's superiority over existing methods, showcasing its effectiveness in capturing behavioral evolution discrepancies between vessel types (tramp vs. liner) and within emission control areas.","Results show that our method outperforms NN-Kmeans and Robust DAA by 3.9% and 6.4% of the purity score."],"url":"http://arxiv.org/abs/2403.08838v1","category":"cs.LG"}
{"created":"2024-03-14 17:57:40","title":"On spontaneous breaking of the $\\text{SO}(2N)$ symmetry in the Gross-Neveu model","abstract":"The canonical Gross-Neveu model for $N$ two-component Dirac fermions in $2+1$ dimensions suffers a continuous phase transition at a critical interaction $g_{c1} \\sim 1/N$ at large $N$, at which its continuous symmetry $\\text{SO}(2N)$ is preserved and a discrete (Ising) symmetry becomes spontaneously broken. A recent mean-field calculation, however, points to an additional transition at a diffferent critical $g_{c2}\\sim -N g_{c1}$, at which $\\text{SO}(2N) \\rightarrow \\text{SO}(N) \\times \\text{SO}(N)$. To study the latter phase transition we rewrite the Gross-Neveu interaction $g (\\bar{\\psi} \\psi)^2$ in terms of three different quartic terms for the single ($L=1$) $4N$-component real (Majorana) fermion, and then extend the theory to $L>1$. This allows us to track the evolution of the fixed points of the renormalization group transformation starting from $L\\gg 1$, where one can discern three distinct critical points which correspond to continuous phase transitions into (1) $\\text{SO}(2N)$-singlet mass-order-parameter, (2) $\\text{SO}(2N)$-symmetric-tensor mass-order-parameters, and (3) $\\text{SO}(2N)$-adjoint nematic-order-parameters, down to $L=1$ value that is relevant to standard Gross-Neveu model. Below the critical value of $L_c (N)\\approx 0.35 N$ for $N\\gg1$ only the Gross-Neveu critical point (1) still implies a diverging susceptibility for its corresponding ($\\text{SO}(2N)$-singlet) order parameter, whereas the two new critical points that existed at large $L$ ultimately become equivalent to the Gaussian fixed point at $L=1$. We interpret this metamorphosis of the $\\text{SO}(2N)$-symmetric-tensor fixed point from critical to spurious as an indication that the transition at $g_{c2}$ in the original Gross-Neveu model is turned first-order by fluctuations.","sentences":["The canonical Gross-Neveu model for $N$ two-component Dirac fermions in $2+1$ dimensions suffers a continuous phase transition at a critical interaction $g_{c1} \\sim 1/N$ at large $N$, at which its continuous symmetry $\\text{SO}(2N)$ is preserved and a discrete (Ising) symmetry becomes spontaneously broken.","A recent mean-field calculation, however, points to an additional transition at a diffferent critical $g_{c2}\\sim -N g_{c1}$, at which $\\text{SO}(2N) \\rightarrow \\text{SO}(N) \\times \\text{SO}(N)$. To study the latter phase transition we rewrite the Gross-Neveu interaction $g (\\bar{\\psi} \\psi)^2$ in terms of three different quartic terms for the single ($L=1$) $4N$-component real (Majorana) fermion, and then extend the theory to $L>1$.","This allows us to track the evolution of the fixed points of the renormalization group transformation starting from $L\\gg 1$, where one can discern three distinct critical points which correspond to continuous phase transitions into (1) $\\text{SO}(2N)$-singlet mass-order-parameter, (2) $\\text{SO}(2N)$-symmetric-tensor mass-order-parameters, and (3) $\\text{SO}(2N)$-adjoint nematic-order-parameters, down to $L=1$ value that is relevant to standard Gross-Neveu model.","Below the critical value of $L_c (N)\\approx 0.35 N$ for $N\\gg1$ only the Gross-Neveu critical point (1) still implies a diverging susceptibility for its corresponding ($\\text{SO}(2N)$-singlet) order parameter, whereas the two new critical points that existed at large $L$ ultimately become equivalent to the Gaussian fixed point at $L=1$. We interpret this metamorphosis of the $\\text{SO}(2N)$-symmetric-tensor fixed point from critical to spurious as an indication that the transition at $g_{c2}$ in the original Gross-Neveu model is turned first-order by fluctuations."],"url":"http://arxiv.org/abs/2403.09627v1","category":"hep-th"}
{"created":"2024-03-14 17:42:03","title":"Tree-Level Superstring Amplitudes: The Neveu-Schwarz Sector","abstract":"We present a complete computation of superstring scattering amplitudes at tree level, for the case of Neveu-Schwarz insertions. Mathematically, this is to say that we determine explicitly the superstring measure on the moduli space $\\mathcal{M}_{0,n,0}$ of super Riemann surfaces of genus zero with $n \\ge 3$ Neveu-Schwarz punctures. While, of course, an expression for the measure was previously known, we do this from first principles, using the canonically defined super Mumford isomorphism. We thus determine the scattering amplitudes, explicitly in the global coordinates on $\\mathcal{M}_{0,n,0}$, without the need for picture changing operators or ghosts, and are also able to determine canonically the value of the coupling constant. Our computation should be viewed as a step towards performing similar analysis on $\\mathcal{M}_{0,0,n}$, to derive explicit tree-level scattering amplitudes with Ramond insertions.","sentences":["We present a complete computation of superstring scattering amplitudes at tree level, for the case of Neveu-Schwarz insertions.","Mathematically, this is to say that we determine explicitly the superstring measure on the moduli space $\\mathcal{M}_{0,n,0}$ of super Riemann surfaces of genus zero with $n \\ge 3$ Neveu-Schwarz punctures.","While, of course, an expression for the measure was previously known, we do this from first principles, using the canonically defined super Mumford isomorphism.","We thus determine the scattering amplitudes, explicitly in the global coordinates on $\\mathcal{M}_{0,n,0}$, without the need for picture changing operators or ghosts, and are also able to determine canonically the value of the coupling constant.","Our computation should be viewed as a step towards performing similar analysis on $\\mathcal{M}_{0,0,n}$, to derive explicit tree-level scattering amplitudes with Ramond insertions."],"url":"http://arxiv.org/abs/2403.09600v1","category":"hep-th"}
{"created":"2024-03-14 17:35:48","title":"The Size-Linewidth Relation and Signatures of Feedback from Quiescent to Active Star Forming Regions in the LMC","abstract":"To investigate the effects of stellar feedback on the gravitational state of giant molecular clouds (GMCs), we study $^{12}$CO and $^{13}$CO ALMA maps of nine GMCs distributed throughout the Large Magellanic Cloud (LMC), the nearest star-forming galaxy to our own. We perform noise and resolution matching on the sample, working at a common resolution of 3.5 arcseconds (0.85 pc at the LMC distance of 50 kpc), and use the \\textit{SCIMES} clustering algorithm to identify discrete substructure, or \"clumps.\" We supplement these data with three tracers of recent star formation: $8\\mu$m surface brightness, continuum-subtracted H$\\alpha$ flux, and interstellar radiation field energy density inferred from dust emission. The $^{12}$CO clumps identified cover a range of 3.6 dex in luminosity-based mass and 2.4 dex in average $8\\mu$m surface brightness, representative of the wide range of conditions of the interstellar medium in the LMC. Our observations suggest evidence for increased turbulence in these clouds. While the turbulent linewidths are correlated with clump surface density, in agreement with previous observations, we find even better correlation with the three star formation activity tracers considered, suggesting stellar energy injection plays a significant role in the dynamical state of the clumps. The excess linewidths we measure do not appear to result from opacity broadening. $^{12}$CO clumps are found to be typically less gravitationally bound than $^{13}$CO clumps, with some evidence of the kinetic-to-gravitational potential energy ratio increasing with star-formation tracers. Further multi-line analysis may better constrain the assumptions made in these calculations.","sentences":["To investigate the effects of stellar feedback on the gravitational state of giant molecular clouds (GMCs), we study $^{12}$CO and $^{13}$CO ALMA maps of nine GMCs distributed throughout the Large Magellanic Cloud (LMC), the nearest star-forming galaxy to our own.","We perform noise and resolution matching on the sample, working at a common resolution of 3.5 arcseconds (0.85 pc at the LMC distance of 50 kpc), and use the \\textit{SCIMES} clustering algorithm to identify discrete substructure, or \"clumps.\"","We supplement these data with three tracers of recent star formation: $8\\mu$m surface brightness, continuum-subtracted H$\\alpha$ flux, and interstellar radiation field energy density inferred from dust emission.","The $^{12}$CO clumps identified cover a range of 3.6 dex in luminosity-based mass and 2.4 dex in average $8\\mu$m surface brightness, representative of the wide range of conditions of the interstellar medium in the LMC.","Our observations suggest evidence for increased turbulence in these clouds.","While the turbulent linewidths are correlated with clump surface density, in agreement with previous observations, we find even better correlation with the three star formation activity tracers considered, suggesting stellar energy injection plays a significant role in the dynamical state of the clumps.","The excess linewidths we measure do not appear to result from opacity broadening.","$^{12}$CO clumps are found to be typically less gravitationally bound than $^{13}$CO clumps, with some evidence of the kinetic-to-gravitational potential energy ratio increasing with star-formation tracers.","Further multi-line analysis may better constrain the assumptions made in these calculations."],"url":"http://arxiv.org/abs/2403.09594v1","category":"astro-ph.GA"}
{"created":"2024-03-14 17:33:32","title":"The effect of spatially-varying collision frequency on the development of the Rayleigh-Taylor instability","abstract":"The Rayleigh-Taylor (RT) instability is ubiquitously observed, yet has traditionally been studied using ideal fluid models. Collisionality can vary strongly across the fluid interface, and previous work demonstrates the necessity of kinetic models to completely capture dynamics in certain collisional regimes. Where previous kinetic simulations used spatially- and temporally-constant collision frequency, this work presents 5-dimensional (two spatial, three velocity dimensions) continuum-kinetic simulations of the RT instability using a more realistic spatially-varying collision frequency. Three cases of collisional variation are explored for two Atwood numbers: low to intermediate, intermediate to high, and low to high. The low to intermediate case exhibits no RT instability growth, while the intermediate to high case is similar to a fluid limit kinetic case with interface widening biased towards the lower collisionality region. A novel contribution of this work is the low to high collisionality case that shows significantly altered instability growth through upward movement of the interface and damped spike growth due to increased free-streaming particle diffusion in the lower region. Contributions to the energy-flux from the non-Maxwellian portions of the distribution function are not accessible to fluid models and are greatest in magnitude in the spike and regions of low collisionality. Increasing the Atwood number results in greater RT instability growth and reduced upward interface movement. Deviation of the distribution function from Maxwellian is inversely proportional to collision frequency and concentrated around the fluid interface. The linear phase of RT instability growth is well-described by theoretical linear growth rates accounting for viscosity and diffusion.","sentences":["The Rayleigh-Taylor (RT) instability is ubiquitously observed, yet has traditionally been studied using ideal fluid models.","Collisionality can vary strongly across the fluid interface, and previous work demonstrates the necessity of kinetic models to completely capture dynamics in certain collisional regimes.","Where previous kinetic simulations used spatially- and temporally-constant collision frequency, this work presents 5-dimensional (two spatial, three velocity dimensions) continuum-kinetic simulations of the RT instability using a more realistic spatially-varying collision frequency.","Three cases of collisional variation are explored for two Atwood numbers: low to intermediate, intermediate to high, and low to high.","The low to intermediate case exhibits no RT instability growth, while the intermediate to high case is similar to a fluid limit kinetic case with interface widening biased towards the lower collisionality region.","A novel contribution of this work is the low to high collisionality case that shows significantly altered instability growth through upward movement of the interface and damped spike growth due to increased free-streaming particle diffusion in the lower region.","Contributions to the energy-flux from the non-Maxwellian portions of the distribution function are not accessible to fluid models and are greatest in magnitude in the spike and regions of low collisionality.","Increasing the Atwood number results in greater RT instability growth and reduced upward interface movement.","Deviation of the distribution function from Maxwellian is inversely proportional to collision frequency and concentrated around the fluid interface.","The linear phase of RT instability growth is well-described by theoretical linear growth rates accounting for viscosity and diffusion."],"url":"http://arxiv.org/abs/2403.09591v1","category":"physics.flu-dyn"}
{"created":"2024-03-14 16:54:58","title":"A perturbative approach to the non-relativistic string spectrum","abstract":"In this letter we use a perturbative approach to find the spectrum of non-relativistic strings in the String Newton-Cartan (SNC) AdS$_5\\times$S$^5$ spacetime. We perturb the bosonic sector of the action around a BMN-like folded string solution in light-cone gauge. We find strong evidence that the theory is described by a combination of massive and massless free fields in an anti-de Sitter background by showing that interaction terms up to six scalars vanish after field redefinitions.","sentences":["In this letter we use a perturbative approach to find the spectrum of non-relativistic strings in the String Newton-Cartan (SNC) AdS$_5\\times$S$^5$ spacetime.","We perturb the bosonic sector of the action around a BMN-like folded string solution in light-cone gauge.","We find strong evidence that the theory is described by a combination of massive and massless free fields in an anti-de Sitter background by showing that interaction terms up to six scalars vanish after field redefinitions."],"url":"http://arxiv.org/abs/2403.09563v1","category":"hep-th"}
{"created":"2024-03-14 16:44:35","title":"Cosmologically Consistent Analysis of Gravitational Waves from hidden sectors","abstract":"Production of gravitational waves in the early universe is discussed in a cosmologically consistent analysis within a first order phase transition involving a hidden sector feebly coupled with the visible sector. Each sector resides in its own heat bath leading to a potential dependent on two temperatures, and on two fields: one a standard model Higgs and the other a scalar arising from a hidden sector $U(1)$ gauge theory. A synchronous evolution of the hidden and visible sector temperatures is carried out from the reheat temperature down to the electroweak scale.The hydrodynamics of two-field phase transitions, one for the visible and the other for the hidden is discussed, which leads to separate tunneling temperatures, and different sound speeds for the two sectors. Gravitational waves emerging from the two sectors are computed and their imprint on the measured gravitational wave power spectrum vs frequency is analyzed in terms of bubble nucleation signature, i.e., detonation, deflagration, and hybrid. It is shown that the two-field model predicts gravitational waves accessible at several proposed gravitational wave detectors: LISA, DECIGO, BBO, Taiji and their discovery would probe specific regions of the hidden sector parameter space and may also shed light on the nature of bubble nucleation in the early universe. The analysis presented here indicates that the cosmologically preferred models are those where the tunneling in the visible sector precedes the tunneling in the hidden sector and the sound speed $c_s$ lies below its maximum, i.e., $c^2_s<\\frac{1}{3}$. It is of interest to investigate if these features are universal and applicable to a wider class of cosmologically consistent models.","sentences":["Production of gravitational waves in the early universe is discussed in a cosmologically consistent analysis within a first order phase transition involving a hidden sector feebly coupled with the visible sector.","Each sector resides in its own heat bath leading to a potential dependent on two temperatures, and on two fields: one a standard model Higgs and the other a scalar arising from a hidden sector $U(1)$ gauge theory.","A synchronous evolution of the hidden and visible sector temperatures is carried out from the reheat temperature down to the electroweak scale.","The hydrodynamics of two-field phase transitions, one for the visible and the other for the hidden is discussed, which leads to separate tunneling temperatures, and different sound speeds for the two sectors.","Gravitational waves emerging from the two sectors are computed and their imprint on the measured gravitational wave power spectrum vs frequency is analyzed in terms of bubble nucleation signature, i.e., detonation, deflagration, and hybrid.","It is shown that the two-field model predicts gravitational waves accessible at several proposed gravitational wave detectors: LISA, DECIGO, BBO, Taiji and their discovery would probe specific regions of the hidden sector parameter space and may also shed light on the nature of bubble nucleation in the early universe.","The analysis presented here indicates that the cosmologically preferred models are those where the tunneling in the visible sector precedes the tunneling in the hidden sector and the sound speed $c_s$ lies below its maximum, i.e., $c^2_s<\\frac{1}{3}$. It is of interest to investigate if these features are universal and applicable to a wider class of cosmologically consistent models."],"url":"http://arxiv.org/abs/2403.09558v1","category":"hep-ph"}
{"created":"2024-03-14 16:27:21","title":"Supermassive Black Hole Winds in X-rays -- SUBWAYS. III. A population study on Ultra-Fast Outflows","abstract":"The detection of blue-shifted absorption lines likely associated with ionized Iron K-shell transitions in the X-ray spectra of many Active Galactic Nuclei (AGN) suggests the presence of a highly ionized gas outflowing with mildly relativistic velocities (0.03c-0.6c), named Ultra-Fast Outflow (UFO). Within the SUBWAYS project we characterized these winds starting from a sample of 22 radio-quiet quasars at 0.1 < z < 0.4, and compared the results with similar studies in the literature on samples of 42 local radio-quiet Seyfert galaxies and 14 high redshift radio-quiet quasars. The scope of our work is a statistical study of UFO parameters and incidence, considering key physical properties of the sources, e.g. supermassive black hole (SMBH) mass, bolometric luminosity, accretion rates and Spectral Energy Distribution, with the aim of gaining new insights into the UFO launching mechanisms. We find indications that highly luminous AGN with steeper X-ray/UV ratio, are more likely to host UFO. The presence of UFO is not significantly related to any other AGN property in our sample. These findings suggest that the UFO phenomenon may be transient. Focusing on AGN with UFO, other important results are: (1) faster UFO have larger ionization parameters and column densities; (2) X-ray radiation plays a more crucial role in driving highly ionized winds compared to UV; (3) the correlation between outflow velocity and luminosity is significantly flatter than what expected for radiatively driven winds; (4) more massive BH experience higher wind mass-losses, suppressing accretion of matter onto the BH; (5) the UFO launching radius is positively correlated with the Eddington ratio. Furthermore, our analysis suggest the involvement of multiple launching mechanisms, including radiation pressure and magneto-hydrodynamic processes, rather than pointing to a single, universally applicable mechanism.","sentences":["The detection of blue-shifted absorption lines likely associated with ionized Iron K-shell transitions in the X-ray spectra of many Active Galactic Nuclei (AGN) suggests the presence of a highly ionized gas outflowing with mildly relativistic velocities (0.03c-0.6c), named Ultra-Fast Outflow (UFO).","Within the SUBWAYS project we characterized these winds starting from a sample of 22 radio-quiet quasars at 0.1 < z < 0.4, and compared the results with similar studies in the literature on samples of 42 local radio-quiet Seyfert galaxies and 14 high redshift radio-quiet quasars.","The scope of our work is a statistical study of UFO parameters and incidence, considering key physical properties of the sources, e.g. supermassive black hole (SMBH) mass, bolometric luminosity, accretion rates and Spectral Energy Distribution, with the aim of gaining new insights into the UFO launching mechanisms.","We find indications that highly luminous AGN with steeper X-ray/UV ratio, are more likely to host UFO.","The presence of UFO is not significantly related to any other AGN property in our sample.","These findings suggest that the UFO phenomenon may be transient.","Focusing on AGN with UFO, other important results are: (1) faster UFO have larger ionization parameters and column densities; (2) X-ray radiation plays a more crucial role in driving highly ionized winds compared to UV; (3) the correlation between outflow velocity and luminosity is significantly flatter than what expected for radiatively driven winds; (4) more massive BH experience higher wind mass-losses, suppressing accretion of matter onto the BH; (5) the UFO launching radius is positively correlated with the Eddington ratio.","Furthermore, our analysis suggest the involvement of multiple launching mechanisms, including radiation pressure and magneto-hydrodynamic processes, rather than pointing to a single, universally applicable mechanism."],"url":"http://arxiv.org/abs/2403.09538v1","category":"astro-ph.GA"}
{"created":"2024-03-14 16:21:42","title":"The affine Artin group of type $\\widetilde B_n$ is virtually poly-free","abstract":"In this note we prove that the affine Artin group of type $\\widetilde B_n$ is virtually poly-free. The proof also gives another solution of the $K(\\pi, 1)$ problem for $\\widetilde B_n$.","sentences":["In this note we prove that the affine Artin group of type $\\widetilde B_n$ is virtually poly-free.","The proof also gives another solution of the $K(\\pi, 1)$ problem for $\\widetilde B_n$."],"url":"http://arxiv.org/abs/2403.09533v1","category":"math.GR"}
{"created":"2024-03-14 16:10:04","title":"Vortex pattern stabilization in thin films resulting from shear thickening of active suspensions","abstract":"The need for structuring on micrometer scales is abundant, for example, in view of phononic applications. We here outline a novel approach based on the phenomenon of active turbulence on the mesoscale. As we demonstrate, a shear-thickening carrier fluid of active microswimmers intrinsically stabilizes regular vortex patterns of otherwise turbulent active suspensions. The fluid self-organizes into a periodically structured nonequilibrium state. Introducing additional passive particles of intermediate size leads to regular spatial organization of these objects. Our approach opens a new path towards functionalization through patterning of thin films and membranes.","sentences":["The need for structuring on micrometer scales is abundant, for example, in view of phononic applications.","We here outline a novel approach based on the phenomenon of active turbulence on the mesoscale.","As we demonstrate, a shear-thickening carrier fluid of active microswimmers intrinsically stabilizes regular vortex patterns of otherwise turbulent active suspensions.","The fluid self-organizes into a periodically structured nonequilibrium state.","Introducing additional passive particles of intermediate size leads to regular spatial organization of these objects.","Our approach opens a new path towards functionalization through patterning of thin films and membranes."],"url":"http://arxiv.org/abs/2403.09523v1","category":"cond-mat.soft"}
{"created":"2024-03-14 15:56:41","title":"The study of weak decays of doubly charmed baryons within rescattering mechanism","abstract":"The doubly charmed baryon $\\Xi_{cc}^{++}$ has been observed by LHCb through the non-leptonic decay modes of $\\Xi_{cc}^{++}\\to\\Lambda_{c}^{+}K^{-}\\pi^{+}\\pi^{+}$ and $\\Xi_{c}^{+}\\pi^{+}$ in 2017. After that, the experimentalists turn their attention to finding other doubly charmed baryons $\\Xi_{cc}^{+}$ and $\\Omega_{cc}^{+}$. In this work, we investigate the nonleptonic weak decays of doubly charmed baryons ${\\cal B}_{cc}\\to{\\cal B}_{c}P$, where ${\\cal B}_{cc}$ denotes the doubly charmed baryons $(\\Xi_{cc}^{++},\\Xi_{cc}^{+},\\Omega_{cc}^{+})$, ${\\cal B}_{c}$ represents the singly charmed baryons $({\\cal B}_{\\bar{3}},{\\cal B}_{6})$ and $P$ is the light pseudoscalar. For these non-leptonic decay modes, their short-distance contributions can be accurately estimated in theoretical calculations. However, dealing with the long-distance contributions for final-state-interaction effects is challenging. To address this, we use the rescattering mechanism to calculate the long-distance contributions and first derive the whole hadronic loop contributions for these two-body nonleptonic decays of doubly charmed baryons. Then the decay widths and branching ratios of the 45 nonleptonic decays of doubly charmed baryon are predicted. Among that, the ratio of the branching ratios ${\\cal RB}=\\frac{{\\cal B}(\\Xi_{cc}^{++}\\to\\Xi_{c}^{\\prime+}\\pi^{+})}{{\\cal B}(\\Xi_{cc}^{++}\\to\\Xi_{c}^{+}\\pi^{+})}=1.15\\pm0.45$ is consistent with the experimental results within statistical errors.","sentences":["The doubly charmed baryon $\\Xi_{cc}^{++}$ has been observed by LHCb through the non-leptonic decay modes of $\\Xi_{cc}^{++}\\to\\Lambda_{c}^{+}K^{-}\\pi^{+}\\pi^{+}$ and $\\Xi_{c}^{+}\\pi^{+}$ in 2017.","After that, the experimentalists turn their attention to finding other doubly charmed baryons $\\Xi_{cc}^{+}$ and $\\Omega_{cc}^{+}$. In this work, we investigate the nonleptonic weak decays of doubly charmed baryons ${\\cal B}_{cc}\\to{\\cal B}_{c}P$, where ${\\cal B}_{cc}$ denotes the doubly charmed baryons $(\\Xi_{cc}^{++},\\Xi_{cc}^{+},\\Omega_{cc}^{+})$, ${\\cal B}_{c}$ represents the singly charmed baryons $({\\cal B}_{\\bar{3}},{\\cal B}_{6})$ and $P$ is the light pseudoscalar.","For these non-leptonic decay modes, their short-distance contributions can be accurately estimated in theoretical calculations.","However, dealing with the long-distance contributions for final-state-interaction effects is challenging.","To address this, we use the rescattering mechanism to calculate the long-distance contributions and first derive the whole hadronic loop contributions for these two-body nonleptonic decays of doubly charmed baryons.","Then the decay widths and branching ratios of the 45 nonleptonic decays of doubly charmed baryon are predicted.","Among that, the ratio of the branching ratios ${\\cal RB}=\\frac{{\\cal B}(\\Xi_{cc}^{++}\\to\\Xi_{c}^{\\prime+}\\pi^{+})}{{\\cal B}(\\Xi_{cc}^{++}\\to\\Xi_{c}^{+}\\pi^{+})}=1.15\\pm0.45$ is consistent with the experimental results within statistical errors."],"url":"http://arxiv.org/abs/2403.09511v1","category":"hep-ph"}
{"created":"2024-03-14 15:43:42","title":"Operation of BGO with SiPM readout at dry ice and liquid nitrogen temperatures","abstract":"The light yield and decay constant of BGO were measured at both dry ice and liquid nitrogen temperatures using two SiPMs directly coupled to a $6\\times6\\times6$ cm$^2$ cubic BGO crystal. With the measured light yield (5.2$\\pm$0.3 PE/keV at dry ice temperature and 10.5$\\pm$0.4 PE/keV at liquid nitrogen temperature) and decay constants, potential applications of BGO in ToF-PET and SPECT were discussed.","sentences":["The light yield and decay constant of BGO were measured at both dry ice and liquid nitrogen temperatures using two SiPMs directly coupled to a $6\\times6\\times6$ cm$^2$ cubic BGO crystal.","With the measured light yield (5.2$\\pm$0.3 PE/keV at dry ice temperature and 10.5$\\pm$0.4 PE/keV at liquid nitrogen temperature) and decay constants, potential applications of BGO in ToF-PET and SPECT were discussed."],"url":"http://arxiv.org/abs/2403.09501v1","category":"physics.ins-det"}
{"created":"2024-03-14 15:25:35","title":"Theta Dependence in the Presence of Massless Fermions","abstract":"We show that if there are conserved flavor symmetries then some properties of a monopole can depend on $\\theta$ even when a fermion is massless. The quantized nature of global symmetries and the fractional nature of the Witten effect can lead to interesting structure. Seen from another point of view, aside from possibly breaking baryon and lepton flavor symmetries (the Callan-Rubakov effect), monopole boundary conditions can also break the axial symmetry that otherwise could have been used to remove $\\theta$ from the Lagrangian. As an example, in a toy model, we calculate the $\\theta$ dependence of the mass of the monopole and properties of the non-zero charge density surrounding the monopole.","sentences":["We show that if there are conserved flavor symmetries then some properties of a monopole can depend on $\\theta$ even when a fermion is massless.","The quantized nature of global symmetries and the fractional nature of the Witten effect can lead to interesting structure.","Seen from another point of view, aside from possibly breaking baryon and lepton flavor symmetries (the Callan-Rubakov effect), monopole boundary conditions can also break the axial symmetry that otherwise could have been used to remove $\\theta$ from the Lagrangian.","As an example, in a toy model, we calculate the $\\theta$ dependence of the mass of the monopole and properties of the non-zero charge density surrounding the monopole."],"url":"http://arxiv.org/abs/2403.09482v1","category":"hep-th"}
{"created":"2024-03-14 14:41:30","title":"Expansion and Spectral Softening of the Dust Scattering Rings of GRB 221009A","abstract":"Expanding X-ray halo or rings appear when short pulses of X-ray radiation from a background source are scattered by clouds of dust in the Milky Way. We study the X-ray rings of the brightest gamma-ray burst (GRB) 221009A, detected by the {\\it Swift} X-Ray Telescope. The rings center on the GRB position and their angular radii increase with time. We identify five major expanding rings, and our modeling of their expansion history suggests that they are scattered off, respectively, from five dusty clouds at distances of 0.4-13 kpc from the observer. Given an assumed prompt X-ray fluence of this GRB, the fluxes of those rings suggest that these clouds have dust grain column densities of $10^{7\\sim8}~\\mathrm{cm^{-2}}$. More interestingly, our time-dependent spectral analysis of these rings show that they all experience spectral softening, i.e., getting softer as they expand, with spectral indices ranging from 2.2 to 5, consistent with what the dust scattering model predicts.","sentences":["Expanding X-ray halo or rings appear when short pulses of X-ray radiation from a background source are scattered by clouds of dust in the Milky Way.","We study the X-ray rings of the brightest gamma-ray burst (GRB) 221009A, detected by the {\\it Swift} X-Ray Telescope.","The rings center on the GRB position and their angular radii increase with time.","We identify five major expanding rings, and our modeling of their expansion history suggests that they are scattered off, respectively, from five dusty clouds at distances of 0.4-13 kpc from the observer.","Given an assumed prompt X-ray fluence of this GRB, the fluxes of those rings suggest that these clouds have dust grain column densities of $10^{7\\sim8}~\\mathrm{cm^{-2}}$. More interestingly, our time-dependent spectral analysis of these rings show that they all experience spectral softening, i.e., getting softer as they expand, with spectral indices ranging from 2.2 to 5, consistent with what the dust scattering model predicts."],"url":"http://arxiv.org/abs/2403.09444v1","category":"astro-ph.HE"}
{"created":"2024-03-14 14:22:29","title":"Search for Higgs boson pair production in the bbWW decay mode in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for Higgs boson pair (HH) production with one Higgs boson decaying to two bottom quarks and the other to two W bosons are presented. The search is done using proton-proton collisions data at a centre-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 138 fb$^{-1}$ recorded by the CMS detector at the LHC from 2016 to 2018. The final states considered include at least one leptonically decaying W boson. No evidence for the presence of a signal is observed and corresponding upper limits on the HH production cross section are derived. The limit on the inclusive cross section of the nonresonant HH production, assuming that the distributions of kinematic observables are as expected in the standard model (SM), is observed (expected) to be 14 (18) times the value predicted by the SM, at 95% confidence level. The limits on the cross section are also presented as functions of various Higgs boson coupling modifiers, and anomalous Higgs boson coupling scenarios. In addition, limits are set on the resonant HH production via spin-0 and spin-2 resonances within the mass range 250-900 GeV.","sentences":["A search for Higgs boson pair (HH) production with one Higgs boson decaying to two bottom quarks and the other to two W bosons are presented.","The search is done using proton-proton collisions data at a centre-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 138 fb$^{-1}$","recorded by the CMS detector at the LHC from 2016 to 2018.","The final states considered include at least one leptonically decaying W boson.","No evidence for the presence of a signal is observed and corresponding upper limits on the HH production cross section are derived.","The limit on the inclusive cross section of the nonresonant HH production, assuming that the distributions of kinematic observables are as expected in the standard model (SM), is observed (expected) to be 14 (18) times the value predicted by the SM, at 95% confidence level.","The limits on the cross section are also presented as functions of various Higgs boson coupling modifiers, and anomalous Higgs boson coupling scenarios.","In addition, limits are set on the resonant HH production via spin-0 and spin-2 resonances within the mass range 250-900 GeV."],"url":"http://arxiv.org/abs/2403.09430v1","category":"hep-ex"}
{"created":"2024-03-14 14:18:21","title":"Stable high-transformer ratio beam-wakefield acceleration in cusp plasma channels","abstract":"Wakefield excitation by structured electron bunches in hollow gaps between plasma wedges is studied using three-dimensional particle-in-cell simulations. The main part of the electron bunch has a triangular current distribution in the longitudinal direction with a smooth head and short tail. These bunches propagate stably in the hollow gap while being attached to cusps of the plasma wedges. The excited wakefield profile may have a very high transformer ratio and allows to accelerate witness bunches to energies much higher than that of the driver bunch. Unlike round hollow channels, where asymmetric wakefields are difficult to avoid, no deleterious transverse beam break-up (BBU) is observed in the gap between cusp-shaped plasma layers.","sentences":["Wakefield excitation by structured electron bunches in hollow gaps between plasma wedges is studied using three-dimensional particle-in-cell simulations.","The main part of the electron bunch has a triangular current distribution in the longitudinal direction with a smooth head and short tail.","These bunches propagate stably in the hollow gap while being attached to cusps of the plasma wedges.","The excited wakefield profile may have a very high transformer ratio and allows to accelerate witness bunches to energies much higher than that of the driver bunch.","Unlike round hollow channels, where asymmetric wakefields are difficult to avoid, no deleterious transverse beam break-up (BBU) is observed in the gap between cusp-shaped plasma layers."],"url":"http://arxiv.org/abs/2403.09427v1","category":"physics.plasm-ph"}
{"created":"2024-03-14 14:15:05","title":"A survey on conjugacy class graphs of groups","abstract":"There are several graphs defined on groups. Among them we consider graphs whose vertex set consists conjugacy classes of a group $G$ and adjacency is defined by properties of the elements of conjugacy classes. In particular, we consider commuting/nilpotent/solvable conjugacy class graph of $G$ where two distinct conjugacy classes $a^G$ and $b^G$ are adjacent if there exist some elements $x\\in a^G$ and $y\\in b^G$ such that $\\langle x, y \\rangle$ is abelian/nilpotent/solvable. After a section of introductory results and examples, we discuss all the available results on connectedness, graph realization, genus, various spectra and energies of certain induced subgraphs of these graphs. Proofs of the results are not included. However, many open problems for further investigation are stated.","sentences":["There are several graphs defined on groups.","Among them we consider graphs whose vertex set consists conjugacy classes of a group $G$ and adjacency is defined by properties of the elements of conjugacy classes.","In particular, we consider commuting/nilpotent/solvable conjugacy class graph of $G$ where two distinct conjugacy classes $a^G$ and $b^G$ are adjacent if there exist some elements $x\\in a^G$ and $y\\in b^G$ such that $\\langle x, y \\rangle$ is abelian/nilpotent/solvable.","After a section of introductory results and examples, we discuss all the available results on connectedness, graph realization, genus, various spectra and energies of certain induced subgraphs of these graphs.","Proofs of the results are not included.","However, many open problems for further investigation are stated."],"url":"http://arxiv.org/abs/2403.09423v1","category":"math.GR"}
{"created":"2024-03-14 13:45:28","title":"Exploring the lepton flavor violating decay modes $b \\to s \u03bc^{\\pm} \u03c4^{\\mp}$ in SMEFT approach","abstract":"We perform an analysis of the consequences of various new physics operators on the lepton flavor violating (LFV) decay modes mediated through $b \\to s \\ell _1 \\ell _2$ transitions. We scrutinize the imprints of the (pseudo)scalar and axial(vector) operators on the exclusive LFV decay channels $ B_{(s)} \\rightarrow (\\phi, K^{*}, K_{2}^{*})\\ell_{1}\\ell_{2}$ and $\\Lambda_{b}\\rightarrow \\Lambda \\ell_{1}\\ell_{2}$, where $\\ell_{1}, \\ell_{2}$ represent $\\mu$ or $\\tau$. The new physics parameters are constrained by using the upper limits of the branching fractions of the $B \\to \\tau \\mu$ and $B \\to K \\tau \\mu$ processes, assuming the new physics couplings to be real. We then explore the key observables such as the branching fraction, the forward-backward asymmetry, and the longitudinal polarisation fraction of the $B \\to (K^*, \\phi, K_2^*) \\tau ^{\\pm} \\mu ^{\\mp}$ decays. In addition, we also investigate the impact of the new physics couplings on the baryonic $\\Lambda _b \\to \\Lambda \\tau ^{\\pm} \\mu ^{\\mp}$ decay channels mediated by the $b \\to s$ quark level transition. With the experimental prospects at LHCb upgrade and Belle II, we also predict the upper limits of the above-discussed observables, which could intrigue the new physics search in these channels.","sentences":["We perform an analysis of the consequences of various new physics operators on the lepton flavor violating (LFV) decay modes mediated through $b \\to s \\ell _1 \\ell _2$ transitions.","We scrutinize the imprints of the (pseudo)scalar and axial(vector) operators on the exclusive LFV decay channels $ B_{(s)} \\rightarrow (\\phi, K^{*}, K_{2}^{*})\\ell_{1}\\ell_{2}$ and $\\Lambda_{b}\\rightarrow \\Lambda \\ell_{1}\\ell_{2}$, where $\\ell_{1}, \\ell_{2}$ represent $\\mu$ or $\\tau$. The new physics parameters are constrained by using the upper limits of the branching fractions of the $B \\to \\tau \\mu$ and $B \\to K \\tau \\mu$ processes, assuming the new physics couplings to be real.","We then explore the key observables such as the branching fraction, the forward-backward asymmetry, and the longitudinal polarisation fraction of the $B \\to (K^*, \\phi, K_2^*) \\tau ^{\\pm} \\mu ^{\\mp}$ decays.","In addition, we also investigate the impact of the new physics couplings on the baryonic $\\Lambda _b \\to \\Lambda \\tau ^{\\pm} \\mu ^{\\mp}$ decay channels mediated by the $b \\to s$ quark level transition.","With the experimental prospects at LHCb upgrade and Belle II, we also predict the upper limits of the above-discussed observables, which could intrigue the new physics search in these channels."],"url":"http://arxiv.org/abs/2403.09393v1","category":"hep-ph"}
{"created":"2024-03-14 12:45:40","title":"Sensitivity to sub-GeV dark matter from cosmic-ray scattering with very-high-energy gamma-ray observatories","abstract":"Huge efforts have been deployed to detect dark matter (DM) in the GeV-TeV mass range involving various detection techniques, and led to strong constraints in the available parameter space. We compute here the sensitivity to sub-GeV DM that can be probed from the inevitable cosmic-ray scattering onto DM particles populating the Milky Way halo. Inelastic scattering of energetic cosmic rays off DM would produce high-energy gamma rays in the final state, providing a new avenue to probe the poorly-constrained so far sub-GeV dark matter mass range. In this work we derive sensitivity forecasts for the inelastic cosmic-ray proton - DM cross section for current and future very-high-energy gamma-ray observatories such as H.E.S.S., LHAASO, CTA and SWGO in the 100 eV to 100 MeV mass range. These inelastic cross section constraints are converted to the elastic proton - DM cross section to highlight further complementarity with cosmological, collider and direct detection searches. The sensitivity computed at 95\\% confidence level on the elastic cross section reaches $\\sim$2$\\times$ 10$^{-32}$ cm$^2$ for a 100 keV DM mass for H.E.S.S.-like and $\\sim$7$\\times$ 10$^{-34}$ cm$^2$ for a $\\sim$1 keV DM mass for LHAASO. The sensitivity prospects for CTA and a strawman SWGO model reach $\\sim$6$\\times$ 10$^{-34}$ cm$^2$ and $\\sim$4$\\times$ 10$^{-35}$ cm$^2$, for DM masses of 10 keV and 1 keV, respectively. The sensitivity reach of the gamma-ray observatories considered here enables to probe an uncharted region of the DM mass - cross section parameter space.","sentences":["Huge efforts have been deployed to detect dark matter (DM) in the GeV-TeV mass range involving various detection techniques, and led to strong constraints in the available parameter space.","We compute here the sensitivity to sub-GeV DM that can be probed from the inevitable cosmic-ray scattering onto DM particles populating the Milky Way halo.","Inelastic scattering of energetic cosmic rays off DM would produce high-energy gamma rays in the final state, providing a new avenue to probe the poorly-constrained so far sub-GeV dark matter mass range.","In this work we derive sensitivity forecasts for the inelastic cosmic-ray proton - DM cross section for current and future very-high-energy gamma-ray observatories such as H.E.S.S., LHAASO, CTA and SWGO in the 100 eV to 100 MeV mass range.","These inelastic cross section constraints are converted to the elastic proton - DM cross section to highlight further complementarity with cosmological, collider and direct detection searches.","The sensitivity computed at 95\\% confidence level on the elastic cross section reaches $\\sim$2$\\times$ 10$^{-32}$ cm$^2$ for a 100 keV DM mass for H.E.S.S.-like and $\\sim$7$\\times$ 10$^{-34}$","cm$^2$ for a $\\sim$1 keV DM mass for LHAASO.","The sensitivity prospects for CTA and a strawman SWGO model reach $\\sim$6$\\times$ 10$^{-34}$ cm$^2$ and $\\sim$4$\\times$ 10$^{-35}$ cm$^2$, for DM masses of 10 keV and 1 keV, respectively.","The sensitivity reach of the gamma-ray observatories considered here enables to probe an uncharted region of the DM mass - cross section parameter space."],"url":"http://arxiv.org/abs/2403.09343v1","category":"hep-ph"}
{"created":"2024-03-14 12:44:39","title":"Likely detection of magnetic field related LFQPO in the soft X-ray re-brightening of GRS~1915+105","abstract":"Utilizing NICER observations, we present an analysis of the soft X-ray re-brightening event of GRS 1915+105 observed in 2021. During this event, we observed the emergence of a stable, long-lasting low-frequency quasi-periodic oscillation (LFQPO) with frequencies ranging from 0.17 to 0.21 Hz. Through a careful spectral analysis, we demonstrate that a low-temperature Compton-thick gas model well characterizes the emitted radiation. By examining the spectrum and identifying numerous absorption lines, we discerned a transition in the wind properties. This transition was marked by a shift from a state characterized by low speed, high column density, and high ionization degree to one featuring still low speed but low column density and ionization degree. Intriguingly, the presence or absence of the QPO signal is perfectly correlated with these distinct wind characteristics. The low-speed wind observed could be indicative of a 'failed wind', while the observed shift implies a transition from a magnetically to a thermally driven wind. Notably, this QPO signal exclusively manifested itself during the magnetically driven phase, suggesting the possibility of a novel perturbation associated with magnetic effects.","sentences":["Utilizing NICER observations, we present an analysis of the soft X-ray re-brightening event of GRS 1915+105 observed in 2021.","During this event, we observed the emergence of a stable, long-lasting low-frequency quasi-periodic oscillation (LFQPO) with frequencies ranging from 0.17 to 0.21 Hz.","Through a careful spectral analysis, we demonstrate that a low-temperature Compton-thick gas model well characterizes the emitted radiation.","By examining the spectrum and identifying numerous absorption lines, we discerned a transition in the wind properties.","This transition was marked by a shift from a state characterized by low speed, high column density, and high ionization degree to one featuring still low speed but low column density and ionization degree.","Intriguingly, the presence or absence of the QPO signal is perfectly correlated with these distinct wind characteristics.","The low-speed wind observed could be indicative of a 'failed wind', while the observed shift implies a transition from a magnetically to a thermally driven wind.","Notably, this QPO signal exclusively manifested itself during the magnetically driven phase, suggesting the possibility of a novel perturbation associated with magnetic effects."],"url":"http://arxiv.org/abs/2403.09341v1","category":"astro-ph.HE"}
{"created":"2024-03-14 12:12:25","title":"Improved Constraints on the Variation of the Weak Scale from Big Bang Nucleosynthesis","abstract":"We analyze the recent EMPRESS data on the $^4$He abundance in Big Bang nucleosynthesis as a function of the Higgs vacuum expectation value $v$ and compare our calculation to the recently published work of Burns et al.~[1]. The EMPRESS result for the $^4$He abundance can be explained within $2\\sigma$ by $0.03 \\leq \\delta v / v \\leq 0.07$. However, as first noted in~[1], a Higgs VEV in this range would worsen the discrepancy between theory and experiment for the deuterium abundance significantly.","sentences":["We analyze the recent EMPRESS data on the $^4$He abundance in Big Bang nucleosynthesis as a function of the Higgs vacuum expectation value $v$ and compare our calculation to the recently published work of Burns et al.~[1].","The EMPRESS result for the $^4$He abundance can be explained within $2\\sigma$ by $0.03 \\leq \\delta v / v \\leq 0.07$.","However, as first noted in~[1], a Higgs VEV in this range would worsen the discrepancy between theory and experiment for the deuterium abundance significantly."],"url":"http://arxiv.org/abs/2403.09325v1","category":"hep-ph"}
{"created":"2024-03-14 12:09:49","title":"Stopping mass-selected alkaline-earth metal monofluoride beams of high energy via formation of unusually stable anions","abstract":"Direct laser-coolability and a comparatively simple electronic structure render alkaline-earth metal monofluoride molecules versatile laboratories for precision tests of fundamental physics. In this theoretical work, a route for efficient stopping and cooling of high-energy hot beams of mass-selected alkaline-earth metal monofluorides via their anions is explored to facilitate subsequent precision experiments with trapped molecules. It is shown that these molecular anions possess an unusually strong chemical bond and that RaF$^-$ features properties favourable for efficient pre-cooling, indicating the applicability of direct laser-cooling of the anion.","sentences":["Direct laser-coolability and a comparatively simple electronic structure render alkaline-earth metal monofluoride molecules versatile laboratories for precision tests of fundamental physics.","In this theoretical work, a route for efficient stopping and cooling of high-energy hot beams of mass-selected alkaline-earth metal monofluorides via their anions is explored to facilitate subsequent precision experiments with trapped molecules.","It is shown that these molecular anions possess an unusually strong chemical bond and that RaF$^-$ features properties favourable for efficient pre-cooling, indicating the applicability of direct laser-cooling of the anion."],"url":"http://arxiv.org/abs/2403.09320v1","category":"physics.chem-ph"}
{"created":"2024-03-14 11:25:20","title":"Exploration at the high-energy frontier: ATLAS Run 2 searches investigating the exotic jungle beyond the Standard Model","abstract":"The Large Hadron Collider is at the high-energy frontier of particle physics, having produced proton$-$proton collisions during Run 2, from 2015 to 2018, at an unprecedented centre-of-mass energy of 13 TeV. The ATLAS experiment has a broad programme to thoroughly explore this uncharted territory, to seek out new exotic particles predicted by theories that boldly go beyond the Standard Model. Although the Standard Model has been very successful, it leaves many questions unanswered, prompting novel ideas about particles that could be hiding in the data, waiting to be discovered by intrepid explorers. These include dark-matter candidates, hidden-sector particles, new vector resonances, leptoquarks, and vector-like quarks, to name a few examples. Searches for new particles are reported here, with the exception of searches for supersymmetry or extended Higgs sectors as these are the subjects of separate reports.","sentences":["The Large Hadron Collider is at the high-energy frontier of particle physics, having produced proton$-$proton collisions during Run 2, from 2015 to 2018, at an unprecedented centre-of-mass energy of 13 TeV.","The ATLAS experiment has a broad programme to thoroughly explore this uncharted territory, to seek out new exotic particles predicted by theories that boldly go beyond the Standard Model.","Although the Standard Model has been very successful, it leaves many questions unanswered, prompting novel ideas about particles that could be hiding in the data, waiting to be discovered by intrepid explorers.","These include dark-matter candidates, hidden-sector particles, new vector resonances, leptoquarks, and vector-like quarks, to name a few examples.","Searches for new particles are reported here, with the exception of searches for supersymmetry or extended Higgs sectors as these are the subjects of separate reports."],"url":"http://arxiv.org/abs/2403.09292v1","category":"hep-ex"}
{"created":"2024-03-14 11:12:05","title":"Observation of quantum oscillations near the Mott-Ioffe-Regel limit in CaAs3","abstract":"The Mott-Ioffe-Regel limit sets the lower bound of carrier mean free path for coherent quasiparticle transport. Metallicity beyond this limit is of great interest because it is often closely related to quantum criticality and unconventional superconductivity. Progress along this direction mainly focuses on the strange-metal behaviors originating from the evolution of quasiparticle scattering rate such as linear-in-temperature resistivity, while the quasiparticle coherence phenomena in this regime are much less explored due to the short mean free path at the diffusive bound. Here we report the observation of quantum oscillations from Landau quantization near the Mott-Ioffe-Regel limit in CaAs3. Despite the insulator-like temperature dependence of resistivity, CaAs3 presents giant magnetoresistance and prominent Shubnikov-de Haas oscillations from Fermi surfaces, indicating highly coherent band transport. In contrast, the quantum oscillation is absent in the magnetic torque. The quasiparticle effective mass increases systematically with magnetic fields, manifesting a much larger value than the expectation given by magneto-infrared spectroscopy. It suggests a strong many-body renormalization effect near Fermi surface. We find that these unconventional behaviors may be explained by the interplay between the mobility edge and the van Hove singularity, which results in the formation of coherent cyclotron orbits emerging at the diffusive bound. Our results call for further study on the electron correlation effect of the van Hove singularity.","sentences":["The Mott-Ioffe-Regel limit sets the lower bound of carrier mean free path for coherent quasiparticle transport.","Metallicity beyond this limit is of great interest because it is often closely related to quantum criticality and unconventional superconductivity.","Progress along this direction mainly focuses on the strange-metal behaviors originating from the evolution of quasiparticle scattering rate such as linear-in-temperature resistivity, while the quasiparticle coherence phenomena in this regime are much less explored due to the short mean free path at the diffusive bound.","Here we report the observation of quantum oscillations from Landau quantization near the Mott-Ioffe-Regel limit in CaAs3.","Despite the insulator-like temperature dependence of resistivity, CaAs3 presents giant magnetoresistance and prominent Shubnikov-de Haas oscillations from Fermi surfaces, indicating highly coherent band transport.","In contrast, the quantum oscillation is absent in the magnetic torque.","The quasiparticle effective mass increases systematically with magnetic fields, manifesting a much larger value than the expectation given by magneto-infrared spectroscopy.","It suggests a strong many-body renormalization effect near Fermi surface.","We find that these unconventional behaviors may be explained by the interplay between the mobility edge and the van Hove singularity, which results in the formation of coherent cyclotron orbits emerging at the diffusive bound.","Our results call for further study on the electron correlation effect of the van Hove singularity."],"url":"http://arxiv.org/abs/2403.09283v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 11:07:40","title":"The elastic stored energy of initially strained, or stressed, materials: restrictions and third-order expansions","abstract":"A large variety of materials, widely encountered both in engineering applications and in the biological realm, are characterised by a non-vanishing internal stress distribution, even in the absence of external deformations or applied forces. These initial stresses are due to initial strains or microstructural changes, such as thermal expansion, manufacturing processes, volumetric growth, remodelling, etc. A common constitutive choice for modelling such materials extends the classical approach in the field theory of continuum mechanics to include, explicitly, the initial stress or strain in the elastic stored energy function. Here, we discuss why these energy functions need to satisfy some restrictions to avoid unphysical behaviours, such as non-conservation of energy, {and we derive the required restrictions from the classical assumptions of elasticity.} To illustrate their need, we perform a rigorous asymptotic expansion for proving that these restrictions on stored energy functions that depend on the strain and initial stress are required for consistency with strain energy functions of classical third-order weakly nonlinear elasticity.","sentences":["A large variety of materials, widely encountered both in engineering applications and in the biological realm, are characterised by a non-vanishing internal stress distribution, even in the absence of external deformations or applied forces.","These initial stresses are due to initial strains or microstructural changes, such as thermal expansion, manufacturing processes, volumetric growth, remodelling, etc.","A common constitutive choice for modelling such materials extends the classical approach in the field theory of continuum mechanics to include, explicitly, the initial stress or strain in the elastic stored energy function.","Here, we discuss why these energy functions need to satisfy some restrictions to avoid unphysical behaviours, such as non-conservation of energy, {and we derive the required restrictions from the classical assumptions of elasticity.}","To illustrate their need, we perform a rigorous asymptotic expansion for proving that these restrictions on stored energy functions that depend on the strain and initial stress are required for consistency with strain energy functions of classical third-order weakly nonlinear elasticity."],"url":"http://arxiv.org/abs/2403.09280v1","category":"cond-mat.soft"}
{"created":"2024-03-14 10:47:25","title":"Research and teaching in physics at the University of Franche-Comt\u00e9 1845-1970","abstract":"Recently uncovered archives at the University of Franche-Comt\\'e in Besan\\c{c}on (France) reveal a rich history of research and teaching in physics since the Faculty of Science was first established in 1845. Here, we describe a selection of notable activities conducted by the named Chairs of Physics during the period 1845-1970. We uncover a long tradition of major contributions to physics education and research, including the production of highly regarded physics textbooks that were widely used in Europe, as well as pioneering contributions to electron diffraction and microscopy, Fourier optics, and holography. These discoveries yield valuable insights into the historical development of physics research in France, and show how even a small provincial university was able to stay up-to-date with international developments across several areas of physics.","sentences":["Recently uncovered archives at the University of Franche-Comt\\'e in Besan\\c{c}on (France) reveal a rich history of research and teaching in physics since the Faculty of Science was first established in 1845.","Here, we describe a selection of notable activities conducted by the named Chairs of Physics during the period 1845-1970.","We uncover a long tradition of major contributions to physics education and research, including the production of highly regarded physics textbooks that were widely used in Europe, as well as pioneering contributions to electron diffraction and microscopy, Fourier optics, and holography.","These discoveries yield valuable insights into the historical development of physics research in France, and show how even a small provincial university was able to stay up-to-date with international developments across several areas of physics."],"url":"http://arxiv.org/abs/2403.09271v1","category":"physics.hist-ph"}
{"created":"2024-03-14 10:04:24","title":"Electronic diffusion in a normal state of high-Tc cuprate YBa$_2$Cu$_3$O$_{6+x}$","abstract":"The bad metallic phase with resistivity above the Mott-Ioffe-Regel limit, which appears also in cuprate superconductors, was recently understood by cold atom and computer simulations of the Hubbard model via charge susceptibility and charge diffusion constant. However, since reliable simulations can be typically done only at temperatures above the experimental temperatures, the question for cuprate superconductors is still open. This paper addresses this question by resorting to heat transport, which allows for the estimate of electronic diffusion and it further combines it with the resistivity to estimate the charge susceptibility. The doping and temperature dependencies of diffusion constant and charge susceptibilities are shown and discussed for two samples of YBa$_2$Cu$_3$O$_{6+x}$. Results indicate strongly incoherent transport, mean free path corresponding to the Mott-Ioffe-Regel limit for the underdoped sample at temperatures above ~200 K and significant effect of the charge susceptibility on the resistivity.","sentences":["The bad metallic phase with resistivity above the Mott-Ioffe-Regel limit, which appears also in cuprate superconductors, was recently understood by cold atom and computer simulations of the Hubbard model via charge susceptibility and charge diffusion constant.","However, since reliable simulations can be typically done only at temperatures above the experimental temperatures, the question for cuprate superconductors is still open.","This paper addresses this question by resorting to heat transport, which allows for the estimate of electronic diffusion and it further combines it with the resistivity to estimate the charge susceptibility.","The doping and temperature dependencies of diffusion constant and charge susceptibilities are shown and discussed for two samples of YBa$_2$Cu$_3$O$_{6+x}$. Results indicate strongly incoherent transport, mean free path corresponding to the Mott-Ioffe-Regel limit for the underdoped sample at temperatures above ~200 K and significant effect of the charge susceptibility on the resistivity."],"url":"http://arxiv.org/abs/2403.09241v1","category":"cond-mat.supr-con"}
{"created":"2024-03-14 09:33:37","title":"On a criterion for a cutoff regularization in the coordinate representation","abstract":"The paper discusses an applicability criterion for a cutoff regularization in the coordinate representation in the Euclidean space with a dimension larger than two. It is shown that the set of functions satisfying the criterion is not empty. As an example, an explicit function is presented. It is proved by explicit construction that there are functions satisfying the criterion in a stronger formulation.","sentences":["The paper discusses an applicability criterion for a cutoff regularization in the coordinate representation in the Euclidean space with a dimension larger than two.","It is shown that the set of functions satisfying the criterion is not empty.","As an example, an explicit function is presented.","It is proved by explicit construction that there are functions satisfying the criterion in a stronger formulation."],"url":"http://arxiv.org/abs/2403.09218v1","category":"math-ph"}
{"created":"2024-03-14 08:41:45","title":"Properties of a Fading AGN from SDSS-IV MaNGA","abstract":"We identify a fading AGN SDSS J220141.64+115124.3 from the internal Product Launch-11 (MPL-11) in Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey. The central region with a projected radius of $\\sim$2.4 kpc is characterized as LINER-like line ratios while the outskirts extended to $\\sim$15 kpc show Seyfert-like line ratios. The [OIII]$\\lambda$5007 luminosity of the Seyfert regions is a factor of 37 (2) higher than the LINER regions without (with) dust attenuation correction, suggesting that the AGN activity decreases at least $\\sim$8 $\\times$ 10$^3$ yrs ($\\sim$2.4 kpc/light-speed) ago. We model the emission line spectra in the central region with double Gaussian components (a narrow core and a broad wing) and analyze the properties of each component. The narrow core component mostly co-rotates with the stellar disc, whereas the broad wing component with a median of the velocity dispersion $\\sim$300 km s$^{-1}$ is related to a wind outflow. The kinematic position angle (PA) of the ionized gas shows a $\\sim$20{\\deg} twist from the galaxy center to 1.5 effective radius. The median of the PA difference between the gas and stellar components is as large as $\\sim$50{\\deg} within 0.4 effective radius. The tidal feature in DESI image and star-gas misalignment suggest this galaxy is a merger remnant. Combining all these observational results as well as public available X-ray and MIR luminosities, we confirm this is a fading AGN, the merger process kick-started the central engine to quasar phase which ionized gas composed of tidal debris, and now the activity of the central black hole decreases. The discontinuity in [OIII]$\\lambda$5007 flux and EQW maps is due to multiple AGN outbursts triggered by merger remnant gas inflows.","sentences":["We identify a fading AGN SDSS J220141.64+115124.3 from the internal Product Launch-11 (MPL-11) in Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey.","The central region with a projected radius of $\\sim$2.4 kpc is characterized as LINER-like line ratios while the outskirts extended to $\\sim$15 kpc show Seyfert-like line ratios.","The [OIII]$\\lambda$5007 luminosity of the Seyfert regions is a factor of 37 (2) higher than the LINER regions without (with) dust attenuation correction, suggesting that the AGN activity decreases at least $\\sim$8 $\\times$ 10$^3$ yrs ($\\sim$2.4 kpc/light-speed) ago.","We model the emission line spectra in the central region with double Gaussian components (a narrow core and a broad wing) and analyze the properties of each component.","The narrow core component mostly co-rotates with the stellar disc, whereas the broad wing component with a median of the velocity dispersion $\\sim$300 km s$^{-1}$ is related to a wind outflow.","The kinematic position angle (PA) of the ionized gas shows a $\\sim$20{\\deg} twist from the galaxy center to 1.5 effective radius.","The median of the PA difference between the gas and stellar components is as large as $\\sim$50{\\deg} within 0.4 effective radius.","The tidal feature in DESI image and star-gas misalignment suggest this galaxy is a merger remnant.","Combining all these observational results as well as public available X-ray and MIR luminosities, we confirm this is a fading AGN, the merger process kick-started the central engine to quasar phase which ionized gas composed of tidal debris, and now the activity of the central black hole decreases.","The discontinuity in [OIII]$\\lambda$5007 flux and EQW maps is due to multiple AGN outbursts triggered by merger remnant gas inflows."],"url":"http://arxiv.org/abs/2403.09174v1","category":"astro-ph.GA"}
{"created":"2024-03-14 08:32:14","title":"SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph","abstract":"With the rise of social platforms, protecting privacy has become an important issue. Privacy object detection aims to accurately locate private objects in images. It is the foundation of safeguarding individuals' privacy rights and ensuring responsible data handling practices in the digital age. Since privacy of object is not shift-invariant, the essence of the privacy object detection task is inferring object privacy based on scene information. However, privacy object detection has long been studied as a subproblem of common object detection tasks. Therefore, existing methods suffer from serious deficiencies in accuracy, generalization, and interpretability. Moreover, creating large-scale privacy datasets is difficult due to legal constraints and existing privacy datasets lack label granularity. The granularity of existing privacy detection methods remains limited to the image level. To address the above two issues, we introduce two benchmark datasets for object-level privacy detection and propose SHAN, Scene Heterogeneous graph Attention Network, a model constructs a scene heterogeneous graph from an image and utilizes self-attention mechanisms for scene inference to obtain object privacy. Through experiments, we demonstrated that SHAN performs excellently in privacy object detection tasks, with all metrics surpassing those of the baseline model.","sentences":["With the rise of social platforms, protecting privacy has become an important issue.","Privacy object detection aims to accurately locate private objects in images.","It is the foundation of safeguarding individuals' privacy rights and ensuring responsible data handling practices in the digital age.","Since privacy of object is not shift-invariant, the essence of the privacy object detection task is inferring object privacy based on scene information.","However, privacy object detection has long been studied as a subproblem of common object detection tasks.","Therefore, existing methods suffer from serious deficiencies in accuracy, generalization, and interpretability.","Moreover, creating large-scale privacy datasets is difficult due to legal constraints and existing privacy datasets lack label granularity.","The granularity of existing privacy detection methods remains limited to the image level.","To address the above two issues, we introduce two benchmark datasets for object-level privacy detection and propose SHAN, Scene Heterogeneous graph Attention Network, a model constructs a scene heterogeneous graph from an image and utilizes self-attention mechanisms for scene inference to obtain object privacy.","Through experiments, we demonstrated that SHAN performs excellently in privacy object detection tasks, with all metrics surpassing those of the baseline model."],"url":"http://arxiv.org/abs/2403.09172v1","category":"cs.CV"}
{"created":"2024-03-14 08:25:21","title":"The Algebraic Page Curve","abstract":"The Page curve describing the process of black hole evaporation is derived in terms of a family, parametrized in terms of the evaporation time, of finite type II_1 factors, associated, respectively, to the entanglement wedges of the black hole and the radiation. The so defined Page curve measures the relative continuous dimension of the black hole and the radiation along the evaporation process. The transfer of information is quantitatively defined in terms of the Murray von Neumann parameter describing the change of the spatial properties of the factors during the evaporation. In the simplest case the generator of the evaporation process is defined in terms of the action of the fundamental group of the hyperfinite type II_1 factor. In this setup the Page curve describes a phase transition with the transfer of information as order parameter. We discuss the limits of either a type I or a type III description of the black hole evaporation.","sentences":["The Page curve describing the process of black hole evaporation is derived in terms of a family, parametrized in terms of the evaporation time, of finite type II_1 factors, associated, respectively, to the entanglement wedges of the black hole and the radiation.","The so defined Page curve measures the relative continuous dimension of the black hole and the radiation along the evaporation process.","The transfer of information is quantitatively defined in terms of the Murray von Neumann parameter describing the change of the spatial properties of the factors during the evaporation.","In the simplest case the generator of the evaporation process is defined in terms of the action of the fundamental group of the hyperfinite type II_1 factor.","In this setup the Page curve describes a phase transition with the transfer of information as order parameter.","We discuss the limits of either a type I or a type III description of the black hole evaporation."],"url":"http://arxiv.org/abs/2403.09165v1","category":"hep-th"}
{"created":"2024-03-14 08:12:42","title":"An FFT based approach to account for elastic interactions in OkMC: Application to dislocation loops in iron","abstract":"Object kinetic Montecarlo (OkMC) is a fundamental tool for modeling defect evolution in volumes and times far beyond atomistic models. The elastic interaction between defects is classically considered using a dipolar approximation but this approach is limited to simple cases and can be inaccurate for large and close interacting defects. In this work a novel framework is proposed to include \"exact\" elastic interactions between defects in OkMC valid for any type of defect and anisotropic media. In this method, the elastic interaction energy of a defect is computed by volume integration of its elastic strain multiplied by the stress created by all the other defects, being both fields obtained numerically using a FFT solver. The resulting interaction energies reproduce analytical elastic solutions and show the limited accuracy of dipole approaches for close and large defects.   The OkMC framework proposed is used to simulate the evolution in space and time of self-interstitial atoms and dislocation loops in iron. It is found that including the anisotropy has a quantitative effect in the evolution of all the type of defects studied. Regarding dislocation loops, it is observed that using the \"exact\" interaction energy result in higher interactions than using the dipole approximation for close loops.","sentences":["Object kinetic Montecarlo (OkMC) is a fundamental tool for modeling defect evolution in volumes and times far beyond atomistic models.","The elastic interaction between defects is classically considered using a dipolar approximation but this approach is limited to simple cases and can be inaccurate for large and close interacting defects.","In this work a novel framework is proposed to include \"exact\" elastic interactions between defects in OkMC valid for any type of defect and anisotropic media.","In this method, the elastic interaction energy of a defect is computed by volume integration of its elastic strain multiplied by the stress created by all the other defects, being both fields obtained numerically using a FFT solver.","The resulting interaction energies reproduce analytical elastic solutions and show the limited accuracy of dipole approaches for close and large defects.   ","The OkMC framework proposed is used to simulate the evolution in space and time of self-interstitial atoms and dislocation loops in iron.","It is found that including the anisotropy has a quantitative effect in the evolution of all the type of defects studied.","Regarding dislocation loops, it is observed that using the \"exact\" interaction energy result in higher interactions than using the dipole approximation for close loops."],"url":"http://arxiv.org/abs/2403.09158v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 04:44:11","title":"Subsystem Symmetry Fractionalization and Foliated Field Theory","abstract":"Topological quantum matter exhibits a range of exotic phenomena when enriched by subdimensional symmetries. This includes new features beyond those that appear in the conventional setting of global symmetry enrichment. A recently discovered example is a type of subsystem symmetry fractionalization that occurs through a different mechanism to global symmetry fractionalization. In this work we extend the study of subsystem symmetry fractionalization through new examples derived from the general principle of embedding subsystem symmetry into higher-form symmetry. This leads to new types of symmetry fractionalization that are described by foliation dependent higher-form symmetries. This leads to field theories and lattice models that support previously unseen anomalous subsystem symmetry fractionalization. Our work expands the range of exotic topological physics that is enabled by subsystem symmetry in field theory and on the lattice.","sentences":["Topological quantum matter exhibits a range of exotic phenomena when enriched by subdimensional symmetries.","This includes new features beyond those that appear in the conventional setting of global symmetry enrichment.","A recently discovered example is a type of subsystem symmetry fractionalization that occurs through a different mechanism to global symmetry fractionalization.","In this work we extend the study of subsystem symmetry fractionalization through new examples derived from the general principle of embedding subsystem symmetry into higher-form symmetry.","This leads to new types of symmetry fractionalization that are described by foliation dependent higher-form symmetries.","This leads to field theories and lattice models that support previously unseen anomalous subsystem symmetry fractionalization.","Our work expands the range of exotic topological physics that is enabled by subsystem symmetry in field theory and on the lattice."],"url":"http://arxiv.org/abs/2403.09098v1","category":"cond-mat.str-el"}
{"created":"2024-03-14 04:32:28","title":"Desigen: A Pipeline for Controllable Design Template Generation","abstract":"Templates serve as a good starting point to implement a design (e.g., banner, slide) but it takes great effort from designers to manually create. In this paper, we present Desigen, an automatic template creation pipeline which generates background images as well as harmonious layout elements over the background. Different from natural images, a background image should preserve enough non-salient space for the overlaying layout elements. To equip existing advanced diffusion-based models with stronger spatial control, we propose two simple but effective techniques to constrain the saliency distribution and reduce the attention weight in desired regions during the background generation process. Then conditioned on the background, we synthesize the layout with a Transformer-based autoregressive generator. To achieve a more harmonious composition, we propose an iterative inference strategy to adjust the synthesized background and layout in multiple rounds. We constructed a design dataset with more than 40k advertisement banners to verify our approach. Extensive experiments demonstrate that the proposed pipeline generates high-quality templates comparable to human designers. More than a single-page design, we further show an application of presentation generation that outputs a set of theme-consistent slides. The data and code are available at https://whaohan.github.io/desigen.","sentences":["Templates serve as a good starting point to implement a design (e.g., banner, slide) but it takes great effort from designers to manually create.","In this paper, we present Desigen, an automatic template creation pipeline which generates background images as well as harmonious layout elements over the background.","Different from natural images, a background image should preserve enough non-salient space for the overlaying layout elements.","To equip existing advanced diffusion-based models with stronger spatial control, we propose two simple but effective techniques to constrain the saliency distribution and reduce the attention weight in desired regions during the background generation process.","Then conditioned on the background, we synthesize the layout with a Transformer-based autoregressive generator.","To achieve a more harmonious composition, we propose an iterative inference strategy to adjust the synthesized background and layout in multiple rounds.","We constructed a design dataset with more than 40k advertisement banners to verify our approach.","Extensive experiments demonstrate that the proposed pipeline generates high-quality templates comparable to human designers.","More than a single-page design, we further show an application of presentation generation that outputs a set of theme-consistent slides.","The data and code are available at https://whaohan.github.io/desigen."],"url":"http://arxiv.org/abs/2403.09093v1","category":"cs.CV"}
{"created":"2024-03-14 04:10:47","title":"The fundamental plane of blazars based on the black hole spin-mass energy","abstract":"We examine the fundamental plane of 91 Blazars which include FSRQs and BL Lacs with known X-ray luminosity ($L_{R}$), radio luminosity ($L_X$), and black hole mass measurements ($M$) to reflect the relationship between jet and accretion for blazars. The fundamental plane of Blazars are log$L_{R}$=${0.273}_{+0.059}^{-0.059}$log$L_X$+${0.695}_{+0.191}^{-0.191}$log$M$+${25.457}_{+2.728}^{-2.728}$ and log$L_{R}$=${0.190}_{+0.049}^{-0.049}$log$L_X$+${0.475}_{+0.157}^{-0.157}$log$M$+${28.568}_{+2.245}^{-2.245}$ after considering the effect of beam factor. Our results suggest that the jet of blazars has connection with accretion. We set the black hole spin energy as a new variable to correct the black hole mass and explore the effect of black hole spin on the fundamental relationship. We find that the fundamental plane of Blazars is effected by the black hole spin, which is similar to the previous work for AGNs. We additionally examine a new fundamental plane which is based on the black hole spin-mass energy ($M_{spin}$). The new fundamental plane (log$L_{R}$=${0.332}_{+0.081}^{-0.081}$log$L_X$+${0.502}_{+0.091}^{-0.091}$log$M_{spin}$+${22.606}_{+3.346}^{-3.346}$ with R-Square=0.575) shows that $M_{spin}$ has a better correlation coefficient comparing to the $M$ for fundamental plane of Blazars. These results support that the black hole spin should be considered as a important factor for the study of fundamental plane for Blazars. And these may further our understanding of the Blandford-Znajek process in blazars.","sentences":["We examine the fundamental plane of 91 Blazars which include FSRQs and BL Lacs with known X-ray luminosity ($L_{R}$), radio luminosity ($L_X$), and black hole mass measurements ($M$) to reflect the relationship between jet and accretion for blazars.","The fundamental plane of Blazars are log$L_{R}$=${0.273}_{+0.059}^{-0.059}$log$L_X$+${0.695}_{+0.191}^{-0.191}$log$M$+${25.457}_{+2.728}^{-2.728}$ and log$L_{R}$=${0.190}_{+0.049}^{-0.049}$log$L_X$+${0.475}_{+0.157}^{-0.157}$log$M$+${28.568}_{+2.245}^{-2.245}$ after considering the effect of beam factor.","Our results suggest that the jet of blazars has connection with accretion.","We set the black hole spin energy as a new variable to correct the black hole mass and explore the effect of black hole spin on the fundamental relationship.","We find that the fundamental plane of Blazars is effected by the black hole spin, which is similar to the previous work for AGNs.","We additionally examine a new fundamental plane which is based on the black hole spin-mass energy ($M_{spin}$).","The new fundamental plane (log$L_{R}$=${0.332}_{+0.081}^{-0.081}$log$L_X$+${0.502}_{+0.091}^{-0.091}$log$M_{spin}$+${22.606}_{+3.346}^{-3.346}$ with R-Square=0.575) shows that $M_{spin}$ has a better correlation coefficient comparing to the $M$ for fundamental plane of Blazars.","These results support that the black hole spin should be considered as a important factor for the study of fundamental plane for Blazars.","And these may further our understanding of the Blandford-Znajek process in blazars."],"url":"http://arxiv.org/abs/2403.09088v1","category":"astro-ph.HE"}
{"created":"2024-03-14 02:51:01","title":"StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control","abstract":"The enormous success of diffusion models in text-to-image synthesis has made them promising candidates for the next generation of end-user applications for image generation and editing. Previous works have focused on improving the usability of diffusion models by reducing the inference time or increasing user interactivity by allowing new, fine-grained controls such as region-based text prompts. However, we empirically find that integrating both branches of works is nontrivial, limiting the potential of diffusion models. To solve this incompatibility, we present StreamMultiDiffusion, the first real-time region-based text-to-image generation framework. By stabilizing fast inference techniques and restructuring the model into a newly proposed multi-prompt stream batch architecture, we achieve $\\times 10$ faster panorama generation than existing solutions, and the generation speed of 1.57 FPS in region-based text-to-image synthesis on a single RTX 2080 Ti GPU. Our solution opens up a new paradigm for interactive image generation named semantic palette, where high-quality images are generated in real-time from given multiple hand-drawn regions, encoding prescribed semantic meanings (e.g., eagle, girl). Our code and demo application are available at https://github.com/ironjr/StreamMultiDiffusion.","sentences":["The enormous success of diffusion models in text-to-image synthesis has made them promising candidates for the next generation of end-user applications for image generation and editing.","Previous works have focused on improving the usability of diffusion models by reducing the inference time or increasing user interactivity by allowing new, fine-grained controls such as region-based text prompts.","However, we empirically find that integrating both branches of works is nontrivial, limiting the potential of diffusion models.","To solve this incompatibility, we present StreamMultiDiffusion, the first real-time region-based text-to-image generation framework.","By stabilizing fast inference techniques and restructuring the model into a newly proposed multi-prompt stream batch architecture, we achieve $\\times 10$ faster panorama generation than existing solutions, and the generation speed of 1.57 FPS in region-based text-to-image synthesis on a single RTX 2080 Ti GPU.","Our solution opens up a new paradigm for interactive image generation named semantic palette, where high-quality images are generated in real-time from given multiple hand-drawn regions, encoding prescribed semantic meanings (e.g., eagle, girl).","Our code and demo application are available at https://github.com/ironjr/StreamMultiDiffusion."],"url":"http://arxiv.org/abs/2403.09055v1","category":"cs.CV"}
{"created":"2024-03-14 02:39:01","title":"Dual-polarization RF Channelizer Based on Kerr Soliton Microcomb Sources","abstract":"We report a dual-polarization radio frequency (RF) channelizer based on microcombs. With the tailored mismatch between the FSRs of the active and passive MRRs, wideband RF spectra can be channelized into multiple segments featuring digital compatible bandwidths via the Vernier effect. Due to the use of dual polarization states, the number of channelized spectral segments, and thus the RF instantaneous bandwidth (with a certain spectral resolution), can be doubled. In our experiments, we used 20 microcomb lines with 49 GHz FSR to achieve 20 channels for each polarization, with high RF spectra slicing resolutions at 144 MHz (TE) and 163 MHz (TM), respectively; achieving an instantaneous RF operation bandwidth of 3.1 GHz (TE) and 2.2 GHz (TM). Our approach paves the path towards monolithically integrated photonic RF receivers (the key components active and passive MRRs are all fabricated on the same platform) with reduced complexity, size, and unprecedented performance, which is important for wide RF applications with digital compatible signal detection.","sentences":["We report a dual-polarization radio frequency (RF) channelizer based on microcombs.","With the tailored mismatch between the FSRs of the active and passive MRRs, wideband RF spectra can be channelized into multiple segments featuring digital compatible bandwidths via the Vernier effect.","Due to the use of dual polarization states, the number of channelized spectral segments, and thus the RF instantaneous bandwidth (with a certain spectral resolution), can be doubled.","In our experiments, we used 20 microcomb lines with 49 GHz FSR to achieve 20 channels for each polarization, with high RF spectra slicing resolutions at 144 MHz (TE) and 163 MHz (TM), respectively; achieving an instantaneous RF operation bandwidth of 3.1 GHz (TE) and 2.2 GHz (TM).","Our approach paves the path towards monolithically integrated photonic RF receivers (the key components active and passive MRRs are all fabricated on the same platform) with reduced complexity, size, and unprecedented performance, which is important for wide RF applications with digital compatible signal detection."],"url":"http://arxiv.org/abs/2403.09051v1","category":"physics.optics"}
{"created":"2024-03-14 02:25:46","title":"Dynamical Friction and Black Holes in Ultralight Dark Matter Solitons","abstract":"We numerically simulate the motion of a black hole as it plunges radially through an ultralight dark matter soliton. We investigate the timescale in which dynamical friction reduces the kinetic energy of the black hole to a minimum, and consider the sensitivity of this timescale to changes in the ULDM particle mass, the total soliton mass, and the mass of the black hole. We contrast our numerical results with a semi-analytic treatment of dynamical friction, and find that the latter is poorly suited to this scenario. In particular, we find that the back-reaction of the soliton to the presence of the black hole is significant, resulting in oscillations in the coefficient of dynamical friction which cannot be described in the simple semi-analytical framework. Furthermore, we observe a late-time reheating effect, in which a significant amount of kinetic energy is transferred back to the black hole after an initial damping phase. This complicates the discussion of ULDM dynamical friction on the scales relevant to the final parsec problem.","sentences":["We numerically simulate the motion of a black hole as it plunges radially through an ultralight dark matter soliton.","We investigate the timescale in which dynamical friction reduces the kinetic energy of the black hole to a minimum, and consider the sensitivity of this timescale to changes in the ULDM particle mass, the total soliton mass, and the mass of the black hole.","We contrast our numerical results with a semi-analytic treatment of dynamical friction, and find that the latter is poorly suited to this scenario.","In particular, we find that the back-reaction of the soliton to the presence of the black hole is significant, resulting in oscillations in the coefficient of dynamical friction which cannot be described in the simple semi-analytical framework.","Furthermore, we observe a late-time reheating effect, in which a significant amount of kinetic energy is transferred back to the black hole after an initial damping phase.","This complicates the discussion of ULDM dynamical friction on the scales relevant to the final parsec problem."],"url":"http://arxiv.org/abs/2403.09038v1","category":"astro-ph.CO"}
{"created":"2024-03-14 01:02:28","title":"Von Neumann Algebras in Double-Scaled SYK","abstract":"It's been argued that a finite effective temperature emerges and characterizes the thermal property of double-scaled SYK model in the infinite temperature limit. On the other hand, in static patch of de Sitter, the maximally entangled state exhibits KMS condition of infinite temperature, suggesting the Type II$_1$ nature of the algebra formed by operators that are gravitationally dressed to the static patch observer. In the current work we study the double-scaled algebra generated by chord operators in double-scaled SYK model. We show that the algebra exhibits a behavior reminiscent of both perspectives. In particular, we prove that it's a Type II$_1$ factor, and the empty state with no chords satisfies the tracial property, thus aligning with the expectation in previous work. Furthermore, we show it's a cyclic separating state by exploring the modular structure of the algebra. We then study various limits of the theory and explore corresponding relations to JT gravity, theory of baby universe, and Brownian double-scaled SYK. We also present a full solution to the energy spectrum of $0$- and $1$- particle irreducible representations.","sentences":["It's been argued that a finite effective temperature emerges and characterizes the thermal property of double-scaled SYK model in the infinite temperature limit.","On the other hand, in static patch of de Sitter, the maximally entangled state exhibits KMS condition of infinite temperature, suggesting the Type II$_1$ nature of the algebra formed by operators that are gravitationally dressed to the static patch observer.","In the current work we study the double-scaled algebra generated by chord operators in double-scaled SYK model.","We show that the algebra exhibits a behavior reminiscent of both perspectives.","In particular, we prove that it's a Type II$_1$ factor, and the empty state with no chords satisfies the tracial property, thus aligning with the expectation in previous work.","Furthermore, we show it's a cyclic separating state by exploring the modular structure of the algebra.","We then study various limits of the theory and explore corresponding relations to JT gravity, theory of baby universe, and Brownian double-scaled SYK.","We also present a full solution to the energy spectrum of $0$- and $1$- particle irreducible representations."],"url":"http://arxiv.org/abs/2403.09021v1","category":"hep-th"}
{"created":"2024-03-14 00:27:35","title":"Impacts of Point Defects on Shallow Doping in Cubic Boron Arsenide: A First Principles Study","abstract":"Cubic boron arsenide (BAs) stands out as a promising material for advanced electronics, thanks to its exceptional thermal conductivity and ambipolar mobility. However, effective control of p- and n-type doping in BAs poses a significant challenge, mostly as a result of the influence of defects. In the present study, we employed density functional theory to explore the impacts of the common point defects and impurity on p-type doping Be$_\\text{B}$ and Si$_\\text{As}$, and n-type doping Si$_\\text{B}$ and Se$_\\text{As}$. We find that the most favorable points defects formed by C, O, and Si are C$_\\text{As}$, O$_\\text{B}$O$_\\text{As}$, Si$_\\text{As}$, C$_\\text{As}$Si$_\\text{B}$, and O$_\\text{B}$Si$_\\text{As}$, which have formation energies of less than $1.5$ eV. For p-type doping, C, O, and Si impurities do not harm the shallow state of Be$_\\text{B}$ doping, while only O impurity detrimentally affects Si$_\\text{As}$ doping. However for n-type dopings, C, O, and Si impurities are all harmful. Interestingly, the antisite defect pair As$_\\text{B}$B$_\\text{As}$ benefits both p- and n-type doping. The doping limitation analysis presented in this study can potentially pave the way for strategic development in the area of BAs-based electronics.","sentences":["Cubic boron arsenide (BAs) stands out as a promising material for advanced electronics, thanks to its exceptional thermal conductivity and ambipolar mobility.","However, effective control of p- and n-type doping in BAs poses a significant challenge, mostly as a result of the influence of defects.","In the present study, we employed density functional theory to explore the impacts of the common point defects and impurity on p-type doping Be$_\\text{B}$ and Si$_\\text{As}$, and n-type doping Si$_\\text{B}$ and Se$_\\text{As}$.","We find that the most favorable points defects formed by C, O, and Si are C$_\\text{As}$, O$_\\text{B}$O$_\\text{As}$, Si$_\\text{As}$, C$_\\text{As}$Si$_\\text{B}$, and O$_\\text{B}$Si$_\\text{As}$, which have formation energies of less than $1.5$ eV. For p-type doping, C, O, and Si impurities do not harm the shallow state of Be$_\\text{B}$ doping, while only O impurity detrimentally affects Si$_\\text{As}$ doping.","However for n-type dopings, C, O, and Si impurities are all harmful.","Interestingly, the antisite defect pair As$_\\text{B}$B$_\\text{As}$ benefits both p- and n-type doping.","The doping limitation analysis presented in this study can potentially pave the way for strategic development in the area of BAs-based electronics."],"url":"http://arxiv.org/abs/2403.09013v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-14 00:04:13","title":"Casimir repulsion with biased semiconductors","abstract":"Quantum and thermal fluctuations are fundamental to a plethora of phenomena within quantum optics, including the Casimir effect that acts between closely separated surfaces typically found in MEMS and NEMS devices. Particularly promising for engineering and harnessing these forces are systems out of thermal equilibrium. Recently, semiconductors with external bias have been proposed to study the nonequilibrium Casimir force. Here, we explore systems involving moderately biased semiconductors that exhibit strong repulsive Casimir forces, and we determine the effects of bias voltage, semiconductor bandgap energy, and separation for experimentally accessible configurations. Modes emitted from the semiconductors exert a repulsive force on a near surface that overcomes the attractive equilibrium Casimir force contribution at submicron distances. For the geometry of two parallel planes, those modes undergo Fabry-P\\'erot interference resulting in an oscillatory force behavior as a function of separation. Utilizing the proximity-force approximation, we predict that the repulsive force exerted on a gold sphere is well within the accuracy of typical Casimir force experiments. Our work opens up new possibilities of controlling forces at the nano- and micrometer scale with applications in sensing and actuation in nanotechnology.","sentences":["Quantum and thermal fluctuations are fundamental to a plethora of phenomena within quantum optics, including the Casimir effect that acts between closely separated surfaces typically found in MEMS and NEMS devices.","Particularly promising for engineering and harnessing these forces are systems out of thermal equilibrium.","Recently, semiconductors with external bias have been proposed to study the nonequilibrium Casimir force.","Here, we explore systems involving moderately biased semiconductors that exhibit strong repulsive Casimir forces, and we determine the effects of bias voltage, semiconductor bandgap energy, and separation for experimentally accessible configurations.","Modes emitted from the semiconductors exert a repulsive force on a near surface that overcomes the attractive equilibrium Casimir force contribution at submicron distances.","For the geometry of two parallel planes, those modes undergo Fabry-P\\'erot interference resulting in an oscillatory force behavior as a function of separation.","Utilizing the proximity-force approximation, we predict that the repulsive force exerted on a gold sphere is well within the accuracy of typical Casimir force experiments.","Our work opens up new possibilities of controlling forces at the nano- and micrometer scale with applications in sensing and actuation in nanotechnology."],"url":"http://arxiv.org/abs/2403.09007v1","category":"quant-ph"}
{"created":"2024-03-13 23:58:52","title":"Uncovering the Invisible: A Study of Gaia18ajz, a Candidate Black Hole Revealed by Microlensing","abstract":"Identifying black holes is essential for comprehending the development of stars and uncovering novel principles of physics. Gravitational microlensing provides an exceptional opportunity to examine an undetectable population of black holes in the Milky Way. In particular, long-lasting events are likely to be associated with massive lenses, including black holes. We present an analysis of the Gaia18ajz microlensing event, reported by the Gaia Science Alerts system, which has exhibited a long timescale and features indicative of the annual microlensing parallax effect. Our objective is to estimate the parameters of the lens based on the best-fitting model. We utilized photometric data obtained from the Gaia satellite and terrestrial observatories to investigate a variety of microlensing models and calculate the most probable mass and distance to the lens, taking into consideration a Galactic model as a prior. Subsequently, we applied a mass-brightness relation to evaluate the likelihood that the lens is a main sequence star. We also describe DarkLensCode, an open-source routine which computes the distribution of probable lens mass, distance and luminosity employing the Galaxy priors on stellar density and velocity for microlensing events with detected microlensing parallax. We modelled Gaia18ajz event and found its two possible models with most likely Einstein timescale of $t_\\mathrm{E}=316^{+36}_{-30}$ days and $t_\\mathrm{E}=299^{+25}_{-22}$ days. Applying Galaxy priors for stellar density and motion, we calculated the most probable lens mass of $M_L = 5.6^{+7.5}_{-2.5} M_\\odot$ located at $D_S = 1.05^{+0.78}_{-0.60}\\,\\text{kpc}$ or $M_L = 12.0^{+14.9}_{-5.4} M_\\odot$ located at $D_S = 1.18^{+0.82}_{-0.63}\\,\\text{kpc}$. Our analysis of the blended light suggests that the lens is likely a dark remnant of stellar evolution, rather than a main sequence star.","sentences":["Identifying black holes is essential for comprehending the development of stars and uncovering novel principles of physics.","Gravitational microlensing provides an exceptional opportunity to examine an undetectable population of black holes in the Milky Way.","In particular, long-lasting events are likely to be associated with massive lenses, including black holes.","We present an analysis of the Gaia18ajz microlensing event, reported by the Gaia Science Alerts system, which has exhibited a long timescale and features indicative of the annual microlensing parallax effect.","Our objective is to estimate the parameters of the lens based on the best-fitting model.","We utilized photometric data obtained from the Gaia satellite and terrestrial observatories to investigate a variety of microlensing models and calculate the most probable mass and distance to the lens, taking into consideration a Galactic model as a prior.","Subsequently, we applied a mass-brightness relation to evaluate the likelihood that the lens is a main sequence star.","We also describe DarkLensCode, an open-source routine which computes the distribution of probable lens mass, distance and luminosity employing the Galaxy priors on stellar density and velocity for microlensing events with detected microlensing parallax.","We modelled Gaia18ajz event and found its two possible models with most likely Einstein timescale of $t_\\mathrm{E}=316^{+36}_{-30}$ days and $t_\\mathrm{E}=299^{+25}_{-22}$ days.","Applying Galaxy priors for stellar density and motion, we calculated the most probable lens mass of $M_L = 5.6^{+7.5}_{-2.5} M_\\odot$ located at $D_S = 1.05^{+0.78}_{-0.60}\\,\\text{kpc}$ or $M_L = 12.0^{+14.9}_{-5.4} M_\\odot$ located at $D_S = 1.18^{+0.82}_{-0.63}\\,\\text{kpc}$.","Our analysis of the blended light suggests that the lens is likely a dark remnant of stellar evolution, rather than a main sequence star."],"url":"http://arxiv.org/abs/2403.09006v1","category":"astro-ph.GA"}
{"created":"2024-03-13 23:11:39","title":"Barrow Entropy and AdS Black Holes in RPS Thermodynamics","abstract":"In this paper, we examine the restricted phase space (RPS) thermodynamics for charged AdS black holes by considering the impact of quantum gravity on the event horizon area. The primary aim of this work is to elucidate the influence of quantum gravitational effects on thermodynamic behaviors, critical phenomena, phase transitions, and the stability of black holes. We observe that charged AdS black holes exhibit thermodynamic behavior similar to that of Van der Waals fluids when influenced by quantum gravity. Furthermore, we introduce a novel black hole thermodynamic phenomenon, which we term ``resistance of phase transitions\". Our study uncovers a violation of the homogeneity property of the Smarr relation in RPS thermodynamics due to the effects of quantum gravity.","sentences":["In this paper, we examine the restricted phase space (RPS) thermodynamics for charged AdS black holes by considering the impact of quantum gravity on the event horizon area.","The primary aim of this work is to elucidate the influence of quantum gravitational effects on thermodynamic behaviors, critical phenomena, phase transitions, and the stability of black holes.","We observe that charged AdS black holes exhibit thermodynamic behavior similar to that of Van der Waals fluids when influenced by quantum gravity.","Furthermore, we introduce a novel black hole thermodynamic phenomenon, which we term ``resistance of phase transitions\".","Our study uncovers a violation of the homogeneity property of the Smarr relation in RPS thermodynamics due to the effects of quantum gravity."],"url":"http://arxiv.org/abs/2403.08991v1","category":"hep-th"}
