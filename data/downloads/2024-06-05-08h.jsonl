{"created":"2024-06-04 17:59:36","title":"Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks","abstract":"Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a \\, x + b \\, y \\;\\mathrm{mod}\\; p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is \\emph{transient}, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing the highly structured representations in both phases; and discuss the learnt algorithm.","sentences":["Large language models can solve tasks that were not present in the training set.","This capability is believed to be due to in-context learning and skill composition.","In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks.","Specifically, we consider a finite collection of linear modular functions $z = a \\, x + b \\, y \\;\\mathrm{mod}\\;","p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing.","We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases.","We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is \\emph{transient}, necessitating early stopping.","Finally, we perform an interpretability study of the pre-trained models, revealing the highly structured representations in both phases; and discuss the learnt algorithm."],"url":"http://arxiv.org/abs/2406.02550v1","category":"cs.LG"}
{"created":"2024-06-04 17:59:32","title":"Dreamguider: Improved Training free Diffusion-based Conditional Generation","abstract":"Diffusion models have emerged as a formidable tool for training-free conditional generation.However, a key hurdle in inference-time guidance techniques is the need for compute-heavy backpropagation through the diffusion network for estimating the guidance direction. Moreover, these techniques often require handcrafted parameter tuning on a case-by-case basis. Although some recent works have introduced minimal compute methods for linear inverse problems, a generic lightweight guidance solution to both linear and non-linear guidance problems is still missing. To this end, we propose Dreamguider, a method that enables inference-time guidance without compute-heavy backpropagation through the diffusion network. The key idea is to regulate the gradient flow through a time-varying factor. Moreover, we propose an empirical guidance scale that works for a wide variety of tasks, hence removing the need for handcrafted parameter tuning. We further introduce an effective lightweight augmentation strategy that significantly boosts the performance during inference-time guidance. We present experiments using Dreamguider on multiple tasks across multiple datasets and models to show the effectiveness of the proposed modules. To facilitate further research, we will make the code public after the review process.","sentences":["Diffusion models have emerged as a formidable tool for training-free conditional generation.","However, a key hurdle in inference-time guidance techniques is the need for compute-heavy backpropagation through the diffusion network for estimating the guidance direction.","Moreover, these techniques often require handcrafted parameter tuning on a case-by-case basis.","Although some recent works have introduced minimal compute methods for linear inverse problems, a generic lightweight guidance solution to both linear and non-linear guidance problems is still missing.","To this end, we propose Dreamguider, a method that enables inference-time guidance without compute-heavy backpropagation through the diffusion network.","The key idea is to regulate the gradient flow through a time-varying factor.","Moreover, we propose an empirical guidance scale that works for a wide variety of tasks, hence removing the need for handcrafted parameter tuning.","We further introduce an effective lightweight augmentation strategy that significantly boosts the performance during inference-time guidance.","We present experiments using Dreamguider on multiple tasks across multiple datasets and models to show the effectiveness of the proposed modules.","To facilitate further research, we will make the code public after the review process."],"url":"http://arxiv.org/abs/2406.02549v1","category":"cs.CV"}
{"created":"2024-06-04 17:59:31","title":"Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation","abstract":"Recent works on open-vocabulary 3D instance segmentation show strong promise, but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on 3D clip features, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP for multi-view aggregation into 3D. As a consequence, this hampers their applicability in many real-world applications that require both fast and accurate predictions. To this end, we propose a fast yet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that effectively leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation. We address this task by generating class-agnostic 3D masks for objects in the scene and associating them with text prompts. We observe that the projection of class-agnostic 3D point cloud instances already holds instance information; thus, using SAM might only result in redundancy that unnecessarily increases the inference time. We empirically find that a better performance of matching text prompts to 3D masks can be achieved in a faster fashion with a 2D object detector. We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network. Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7\\% while operating at 22 seconds per scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.","sentences":["Recent works on open-vocabulary 3D instance segmentation show strong promise, but at the cost of slow inference speed and high computation requirements.","This high computation cost is typically due to their heavy reliance on 3D clip features, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP for multi-view aggregation into 3D.","As a consequence, this hampers their applicability in many real-world applications that require both fast and accurate predictions.","To this end, we propose a fast yet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that effectively leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation.","We address this task by generating class-agnostic 3D masks for objects in the scene and associating them with text prompts.","We observe that the projection of class-agnostic 3D point cloud instances already holds instance information; thus, using SAM might only result in redundancy that unnecessarily increases the inference time.","We empirically find that a better performance of matching text prompts to 3D masks can be achieved in a faster fashion with a 2D object detector.","We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network.","Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the best existing method in literature.","On ScanNet200 val.","set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7\\% while operating at 22 seconds per scene.","Code and model are available at github.com/aminebdj/OpenYOLO3D."],"url":"http://arxiv.org/abs/2406.02548v1","category":"cs.CV"}
{"created":"2024-06-04 17:58:33","title":"Robust and highly scalable estimation of directional couplings from time-shifted signals","abstract":"The estimation of directed couplings between the nodes of a network from indirect measurements is a central methodological challenge in scientific fields such as neuroscience, systems biology and economics. Unfortunately, the problem is generally ill-posed due to the possible presence of unknown delays in the measurements. In this paper, we offer a solution of this problem by using a variational Bayes framework, where the uncertainty over the delays is marginalized in order to obtain conservative coupling estimates. To overcome the well-known overconfidence of classical variational methods, we use a hybrid-VI scheme where the (possibly flat or multimodal) posterior over the measurement parameters is estimated using a forward KL loss while the (nearly convex) conditional posterior over the couplings is estimated using the highly scalable gradient-based VI. In our ground-truth experiments, we show that the network provides reliable and conservative estimates of the couplings, greatly outperforming similar methods such as regression DCM.","sentences":["The estimation of directed couplings between the nodes of a network from indirect measurements is a central methodological challenge in scientific fields such as neuroscience, systems biology and economics.","Unfortunately, the problem is generally ill-posed due to the possible presence of unknown delays in the measurements.","In this paper, we offer a solution of this problem by using a variational Bayes framework, where the uncertainty over the delays is marginalized in order to obtain conservative coupling estimates.","To overcome the well-known overconfidence of classical variational methods, we use a hybrid-VI scheme where the (possibly flat or multimodal) posterior over the measurement parameters is estimated using a forward KL loss while the (nearly convex) conditional posterior over the couplings is estimated using the highly scalable gradient-based VI.","In our ground-truth experiments, we show that the network provides reliable and conservative estimates of the couplings, greatly outperforming similar methods such as regression DCM."],"url":"http://arxiv.org/abs/2406.02545v1","category":"cs.LG"}
{"created":"2024-06-04 17:58:18","title":"To Believe or Not to Believe Your LLM","abstract":"We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.","sentences":["We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large.","We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers).","In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable.","This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses.","Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses.","This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected.","We conduct a series of experiments which demonstrate the advantage of our formulation.","Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest."],"url":"http://arxiv.org/abs/2406.02543v1","category":"cs.LG"}
{"created":"2024-06-04 17:57:37","title":"Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting","abstract":"Recent advancements in zero-shot video diffusion models have shown promise for text-driven video editing, but challenges remain in achieving high temporal consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting (3DGS)-based video refiner designed to enhance temporal consistency in zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian optimizing process tailored for editing dynamic monocular videos. In the first stage, Video-3DGS employs an improved version of COLMAP, referred to as MC-COLMAP, which processes original videos using a Masked and Clipped approach. For each video clip, MC-COLMAP generates the point clouds for dynamic foreground objects and complex backgrounds. These point clouds are utilized to initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent foreground and background views. Both foreground and background views are then merged with a 2D learnable parameter map to reconstruct full views. In the second stage, we leverage the reconstruction ability developed in the first stage to impose the temporal constraints on the video diffusion model. To demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive experiments across two related tasks: Video Reconstruction and Video Editing. Video-3DGS trained with 3k iterations significantly improves video reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency (x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring temporal consistency across 58 dynamic monocular videos.","sentences":["Recent advancements in zero-shot video diffusion models have shown promise for text-driven video editing, but challenges remain in achieving high temporal consistency.","To address this, we introduce Video-3DGS, a 3D Gaussian Splatting (3DGS)-based video refiner designed to enhance temporal consistency in zero-shot video editors.","Our approach utilizes a two-stage 3D Gaussian optimizing process tailored for editing dynamic monocular videos.","In the first stage, Video-3DGS employs an improved version of COLMAP, referred to as MC-COLMAP, which processes original videos using a Masked and Clipped approach.","For each video clip, MC-COLMAP generates the point clouds for dynamic foreground objects and complex backgrounds.","These point clouds are utilized to initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent foreground and background views.","Both foreground and background views are then merged with a 2D learnable parameter map to reconstruct full views.","In the second stage, we leverage the reconstruction ability developed in the first stage to impose the temporal constraints on the video diffusion model.","To demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive experiments across two related tasks: Video Reconstruction and Video Editing.","Video-3DGS trained with 3k iterations significantly improves video reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency (x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods on DAVIS dataset, respectively.","Moreover, it enhances video editing by ensuring temporal consistency across 58 dynamic monocular videos."],"url":"http://arxiv.org/abs/2406.02541v1","category":"cs.CV"}
{"created":"2024-06-04 17:57:10","title":"ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation","abstract":"Diffusion transformers (DiTs) have exhibited remarkable performance in visual generation tasks, such as generating realistic images or videos based on textual instructions. However, larger model sizes and multi-frame processing for video generation lead to increased computational and memory costs, posing challenges for practical deployment on edge devices. Post-Training Quantization (PTQ) is an effective method for reducing memory costs and computational complexity. When quantizing diffusion transformers, we find that applying existing diffusion quantization methods designed for U-Net faces challenges in preserving quality. After analyzing the major challenges for quantizing diffusion transformers, we design an improved quantization scheme: \"ViDiT-Q\": Video and Image Diffusion Transformer Quantization) to address these issues. Furthermore, we identify highly sensitive layers and timesteps hinder quantization for lower bit-widths. To tackle this, we improve ViDiT-Q with a novel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP). We validate the effectiveness of ViDiT-Q across a variety of text-to-image and video models. While baseline quantization methods fail at W8A8 and produce unreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization. ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resulting in a 2.5x memory optimization and a 1.5x latency speedup.","sentences":["Diffusion transformers (DiTs) have exhibited remarkable performance in visual generation tasks, such as generating realistic images or videos based on textual instructions.","However, larger model sizes and multi-frame processing for video generation lead to increased computational and memory costs, posing challenges for practical deployment on edge devices.","Post-Training Quantization (PTQ) is an effective method for reducing memory costs and computational complexity.","When quantizing diffusion transformers, we find that applying existing diffusion quantization methods designed for U-Net faces challenges in preserving quality.","After analyzing the major challenges for quantizing diffusion transformers, we design an improved quantization scheme: \"ViDiT-Q\": Video and Image Diffusion Transformer Quantization) to address these issues.","Furthermore, we identify highly sensitive layers and timesteps hinder quantization for lower bit-widths.","To tackle this, we improve ViDiT-Q with a novel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP).","We validate the effectiveness of ViDiT-Q across a variety of text-to-image and video models.","While baseline quantization methods fail at W8A8 and produce unreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization.","ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resulting in a 2.5x memory optimization and a 1.5x latency speedup."],"url":"http://arxiv.org/abs/2406.02540v1","category":"cs.CV"}
{"created":"2024-06-04 17:56:28","title":"Parrot: Multilingual Visual Instruction Tuning","abstract":"The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V has marked a significant step towards artificial general intelligence. Existing methods mainly focus on aligning vision encoders with LLMs through supervised fine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs' inherent ability to react to multiple languages progressively deteriorate as the training process evolves. We empirically find that the imbalanced SFT datasets, primarily composed of English-centric image-text pairs, lead to significantly reduced performance in non-English languages. This is due to the failure of aligning the vision encoder and LLM with multilingual tokens during the SFT process. In this paper, we introduce Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level. Parrot makes the visual tokens condition on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens. Specifically, to enhance non-English visual tokens alignment, we compute the cross-attention using the initial visual features and textual embeddings, the result of which is then fed into the MoE router to select the most relevant experts. The selected experts subsequently convert the initial visual tokens into language-specific visual tokens. Moreover, considering the current lack of benchmarks for evaluating multilingual capabilities within the field, we collect and make available a Massive Multilingual Multimodal Benchmark which includes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our method not only demonstrates state-of-the-art performance on multilingual MMBench and MMMB, but also excels across a broad range of multimodal tasks. Both the source code and the training dataset of Parrot will be made publicly available.","sentences":["The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V has marked a significant step towards artificial general intelligence.","Existing methods mainly focus on aligning vision encoders with LLMs through supervised fine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs' inherent ability to react to multiple languages progressively deteriorate as the training process evolves.","We empirically find that the imbalanced SFT datasets, primarily composed of English-centric image-text pairs, lead to significantly reduced performance in non-English languages.","This is due to the failure of aligning the vision encoder and LLM with multilingual tokens during the SFT process.","In this paper, we introduce Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level.","Parrot makes the visual tokens condition on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.","Specifically, to enhance non-English visual tokens alignment, we compute the cross-attention using the initial visual features and textual embeddings, the result of which is then fed into the MoE router to select the most relevant experts.","The selected experts subsequently convert the initial visual tokens into language-specific visual tokens.","Moreover, considering the current lack of benchmarks for evaluating multilingual capabilities within the field, we collect and make available a Massive Multilingual Multimodal Benchmark which includes 6 languages, 15 categories, and 12,000 questions, named as MMMB.","Our method not only demonstrates state-of-the-art performance on multilingual MMBench and MMMB, but also excels across a broad range of multimodal tasks.","Both the source code and the training dataset of Parrot will be made publicly available."],"url":"http://arxiv.org/abs/2406.02539v1","category":"cs.CV"}
{"created":"2024-06-04 17:55:38","title":"Mitigate Position Bias in Large Language Models via Scaling a Single Dimension","abstract":"Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as \"lost in the middle\", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at https://aka.ms/PositionalHidden.","sentences":["Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities.","However, they exhibit position bias, also known as \"lost in the middle\", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy.","This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias.","It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states.","Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states.","Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach.","Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states.","Our code is available at https://aka.ms/PositionalHidden."],"url":"http://arxiv.org/abs/2406.02536v1","category":"cs.CL"}
{"created":"2024-06-04 17:54:44","title":"Enhancing predictive imaging biomarker discovery through treatment effect analysis","abstract":"Identifying predictive biomarkers, which forecast individual treatment effectiveness, is crucial for personalized medicine and informs decision-making across diverse disciplines. These biomarkers are extracted from pre-treatment data, often within randomized controlled trials, and have to be distinguished from prognostic biomarkers, which are independent of treatment assignment. Our study focuses on the discovery of predictive imaging biomarkers, aiming to leverage pre-treatment images to unveil new causal relationships. Previous approaches relied on labor-intensive handcrafted or manually derived features, which may introduce biases. In response, we present a new task of discovering predictive imaging biomarkers directly from the pre-treatment images to learn relevant image features. We propose an evaluation protocol for this task to assess a model's ability to identify predictive imaging biomarkers and differentiate them from prognostic ones. It employs statistical testing and a comprehensive analysis of image feature attribution. We explore the suitability of deep learning models originally designed for estimating the conditional average treatment effect (CATE) for this task, which previously have been primarily assessed for the precision of CATE estimation, overlooking the evaluation of imaging biomarker discovery. Our proof-of-concept analysis demonstrates promising results in discovering and validating predictive imaging biomarkers from synthetic outcomes and real-world image datasets.","sentences":["Identifying predictive biomarkers, which forecast individual treatment effectiveness, is crucial for personalized medicine and informs decision-making across diverse disciplines.","These biomarkers are extracted from pre-treatment data, often within randomized controlled trials, and have to be distinguished from prognostic biomarkers, which are independent of treatment assignment.","Our study focuses on the discovery of predictive imaging biomarkers, aiming to leverage pre-treatment images to unveil new causal relationships.","Previous approaches relied on labor-intensive handcrafted or manually derived features, which may introduce biases.","In response, we present a new task of discovering predictive imaging biomarkers directly from the pre-treatment images to learn relevant image features.","We propose an evaluation protocol for this task to assess a model's ability to identify predictive imaging biomarkers and differentiate them from prognostic ones.","It employs statistical testing and a comprehensive analysis of image feature attribution.","We explore the suitability of deep learning models originally designed for estimating the conditional average treatment effect (CATE) for this task, which previously have been primarily assessed for the precision of CATE estimation, overlooking the evaluation of imaging biomarker discovery.","Our proof-of-concept analysis demonstrates promising results in discovering and validating predictive imaging biomarkers from synthetic outcomes and real-world image datasets."],"url":"http://arxiv.org/abs/2406.02534v1","category":"eess.IV"}
{"created":"2024-06-04 17:54:20","title":"SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection Ensembles for Satellite Feature Recognition","abstract":"On-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR). Such missions require precise rendezvous and proximity operations in the vicinity of non-cooperative, possibly unknown, resident space objects. Safety concerns with manned missions and lag times with ground-based control necessitate complete autonomy. In this article, we present an approach for mapping geometries and high-confidence detection of components of unknown, non-cooperative satellites on orbit. We implement accelerated 3D Gaussian splatting to learn a 3D representation of the satellite, render virtual views of the target, and ensemble the YOLOv5 object detector over the virtual views, resulting in reliable, accurate, and precise satellite component detections. The full pipeline capable of running on-board and stand to enable downstream machine intelligence tasks necessary for autonomous guidance, navigation, and control tasks.","sentences":["On-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR).","Such missions require precise rendezvous and proximity operations in the vicinity of non-cooperative, possibly unknown, resident space objects.","Safety concerns with manned missions and lag times with ground-based control necessitate complete autonomy.","In this article, we present an approach for mapping geometries and high-confidence detection of components of unknown, non-cooperative satellites on orbit.","We implement accelerated 3D Gaussian splatting to learn a 3D representation of the satellite, render virtual views of the target, and ensemble the YOLOv5 object detector over the virtual views, resulting in reliable, accurate, and precise satellite component detections.","The full pipeline capable of running on-board and stand to enable downstream machine intelligence tasks necessary for autonomous guidance, navigation, and control tasks."],"url":"http://arxiv.org/abs/2406.02533v1","category":"cs.CV"}
{"created":"2024-06-04 17:53:36","title":"SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices","abstract":"As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a \"cache\" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.","sentences":["As large language models gain widespread adoption, running them efficiently becomes crucial.","Recent works on LLM inference use speculative decoding to achieve extreme speedups.","However, most of these works implicitly design their algorithms for high-end datacenter hardware.","In this work, we ask the opposite question: how fast can we run LLMs on consumer machines?","Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD.","When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding.","We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families.","It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities.","SpecExec takes the most probable tokens continuation from the draft model to build a \"cache\" tree for the target model, which then gets validated in a single pass.","Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights."],"url":"http://arxiv.org/abs/2406.02532v1","category":"cs.CL"}
{"created":"2024-06-04 17:52:52","title":"Hybrid inflation and gravitational waves from accidentally light scalars","abstract":"We construct a hybrid-inflation model where the inflaton potential is generated radiatively, as gauge symmetries guarantee it to be accidentally flat at tree level. The model can be regarded as a small-field version of Natural Inflation, with inflation ending when the mass of a second scalar, the waterfall field, turns tachyonic. This provides a minimal, robust realisation of hybrid inflation, which predicts specific correlations among CMB observables. Tachyonic preheating leads to the production of gravitational waves which, for a low inflationary scale, might be detected by upcoming experiments. Simple variations of the model can give rise to topological defects, such as unstable domain walls. Their dynamics produces a stochastic gravitational-wave background, which can be compatible with the recent detection by pulsar timing arrays.","sentences":["We construct a hybrid-inflation model where the inflaton potential is generated radiatively, as gauge symmetries guarantee it to be accidentally flat at tree level.","The model can be regarded as a small-field version of Natural Inflation, with inflation ending when the mass of a second scalar, the waterfall field, turns tachyonic.","This provides a minimal, robust realisation of hybrid inflation, which predicts specific correlations among CMB observables.","Tachyonic preheating leads to the production of gravitational waves which, for a low inflationary scale, might be detected by upcoming experiments.","Simple variations of the model can give rise to topological defects, such as unstable domain walls.","Their dynamics produces a stochastic gravitational-wave background, which can be compatible with the recent detection by pulsar timing arrays."],"url":"http://arxiv.org/abs/2406.02531v1","category":"astro-ph.CO"}
{"created":"2024-06-04 17:51:08","title":"ReLUs Are Sufficient for Learning Implicit Neural Representations","abstract":"Motivated by the growing theoretical understanding of neural networks that employ the Rectified Linear Unit (ReLU) as their activation function, we revisit the use of ReLU activation functions for learning implicit neural representations (INRs). Inspired by second order B-spline wavelets, we incorporate a set of simple constraints to the ReLU neurons in each layer of a deep neural network (DNN) to remedy the spectral bias. This in turn enables its use for various INR tasks. Empirically, we demonstrate that, contrary to popular belief, one can learn state-of-the-art INRs based on a DNN composed of only ReLU neurons. Next, by leveraging recent theoretical works which characterize the kinds of functions ReLU neural networks learn, we provide a way to quantify the regularity of the learned function. This offers a principled approach to selecting the hyperparameters in INR architectures. We substantiate our claims through experiments in signal representation, super resolution, and computed tomography, demonstrating the versatility and effectiveness of our method. The code for all experiments can be found at https://github.com/joeshenouda/relu-inrs.","sentences":["Motivated by the growing theoretical understanding of neural networks that employ the Rectified Linear Unit (ReLU) as their activation function, we revisit the use of ReLU activation functions for learning implicit neural representations (INRs).","Inspired by second order B-spline wavelets, we incorporate a set of simple constraints to the ReLU neurons in each layer of a deep neural network (DNN) to remedy the spectral bias.","This in turn enables its use for various INR tasks.","Empirically, we demonstrate that, contrary to popular belief, one can learn state-of-the-art INRs based on a DNN composed of only ReLU neurons.","Next, by leveraging recent theoretical works which characterize the kinds of functions ReLU neural networks learn, we provide a way to quantify the regularity of the learned function.","This offers a principled approach to selecting the hyperparameters in INR architectures.","We substantiate our claims through experiments in signal representation, super resolution, and computed tomography, demonstrating the versatility and effectiveness of our method.","The code for all experiments can be found at https://github.com/joeshenouda/relu-inrs."],"url":"http://arxiv.org/abs/2406.02529v1","category":"eess.IV"}
{"created":"2024-06-04 17:50:34","title":"Scalable MatMul-free Language Modeling","abstract":"Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at \\url{https://github.com/ridgerchu/matmulfreellm}.","sentences":["Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs).","This cost only grows as LLMs scale to larger embedding dimensions and context lengths.","In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales.","Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters.","We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases.","We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training.","By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models.","To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of.","We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency.","This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.","Our code implementation is available at \\url{https://github.com/ridgerchu/matmulfreellm}."],"url":"http://arxiv.org/abs/2406.02528v1","category":"cs.CL"}
{"created":"2024-06-04 17:49:57","title":"Non-holomorphic modular flavor symmetry","abstract":"The formalism of non-holomorphic modular flavor symmetry is developed, and the Yukawa couplings are level $N$ polyharmonic Maa{\\ss} forms satisfying the Laplacian condition. We find that the integer (even) weight polyharmonic Maa{\\ss} forms of level $N$ can be decomposed into multiplets of the finite modular group $\\Gamma'_N$ ($\\Gamma_N$). The original modular invariance approach is extended by the presence of negative weight polyharmonic Maa{\\ss} forms. The non-holomorphic modular flavor symmetry can be consistently combined with the generalized CP symmetry. We present three example models for lepton sector based on the $\\Gamma_3\\cong A_4$ modular symmetry, the charged lepton masses and the neutrino oscillation data can be accommodated very well, and the predictions for the leptonic CP violation phases and the effective Majorana neutrino mass are studied.","sentences":["The formalism of non-holomorphic modular flavor symmetry is developed, and the Yukawa couplings are level $N$ polyharmonic Maa{\\ss} forms satisfying the Laplacian condition.","We find that the integer (even) weight polyharmonic Maa{\\ss} forms of level $N$ can be decomposed into multiplets of the finite modular group $\\Gamma'_N$ ($\\Gamma_N$).","The original modular invariance approach is extended by the presence of negative weight polyharmonic Maa{\\ss} forms.","The non-holomorphic modular flavor symmetry can be consistently combined with the generalized CP symmetry.","We present three example models for lepton sector based on the $\\Gamma_3\\cong A_4$ modular symmetry, the charged lepton masses and the neutrino oscillation data can be accommodated very well, and the predictions for the leptonic CP violation phases and the effective Majorana neutrino mass are studied."],"url":"http://arxiv.org/abs/2406.02527v1","category":"hep-ph"}
{"created":"2024-06-04 17:41:31","title":"RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots","abstract":"Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling. In Robotics, scaling is hindered by the lack of access to massive robot datasets. We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods. We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments. RoboCasa features realistic and diverse scenes focusing on kitchen environments. We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances. We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models. We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models. To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden. Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks. Videos and open-source code are available at https://robocasa.ai/","sentences":["Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling.","In Robotics, scaling is hindered by the lack of access to massive robot datasets.","We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods.","We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments.","RoboCasa features realistic and diverse scenes focusing on kitchen environments.","We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances.","We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models.","We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models.","To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden.","Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks.","Videos and open-source code are available at https://robocasa.ai/"],"url":"http://arxiv.org/abs/2406.02523v1","category":"cs.RO"}
{"created":"2024-06-04 17:41:25","title":"Lichen-Mediated Self-Growing Construc8on Materials for Habitat Outfitting on Mars","abstract":"As its next step in space exploration, the National Aeronautics and Space Administration (NASA) revealed plans to establish a permanent human presence on Mars. To build the centrally located, monolithic habitat, NASA has a history of experimenting with lightweight inflatable habitats to reduce mass and volume. However, the physical structures used to outfit the inflatable must generally be launched by a second spacecraft. This study proposes that, rather than shipping prefabricated outfitting elements to Mars, habitat outfitting can be realized by in-situ construction using cyanobacteria and fungi as building agents. A synthetic lichen system, composed of diazotrophic cyanobacteria and filamentous fungi, can be created to produce abundant biominerals (CaCO3) and biopolymers, which will glue Martian regolith into consolidated building blocks. These self-growing building blocks can be assembled into various structures, such as floors, walls, partitions, and furniture.","sentences":["As its next step in space exploration, the National Aeronautics and Space Administration (NASA) revealed plans to establish a permanent human presence on Mars.","To build the centrally located, monolithic habitat, NASA has a history of experimenting with lightweight inflatable habitats to reduce mass and volume.","However, the physical structures used to outfit the inflatable must generally be launched by a second spacecraft.","This study proposes that, rather than shipping prefabricated outfitting elements to Mars, habitat outfitting can be realized by in-situ construction using cyanobacteria and fungi as building agents.","A synthetic lichen system, composed of diazotrophic cyanobacteria and filamentous fungi, can be created to produce abundant biominerals (CaCO3) and biopolymers, which will glue Martian regolith into consolidated building blocks.","These self-growing building blocks can be assembled into various structures, such as floors, walls, partitions, and furniture."],"url":"http://arxiv.org/abs/2406.02522v1","category":"q-bio.CB"}
{"created":"2024-06-04 17:39:31","title":"DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering","abstract":"Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images generated from 3D CT volumes, widely used in preoperative settings but limited in intraoperative applications due to computational bottlenecks, especially for accurate but heavy physics-based Monte Carlo methods. While analytical DRR renderers offer greater efficiency, they overlook anisotropic X-ray image formation phenomena, such as Compton scattering. We present a novel approach that marries realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS). Our direction-disentangled 3DGS (DDGS) method separates the radiosity contribution into isotropic and direction-dependent components, approximating complex anisotropic interactions without intricate runtime simulations. Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency. Our method outperforms state-of-the-art techniques in image accuracy. Furthermore, our DDGS shows promise for intraoperative applications and inverse problems such as pose registration, delivering superior registration accuracy and runtime performance compared to analytical DRR methods.","sentences":["Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images generated from 3D CT volumes, widely used in preoperative settings but limited in intraoperative applications due to computational bottlenecks, especially for accurate but heavy physics-based Monte Carlo methods.","While analytical DRR renderers offer greater efficiency, they overlook anisotropic X-ray image formation phenomena, such as Compton scattering.","We present a novel approach that marries realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS).","Our direction-disentangled 3DGS (DDGS) method separates the radiosity contribution into isotropic and direction-dependent components, approximating complex anisotropic interactions without intricate runtime simulations.","Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency.","Our method outperforms state-of-the-art techniques in image accuracy.","Furthermore, our DDGS shows promise for intraoperative applications and inverse problems such as pose registration, delivering superior registration accuracy and runtime performance compared to analytical DRR methods."],"url":"http://arxiv.org/abs/2406.02518v1","category":"cs.CV"}
{"created":"2024-06-04 17:39:23","title":"Deterministic Reversible Data Augmentation for Neural Machine Translation","abstract":"Data augmentation is an effective way to diversify corpora in machine translation, but previous methods may introduce semantic inconsistency between original and augmented data because of irreversible operations and random subword sampling procedures. To generate both symbolically diverse and semantically consistent augmentation data, we propose Deterministic Reversible Data Augmentation (DRDA), a simple but effective data augmentation method for neural machine translation. DRDA adopts deterministic segmentations and reversible operations to generate multi-granularity subword representations and pulls them closer together with multi-view techniques. With no extra corpora or model changes required, DRDA outperforms strong baselines on several translation tasks with a clear margin (up to 4.3 BLEU gain over Transformer) and exhibits good robustness in noisy, low-resource, and cross-domain datasets.","sentences":["Data augmentation is an effective way to diversify corpora in machine translation, but previous methods may introduce semantic inconsistency between original and augmented data because of irreversible operations and random subword sampling procedures.","To generate both symbolically diverse and semantically consistent augmentation data, we propose Deterministic Reversible Data Augmentation (DRDA), a simple but effective data augmentation method for neural machine translation.","DRDA adopts deterministic segmentations and reversible operations to generate multi-granularity subword representations and pulls them closer together with multi-view techniques.","With no extra corpora or model changes required, DRDA outperforms strong baselines on several translation tasks with a clear margin (up to 4.3 BLEU gain over Transformer) and exhibits good robustness in noisy, low-resource, and cross-domain datasets."],"url":"http://arxiv.org/abs/2406.02517v1","category":"cs.CL"}
{"created":"2024-06-04 17:32:52","title":"V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation","abstract":"In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent. A common approach involves leveraging generative models to enhance adapters for controlled generation. However, control signals (e.g., text, audio, reference image, pose, depth map, etc.) can vary in strength. Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions. In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as facial pose and reference image. However, direct training with weak signals often leads to difficulties in convergence. To address this, we propose V-Express, a simple method that balances different control signals through the progressive training and the conditional dropout operation. Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account the facial pose, reference image, and audio. The experimental results demonstrate that our method can effectively generate portrait videos controlled by audio. Furthermore, a potential solution is provided for the simultaneous and effective use of conditions of varying strengths.","sentences":["In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent.","A common approach involves leveraging generative models to enhance adapters for controlled generation.","However, control signals (e.g., text, audio, reference image, pose, depth map, etc.) can vary in strength.","Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions.","In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as facial pose and reference image.","However, direct training with weak signals often leads to difficulties in convergence.","To address this, we propose V-Express, a simple method that balances different control signals through the progressive training and the conditional dropout operation.","Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account the facial pose, reference image, and audio.","The experimental results demonstrate that our method can effectively generate portrait videos controlled by audio.","Furthermore, a potential solution is provided for the simultaneous and effective use of conditions of varying strengths."],"url":"http://arxiv.org/abs/2406.02511v1","category":"cs.CV"}
{"created":"2024-06-04 17:29:21","title":"Fairness-Optimized Synthetic EHR Generation for Arbitrary Downstream Predictive Tasks","abstract":"Among various aspects of ensuring the responsible design of AI tools for healthcare applications, addressing fairness concerns has been a key focus area. Specifically, given the wide spread of electronic health record (EHR) data and their huge potential to inform a wide range of clinical decision support tasks, improving fairness in this category of health AI tools is of key importance. While such a broad problem (that is, mitigating fairness in EHR-based AI models) has been tackled using various methods, task- and model-agnostic methods are noticeably rare. In this study, we aimed to target this gap by presenting a new pipeline that generates synthetic EHR data, which is not only consistent with (faithful to) the real EHR data but also can reduce the fairness concerns (defined by the end-user) in the downstream tasks, when combined with the real data. We demonstrate the effectiveness of our proposed pipeline across various downstream tasks and two different EHR datasets. Our proposed pipeline can add a widely applicable and complementary tool to the existing toolbox of methods to address fairness in health AI applications such as those modifying the design of a downstream model. The codebase for our project is available at https://github.com/healthylaife/FairSynth","sentences":["Among various aspects of ensuring the responsible design of AI tools for healthcare applications, addressing fairness concerns has been a key focus area.","Specifically, given the wide spread of electronic health record (EHR) data and their huge potential to inform a wide range of clinical decision support tasks, improving fairness in this category of health AI tools is of key importance.","While such a broad problem (that is, mitigating fairness in EHR-based AI models) has been tackled using various methods, task- and model-agnostic methods are noticeably rare.","In this study, we aimed to target this gap by presenting a new pipeline that generates synthetic EHR data, which is not only consistent with (faithful to) the real EHR data but also can reduce the fairness concerns (defined by the end-user) in the downstream tasks, when combined with the real data.","We demonstrate the effectiveness of our proposed pipeline across various downstream tasks and two different EHR datasets.","Our proposed pipeline can add a widely applicable and complementary tool to the existing toolbox of methods to address fairness in health AI applications such as those modifying the design of a downstream model.","The codebase for our project is available at https://github.com/healthylaife/FairSynth"],"url":"http://arxiv.org/abs/2406.02510v1","category":"cs.LG"}
{"created":"2024-06-04 17:27:19","title":"CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation","abstract":"Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users. However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control. To address this issue, we introduce CamCo, which allows fine-grained Camera pose Control for image-to-video generation. We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Pl\\\"ucker coordinates. To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion. Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion. Project page: https://ir1d.github.io/CamCo/","sentences":["Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users.","However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control.","To address this issue, we introduce CamCo, which allows fine-grained Camera pose Control for image-to-video generation.","We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Pl\\\"ucker coordinates.","To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps.","Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion.","Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion.","Project page: https://ir1d.github.io/CamCo/"],"url":"http://arxiv.org/abs/2406.02509v1","category":"cs.CV"}
{"created":"2024-06-04 17:27:07","title":"Radial canonical AdS$_3$ gravity and $T\\bar{T}$","abstract":"We employ an ADM deparametrization strategy to discuss the radial canonical formalism of asymptotically AdS$_3$ gravity. It leads to the identification of a radial 'time' before quantization, which is the volume time, canonically conjugate to York time. Holographically, this allows to interpret the semi-classical path integral of $T\\bar T$ theory as a Schr\\\"odinger wavefunctional satisfying a Schr\\\"odinger evolution equation in volume time, and the $T\\bar T$ operator expectation value in terms of the Hamiltonian that generates volume time translations -- both consistent with cut-off holography. We make use of the canonical perspective to construct the rotating BTZ solution from the Hamilton-Jacobi equation, with a finite cut-off energy spectrum that has a known holographic $T\\bar T$ interpretation, as well as semi-classical Wheeler-DeWitt states for that solution.","sentences":["We employ an ADM deparametrization strategy to discuss the radial canonical formalism of asymptotically AdS$_3$ gravity.","It leads to the identification of a radial 'time' before quantization, which is the volume time, canonically conjugate to York time.","Holographically, this allows to interpret the semi-classical path integral of $T\\bar T$ theory as a Schr\\\"odinger wavefunctional satisfying a Schr\\\"odinger evolution equation in volume time, and the $T\\bar T$ operator expectation value in terms of the Hamiltonian that generates volume time translations -- both consistent with cut-off holography.","We make use of the canonical perspective to construct the rotating BTZ solution from the Hamilton-Jacobi equation, with a finite cut-off energy spectrum that has a known holographic $T\\bar T$ interpretation, as well as semi-classical Wheeler-DeWitt states for that solution."],"url":"http://arxiv.org/abs/2406.02508v1","category":"hep-th"}
{"created":"2024-06-04 17:25:59","title":"Guiding a Diffusion Model with a Bad Version of Itself","abstract":"The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.","sentences":["The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt.","The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation.","These effects seem inherently entangled, and thus hard to control.","We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model.","This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks.","Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality."],"url":"http://arxiv.org/abs/2406.02507v1","category":"cs.CV"}
{"created":"2024-06-04 17:22:05","title":"Towards an optimal marked correlation function analysis for the detection of modified gravity","abstract":"Modified gravity (MG) theories have emerged as a promising alternative to explain the late-time acceleration of the Universe. However, the detection of MG in observations of the large-scale structure remains challenging due to the screening mechanisms that obscure any deviations from General Relativity (GR) in high-density regions. The marked two-point correlation function offers a promising approach to potentially detect MG signals. This work investigates novel marks based on large-scale environment estimates but also that exploit the anti-correlation between objects in low- and high-density regions. This is the first time discreteness effects in density-dependent marked correlation functions are investigated in depth. We assess the performance of various marks to distinguish GR from MG by using the ELEPHANT simulations, comprised of realisations of GR as well as $f(R)$ and nDGP gravity. In addition, discreteness effects are studied using the high-density Covmos catalogues. We establish a robust method to correct for shot-noise effects that allows the recovery of the true signal with an accuracy below $5\\%$ over a wide range of scales. We find such correction to be crucial to measure the amplitude of the marked correlation function in an unbiased manner. Furthermore, we demonstrate that marks, anti-correlating objects in low- and high-density regions, are among the most effective in distinguishing between MG and GR. We report differences in the marked correlation function between $f(R)$ with $|f_{R0}|=10^{-6}$ and GR simulations of the order of 3-5$\\sigma$ in real space up to scales of about $80\\, h^{-1} \\, {\\rm Mpc}$. The redshift-space monopole exhibits similar features and performances. The combination of the proposed $\\tanh$-mark with shot-noise correction paves the way towards an optimal approach for the detection of MG in current and future galaxy spectroscopic surveys.","sentences":["Modified gravity (MG) theories have emerged as a promising alternative to explain the late-time acceleration of the Universe.","However, the detection of MG in observations of the large-scale structure remains challenging due to the screening mechanisms that obscure any deviations from General Relativity (GR) in high-density regions.","The marked two-point correlation function offers a promising approach to potentially detect MG signals.","This work investigates novel marks based on large-scale environment estimates but also that exploit the anti-correlation between objects in low- and high-density regions.","This is the first time discreteness effects in density-dependent marked correlation functions are investigated in depth.","We assess the performance of various marks to distinguish GR from MG by using the ELEPHANT simulations, comprised of realisations of GR as well as $f(R)$ and nDGP gravity.","In addition, discreteness effects are studied using the high-density Covmos catalogues.","We establish a robust method to correct for shot-noise effects that allows the recovery of the true signal with an accuracy below $5\\%$ over a wide range of scales.","We find such correction to be crucial to measure the amplitude of the marked correlation function in an unbiased manner.","Furthermore, we demonstrate that marks, anti-correlating objects in low- and high-density regions, are among the most effective in distinguishing between MG and GR.","We report differences in the marked correlation function between $f(R)$ with $|f_{R0}|=10^{-6}$ and GR simulations of the order of 3-5$\\sigma$ in real space up to scales of about $80\\, h^{-1} \\, {\\rm Mpc}$. The redshift-space monopole exhibits similar features and performances.","The combination of the proposed $\\tanh$-mark with shot-noise correction paves the way towards an optimal approach for the detection of MG in current and future galaxy spectroscopic surveys."],"url":"http://arxiv.org/abs/2406.02504v1","category":"astro-ph.CO"}
{"created":"2024-06-04 17:20:30","title":"The computational power of random quantum circuits in arbitrary geometries","abstract":"Empirical evidence for a gap between the computational powers of classical and quantum computers has been provided by experiments that sample the output distributions of two-dimensional quantum circuits. Many attempts to close this gap have utilized classical simulations based on tensor network techniques, and their limitations shed light on the improvements to quantum hardware required to frustrate classical simulability. In particular, quantum computers having in excess of $\\sim 50$ qubits are primarily vulnerable to classical simulation due to restrictions on their gate fidelity and their connectivity, the latter determining how many gates are required (and therefore how much infidelity is suffered) in generating highly-entangled states. Here, we describe recent hardware upgrades to Quantinuum's H2 quantum computer enabling it to operate on up to $56$ qubits with arbitrary connectivity and $99.843(5)\\%$ two-qubit gate fidelity. Utilizing the flexible connectivity of H2, we present data from random circuit sampling in highly connected geometries, doing so at unprecedented fidelities and a scale that appears to be beyond the capabilities of state-of-the-art classical algorithms. The considerable difficulty of classically simulating H2 is likely limited only by qubit number, demonstrating the promise and scalability of the QCCD architecture as continued progress is made towards building larger machines.","sentences":["Empirical evidence for a gap between the computational powers of classical and quantum computers has been provided by experiments that sample the output distributions of two-dimensional quantum circuits.","Many attempts to close this gap have utilized classical simulations based on tensor network techniques, and their limitations shed light on the improvements to quantum hardware required to frustrate classical simulability.","In particular, quantum computers having in excess of $\\sim 50$ qubits are primarily vulnerable to classical simulation due to restrictions on their gate fidelity and their connectivity, the latter determining how many gates are required (and therefore how much infidelity is suffered) in generating highly-entangled states.","Here, we describe recent hardware upgrades to Quantinuum's H2 quantum computer enabling it to operate on up to $56$ qubits with arbitrary connectivity and $99.843(5)\\%$ two-qubit gate fidelity.","Utilizing the flexible connectivity of H2, we present data from random circuit sampling in highly connected geometries, doing so at unprecedented fidelities and a scale that appears to be beyond the capabilities of state-of-the-art classical algorithms.","The considerable difficulty of classically simulating H2 is likely limited only by qubit number, demonstrating the promise and scalability of the QCCD architecture as continued progress is made towards building larger machines."],"url":"http://arxiv.org/abs/2406.02501v1","category":"quant-ph"}
{"created":"2024-06-04 17:18:40","title":"Demystifying the Compression of Mixture-of-Experts Through a Unified Framework","abstract":"Scaling large language models has revolutionized the performance across diverse domains, yet the continual growth in model size poses significant challenges for real-world deployment. The Mixture of Experts (MoE) approach addresses this by dynamically selecting and activating only a subset of experts, significantly reducing computational costs while maintaining high performance. However, MoE introduces potential redundancy (e.g., parameters) and extra costs (e.g., communication overhead). Despite numerous compression techniques developed for mitigating the redundancy in dense models, the compression of MoE remains under-explored. We first bridge this gap with a cutting-edge unified framework that not only seamlessly integrates mainstream compression methods but also helps systematically understand MoE compression. This framework approaches compression from two perspectives: Expert Slimming which compresses individual experts and Expert Trimming which removes structured modules. Within this framework, we explore the optimization space unexplored by existing methods,and further introduce aggressive Expert Trimming techniques, i.e., Layer Drop and Block Drop, to eliminate redundancy at larger scales. Based on these insights,we present a comprehensive recipe to guide practitioners in compressing MoE effectively. Extensive experimental results demonstrate the effectiveness of the compression methods under our framework and the proposed recipe, achieving a 6.05x speedup and only 20.0GB memory usage while maintaining over 92% of performance on Mixtral-8x7B.","sentences":["Scaling large language models has revolutionized the performance across diverse domains, yet the continual growth in model size poses significant challenges for real-world deployment.","The Mixture of Experts (MoE) approach addresses this by dynamically selecting and activating only a subset of experts, significantly reducing computational costs while maintaining high performance.","However, MoE introduces potential redundancy (e.g., parameters) and extra costs (e.g., communication overhead).","Despite numerous compression techniques developed for mitigating the redundancy in dense models, the compression of MoE remains under-explored.","We first bridge this gap with a cutting-edge unified framework that not only seamlessly integrates mainstream compression methods but also helps systematically understand MoE compression.","This framework approaches compression from two perspectives: Expert Slimming which compresses individual experts and Expert Trimming which removes structured modules.","Within this framework, we explore the optimization space unexplored by existing methods,and further introduce aggressive Expert Trimming techniques, i.e., Layer Drop and Block Drop, to eliminate redundancy at larger scales.","Based on these insights,we present a comprehensive recipe to guide practitioners in compressing MoE effectively.","Extensive experimental results demonstrate the effectiveness of the compression methods under our framework and the proposed recipe, achieving a 6.05x speedup and only 20.0GB memory usage while maintaining over 92% of performance on Mixtral-8x7B."],"url":"http://arxiv.org/abs/2406.02500v1","category":"cs.LG"}
{"created":"2024-06-04 17:15:25","title":"Dropout MPC: An Ensemble Neural MPC Approach for Systems with Learned Dynamics","abstract":"Neural networks are lately more and more often being used in the context of data-driven control, as an approximate model of the true system dynamics. Model Predictive Control (MPC) adopts this practise leading to neural MPC strategies. This raises a question of whether the trained neural network has converged and generalized in a way that the learned model encapsulates an accurate approximation of the true dynamic model of the system, thus making it a reliable choice for model-based control, especially for disturbed and uncertain systems. To tackle that, we propose Dropout MPC, a novel sampling-based ensemble neural MPC algorithm that employs the Monte-Carlo dropout technique on the learned system model. The closed loop is based on an ensemble of predictive controllers, that are used simultaneously at each time-step for trajectory optimization. Each member of the ensemble influences the control input, based on a weighted voting scheme, thus by employing different realizations of the learned system dynamics, neural control becomes more reliable by design. An additional strength of the method is that it offers by design a way to estimate future uncertainty, leading to cautious control. While the method aims in general at uncertain systems with complex dynamics, where models derived from first principles are hard to infer, to showcase the application we utilize data gathered in the laboratory from a real mobile manipulator and employ the proposed algorithm for the navigation of the robot in simulation.","sentences":["Neural networks are lately more and more often being used in the context of data-driven control, as an approximate model of the true system dynamics.","Model Predictive Control (MPC) adopts this practise leading to neural MPC strategies.","This raises a question of whether the trained neural network has converged and generalized in a way that the learned model encapsulates an accurate approximation of the true dynamic model of the system, thus making it a reliable choice for model-based control, especially for disturbed and uncertain systems.","To tackle that, we propose Dropout MPC, a novel sampling-based ensemble neural MPC algorithm that employs the Monte-Carlo dropout technique on the learned system model.","The closed loop is based on an ensemble of predictive controllers, that are used simultaneously at each time-step for trajectory optimization.","Each member of the ensemble influences the control input, based on a weighted voting scheme, thus by employing different realizations of the learned system dynamics, neural control becomes more reliable by design.","An additional strength of the method is that it offers by design a way to estimate future uncertainty, leading to cautious control.","While the method aims in general at uncertain systems with complex dynamics, where models derived from first principles are hard to infer, to showcase the application we utilize data gathered in the laboratory from a real mobile manipulator and employ the proposed algorithm for the navigation of the robot in simulation."],"url":"http://arxiv.org/abs/2406.02497v1","category":"eess.SY"}
{"created":"2024-06-04 17:14:31","title":"Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability","abstract":"Kolmogorov-Arnold Networks (KAN) is a groundbreaking model recently proposed by the MIT team, representing a revolutionary approach with the potential to be a game-changer in the field. This innovative concept has rapidly garnered worldwide interest within the AI community. Inspired by the Kolmogorov-Arnold representation theorem, KAN utilizes spline-parametrized univariate functions in place of traditional linear weights, enabling them to dynamically learn activation patterns and significantly enhancing interpretability. In this paper, we explore the application of KAN to time series forecasting and propose two variants: T-KAN and MT-KAN. T-KAN is designed to detect concept drift within time series and can explain the nonlinear relationships between predictions and previous time steps through symbolic regression, making it highly interpretable in dynamically changing environments. MT-KAN, on the other hand, improves predictive performance by effectively uncovering and leveraging the complex relationships among variables in multivariate time series. Experiments validate the effectiveness of these approaches, demonstrating that T-KAN and MT-KAN significantly outperform traditional methods in time series forecasting tasks, not only enhancing predictive accuracy but also improving model interpretability. This research opens new avenues for adaptive forecasting models, highlighting the potential of KAN as a powerful and interpretable tool in predictive analytics.","sentences":["Kolmogorov-Arnold Networks (KAN) is a groundbreaking model recently proposed by the MIT team, representing a revolutionary approach with the potential to be a game-changer in the field.","This innovative concept has rapidly garnered worldwide interest within the AI community.","Inspired by the Kolmogorov-Arnold representation theorem, KAN utilizes spline-parametrized univariate functions in place of traditional linear weights, enabling them to dynamically learn activation patterns and significantly enhancing interpretability.","In this paper, we explore the application of KAN to time series forecasting and propose two variants: T-KAN and MT-KAN.","T-KAN is designed to detect concept drift within time series and can explain the nonlinear relationships between predictions and previous time steps through symbolic regression, making it highly interpretable in dynamically changing environments.","MT-KAN, on the other hand, improves predictive performance by effectively uncovering and leveraging the complex relationships among variables in multivariate time series.","Experiments validate the effectiveness of these approaches, demonstrating that T-KAN and MT-KAN significantly outperform traditional methods in time series forecasting tasks, not only enhancing predictive accuracy but also improving model interpretability.","This research opens new avenues for adaptive forecasting models, highlighting the potential of KAN as a powerful and interpretable tool in predictive analytics."],"url":"http://arxiv.org/abs/2406.02496v1","category":"cs.LG"}
{"created":"2024-06-04 17:13:10","title":"GenS: Generalizable Neural Surface Reconstruction from Multi-View Images","abstract":"Combining the signed distance function (SDF) and differentiable volume rendering has emerged as a powerful paradigm for surface reconstruction from multi-view images without 3D supervision. However, current methods are impeded by requiring long-time per-scene optimizations and cannot generalize to new scenes. In this paper, we present GenS, an end-to-end generalizable neural surface reconstruction model. Unlike coordinate-based methods that train a separate network for each scene, we construct a generalized multi-scale volume to directly encode all scenes. Compared with existing solutions, our representation is more powerful, which can recover high-frequency details while maintaining global smoothness. Meanwhile, we introduce a multi-scale feature-metric consistency to impose the multi-view consistency in a more discriminative multi-scale feature space, which is robust to the failures of the photometric consistency. And the learnable feature can be self-enhanced to continuously improve the matching accuracy and mitigate aggregation ambiguity. Furthermore, we design a view contrast loss to force the model to be robust to those regions covered by few viewpoints through distilling the geometric prior from dense input to sparse input. Extensive experiments on popular benchmarks show that our model can generalize well to new scenes and outperform existing state-of-the-art methods even those employing ground-truth depth supervision. Code is available at https://github.com/prstrive/GenS.","sentences":["Combining the signed distance function (SDF) and differentiable volume rendering has emerged as a powerful paradigm for surface reconstruction from multi-view images without 3D supervision.","However, current methods are impeded by requiring long-time per-scene optimizations and cannot generalize to new scenes.","In this paper, we present GenS, an end-to-end generalizable neural surface reconstruction model.","Unlike coordinate-based methods that train a separate network for each scene, we construct a generalized multi-scale volume to directly encode all scenes.","Compared with existing solutions, our representation is more powerful, which can recover high-frequency details while maintaining global smoothness.","Meanwhile, we introduce a multi-scale feature-metric consistency to impose the multi-view consistency in a more discriminative multi-scale feature space, which is robust to the failures of the photometric consistency.","And the learnable feature can be self-enhanced to continuously improve the matching accuracy and mitigate aggregation ambiguity.","Furthermore, we design a view contrast loss to force the model to be robust to those regions covered by few viewpoints through distilling the geometric prior from dense input to sparse input.","Extensive experiments on popular benchmarks show that our model can generalize well to new scenes and outperform existing state-of-the-art methods even those employing ground-truth depth supervision.","Code is available at https://github.com/prstrive/GenS."],"url":"http://arxiv.org/abs/2406.02495v1","category":"cs.CV"}
{"created":"2024-06-04 17:07:18","title":"Toggleability Spaces of Fences","abstract":"We completely describe the order ideal (resp. antichain) toggleability space for general fences: the space of statistics which are linear combinations of order ideal (antichain) indicator functions and equal to a constant plus a linear combination of toggleability statistics. This allows us to strengthen some homomesies under rowmotion on fences proven by Elizalde et al. and prove some new homomesy results for combinatorial, piecewise-linear, and birational rowmotion.","sentences":["We completely describe the order ideal (resp.","antichain) toggleability space for general fences: the space of statistics which are linear combinations of order ideal (antichain) indicator functions and equal to a constant plus a linear combination of toggleability statistics.","This allows us to strengthen some homomesies under rowmotion on fences proven by Elizalde et al. and prove some new homomesy results for combinatorial, piecewise-linear, and birational rowmotion."],"url":"http://arxiv.org/abs/2406.02493v1","category":"math.CO"}
{"created":"2024-06-04 17:05:45","title":"Distrust of social media influencers in America","abstract":"The popularity of social media influencers (SMI) as a means for businesses and causes to engage with the public and develop followers is undeniable. However, the use of SMI have been scrutinized due to various scandals that reflect poorly on brands and firms. Consequently, the distrust of SMI can create the potential for damage to a brand if audiences are not receptive to communications and messaging. This study (n=351) shares insights and findings of the apparent distrust of SMI that were discovered during the data analysis phase of my dissertation (Berry, 2024a). The study examines levels of trust and distrust toward SMI in the United States according to various demographic characteristics of respondents, specifically, age, gender, income level, education level, and region of the United States. Chi square analysis of the variables and a predictive model of trust of SMI are presented. Finally, recommendations and suggestions for future research are discussed.","sentences":["The popularity of social media influencers (SMI) as a means for businesses and causes to engage with the public and develop followers is undeniable.","However, the use of SMI have been scrutinized due to various scandals that reflect poorly on brands and firms.","Consequently, the distrust of SMI can create the potential for damage to a brand if audiences are not receptive to communications and messaging.","This study (n=351) shares insights and findings of the apparent distrust of SMI that were discovered during the data analysis phase of my dissertation (Berry, 2024a).","The study examines levels of trust and distrust toward SMI in the United States according to various demographic characteristics of respondents, specifically, age, gender, income level, education level, and region of the United States.","Chi square analysis of the variables and a predictive model of trust of SMI are presented.","Finally, recommendations and suggestions for future research are discussed."],"url":"http://arxiv.org/abs/2406.02492v1","category":"econ.GN"}
{"created":"2024-06-04 17:00:11","title":"Stability theory over toroidal or Novikov type base and Canonical modifications","abstract":"We set up a generalization of ubiquitous one parameter families in algebraic geometry and its use for stability theories ([Mum65, HL14, AHLH23]) to families over toric varieties, Novikov type rings, or its complex analytic analogues. The language allows us to reformulate degenerations of \"irrational\" direction in various literatures as canonical objects in a unified manner.   Compatibly, we generalize the stable reduction type theorem for $\\Theta$-stratification in [AHLH23] of Langton type to our higher rank setup.","sentences":["We set up a generalization of ubiquitous one parameter families in algebraic geometry and its use for stability theories ([Mum65, HL14, AHLH23]) to families over toric varieties, Novikov type rings, or its complex analytic analogues.","The language allows us to reformulate degenerations of \"irrational\" direction in various literatures as canonical objects in a unified manner.   ","Compatibly, we generalize the stable reduction type theorem for $\\Theta$-stratification in [AHLH23] of Langton type to our higher rank setup."],"url":"http://arxiv.org/abs/2406.02489v1","category":"math.AG"}
{"created":"2024-06-04 16:59:11","title":"Language-Universal Speech Attributes Modeling for Zero-Shot Multilingual Spoken Keyword Recognition","abstract":"We propose a novel language-universal approach to end-to-end automatic spoken keyword recognition (SKR) leveraging upon (i) a self-supervised pre-trained model, and (ii) a set of universal speech attributes (manner and place of articulation). Specifically, Wav2Vec2.0 is used to generate robust speech representations, followed by a linear output layer to produce attribute sequences. A non-trainable pronunciation model then maps sequences of attributes into spoken keywords in a multilingual setting. Experiments on the Multilingual Spoken Words Corpus show comparable performances to character- and phoneme-based SKR in seen languages. The inclusion of domain adversarial training (DAT) improves the proposed framework, outperforming both character- and phoneme-based SKR approaches with 13.73% and 17.22% relative word error rate (WER) reduction in seen languages, and achieves 32.14% and 19.92% WER reduction for unseen languages in zero-shot settings.","sentences":["We propose a novel language-universal approach to end-to-end automatic spoken keyword recognition (SKR) leveraging upon (i) a self-supervised pre-trained model, and (ii) a set of universal speech attributes (manner and place of articulation).","Specifically, Wav2Vec2.0 is used to generate robust speech representations, followed by a linear output layer to produce attribute sequences.","A non-trainable pronunciation model then maps sequences of attributes into spoken keywords in a multilingual setting.","Experiments on the Multilingual Spoken Words Corpus show comparable performances to character- and phoneme-based SKR in seen languages.","The inclusion of domain adversarial training (DAT) improves the proposed framework, outperforming both character- and phoneme-based SKR approaches with 13.73% and 17.22% relative word error rate (WER) reduction in seen languages, and achieves 32.14% and 19.92% WER reduction for unseen languages in zero-shot settings."],"url":"http://arxiv.org/abs/2406.02488v1","category":"eess.AS"}
{"created":"2024-06-04 16:54:28","title":"Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation","abstract":"Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions. Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures. To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis. We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons. Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the model's precision in capturing intricate pose details. We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around 13% improvement over the established technique ControlNet. The project link and code is available at https://github.com/ai-med/StablePose.","sentences":["Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions.","Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures.","To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models.","Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis.","We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons.","Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels.","Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the model's precision in capturing intricate pose details.","We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios.","Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around 13% improvement over the established technique ControlNet.","The project link and code is available at https://github.com/ai-med/StablePose."],"url":"http://arxiv.org/abs/2406.02485v1","category":"cs.CV"}
{"created":"2024-06-04 16:51:42","title":"How Do Neural Spoofing Countermeasures Detect Partially Spoofed Audio?","abstract":"Partially manipulating a sentence can greatly change its meaning. Recent work shows that countermeasures (CMs) trained on partially spoofed audio can effectively detect such spoofing. However, the current understanding of the decision-making process of CMs is limited. We utilize Grad-CAM and introduce a quantitative analysis metric to interpret CMs' decisions. We find that CMs prioritize the artifacts of transition regions created when concatenating bona fide and spoofed audio. This focus differs from that of CMs trained on fully spoofed audio, which concentrate on the pattern differences between bona fide and spoofed parts. Our further investigation explains the varying nature of CMs' focus while making correct or incorrect predictions. These insights provide a basis for the design of CM models and the creation of datasets. Moreover, this work lays a foundation of interpretability in the field of partial spoofed audio detection that has not been well explored previously.","sentences":["Partially manipulating a sentence can greatly change its meaning.","Recent work shows that countermeasures (CMs) trained on partially spoofed audio can effectively detect such spoofing.","However, the current understanding of the decision-making process of CMs is limited.","We utilize Grad-CAM and introduce a quantitative analysis metric to interpret CMs' decisions.","We find that CMs prioritize the artifacts of transition regions created when concatenating bona fide and spoofed audio.","This focus differs from that of CMs trained on fully spoofed audio, which concentrate on the pattern differences between bona fide and spoofed parts.","Our further investigation explains the varying nature of CMs' focus while making correct or incorrect predictions.","These insights provide a basis for the design of CM models and the creation of datasets.","Moreover, this work lays a foundation of interpretability in the field of partial spoofed audio detection that has not been well explored previously."],"url":"http://arxiv.org/abs/2406.02483v1","category":"eess.AS"}
{"created":"2024-06-04 16:49:40","title":"Toeplitz non-liquids and Toeplitz braiding","abstract":"We study a class of $3$D non-liquid states called ``Toeplitz non-liquids''. These states consist of a stack of $2$D twisted $\\mathbb{Z}_N$ topologically ordered layers along the $z$-direction; nearby layers are coupled while keeping translational symmetry along $z$. The effective field theory is described by infinite Chern-Simons (iCS) theory, with a coefficient matrix called ``$K$-matrix'' that is of block-tridiagonal Toeplitz matrix-type. With open boundary conditions (OBC) along the $z$-direction, certain $K$-matrices exhibit an exotic phenomenon called ``Toeplitz braiding'', where the mutual braiding statistical phase between two anyons at opposite boundaries oscillates and remains non-zero in the thermodynamic limit. As a necessary condition, this requires boundary zero modes in the $K$-matrix spectrum under OBC. A key example is the $K$-matrix resembling the Hamiltonian of the $1$D Su-Schrieffer-Heeger insulator. Since the gauge invariance of Chern-Simons theory guarantees integer quantized entries for $K$-matrices, no usual global symmetries are needed to protect these zero modes or Toeplitz braiding. In order to obtain the general theory, we categorize $K$-matrices that support Toeplitz braiding into three types and analyze the conditions for each. We further numerically study the analytical results for all types of $K$-matrices. For comparison, a trivial case is numerically shown, where the mutual statistical phase angle decays exponentially to zero in the thermodynamic limit.","sentences":["We study a class of $3$D non-liquid states called ``Toeplitz non-liquids''.","These states consist of a stack of $2$D twisted $\\mathbb{Z}_N$ topologically ordered layers along the $z$-direction; nearby layers are coupled while keeping translational symmetry along $z$. The effective field theory is described by infinite Chern-Simons (iCS) theory, with a coefficient matrix called ``$K$-matrix'' that is of block-tridiagonal Toeplitz matrix-type.","With open boundary conditions (OBC) along the $z$-direction, certain $K$-matrices exhibit an exotic phenomenon called ``Toeplitz braiding'', where the mutual braiding statistical phase between two anyons at opposite boundaries oscillates and remains non-zero in the thermodynamic limit.","As a necessary condition, this requires boundary zero modes in the $K$-matrix spectrum under OBC.","A key example is the $K$-matrix resembling the Hamiltonian of the $1$D Su-Schrieffer-Heeger insulator.","Since the gauge invariance of Chern-Simons theory guarantees integer quantized entries for $K$-matrices, no usual global symmetries are needed to protect these zero modes or Toeplitz braiding.","In order to obtain the general theory, we categorize $K$-matrices that support Toeplitz braiding into three types and analyze the conditions for each.","We further numerically study the analytical results for all types of $K$-matrices.","For comparison, a trivial case is numerically shown, where the mutual statistical phase angle decays exponentially to zero in the thermodynamic limit."],"url":"http://arxiv.org/abs/2406.02482v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 16:49:06","title":"Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion","abstract":"With the help of simple fine-tuning, one can artificially embed hidden text into large language models (LLMs). This text is revealed only when triggered by a specific query to the LLM. Two primary applications are LLM fingerprinting and steganography. In the context of LLM fingerprinting, a unique text identifier (fingerprint) is embedded within the model to verify licensing compliance. In the context of steganography, the LLM serves as a carrier for hidden messages that can be disclosed through a designated trigger.   Our work demonstrates that embedding hidden text in the LLM via fine-tuning, though seemingly secure due to the vast number of potential triggers (any sequence of characters or tokens could serve as a trigger), is susceptible to extraction through analysis of the LLM's output decoding process. We propose a novel approach to extraction called Unconditional Token Forcing. It is premised on the hypothesis that iteratively feeding each token from the LLM's vocabulary into the model should reveal sequences with abnormally high token probabilities, indicating potential embedded text candidates. Additionally, our experiments show that when the first token of a hidden fingerprint is used as an input, the LLM not only produces an output sequence with high token probabilities, but also repetitively generates the fingerprint itself. We also present a method to hide text in such a way that it is resistant to Unconditional Token Forcing, which we named Unconditional Token Forcing Confusion.","sentences":["With the help of simple fine-tuning, one can artificially embed hidden text into large language models (LLMs).","This text is revealed only when triggered by a specific query to the LLM.","Two primary applications are LLM fingerprinting and steganography.","In the context of LLM fingerprinting, a unique text identifier (fingerprint) is embedded within the model to verify licensing compliance.","In the context of steganography, the LLM serves as a carrier for hidden messages that can be disclosed through a designated trigger.   ","Our work demonstrates that embedding hidden text in the LLM via fine-tuning, though seemingly secure due to the vast number of potential triggers (any sequence of characters or tokens could serve as a trigger), is susceptible to extraction through analysis of the LLM's output decoding process.","We propose a novel approach to extraction called Unconditional Token Forcing.","It is premised on the hypothesis that iteratively feeding each token from the LLM's vocabulary into the model should reveal sequences with abnormally high token probabilities, indicating potential embedded text candidates.","Additionally, our experiments show that when the first token of a hidden fingerprint is used as an input, the LLM not only produces an output sequence with high token probabilities, but also repetitively generates the fingerprint itself.","We also present a method to hide text in such a way that it is resistant to Unconditional Token Forcing, which we named Unconditional Token Forcing Confusion."],"url":"http://arxiv.org/abs/2406.02481v1","category":"cs.CL"}
{"created":"2024-06-04 16:47:47","title":"Inpainting Pathology in Lumbar Spine MRI with Latent Diffusion","abstract":"Data driven models for automated diagnosis in radiology suffer from insufficient and imbalanced datasets due to low representation of pathology in a population and the cost of expert annotations. Datasets can be bolstered through data augmentation. However, even when utilizing a full suite of transformations during model training, typical data augmentations do not address variations in human anatomy. An alternative direction is to synthesize data using generative models, which can potentially craft datasets with specific attributes. While this holds promise, commonly used generative models such as Generative Adversarial Networks may inadvertently produce anatomically inaccurate features. On the other hand, diffusion models, which offer greater stability, tend to memorize training data, raising concerns about privacy and generative diversity. Alternatively, inpainting has the potential to augment data through directly inserting pathology in medical images. However, this approach introduces a new challenge: accurately merging the generated pathological features with the surrounding anatomical context. While inpainting is a well established method for addressing simple lesions, its application to pathologies that involve complex structural changes remains relatively unexplored. We propose an efficient method for inpainting pathological features onto healthy anatomy in MRI through voxelwise noise scheduling in a latent diffusion model. We evaluate the method's ability to insert disc herniation and central canal stenosis in lumbar spine sagittal T2 MRI, and it achieves superior Frechet Inception Distance compared to state-of-the-art methods.","sentences":["Data driven models for automated diagnosis in radiology suffer from insufficient and imbalanced datasets due to low representation of pathology in a population and the cost of expert annotations.","Datasets can be bolstered through data augmentation.","However, even when utilizing a full suite of transformations during model training, typical data augmentations do not address variations in human anatomy.","An alternative direction is to synthesize data using generative models, which can potentially craft datasets with specific attributes.","While this holds promise, commonly used generative models such as Generative Adversarial Networks may inadvertently produce anatomically inaccurate features.","On the other hand, diffusion models, which offer greater stability, tend to memorize training data, raising concerns about privacy and generative diversity.","Alternatively, inpainting has the potential to augment data through directly inserting pathology in medical images.","However, this approach introduces a new challenge: accurately merging the generated pathological features with the surrounding anatomical context.","While inpainting is a well established method for addressing simple lesions, its application to pathologies that involve complex structural changes remains relatively unexplored.","We propose an efficient method for inpainting pathological features onto healthy anatomy in MRI through voxelwise noise scheduling in a latent diffusion model.","We evaluate the method's ability to insert disc herniation and central canal stenosis in lumbar spine sagittal T2 MRI, and it achieves superior Frechet Inception Distance compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.02477v1","category":"eess.IV"}
{"created":"2024-06-04 16:45:10","title":"Around the codifferential of products of p-forms and generalized interior products","abstract":"Identities pertaining to the de Rham codifferential {\\delta} in differential geometry are scattered in the literature. This article gathers such formulas involving usual differential operators (Lie derivative, Schouten-Nijenhuis bracket, ...), while adding a few new ones using a natural extension of the interior product, to provide a compact handy summary.","sentences":["Identities pertaining to the de Rham codifferential {\\delta} in differential geometry are scattered in the literature.","This article gathers such formulas involving usual differential operators (Lie derivative, Schouten-Nijenhuis bracket, ...), while adding a few new ones using a natural extension of the interior product, to provide a compact handy summary."],"url":"http://arxiv.org/abs/2406.02476v1","category":"math-ph"}
{"created":"2024-06-04 16:42:17","title":"Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding","abstract":"The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.","sentences":["The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.","We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE).","This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps.","We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text.","This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting.","In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE.","Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window."],"url":"http://arxiv.org/abs/2406.02472v1","category":"cs.CL"}
{"created":"2024-06-04 16:40:55","title":"Meta-Designing Quantum Experiments with Language Models","abstract":"Artificial Intelligence (AI) has the potential to significantly advance scientific discovery by finding solutions beyond human capabilities. However, these super-human solutions are often unintuitive and require considerable effort to uncover underlying principles, if possible at all. Here, we show how a code-generating language model trained on synthetic data can not only find solutions to specific problems but can create meta-solutions, which solve an entire class of problems in one shot and simultaneously offer insight into the underlying design principles. Specifically, for the design of new quantum physics experiments, our sequence-to-sequence transformer architecture generates interpretable Python code that describes experimental blueprints for a whole class of quantum systems. We discover general and previously unknown design rules for infinitely large classes of quantum states. The ability to automatically generate generalized patterns in readable computer code is a crucial step toward machines that help discover new scientific understanding -- one of the central aims of physics.","sentences":["Artificial Intelligence (AI) has the potential to significantly advance scientific discovery by finding solutions beyond human capabilities.","However, these super-human solutions are often unintuitive and require considerable effort to uncover underlying principles, if possible at all.","Here, we show how a code-generating language model trained on synthetic data can not only find solutions to specific problems but can create meta-solutions, which solve an entire class of problems in one shot and simultaneously offer insight into the underlying design principles.","Specifically, for the design of new quantum physics experiments, our sequence-to-sequence transformer architecture generates interpretable Python code that describes experimental blueprints for a whole class of quantum systems.","We discover general and previously unknown design rules for infinitely large classes of quantum states.","The ability to automatically generate generalized patterns in readable computer code is a crucial step toward machines that help discover new scientific understanding -- one of the central aims of physics."],"url":"http://arxiv.org/abs/2406.02470v1","category":"quant-ph"}
{"created":"2024-06-04 16:38:06","title":"DL-KDD: Dual-Light Knowledge Distillation for Action Recognition in the Dark","abstract":"Human action recognition in dark videos is a challenging task for computer vision. Recent research focuses on applying dark enhancement methods to improve the visibility of the video. However, such video processing results in the loss of critical information in the original (un-enhanced) video. Conversely, traditional two-stream methods are capable of learning information from both original and processed videos, but it can lead to a significant increase in the computational cost during the inference phase in the task of video classification. To address these challenges, we propose a novel teacher-student video classification framework, named Dual-Light KnowleDge Distillation for Action Recognition in the Dark (DL-KDD). This framework enables the model to learn from both original and enhanced video without introducing additional computational cost during inference. Specifically, DL-KDD utilizes the strategy of knowledge distillation during training. The teacher model is trained with enhanced video, and the student model is trained with both the original video and the soft target generated by the teacher model. This teacher-student framework allows the student model to predict action using only the original input video during inference. In our experiments, the proposed DL-KDD framework outperforms state-of-the-art methods on the ARID, ARID V1.5, and Dark-48 datasets. We achieve the best performance on each dataset and up to a 4.18% improvement on Dark-48, using only original video inputs, thus avoiding the use of two-stream framework or enhancement modules for inference. We further validate the effectiveness of the distillation strategy in ablative experiments. The results highlight the advantages of our knowledge distillation framework in dark human action recognition.","sentences":["Human action recognition in dark videos is a challenging task for computer vision.","Recent research focuses on applying dark enhancement methods to improve the visibility of the video.","However, such video processing results in the loss of critical information in the original (un-enhanced) video.","Conversely, traditional two-stream methods are capable of learning information from both original and processed videos, but it can lead to a significant increase in the computational cost during the inference phase in the task of video classification.","To address these challenges, we propose a novel teacher-student video classification framework, named Dual-Light KnowleDge Distillation for Action Recognition in the Dark (DL-KDD).","This framework enables the model to learn from both original and enhanced video without introducing additional computational cost during inference.","Specifically, DL-KDD utilizes the strategy of knowledge distillation during training.","The teacher model is trained with enhanced video, and the student model is trained with both the original video and the soft target generated by the teacher model.","This teacher-student framework allows the student model to predict action using only the original input video during inference.","In our experiments, the proposed DL-KDD framework outperforms state-of-the-art methods on the ARID, ARID V1.5, and Dark-48 datasets.","We achieve the best performance on each dataset and up to a 4.18% improvement on Dark-48, using only original video inputs, thus avoiding the use of two-stream framework or enhancement modules for inference.","We further validate the effectiveness of the distillation strategy in ablative experiments.","The results highlight the advantages of our knowledge distillation framework in dark human action recognition."],"url":"http://arxiv.org/abs/2406.02468v1","category":"cs.CV"}
{"created":"2024-06-04 16:34:41","title":"What no one has seen before: gravitational waveforms from warp drive collapse","abstract":"Despite originating in science fiction, warp drives have a concrete description in general relativity, with Alcubierre first proposing a spacetime metric that supported faster-than-light travel. Whilst there are numerous practical barriers to their implementation in real life, including a requirement for negative energy, computationally, one can simulate their evolution in time given an equation of state describing the matter. In this work, we study the signatures arising from a warp drive \"containment failure\", assuming a stiff equation of state for the fluid. We compute the emitted gravitational-wave signal and track the energy fluxes of the fluid. Apart from its rather speculative application to the search for extraterrestrial life in gravitational-wave detector data, this work is interesting as a study of the dynamical evolution and stability of spacetimes that violate the null energy condition. Our work highlights the importance of exploring strange new spacetimes, to (boldly) simulate what no one has seen before.","sentences":["Despite originating in science fiction, warp drives have a concrete description in general relativity, with Alcubierre first proposing a spacetime metric that supported faster-than-light travel.","Whilst there are numerous practical barriers to their implementation in real life, including a requirement for negative energy, computationally, one can simulate their evolution in time given an equation of state describing the matter.","In this work, we study the signatures arising from a warp drive \"containment failure\", assuming a stiff equation of state for the fluid.","We compute the emitted gravitational-wave signal and track the energy fluxes of the fluid.","Apart from its rather speculative application to the search for extraterrestrial life in gravitational-wave detector data, this work is interesting as a study of the dynamical evolution and stability of spacetimes that violate the null energy condition.","Our work highlights the importance of exploring strange new spacetimes, to (boldly) simulate what no one has seen before."],"url":"http://arxiv.org/abs/2406.02466v1","category":"gr-qc"}
{"created":"2024-06-04 16:34:17","title":"An Empirical Study into Clustering of Unseen Datasets with Self-Supervised Encoders","abstract":"Can pretrained models generalize to new datasets without any retraining? We deploy pretrained image models on datasets they were not trained for, and investigate whether their embeddings form meaningful clusters. Our suite of benchmarking experiments use encoders pretrained solely on ImageNet-1k with either supervised or self-supervised training techniques, deployed on image datasets that were not seen during training, and clustered with conventional clustering algorithms. This evaluation provides new insights into the embeddings of self-supervised models, which prioritize different features to supervised models. Supervised encoders typically offer more utility than SSL encoders within the training domain, and vice-versa far outside of it, however, fine-tuned encoders demonstrate the opposite trend. Clustering provides a way to evaluate the utility of self-supervised learned representations orthogonal to existing methods such as kNN. Additionally, we find the silhouette score when measured in a UMAP-reduced space is highly correlated with clustering performance, and can therefore be used as a proxy for clustering performance on data with no ground truth labels. Our code implementation is available at \\url{https://github.com/scottclowe/zs-ssl-clustering/}.","sentences":["Can pretrained models generalize to new datasets without any retraining?","We deploy pretrained image models on datasets they were not trained for, and investigate whether their embeddings form meaningful clusters.","Our suite of benchmarking experiments use encoders pretrained solely on ImageNet-1k with either supervised or self-supervised training techniques, deployed on image datasets that were not seen during training, and clustered with conventional clustering algorithms.","This evaluation provides new insights into the embeddings of self-supervised models, which prioritize different features to supervised models.","Supervised encoders typically offer more utility than SSL encoders within the training domain, and vice-versa far outside of it, however, fine-tuned encoders demonstrate the opposite trend.","Clustering provides a way to evaluate the utility of self-supervised learned representations orthogonal to existing methods such as kNN.","Additionally, we find the silhouette score when measured in a UMAP-reduced space is highly correlated with clustering performance, and can therefore be used as a proxy for clustering performance on data with no ground truth labels.","Our code implementation is available at \\url{https://github.com/scottclowe/zs-ssl-clustering/}."],"url":"http://arxiv.org/abs/2406.02465v1","category":"cs.LG"}
{"created":"2024-06-04 16:31:43","title":"Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments","abstract":"Estimating the conditional average treatment effect (CATE) from observational data is relevant for many applications such as personalized medicine. Here, we focus on the widespread setting where the observational data come from multiple environments, such as different hospitals, physicians, or countries. Furthermore, we allow for violations of standard causal assumptions, namely, overlap within the environments and unconfoundedness. To this end, we move away from point identification and focus on partial identification. Specifically, we show that current assumptions from the literature on multiple environments allow us to interpret the environment as an instrumental variable (IV). This allows us to adapt bounds from the IV literature for partial identification of CATE by leveraging treatment assignment mechanisms across environments. Then, we propose different model-agnostic learners (so-called meta-learners) to estimate the bounds that can be used in combination with arbitrary machine learning models. We further demonstrate the effectiveness of our meta-learners across various experiments using both simulated and real-world data. Finally, we discuss the applicability of our meta-learners to partial identification in instrumental variable settings, such as randomized controlled trials with non-compliance.","sentences":["Estimating the conditional average treatment effect (CATE) from observational data is relevant for many applications such as personalized medicine.","Here, we focus on the widespread setting where the observational data come from multiple environments, such as different hospitals, physicians, or countries.","Furthermore, we allow for violations of standard causal assumptions, namely, overlap within the environments and unconfoundedness.","To this end, we move away from point identification and focus on partial identification.","Specifically, we show that current assumptions from the literature on multiple environments allow us to interpret the environment as an instrumental variable (IV).","This allows us to adapt bounds from the IV literature for partial identification of CATE by leveraging treatment assignment mechanisms across environments.","Then, we propose different model-agnostic learners (so-called meta-learners) to estimate the bounds that can be used in combination with arbitrary machine learning models.","We further demonstrate the effectiveness of our meta-learners across various experiments using both simulated and real-world data.","Finally, we discuss the applicability of our meta-learners to partial identification in instrumental variable settings, such as randomized controlled trials with non-compliance."],"url":"http://arxiv.org/abs/2406.02464v1","category":"cs.LG"}
{"created":"2024-06-04 16:30:37","title":"Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems","abstract":"Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems, but the training process is computationally expensive and requires lots of data. Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images. This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images. Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems. First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiency while still maintaining the capability to generate entire images via positional encoding. Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS). We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors. Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.","sentences":["Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems, but the training process is computationally expensive and requires lots of data.","Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images.","This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images.","Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems.","First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiency while still maintaining the capability to generate entire images via positional encoding.","Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS).","We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors.","Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior."],"url":"http://arxiv.org/abs/2406.02462v1","category":"cs.CV"}
{"created":"2024-06-04 16:27:09","title":"RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting","abstract":"The advancement of diffusion models has pushed the boundary of text-to-3D object generation. While it is straightforward to composite objects into a scene with reasonable geometry, it is nontrivial to texture such a scene perfectly due to style inconsistency and occlusions between objects. To tackle these problems, we propose a coarse-to-fine 3D scene texturing framework, referred to as RoomTex, to generate high-fidelity and style-consistent textures for untextured compositional scene meshes. In the coarse stage, RoomTex first unwraps the scene mesh to a panoramic depth map and leverages ControlNet to generate a room panorama, which is regarded as the coarse reference to ensure the global texture consistency. In the fine stage, based on the panoramic image and perspective depth maps, RoomTex will refine and texture every single object in the room iteratively along a series of selected camera views, until this object is completely painted. Moreover, we propose to maintain superior alignment between RGB and depth spaces via subtle edge detection methods. Extensive experiments show our method is capable of generating high-quality and diverse room textures, and more importantly, supporting interactive fine-grained texture control and flexible scene editing thanks to our inpainting-based framework and compositional mesh input. Our project page is available at https://qwang666.github.io/RoomTex/.","sentences":["The advancement of diffusion models has pushed the boundary of text-to-3D object generation.","While it is straightforward to composite objects into a scene with reasonable geometry, it is nontrivial to texture such a scene perfectly due to style inconsistency and occlusions between objects.","To tackle these problems, we propose a coarse-to-fine 3D scene texturing framework, referred to as RoomTex, to generate high-fidelity and style-consistent textures for untextured compositional scene meshes.","In the coarse stage, RoomTex first unwraps the scene mesh to a panoramic depth map and leverages ControlNet to generate a room panorama, which is regarded as the coarse reference to ensure the global texture consistency.","In the fine stage, based on the panoramic image and perspective depth maps, RoomTex will refine and texture every single object in the room iteratively along a series of selected camera views, until this object is completely painted.","Moreover, we propose to maintain superior alignment between RGB and depth spaces via subtle edge detection methods.","Extensive experiments show our method is capable of generating high-quality and diverse room textures, and more importantly, supporting interactive fine-grained texture control and flexible scene editing thanks to our inpainting-based framework and compositional mesh input.","Our project page is available at https://qwang666.github.io/RoomTex/."],"url":"http://arxiv.org/abs/2406.02461v1","category":"cs.CV"}
{"created":"2024-06-04 16:24:15","title":"Do Cure Violence Programs Reduce Gun Violence? Evidence from New York City","abstract":"Cure Violence is a community violence intervention program that aims to reduce gun violence by mediating conflicts, \"treating\" high-risk individuals, and changing community norms. Using NYC shootings data from 2006-2023, we assess the efficacy of Cure Violence using both difference-in-differences and event study models. We find that, on average, Cure Violence is associated with a 14% reduction in shootings relative to the counterfactual. This association persists in the years after treatment, neither increasing nor decreasing much over time. We exploit variation in the geography and timing of Cure implementation to argue against alternative explanations. Additionally, we find suggestive evidence of spillover effects into nearby precincts, as well as increasing returns to opening new Cure programs. Interpreted causally, our results imply that around 1,300 shootings were avoided between 2012-2023 due to Cure -- generating a net social surplus of $2.45 billion and achieving a benefit-cost ratio of 6.5:1.","sentences":["Cure Violence is a community violence intervention program that aims to reduce gun violence by mediating conflicts, \"treating\" high-risk individuals, and changing community norms.","Using NYC shootings data from 2006-2023, we assess the efficacy of Cure Violence using both difference-in-differences and event study models.","We find that, on average, Cure Violence is associated with a 14% reduction in shootings relative to the counterfactual.","This association persists in the years after treatment, neither increasing nor decreasing much over time.","We exploit variation in the geography and timing of Cure implementation to argue against alternative explanations.","Additionally, we find suggestive evidence of spillover effects into nearby precincts, as well as increasing returns to opening new Cure programs.","Interpreted causally, our results imply that around 1,300 shootings were avoided between 2012-2023 due to Cure -- generating a net social surplus of $2.45 billion and achieving a benefit-cost ratio of 6.5:1."],"url":"http://arxiv.org/abs/2406.02459v1","category":"econ.GN"}
{"created":"2024-06-04 16:23:18","title":"Deep Block Proximal Linearised Minimisation Algorithm for Non-convex Inverse Problems","abstract":"Image restoration is typically addressed through non-convex inverse problems, which are often solved using first-order block-wise splitting methods. In this paper, we consider a general type of non-convex optimisation model that captures many inverse image problems and present an inertial block proximal linearised minimisation (iBPLM) algorithm. Our new method unifies the Jacobi-type parallel and the Gauss-Seidel-type alternating update rules, and extends beyond these approaches. The inertial technique is also incorporated into each block-wise subproblem update, which can accelerate numerical convergence. Furthermore, we extend this framework with a plug-and-play variant (PnP-iBPLM) that integrates deep gradient denoisers, offering a flexible and robust solution for complex imaging tasks. We provide comprehensive theoretical analysis, demonstrating both subsequential and global convergence of the proposed algorithms. To validate our methods, we apply them to multi-block dictionary learning problems in image denoising and deblurring. Experimental results show that both iBPLM and PnP-iBPLM significantly enhance numerical performance and robustness in these applications.","sentences":["Image restoration is typically addressed through non-convex inverse problems, which are often solved using first-order block-wise splitting methods.","In this paper, we consider a general type of non-convex optimisation model that captures many inverse image problems and present an inertial block proximal linearised minimisation (iBPLM) algorithm.","Our new method unifies the Jacobi-type parallel and the Gauss-Seidel-type alternating update rules, and extends beyond these approaches.","The inertial technique is also incorporated into each block-wise subproblem update, which can accelerate numerical convergence.","Furthermore, we extend this framework with a plug-and-play variant (PnP-iBPLM) that integrates deep gradient denoisers, offering a flexible and robust solution for complex imaging tasks.","We provide comprehensive theoretical analysis, demonstrating both subsequential and global convergence of the proposed algorithms.","To validate our methods, we apply them to multi-block dictionary learning problems in image denoising and deblurring.","Experimental results show that both iBPLM and PnP-iBPLM significantly enhance numerical performance and robustness in these applications."],"url":"http://arxiv.org/abs/2406.02458v1","category":"math.NA"}
{"created":"2024-06-04 16:21:14","title":"Offline Bayesian Aleatoric and Epistemic Uncertainty Quantification and Posterior Value Optimisation in Finite-State MDPs","abstract":"We address the challenge of quantifying Bayesian uncertainty and incorporating it in offline use cases of finite-state Markov Decision Processes (MDPs) with unknown dynamics. Our approach provides a principled method to disentangle epistemic and aleatoric uncertainty, and a novel technique to find policies that optimise Bayesian posterior expected value without relying on strong assumptions about the MDP's posterior distribution. First, we utilise standard Bayesian reinforcement learning methods to capture the posterior uncertainty in MDP parameters based on available data. We then analytically compute the first two moments of the return distribution across posterior samples and apply the law of total variance to disentangle aleatoric and epistemic uncertainties. To find policies that maximise posterior expected value, we leverage the closed-form expression for value as a function of policy. This allows us to propose a stochastic gradient-based approach for solving the problem. We illustrate the uncertainty quantification and Bayesian posterior value optimisation performance of our agent in simple, interpretable gridworlds and validate it through ground-truth evaluations on synthetic MDPs. Finally, we highlight the real-world impact and computational scalability of our method by applying it to the AI Clinician problem, which recommends treatment for patients in intensive care units and has emerged as a key use case of finite-state MDPs with offline data. We discuss the challenges that arise with Bayesian modelling of larger scale MDPs while demonstrating the potential to apply our methods rooted in Bayesian decision theory into the real world. We make our code available at https://github.com/filippovaldettaro/finite-state-mdps .","sentences":["We address the challenge of quantifying Bayesian uncertainty and incorporating it in offline use cases of finite-state Markov Decision Processes (MDPs) with unknown dynamics.","Our approach provides a principled method to disentangle epistemic and aleatoric uncertainty, and a novel technique to find policies that optimise Bayesian posterior expected value without relying on strong assumptions about the MDP's posterior distribution.","First, we utilise standard Bayesian reinforcement learning methods to capture the posterior uncertainty in MDP parameters based on available data.","We then analytically compute the first two moments of the return distribution across posterior samples and apply the law of total variance to disentangle aleatoric and epistemic uncertainties.","To find policies that maximise posterior expected value, we leverage the closed-form expression for value as a function of policy.","This allows us to propose a stochastic gradient-based approach for solving the problem.","We illustrate the uncertainty quantification and Bayesian posterior value optimisation performance of our agent in simple, interpretable gridworlds and validate it through ground-truth evaluations on synthetic MDPs.","Finally, we highlight the real-world impact and computational scalability of our method by applying it to the AI Clinician problem, which recommends treatment for patients in intensive care units and has emerged as a key use case of finite-state MDPs with offline data.","We discuss the challenges that arise with Bayesian modelling of larger scale MDPs while demonstrating the potential to apply our methods rooted in Bayesian decision theory into the real world.","We make our code available at https://github.com/filippovaldettaro/finite-state-mdps ."],"url":"http://arxiv.org/abs/2406.02456v1","category":"cs.LG"}
{"created":"2024-06-04 16:20:14","title":"Parameterized Non-circular Deviation from the Kerr Paradigm and Its Observational Signatures: Extreme Mass Ratio Inspirals and Lense-Thirring Effect","abstract":"Recent gravitational wave observations and shadow imaging have demonstrated the astonishing consistency of the Kerr paradigm despite all the special symmetries assumed in deriving the Kerr metric. Hence, it is crucial to test the presence of these symmetries in astrophysical scenarios and constraint possible deviations from them, especially in strong field regimes. With this motivation, the present work aims to investigate the theoretical consequences and observational signatures of non-circularity in a unified theory-agnostic manner. For this purpose, we construct a general non-circular metric with a small parameterized deviation from Kerr. This metric preserves all the other properties of Kerr, such as stationarity, axisymmetry, geodesic separability and asymptotic flatness. Apart from the resulting mathematical simplifications, this assumption is crucial to disentangle the consequences of relaxing circularity from other symmetries. Then, after discussing various novel mathematical properties and interesting theoretical consequences, we perform a detailed analysis of extreme mass ratio inspirals and Lense-Thirring precession in the context of this newly constructed metric. Our study clearly shows the promising prospects of detecting and constraining even a slight non-circular deviation from the Kerr paradigm using the future gravitational wave observations by the Laser Interferometer Space Antenna.","sentences":["Recent gravitational wave observations and shadow imaging have demonstrated the astonishing consistency of the Kerr paradigm despite all the special symmetries assumed in deriving the Kerr metric.","Hence, it is crucial to test the presence of these symmetries in astrophysical scenarios and constraint possible deviations from them, especially in strong field regimes.","With this motivation, the present work aims to investigate the theoretical consequences and observational signatures of non-circularity in a unified theory-agnostic manner.","For this purpose, we construct a general non-circular metric with a small parameterized deviation from Kerr.","This metric preserves all the other properties of Kerr, such as stationarity, axisymmetry, geodesic separability and asymptotic flatness.","Apart from the resulting mathematical simplifications, this assumption is crucial to disentangle the consequences of relaxing circularity from other symmetries.","Then, after discussing various novel mathematical properties and interesting theoretical consequences, we perform a detailed analysis of extreme mass ratio inspirals and Lense-Thirring precession in the context of this newly constructed metric.","Our study clearly shows the promising prospects of detecting and constraining even a slight non-circular deviation from the Kerr paradigm using the future gravitational wave observations by the Laser Interferometer Space Antenna."],"url":"http://arxiv.org/abs/2406.02454v1","category":"gr-qc"}
{"created":"2024-06-04 16:14:55","title":"A Generalized Apprenticeship Learning Framework for Modeling Heterogeneous Student Pedagogical Strategies","abstract":"A key challenge in e-learning environments like Intelligent Tutoring Systems (ITSs) is to induce effective pedagogical policies efficiently. While Deep Reinforcement Learning (DRL) often suffers from sample inefficiency and reward function design difficulty, Apprenticeship Learning(AL) algorithms can overcome them. However, most AL algorithms can not handle heterogeneity as they assume all demonstrations are generated with a homogeneous policy driven by a single reward function. Still, some AL algorithms which consider heterogeneity, often can not generalize to large continuous state space and only work with discrete states. In this paper, we propose an expectation-maximization(EM)-EDM, a general AL framework to induce effective pedagogical policies from given optimal or near-optimal demonstrations, which are assumed to be driven by heterogeneous reward functions. We compare the effectiveness of the policies induced by our proposed EM-EDM against four AL-based baselines and two policies induced by DRL on two different but related tasks that involve pedagogical action prediction. Our overall results showed that, for both tasks, EM-EDM outperforms the four AL baselines across all performance metrics and the two DRL baselines. This suggests that EM-EDM can effectively model complex student pedagogical decision-making processes through the ability to manage a large, continuous state space and adapt to handle diverse and heterogeneous reward functions with very few given demonstrations.","sentences":["A key challenge in e-learning environments like Intelligent Tutoring Systems (ITSs) is to induce effective pedagogical policies efficiently.","While Deep Reinforcement Learning (DRL) often suffers from sample inefficiency and reward function design difficulty, Apprenticeship Learning(AL) algorithms can overcome them.","However, most AL algorithms can not handle heterogeneity as they assume all demonstrations are generated with a homogeneous policy driven by a single reward function.","Still, some AL algorithms which consider heterogeneity, often can not generalize to large continuous state space and only work with discrete states.","In this paper, we propose an expectation-maximization(EM)-EDM, a general AL framework to induce effective pedagogical policies from given optimal or near-optimal demonstrations, which are assumed to be driven by heterogeneous reward functions.","We compare the effectiveness of the policies induced by our proposed EM-EDM against four AL-based baselines and two policies induced by DRL on two different but related tasks that involve pedagogical action prediction.","Our overall results showed that, for both tasks, EM-EDM outperforms the four AL baselines across all performance metrics and the two DRL baselines.","This suggests that EM-EDM can effectively model complex student pedagogical decision-making processes through the ability to manage a large, continuous state space and adapt to handle diverse and heterogeneous reward functions with very few given demonstrations."],"url":"http://arxiv.org/abs/2406.02450v1","category":"cs.LG"}
{"created":"2024-06-04 16:14:00","title":"Representations as Language: An Information-Theoretic Framework for Interpretability","abstract":"Large scale neural models show impressive performance across a wide array of linguistic tasks. Despite this they remain, largely, black-boxes - inducing vector-representations of their input that prove difficult to interpret. This limits our ability to understand what they learn, and when the learn it, or describe what kinds of representations generalise well out of distribution. To address this we introduce a novel approach to interpretability that looks at the mapping a model learns from sentences to representations as a kind of language in its own right. In doing so we introduce a set of information-theoretic measures that quantify how structured a model's representations are with respect to its input, and when during training that structure arises. Our measures are fast to compute, grounded in linguistic theory, and can predict which models will generalise best based on their representations. We use these measures to describe two distinct phases of training a transformer: an initial phase of in-distribution learning which reduces task loss, then a second stage where representations becoming robust to noise. Generalisation performance begins to increase during this second phase, drawing a link between generalisation and robustness to noise. Finally we look at how model size affects the structure of the representational space, showing that larger models ultimately compress their representations more than their smaller counterparts.","sentences":["Large scale neural models show impressive performance across a wide array of linguistic tasks.","Despite this they remain, largely, black-boxes - inducing vector-representations of their input that prove difficult to interpret.","This limits our ability to understand what they learn, and when the learn it, or describe what kinds of representations generalise well out of distribution.","To address this we introduce a novel approach to interpretability that looks at the mapping a model learns from sentences to representations as a kind of language in its own right.","In doing so we introduce a set of information-theoretic measures that quantify how structured a model's representations are with respect to its input, and when during training that structure arises.","Our measures are fast to compute, grounded in linguistic theory, and can predict which models will generalise best based on their representations.","We use these measures to describe two distinct phases of training a transformer: an initial phase of in-distribution learning which reduces task loss, then a second stage where representations becoming robust to noise.","Generalisation performance begins to increase during this second phase, drawing a link between generalisation and robustness to noise.","Finally we look at how model size affects the structure of the representational space, showing that larger models ultimately compress their representations more than their smaller counterparts."],"url":"http://arxiv.org/abs/2406.02449v1","category":"cs.CL"}
{"created":"2024-06-04 16:13:50","title":"Tunable $t-t'-U$ Hubbard models in twisted square homobilayers","abstract":"Square lattice Hubbard models with tunable hopping ratio $t'/t$ are highly promising for realizing a variety of quantum phases and for shedding light on key puzzles in correlated quantum materials, including higher-temperature superconductivity. We show that twisted square lattice homo-bilayers generically offer such tunability when the flat bands originate from the corner of the Brillouin zone. We reveal an emergent symmetry at low twist-angles, absent in single layers, that necessitates the vanishing of nearest neighbor hopping ($t=0$). This symmetry can be lifted by an inter-layer displacement field or by an in-plane magnetic field, introducing tunable $t$ and anisotropy, allowing access to a wide range of $t'/t$ ratios for correlated electrons on a moir\\'e square lattice.","sentences":["Square lattice Hubbard models with tunable hopping ratio $t'/t$ are highly promising for realizing a variety of quantum phases and for shedding light on key puzzles in correlated quantum materials, including higher-temperature superconductivity.","We show that twisted square lattice homo-bilayers generically offer such tunability when the flat bands originate from the corner of the Brillouin zone.","We reveal an emergent symmetry at low twist-angles, absent in single layers, that necessitates the vanishing of nearest neighbor hopping ($t=0$).","This symmetry can be lifted by an inter-layer displacement field or by an in-plane magnetic field, introducing tunable $t$ and anisotropy, allowing access to a wide range of $t'/t$ ratios for correlated electrons on a moir\\'e square lattice."],"url":"http://arxiv.org/abs/2406.02448v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 16:12:27","title":"Reducing Bias in Federated Class-Incremental Learning with Hierarchical Generative Prototypes","abstract":"Federated Learning (FL) aims at unburdening the training of deep models by distributing computation across multiple devices (clients) while safeguarding data privacy. On top of that, Federated Continual Learning (FCL) also accounts for data distribution evolving over time, mirroring the dynamic nature of real-world environments. In this work, we shed light on the Incremental and Federated biases that naturally emerge in FCL. While the former is a known problem in Continual Learning, stemming from the prioritization of recently introduced classes, the latter (i.e., the bias towards local distributions) remains relatively unexplored. Our proposal constrains both biases in the last layer by efficiently fine-tuning a pre-trained backbone using learnable prompts, resulting in clients that produce less biased representations and more biased classifiers. Therefore, instead of solely relying on parameter aggregation, we also leverage generative prototypes to effectively balance the predictions of the global model. Our method improves on the current State Of The Art, providing an average increase of +7.9% in accuracy.","sentences":["Federated Learning (FL) aims at unburdening the training of deep models by distributing computation across multiple devices (clients) while safeguarding data privacy.","On top of that, Federated Continual Learning (FCL) also accounts for data distribution evolving over time, mirroring the dynamic nature of real-world environments.","In this work, we shed light on the Incremental and Federated biases that naturally emerge in FCL.","While the former is a known problem in Continual Learning, stemming from the prioritization of recently introduced classes, the latter (i.e., the bias towards local distributions) remains relatively unexplored.","Our proposal constrains both biases in the last layer by efficiently fine-tuning a pre-trained backbone using learnable prompts, resulting in clients that produce less biased representations and more biased classifiers.","Therefore, instead of solely relying on parameter aggregation, we also leverage generative prototypes to effectively balance the predictions of the global model.","Our method improves on the current State Of The Art, providing an average increase of +7.9% in accuracy."],"url":"http://arxiv.org/abs/2406.02447v1","category":"cs.LG"}
{"created":"2024-06-04 16:06:51","title":"Explainable Deep Learning Analysis for Raga Identification in Indian Art Music","abstract":"The task of Raga Identification is a very popular research problem in Music Information Retrieval. Few studies that have explored this task employed various approaches, such as signal processing, Machine Learning (ML) methods, and more recently Deep Learning (DL) based methods. However, a key question remains unanswered in all of these works: do these ML/DL methods learn and interpret Ragas in a manner similar to human experts? Besides, a significant roadblock in this research is the unavailability of ample supply of rich, labeled datasets, which drives these ML/DL based methods. In this paper, we introduce \"Prasarbharti Indian Music\" version-1 (PIM-v1), a novel dataset comprising of 191 hours of meticulously labeled Hindustani Classical Music (HCM) recordings, which is the largest labeled dataset for HCM recordings to the best of our knowledge. Our approach involves conducting ablation studies to find the benchmark classification model for Automatic Raga Identification (ARI) using PIM-v1 dataset. We achieve a chunk-wise f1-score of 0.89 for a subset of 12 Raga classes. Subsequently, we employ model explainability techniques to evaluate the classifier's predictions, aiming to ascertain whether they align with human understanding of Ragas or are driven by arbitrary patterns. We validate the correctness of model's predictions by comparing the explanations given by two ExAI models with human expert annotations. Following this, we analyze explanations for individual test examples to understand the role of regions highlighted by explanations in correct or incorrect predictions made by the model.","sentences":["The task of Raga Identification is a very popular research problem in Music Information Retrieval.","Few studies that have explored this task employed various approaches, such as signal processing, Machine Learning (ML) methods, and more recently Deep Learning (DL) based methods.","However, a key question remains unanswered in all of these works: do these ML/DL methods learn and interpret Ragas in a manner similar to human experts?","Besides, a significant roadblock in this research is the unavailability of ample supply of rich, labeled datasets, which drives these ML/DL based methods.","In this paper, we introduce \"Prasarbharti Indian Music\" version-1 (PIM-v1), a novel dataset comprising of 191 hours of meticulously labeled Hindustani Classical Music (HCM) recordings, which is the largest labeled dataset for HCM recordings to the best of our knowledge.","Our approach involves conducting ablation studies to find the benchmark classification model for Automatic Raga Identification (ARI) using PIM-v1 dataset.","We achieve a chunk-wise f1-score of 0.89 for a subset of 12 Raga classes.","Subsequently, we employ model explainability techniques to evaluate the classifier's predictions, aiming to ascertain whether they align with human understanding of Ragas or are driven by arbitrary patterns.","We validate the correctness of model's predictions by comparing the explanations given by two ExAI models with human expert annotations.","Following this, we analyze explanations for individual test examples to understand the role of regions highlighted by explanations in correct or incorrect predictions made by the model."],"url":"http://arxiv.org/abs/2406.02443v1","category":"eess.AS"}
{"created":"2024-06-04 16:00:18","title":"CtrSVDD: A Benchmark Dataset and Baseline Analysis for Controlled Singing Voice Deepfake Detection","abstract":"Recent singing voice synthesis and conversion advancements necessitate robust singing voice deepfake detection (SVDD) models. Current SVDD datasets face challenges due to limited controllability, diversity in deepfake methods, and licensing restrictions. Addressing these gaps, we introduce CtrSVDD, a large-scale, diverse collection of bonafide and deepfake singing vocals. These vocals are synthesized using state-of-the-art methods from publicly accessible singing voice datasets. CtrSVDD includes 47.64 hours of bonafide and 260.34 hours of deepfake singing vocals, spanning 14 deepfake methods and involving 164 singer identities. We also present a baseline system with flexible front-end features, evaluated against a structured train/dev/eval split. The experiments show the importance of feature selection and highlight a need for generalization towards deepfake methods that deviate further from training distribution. The CtrSVDD dataset and baselines are publicly accessible.","sentences":["Recent singing voice synthesis and conversion advancements necessitate robust singing voice deepfake detection (SVDD) models.","Current SVDD datasets face challenges due to limited controllability, diversity in deepfake methods, and licensing restrictions.","Addressing these gaps, we introduce CtrSVDD, a large-scale, diverse collection of bonafide and deepfake singing vocals.","These vocals are synthesized using state-of-the-art methods from publicly accessible singing voice datasets.","CtrSVDD includes 47.64 hours of bonafide and 260.34 hours of deepfake singing vocals, spanning 14 deepfake methods and involving 164 singer identities.","We also present a baseline system with flexible front-end features, evaluated against a structured train/dev/eval split.","The experiments show the importance of feature selection and highlight a need for generalization towards deepfake methods that deviate further from training distribution.","The CtrSVDD dataset and baselines are publicly accessible."],"url":"http://arxiv.org/abs/2406.02438v1","category":"eess.AS"}
{"created":"2024-06-04 15:59:36","title":"Algorithmic Collusion in Dynamic Pricing with Deep Reinforcement Learning","abstract":"Nowadays, a significant share of the Business-to-Consumer sector is based on online platforms like Amazon and Alibaba and uses Artificial Intelligence for pricing strategies. This has sparked debate on whether pricing algorithms may tacitly collude to set supra-competitive prices without being explicitly designed to do so. Our study addresses these concerns by examining the risk of collusion when Reinforcement Learning algorithms are used to decide on pricing strategies in competitive markets. Prior research in this field focused on Tabular Q-learning (TQL) and led to opposing views on whether learning-based algorithms can lead to supra-competitive prices. Our work contributes to this ongoing discussion by providing a more nuanced numerical study that goes beyond TQL by additionally capturing off- and on-policy Deep Reinforcement Learning (DRL) algorithms. We study multiple Bertrand oligopoly variants and show that algorithmic collusion depends on the algorithm used. In our experiments, TQL exhibits higher collusion and price dispersion phenomena compared to DRL algorithms. We show that the severity of collusion depends not only on the algorithm used but also on the characteristics of the market environment. We further find that Proximal Policy Optimization appears to be less sensitive to collusive outcomes compared to other state-of-the-art DRL algorithms.","sentences":["Nowadays, a significant share of the Business-to-Consumer sector is based on online platforms like Amazon and Alibaba and uses Artificial Intelligence for pricing strategies.","This has sparked debate on whether pricing algorithms may tacitly collude to set supra-competitive prices without being explicitly designed to do so.","Our study addresses these concerns by examining the risk of collusion when Reinforcement Learning algorithms are used to decide on pricing strategies in competitive markets.","Prior research in this field focused on Tabular Q-learning (TQL) and led to opposing views on whether learning-based algorithms can lead to supra-competitive prices.","Our work contributes to this ongoing discussion by providing a more nuanced numerical study that goes beyond TQL by additionally capturing off- and on-policy Deep Reinforcement Learning (DRL) algorithms.","We study multiple Bertrand oligopoly variants and show that algorithmic collusion depends on the algorithm used.","In our experiments, TQL exhibits higher collusion and price dispersion phenomena compared to DRL algorithms.","We show that the severity of collusion depends not only on the algorithm used but also on the characteristics of the market environment.","We further find that Proximal Policy Optimization appears to be less sensitive to collusive outcomes compared to other state-of-the-art DRL algorithms."],"url":"http://arxiv.org/abs/2406.02437v1","category":"econ.GN"}
{"created":"2024-06-04 15:57:43","title":"Generative Active Learning for Long-tailed Instance Segmentation","abstract":"Recently, large-scale language-image generative models have gained widespread attention and many works have utilized generated data from these models to further enhance the performance of perception tasks. However, not all generated data can positively impact downstream models, and these methods do not thoroughly explore how to better select and utilize generated data. On the other hand, there is still a lack of research oriented towards active learning on generated data. In this paper, we explore how to perform active learning specifically for generated data in the long-tailed instance segmentation task. Subsequently, we propose BSGAL, a new algorithm that online estimates the contribution of the generated data based on gradient cache. BSGAL can handle unlimited generated data and complex downstream segmentation tasks effectively. Experiments show that BSGAL outperforms the baseline approach and effectually improves the performance of long-tailed segmentation. Our code can be found at https://github.com/aim-uofa/DiverGen.","sentences":["Recently, large-scale language-image generative models have gained widespread attention and many works have utilized generated data from these models to further enhance the performance of perception tasks.","However, not all generated data can positively impact downstream models, and these methods do not thoroughly explore how to better select and utilize generated data.","On the other hand, there is still a lack of research oriented towards active learning on generated data.","In this paper, we explore how to perform active learning specifically for generated data in the long-tailed instance segmentation task.","Subsequently, we propose BSGAL, a new algorithm that online estimates the contribution of the generated data based on gradient cache.","BSGAL can handle unlimited generated data and complex downstream segmentation tasks effectively.","Experiments show that BSGAL outperforms the baseline approach and effectually improves the performance of long-tailed segmentation.","Our code can be found at https://github.com/aim-uofa/DiverGen."],"url":"http://arxiv.org/abs/2406.02435v1","category":"cs.CV"}
{"created":"2024-06-04 15:57:23","title":"InGaP $\u03c7^{(2)}$ integrated photonics platform for broadband, ultra-efficient nonlinear conversion and entangled photon generation","abstract":"Nonlinear optics plays an important role in many areas of science and technology. The advance of nonlinear optics is empowered by the discovery and utilization of materials with growing optical nonlinearity. Here we demonstrate an indium gallium phosphide (InGaP) integrated photonics platform for broadband, ultra-efficient second-order nonlinear optics. The InGaP nanophotonic waveguide enables second-harmonic generation with a normalized efficiency of $128,000\\%$/W/cm$^2$ at 1.55 $\\mu$m pump wavelength, nearly two orders of magnitude higher than the state of the art in the telecommunication C band. Further, we realize an ultra-bright, broadband time-energy entangled photon source with a pair generation rate of 97 GHz/mW and a bandwidth of 115 nm centered at the telecommunication C band. The InGaP entangled photon source shows high coincidence-to-accidental counts ratio CAR $>10^4$ and two-photon interference visibility $>98\\%$. The InGaP second-order nonlinear photonics platform will have wide-ranging implications for non-classical light generation, optical signal processing, and quantum networking.","sentences":["Nonlinear optics plays an important role in many areas of science and technology.","The advance of nonlinear optics is empowered by the discovery and utilization of materials with growing optical nonlinearity.","Here we demonstrate an indium gallium phosphide (InGaP) integrated photonics platform for broadband, ultra-efficient second-order nonlinear optics.","The InGaP nanophotonic waveguide enables second-harmonic generation with a normalized efficiency of $128,000\\%$/W/cm$^2$ at 1.55 $\\mu$m pump wavelength, nearly two orders of magnitude higher than the state of the art in the telecommunication C band.","Further, we realize an ultra-bright, broadband time-energy entangled photon source with a pair generation rate of 97 GHz/mW and a bandwidth of 115 nm centered at the telecommunication C band.","The InGaP entangled photon source shows high coincidence-to-accidental counts ratio CAR $>10^4$ and two-photon interference visibility $>98\\%$. The InGaP second-order nonlinear photonics platform will have wide-ranging implications for non-classical light generation, optical signal processing, and quantum networking."],"url":"http://arxiv.org/abs/2406.02434v1","category":"physics.optics"}
{"created":"2024-06-04 15:48:29","title":"Seed-TTS: A Family of High-Quality Versatile Speech Generation Models","abstract":"We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.","sentences":["We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech.","Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations.","With fine-tuning, we achieve even higher subjective scores across these metrics.","Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild.","Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability.","We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture.","Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing.","We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing.","We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}."],"url":"http://arxiv.org/abs/2406.02430v1","category":"eess.AS"}
{"created":"2024-06-04 15:47:59","title":"Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion","abstract":"Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model. We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.","sentences":["Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data.","Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research.","This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model.","We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion.","We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity.","SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models.","Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors.","Audio samples are available at https://speech2sing.github.io."],"url":"http://arxiv.org/abs/2406.02429v1","category":"eess.AS"}
{"created":"2024-06-04 15:46:41","title":"Contextual Optimization under Covariate Shift: A Robust Approach by Intersecting Wasserstein Balls","abstract":"In contextual optimization, a decision-maker observes historical samples of uncertain variables and associated concurrent covariates, without knowing their joint distribution. Given an additional covariate observation, the goal is to choose a decision that minimizes some operational costs. A prevalent issue here is covariate shift, where the marginal distribution of the new covariate differs from historical samples, leading to decision performance variations with nonparametric or parametric estimators. To address this, we propose a distributionally robust approach that uses an ambiguity set by the intersection of two Wasserstein balls, each centered on typical nonparametric or parametric distribution estimators. Computationally, we establish the tractable reformulation of this distributionally robust optimization problem. Statistically, we provide guarantees for our Wasserstein ball intersection approach under covariate shift by analyzing the measure concentration of the estimators. Furthermore, to reduce computational complexity, we employ a surrogate objective that maintains similar generalization guarantees. Through synthetic and empirical case studies on income prediction and portfolio optimization, we demonstrate the strong empirical performance of our proposed models.","sentences":["In contextual optimization, a decision-maker observes historical samples of uncertain variables and associated concurrent covariates, without knowing their joint distribution.","Given an additional covariate observation, the goal is to choose a decision that minimizes some operational costs.","A prevalent issue here is covariate shift, where the marginal distribution of the new covariate differs from historical samples, leading to decision performance variations with nonparametric or parametric estimators.","To address this, we propose a distributionally robust approach that uses an ambiguity set by the intersection of two Wasserstein balls, each centered on typical nonparametric or parametric distribution estimators.","Computationally, we establish the tractable reformulation of this distributionally robust optimization problem.","Statistically, we provide guarantees for our Wasserstein ball intersection approach under covariate shift by analyzing the measure concentration of the estimators.","Furthermore, to reduce computational complexity, we employ a surrogate objective that maintains similar generalization guarantees.","Through synthetic and empirical case studies on income prediction and portfolio optimization, we demonstrate the strong empirical performance of our proposed models."],"url":"http://arxiv.org/abs/2406.02426v1","category":"math.OC"}
{"created":"2024-06-04 15:44:25","title":"CoNav: A Benchmark for Human-Centered Collaborative Navigation","abstract":"Human-robot collaboration, in which the robot intelligently assists the human with the upcoming task, is an appealing objective. To achieve this goal, the agent needs to be equipped with a fundamental collaborative navigation ability, where the agent should reason human intention by observing human activities and then navigate to the human's intended destination in advance of the human. However, this vital ability has not been well studied in previous literature. To fill this gap, we propose a collaborative navigation (CoNav) benchmark. Our CoNav tackles the critical challenge of constructing a 3D navigation environment with realistic and diverse human activities. To achieve this, we design a novel LLM-based humanoid animation generation framework, which is conditioned on both text descriptions and environmental context. The generated humanoid trajectory obeys the environmental context and can be easily integrated into popular simulators. We empirically find that the existing navigation methods struggle in CoNav task since they neglect the perception of human intention. To solve this problem, we propose an intention-aware agent for reasoning both long-term and short-term human intention. The agent predicts navigation action based on the predicted intention and panoramic observation. The emergent agent behavior including observing humans, avoiding human collision, and navigation reveals the efficiency of the proposed datasets and agents.","sentences":["Human-robot collaboration, in which the robot intelligently assists the human with the upcoming task, is an appealing objective.","To achieve this goal, the agent needs to be equipped with a fundamental collaborative navigation ability, where the agent should reason human intention by observing human activities and then navigate to the human's intended destination in advance of the human.","However, this vital ability has not been well studied in previous literature.","To fill this gap, we propose a collaborative navigation (CoNav) benchmark.","Our CoNav tackles the critical challenge of constructing a 3D navigation environment with realistic and diverse human activities.","To achieve this, we design a novel LLM-based humanoid animation generation framework, which is conditioned on both text descriptions and environmental context.","The generated humanoid trajectory obeys the environmental context and can be easily integrated into popular simulators.","We empirically find that the existing navigation methods struggle in CoNav task since they neglect the perception of human intention.","To solve this problem, we propose an intention-aware agent for reasoning both long-term and short-term human intention.","The agent predicts navigation action based on the predicted intention and panoramic observation.","The emergent agent behavior including observing humans, avoiding human collision, and navigation reveals the efficiency of the proposed datasets and agents."],"url":"http://arxiv.org/abs/2406.02425v1","category":"cs.CV"}
{"created":"2024-06-04 15:44:10","title":"Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints","abstract":"We study the contextual dynamic pricing problem where a firm sells products to $T$ sequentially arriving consumers that behave according to an unknown demand model. The firm aims to maximize its revenue, i.e. minimize its regret over a clairvoyant that knows the model in advance. The demand model is a generalized linear model (GLM), allowing for a stochastic feature vector in $\\mathbb R^d$ that encodes product and consumer information. We first show that the optimal regret upper bound is of order $\\sqrt{dT}$, up to a logarithmic factor, improving upon existing upper bounds in the literature by a $\\sqrt{d}$ factor. This sharper rate is materialised by two algorithms: a confidence bound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A key insight of our theoretical result is an intrinsic connection between dynamic pricing and the contextual multi-armed bandit problem with many arms based on a careful discretization. We further study contextual dynamic pricing under the local differential privacy (LDP) constraints. In particular, we propose a stochastic gradient descent based ETC algorithm that achieves an optimal regret upper bound of order $d\\sqrt{T}/\\epsilon$, up to a logarithmic factor, where $\\epsilon>0$ is the privacy parameter. The regret upper bounds with and without LDP constraints are accompanied by newly constructed minimax lower bounds, which further characterize the cost of privacy. Extensive numerical experiments and a real data application on online lending are conducted to illustrate the efficiency and practical value of the proposed algorithms in dynamic pricing.","sentences":["We study the contextual dynamic pricing problem where a firm sells products to $T$ sequentially arriving consumers that behave according to an unknown demand model.","The firm aims to maximize its revenue, i.e. minimize its regret over a clairvoyant that knows the model in advance.","The demand model is a generalized linear model (GLM), allowing for a stochastic feature vector in $\\mathbb R^d$ that encodes product and consumer information.","We first show that the optimal regret upper bound is of order $\\sqrt{dT}$, up to a logarithmic factor, improving upon existing upper bounds in the literature by a $\\sqrt{d}$ factor.","This sharper rate is materialised by two algorithms: a confidence bound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm.","A key insight of our theoretical result is an intrinsic connection between dynamic pricing and the contextual multi-armed bandit problem with many arms based on a careful discretization.","We further study contextual dynamic pricing under the local differential privacy (LDP) constraints.","In particular, we propose a stochastic gradient descent based ETC algorithm that achieves an optimal regret upper bound of order $d\\sqrt{T}/\\epsilon$, up to a logarithmic factor, where $\\epsilon>0$ is the privacy parameter.","The regret upper bounds with and without LDP constraints are accompanied by newly constructed minimax lower bounds, which further characterize the cost of privacy.","Extensive numerical experiments and a real data application on online lending are conducted to illustrate the efficiency and practical value of the proposed algorithms in dynamic pricing."],"url":"http://arxiv.org/abs/2406.02424v1","category":"cs.LG"}
{"created":"2024-06-04 15:32:45","title":"N-photon solutions to the two-qubit quantum Rabi model","abstract":"We studied the two-qubit quantum Rabi model and found its dark state solutions with at most N photons. One peculiar case presents when $N=3$, which has constant eigenenergy in the whole coupling regime and leads to level crossings within the same parity subspace. We also discovered asymptotic solutions with at most $N=2i+3$ $(i=1,2,3,\\dots)$ photons, and constant eigenenergy $N\\hbar \\omega$ when coupling $g$ becomes much larger than photon frequency $\\omega$. Although generally all photon number states are involved in the two-qubit quantum Rabi model, such $N$-photon solutions exist and may have applications in quantum information processing with ultrastrong couplings.","sentences":["We studied the two-qubit quantum Rabi model and found its dark state solutions with at most N photons.","One peculiar case presents when $N=3$, which has constant eigenenergy in the whole coupling regime and leads to level crossings within the same parity subspace.","We also discovered asymptotic solutions with at most $N=2i+3$ $(i=1,2,3,\\dots)$ photons, and constant eigenenergy $N\\hbar \\omega$ when coupling $g$ becomes much larger than photon frequency $\\omega$.","Although generally all photon number states are involved in the two-qubit quantum Rabi model, such $N$-photon solutions exist and may have applications in quantum information processing with ultrastrong couplings."],"url":"http://arxiv.org/abs/2406.02418v1","category":"quant-ph"}
{"created":"2024-06-04 15:31:33","title":"Non-Gaussian tails without stochastic inflation","abstract":"We show, both analytically and numerically, that non-Gaussian tails in the probability density function of curvature perturbations arise in ultra-slow-roll inflation from the $\\delta N$ formalism, without invoking stochastic inflation. Previously reported discrepancies between both approaches are a consequence of not correctly accounting for momentum perturbations. Once they are taken into account, both approaches agree to an excellent degree. The shape of the tail depends strongly on the phase space of inflation.","sentences":["We show, both analytically and numerically, that non-Gaussian tails in the probability density function of curvature perturbations arise in ultra-slow-roll inflation from the $\\delta N$ formalism, without invoking stochastic inflation.","Previously reported discrepancies between both approaches are a consequence of not correctly accounting for momentum perturbations.","Once they are taken into account, both approaches agree to an excellent degree.","The shape of the tail depends strongly on the phase space of inflation."],"url":"http://arxiv.org/abs/2406.02417v1","category":"astro-ph.CO"}
{"created":"2024-06-04 15:23:53","title":"Piecewise linear potentials for false vacuum decay and negative modes","abstract":"We study bounce solutions and associated negative modes in the class of piecewise linear triangular-shaped potentials that may be viewed as approximations of smooth potentials. In these simple potentials, the bounce solution and action can be obtained analytically for a general spacetime dimension $D$. The eigenequations for the fluctuations around the bounce are universal and have the form of a Schr\\\"odinger-like equation with delta-function potentials. This Schr\\\"odinger equation is solved exactly for the negative modes whose number is confirmed to be one. The latter result may justify the usefulness of such piecewise linear potentials in the study of false vacuum decay.","sentences":["We study bounce solutions and associated negative modes in the class of piecewise linear triangular-shaped potentials that may be viewed as approximations of smooth potentials.","In these simple potentials, the bounce solution and action can be obtained analytically for a general spacetime dimension $D$. The eigenequations for the fluctuations around the bounce are universal and have the form of a Schr\\\"odinger-like equation with delta-function potentials.","This Schr\\\"odinger equation is solved exactly for the negative modes whose number is confirmed to be one.","The latter result may justify the usefulness of such piecewise linear potentials in the study of false vacuum decay."],"url":"http://arxiv.org/abs/2406.02414v1","category":"hep-ph"}
{"created":"2024-06-04 15:21:37","title":"Decoupling of neural network calibration measures","abstract":"A lot of effort is currently invested in safeguarding autonomous driving systems, which heavily rely on deep neural networks for computer vision. We investigate the coupling of different neural network calibration measures with a special focus on the Area Under the Sparsification Error curve (AUSE) metric. We elaborate on the well-known inconsistency in determining optimal calibration using the Expected Calibration Error (ECE) and we demonstrate similar issues for the AUSE, the Uncertainty Calibration Score (UCS), as well as the Uncertainty Calibration Error (UCE). We conclude that the current methodologies leave a degree of freedom, which prevents a unique model calibration for the homologation of safety-critical functionalities. Furthermore, we propose the AUSE as an indirect measure for the residual uncertainty, which is irreducible for a fixed network architecture and is driven by the stochasticity in the underlying data generation process (aleatoric contribution) as well as the limitation in the hypothesis space (epistemic contribution).","sentences":["A lot of effort is currently invested in safeguarding autonomous driving systems, which heavily rely on deep neural networks for computer vision.","We investigate the coupling of different neural network calibration measures with a special focus on the Area Under the Sparsification Error curve (AUSE) metric.","We elaborate on the well-known inconsistency in determining optimal calibration using the Expected Calibration Error (ECE) and we demonstrate similar issues for the AUSE, the Uncertainty Calibration Score (UCS), as well as the Uncertainty Calibration Error (UCE).","We conclude that the current methodologies leave a degree of freedom, which prevents a unique model calibration for the homologation of safety-critical functionalities.","Furthermore, we propose the AUSE as an indirect measure for the residual uncertainty, which is irreducible for a fixed network architecture and is driven by the stochasticity in the underlying data generation process (aleatoric contribution) as well as the limitation in the hypothesis space (epistemic contribution)."],"url":"http://arxiv.org/abs/2406.02411v1","category":"cs.CV"}
{"created":"2024-06-04 15:12:01","title":"MOTIF: A tool for Mutation Testing with Fuzzing","abstract":"Mutation testing consists of generating test cases that detect faults injected into software (generating mutants) which its original test suite could not. By running such an augmented set of test cases, it may discover actual faults that may have gone unnoticed with the original test suite. It is thus a desired practice for embedded software running in safety-critical cyber-physical systems (CPS). Unfortunately, the state-of-the-art tool targeting C, a typical language for CPS software, relies on symbolic execution, whose limitations often prevent its application. MOTIF overcomes such limitations by leveraging grey-box fuzzing tools to generate unit test cases in C that detect injected faults in mutants. Indeed, fuzzing tools automatically generate inputs by exercising the compiled version of the software under test guided by coverage feedback, thus overcoming the limitations of symbolic execution. Our empirical assessment has shown that it detects more faults than symbolic execution (i.e., up to 47 percentage points), when the latter is applicable.","sentences":["Mutation testing consists of generating test cases that detect faults injected into software (generating mutants) which its original test suite could not.","By running such an augmented set of test cases, it may discover actual faults that may have gone unnoticed with the original test suite.","It is thus a desired practice for embedded software running in safety-critical cyber-physical systems (CPS).","Unfortunately, the state-of-the-art tool targeting C, a typical language for CPS software, relies on symbolic execution, whose limitations often prevent its application.","MOTIF overcomes such limitations by leveraging grey-box fuzzing tools to generate unit test cases in C that detect injected faults in mutants.","Indeed, fuzzing tools automatically generate inputs by exercising the compiled version of the software under test guided by coverage feedback, thus overcoming the limitations of symbolic execution.","Our empirical assessment has shown that it detects more faults than symbolic execution (i.e., up to 47 percentage points), when the latter is applicable."],"url":"http://arxiv.org/abs/2406.02398v1","category":"cs.SE"}
{"created":"2024-06-04 15:11:27","title":"The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding","abstract":"The evaluation of English text embeddings has transitioned from evaluating a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB. However, this is not the case for multilingual text embeddings due to a lack of available benchmarks. To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB). SEB is a comprehensive framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26 models, uncovering significant performance disparities between public and commercial solutions not previously captured by MTEB. We open-source SEB and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages.","sentences":["The evaluation of English text embeddings has transitioned from evaluating a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB.","However, this is not the case for multilingual text embeddings due to a lack of available benchmarks.","To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB).","SEB is a comprehensive framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories.","Building on SEB, we evaluate more than 26 models, uncovering significant performance disparities between public and commercial solutions not previously captured by MTEB.","We open-source SEB and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages."],"url":"http://arxiv.org/abs/2406.02396v1","category":"cs.CL"}
{"created":"2024-06-04 15:09:29","title":"GrootVL: Tree Topology is All You Need in State Space Model","abstract":"The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost.","sentences":["The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency.","However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies.","To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features.","Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities.","Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost.","GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks.","Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation.","Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost."],"url":"http://arxiv.org/abs/2406.02395v1","category":"cs.LG"}
{"created":"2024-06-04 15:08:56","title":"Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data","abstract":"Large Language Models (LLMs) like ChatGPT demonstrate significant potential in the medical field, often evaluated using multiple-choice questions (MCQs) similar to those found on the USMLE. Despite their prevalence in medical education, MCQs have limitations that might be exacerbated when assessing LLMs. To evaluate the effectiveness of MCQs in assessing the performance of LLMs, we developed a fictional medical benchmark focused on a non-existent gland, the Glianorex. This approach allowed us to isolate the knowledge of the LLM from its test-taking abilities. We used GPT-4 to generate a comprehensive textbook on the Glianorex in both English and French and developed corresponding multiple-choice questions in both languages. We evaluated various open-source, proprietary, and domain-specific LLMs using these questions in a zero-shot setting. The models achieved average scores around 67%, with minor performance differences between larger and smaller models. Performance was slightly higher in English than in French. Fine-tuned medical models showed some improvement over their base versions in English but not in French. The uniformly high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs' clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills. This study underscores the need for more robust evaluation methods to better assess the true capabilities of LLMs in medical contexts.","sentences":["Large Language Models (LLMs) like ChatGPT demonstrate significant potential in the medical field, often evaluated using multiple-choice questions (MCQs) similar to those found on the USMLE.","Despite their prevalence in medical education, MCQs have limitations that might be exacerbated when assessing LLMs.","To evaluate the effectiveness of MCQs in assessing the performance of LLMs, we developed a fictional medical benchmark focused on a non-existent gland, the Glianorex.","This approach allowed us to isolate the knowledge of the LLM from its test-taking abilities.","We used GPT-4 to generate a comprehensive textbook on the Glianorex in both English and French and developed corresponding multiple-choice questions in both languages.","We evaluated various open-source, proprietary, and domain-specific LLMs using these questions in a zero-shot setting.","The models achieved average scores around 67%, with minor performance differences between larger and smaller models.","Performance was slightly higher in English than in French.","Fine-tuned medical models showed some improvement over their base versions in English but not in French.","The uniformly high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs' clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills.","This study underscores the need for more robust evaluation methods to better assess the true capabilities of LLMs in medical contexts."],"url":"http://arxiv.org/abs/2406.02394v1","category":"cs.CL"}
{"created":"2024-06-04 15:08:20","title":"Valley-dependent transport through graphene quantum dots due to proximity-induced, staggered spin-orbit couplings","abstract":"We study a system composed of graphene decorated with an array of islands with C_3v symmetry that induce quantum dot (IQD) regions via proximity effects and give rise to several spin-orbit couplings (SOCs). We evaluate transport properties for an array of IQDs and analyze the conditions for realizing isolated valley conductances and valley-state localization. The resulting transmission shows a square-type behavior with wide gaps that can be tuned by adjusting the strength of the staggered intrinsic SOCs. Realistic proximity effects are characterized by weak SOC strengths, and the analysis of our results in this regime shows that the Rashba coupling is the important interaction controlling valley properties. As a consequence, a top gate voltage can be used to tune the valley polarization and switch the valley scattering for positive or negative incident energies. A proper choice of SOC strengths leads to higher localization of valley states around the linear array of IQDs. These systems can be implemented in heterostructures composed of graphene and semiconducting transition-metal dichalcogenides (TMDs) such as MoSe2, WSe2, MoS2, or WS2. In these setups, the magnitudes of induced SOCs depend on the twist angle, and due to broken valley degeneracy, valley-polarized currents at the edges can be generated in a controllable manner as well as localized valley states. Our findings suggest an alternative approach for producing valley-polarized currents and propose a corresponding mechanism for valley-dependent electron optics and optoelectronic devices.","sentences":["We study a system composed of graphene decorated with an array of islands with C_3v symmetry that induce quantum dot (IQD) regions via proximity effects and give rise to several spin-orbit couplings (SOCs).","We evaluate transport properties for an array of IQDs and analyze the conditions for realizing isolated valley conductances and valley-state localization.","The resulting transmission shows a square-type behavior with wide gaps that can be tuned by adjusting the strength of the staggered intrinsic SOCs.","Realistic proximity effects are characterized by weak SOC strengths, and the analysis of our results in this regime shows that the Rashba coupling is the important interaction controlling valley properties.","As a consequence, a top gate voltage can be used to tune the valley polarization and switch the valley scattering for positive or negative incident energies.","A proper choice of SOC strengths leads to higher localization of valley states around the linear array of IQDs.","These systems can be implemented in heterostructures composed of graphene and semiconducting transition-metal dichalcogenides (TMDs) such as MoSe2, WSe2, MoS2, or WS2.","In these setups, the magnitudes of induced SOCs depend on the twist angle, and due to broken valley degeneracy, valley-polarized currents at the edges can be generated in a controllable manner as well as localized valley states.","Our findings suggest an alternative approach for producing valley-polarized currents and propose a corresponding mechanism for valley-dependent electron optics and optoelectronic devices."],"url":"http://arxiv.org/abs/2406.02393v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-04 15:01:13","title":"Multifractality in monitored single-particle dynamics","abstract":"We study multifractal properties in time evolution of a single particle subject to repeated measurements. For quantum systems, we consider circuit models consisting of local unitary gates and local projective measurements. For classical systems, we consider models for estimating the trajectory of a particle evolved under local transition processes by partially measuring particle occupations. In both cases, multifractal behaviors appear in the ensemble of wave functions or probability distributions conditioned on measurement outcomes after a sufficiently long time. While the nature of particle transport (diffusive or ballistic) qualitatively affects the multifractal properties, they are even quantitatively robust to the measurement rate or specific protocols. On the other hand, multifractality is generically lost by generalized measurements allowing erroneous outcomes or by postselection of the outcomes with no particle detection. We demonstrate these properties by numerical simulations and also propose several simplified models, which allow us to analytically obtain multifractal properties in the monitored single-particle systems.","sentences":["We study multifractal properties in time evolution of a single particle subject to repeated measurements.","For quantum systems, we consider circuit models consisting of local unitary gates and local projective measurements.","For classical systems, we consider models for estimating the trajectory of a particle evolved under local transition processes by partially measuring particle occupations.","In both cases, multifractal behaviors appear in the ensemble of wave functions or probability distributions conditioned on measurement outcomes after a sufficiently long time.","While the nature of particle transport (diffusive or ballistic) qualitatively affects the multifractal properties, they are even quantitatively robust to the measurement rate or specific protocols.","On the other hand, multifractality is generically lost by generalized measurements allowing erroneous outcomes or by postselection of the outcomes with no particle detection.","We demonstrate these properties by numerical simulations and also propose several simplified models, which allow us to analytically obtain multifractal properties in the monitored single-particle systems."],"url":"http://arxiv.org/abs/2406.02386v1","category":"quant-ph"}
{"created":"2024-06-04 15:00:49","title":"Low-Rank Adaption on Transformer-based Oriented Object Detector for Satellite Onboard Processing of Remote Sensing Images","abstract":"Deep learning models in satellite onboard enable real-time interpretation of remote sensing images, reducing the need for data transmission to the ground and conserving communication resources. As satellite numbers and observation frequencies increase, the demand for satellite onboard real-time image interpretation grows, highlighting the expanding importance and development of this technology. However, updating the extensive parameters of models deployed on the satellites for spaceborne object detection model is challenging due to the limitations of uplink bandwidth in wireless satellite communications. To address this issue, this paper proposes a method based on parameter-efficient fine-tuning technology with low-rank adaptation (LoRA) module. It involves training low-rank matrix parameters and integrating them with the original model's weight matrix through multiplication and summation, thereby fine-tuning the model parameters to adapt to new data distributions with minimal weight updates. The proposed method combines parameter-efficient fine-tuning with full fine-tuning in the parameter update strategy of the oriented object detection algorithm architecture. This strategy enables model performance improvements close to full fine-tuning effects with minimal parameter updates. In addition, low rank approximation is conducted to pick an optimal rank value for LoRA matrices. Extensive experiments verify the effectiveness of the proposed method. By fine-tuning and updating only 12.4$\\%$ of the model's total parameters, it is able to achieve 97$\\%$ to 100$\\%$ of the performance of full fine-tuning models. Additionally, the reduced number of trainable parameters accelerates model training iterations and enhances the generalization and robustness of the oriented object detection model. The source code is available at: \\url{https://github.com/fudanxu/LoRA-Det}.","sentences":["Deep learning models in satellite onboard enable real-time interpretation of remote sensing images, reducing the need for data transmission to the ground and conserving communication resources.","As satellite numbers and observation frequencies increase, the demand for satellite onboard real-time image interpretation grows, highlighting the expanding importance and development of this technology.","However, updating the extensive parameters of models deployed on the satellites for spaceborne object detection model is challenging due to the limitations of uplink bandwidth in wireless satellite communications.","To address this issue, this paper proposes a method based on parameter-efficient fine-tuning technology with low-rank adaptation (LoRA) module.","It involves training low-rank matrix parameters and integrating them with the original model's weight matrix through multiplication and summation, thereby fine-tuning the model parameters to adapt to new data distributions with minimal weight updates.","The proposed method combines parameter-efficient fine-tuning with full fine-tuning in the parameter update strategy of the oriented object detection algorithm architecture.","This strategy enables model performance improvements close to full fine-tuning effects with minimal parameter updates.","In addition, low rank approximation is conducted to pick an optimal rank value for LoRA matrices.","Extensive experiments verify the effectiveness of the proposed method.","By fine-tuning and updating only 12.4$\\%$ of the model's total parameters, it is able to achieve 97$\\%$ to 100$\\%$ of the performance of full fine-tuning models.","Additionally, the reduced number of trainable parameters accelerates model training iterations and enhances the generalization and robustness of the oriented object detection model.","The source code is available at: \\url{https://github.com/fudanxu/LoRA-Det}."],"url":"http://arxiv.org/abs/2406.02385v1","category":"cs.CV"}
{"created":"2024-06-04 14:59:38","title":"Learning to Edit Visual Programs with Self-Supervision","abstract":"We design a system that learns how to edit visual programs. Our edit network consumes a complete input program and a visual target. From this input, we task our network with predicting a local edit operation that could be applied to the input program to improve its similarity to the target. In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in one-shot. Our joint finetuning scheme, when coupled with an inference procedure that initializes a population from the one-shot model and evolves members of this population with the edit network, helps to infer more accurate visual programs. Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages.","sentences":["We design a system that learns how to edit visual programs.","Our edit network consumes a complete input program and a visual target.","From this input, we task our network with predicting a local edit operation that could be applied to the input program to improve its similarity to the target.","In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in one-shot.","Our joint finetuning scheme, when coupled with an inference procedure that initializes a population from the one-shot model and evolves members of this population with the edit network, helps to infer more accurate visual programs.","Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages."],"url":"http://arxiv.org/abs/2406.02383v1","category":"cs.CV"}
{"created":"2024-06-04 14:58:10","title":"Kirigami: large convolutional kernels improve deep learning-based RNA secondary structure prediction","abstract":"We introduce a novel fully convolutional neural network (FCN) architecture for predicting the secondary structure of ribonucleic acid (RNA) molecules. Interpreting RNA structures as weighted graphs, we employ deep learning to estimate the probability of base pairing between nucleotide residues. Unique to our model are its massive 11-pixel kernels, which we argue provide a distinct advantage for FCNs on the specialized domain of RNA secondary structures. On a widely adopted, standardized test set comprised of 1,305 molecules, the accuracy of our method exceeds that of current state-of-the-art (SOTA) secondary structure prediction software, achieving a Matthews Correlation Coefficient (MCC) over 11-40% higher than that of other leading methods on overall structures and 58-400% higher on pseudoknots specifically.","sentences":["We introduce a novel fully convolutional neural network (FCN) architecture for predicting the secondary structure of ribonucleic acid (RNA) molecules.","Interpreting RNA structures as weighted graphs, we employ deep learning to estimate the probability of base pairing between nucleotide residues.","Unique to our model are its massive 11-pixel kernels, which we argue provide a distinct advantage for FCNs on the specialized domain of RNA secondary structures.","On a widely adopted, standardized test set comprised of 1,305 molecules, the accuracy of our method exceeds that of current state-of-the-art (SOTA) secondary structure prediction software, achieving a Matthews Correlation Coefficient (MCC) over 11-40% higher than that of other leading methods on overall structures and 58-400% higher on pseudoknots specifically."],"url":"http://arxiv.org/abs/2406.02381v1","category":"q-bio.BM"}
{"created":"2024-06-04 14:57:56","title":"EUFCC-340K: A Faceted Hierarchical Dataset for Metadata Annotation in GLAM Collections","abstract":"In this paper, we address the challenges of automatic metadata annotation in the domain of Galleries, Libraries, Archives, and Museums (GLAMs) by introducing a novel dataset, EUFCC340K, collected from the Europeana portal. Comprising over 340,000 images, the EUFCC340K dataset is organized across multiple facets: Materials, Object Types, Disciplines, and Subjects, following a hierarchical structure based on the Art & Architecture Thesaurus (AAT). We developed several baseline models, incorporating multiple heads on a ConvNeXT backbone for multi-label image tagging on these facets, and fine-tuning a CLIP model with our image text pairs. Our experiments to evaluate model robustness and generalization capabilities in two different test scenarios demonstrate the utility of the dataset in improving multi-label classification tools that have the potential to alleviate cataloging tasks in the cultural heritage sector.","sentences":["In this paper, we address the challenges of automatic metadata annotation in the domain of Galleries, Libraries, Archives, and Museums (GLAMs) by introducing a novel dataset, EUFCC340K, collected from the Europeana portal.","Comprising over 340,000 images, the EUFCC340K dataset is organized across multiple facets: Materials, Object Types, Disciplines, and Subjects, following a hierarchical structure based on the Art & Architecture Thesaurus (AAT).","We developed several baseline models, incorporating multiple heads on a ConvNeXT backbone for multi-label image tagging on these facets, and fine-tuning a CLIP model with our image text pairs.","Our experiments to evaluate model robustness and generalization capabilities in two different test scenarios demonstrate the utility of the dataset in improving multi-label classification tools that have the potential to alleviate cataloging tasks in the cultural heritage sector."],"url":"http://arxiv.org/abs/2406.02380v1","category":"cs.CV"}
{"created":"2024-06-04 14:55:43","title":"On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept","abstract":"Large Language Models (LLMs) can improve their responses when instructed to do so, a capability known as self-correction. When these instructions lack specific details about the issues in the response, this is referred to as leveraging the intrinsic self-correction capability. The empirical success of self-correction can be found in various applications, e.g., text detoxification and social bias mitigation. However, leveraging this self-correction capability may not always be effective, as it has the potential to revise an initially correct response into an incorrect one. In this paper, we endeavor to understand how and why leveraging the self-correction capability is effective. We identify that appropriate instructions can guide LLMs to a convergence state, wherein additional self-correction steps do not yield further performance improvements. We empirically demonstrate that model uncertainty and activated latent concepts jointly characterize the effectiveness of self-correction. Furthermore, we provide a mathematical formulation indicating that the activated latent concept drives the convergence of the model uncertainty and self-correction performance. Our analysis can also be generalized to the self-correction behaviors observed in Vision-Language Models (VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from our principle in terms of selecting effective fine-tuning samples. Such initial success demonstrates the potential extensibility for better instruction tuning and safety alignment.","sentences":["Large Language Models (LLMs) can improve their responses when instructed to do so, a capability known as self-correction.","When these instructions lack specific details about the issues in the response, this is referred to as leveraging the intrinsic self-correction capability.","The empirical success of self-correction can be found in various applications, e.g., text detoxification and social bias mitigation.","However, leveraging this self-correction capability may not always be effective, as it has the potential to revise an initially correct response into an incorrect one.","In this paper, we endeavor to understand how and why leveraging the self-correction capability is effective.","We identify that appropriate instructions can guide LLMs to a convergence state, wherein additional self-correction steps do not yield further performance improvements.","We empirically demonstrate that model uncertainty and activated latent concepts jointly characterize the effectiveness of self-correction.","Furthermore, we provide a mathematical formulation indicating that the activated latent concept drives the convergence of the model uncertainty and self-correction performance.","Our analysis can also be generalized to the self-correction behaviors observed in Vision-Language Models (VLMs).","Moreover, we highlight that task-agnostic debiasing can benefit from our principle in terms of selecting effective fine-tuning samples.","Such initial success demonstrates the potential extensibility for better instruction tuning and safety alignment."],"url":"http://arxiv.org/abs/2406.02378v1","category":"cs.CL"}
{"created":"2024-06-04 14:55:14","title":"XRec: Large Language Models for Explainable Recommendation","abstract":"Recommender systems help users navigate information overload by providing personalized recommendations aligned with their preferences. Collaborative Filtering (CF) is a widely adopted approach, but while advanced techniques like graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced CF models for better user representations, they often lack the ability to provide explanations for the recommended items. Explainable recommendations aim to address this gap by offering transparency and insights into the recommendation decision-making process, enhancing users' understanding. This work leverages the language capabilities of Large Language Models (LLMs) to push the boundaries of explainable recommender systems. We introduce a model-agnostic framework called XRec, which enables LLMs to provide comprehensive explanations for user behaviors in recommender systems. By integrating collaborative signals and designing a lightweight collaborative adaptor, the framework empowers LLMs to understand complex patterns in user-item interactions and gain a deeper understanding of user preferences. Our extensive experiments demonstrate the effectiveness of XRec, showcasing its ability to generate comprehensive and meaningful explanations that outperform baseline approaches in explainable recommender systems. We open-source our model implementation at https://github.com/HKUDS/XRec.","sentences":["Recommender systems help users navigate information overload by providing personalized recommendations aligned with their preferences.","Collaborative Filtering (CF) is a widely adopted approach, but while advanced techniques like graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced CF models for better user representations, they often lack the ability to provide explanations for the recommended items.","Explainable recommendations aim to address this gap by offering transparency and insights into the recommendation decision-making process, enhancing users' understanding.","This work leverages the language capabilities of Large Language Models (LLMs) to push the boundaries of explainable recommender systems.","We introduce a model-agnostic framework called XRec, which enables LLMs to provide comprehensive explanations for user behaviors in recommender systems.","By integrating collaborative signals and designing a lightweight collaborative adaptor, the framework empowers LLMs to understand complex patterns in user-item interactions and gain a deeper understanding of user preferences.","Our extensive experiments demonstrate the effectiveness of XRec, showcasing its ability to generate comprehensive and meaningful explanations that outperform baseline approaches in explainable recommender systems.","We open-source our model implementation at https://github.com/HKUDS/XRec."],"url":"http://arxiv.org/abs/2406.02377v1","category":"cs.IR"}
{"created":"2024-06-04 14:49:14","title":"Values distribution of holomorphic curves on annuli into projective varieties with hypersurface targets","abstract":"The purpose of this paper has twofold. The first is to establish a Cartan-Nochka theorem for holomorphic curves from annuli into projective varieties intersecting hypersurfaces in subgeneral position with truncated counting functions. The second is to give a second main theorem for such curves with arbitrary families of hypersurfaces, where the truncation level of the counting functions is explicitly estimated not depending on the number of hypersurfaces. As an application, we give a uniqueness theorem for such curves sharing a few hypersurfaces in a projective variety ignoring multiplicity. Our results generalize and improve the recent previous results in this topic.","sentences":["The purpose of this paper has twofold.","The first is to establish a Cartan-Nochka theorem for holomorphic curves from annuli into projective varieties intersecting hypersurfaces in subgeneral position with truncated counting functions.","The second is to give a second main theorem for such curves with arbitrary families of hypersurfaces, where the truncation level of the counting functions is explicitly estimated not depending on the number of hypersurfaces.","As an application, we give a uniqueness theorem for such curves sharing a few hypersurfaces in a projective variety ignoring multiplicity.","Our results generalize and improve the recent previous results in this topic."],"url":"http://arxiv.org/abs/2406.02371v1","category":"math.CV"}
{"created":"2024-06-04 14:46:25","title":"Large Language Models Make Sample-Efficient Recommender Systems","abstract":"Large language models (LLMs) have achieved remarkable progress in the field of natural language processing (NLP), demonstrating remarkable abilities in producing text that resembles human language for various tasks. This opens up new opportunities for employing them in recommender systems (RSs). In this paper, we specifically examine the sample efficiency of LLM-enhanced recommender systems, which pertains to the model's capacity to attain superior performance with a limited quantity of training data. Conventional recommendation models (CRMs) often need a large amount of training data because of the sparsity of features and interactions. Hence, we propose and verify our core viewpoint: Large Language Models Make Sample-Efficient Recommender Systems. We propose a simple yet effective framework (i.e., Laser) to validate the viewpoint from two aspects: (1) LLMs themselves are sample-efficient recommenders; and (2) LLMs, as feature generators and encoders, make CRMs more sample-efficient. Extensive experiments on two public datasets show that Laser requires only a small fraction of training samples to match or even surpass CRMs that are trained on the entire training set, demonstrating superior sample efficiency.","sentences":["Large language models (LLMs) have achieved remarkable progress in the field of natural language processing (NLP), demonstrating remarkable abilities in producing text that resembles human language for various tasks.","This opens up new opportunities for employing them in recommender systems (RSs).","In this paper, we specifically examine the sample efficiency of LLM-enhanced recommender systems, which pertains to the model's capacity to attain superior performance with a limited quantity of training data.","Conventional recommendation models (CRMs) often need a large amount of training data because of the sparsity of features and interactions.","Hence, we propose and verify our core viewpoint: Large Language Models Make Sample-Efficient Recommender Systems.","We propose a simple yet effective framework (i.e., Laser) to validate the viewpoint from two aspects: (1) LLMs themselves are sample-efficient recommenders; and (2) LLMs, as feature generators and encoders, make CRMs more sample-efficient.","Extensive experiments on two public datasets show that Laser requires only a small fraction of training samples to match or even surpass CRMs that are trained on the entire training set, demonstrating superior sample efficiency."],"url":"http://arxiv.org/abs/2406.02368v1","category":"cs.IR"}
{"created":"2024-06-04 14:45:59","title":"A Note on and Generalization of \"Exploring Modified Kaniadakis Entropy: MOND Theory and the Bekenstein Bound Conjecture\"","abstract":"In a recent paper by Ambr\\'osio et al. [arXiv:2405.14799], it was shown that the gravitational force law of the Modified Newtonian Dynamics (MOND) phenomenology can be derived within the framework of entropic gravity and the holographic principle by assuming an entropy function other than the conventional Boltzmann-Gibbs entropy. In particular, they derived the standard interpolation function of MOND together with an analytical expression for the acceleration constant $a_0$ by utilizing Kaniadakis' modified entropy. In this short note, using the same methodology, we generalize this result and show that MONDian behavior is a rather general consequence of combining entropic gravity with non-Boltzmann-Gibbs entropies, which depends on only a few conditions imposed on the generalized entropy function.","sentences":["In a recent paper by Ambr\\'osio et al.","[arXiv:2405.14799], it was shown that the gravitational force law of the Modified Newtonian Dynamics (MOND) phenomenology can be derived within the framework of entropic gravity and the holographic principle by assuming an entropy function other than the conventional Boltzmann-Gibbs entropy.","In particular, they derived the standard interpolation function of MOND together with an analytical expression for the acceleration constant $a_0$ by utilizing Kaniadakis' modified entropy.","In this short note, using the same methodology, we generalize this result and show that MONDian behavior is a rather general consequence of combining entropic gravity with non-Boltzmann-Gibbs entropies, which depends on only a few conditions imposed on the generalized entropy function."],"url":"http://arxiv.org/abs/2406.02367v1","category":"gr-qc"}
{"created":"2024-06-04 14:45:47","title":"Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models","abstract":"Diffusion models (DMs) produce very detailed and high-quality images. Their power results from extensive training on large amounts of data, usually scraped from the internet without proper attribution or consent from content creators. Unfortunately, this practice raises privacy and intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples during inference, or removing the memorized data from training altogether. While those are viable solutions when the DM is developed and deployed in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released. To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to the level of neurons in DMs' cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, single neurons are responsible for memorizing particular training samples. By deactivating these memorization neurons, we can avoid the replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data. In this way, our NeMo contributes to a more responsible deployment of DMs.","sentences":["Diffusion models (DMs) produce very detailed and high-quality images.","Their power results from extensive training on large amounts of data, usually scraped from the internet without proper attribution or consent from content creators.","Unfortunately, this practice raises privacy and intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time.","Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples during inference, or removing the memorized data from training altogether.","While those are viable solutions when the DM is developed and deployed in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released.","To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to the level of neurons in DMs' cross-attention layers.","Through our experiments, we make the intriguing finding that in many cases, single neurons are responsible for memorizing particular training samples.","By deactivating these memorization neurons, we can avoid the replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data.","In this way, our NeMo contributes to a more responsible deployment of DMs."],"url":"http://arxiv.org/abs/2406.02366v1","category":"cs.LG"}
{"created":"2024-06-04 14:39:51","title":"Temporal Graph Rewiring with Expander Graphs","abstract":"Evolving relations in real-world networks are often modelled by temporal graphs. Graph rewiring techniques have been utilised on Graph Neural Networks (GNNs) to improve expressiveness and increase model performance. In this work, we propose Temporal Graph Rewiring (TGR), the first approach for graph rewiring on temporal graphs. TGR enables communication between temporally distant nodes in a continuous time dynamic graph by utilising expander graph propagation to construct a message passing highway for message passing between distant nodes. Expander graphs are suitable candidates for rewiring as they help overcome the oversquashing problem often observed in GNNs. On the public tgbl-wiki benchmark, we show that TGR improves the performance of a widely used TGN model by a significant margin. Our code repository is accessible at https://anonymous.4open.science/r/TGR-254C.","sentences":["Evolving relations in real-world networks are often modelled by temporal graphs.","Graph rewiring techniques have been utilised on Graph Neural Networks (GNNs) to improve expressiveness and increase model performance.","In this work, we propose Temporal Graph Rewiring (TGR), the first approach for graph rewiring on temporal graphs.","TGR enables communication between temporally distant nodes in a continuous time dynamic graph by utilising expander graph propagation to construct a message passing highway for message passing between distant nodes.","Expander graphs are suitable candidates for rewiring as they help overcome the oversquashing problem often observed in GNNs.","On the public tgbl-wiki benchmark, we show that TGR improves the performance of a widely used TGN model by a significant margin.","Our code repository is accessible at https://anonymous.4open.science/r/TGR-254C."],"url":"http://arxiv.org/abs/2406.02362v1","category":"cs.LG"}
{"created":"2024-06-04 14:38:30","title":"Using Self-supervised Learning Can Improve Model Fairness","abstract":"Self-supervised learning (SSL) has become the de facto training paradigm of large models, where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Despite demonstrating comparable performance with supervised methods, comprehensive efforts to assess SSL's impact on machine learning fairness (i.e., performing equally on different demographic breakdowns) are lacking. Hypothesizing that SSL models would learn more generic, hence less biased representations, this study explores the impact of pre-training and fine-tuning strategies on fairness. We introduce a fairness assessment framework for SSL, comprising five stages: defining dataset requirements, pre-training, fine-tuning with gradual unfreezing, assessing representation similarity conditioned on demographics, and establishing domain-specific evaluation processes. We evaluate our method's generalizability on three real-world human-centric datasets (i.e., MIMIC, MESA, and GLOBEM) by systematically comparing hundreds of SSL and fine-tuned models on various dimensions spanning from the intermediate representations to appropriate evaluation metrics. Our findings demonstrate that SSL can significantly improve model fairness, while maintaining performance on par with supervised methods-exhibiting up to a 30% increase in fairness with minimal loss in performance through self-supervision. We posit that such differences can be attributed to representation dissimilarities found between the best- and the worst-performing demographics across models-up to x13 greater for protected attributes with larger performance discrepancies between segments.","sentences":["Self-supervised learning (SSL) has become the de facto training paradigm of large models, where pre-training is followed by supervised fine-tuning using domain-specific data and labels.","Despite demonstrating comparable performance with supervised methods, comprehensive efforts to assess SSL's impact on machine learning fairness (i.e., performing equally on different demographic breakdowns) are lacking.","Hypothesizing that SSL models would learn more generic, hence less biased representations, this study explores the impact of pre-training and fine-tuning strategies on fairness.","We introduce a fairness assessment framework for SSL, comprising five stages: defining dataset requirements, pre-training, fine-tuning with gradual unfreezing, assessing representation similarity conditioned on demographics, and establishing domain-specific evaluation processes.","We evaluate our method's generalizability on three real-world human-centric datasets (i.e., MIMIC, MESA, and GLOBEM) by systematically comparing hundreds of SSL and fine-tuned models on various dimensions spanning from the intermediate representations to appropriate evaluation metrics.","Our findings demonstrate that SSL can significantly improve model fairness, while maintaining performance on par with supervised methods-exhibiting up to a 30% increase in fairness with minimal loss in performance through self-supervision.","We posit that such differences can be attributed to representation dissimilarities found between the best- and the worst-performing demographics across models-up to x13 greater for protected attributes with larger performance discrepancies between segments."],"url":"http://arxiv.org/abs/2406.02361v1","category":"cs.LG"}
{"created":"2024-06-04 14:38:04","title":"Primordial gravitational wave backgrounds from phase transitions with next generation ground based detectors","abstract":"Third generation ground-based gravitational wave (GW) detectors, such as Einstein Telescope and Cosmic Explorer, will operate in the $(\\text{few}-10^4)$ Hz frequency band, with a boost in sensitivity providing an unprecedented reach into primordial cosmology. Working concurrently with pulsar timing arrays in the nHz band, and LISA in the mHz band, these 3G detectors will be powerful probes of beyond the standard model particle physics on scales $T\\gtrsim 10^{7}$GeV. Here we focus on their ability to probe phase transitions (PTs) in the early universe. We first overview the landscape of detectors across frequencies, discuss the relevance of astrophysical foregrounds, and provide convenient and up-to-date power-law integrated sensitivity curves for these detectors. We then present the constraints expected from GW observations on first order PTs and on topological defects (strings and domain walls), which may be formed when a symmetry is broken irrespective of the order of the phase transition. These constraints can then be applied to specific models leading to first order PTs and/or topological defects. In particular we discuss the implications for axion models, which solve the strong CP problem by introducing a spontaneously broken Peccei-Quinn (PQ) symmetry. For post-inflationary breaking, the PQ scale must lie in the $10^{8}-10^{11}$ GeV range, and so the signal from a first order PQ PT falls within reach of ground based 3G detectors. A scan in parameter space of signal-to-noise ratio in a representative model reveals their large potential to probe the nature of the PQ transition. Additionally, in heavy axion type models domain walls form, which can lead to a detectable GW background. We discuss their spectrum and summarise the expected constraints on these models from 3G detectors, together with SKA and LISA.","sentences":["Third generation ground-based gravitational wave (GW) detectors, such as Einstein Telescope and Cosmic Explorer, will operate in the $(\\text{few}-10^4)$","Hz frequency band, with a boost in sensitivity providing an unprecedented reach into primordial cosmology.","Working concurrently with pulsar timing arrays in the nHz band, and LISA in the mHz band, these 3G detectors will be powerful probes of beyond the standard model particle physics on scales $T\\gtrsim 10^{7}$GeV. Here we focus on their ability to probe phase transitions (PTs) in the early universe.","We first overview the landscape of detectors across frequencies, discuss the relevance of astrophysical foregrounds, and provide convenient and up-to-date power-law integrated sensitivity curves for these detectors.","We then present the constraints expected from GW observations on first order PTs and on topological defects (strings and domain walls), which may be formed when a symmetry is broken irrespective of the order of the phase transition.","These constraints can then be applied to specific models leading to first order PTs and/or topological defects.","In particular we discuss the implications for axion models, which solve the strong CP problem by introducing a spontaneously broken Peccei-Quinn (PQ) symmetry.","For post-inflationary breaking, the PQ scale must lie in the $10^{8}-10^{11}$ GeV range, and so the signal from a first order PQ PT falls within reach of ground based 3G detectors.","A scan in parameter space of signal-to-noise ratio in a representative model reveals their large potential to probe the nature of the PQ transition.","Additionally, in heavy axion type models domain walls form, which can lead to a detectable GW background.","We discuss their spectrum and summarise the expected constraints on these models from 3G detectors, together with SKA and LISA."],"url":"http://arxiv.org/abs/2406.02359v1","category":"astro-ph.CO"}
{"created":"2024-06-04 14:35:27","title":"The complexity of approximate (coarse) correlated equilibrium for incomplete information games","abstract":"We study the iteration complexity of decentralized learning of approximate correlated equilibria in incomplete information games.   On the negative side, we prove that in $\\mathit{extensive}$-$\\mathit{form}$ $\\mathit{games}$, assuming $\\mathsf{PPAD} \\not\\subset \\mathsf{TIME}(n^{\\mathsf{polylog}(n)})$, any polynomial-time learning algorithms must take at least $2^{\\log_2^{1-o(1)}(|\\mathcal{I}|)}$ iterations to converge to the set of $\\epsilon$-approximate correlated equilibrium, where $|\\mathcal{I}|$ is the number of nodes in the game and $\\epsilon > 0$ is an absolute constant. This nearly matches, up to the $o(1)$ term, the algorithms of [PR'24, DDFG'24] for learning $\\epsilon$-approximate correlated equilibrium, and resolves an open question of Anagnostides, Kalavasis, Sandholm, and Zampetakis [AKSZ'24]. Our lower bound holds even for the easier solution concept of $\\epsilon$-approximate $\\mathit{coarse}$ correlated equilibrium   On the positive side, we give uncoupled dynamics that reach $\\epsilon$-approximate correlated equilibria of a $\\mathit{Bayesian}$ $\\mathit{game}$ in polylogarithmic iterations, without any dependence of the number of types. This demonstrates a separation between Bayesian games and extensive-form games.","sentences":["We study the iteration complexity of decentralized learning of approximate correlated equilibria in incomplete information games.   ","On the negative side, we prove that in $\\mathit{extensive}$-$\\mathit{form}$ $\\mathit{games}$, assuming $\\mathsf{PPAD} \\not\\subset \\mathsf{TIME}(n^{\\mathsf{polylog}(n)})$, any polynomial-time learning algorithms must take at least $2^{\\log_2^{1-o(1)}(|\\mathcal{I}|)}$ iterations to converge to the set of $\\epsilon$-approximate correlated equilibrium, where $|\\mathcal{I}|$ is the number of nodes in the game and $\\epsilon > 0$ is an absolute constant.","This nearly matches, up to the $o(1)$ term, the algorithms of [PR'24, DDFG'24] for learning $\\epsilon$-approximate correlated equilibrium, and resolves an open question of Anagnostides, Kalavasis, Sandholm, and","Zampetakis","[AKSZ'24].","Our lower bound holds even for the easier solution concept of $\\epsilon$-approximate $\\mathit{coarse}$ correlated equilibrium   On the positive side, we give uncoupled dynamics that reach $\\epsilon$-approximate correlated equilibria of a $\\mathit{Bayesian}$ $\\mathit{game}$ in polylogarithmic iterations, without any dependence of the number of types.","This demonstrates a separation between Bayesian games and extensive-form games."],"url":"http://arxiv.org/abs/2406.02357v1","category":"cs.GT"}
{"created":"2024-06-04 14:34:39","title":"Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks","abstract":"The ability (and inability) of large language models (LLMs) to perform arithmetic tasks has been the subject of much theoretical and practical debate. We show that LLMs are frequently able to correctly and confidently predict the first digit of n-digit by m-digit multiplication tasks without using chain of thought reasoning, despite these tasks require compounding operations to solve. Simultaneously, LLMs in practice often fail to correctly or confidently predict the last digit of an n-digit by m-digit multiplication, a task equivalent to 1-digit by 1-digit multiplication which can be easily learned or memorized. We show that the latter task can be solved more robustly when the LLM is conditioned on all of the correct higher-order digits, which on average increases the confidence of the correct last digit on 5-digit by 5-digit multiplication tasks using Llama 2-13B by over 230% (0.13 to 0.43) and Mistral-7B by 150% (0.22 to 0.55).","sentences":["The ability (and inability) of large language models (LLMs) to perform arithmetic tasks has been the subject of much theoretical and practical debate.","We show that LLMs are frequently able to correctly and confidently predict the first digit of n-digit by m-digit multiplication tasks without using chain of thought reasoning, despite these tasks require compounding operations to solve.","Simultaneously, LLMs in practice often fail to correctly or confidently predict the last digit of an n-digit by m-digit multiplication, a task equivalent to 1-digit by 1-digit multiplication which can be easily learned or memorized.","We show that the latter task can be solved more robustly when the LLM is conditioned on all of the correct higher-order digits, which on average increases the confidence of the correct last digit on 5-digit by 5-digit multiplication tasks using Llama 2-13B by over 230% (0.13 to 0.43) and Mistral-7B by 150% (0.22 to 0.55)."],"url":"http://arxiv.org/abs/2406.02356v1","category":"cs.LG"}
{"created":"2024-06-04 14:34:13","title":"FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning","abstract":"Federated Learning (FL) has emerged as a pivotal framework for the development of effective global models (global FL) or personalized models (personalized FL) across clients with heterogeneous, non-iid data distribution. A key challenge in FL is client drift, where data heterogeneity impedes the aggregation of scattered knowledge. Recent studies have tackled the client drift issue by identifying significant divergence in the last classifier layer. To mitigate this divergence, strategies such as freezing the classifier weights and aligning the feature extractor accordingly have proven effective. Although the local alignment between classifier and feature extractor has been studied as a crucial factor in FL, we observe that it may lead the model to overemphasize the observed classes within each client. Thus, our objectives are twofold: (1) enhancing local alignment while (2) preserving the representation of unseen class samples. This approach aims to effectively integrate knowledge from individual clients, thereby improving performance for both global and personalized FL. To achieve this, we introduce a novel algorithm named FedDr+, which empowers local model alignment using dot-regression loss. FedDr+ freezes the classifier as a simplex ETF to align the features and improves aggregated global models by employing a feature distillation mechanism to retain information about unseen/missing classes. Consequently, we provide empirical evidence demonstrating that our algorithm surpasses existing methods that use a frozen classifier to boost alignment across the diverse distribution.","sentences":["Federated Learning (FL) has emerged as a pivotal framework for the development of effective global models (global FL) or personalized models (personalized FL) across clients with heterogeneous, non-iid data distribution.","A key challenge in FL is client drift, where data heterogeneity impedes the aggregation of scattered knowledge.","Recent studies have tackled the client drift issue by identifying significant divergence in the last classifier layer.","To mitigate this divergence, strategies such as freezing the classifier weights and aligning the feature extractor accordingly have proven effective.","Although the local alignment between classifier and feature extractor has been studied as a crucial factor in FL, we observe that it may lead the model to overemphasize the observed classes within each client.","Thus, our objectives are twofold: (1) enhancing local alignment while (2) preserving the representation of unseen class samples.","This approach aims to effectively integrate knowledge from individual clients, thereby improving performance for both global and personalized FL.","To achieve this, we introduce a novel algorithm named FedDr+, which empowers local model alignment using dot-regression loss.","FedDr+ freezes the classifier as a simplex ETF to align the features and improves aggregated global models by employing a feature distillation mechanism to retain information about unseen/missing classes.","Consequently, we provide empirical evidence demonstrating that our algorithm surpasses existing methods that use a frozen classifier to boost alignment across the diverse distribution."],"url":"http://arxiv.org/abs/2406.02355v1","category":"cs.CV"}
{"created":"2024-06-04 14:33:23","title":"Label-wise Aleatoric and Epistemic Uncertainty Quantification","abstract":"We present a novel approach to uncertainty quantification in classification tasks based on label-wise decomposition of uncertainty measures. This label-wise perspective allows uncertainty to be quantified at the individual class level, thereby improving cost-sensitive decision-making and helping understand the sources of uncertainty. Furthermore, it allows to define total, aleatoric, and epistemic uncertainty on the basis of non-categorical measures such as variance, going beyond common entropy-based measures. In particular, variance-based measures address some of the limitations associated with established methods that have recently been discussed in the literature. We show that our proposed measures adhere to a number of desirable properties. Through empirical evaluation on a variety of benchmark data sets -- including applications in the medical domain where accurate uncertainty quantification is crucial -- we establish the effectiveness of label-wise uncertainty quantification.","sentences":["We present a novel approach to uncertainty quantification in classification tasks based on label-wise decomposition of uncertainty measures.","This label-wise perspective allows uncertainty to be quantified at the individual class level, thereby improving cost-sensitive decision-making and helping understand the sources of uncertainty.","Furthermore, it allows to define total, aleatoric, and epistemic uncertainty on the basis of non-categorical measures such as variance, going beyond common entropy-based measures.","In particular, variance-based measures address some of the limitations associated with established methods that have recently been discussed in the literature.","We show that our proposed measures adhere to a number of desirable properties.","Through empirical evaluation on a variety of benchmark data sets -- including applications in the medical domain where accurate uncertainty quantification is crucial -- we establish the effectiveness of label-wise uncertainty quantification."],"url":"http://arxiv.org/abs/2406.02354v1","category":"cs.LG"}
{"created":"2024-06-04 14:24:53","title":"LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing","abstract":"Large language models (LLMs) have shown amazing capabilities in knowledge memorization and present. However, when it comes to domain-specific knowledge and downstream tasks like medical, general LLMs are often unable to give precise answers. In addition, when people want LLMs to answer classification questions, they usually go through instruction tuning first, however, LLMs do not always give a direct index of the categorization after instruction tuning. In this paper, we proposed LlamaCare, a fine-tuned medical language model, and Extended Classification Integration(ECI), a module to handle classification problems of LLMs. Our contributions are : (i) We fine-tuned a large language model of medical knowledge with very low carbon emissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundant categorical answers and improved the performance of LLMs by proposing a new module called Extended Classification Integration. (iii) We released our processed data for one-shot and few-shot training for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method achieves a close effect with the state-of-the-art model in benchmarks while costing lower GPU resources compared to LLMs with the same quantity of parameters. Our models, codes, and datasets can be found in https://github.com/Stephen-SMJ/LLamaCare","sentences":["Large language models (LLMs) have shown amazing capabilities in knowledge memorization and present.","However, when it comes to domain-specific knowledge and downstream tasks like medical, general LLMs are often unable to give precise answers.","In addition, when people want LLMs to answer classification questions, they usually go through instruction tuning first, however, LLMs do not always give a direct index of the categorization after instruction tuning.","In this paper, we proposed LlamaCare, a fine-tuned medical language model, and Extended Classification Integration(ECI), a module to handle classification problems of LLMs.","Our contributions are : (i) We fine-tuned a large language model of medical knowledge with very low carbon emissions and achieved similar performance with ChatGPT by a 24G GPU.","(ii) We solved the problem of redundant categorical answers and improved the performance of LLMs by proposing a new module called Extended Classification Integration.","(iii) We released our processed data for one-shot and few-shot training for some benchmarks such as PubMedQA and USMLE 1-3 step.","Our method achieves a close effect with the state-of-the-art model in benchmarks while costing lower GPU resources compared to LLMs with the same quantity of parameters.","Our models, codes, and datasets can be found in https://github.com/Stephen-SMJ/LLamaCare"],"url":"http://arxiv.org/abs/2406.02350v1","category":"cs.CL"}
{"created":"2024-06-04 14:24:35","title":"CADE: Cosine Annealing Differential Evolution for Spiking Neural Network","abstract":"Spiking neural networks (SNNs) have gained prominence for their potential in neuromorphic computing and energy-efficient artificial intelligence, yet optimizing them remains a formidable challenge for gradient-based methods due to their discrete, spike-based computation. This paper attempts to tackle the challenges by introducing Cosine Annealing Differential Evolution (CADE), designed to modulate the mutation factor (F) and crossover rate (CR) of differential evolution (DE) for the SNN model, i.e., Spiking Element Wise (SEW) ResNet. Extensive empirical evaluations were conducted to analyze CADE. CADE showed a balance in exploring and exploiting the search space, resulting in accelerated convergence and improved accuracy compared to existing gradient-based and DE-based methods. Moreover, an initialization method based on a transfer learning setting was developed, pretraining on a source dataset (i.e., CIFAR-10) and fine-tuning the target dataset (i.e., CIFAR-100), to improve population diversity. It was found to further enhance CADE for SNN. Remarkably, CADE elevates the performance of the highest accuracy SEW model by an additional 0.52 percentage points, underscoring its effectiveness in fine-tuning and enhancing SNNs. These findings emphasize the pivotal role of a scheduler for F and CR adjustment, especially for DE-based SNN. Source Code on Github: https://github.com/Tank-Jiang/CADE4SNN.","sentences":["Spiking neural networks (SNNs) have gained prominence for their potential in neuromorphic computing and energy-efficient artificial intelligence, yet optimizing them remains a formidable challenge for gradient-based methods due to their discrete, spike-based computation.","This paper attempts to tackle the challenges by introducing Cosine Annealing Differential Evolution (CADE), designed to modulate the mutation factor (F) and crossover rate (CR) of differential evolution (DE) for the SNN model, i.e., Spiking Element Wise (SEW) ResNet.","Extensive empirical evaluations were conducted to analyze CADE.","CADE showed a balance in exploring and exploiting the search space, resulting in accelerated convergence and improved accuracy compared to existing gradient-based and DE-based methods.","Moreover, an initialization method based on a transfer learning setting was developed, pretraining on a source dataset (i.e., CIFAR-10) and fine-tuning the target dataset (i.e., CIFAR-100), to improve population diversity.","It was found to further enhance CADE for SNN.","Remarkably, CADE elevates the performance of the highest accuracy SEW model by an additional 0.52 percentage points, underscoring its effectiveness in fine-tuning and enhancing SNNs.","These findings emphasize the pivotal role of a scheduler for F and CR adjustment, especially for DE-based SNN.","Source Code on Github: https://github.com/Tank-Jiang/CADE4SNN."],"url":"http://arxiv.org/abs/2406.02349v1","category":"cs.NE"}
{"created":"2024-06-04 14:24:30","title":"AMOSL: Adaptive Modality-wise Structure Learning in Multi-view Graph Neural Networks For Enhanced Unified Representation","abstract":"While Multi-view Graph Neural Networks (MVGNNs) excel at leveraging diverse modalities for learning object representation, existing methods assume identical local topology structures across modalities that overlook real-world discrepancies. This leads MVGNNs straggles in modality fusion and representations denoising. To address these issues, we propose adaptive modality-wise structure learning (AMoSL). AMoSL captures node correspondences between modalities via optimal transport, and jointly learning with graph embedding. To enable efficient end-to-end training, we employ an efficient solution for the resulting complex bilevel optimization problem. Furthermore, AMoSL adapts to downstream tasks through unsupervised learning on inter-modality distances. The effectiveness of AMoSL is demonstrated by its ability to train more accurate graph classifiers on six benchmark datasets.","sentences":["While Multi-view Graph Neural Networks (MVGNNs) excel at leveraging diverse modalities for learning object representation, existing methods assume identical local topology structures across modalities that overlook real-world discrepancies.","This leads MVGNNs straggles in modality fusion and representations denoising.","To address these issues, we propose adaptive modality-wise structure learning (AMoSL).","AMoSL captures node correspondences between modalities via optimal transport, and jointly learning with graph embedding.","To enable efficient end-to-end training, we employ an efficient solution for the resulting complex bilevel optimization problem.","Furthermore, AMoSL adapts to downstream tasks through unsupervised learning on inter-modality distances.","The effectiveness of AMoSL is demonstrated by its ability to train more accurate graph classifiers on six benchmark datasets."],"url":"http://arxiv.org/abs/2406.02348v1","category":"cs.LG"}
{"created":"2024-06-04 14:23:27","title":"Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation","abstract":"In this paper, we propose an efficient, fast, and versatile distillation method to accelerate the generation of pre-trained diffusion models: Flash Diffusion. The method reaches state-of-the-art performances in terms of FID and CLIP-Score for few steps image generation on the COCO2014 and COCO2017 datasets, while requiring only several GPU hours of training and fewer trainable parameters than existing methods. In addition to its efficiency, the versatility of the method is also exposed across several tasks such as text-to-image, inpainting, face-swapping, super-resolution and using different backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\alpha$), as well as adapters. In all cases, the method allowed to reduce drastically the number of sampling steps while maintaining very high-quality image generation. The official implementation is available at https://github.com/gojasper/flash-diffusion.","sentences":["In this paper, we propose an efficient, fast, and versatile distillation method to accelerate the generation of pre-trained diffusion models: Flash Diffusion.","The method reaches state-of-the-art performances in terms of FID and CLIP-Score for few steps image generation on the COCO2014 and COCO2017 datasets, while requiring only several GPU hours of training and fewer trainable parameters than existing methods.","In addition to its efficiency, the versatility of the method is also exposed across several tasks such as text-to-image, inpainting, face-swapping, super-resolution and using different backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\alpha$), as well as adapters.","In all cases, the method allowed to reduce drastically the number of sampling steps while maintaining very high-quality image generation.","The official implementation is available at https://github.com/gojasper/flash-diffusion."],"url":"http://arxiv.org/abs/2406.02347v1","category":"cs.CV"}
{"created":"2024-06-04 14:21:56","title":"Noninvasive magnetic detection of 2D van der Waals room-temperature ferromagnet Fe3GaTe2 using divacancy spins in SiC","abstract":"Room-temperature (RT) two-dimensional (2D) van der Waals (vdW) ferromagnets hold immense promise for next-generation spintronic devices for information storage and processing. To achieve high-density energy-efficient spintronic devices, it is essential to understand local magnetic properties of RT 2D vdW magnets. In this work, we realize noninvasive in situ magnetic detection in vdW-layered ferromagnet Fe3GaTe2 using divacancy spins quantum sensor in silicon carbide (SiC) at RT. The structural features and magnetic properties of the Fe3GaTe2 are characterized utilizing Raman spectrum, magnetization and magneto-transport measurements. Further detailed analysis of temperature- and magnetic field-dependent optically detected magnetic resonances of the PL6 divacancy near the Fe3GaTe2 reveal that, the Curie temperature (Tc) of Fe3GaTe2 is ~360K, and the magnetization increases with external magnetic fields. Additionally, spin relaxometry technology is employed to probe the magnetic fluctuations of Fe3GaTe2, revealing a peak in the spin relaxation rate around Tc. These experiments give insights into the intriguing local magnetic properties of 2D vdW RT ferromagnet Fe3GaTe2 and pave the way for the application of SiC quantum sensors in noninvasive in situ magnetic detection of related 2D vdW magnets.","sentences":["Room-temperature (RT) two-dimensional (2D) van der Waals (vdW) ferromagnets hold immense promise for next-generation spintronic devices for information storage and processing.","To achieve high-density energy-efficient spintronic devices, it is essential to understand local magnetic properties of RT 2D vdW magnets.","In this work, we realize noninvasive in situ magnetic detection in vdW-layered ferromagnet Fe3GaTe2 using divacancy spins quantum sensor in silicon carbide (SiC) at RT.","The structural features and magnetic properties of the Fe3GaTe2 are characterized utilizing Raman spectrum, magnetization and magneto-transport measurements.","Further detailed analysis of temperature- and magnetic field-dependent optically detected magnetic resonances of the PL6 divacancy near the Fe3GaTe2 reveal that, the Curie temperature (Tc) of Fe3GaTe2 is ~360K, and the magnetization increases with external magnetic fields.","Additionally, spin relaxometry technology is employed to probe the magnetic fluctuations of Fe3GaTe2, revealing a peak in the spin relaxation rate around Tc.","These experiments give insights into the intriguing local magnetic properties of 2D vdW RT ferromagnet Fe3GaTe2 and pave the way for the application of SiC quantum sensors in noninvasive in situ magnetic detection of related 2D vdW magnets."],"url":"http://arxiv.org/abs/2406.02346v1","category":"quant-ph"}
{"created":"2024-06-04 14:21:41","title":"Progressive Confident Masking Attention Network for Audio-Visual Segmentation","abstract":"Audio and visual signals typically occur simultaneously, and humans possess an innate ability to correlate and synchronize information from these two modalities. Recently, a challenging problem known as Audio-Visual Segmentation (AVS) has emerged, intending to produce segmentation maps for sounding objects within a scene. However, the methods proposed so far have not sufficiently integrated audio and visual information, and the computational costs have been extremely high. Additionally, the outputs of different stages have not been fully utilized. To facilitate this research, we introduce a novel Progressive Confident Masking Attention Network (PMCANet). It leverages attention mechanisms to uncover the intrinsic correlations between audio signals and visual frames. Furthermore, we design an efficient and effective cross-attention module to enhance semantic perception by selecting query tokens. This selection is determined through confidence-driven units based on the network's multi-stage predictive outputs. Experiments demonstrate that our network outperforms other AVS methods while requiring less computational resources.","sentences":["Audio and visual signals typically occur simultaneously, and humans possess an innate ability to correlate and synchronize information from these two modalities.","Recently, a challenging problem known as Audio-Visual Segmentation (AVS) has emerged, intending to produce segmentation maps for sounding objects within a scene.","However, the methods proposed so far have not sufficiently integrated audio and visual information, and the computational costs have been extremely high.","Additionally, the outputs of different stages have not been fully utilized.","To facilitate this research, we introduce a novel Progressive Confident Masking Attention Network (PMCANet).","It leverages attention mechanisms to uncover the intrinsic correlations between audio signals and visual frames.","Furthermore, we design an efficient and effective cross-attention module to enhance semantic perception by selecting query tokens.","This selection is determined through confidence-driven units based on the network's multi-stage predictive outputs.","Experiments demonstrate that our network outperforms other AVS methods while requiring less computational resources."],"url":"http://arxiv.org/abs/2406.02345v1","category":"cs.CV"}
{"created":"2024-06-04 14:20:10","title":"Incorporating Navigation Context into Inland Vessel Trajectory Prediction: A Gaussian Mixture Model and Transformer Approach","abstract":"Using data sources beyond the Automatic Identification System to represent the context a vessel is navigating in and consequently improve situation awareness is still rare in machine learning approaches to vessel trajectory prediction (VTP). In inland shipping, where vessel movement is constrained within fairways, navigational context information is indispensable. In this contribution targeting inland VTP, Gaussian Mixture Models (GMMs) are applied, on a fused dataset of AIS and discharge measurements, to generate multi-modal distribution curves, capturing typical lateral vessel positioning in the fairway and dislocation speeds along the waterway. By sampling the probability density curves of the GMMs, feature vectors are derived which are used, together with spatio-temporal vessel features and fairway geometries, as input to a VTP transformer model. The incorporation of these distribution features of both the current and forthcoming navigation context improves prediction accuracy. The superiority of the model over a previously proposed transformer model for inland VTP is shown. The novelty lies in the provision of preprocessed, statistics-based features representing the conditioned spatial context, rather than relying on the model to extract relevant features for the VTP task from contextual data. Oversimplification of the complexity of inland navigation patterns by assuming a single typical route or selecting specific clusters prior to model application is avoided by giving the model access to the entire distribution information. The methodology's generalizability is demonstrated through the usage of data of 3 distinct river sections. It can be integrated into an interaction-aware prediction framework, where insights into the positioning of the actual vessel behavior in the overall distribution at the current location and discharge can enhance trajectory prediction accuracy.","sentences":["Using data sources beyond the Automatic Identification System to represent the context a vessel is navigating in and consequently improve situation awareness is still rare in machine learning approaches to vessel trajectory prediction (VTP).","In inland shipping, where vessel movement is constrained within fairways, navigational context information is indispensable.","In this contribution targeting inland VTP, Gaussian Mixture Models (GMMs) are applied, on a fused dataset of AIS and discharge measurements, to generate multi-modal distribution curves, capturing typical lateral vessel positioning in the fairway and dislocation speeds along the waterway.","By sampling the probability density curves of the GMMs, feature vectors are derived which are used, together with spatio-temporal vessel features and fairway geometries, as input to a VTP transformer model.","The incorporation of these distribution features of both the current and forthcoming navigation context improves prediction accuracy.","The superiority of the model over a previously proposed transformer model for inland VTP is shown.","The novelty lies in the provision of preprocessed, statistics-based features representing the conditioned spatial context, rather than relying on the model to extract relevant features for the VTP task from contextual data.","Oversimplification of the complexity of inland navigation patterns by assuming a single typical route or selecting specific clusters prior to model application is avoided by giving the model access to the entire distribution information.","The methodology's generalizability is demonstrated through the usage of data of 3 distinct river sections.","It can be integrated into an interaction-aware prediction framework, where insights into the positioning of the actual vessel behavior in the overall distribution at the current location and discharge can enhance trajectory prediction accuracy."],"url":"http://arxiv.org/abs/2406.02344v1","category":"cs.LG"}
{"created":"2024-06-04 14:09:36","title":"Linguistic Fingerprint in Transformer Models: How Language Variation Influences Parameter Selection in Irony Detection","abstract":"This paper explores the correlation between linguistic diversity, sentiment analysis and transformer model architectures. We aim to investigate how different English variations impact transformer-based models for irony detection. To conduct our study, we used the EPIC corpus to extract five diverse English variation-specific datasets and applied the KEN pruning algorithm on five different architectures. Our results reveal several similarities between optimal subnetworks, which provide insights into the linguistic variations that share strong resemblances and those that exhibit greater dissimilarities. We discovered that optimal subnetworks across models share at least 60% of their parameters, emphasizing the significance of parameter values in capturing and interpreting linguistic variations. This study highlights the inherent structural similarities between models trained on different variants of the same language and also the critical role of parameter values in capturing these nuances.","sentences":["This paper explores the correlation between linguistic diversity, sentiment analysis and transformer model architectures.","We aim to investigate how different English variations impact transformer-based models for irony detection.","To conduct our study, we used the EPIC corpus to extract five diverse English variation-specific datasets and applied the KEN pruning algorithm on five different architectures.","Our results reveal several similarities between optimal subnetworks, which provide insights into the linguistic variations that share strong resemblances and those that exhibit greater dissimilarities.","We discovered that optimal subnetworks across models share at least 60% of their parameters, emphasizing the significance of parameter values in capturing and interpreting linguistic variations.","This study highlights the inherent structural similarities between models trained on different variants of the same language and also the critical role of parameter values in capturing these nuances."],"url":"http://arxiv.org/abs/2406.02338v1","category":"cs.CL"}
{"created":"2024-06-04 14:08:19","title":"Laplacian Renormalization Group: An introduction to heterogeneous coarse-graining","abstract":"The renormalization group (RG) constitutes a fundamental framework in modern theoretical physics. It allows the study of many systems showing states with large-scale correlations and their classification in a relatively small set of universality classes. RG is the most powerful tool for investigating organizational scales within dynamic systems. However, the application of RG techniques to complex networks has presented significant challenges, primarily due to the intricate interplay of correlations on multiple scales. Existing approaches have relied on hypotheses involving hidden geometries and based on embedding complex networks into hidden metric spaces. Here, we present a practical overview of the recently introduced Laplacian Renormalization Group for heterogeneous networks. First, we present a brief overview that justifies the use of the Laplacian as a natural extension for well-known field theories to analyze spatial disorder. We then draw an analogy to traditional real-space renormalization group procedures, explaining how the LRG generalizes the concept of \"Kadanoff supernodes\" as block nodes that span multiple scales. These supernodes help mitigate the effects of cross-scale correlations due to small-world properties. Additionally, we rigorously define the LRG procedure in momentum space in the spirit of Wilson RG. Finally, we show different analyses for the evolution of network properties along the LRG flow following structural changes when the network is properly reduced.","sentences":["The renormalization group (RG) constitutes a fundamental framework in modern theoretical physics.","It allows the study of many systems showing states with large-scale correlations and their classification in a relatively small set of universality classes.","RG is the most powerful tool for investigating organizational scales within dynamic systems.","However, the application of RG techniques to complex networks has presented significant challenges, primarily due to the intricate interplay of correlations on multiple scales.","Existing approaches have relied on hypotheses involving hidden geometries and based on embedding complex networks into hidden metric spaces.","Here, we present a practical overview of the recently introduced Laplacian Renormalization Group for heterogeneous networks.","First, we present a brief overview that justifies the use of the Laplacian as a natural extension for well-known field theories to analyze spatial disorder.","We then draw an analogy to traditional real-space renormalization group procedures, explaining how the LRG generalizes the concept of \"Kadanoff supernodes\" as block nodes that span multiple scales.","These supernodes help mitigate the effects of cross-scale correlations due to small-world properties.","Additionally, we rigorously define the LRG procedure in momentum space in the spirit of Wilson RG.","Finally, we show different analyses for the evolution of network properties along the LRG flow following structural changes when the network is properly reduced."],"url":"http://arxiv.org/abs/2406.02337v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-04 14:04:34","title":"$\\textit{Kilonova Seekers}$: the GOTO project for real-time citizen science in time-domain astrophysics","abstract":"Time-domain astrophysics continues to grow rapidly, with the inception of new surveys drastically increasing data volumes. Democratised, distributed approaches to training sets for machine learning classifiers are crucial to make the most of this torrent of discovery -- with citizen science approaches proving effective at meeting these requirements. In this paper, we describe the creation of and the initial results from the $\\textit{Kilonova Seekers}$ citizen science project, built to find transient phenomena from the GOTO telescopes in near real-time. $\\textit{Kilonova Seekers}$ launched in July 2023 and received over 600,000 classifications from approximately 2,000 volunteers over the course of the LIGO-Virgo-KAGRA O4a observing run. During this time, the project has yielded 20 discoveries, generated a `gold-standard' training set of 17,682 detections for augmenting deep-learned classifiers, and measured the performance and biases of Zooniverse volunteers on real-bogus classification. This project will continue throughout the lifetime of GOTO, pushing candidates at ever-greater cadence, and directly facilitate the next-generation classification algorithms currently in development.","sentences":["Time-domain astrophysics continues to grow rapidly, with the inception of new surveys drastically increasing data volumes.","Democratised, distributed approaches to training sets for machine learning classifiers are crucial to make the most of this torrent of discovery -- with citizen science approaches proving effective at meeting these requirements.","In this paper, we describe the creation of and the initial results from the $\\textit{Kilonova Seekers}$ citizen science project, built to find transient phenomena from the GOTO telescopes in near real-time.","$\\textit{Kilonova Seekers}$ launched in July 2023 and received over 600,000 classifications from approximately 2,000 volunteers over the course of the LIGO-Virgo-KAGRA O4a observing run.","During this time, the project has yielded 20 discoveries, generated a `gold-standard' training set of 17,682 detections for augmenting deep-learned classifiers, and measured the performance and biases of Zooniverse volunteers on real-bogus classification.","This project will continue throughout the lifetime of GOTO, pushing candidates at ever-greater cadence, and directly facilitate the next-generation classification algorithms currently in development."],"url":"http://arxiv.org/abs/2406.02334v1","category":"astro-ph.IM"}
{"created":"2024-06-04 14:01:03","title":"Towards Neural Architecture Search for Transfer Learning in 6G Networks","abstract":"The future 6G network is envisioned to be AI-native, and as such, ML models will be pervasive in support of optimizing performance, reducing energy consumption, and in coping with increasing complexity and heterogeneity. A key challenge is automating the process of finding optimal model architectures satisfying stringent requirements stemming from varying tasks, dynamicity and available resources in the infrastructure and deployment positions. In this paper, we describe and review the state-of-the-art in Neural Architecture Search and Transfer Learning and their applicability in networking. Further, we identify open research challenges and set directions with a specific focus on three main requirements with elements unique to the future network, namely combining NAS and TL, multi-objective search, and tabular data. Finally, we outline and discuss both near-term and long-term work ahead.","sentences":["The future 6G network is envisioned to be AI-native, and as such, ML models will be pervasive in support of optimizing performance, reducing energy consumption, and in coping with increasing complexity and heterogeneity.","A key challenge is automating the process of finding optimal model architectures satisfying stringent requirements stemming from varying tasks, dynamicity and available resources in the infrastructure and deployment positions.","In this paper, we describe and review the state-of-the-art in Neural Architecture Search and Transfer Learning and their applicability in networking.","Further, we identify open research challenges and set directions with a specific focus on three main requirements with elements unique to the future network, namely combining NAS and TL, multi-objective search, and tabular data.","Finally, we outline and discuss both near-term and long-term work ahead."],"url":"http://arxiv.org/abs/2406.02333v1","category":"cs.NI"}
{"created":"2024-06-04 14:00:25","title":"Extended Mind Transformers","abstract":"Pre-trained language models demonstrate general intelligence and common sense, but long inputs quickly become a bottleneck for memorizing information at inference time. We resurface a simple method, Memorizing Transformers (Wu et al., 2022), that gives the model access to a bank of pre-computed memories. We show that it is possible to fix many of the shortcomings of the original method, such as the need for fine-tuning, by critically assessing how positional encodings should be updated for the keys and values retrieved. This intuitive method uses the model's own key/query system to select and attend to the most relevant memories at each generation step, rather than using external embeddings. We demonstrate the importance of external information being retrieved in a majority of decoder layers, contrary to previous work. We open source a new counterfactual long-range retrieval benchmark, and show that Extended Mind Transformers outperform today's state of the art by 6% on average.","sentences":["Pre-trained language models demonstrate general intelligence and common sense, but long inputs quickly become a bottleneck for memorizing information at inference time.","We resurface a simple method, Memorizing Transformers (Wu et al., 2022), that gives the model access to a bank of pre-computed memories.","We show that it is possible to fix many of the shortcomings of the original method, such as the need for fine-tuning, by critically assessing how positional encodings should be updated for the keys and values retrieved.","This intuitive method uses the model's own key/query system to select and attend to the most relevant memories at each generation step, rather than using external embeddings.","We demonstrate the importance of external information being retrieved in a majority of decoder layers, contrary to previous work.","We open source a new counterfactual long-range retrieval benchmark, and show that Extended Mind Transformers outperform today's state of the art by 6% on average."],"url":"http://arxiv.org/abs/2406.02332v1","category":"cs.LG"}
{"created":"2024-06-04 13:58:28","title":"SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models","abstract":"In this study, we propose a simple and efficient Non-Autoregressive (NAR) text-to-speech (TTS) system based on diffusion, named SimpleSpeech. Its simpleness shows in three aspects: (1) It can be trained on the speech-only dataset, without any alignment information; (2) It directly takes plain text as input and generates speech through an NAR way; (3) It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion. More specifically, we propose a novel speech codec model (SQ-Codec) with scalar quantization, SQ-Codec effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space. Benefits from SQ-Codec, we apply a novel transformer diffusion model in the scalar latent space of SQ-Codec. We train SimpleSpeech on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability. Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement. Demos are released.","sentences":["In this study, we propose a simple and efficient Non-Autoregressive (NAR) text-to-speech (TTS) system based on diffusion, named SimpleSpeech.","Its simpleness shows in three aspects: (1) It can be trained on the speech-only dataset, without any alignment information; (2) It directly takes plain text as input and generates speech through an NAR way; (3) It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion.","More specifically, we propose a novel speech codec model (SQ-Codec) with scalar quantization, SQ-Codec effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space.","Benefits from SQ-Codec, we apply a novel transformer diffusion model in the scalar latent space of SQ-Codec.","We train SimpleSpeech on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability.","Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement.","Demos are released."],"url":"http://arxiv.org/abs/2406.02328v1","category":"cs.SD"}
{"created":"2024-06-04 13:57:26","title":"Jet formation model from accretion disks of electron-ion-photon gas","abstract":"The problem of Astrophysical Jet formation from relativistic accretion disks through the establishment of relativistic disk-powerful jet equilibrium structure is studied applying the Beltrami-Bernoulli equilibrium approach of Shatashvili & Yoshida 2011; Arshilava et al. 2019. Accretion disk is weakly magnetized consisting of fully ionized relativistic electron-ion plasma and photon gas strongly coupled to electrons due to Thompson Scattering. %hence, making the behavior of photon gas similar to that of \"a charged fluid\". Analysis is based on the generalized Shakura-Sunyaev $\\alpha $-turbulent dissipation model for local viscosity (being the main source of accretion), in which the contributions from both the photon and ion gases are taken into account. Ignoring the self-gravitation in the disk we constructed the analytical self-similar solutions for the equilibrium relativistic disk-jet structure characteristic parameters in the field of gravitating central compact object for the force-free condition. It is shown, that the magnetic field energy in the Jet is several orders greater compared to that of accretion disk, while jet-outflow is locally Super-Alfv\\'enic with local {\\it Plasma-beta} $< 1$ near the jet-axis. The derived solutions can be used to analyze the astrophysical jets observed in binary systems during the star formation process linking the jet properties with the parameters of relativistic accretion disks of electron-ion-photon gas.","sentences":["The problem of Astrophysical Jet formation from relativistic accretion disks through the establishment of relativistic disk-powerful jet equilibrium structure is studied applying the Beltrami-Bernoulli equilibrium approach of Shatashvili & Yoshida 2011; Arshilava et al. 2019.","Accretion disk is weakly magnetized consisting of fully ionized relativistic electron-ion plasma and photon gas strongly coupled to electrons due to Thompson Scattering.","%hence, making the behavior of photon gas similar to that of \"a charged fluid\".","Analysis is based on the generalized Shakura-Sunyaev $\\alpha $-turbulent dissipation model for local viscosity (being the main source of accretion), in which the contributions from both the photon and ion gases are taken into account.","Ignoring the self-gravitation in the disk we constructed the analytical self-similar solutions for the equilibrium relativistic disk-jet structure characteristic parameters in the field of gravitating central compact object for the force-free condition.","It is shown, that the magnetic field energy in the Jet is several orders greater compared to that of accretion disk, while jet-outflow is locally Super-Alfv\\'enic with local {\\it Plasma-beta} $< 1$ near the jet-axis.","The derived solutions can be used to analyze the astrophysical jets observed in binary systems during the star formation process linking the jet properties with the parameters of relativistic accretion disks of electron-ion-photon gas."],"url":"http://arxiv.org/abs/2406.02326v1","category":"astro-ph.HE"}
{"created":"2024-06-04 13:57:22","title":"Technical Language Processing for Telecommunications Specifications","abstract":"Large Language Models (LLMs) are continuously being applied in a more diverse set of contexts. At their current state, however, even state-of-the-art LLMs such as Generative Pre-Trained Transformer 4 (GTP-4) have challenges when extracting information from real-world technical documentation without a heavy preprocessing. One such area with real-world technical documentation is telecommunications engineering, which could greatly benefit from domain-specific LLMs. The unique format and overall structure of telecommunications internal specifications differs greatly from standard English and thus it is evident that the application of out-of-the-box Natural Language Processing (NLP) tools is not a viable option. In this article, we outline the limitations of out-of-the-box NLP tools for processing technical information generated by telecommunications experts, and expand the concept of Technical Language Processing (TLP) to the telecommunication domain. Additionally, we explore the effect of domain-specific LLMs in the work of Specification Engineers, emphasizing the potential benefits of adopting domain-specific LLMs to speed up the training of experts in different telecommunications fields.","sentences":["Large Language Models (LLMs) are continuously being applied in a more diverse set of contexts.","At their current state, however, even state-of-the-art LLMs such as Generative Pre-Trained Transformer 4 (GTP-4) have challenges when extracting information from real-world technical documentation without a heavy preprocessing.","One such area with real-world technical documentation is telecommunications engineering, which could greatly benefit from domain-specific LLMs.","The unique format and overall structure of telecommunications internal specifications differs greatly from standard English and thus it is evident that the application of out-of-the-box Natural Language Processing (NLP) tools is not a viable option.","In this article, we outline the limitations of out-of-the-box NLP tools for processing technical information generated by telecommunications experts, and expand the concept of Technical Language Processing (TLP) to the telecommunication domain.","Additionally, we explore the effect of domain-specific LLMs in the work of Specification Engineers, emphasizing the potential benefits of adopting domain-specific LLMs to speed up the training of experts in different telecommunications fields."],"url":"http://arxiv.org/abs/2406.02325v1","category":"cs.CL"}
{"created":"2024-06-04 13:56:20","title":"V-SeMo: a digital learning environment for teaching general relativity with sector models","abstract":"The teaching of general relativity at the secondary school and lower undergraduate university levels is necessarily based on approaches with a restricted use of mathematics. An important aspect of teaching general relativity at this level are suitable learner activities. While a substantial number of such activities has been reported for studying the non-Euclidean geometry of curved surfaces, there are far fewer reports of activities that let learners actually study spacetimes and infer physical phenomena from their geometry. In this article we report on the digital learning environment V-SeMo that brings sector models (Zahn and Kraus 2014, arXiv:1405.0323) into an interactive web application. V-SeMo lets learners explore relativistic spacetimes by constructing geodesics and assessing curvature. We describe the didactic design and the user interface of V-SeMo, discuss the extended possibilities of virtual sector models compared to the paper models described previously, and present an activity on light deflection near neutron stars by way of example. We further report on an evaluation carried out with secondary school students. Results indicate a high learning effectiveness in both the V-SeMo-based and the paper-based versions of a teaching unit on relativistic light deflection. The teaching materials presented in this article are available online for teaching purposes at https://www.spacetimetravel.org .","sentences":["The teaching of general relativity at the secondary school and lower undergraduate university levels is necessarily based on approaches with a restricted use of mathematics.","An important aspect of teaching general relativity at this level are suitable learner activities.","While a substantial number of such activities has been reported for studying the non-Euclidean geometry of curved surfaces, there are far fewer reports of activities that let learners actually study spacetimes and infer physical phenomena from their geometry.","In this article we report on the digital learning environment V-SeMo that brings sector models (Zahn and Kraus 2014, arXiv:1405.0323) into an interactive web application.","V-SeMo lets learners explore relativistic spacetimes by constructing geodesics and assessing curvature.","We describe the didactic design and the user interface of V-SeMo, discuss the extended possibilities of virtual sector models compared to the paper models described previously, and present an activity on light deflection near neutron stars by way of example.","We further report on an evaluation carried out with secondary school students.","Results indicate a high learning effectiveness in both the V-SeMo-based and the paper-based versions of a teaching unit on relativistic light deflection.","The teaching materials presented in this article are available online for teaching purposes at https://www.spacetimetravel.org ."],"url":"http://arxiv.org/abs/2406.02324v1","category":"physics.ed-ph"}
{"created":"2024-06-04 13:52:42","title":"A Survey of Transformer Enabled Time Series Synthesis","abstract":"Generative AI has received much attention in the image and language domains, with the transformer neural network continuing to dominate the state of the art. Application of these models to time series generation is less explored, however, and is of great utility to machine learning, privacy preservation, and explainability research. The present survey identifies this gap at the intersection of the transformer, generative AI, and time series data, and reviews works in this sparsely populated subdomain. The reviewed works show great variety in approach, and have not yet converged on a conclusive answer to the problems the domain poses. GANs, diffusion models, state space models, and autoencoders were all encountered alongside or surrounding the transformers which originally motivated the survey. While too open a domain to offer conclusive insights, the works surveyed are quite suggestive, and several recommendations for best practice, and suggestions of valuable future work, are provided.","sentences":["Generative AI has received much attention in the image and language domains, with the transformer neural network continuing to dominate the state of the art.","Application of these models to time series generation is less explored, however, and is of great utility to machine learning, privacy preservation, and explainability research.","The present survey identifies this gap at the intersection of the transformer, generative AI, and time series data, and reviews works in this sparsely populated subdomain.","The reviewed works show great variety in approach, and have not yet converged on a conclusive answer to the problems the domain poses.","GANs, diffusion models, state space models, and autoencoders were all encountered alongside or surrounding the transformers which originally motivated the survey.","While too open a domain to offer conclusive insights, the works surveyed are quite suggestive, and several recommendations for best practice, and suggestions of valuable future work, are provided."],"url":"http://arxiv.org/abs/2406.02322v1","category":"cs.LG"}
{"created":"2024-06-04 13:51:08","title":"PeFAD: A Parameter-Efficient Federated Framework for Time Series Anomaly Detection","abstract":"With the proliferation of mobile sensing techniques, huge amounts of time series data are generated and accumulated in various domains, fueling plenty of real-world applications. In this setting, time series anomaly detection is practically important. It endeavors to identify deviant samples from the normal sample distribution in time series. Existing approaches generally assume that all the time series is available at a central location. However, we are witnessing the decentralized collection of time series due to the deployment of various edge devices. To bridge the gap between the decentralized time series data and the centralized anomaly detection algorithms, we propose a Parameter-efficient Federated Anomaly Detection framework named PeFAD with the increasing privacy concerns. PeFAD for the first time employs the pre-trained language model (PLM) as the body of the client's local model, which can benefit from its cross-modality knowledge transfer capability. To reduce the communication overhead and local model adaptation cost, we propose a parameter-efficient federated training module such that clients only need to fine-tune small-scale parameters and transmit them to the server for update. PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate the impact of neglected anomalies during training. A knowledge distillation operation on a synthetic privacy-preserving dataset that is shared by all the clients is also proposed to address the data heterogeneity issue across clients. We conduct extensive evaluations on four real datasets, where PeFAD outperforms existing state-of-the-art baselines by up to 28.74\\%.","sentences":["With the proliferation of mobile sensing techniques, huge amounts of time series data are generated and accumulated in various domains, fueling plenty of real-world applications.","In this setting, time series anomaly detection is practically important.","It endeavors to identify deviant samples from the normal sample distribution in time series.","Existing approaches generally assume that all the time series is available at a central location.","However, we are witnessing the decentralized collection of time series due to the deployment of various edge devices.","To bridge the gap between the decentralized time series data and the centralized anomaly detection algorithms, we propose a Parameter-efficient Federated Anomaly Detection framework named PeFAD with the increasing privacy concerns.","PeFAD for the first time employs the pre-trained language model (PLM) as the body of the client's local model, which can benefit from its cross-modality knowledge transfer capability.","To reduce the communication overhead and local model adaptation cost, we propose a parameter-efficient federated training module such that clients only need to fine-tune small-scale parameters and transmit them to the server for update.","PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate the impact of neglected anomalies during training.","A knowledge distillation operation on a synthetic privacy-preserving dataset that is shared by all the clients is also proposed to address the data heterogeneity issue across clients.","We conduct extensive evaluations on four real datasets, where PeFAD outperforms existing state-of-the-art baselines by up to 28.74\\%."],"url":"http://arxiv.org/abs/2406.02318v1","category":"cs.LG"}
{"created":"2024-06-04 13:45:35","title":"Generative Conditional Distributions by Neural (Entropic) Optimal Transport","abstract":"Learning conditional distributions is challenging because the desired outcome is not a single distribution but multiple distributions that correspond to multiple instances of the covariates. We introduce a novel neural entropic optimal transport method designed to effectively learn generative models of conditional distributions, particularly in scenarios characterized by limited sample sizes. Our method relies on the minimax training of two neural networks: a generative network parametrizing the inverse cumulative distribution functions of the conditional distributions and another network parametrizing the conditional Kantorovich potential. To prevent overfitting, we regularize the objective function by penalizing the Lipschitz constant of the network output. Our experiments on real-world datasets show the effectiveness of our algorithm compared to state-of-the-art conditional distribution learning techniques. Our implementation can be found at https://github.com/nguyenngocbaocmt02/GENTLE.","sentences":["Learning conditional distributions is challenging because the desired outcome is not a single distribution but multiple distributions that correspond to multiple instances of the covariates.","We introduce a novel neural entropic optimal transport method designed to effectively learn generative models of conditional distributions, particularly in scenarios characterized by limited sample sizes.","Our method relies on the minimax training of two neural networks: a generative network parametrizing the inverse cumulative distribution functions of the conditional distributions and another network parametrizing the conditional Kantorovich potential.","To prevent overfitting, we regularize the objective function by penalizing the Lipschitz constant of the network output.","Our experiments on real-world datasets show the effectiveness of our algorithm compared to state-of-the-art conditional distribution learning techniques.","Our implementation can be found at https://github.com/nguyenngocbaocmt02/GENTLE."],"url":"http://arxiv.org/abs/2406.02317v1","category":"cs.LG"}
{"created":"2024-06-04 13:44:39","title":"An Independence-promoting Loss for Music Generation with Language Models","abstract":"Music generation schemes using language modeling rely on a vocabulary of audio tokens, generally provided as codes in a discrete latent space learnt by an auto-encoder. Multi-stage quantizers are often employed to produce these tokens, therefore the decoding strategy used for token prediction must be adapted to account for multiple codebooks: either it should model the joint distribution over all codebooks, or fit the product of the codebook marginal distributions. Modelling the joint distribution requires a costly increase in the number of auto-regressive steps, while fitting the product of the marginals yields an inexact model unless the codebooks are mutually independent. In this work, we introduce an independence-promoting loss to regularize the auto-encoder used as the tokenizer in language models for music generation. The proposed loss is a proxy for mutual information based on the maximum mean discrepancy principle, applied in reproducible kernel Hilbert spaces. Our criterion is simple to implement and train, and it is generalizable to other multi-stream codecs. We show that it reduces the statistical dependence between codebooks during auto-encoding. This leads to an increase in the generated music quality when modelling the product of the marginal distributions, while generating audio much faster than the joint distribution model.","sentences":["Music generation schemes using language modeling rely on a vocabulary of audio tokens, generally provided as codes in a discrete latent space learnt by an auto-encoder.","Multi-stage quantizers are often employed to produce these tokens, therefore the decoding strategy used for token prediction must be adapted to account for multiple codebooks: either it should model the joint distribution over all codebooks, or fit the product of the codebook marginal distributions.","Modelling the joint distribution requires a costly increase in the number of auto-regressive steps, while fitting the product of the marginals yields an inexact model unless the codebooks are mutually independent.","In this work, we introduce an independence-promoting loss to regularize the auto-encoder used as the tokenizer in language models for music generation.","The proposed loss is a proxy for mutual information based on the maximum mean discrepancy principle, applied in reproducible kernel Hilbert spaces.","Our criterion is simple to implement and train, and it is generalizable to other multi-stream codecs.","We show that it reduces the statistical dependence between codebooks during auto-encoding.","This leads to an increase in the generated music quality when modelling the product of the marginal distributions, while generating audio much faster than the joint distribution model."],"url":"http://arxiv.org/abs/2406.02315v1","category":"cs.SD"}
{"created":"2024-06-04 13:43:53","title":"Integrating Sustainability in Controlling and Accounting Practices: A Critical Review and Implications for Competences in German Vocational Business Education","abstract":"Sustainability in accounting and controlling has traditionally been understood in terms of securing the long-term existence of companies. However, with the introduction of integrated non-financial reporting, sustainability, as per the triple bottom line model, is increasingly being discussed as a component of accounting and controlling. Yet, integration primarily occurs in separate sustainability management and controlling departments. Moreover, the implementation of sustainability efforts requires suitably qualified employees, who drive the transition. The academic discourse surrounding sustainability in businesses in general, and in accounting and controlling specifically, is complex. It remains unclear to what extent sustainability has been integrated into accounting and controlling, and what competencies employees need to manage this transformation. These questions will be critically analyzed in this structured literature review of 79 publications. The results provide insights into a) how companies conceptualize sustainability, b) whether and how they integrate it into their value creation processes, and c) the relevance of accounting and controlling for these developments. To contextualize the role of employees, the competency requirements within companies will be analyzed to enable employees in accounting and controlling to engage effectively in sustainability-oriented activities. Specifically, implications for changes in curricula with a focus on accounting and controlling are derived.","sentences":["Sustainability in accounting and controlling has traditionally been understood in terms of securing the long-term existence of companies.","However, with the introduction of integrated non-financial reporting, sustainability, as per the triple bottom line model, is increasingly being discussed as a component of accounting and controlling.","Yet, integration primarily occurs in separate sustainability management and controlling departments.","Moreover, the implementation of sustainability efforts requires suitably qualified employees, who drive the transition.","The academic discourse surrounding sustainability in businesses in general, and in accounting and controlling specifically, is complex.","It remains unclear to what extent sustainability has been integrated into accounting and controlling, and what competencies employees need to manage this transformation.","These questions will be critically analyzed in this structured literature review of 79 publications.","The results provide insights into a) how companies conceptualize sustainability, b) whether and how they integrate it into their value creation processes, and c) the relevance of accounting and controlling for these developments.","To contextualize the role of employees, the competency requirements within companies will be analyzed to enable employees in accounting and controlling to engage effectively in sustainability-oriented activities.","Specifically, implications for changes in curricula with a focus on accounting and controlling are derived."],"url":"http://arxiv.org/abs/2406.02314v1","category":"econ.GN"}
{"created":"2024-06-04 13:41:00","title":"Effects of Exponential Gaussian Distribution on (Double Sampling) Randomized Smoothing","abstract":"Randomized Smoothing (RS) is currently a scalable certified defense method providing robustness certification against adversarial examples. Although significant progress has been achieved in providing defenses against $\\ell_p$ adversaries, the interaction between the smoothing distribution and the robustness certification still remains vague. In this work, we comprehensively study the effect of two families of distributions, named Exponential Standard Gaussian (ESG) and Exponential General Gaussian (EGG) distributions, on Randomized Smoothing and Double Sampling Randomized Smoothing (DSRS). We derive an analytic formula for ESG's certified radius, which converges to the origin formula of RS as the dimension $d$ increases. Additionally, we prove that EGG can provide tighter constant factors than DSRS in providing $\\Omega(\\sqrt{d})$ lower bounds of $\\ell_2$ certified radius, and thus further addresses the curse of dimensionality in RS. Our experiments on real-world datasets confirm our theoretical analysis of the ESG distributions, that they provide almost the same certification under different exponents $\\eta$ for both RS and DSRS. In addition, EGG","sentences":["Randomized Smoothing (RS) is currently a scalable certified defense method providing robustness certification against adversarial examples.","Although significant progress has been achieved in providing defenses against $\\ell_p$ adversaries, the interaction between the smoothing distribution and the robustness certification still remains vague.","In this work, we comprehensively study the effect of two families of distributions, named Exponential Standard Gaussian (ESG) and Exponential General Gaussian (EGG) distributions, on Randomized Smoothing and Double Sampling Randomized Smoothing (DSRS).","We derive an analytic formula for ESG's certified radius, which converges to the origin formula of RS as the dimension $d$ increases.","Additionally, we prove that EGG can provide tighter constant factors than DSRS in providing $\\Omega(\\sqrt{d})$ lower bounds of $\\ell_2$ certified radius, and thus further addresses the curse of dimensionality in RS.","Our experiments on real-world datasets confirm our theoretical analysis of the ESG distributions, that they provide almost the same certification under different exponents $\\eta$ for both RS and DSRS.","In addition, EGG"],"url":"http://arxiv.org/abs/2406.02309v1","category":"cs.LG"}
{"created":"2024-06-04 13:39:58","title":"Gradient-free algorithm for saddle point problems under overparametrization","abstract":"This paper focuses on solving a stochastic saddle point problem (SPP) under an overparameterized regime for the case, when the gradient computation is impractical. As an intermediate step, we generalize Same-sample Stochastic Extra-gradient algorithm (Gorbunov et al., 2022) to a biased oracle and estimate novel convergence rates. As the result of the paper we introduce an algorithm, which uses gradient approximation instead of a gradient oracle. We also conduct an analysis to find the maximum admissible level of adversarial noise and the optimal number of iterations at which our algorithm can guarantee achieving the desired accuracy.","sentences":["This paper focuses on solving a stochastic saddle point problem (SPP) under an overparameterized regime for the case, when the gradient computation is impractical.","As an intermediate step, we generalize Same-sample Stochastic Extra-gradient algorithm (Gorbunov et al., 2022) to a biased oracle and estimate novel convergence rates.","As the result of the paper we introduce an algorithm, which uses gradient approximation instead of a gradient oracle.","We also conduct an analysis to find the maximum admissible level of adversarial noise and the optimal number of iterations at which our algorithm can guarantee achieving the desired accuracy."],"url":"http://arxiv.org/abs/2406.02308v1","category":"math.OC"}
{"created":"2024-06-04 13:38:33","title":"Bridging the micro-Hz gravitational wave gap via Doppler tracking with the Uranus Orbiter and Probe Mission: Massive black hole binaries, early universe signals and ultra-light dark matter","abstract":"With the recent announcement by NASA's Planetary Science and Astrobiology Decadal Survey 2023-2032, a priority flagship mission to the planet Uranus is anticipated. Here, we explore the prospects of using the mission's radio Doppler tracking equipment to detect gravitational waves (GWs) and other analogous signals related to dark matter (DM) over the duration of its interplanetary cruise. By employing a methodology to stack tracking data in combination with Monte-Carlo Markov-Chain parameter recovery tests, we show that the mission will be sensitive to GWs over the wide frequency range of $3\\times 10^{-9}$ Hz to $10^{-1}$ Hz, provided that tracking data is taken consistently over a large fraction of the cruise duration. Thus, the mission has the potential to fill the gap between pulsar timing and space-based-interferometry GW observatories. Within this assumption, we forecast the detection of $\\mathcal{\\mathcal{O}}(1 - 100)$ individual massive black hole binaries using two independent population models. Additionally, we determine the mission's sensitivity to both astrophysical and primordial stochastic gravitational wave backgrounds, as well as its capacity to test, or even confirm via detection, ultralight DM models. In all these cases, the tracking of the spacecraft over its interplanetary cruise would enable coverage of unexplored regions of parameter space, where signals from new phenomena in our Universe may be lurking.","sentences":["With the recent announcement by NASA's Planetary Science and Astrobiology Decadal Survey 2023-2032, a priority flagship mission to the planet Uranus is anticipated.","Here, we explore the prospects of using the mission's radio Doppler tracking equipment to detect gravitational waves (GWs) and other analogous signals related to dark matter (DM) over the duration of its interplanetary cruise.","By employing a methodology to stack tracking data in combination with Monte-Carlo Markov-Chain parameter recovery tests, we show that the mission will be sensitive to GWs over the wide frequency range of $3\\times 10^{-9}$ Hz to $10^{-1}$ Hz, provided that tracking data is taken consistently over a large fraction of the cruise duration.","Thus, the mission has the potential to fill the gap between pulsar timing and space-based-interferometry GW observatories.","Within this assumption, we forecast the detection of $\\mathcal{\\mathcal{O}}(1 - 100)$ individual massive black hole binaries using two independent population models.","Additionally, we determine the mission's sensitivity to both astrophysical and primordial stochastic gravitational wave backgrounds, as well as its capacity to test, or even confirm via detection, ultralight DM models.","In all these cases, the tracking of the spacecraft over its interplanetary cruise would enable coverage of unexplored regions of parameter space, where signals from new phenomena in our Universe may be lurking."],"url":"http://arxiv.org/abs/2406.02306v1","category":"astro-ph.HE"}
{"created":"2024-06-04 13:34:48","title":"Phenotype control and elimination of variables in Boolean networks","abstract":"We investigate how elimination of variables can affect the asymptotic dynamics and phenotype control of Boolean networks. In particular, we look at the impact on minimal trap spaces, and identify a structural condition that guarantees their preservation. We examine the possible effects of variable elimination under three of the most popular approaches to control (attractor-based control, value propagation and control of minimal trap spaces), and under different update schemes (synchronous, asynchronous, generalized asynchronous). We provide some insights on the application of reduction, and an ample inventory of examples and counterexamples.","sentences":["We investigate how elimination of variables can affect the asymptotic dynamics and phenotype control of Boolean networks.","In particular, we look at the impact on minimal trap spaces, and identify a structural condition that guarantees their preservation.","We examine the possible effects of variable elimination under three of the most popular approaches to control (attractor-based control, value propagation and control of minimal trap spaces), and under different update schemes (synchronous, asynchronous, generalized asynchronous).","We provide some insights on the application of reduction, and an ample inventory of examples and counterexamples."],"url":"http://arxiv.org/abs/2406.02304v1","category":"cs.DM"}
{"created":"2024-06-04 13:31:57","title":"Towards AI-Assisted Sustainable Adaptive Video Streaming Systems: Tutorial and Survey","abstract":"Improvements in networking technologies and the steadily increasing numbers of users, as well as the shift from traditional broadcasting to streaming content over the Internet, have made video applications (e.g., live and Video-on-Demand (VoD)) predominant sources of traffic. Recent advances in Artificial Intelligence (AI) and its widespread application in various academic and industrial fields have focused on designing and implementing a variety of video compression and content delivery techniques to improve user Quality of Experience (QoE). However, providing high QoE services results in more energy consumption and carbon footprint across the service delivery path, extending from the end user's device through the network and service infrastructure (e.g., cloud providers). Despite the importance of energy efficiency in video streaming, there is a lack of comprehensive surveys covering state-of-the-art AI techniques and their applications throughout the video streaming lifecycle. Existing surveys typically focus on specific parts, such as video encoding, delivery networks, playback, or quality assessment, without providing a holistic view of the entire lifecycle and its impact on energy consumption and QoE. Motivated by this research gap, this survey provides a comprehensive overview of the video streaming lifecycle, content delivery, energy and Video Quality Assessment (VQA) metrics and models, and AI techniques employed in video streaming. In addition, it conducts an in-depth state-of-the-art analysis focused on AI-driven approaches to enhance the energy efficiency of end-to-end aspects of video streaming systems (i.e., encoding, delivery network, playback, and VQA approaches). Finally, it discusses prospective research directions for developing AI-assisted energy-aware video streaming systems.","sentences":["Improvements in networking technologies and the steadily increasing numbers of users, as well as the shift from traditional broadcasting to streaming content over the Internet, have made video applications (e.g., live and Video-on-Demand (VoD)) predominant sources of traffic.","Recent advances in Artificial Intelligence (AI) and its widespread application in various academic and industrial fields have focused on designing and implementing a variety of video compression and content delivery techniques to improve user Quality of Experience (QoE).","However, providing high QoE services results in more energy consumption and carbon footprint across the service delivery path, extending from the end user's device through the network and service infrastructure (e.g., cloud providers).","Despite the importance of energy efficiency in video streaming, there is a lack of comprehensive surveys covering state-of-the-art AI techniques and their applications throughout the video streaming lifecycle.","Existing surveys typically focus on specific parts, such as video encoding, delivery networks, playback, or quality assessment, without providing a holistic view of the entire lifecycle and its impact on energy consumption and QoE. Motivated by this research gap, this survey provides a comprehensive overview of the video streaming lifecycle, content delivery, energy and Video Quality Assessment (VQA) metrics and models, and AI techniques employed in video streaming.","In addition, it conducts an in-depth state-of-the-art analysis focused on AI-driven approaches to enhance the energy efficiency of end-to-end aspects of video streaming systems (i.e., encoding, delivery network, playback, and VQA approaches).","Finally, it discusses prospective research directions for developing AI-assisted energy-aware video streaming systems."],"url":"http://arxiv.org/abs/2406.02302v1","category":"cs.MM"}
{"created":"2024-06-04 13:26:47","title":"On the structure of Kauffman bracket skein algebra of a surface","abstract":"Suppose $R$ is a commutative ring with identity and a fixed invertible element $q^{\\frac{1}{2}}$ such that $q+q^{-1}$ is invertible.   For an oriented surface $\\Sigma$, let $\\mathcal{S}(\\Sigma;R)$ denote the Kauffman bracket skein algebra of $\\Sigma$ over $R$. It is shown that to each embedded graph $G\\subset\\Sigma$ satisfying that $\\Sigma\\setminus G$ is homeomorphic to a disk and some other mild conditions, one can associate a generating set for $\\mathcal{S}(\\Sigma;R)$, and the ideal of defining relations is generated by relations of degree at most $6$ supported by certain small subsurfaces.","sentences":["Suppose $R$ is a commutative ring with identity and a fixed invertible element $q^{\\frac{1}{2}}$ such that $q+q^{-1}$ is invertible.   ","For an oriented surface $\\Sigma$, let $\\mathcal{S}(\\Sigma;R)$ denote the Kauffman bracket skein algebra of $\\Sigma$ over $R$. It is shown that to each embedded graph $G\\subset\\Sigma$ satisfying that $\\Sigma\\setminus G$ is homeomorphic to a disk and some other mild conditions, one can associate a generating set for $\\mathcal{S}(\\Sigma;R)$, and the ideal of defining relations is generated by relations of degree at most $6$ supported by certain small subsurfaces."],"url":"http://arxiv.org/abs/2406.02299v1","category":"math.GT"}
{"created":"2024-06-04 13:18:14","title":"Optimal Stock Portfolio Selection with a Multivariate Hidden Markov Model","abstract":"The underlying market trends that drive stock price fluctuations are often referred to in terms of bull and bear markets. Optimal stock portfolio selection methods need to take into account these market trends; however, the bull and bear market states tend to be unobserved and can only be assigned retrospectively. We fit a linked hidden Markov model (LHMM) to relative stock price changes for S&P 500 stocks from 2011--2016 based on weekly closing values. The LHMM consists of a multivariate state process whose individual components correspond to HMMs for each of the 12 sectors of the S\\&P 500 stocks. The state processes are linked using a Gaussian copula so that the states of the component chains are correlated at any given time point. The LHMM allows us to capture more heterogeneity in the underlying market dynamics for each sector. In this study, stock performances are evaluated in terms of capital gains using the LHMM by utilizing historical stock price data. Based on the fitted LHMM, optimal stock portfolios are constructed to maximize capital gain while balancing reward and risk. Under out-of-sample testing, the annual capital gain for the portfolios for 2016--2017 are calculated. Portfolios constructed using the LHMM are able to generate returns comparable to the S&P 500 index.","sentences":["The underlying market trends that drive stock price fluctuations are often referred to in terms of bull and bear markets.","Optimal stock portfolio selection methods need to take into account these market trends; however, the bull and bear market states tend to be unobserved and can only be assigned retrospectively.","We fit a linked hidden Markov model (LHMM) to relative stock price changes for S&P 500 stocks from 2011--2016 based on weekly closing values.","The LHMM consists of a multivariate state process whose individual components correspond to HMMs for each of the 12 sectors of the S\\&P 500 stocks.","The state processes are linked using a Gaussian copula so that the states of the component chains are correlated at any given time point.","The LHMM allows us to capture more heterogeneity in the underlying market dynamics for each sector.","In this study, stock performances are evaluated in terms of capital gains using the LHMM by utilizing historical stock price data.","Based on the fitted LHMM, optimal stock portfolios are constructed to maximize capital gain while balancing reward and risk.","Under out-of-sample testing, the annual capital gain for the portfolios for 2016--2017 are calculated.","Portfolios constructed using the LHMM are able to generate returns comparable to the S&P 500 index."],"url":"http://arxiv.org/abs/2406.02297v1","category":"stat.ME"}
{"created":"2024-06-04 13:16:34","title":"How to Explore with Belief: State Entropy Maximization in POMDPs","abstract":"Recent works have studied *state entropy maximization* in reinforcement learning, in which the agent's objective is to learn a policy inducing high entropy over states visitation (Hazan et al., 2019). They typically assume full observability of the state of the system, so that the entropy of the observations is maximized. In practice, the agent may only get *partial* observations, e.g., a robot perceiving the state of a physical space through proximity sensors and cameras. A significant mismatch between the entropy over observations and true states of the system can arise in those settings. In this paper, we address the problem of entropy maximization over the *true states* with a decision policy conditioned on partial observations *only*. The latter is a generalization of POMDPs, which is intractable in general. We develop a memory and computationally efficient *policy gradient* method to address a first-order relaxation of the objective defined on *belief* states, providing various formal characterizations of approximation gaps, the optimization landscape, and the *hallucination* problem. This paper aims to generalize state entropy maximization to more realistic domains that meet the challenges of applications.","sentences":["Recent works have studied *state entropy maximization* in reinforcement learning, in which the agent's objective is to learn a policy inducing high entropy over states visitation (Hazan et al., 2019).","They typically assume full observability of the state of the system, so that the entropy of the observations is maximized.","In practice, the agent may only get *partial* observations, e.g., a robot perceiving the state of a physical space through proximity sensors and cameras.","A significant mismatch between the entropy over observations and true states of the system can arise in those settings.","In this paper, we address the problem of entropy maximization over the *true states* with a decision policy conditioned on partial observations *only*.","The latter is a generalization of POMDPs, which is intractable in general.","We develop a memory and computationally efficient *policy gradient* method to address a first-order relaxation of the objective defined on *belief* states, providing various formal characterizations of approximation gaps, the optimization landscape, and the *hallucination* problem.","This paper aims to generalize state entropy maximization to more realistic domains that meet the challenges of applications."],"url":"http://arxiv.org/abs/2406.02295v1","category":"cs.LG"}
{"created":"2024-06-04 13:11:01","title":"An Axiomatic Approach to Loss Aggregation and an Adapted Aggregating Algorithm","abstract":"Supervised learning has gone beyond the expected risk minimization framework. Central to most of these developments is the introduction of more general aggregation functions for losses incurred by the learner. In this paper, we turn towards online learning under expert advice. Via easily justified assumptions we characterize a set of reasonable loss aggregation functions as quasi-sums. Based upon this insight, we suggest a variant of the Aggregating Algorithm tailored to these more general aggregation functions. This variant inherits most of the nice theoretical properties of the AA, such as recovery of Bayes' updating and a time-independent bound on quasi-sum regret. Finally, we argue that generalized aggregations express the attitude of the learner towards losses.","sentences":["Supervised learning has gone beyond the expected risk minimization framework.","Central to most of these developments is the introduction of more general aggregation functions for losses incurred by the learner.","In this paper, we turn towards online learning under expert advice.","Via easily justified assumptions we characterize a set of reasonable loss aggregation functions as quasi-sums.","Based upon this insight, we suggest a variant of the Aggregating Algorithm tailored to these more general aggregation functions.","This variant inherits most of the nice theoretical properties of the AA, such as recovery of Bayes' updating and a time-independent bound on quasi-sum regret.","Finally, we argue that generalized aggregations express the attitude of the learner towards losses."],"url":"http://arxiv.org/abs/2406.02292v1","category":"cs.LG"}
{"created":"2024-06-04 13:01:44","title":"OFDM-Based Active STAR-RIS-Aided Integrated Sensing and Communication Systems","abstract":"Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS), which consists of numerous passive elements, has recently emerged in wireless communication systems as a promising technology providing 360$^\\circ$ coverage and better performance. In our research, we introduce an active STAR-RIS (ASTARS)-aided integrated sensing and communications (ISAC) system designed to optimize the radar signal-to-noise ratio (SNR), enhancing detection and signal transmission efficiency. The introduction of an ISAC system aims to improve both communication efficiency and sensing capabilities. Also, we employ orthogonal frequency division multiplexing (OFDM) to address the frequency-selective fading problem. Furthermore, we evaluate the radar sensing capabilities by examining the range and velocity, and assess the performance through the mean-squared error (MSE) of their estimations. Our simulation results demonstrate that ASTARS outperforms STAR-RIS in our system configurations, and that the proposed optimization approach further enhances the system performance. Additionally, we confirm that an increase in the subcarrier spacing can reduce the transmission bit error rate (BER) under high-velocity conditions.","sentences":["Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS), which consists of numerous passive elements, has recently emerged in wireless communication systems as a promising technology providing 360$^\\circ$ coverage and better performance.","In our research, we introduce an active STAR-RIS (ASTARS)-aided integrated sensing and communications (ISAC) system designed to optimize the radar signal-to-noise ratio (SNR), enhancing detection and signal transmission efficiency.","The introduction of an ISAC system aims to improve both communication efficiency and sensing capabilities.","Also, we employ orthogonal frequency division multiplexing (OFDM) to address the frequency-selective fading problem.","Furthermore, we evaluate the radar sensing capabilities by examining the range and velocity, and assess the performance through the mean-squared error (MSE) of their estimations.","Our simulation results demonstrate that ASTARS outperforms STAR-RIS in our system configurations, and that the proposed optimization approach further enhances the system performance.","Additionally, we confirm that an increase in the subcarrier spacing can reduce the transmission bit error rate (BER) under high-velocity conditions."],"url":"http://arxiv.org/abs/2406.02289v1","category":"eess.SP"}
{"created":"2024-06-04 13:01:20","title":"Domain walls in light of Cosmic Microwave Background and Pulsar Timing Array data","abstract":"In this paper, we study the compatibility of biased domain wall scenarios with current gravitational wave data. We show that the Cosmic Microwave Background bounds on the fractional density of gravitational waves at the time of decoupling may only slightly improve on the constraints that result from requiring that domain walls never dominate the cosmic energy budget. We show that, despite this, the range of energy scales of the domain-wall forming phase transitions are already quite constricted, even if the networks decay early in cosmological history. We also show that, if domain walls are to provide an explanation to the stochastic gravitational wave background that was recently detected by pulsar timing arrays, they not only have to decay early in the radiation dominated era but also their energy density would have to be close to dominating the energy density of the universe, which would require some fine tuning of the parameters of the models.","sentences":["In this paper, we study the compatibility of biased domain wall scenarios with current gravitational wave data.","We show that the Cosmic Microwave Background bounds on the fractional density of gravitational waves at the time of decoupling may only slightly improve on the constraints that result from requiring that domain walls never dominate the cosmic energy budget.","We show that, despite this, the range of energy scales of the domain-wall forming phase transitions are already quite constricted, even if the networks decay early in cosmological history.","We also show that, if domain walls are to provide an explanation to the stochastic gravitational wave background that was recently detected by pulsar timing arrays, they not only have to decay early in the radiation dominated era but also their energy density would have to be close to dominating the energy density of the universe, which would require some fine tuning of the parameters of the models."],"url":"http://arxiv.org/abs/2406.02288v1","category":"gr-qc"}
{"created":"2024-06-04 12:52:15","title":"Jacob's ladders, logarithmic modification of the Hardy-Littlewood integral (1918), Titchmarsh's $\u03a9$-theorem (1928) and new point of contact with the Fermat-Wiles theorem","abstract":"In this paper we obtain two new points of contact between Jacob's ladders and Fermat-Wiles theorem. They are generated by a logarithmic modification of the Hardy-Littlewood integral. Furthermore, we present a kind of asymptotic laws of conservation for a set of areas connected with above mentioned modification of the Hardy-Littlewood integral.","sentences":["In this paper we obtain two new points of contact between Jacob's ladders and Fermat-Wiles theorem.","They are generated by a logarithmic modification of the Hardy-Littlewood integral.","Furthermore, we present a kind of asymptotic laws of conservation for a set of areas connected with above mentioned modification of the Hardy-Littlewood integral."],"url":"http://arxiv.org/abs/2406.02278v1","category":"math.NT"}
{"created":"2024-06-04 12:51:23","title":"Environment-induced Transitions in Many-body Quantum Teleportation","abstract":"Quantum teleportation is a phenomenon arising from entanglement, decisively distinguishing the classical and quantum worlds. The recent success of many-body quantum teleportation is even more surprising: although input information is initially dispersed and encoded into the many-body state in a complex way, the teleportation process can refocus this highly non-local information at the receiver's end. This success manifests intriguing capability of many-body systems in quantum information processing. Current studies indicate that information scrambling, a generic dynamic process in many-body systems, underlies the effectiveness of many-body quantum teleportation. However, this process is known to undergo a novel scrambling-dissipation transition in the presence of environments. How environments affect the quantum information processing capability of many-body systems calls for further investigation. In this work, we study many-body quantum teleportation in the presence of environments. We predict two emergent critical points that hallmark the transitions of the teleportation performance from the quantum regime to the classical regime, and finally to the no-signal regime as the system-environment coupling, quantified by $\\gamma$, increases. In the quantum regime, teleportation can outperform its classical counterparts, while in the classical regime, it can be replaced by a classical channel. Our prediction is based on a generic argument harnessing the relationship between many-body quantum teleportation and information scrambling, corroborated by solvable Brownian Sachdev-Ye-Kitaev models.","sentences":["Quantum teleportation is a phenomenon arising from entanglement, decisively distinguishing the classical and quantum worlds.","The recent success of many-body quantum teleportation is even more surprising: although input information is initially dispersed and encoded into the many-body state in a complex way, the teleportation process can refocus this highly non-local information at the receiver's end.","This success manifests intriguing capability of many-body systems in quantum information processing.","Current studies indicate that information scrambling, a generic dynamic process in many-body systems, underlies the effectiveness of many-body quantum teleportation.","However, this process is known to undergo a novel scrambling-dissipation transition in the presence of environments.","How environments affect the quantum information processing capability of many-body systems calls for further investigation.","In this work, we study many-body quantum teleportation in the presence of environments.","We predict two emergent critical points that hallmark the transitions of the teleportation performance from the quantum regime to the classical regime, and finally to the no-signal regime as the system-environment coupling, quantified by $\\gamma$, increases.","In the quantum regime, teleportation can outperform its classical counterparts, while in the classical regime, it can be replaced by a classical channel.","Our prediction is based on a generic argument harnessing the relationship between many-body quantum teleportation and information scrambling, corroborated by solvable Brownian Sachdev-Ye-Kitaev models."],"url":"http://arxiv.org/abs/2406.02277v1","category":"quant-ph"}
{"created":"2024-06-04 12:50:33","title":"Position: The Causal Revolution Needs Scientific Pragmatism","abstract":"Causal models and methods have great promise, but their progress has been stalled. Proposals using causality get squeezed between two opposing worldviews. Scientific perfectionism--an insistence on only using \"correct\" models--slows the adoption of causal methods in knowledge generating applications. Pushing in the opposite direction, the academic discipline of computer science prefers algorithms with no or few assumptions, and technologies based on automation and scalability are often selected for economic and business applications. We argue that these system-centric inductive biases should be replaced with a human-centric philosophy we refer to as scientific pragmatism. The machine learning community must strike the right balance to make space for the causal revolution to prosper.","sentences":["Causal models and methods have great promise, but their progress has been stalled.","Proposals using causality get squeezed between two opposing worldviews.","Scientific perfectionism--an insistence on only using \"correct\" models--slows the adoption of causal methods in knowledge generating applications.","Pushing in the opposite direction, the academic discipline of computer science prefers algorithms with no or few assumptions, and technologies based on automation and scalability are often selected for economic and business applications.","We argue that these system-centric inductive biases should be replaced with a human-centric philosophy we refer to as scientific pragmatism.","The machine learning community must strike the right balance to make space for the causal revolution to prosper."],"url":"http://arxiv.org/abs/2406.02275v1","category":"cs.CY"}
{"created":"2024-06-04 12:50:00","title":"Positive Ricci curvature on connected sums of fibre bundles","abstract":"We consider the problem of preserving positive Ricci curvature along connected sums. In this context, based on earlier work by Perelman, Burdick introduced the notion of core metrics and showed that the connected sum of manifolds with core metrics admits a Riemannian metric of positive Ricci curvature. He subsequently showed that core metrics exist on compact rank one symmetric spaces, certain sphere bundles and manifolds obtained as boundaries of certain plumbings. In this article, we show that core metrics can be lifted along general fibre bundles, including sphere bundles and projective bundles. Our techniques also apply to spaces that decompose as the union of two disc bundles such as the Wu manifold. As application we show that all classes in the torsion-free oriented bordism ring can be represented by connected manifolds of positive Ricci curvature.","sentences":["We consider the problem of preserving positive Ricci curvature along connected sums.","In this context, based on earlier work by Perelman, Burdick introduced the notion of core metrics and showed that the connected sum of manifolds with core metrics admits a Riemannian metric of positive Ricci curvature.","He subsequently showed that core metrics exist on compact rank one symmetric spaces, certain sphere bundles and manifolds obtained as boundaries of certain plumbings.","In this article, we show that core metrics can be lifted along general fibre bundles, including sphere bundles and projective bundles.","Our techniques also apply to spaces that decompose as the union of two disc bundles such as the Wu manifold.","As application we show that all classes in the torsion-free oriented bordism ring can be represented by connected manifolds of positive Ricci curvature."],"url":"http://arxiv.org/abs/2406.02274v1","category":"math.DG"}
{"created":"2024-06-04 12:47:39","title":"Steady-State Entanglement Generation via Casimir-Polder Interactions","abstract":"We investigate the generation of steady-state entanglement between two atoms resulting from the fluctuation-mediated Casimir-Polder (CP) interactions near a surface. Starting with an initially separable state of the atoms, we analyze the atom-atom entanglement dynamics for atoms placed at distances in the range of $\\sim25$~nm away from a planar medium, examining the effect of medium properties and geometrical configuration of the atomic dipoles. We show that perfectly conducting and superconducting surfaces yield an optimal steady-state concurrence value of approximately 0.5. Furthermore, although the generated entanglement decreases with medium losses for a metal surface, we identify an optimal distance from the metal surface that assists in the generation of entanglement by the surface. While fluctuation-mediated interactions are typically considered detrimental to the coherence of quantum systems at nanoscales, our results demonstrate a mechanism for leveraging such interactions for entanglement generation.","sentences":["We investigate the generation of steady-state entanglement between two atoms resulting from the fluctuation-mediated Casimir-Polder (CP) interactions near a surface.","Starting with an initially separable state of the atoms, we analyze the atom-atom entanglement dynamics for atoms placed at distances in the range of $\\sim25$~nm away from a planar medium, examining the effect of medium properties and geometrical configuration of the atomic dipoles.","We show that perfectly conducting and superconducting surfaces yield an optimal steady-state concurrence value of approximately 0.5.","Furthermore, although the generated entanglement decreases with medium losses for a metal surface, we identify an optimal distance from the metal surface that assists in the generation of entanglement by the surface.","While fluctuation-mediated interactions are typically considered detrimental to the coherence of quantum systems at nanoscales, our results demonstrate a mechanism for leveraging such interactions for entanglement generation."],"url":"http://arxiv.org/abs/2406.02270v1","category":"quant-ph"}
{"created":"2024-06-04 12:47:13","title":"Graph Neural Networks Do Not Always Oversmooth","abstract":"Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications. However, GNNs suffer from the problem of oversmoothing, the property that the features of all nodes exponentially converge to the same vector over layers, prohibiting the design of deep GNNs. In this work we study oversmoothing in graph convolutional networks (GCNs) by using their Gaussian process (GP) equivalence in the limit of infinitely many hidden features. By generalizing methods from conventional deep neural networks (DNNs), we can describe the distribution of features at the output layer of deep GCNs in terms of a GP: as expected, we find that typical parameter choices from the literature lead to oversmoothing. The theory, however, allows us to identify a new, nonoversmoothing phase: if the initial weights of the network have sufficiently large variance, GCNs do not oversmooth, and node features remain informative even at large depth. We demonstrate the validity of this prediction in finite-size GCNs by training a linear classifier on their output. Moreover, using the linearization of the GCN GP, we generalize the concept of propagation depth of information from DNNs to GCNs. This propagation depth diverges at the transition between the oversmoothing and non-oversmoothing phase. We test the predictions of our approach and find good agreement with finite-size GCNs. Initializing GCNs near the transition to the non-oversmoothing phase, we obtain networks which are both deep and expressive.","sentences":["Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications.","However, GNNs suffer from the problem of oversmoothing, the property that the features of all nodes exponentially converge to the same vector over layers, prohibiting the design of deep GNNs.","In this work we study oversmoothing in graph convolutional networks (GCNs) by using their Gaussian process (GP) equivalence in the limit of infinitely many hidden features.","By generalizing methods from conventional deep neural networks (DNNs), we can describe the distribution of features at the output layer of deep GCNs in terms of a GP: as expected, we find that typical parameter choices from the literature lead to oversmoothing.","The theory, however, allows us to identify a new, nonoversmoothing phase: if the initial weights of the network have sufficiently large variance, GCNs do not oversmooth, and node features remain informative even at large depth.","We demonstrate the validity of this prediction in finite-size GCNs by training a linear classifier on their output.","Moreover, using the linearization of the GCN GP, we generalize the concept of propagation depth of information from DNNs to GCNs.","This propagation depth diverges at the transition between the oversmoothing and non-oversmoothing phase.","We test the predictions of our approach and find good agreement with finite-size GCNs.","Initializing GCNs near the transition to the non-oversmoothing phase, we obtain networks which are both deep and expressive."],"url":"http://arxiv.org/abs/2406.02269v1","category":"stat.ML"}
{"created":"2024-06-04 12:43:47","title":"Prompting Large Language Models with Human Error Markings for Self-Correcting Machine Translation","abstract":"While large language models (LLMs) pre-trained on massive amounts of unpaired language data have reached the state-of-the-art in machine translation (MT) of general domain texts, post-editing (PE) is still required to correct errors and to enhance term translation quality in specialized domains. In this paper we present a pilot study of enhancing translation memories (TM) produced by PE (source segments, machine translations, and reference translations, henceforth called PE-TM) for the needs of correct and consistent term translation in technical domains.   We investigate a light-weight two-step scenario where, at inference time, a human translator marks errors in the first translation step, and in a second step a few similar examples are extracted from the PE-TM to prompt an LLM. Our experiment shows that the additional effort of augmenting translations with human error markings guides the LLM to focus on a correction of the marked errors, yielding consistent improvements over automatic PE (APE) and MT from scratch.","sentences":["While large language models (LLMs) pre-trained on massive amounts of unpaired language data have reached the state-of-the-art in machine translation (MT) of general domain texts, post-editing (PE) is still required to correct errors and to enhance term translation quality in specialized domains.","In this paper we present a pilot study of enhancing translation memories (TM) produced by PE (source segments, machine translations, and reference translations, henceforth called PE-TM) for the needs of correct and consistent term translation in technical domains.   ","We investigate a light-weight two-step scenario where, at inference time, a human translator marks errors in the first translation step, and in a second step a few similar examples are extracted from the PE-TM to prompt an LLM.","Our experiment shows that the additional effort of augmenting translations with human error markings guides the LLM to focus on a correction of the marked errors, yielding consistent improvements over automatic PE (APE) and MT from scratch."],"url":"http://arxiv.org/abs/2406.02267v1","category":"cs.CL"}
{"created":"2024-06-04 12:43:23","title":"Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning Compressor","abstract":"Despite the prevalence of retrieval-augmented language models (RALMs), the seamless integration of these models with retrieval mechanisms to enhance performance in document-based tasks remains challenging. While some post-retrieval processing Retrieval-Augmented Generation (RAG) methods have achieved success, most still lack the ability to distinguish pertinent from extraneous information, leading to potential inconsistencies and reduced precision in the generated output, which subsequently affects the truthfulness of the language model's responses. To address these limitations, this work proposes a novel two-stage consistency learning approach for retrieved information compression in retrieval-augmented language models to enhance performance. By incorporating consistency learning, the aim is to generate summaries that maintain coherence and alignment with the intended semantic representations of a teacher model while improving faithfulness to the original retrieved documents. The proposed method is empirically validated across multiple datasets, demonstrating notable enhancements in precision and efficiency for question-answering tasks. It outperforms existing baselines and showcases the synergistic effects of combining contrastive and consistency learning paradigms within the retrieval-augmented generation framework.","sentences":["Despite the prevalence of retrieval-augmented language models (RALMs), the seamless integration of these models with retrieval mechanisms to enhance performance in document-based tasks remains challenging.","While some post-retrieval processing Retrieval-Augmented Generation (RAG) methods have achieved success, most still lack the ability to distinguish pertinent from extraneous information, leading to potential inconsistencies and reduced precision in the generated output, which subsequently affects the truthfulness of the language model's responses.","To address these limitations, this work proposes a novel two-stage consistency learning approach for retrieved information compression in retrieval-augmented language models to enhance performance.","By incorporating consistency learning, the aim is to generate summaries that maintain coherence and alignment with the intended semantic representations of a teacher model while improving faithfulness to the original retrieved documents.","The proposed method is empirically validated across multiple datasets, demonstrating notable enhancements in precision and efficiency for question-answering tasks.","It outperforms existing baselines and showcases the synergistic effects of combining contrastive and consistency learning paradigms within the retrieval-augmented generation framework."],"url":"http://arxiv.org/abs/2406.02266v1","category":"cs.CL"}
{"created":"2024-06-04 12:41:54","title":"Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning","abstract":"Recent advancements in retrieval-augmented models for image captioning highlight the significance of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice. Retrieved information can sometimes mislead the model generation, negatively impacting performance. In this paper, we analyze the robustness of the SmallCap retrieval-augmented captioning model. Our analysis shows that SmallCap is sensitive to tokens that appear in the majority of the retrieved captions, and integrated gradients attribution shows that those tokens are likely copied into the final caption. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This reduces the probability that the model learns to copy majority tokens and improves both in-domain and cross-domain performance effectively.","sentences":["Recent advancements in retrieval-augmented models for image captioning highlight the significance of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities.","While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice.","Retrieved information can sometimes mislead the model generation, negatively impacting performance.","In this paper, we analyze the robustness of the SmallCap retrieval-augmented captioning model.","Our analysis shows that SmallCap is sensitive to tokens that appear in the majority of the retrieved captions, and integrated gradients attribution shows that those tokens are likely copied into the final caption.","Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets.","This reduces the probability that the model learns to copy majority tokens and improves both in-domain and cross-domain performance effectively."],"url":"http://arxiv.org/abs/2406.02265v1","category":"cs.CV"}
{"created":"2024-06-04 12:33:02","title":"M3DM-NR: RGB-3D Noisy-Resistant Industrial Anomaly Detection via Multimodal Denoising","abstract":"Existing industrial anomaly detection methods primarily concentrate on unsupervised learning with pristine RGB images. Yet, both RGB and 3D data are crucial for anomaly detection, and the datasets are seldom completely clean in practical scenarios. To address above challenges, this paper initially delves into the RGB-3D multi-modal noisy anomaly detection, proposing a novel noise-resistant M3DM-NR framework to leveraging strong multi-modal discriminative capabilities of CLIP. M3DM-NR consists of three stages: Stage-I introduces the Suspected References Selection module to filter a few normal samples from the training dataset, using the multimodal features extracted by the Initial Feature Extraction, and a Suspected Anomaly Map Computation module to generate a suspected anomaly map to focus on abnormal regions as reference. Stage-II uses the suspected anomaly maps of the reference samples as reference, and inputs image, point cloud, and text information to achieve denoising of the training samples through intra-modal comparison and multi-scale aggregation operations. Finally, Stage-III proposes the Point Feature Alignment, Unsupervised Feature Fusion, Noise Discriminative Coreset Selection, and Decision Layer Fusion modules to learn the pattern of the training dataset, enabling anomaly detection and segmentation while filtering out noise. Extensive experiments show that M3DM-NR outperforms state-of-the-art methods in 3D-RGB multi-modal noisy anomaly detection.","sentences":["Existing industrial anomaly detection methods primarily concentrate on unsupervised learning with pristine RGB images.","Yet, both RGB and 3D data are crucial for anomaly detection, and the datasets are seldom completely clean in practical scenarios.","To address above challenges, this paper initially delves into the RGB-3D multi-modal noisy anomaly detection, proposing a novel noise-resistant M3DM-NR framework to leveraging strong multi-modal discriminative capabilities of CLIP.","M3DM-NR consists of three stages: Stage-I introduces the Suspected References Selection module to filter a few normal samples from the training dataset, using the multimodal features extracted by the Initial Feature Extraction, and a Suspected Anomaly Map Computation module to generate a suspected anomaly map to focus on abnormal regions as reference.","Stage-II uses the suspected anomaly maps of the reference samples as reference, and inputs image, point cloud, and text information to achieve denoising of the training samples through intra-modal comparison and multi-scale aggregation operations.","Finally, Stage-III proposes the Point Feature Alignment, Unsupervised Feature Fusion, Noise Discriminative Coreset Selection, and Decision Layer Fusion modules to learn the pattern of the training dataset, enabling anomaly detection and segmentation while filtering out noise.","Extensive experiments show that M3DM-NR outperforms state-of-the-art methods in 3D-RGB multi-modal noisy anomaly detection."],"url":"http://arxiv.org/abs/2406.02263v1","category":"cs.CV"}
{"created":"2024-06-04 12:32:48","title":"Experimental demonstration of the combined arm- and cavity-locking system for LISA","abstract":"Laser frequency noise suppression is a critical requirement for the Laser Interferometer Space Antenna (LISA) mission to detect gravitational waves. The baseline laser stabilization is achieved using cavity pre-stabilization and a post-processing technique called Time-Delay-Interferometry (TDI). To enhance the margins for TDI, alternate laser locking schemes should be investigated. A novel stabilisation blending the excellent stability of the arm with the existing cavity reference has been shown theoretically to meet the first-generation TDI margins. This locking system was designed to be implemented as a firmware change and have minimal or no changes to the LISA hardware. This paper experimentally verifies the hybrid laser locking technique by utilizing two references - an optical cavity, and an interferometer with delay imparted using 10 km of optical fiber. The results indicate the viability of the combination of arm-cavity locking system for LISA. They show the key benefits envisioned by this technique; suppression of the cavity fluctuations by the arm sensor (by 21 dB in this demonstration) and reduction of Doppler pulling of the laser frequency, a key technical challenge for arm locking.","sentences":["Laser frequency noise suppression is a critical requirement for the Laser Interferometer Space Antenna (LISA) mission to detect gravitational waves.","The baseline laser stabilization is achieved using cavity pre-stabilization and a post-processing technique called Time-Delay-Interferometry (TDI).","To enhance the margins for TDI, alternate laser locking schemes should be investigated.","A novel stabilisation blending the excellent stability of the arm with the existing cavity reference has been shown theoretically to meet the first-generation TDI margins.","This locking system was designed to be implemented as a firmware change and have minimal or no changes to the LISA hardware.","This paper experimentally verifies the hybrid laser locking technique by utilizing two references - an optical cavity, and an interferometer with delay imparted using 10 km of optical fiber.","The results indicate the viability of the combination of arm-cavity locking system for LISA.","They show the key benefits envisioned by this technique; suppression of the cavity fluctuations by the arm sensor (by 21 dB in this demonstration) and reduction of Doppler pulling of the laser frequency, a key technical challenge for arm locking."],"url":"http://arxiv.org/abs/2406.02261v1","category":"gr-qc"}
{"created":"2024-06-04 12:21:55","title":"MidiCaps -- A large-scale MIDI dataset with text captions","abstract":"Generative models guided by text prompts are increasingly becoming more popular. However, no text-to-MIDI models currently exist, mostly due to the lack of a captioned MIDI dataset. This work aims to enable research that combines LLMs with symbolic music by presenting the first large-scale MIDI dataset with text captions that is openly available: MidiCaps. MIDI (Musical Instrument Digital Interface) files are a widely used format for encoding musical information. Their structured format captures the nuances of musical composition and has practical applications by music producers, composers, musicologists, as well as performers. Inspired by recent advancements in captioning techniques applied to various domains, we present a large-scale curated dataset of over 168k MIDI files accompanied by textual descriptions. Each MIDI caption succinctly describes the musical content, encompassing tempo, chord progression, time signature, instruments present, genre and mood; thereby facilitating multi-modal exploration and analysis. The dataset contains a mix of various genres, styles, and complexities, offering a rich source for training and evaluating models for tasks such as music information retrieval, music understanding and cross-modal translation. We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study. We anticipate that this resource will stimulate further research in the intersection of music and natural language processing, fostering advancements in both fields.","sentences":["Generative models guided by text prompts are increasingly becoming more popular.","However, no text-to-MIDI models currently exist, mostly due to the lack of a captioned MIDI dataset.","This work aims to enable research that combines LLMs with symbolic music by presenting the first large-scale MIDI dataset with text captions that is openly available: MidiCaps.","MIDI (Musical Instrument Digital Interface) files are a widely used format for encoding musical information.","Their structured format captures the nuances of musical composition and has practical applications by music producers, composers, musicologists, as well as performers.","Inspired by recent advancements in captioning techniques applied to various domains, we present a large-scale curated dataset of over 168k MIDI files accompanied by textual descriptions.","Each MIDI caption succinctly describes the musical content, encompassing tempo, chord progression, time signature, instruments present, genre and mood; thereby facilitating multi-modal exploration and analysis.","The dataset contains a mix of various genres, styles, and complexities, offering a rich source for training and evaluating models for tasks such as music information retrieval, music understanding and cross-modal translation.","We provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study.","We anticipate that this resource will stimulate further research in the intersection of music and natural language processing, fostering advancements in both fields."],"url":"http://arxiv.org/abs/2406.02255v1","category":"eess.AS"}
{"created":"2024-06-04 12:20:54","title":"System Design and Parameter Optimization for Remote Coverage from NOMA-based High-Altitude Platform Stations (HAPS)","abstract":"Stratospheric solar-powered high-altitude platform stations (HAPS) have recently gained immense popularity for their ubiquitous connectivity and resilient operation while providing/catalyzing advanced mobile wireless communication services. They have particularly emerged as promising alternatives for economic coverage of remote areas in the world. This makes them suitable candidates to meet the UN Sustainable Development Goals (SDG-2030) for global connectivity. HAPS can provide line-of-sight (LoS) communications to the ground users in its ultra-wide coverage area. We propose to divide these users into multiple user groups and serve each group with a high-density flexible narrow spot beam, generated by the phased array antennas mounted on HAPS, to achieve high data rates. We carry out the user association and power allocation in a downlink (DL) non-orthogonal multiple access (NOMA) scheme in each user group. To improve the system performance, a sum rate maximization problem is formulated by jointly designing user grouping, user association, beam optimization, and power allocation while guaranteeing the quality-of-service (QoS) for users with limited power budget. We further investigate the outage performance of the users with the proposed approach as compared to the traditional scheme. Our findings reveal the significance of the joint design of communication parameters for enhanced system performance, optimum energy utilization, and resource allocation.","sentences":["Stratospheric solar-powered high-altitude platform stations (HAPS) have recently gained immense popularity for their ubiquitous connectivity and resilient operation while providing/catalyzing advanced mobile wireless communication services.","They have particularly emerged as promising alternatives for economic coverage of remote areas in the world.","This makes them suitable candidates to meet the UN Sustainable Development Goals (SDG-2030) for global connectivity.","HAPS can provide line-of-sight (LoS) communications to the ground users in its ultra-wide coverage area.","We propose to divide these users into multiple user groups and serve each group with a high-density flexible narrow spot beam, generated by the phased array antennas mounted on HAPS, to achieve high data rates.","We carry out the user association and power allocation in a downlink (DL) non-orthogonal multiple access (NOMA) scheme in each user group.","To improve the system performance, a sum rate maximization problem is formulated by jointly designing user grouping, user association, beam optimization, and power allocation while guaranteeing the quality-of-service (QoS) for users with limited power budget.","We further investigate the outage performance of the users with the proposed approach as compared to the traditional scheme.","Our findings reveal the significance of the joint design of communication parameters for enhanced system performance, optimum energy utilization, and resource allocation."],"url":"http://arxiv.org/abs/2406.02254v1","category":"eess.SP"}
{"created":"2024-06-04 12:19:09","title":"PuFace: Defending against Facial Cloaking Attacks for Facial Recognition Models","abstract":"The recently proposed facial cloaking attacks add invisible perturbation (cloaks) to facial images to protect users from being recognized by unauthorized facial recognition models. However, we show that the \"cloaks\" are not robust enough and can be removed from images.   This paper introduces PuFace, an image purification system leveraging the generalization ability of neural networks to diminish the impact of cloaks by pushing the cloaked images towards the manifold of natural (uncloaked) images before the training process of facial recognition models. Specifically, we devise a purifier that takes all the training images including both cloaked and natural images as input and generates the purified facial images close to the manifold where natural images lie. To meet the defense goal, we propose to train the purifier on particularly amplified cloaked images with a loss function that combines image loss and feature loss. Our empirical experiment shows PuFace can effectively defend against two state-of-the-art facial cloaking attacks and reduces the attack success rate from 69.84\\% to 7.61\\% on average without degrading the normal accuracy for various facial recognition models. Moreover, PuFace is a model-agnostic defense mechanism that can be applied to any facial recognition model without modifying the model structure.","sentences":["The recently proposed facial cloaking attacks add invisible perturbation (cloaks) to facial images to protect users from being recognized by unauthorized facial recognition models.","However, we show that the \"cloaks\" are not robust enough and can be removed from images.   ","This paper introduces PuFace, an image purification system leveraging the generalization ability of neural networks to diminish the impact of cloaks by pushing the cloaked images towards the manifold of natural (uncloaked) images before the training process of facial recognition models.","Specifically, we devise a purifier that takes all the training images including both cloaked and natural images as input and generates the purified facial images close to the manifold where natural images lie.","To meet the defense goal, we propose to train the purifier on particularly amplified cloaked images with a loss function that combines image loss and feature loss.","Our empirical experiment shows PuFace can effectively defend against two state-of-the-art facial cloaking attacks and reduces the attack success rate from 69.84\\% to 7.61\\% on average without degrading the normal accuracy for various facial recognition models.","Moreover, PuFace is a model-agnostic defense mechanism that can be applied to any facial recognition model without modifying the model structure."],"url":"http://arxiv.org/abs/2406.02253v1","category":"cs.CV"}
{"created":"2024-06-04 12:17:16","title":"Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning","abstract":"Telling stories is an integral part of human communication which can evoke emotions and influence the affective states of the audience. Automatically modeling emotional trajectories in stories has thus attracted considerable scholarly interest. However, as most existing works have been limited to unsupervised dictionary-based approaches, there is no benchmark for this task. We address this gap by introducing continuous valence and arousal labels for an existing dataset of children's stories originally annotated with discrete emotion categories. We collect additional annotations for this data and map the categorical labels to the continuous valence and arousal space. For predicting the thus obtained emotionality signals, we fine-tune a DeBERTa model and improve upon this baseline via a weakly supervised learning approach. The best configuration achieves a Concordance Correlation Coefficient (CCC) of $.8221$ for valence and $.7125$ for arousal on the test set, demonstrating the efficacy of our proposed approach. A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story. In addition, we uncover the weaknesses of our approach by investigating examples that prove to be difficult to predict.","sentences":["Telling stories is an integral part of human communication which can evoke emotions and influence the affective states of the audience.","Automatically modeling emotional trajectories in stories has thus attracted considerable scholarly interest.","However, as most existing works have been limited to unsupervised dictionary-based approaches, there is no benchmark for this task.","We address this gap by introducing continuous valence and arousal labels for an existing dataset of children's stories originally annotated with discrete emotion categories.","We collect additional annotations for this data and map the categorical labels to the continuous valence and arousal space.","For predicting the thus obtained emotionality signals, we fine-tune a DeBERTa model and improve upon this baseline via a weakly supervised learning approach.","The best configuration achieves a Concordance Correlation Coefficient (CCC) of $.8221$ for valence and $.7125$ for arousal on the test set, demonstrating the efficacy of our proposed approach.","A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story.","In addition, we uncover the weaknesses of our approach by investigating examples that prove to be difficult to predict."],"url":"http://arxiv.org/abs/2406.02251v1","category":"cs.CL"}
{"created":"2024-06-04 12:17:11","title":"Multi-Stage Speech Bandwidth Extension with Flexible Sampling Rate Control","abstract":"The majority of existing speech bandwidth extension (BWE) methods operate under the constraint of fixed source and target sampling rates, which limits their flexibility in practical applications. In this paper, we propose a multi-stage speech BWE model named MS-BWE, which can handle a set of source and target sampling rate pairs and achieve flexible extensions of frequency bandwidth. The proposed MS-BWE model comprises a cascade of BWE blocks, with each block featuring a dual-stream architecture to realize amplitude and phase extension, progressively painting the speech frequency bands stage by stage. The teacher-forcing strategy is employed to mitigate the discrepancy between training and inference. Experimental results demonstrate that our proposed MS-BWE is comparable to state-of-the-art speech BWE methods in speech quality. Regarding generation efficiency, the one-stage generation of MS-BWE can achieve over one thousand times real-time on GPU and about sixty times on CPU.","sentences":["The majority of existing speech bandwidth extension (BWE) methods operate under the constraint of fixed source and target sampling rates, which limits their flexibility in practical applications.","In this paper, we propose a multi-stage speech BWE model named MS-BWE, which can handle a set of source and target sampling rate pairs and achieve flexible extensions of frequency bandwidth.","The proposed MS-BWE model comprises a cascade of BWE blocks, with each block featuring a dual-stream architecture to realize amplitude and phase extension, progressively painting the speech frequency bands stage by stage.","The teacher-forcing strategy is employed to mitigate the discrepancy between training and inference.","Experimental results demonstrate that our proposed MS-BWE is comparable to state-of-the-art speech BWE methods in speech quality.","Regarding generation efficiency, the one-stage generation of MS-BWE can achieve over one thousand times real-time on GPU and about sixty times on CPU."],"url":"http://arxiv.org/abs/2406.02250v1","category":"eess.AS"}
{"created":"2024-06-04 12:16:52","title":"Probabilistic Cauchy Functional Equations","abstract":"In this short note, we introduce probabilistic Cauchy functional equations, specifically, functional equations of the following form: $$   f(X_1 + X_2) \\stackrel{d}{=} f(X_1) + f(X_2),   $$ where $X_1$ and $X_2$ represent two independent identically distributed real-valued random variables governed by a distribution $\\mu$ having appropriate support on the real line. The symbol $\\stackrel{d}{=}$ denotes equality in distribution. When $\\mu$ follows an exponential distribution, we provide sufficient (regularity) conditions on the function $f$ to ensure that the unique measurable solution to the above equation is solely linear. Furthermore, we present some partial results in the general case, establishing a connection to integrated Cauchy functional equations.","sentences":["In this short note, we introduce probabilistic Cauchy functional equations, specifically, functional equations of the following form: $$   f(X_1 + X_2) \\stackrel{d}{=} f(X_1) + f(X_2),   $$ where $X_1$ and $X_2$ represent two independent identically distributed real-valued random variables governed by a distribution $\\mu$ having appropriate support on the real line.","The symbol $\\stackrel{d}{=}$ denotes equality in distribution.","When $\\mu$ follows an exponential distribution, we provide sufficient (regularity) conditions on the function $f$ to ensure that the unique measurable solution to the above equation is solely linear.","Furthermore, we present some partial results in the general case, establishing a connection to integrated Cauchy functional equations."],"url":"http://arxiv.org/abs/2406.02248v1","category":"math.PR"}
{"created":"2024-06-04 12:13:52","title":"A Study of the Latest Updates of the Readout System for the Hybird-Pixel Detector at HEPS","abstract":"The High Energy Photon Source (HEPS) represents a fourth-generation light source. This facility has made unprecedented advancements in accelerator technology, necessitating the development of new detectors to satisfy physical requirements such as single-photon resolution, large dynamic range, and high frame rates. Since 2016, the Institute of High Energy Physics has introduced the first user-experimental hybrid pixel detector, progressing to the fourth-generation million-pixel detector designed for challenging conditions, with the dual-threshold single-photon detector HEPS-Beijing PIXel (HEPS-BPIX) set as the next-generation target. HEPS-BPIX will employ the entirely new Application-Specific Integrated Circuit (ASIC) BP40 for pixel information readout. Data flow will be managed and controlled through readout electronics based on a two-tier Field-Programmable Gate Array (FPGA) system: the Front-End Electronics (FEE) and the Input-Output Board (IOB) handle the fan-out for 12 ASICs, and the u4FCP is tasked with processing serial data on high-speed links, transferring pixel-level data to the back-end RTM and uTCA chassis, or independently outputting through a network port, enabling remote control of the entire detector. The new HEPS-BPIX firmware has undergone a comprehensive redesign and update to meet the electronic characteristics of the new chip and to improve the overall performance of the detector. We provide an overview of the core subunits of HEPS-BPIX, emphasizing the readout system, evaluating the new hardware and firmware, and highlighting some of its innovative features and characteristics.","sentences":["The High Energy Photon Source (HEPS) represents a fourth-generation light source.","This facility has made unprecedented advancements in accelerator technology, necessitating the development of new detectors to satisfy physical requirements such as single-photon resolution, large dynamic range, and high frame rates.","Since 2016, the Institute of High Energy Physics has introduced the first user-experimental hybrid pixel detector, progressing to the fourth-generation million-pixel detector designed for challenging conditions, with the dual-threshold single-photon detector HEPS-Beijing PIXel (HEPS-BPIX) set as the next-generation target.","HEPS-BPIX will employ the entirely new Application-Specific Integrated Circuit (ASIC)","BP40 for pixel information readout.","Data flow will be managed and controlled through readout electronics based on a two-tier Field-Programmable Gate Array (FPGA) system: the Front-End Electronics (FEE) and the Input-Output Board (IOB) handle the fan-out for 12 ASICs, and the u4FCP is tasked with processing serial data on high-speed links, transferring pixel-level data to the back-end RTM and uTCA chassis, or independently outputting through a network port, enabling remote control of the entire detector.","The new HEPS-BPIX firmware has undergone a comprehensive redesign and update to meet the electronic characteristics of the new chip and to improve the overall performance of the detector.","We provide an overview of the core subunits of HEPS-BPIX, emphasizing the readout system, evaluating the new hardware and firmware, and highlighting some of its innovative features and characteristics."],"url":"http://arxiv.org/abs/2406.02247v1","category":"physics.ins-det"}
{"created":"2024-06-04 12:09:44","title":"Description Boosting for Zero-Shot Entity and Relation Classification","abstract":"Zero-shot entity and relation classification models leverage available external information of unseen classes -- e.g., textual descriptions -- to annotate input text data. Thanks to the minimum data requirement, Zero-Shot Learning (ZSL) methods have high value in practice, especially in applications where labeled data is scarce. Even though recent research in ZSL has demonstrated significant results, our analysis reveals that those methods are sensitive to provided textual descriptions of entities (or relations). Even a minor modification of descriptions can lead to a change in the decision boundary between entity (or relation) classes. In this paper, we formally define the problem of identifying effective descriptions for zero shot inference. We propose a strategy for generating variations of an initial description, a heuristic for ranking them and an ensemble method capable of boosting the predictions of zero-shot models through description enhancement. Empirical results on four different entity and relation classification datasets show that our proposed method outperform existing approaches and achieve new SOTA results on these datasets under the ZSL settings. The source code of the proposed solutions and the evaluation framework are open-sourced.","sentences":["Zero-shot entity and relation classification models leverage available external information of unseen classes -- e.g., textual descriptions -- to annotate input text data.","Thanks to the minimum data requirement, Zero-Shot Learning (ZSL) methods have high value in practice, especially in applications where labeled data is scarce.","Even though recent research in ZSL has demonstrated significant results, our analysis reveals that those methods are sensitive to provided textual descriptions of entities (or relations).","Even a minor modification of descriptions can lead to a change in the decision boundary between entity (or relation) classes.","In this paper, we formally define the problem of identifying effective descriptions for zero shot inference.","We propose a strategy for generating variations of an initial description, a heuristic for ranking them and an ensemble method capable of boosting the predictions of zero-shot models through description enhancement.","Empirical results on four different entity and relation classification datasets show that our proposed method outperform existing approaches and achieve new SOTA results on these datasets under the ZSL settings.","The source code of the proposed solutions and the evaluation framework are open-sourced."],"url":"http://arxiv.org/abs/2406.02245v1","category":"cs.CL"}
{"created":"2024-06-04 12:09:08","title":"On the characterization of chordal graphs using Horn hypergeometric series","abstract":"In 6, Radchenko and Villegas characterized the chordal graphs by their inverse of the independence polynomials being Horn hypergeometric series. In this paper, we reprove their result using some elementary combinatorial methods and also generalize it to PEO graphs that could have a countable number of vertices. Our proof is different from the proof of 6, and it is based on the connection between the inverse of the multi-variate independence polynomials and the multi-colored chromatic polynomials of graphs, established in 1.","sentences":["In 6, Radchenko and Villegas characterized the chordal graphs by their inverse of the independence polynomials being Horn hypergeometric series.","In this paper, we reprove their result using some elementary combinatorial methods and also generalize it to PEO graphs that could have a countable number of vertices.","Our proof is different from the proof of 6, and it is based on the connection between the inverse of the multi-variate independence polynomials and the multi-colored chromatic polynomials of graphs, established in 1."],"url":"http://arxiv.org/abs/2406.02244v1","category":"math.CO"}
{"created":"2024-06-04 12:00:37","title":"Quantum Computing in Wireless Communications and Networking: A Tutorial-cum-Survey","abstract":"Owing to its outstanding parallel computing capabilities, quantum computing (QC) has been a subject of continuous attention. With the gradual maturation of QC platforms, it has increasingly played a significant role in various fields such as transportation, pharmaceuticals, and industrial manufacturing,achieving unprecedented milestones. In modern society, wireless communication stands as an indispensable infrastructure, with its essence lying in optimization. Although artificial intelligence (AI) algorithms such as Reinforcement Learning (RL) and mathematical optimization have greatly enhanced the performance of wireless communication, the rapid attainment of optimal solutions for wireless communication problems remains an unresolved challenge. QC, however, presents a new alternative. This paper aims to elucidate the fundamentals of QC and explore its applications in wireless communications and networking. First, we will provide a tutorial on QC, covering its basics, characteristics, and popular QC algorithms. Next, we will introduce the applications of QC in communication and networking, followed by its applications in miscellaneous areas such as security and privacy, localization and tracking, and video streaming. Finally,we will discuss remaining open issues before concluding.","sentences":["Owing to its outstanding parallel computing capabilities, quantum computing (QC) has been a subject of continuous attention.","With the gradual maturation of QC platforms, it has increasingly played a significant role in various fields such as transportation, pharmaceuticals, and industrial manufacturing,achieving unprecedented milestones.","In modern society, wireless communication stands as an indispensable infrastructure, with its essence lying in optimization.","Although artificial intelligence (AI) algorithms such as Reinforcement Learning (RL) and mathematical optimization have greatly enhanced the performance of wireless communication, the rapid attainment of optimal solutions for wireless communication problems remains an unresolved challenge.","QC, however, presents a new alternative.","This paper aims to elucidate the fundamentals of QC and explore its applications in wireless communications and networking.","First, we will provide a tutorial on QC, covering its basics, characteristics, and popular QC algorithms.","Next, we will introduce the applications of QC in communication and networking, followed by its applications in miscellaneous areas such as security and privacy, localization and tracking, and video streaming.","Finally,we will discuss remaining open issues before concluding."],"url":"http://arxiv.org/abs/2406.02240v1","category":"cs.NI"}
{"created":"2024-06-04 11:57:58","title":"Self-Modifying State Modeling for Simultaneous Machine Translation","abstract":"Simultaneous Machine Translation (SiMT) generates target outputs while receiving stream source inputs and requires a read/write policy to decide whether to wait for the next source token or generate a new target token, whose decisions form a \\textit{decision path}. Existing SiMT methods, which learn the policy by exploring various decision paths in training, face inherent limitations. These methods not only fail to precisely optimize the policy due to the inability to accurately assess the individual impact of each decision on SiMT performance, but also cannot sufficiently explore all potential paths because of their vast number. Besides, building decision paths requires unidirectional encoders to simulate streaming source inputs, which impairs the translation quality of SiMT models. To solve these issues, we propose \\textbf{S}elf-\\textbf{M}odifying \\textbf{S}tate \\textbf{M}odeling (SM$^2$), a novel training paradigm for SiMT task. Without building decision paths, SM$^2$ individually optimizes decisions at each state during training. To precisely optimize the policy, SM$^2$ introduces Self-Modifying process to independently assess and adjust decisions at each state. For sufficient exploration, SM$^2$ proposes Prefix Sampling to efficiently traverse all potential states. Moreover, SM$^2$ ensures compatibility with bidirectional encoders, thus achieving higher translation quality. Experiments show that SM$^2$ outperforms strong baselines. Furthermore, SM$^2$ allows offline machine translation models to acquire SiMT ability with fine-tuning.","sentences":["Simultaneous Machine Translation (SiMT) generates target outputs while receiving stream source inputs and requires a read/write policy to decide whether to wait for the next source token or generate a new target token, whose decisions form a \\textit{decision path}.","Existing SiMT methods, which learn the policy by exploring various decision paths in training, face inherent limitations.","These methods not only fail to precisely optimize the policy due to the inability to accurately assess the individual impact of each decision on SiMT performance, but also cannot sufficiently explore all potential paths because of their vast number.","Besides, building decision paths requires unidirectional encoders to simulate streaming source inputs, which impairs the translation quality of SiMT models.","To solve these issues, we propose \\textbf{S}elf-\\textbf{M}odifying \\textbf{S}tate \\textbf{M}odeling (SM$^2$), a novel training paradigm for SiMT task.","Without building decision paths, SM$^2$ individually optimizes decisions at each state during training.","To precisely optimize the policy, SM$^2$ introduces Self-Modifying process to independently assess and adjust decisions at each state.","For sufficient exploration, SM$^2$ proposes Prefix Sampling to efficiently traverse all potential states.","Moreover, SM$^2$ ensures compatibility with bidirectional encoders, thus achieving higher translation quality.","Experiments show that SM$^2$ outperforms strong baselines.","Furthermore, SM$^2$ allows offline machine translation models to acquire SiMT ability with fine-tuning."],"url":"http://arxiv.org/abs/2406.02237v1","category":"cs.CL"}
{"created":"2024-06-04 11:56:37","title":"Power Mean Estimation in Stochastic Monte-Carlo Tree_Search","abstract":"Monte-Carlo Tree Search (MCTS) is a widely-used strategy for online planning that combines Monte-Carlo sampling with forward tree search. Its success relies on the Upper Confidence bound for Trees (UCT) algorithm, an extension of the UCB method for multi-arm bandits. However, the theoretical foundation of UCT is incomplete due to an error in the logarithmic bonus term for action selection, leading to the development of Fixed-Depth-MCTS with a polynomial exploration bonus to balance exploration and exploitation~\\citep{shah2022journal}. Both UCT and Fixed-Depth-MCTS suffer from biased value estimation: the weighted sum underestimates the optimal value, while the maximum valuation overestimates it~\\citep{coulom2006efficient}. The power mean estimator offers a balanced solution, lying between the average and maximum values. Power-UCT~\\citep{dam2019generalized} incorporates this estimator for more accurate value estimates but its theoretical analysis remains incomplete. This paper introduces Stochastic-Power-UCT, an MCTS algorithm using the power mean estimator and tailored for stochastic MDPs. We analyze its polynomial convergence in estimating root node values and show that it shares the same convergence rate of $\\mathcal{O}(n^{-1/2})$, with $n$ is the number of visited trajectories, as Fixed-Depth-MCTS, with the latter being a special case of the former. Our theoretical results are validated with empirical tests across various stochastic MDP environments.","sentences":["Monte-Carlo Tree Search (MCTS) is a widely-used strategy for online planning that combines Monte-Carlo sampling with forward tree search.","Its success relies on the Upper Confidence bound for Trees (UCT) algorithm, an extension of the UCB method for multi-arm bandits.","However, the theoretical foundation of UCT is incomplete due to an error in the logarithmic bonus term for action selection, leading to the development of Fixed-Depth-MCTS with a polynomial exploration bonus to balance exploration and exploitation~\\citep{shah2022journal}.","Both UCT and Fixed-Depth-MCTS suffer from biased value estimation: the weighted sum underestimates the optimal value, while the maximum valuation overestimates it~\\citep{coulom2006efficient}.","The power mean estimator offers a balanced solution, lying between the average and maximum values.","Power-UCT~\\citep{dam2019generalized} incorporates this estimator for more accurate value estimates but its theoretical analysis remains incomplete.","This paper introduces Stochastic-Power-UCT, an MCTS algorithm using the power mean estimator and tailored for stochastic MDPs.","We analyze its polynomial convergence in estimating root node values and show that it shares the same convergence rate of $\\mathcal{O}(n^{-1/2})$, with $n$ is the number of visited trajectories, as Fixed-Depth-MCTS, with the latter being a special case of the former.","Our theoretical results are validated with empirical tests across various stochastic MDP environments."],"url":"http://arxiv.org/abs/2406.02235v1","category":"cs.AI"}
{"created":"2024-06-04 11:56:19","title":"On the Limitations of Fractal Dimension as a Measure of Generalization","abstract":"Bounding and predicting the generalization gap of overparameterized neural networks remains a central open problem in theoretical machine learning. Neural network optimization trajectories have been proposed to possess fractal structure, leading to bounds and generalization measures based on notions of fractal dimension on these trajectories. Prominently, both the Hausdorff dimension and the persistent homology dimension have been proposed to correlate with generalization gap, thus serving as a measure of generalization. This work performs an extended evaluation of these topological generalization measures. We demonstrate that fractal dimension fails to predict generalization of models trained from poor initializations. We further identify that the $\\ell^2$ norm of the final parameter iterate, one of the simplest complexity measures in learning theory, correlates more strongly with the generalization gap than these notions of fractal dimension. Finally, our study reveals the intriguing manifestation of model-wise double descent in persistent homology-based generalization measures. This work lays the ground for a deeper investigation of the causal relationships between fractal geometry, topological data analysis, and neural network optimization.","sentences":["Bounding and predicting the generalization gap of overparameterized neural networks remains a central open problem in theoretical machine learning.","Neural network optimization trajectories have been proposed to possess fractal structure, leading to bounds and generalization measures based on notions of fractal dimension on these trajectories.","Prominently, both the Hausdorff dimension and the persistent homology dimension have been proposed to correlate with generalization gap, thus serving as a measure of generalization.","This work performs an extended evaluation of these topological generalization measures.","We demonstrate that fractal dimension fails to predict generalization of models trained from poor initializations.","We further identify that the $\\ell^2$ norm of the final parameter iterate, one of the simplest complexity measures in learning theory, correlates more strongly with the generalization gap than these notions of fractal dimension.","Finally, our study reveals the intriguing manifestation of model-wise double descent in persistent homology-based generalization measures.","This work lays the ground for a deeper investigation of the causal relationships between fractal geometry, topological data analysis, and neural network optimization."],"url":"http://arxiv.org/abs/2406.02234v1","category":"cs.LG"}
{"created":"2024-06-04 11:53:59","title":"Optimizing Air-borne Network-in-a-box Deployment for Efficient Remote Coverage","abstract":"Among many envisaged drivers for the sixth generation, one is from the United Nations Sustainability Development Goals 2030 to eliminate digital inequality. Remote coverage in sparsely populated areas, difficult terrains, or emergency scenarios requires on-demand access and flexible deployment with minimal capex and opex. In this context, network-in-a-box (NIB) is an exciting solution that packs the whole wireless network into a single portable and re-configurable box to support multiple access technologies such as WiFi/2G/3G/4G/5G etc. In this paper, we propose low-altitude platform stations (LAPS) based NIBs with stratospheric high-altitude platform station (HAPS) as backhaul. Specifically, backhaul employs non-orthogonal multiple access (NOMA) with superposition coding at the transmitting HAPS and successive interference cancellation (SIC) at the receiving NIBs, whereas the access link (AL) employs superposition coding along with the regularized zero-forcing (RZF) precoding at the NIB in order to elevate the computational overhead from the ground users. The required number of airborne NIBs to serve a desired coverage area, their optimal placement, user association, beam optimization, and resource allocation are optimized by maximizing the sum rate of the AL while maintaining the quality of service. Our findings reveal the significance of thorough system planning and communication parameters optimization for enhanced system performance and best coverage under limited resources.","sentences":["Among many envisaged drivers for the sixth generation, one is from the United Nations Sustainability Development Goals 2030 to eliminate digital inequality.","Remote coverage in sparsely populated areas, difficult terrains, or emergency scenarios requires on-demand access and flexible deployment with minimal capex and opex.","In this context, network-in-a-box (NIB) is an exciting solution that packs the whole wireless network into a single portable and re-configurable box to support multiple access technologies such as WiFi/2G/3G/4G/5G etc.","In this paper, we propose low-altitude platform stations (LAPS) based NIBs with stratospheric high-altitude platform station (HAPS) as backhaul.","Specifically, backhaul employs non-orthogonal multiple access (NOMA) with superposition coding at the transmitting HAPS and successive interference cancellation (SIC) at the receiving NIBs, whereas the access link (AL) employs superposition coding along with the regularized zero-forcing (RZF) precoding at the NIB in order to elevate the computational overhead from the ground users.","The required number of airborne NIBs to serve a desired coverage area, their optimal placement, user association, beam optimization, and resource allocation are optimized by maximizing the sum rate of the AL while maintaining the quality of service.","Our findings reveal the significance of thorough system planning and communication parameters optimization for enhanced system performance and best coverage under limited resources."],"url":"http://arxiv.org/abs/2406.02232v1","category":"eess.SP"}
{"created":"2024-06-04 11:51:10","title":"Longitudinal Filtering, Sponge Layers, and Equatorial Jet Formation in a General Circulation Model of Gaseous Exoplanets","abstract":"General circulation models are a useful tool in understanding the three dimensional structure of hot Jupiter and sub-Neptune atmospheres; however, understanding the validity of the results from these simulations requires an understanding the artificial dissipation required for numerical stability. In this paper, we investigate the impact of the longitudinal filter and vertical ``sponge'' used in the Met Office's {\\sc Unified Model} when simulating gaseous exoplanets. We demonstrate that excessive dissipation can result in counter-rotating jets and a catastrophic failure to conserve angular momentum. Once the dissipation is reduced to a level where a super-rotating jet forms, however, the jet and thermal structure are relatively insensitive to the dissipation, except in the nightside gyres where temperatures can vary by $\\sim 100\\,\\mathrm{K}$. We do find, however, that flattening the latitudinal profile of the longitudinal filtering alters the results more than a reduction in the strength of the filtering itself. We also show that even in situations where the temperatures are relatively insensitive to the dissipation, the vertical velocities can still vary with the dissipation, potentially impacting physical processes that depend on the local vertical transport.","sentences":["General circulation models are a useful tool in understanding the three dimensional structure of hot Jupiter and sub-Neptune atmospheres; however, understanding the validity of the results from these simulations requires an understanding the artificial dissipation required for numerical stability.","In this paper, we investigate the impact of the longitudinal filter and vertical ``sponge'' used in the Met Office's {\\sc Unified Model} when simulating gaseous exoplanets.","We demonstrate that excessive dissipation can result in counter-rotating jets and a catastrophic failure to conserve angular momentum.","Once the dissipation is reduced to a level where a super-rotating jet forms, however, the jet and thermal structure are relatively insensitive to the dissipation, except in the nightside gyres where temperatures can vary by $\\sim 100\\,\\mathrm{K}$. We do find, however, that flattening the latitudinal profile of the longitudinal filtering alters the results more than a reduction in the strength of the filtering itself.","We also show that even in situations where the temperatures are relatively insensitive to the dissipation, the vertical velocities can still vary with the dissipation, potentially impacting physical processes that depend on the local vertical transport."],"url":"http://arxiv.org/abs/2406.02231v1","category":"astro-ph.EP"}
{"created":"2024-06-04 11:48:44","title":"I4VGen: Image as Stepping Stone for Text-to-Video Generation","abstract":"Text-to-video generation has lagged behind text-to-image synthesis in quality and diversity due to the complexity of spatio-temporal modeling and limited video-text datasets. This paper presents I4VGen, a training-free and plug-and-play video diffusion inference framework, which enhances text-to-video generation by leveraging robust image techniques. Specifically, following text-to-image-to-video, I4VGen decomposes the text-to-video generation into two stages: anchor image synthesis and anchor image-guided video synthesis. Correspondingly, a well-designed generation-selection pipeline is employed to achieve visually-realistic and semantically-faithful anchor image, and an innovative Noise-Invariant Video Score Distillation Sampling is incorporated to animate the image to a dynamic video, followed by a video regeneration process to refine the video. This inference strategy effectively mitigates the prevalent issue of non-zero terminal signal-to-noise ratio. Extensive evaluations show that I4VGen not only produces videos with higher visual realism and textual fidelity but also integrates seamlessly into existing image-to-video diffusion models, thereby improving overall video quality.","sentences":["Text-to-video generation has lagged behind text-to-image synthesis in quality and diversity due to the complexity of spatio-temporal modeling and limited video-text datasets.","This paper presents I4VGen, a training-free and plug-and-play video diffusion inference framework, which enhances text-to-video generation by leveraging robust image techniques.","Specifically, following text-to-image-to-video, I4VGen decomposes the text-to-video generation into two stages: anchor image synthesis and anchor image-guided video synthesis.","Correspondingly, a well-designed generation-selection pipeline is employed to achieve visually-realistic and semantically-faithful anchor image, and an innovative Noise-Invariant Video Score Distillation Sampling is incorporated to animate the image to a dynamic video, followed by a video regeneration process to refine the video.","This inference strategy effectively mitigates the prevalent issue of non-zero terminal signal-to-noise ratio.","Extensive evaluations show that I4VGen not only produces videos with higher visual realism and textual fidelity but also integrates seamlessly into existing image-to-video diffusion models, thereby improving overall video quality."],"url":"http://arxiv.org/abs/2406.02230v1","category":"cs.CV"}
{"created":"2024-06-04 11:38:56","title":"Stability for a family of planar systems with nilpotent critical points","abstract":"Consider a family of planar polynomial systems $\\dot x = y^{2l-1} - x^{2k+1}, \\dot y =-x +m y^{2s+1},$ where $l,k,s\\in\\mathbb{N^*},$ $2\\le l \\le 2s$ and $m\\in\\mathbb{R}.$ We study the center-focus problem on its origin which is a monodromic nilpotent critical point. By directly calculating the generalized Lyapunov constants, we find that the origin is always a focus and we complete the classification of its stability. This includes the most difficult case: $s=kl$ and $m=(2k+1)!!/(2kl+1)!_{(2l)}.$ In this case, we prove that the origin is always unstable. Our result extends and completes a previous one.","sentences":["Consider a family of planar polynomial systems $\\dot x = y^{2l-1} - x^{2k+1}, \\dot y =-x +m y^{2s+1},$ where $l,k,s\\in\\mathbb{N^*},$ $2\\le l \\le 2s$ and $m\\in\\mathbb{R}.$ We study the center-focus problem on its origin which is a monodromic nilpotent critical point.","By directly calculating the generalized Lyapunov constants, we find that the origin is always a focus and we complete the classification of its stability.","This includes the most difficult case: $s=kl$ and $m=(2k+1)!!/(2kl+1)!_{(2l)}.$ In this case, we prove that the origin is always unstable.","Our result extends and completes a previous one."],"url":"http://arxiv.org/abs/2406.02226v1","category":"math.DS"}
{"created":"2024-06-04 11:37:11","title":"Riemannian coordinate descent algorithms on matrix manifolds","abstract":"Many machine learning applications are naturally formulated as optimization problems on Riemannian manifolds. The main idea behind Riemannian optimization is to maintain the feasibility of the variables while moving along a descent direction on the manifold. This results in updating all the variables at every iteration. In this work, we provide a general framework for developing computationally efficient coordinate descent (CD) algorithms on matrix manifolds that allows updating only a few variables at every iteration while adhering to the manifold constraint. In particular, we propose CD algorithms for various manifolds such as Stiefel, Grassmann, (generalized) hyperbolic, symplectic, and symmetric positive (semi)definite. While the cost per iteration of the proposed CD algorithms is low, we further develop a more efficient variant via a first-order approximation of the objective function. We analyze their convergence and complexity, and empirically illustrate their efficacy in several applications.","sentences":["Many machine learning applications are naturally formulated as optimization problems on Riemannian manifolds.","The main idea behind Riemannian optimization is to maintain the feasibility of the variables while moving along a descent direction on the manifold.","This results in updating all the variables at every iteration.","In this work, we provide a general framework for developing computationally efficient coordinate descent (CD) algorithms on matrix manifolds that allows updating only a few variables at every iteration while adhering to the manifold constraint.","In particular, we propose CD algorithms for various manifolds such as Stiefel, Grassmann, (generalized) hyperbolic, symplectic, and symmetric positive (semi)definite.","While the cost per iteration of the proposed CD algorithms is low, we further develop a more efficient variant via a first-order approximation of the objective function.","We analyze their convergence and complexity, and empirically illustrate their efficacy in several applications."],"url":"http://arxiv.org/abs/2406.02225v1","category":"math.OC"}
{"created":"2024-06-04 11:36:09","title":"FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models","abstract":"Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, heterogeneous, homogeneous, and one-to-one, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLP text generation tasks. Empirical results demonstrate significant performance improvements in clients' SLMs with the aid of the LLM. Furthermore, the LLM optimized by FedMKT achieves a performance comparable to that achieved through direct fine-tuning based on clients' data, highlighting the effectiveness and adaptability of FedMKT.","sentences":["Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients.","However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs.","To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models.","This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights.","We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance.","Through extensive experiments across three distinct scenarios, heterogeneous, homogeneous, and one-to-one, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLP text generation tasks.","Empirical results demonstrate significant performance improvements in clients' SLMs with the aid of the LLM.","Furthermore, the LLM optimized by FedMKT achieves a performance comparable to that achieved through direct fine-tuning based on clients' data, highlighting the effectiveness and adaptability of FedMKT."],"url":"http://arxiv.org/abs/2406.02224v1","category":"cs.CL"}
{"created":"2024-06-04 11:29:08","title":"A general design method for ultra-long optical path length multipass matrix cells","abstract":"For the first time, we propose a general design method for ultra-long optical path length (OPL) multipass matrix cells (MMCs) based on multi-cycle mode of two-sided field mirrors. The design idea of the dual circulation mode with two-sided field mirrors is elaborated in detail with the example of MMC based on dual Pickett Bradley White cell (PBWC), and the simple design methods of the other three MMCs based on the dual circulation mode of PBWC and Bernstein Herzberg White cell (BHWC) are given. Further, we propose a general design method for ultra-long OPL MMCs with multi-cycle mode by adding cyclic elements. The OPL of the MMCs designed by this method can reach the order of kilometers or even tens of kilometers. The novel MMCs have the advantages of simple structure, strong spot formation regularity, easy expansion, high mirror utilization ratio, high reuse times of spot spatial position, good stability and extremely high ratio of the optical path length to the volume (RLV). In order to evaluate the performance of the new MMCs, an open-path methane gas sensor with the MMC based on triple PBWC was constructed, which was used to continuously measure the methane in the laboratory, and the feasibility, effectiveness and practicability of the new design method were verified. The design method proposed in this paper provides a new idea for the design of multipass cell (MPC), and the new MMCs designed have great potential application value in the field of high-precision trace gas monitoring.","sentences":["For the first time, we propose a general design method for ultra-long optical path length (OPL) multipass matrix cells (MMCs) based on multi-cycle mode of two-sided field mirrors.","The design idea of the dual circulation mode with two-sided field mirrors is elaborated in detail with the example of MMC based on dual Pickett Bradley White cell (PBWC), and the simple design methods of the other three MMCs based on the dual circulation mode of PBWC and Bernstein Herzberg White cell (BHWC) are given.","Further, we propose a general design method for ultra-long OPL MMCs with multi-cycle mode by adding cyclic elements.","The OPL of the MMCs designed by this method can reach the order of kilometers or even tens of kilometers.","The novel MMCs have the advantages of simple structure, strong spot formation regularity, easy expansion, high mirror utilization ratio, high reuse times of spot spatial position, good stability and extremely high ratio of the optical path length to the volume (RLV).","In order to evaluate the performance of the new MMCs, an open-path methane gas sensor with the MMC based on triple PBWC was constructed, which was used to continuously measure the methane in the laboratory, and the feasibility, effectiveness and practicability of the new design method were verified.","The design method proposed in this paper provides a new idea for the design of multipass cell (MPC), and the new MMCs designed have great potential application value in the field of high-precision trace gas monitoring."],"url":"http://arxiv.org/abs/2406.02221v1","category":"physics.optics"}
{"created":"2024-06-04 11:21:10","title":"The Qudit ZH Calculus for Arbitrary Finite Fields: Universality and Application","abstract":"We propose a generalization of the graphical ZH calculus to qudits of prime-power dimensions $q = p^t$, implementing field arithmetic in arbitrary finite fields. This is an extension of a previous result by Roy which implemented arithmetic of prime-sized fields; and an alternative to a result by de Beaudrap which extended the ZH to implement cyclic ring arithmetic in $\\mathbb Z / q\\mathbb Z$ rather than field arithmetic in $\\mathbb F_q$. We show this generalized ZH calculus to be universal over matrices $\\mathbb C^{q^n} \\to \\mathbb C^{q^m}$ with entries in the ring $\\mathbb Z[\\omega]$ where $\\omega$ is a $p$th root of unity. As an illustration of the necessity of such an extension of ZH for field rather than cyclic ring arithmetic, we offer a graphical description and proof for a quantum algorithm for polynomial interpolation. This algorithm relies on the invertibility of multiplication, and therefore can only be described in a graphical language that implements field, rather than ring, multiplication.","sentences":["We propose a generalization of the graphical ZH calculus to qudits of prime-power dimensions $q = p^t$, implementing field arithmetic in arbitrary finite fields.","This is an extension of a previous result by Roy which implemented arithmetic of prime-sized fields; and an alternative to a result by de Beaudrap which extended the ZH to implement cyclic ring arithmetic in $\\mathbb Z / q\\mathbb Z$ rather than field arithmetic in $\\mathbb F_q$. We show this generalized ZH calculus to be universal over matrices $\\mathbb C^{q^n} \\to \\mathbb C^{q^m}$ with entries in the ring $\\mathbb Z[\\omega]$ where $\\omega$ is a $p$th root of unity.","As an illustration of the necessity of such an extension of ZH for field rather than cyclic ring arithmetic, we offer a graphical description and proof for a quantum algorithm for polynomial interpolation.","This algorithm relies on the invertibility of multiplication, and therefore can only be described in a graphical language that implements field, rather than ring, multiplication."],"url":"http://arxiv.org/abs/2406.02219v1","category":"quant-ph"}
{"created":"2024-06-04 11:14:21","title":"SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining","abstract":"Large language models (LLMs) have shown impressive capabilities across various tasks. However, training LLMs from scratch requires significant computational power and extensive memory capacity. Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization. While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace. In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain. The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support. While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning. Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training. Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model.","sentences":["Large language models (LLMs) have shown impressive capabilities across various tasks.","However, training LLMs from scratch requires significant computational power and extensive memory capacity.","Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization.","While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace.","In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain.","The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support.","While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning.","Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training.","Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model."],"url":"http://arxiv.org/abs/2406.02214v1","category":"cs.LG"}
{"created":"2024-06-04 11:11:53","title":"Rectifying Reinforcement Learning for Reward Matching","abstract":"The Generative Flow Network (GFlowNet) is a probabilistic framework in which an agent learns a stochastic policy and flow functions to sample objects with probability proportional to an unnormalized reward function. GFlowNets share a strong resemblance to reinforcement learning (RL), that typically aims to maximize reward, due to their sequential decision-making processes. Recent works have studied connections between GFlowNets and maximum entropy (MaxEnt) RL, which modifies the standard objective of RL agents by learning an entropy-regularized objective. However, a critical theoretical gap persists: despite the apparent similarities in their sequential decision-making nature, a direct link between GFlowNets and standard RL has yet to be discovered, while bridging this gap could further unlock the potential of both fields. In this paper, we establish a new connection between GFlowNets and policy evaluation for a uniform policy. Surprisingly, we find that the resulting value function for the uniform policy has a close relationship to the flows in GFlowNets. Leveraging these insights, we further propose a novel rectified policy evaluation (RPE) algorithm, which achieves the same reward-matching effect as GFlowNets, offering a new perspective. We compare RPE, MaxEnt RL, and GFlowNets in a number of benchmarks, and show that RPE achieves competitive results compared to previous approaches. This work sheds light on the previously unexplored connection between (non-MaxEnt) RL and GFlowNets, potentially opening new avenues for future research in both fields.","sentences":["The Generative Flow Network (GFlowNet) is a probabilistic framework in which an agent learns a stochastic policy and flow functions to sample objects with probability proportional to an unnormalized reward function.","GFlowNets share a strong resemblance to reinforcement learning (RL), that typically aims to maximize reward, due to their sequential decision-making processes.","Recent works have studied connections between GFlowNets and maximum entropy (MaxEnt) RL, which modifies the standard objective of RL agents by learning an entropy-regularized objective.","However, a critical theoretical gap persists: despite the apparent similarities in their sequential decision-making nature, a direct link between GFlowNets and standard RL has yet to be discovered, while bridging this gap could further unlock the potential of both fields.","In this paper, we establish a new connection between GFlowNets and policy evaluation for a uniform policy.","Surprisingly, we find that the resulting value function for the uniform policy has a close relationship to the flows in GFlowNets.","Leveraging these insights, we further propose a novel rectified policy evaluation (RPE) algorithm, which achieves the same reward-matching effect as GFlowNets, offering a new perspective.","We compare RPE, MaxEnt RL, and GFlowNets in a number of benchmarks, and show that RPE achieves competitive results compared to previous approaches.","This work sheds light on the previously unexplored connection between (non-MaxEnt) RL and GFlowNets, potentially opening new avenues for future research in both fields."],"url":"http://arxiv.org/abs/2406.02213v1","category":"cs.LG"}
{"created":"2024-06-04 11:11:03","title":"Generative Pre-Trained Diffusion Paradigm for Zero-Shot Time Series Forecasting","abstract":"In recent years, generative pre-trained paradigms such as Large Language Models (LLMs) and Large Vision Models (LVMs) have achieved revolutionary advancements and widespread real-world applications. Particularly, the emergence of pre-trained LLMs-based temporal works, compared to previous deep model approaches, has demonstrated superior generalization and robustness, showcasing the potential of generative pre-trained paradigms as foundation models for time series. However, those LLMs-based works mainly focus on cross-modal research, i.e., leveraging the language capabilities of LLMs in time series contexts. Although they have achieved impressive performance, there still exist the issues of concept drift caused by differences in data distribution and inflexibility caused by misalignment of dimensions. To this end, inspired by recent work on LVMs, we reconsider the paradigm of time series modeling. In this paper, we comprehensively explore, for the first time, the effectiveness and superiority of the Generative Pre-trained Diffusion (GPD) paradigm in real-world multivariate time series forecasting (TSF). Specifically, to mitigate performance bias introduced by sophisticated networks, we propose a straightforward MLP diffusion network for unconditional modeling of time series. Then we employ a zero-shot and tuning-free method to predict (generate) future data using historical data as prompts. The GPD paradigm is established on the time series modality, effectively preventing the phenomenon of concept drift, and enabling flexible forecasting of arbitrary lengths. We demonstrate that the GPD paradigm achieves comprehensive performance and generalization comparable to current SOTA LLM-based and deep model paradigms on mainstream benchmarks and various TSF tasks. Extensive experiments validate the potential of the GPD paradigm and its assistance in future related research.","sentences":["In recent years, generative pre-trained paradigms such as Large Language Models (LLMs) and Large Vision Models (LVMs) have achieved revolutionary advancements and widespread real-world applications.","Particularly, the emergence of pre-trained LLMs-based temporal works, compared to previous deep model approaches, has demonstrated superior generalization and robustness, showcasing the potential of generative pre-trained paradigms as foundation models for time series.","However, those LLMs-based works mainly focus on cross-modal research, i.e., leveraging the language capabilities of LLMs in time series contexts.","Although they have achieved impressive performance, there still exist the issues of concept drift caused by differences in data distribution and inflexibility caused by misalignment of dimensions.","To this end, inspired by recent work on LVMs, we reconsider the paradigm of time series modeling.","In this paper, we comprehensively explore, for the first time, the effectiveness and superiority of the Generative Pre-trained Diffusion (GPD) paradigm in real-world multivariate time series forecasting (TSF).","Specifically, to mitigate performance bias introduced by sophisticated networks, we propose a straightforward MLP diffusion network for unconditional modeling of time series.","Then we employ a zero-shot and tuning-free method to predict (generate) future data using historical data as prompts.","The GPD paradigm is established on the time series modality, effectively preventing the phenomenon of concept drift, and enabling flexible forecasting of arbitrary lengths.","We demonstrate that the GPD paradigm achieves comprehensive performance and generalization comparable to current SOTA LLM-based and deep model paradigms on mainstream benchmarks and various TSF tasks.","Extensive experiments validate the potential of the GPD paradigm and its assistance in future related research."],"url":"http://arxiv.org/abs/2406.02212v1","category":"cs.CE"}
{"created":"2024-06-04 11:08:37","title":"Novel pre-emptive control solutions for V2X connected electric vehicles","abstract":"V2X technologies will become widespread in the next generation of passenger cars, and enable the development of novel vehicle control functionalities. Although a wide literature describes the energy efficiency benefits of V2X connectivity, e.g., in terms of vehicle speed profiling and platooning, there is a gap in the analysis of the potential of vehicle connectivity in enhancing the performance of active safety control systems. To highlight the impact vehicle connectivity could have on future active safety systems, this paper presents two novel control functions for connected vehicles, benefitting from the precise knowledge of the expected path and tire-road friction conditions ahead, as well as the current position of the ego vehicle. These functions, developed within recent and ongoing European projects, are: i) pre-emptive traction control; and ii) pre-emptive braking control.","sentences":["V2X technologies will become widespread in the next generation of passenger cars, and enable the development of novel vehicle control functionalities.","Although a wide literature describes the energy efficiency benefits of V2X connectivity, e.g., in terms of vehicle speed profiling and platooning, there is a gap in the analysis of the potential of vehicle connectivity in enhancing the performance of active safety control systems.","To highlight the impact vehicle connectivity could have on future active safety systems, this paper presents two novel control functions for connected vehicles, benefitting from the precise knowledge of the expected path and tire-road friction conditions ahead, as well as the current position of the ego vehicle.","These functions, developed within recent and ongoing European projects, are: i) pre-emptive traction control; and ii) pre-emptive braking control."],"url":"http://arxiv.org/abs/2406.02211v1","category":"eess.SY"}
{"created":"2024-06-04 11:06:13","title":"Why Only Text: Empowering Vision-and-Language Navigation with Multi-modal Prompts","abstract":"Current Vision-and-Language Navigation (VLN) tasks mainly employ textual instructions to guide agents. However, being inherently abstract, the same textual instruction can be associated with different visual signals, causing severe ambiguity and limiting the transfer of prior knowledge in the vision domain from the user to the agent. To fill this gap, we propose Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task augmenting traditional VLN by integrating both natural language and images in instructions. VLN-MP not only maintains backward compatibility by effectively handling text-only prompts but also consistently shows advantages with different quantities and relevance of visual prompts. Possible forms of visual prompts include both exact and similar object images, providing adaptability and versatility in diverse navigation scenarios. To evaluate VLN-MP under a unified framework, we implement a new benchmark that offers: (1) a training-free pipeline to transform textual instructions into multi-modal forms with landmark images; (2) diverse datasets with multi-modal instructions for different downstream tasks; (3) a novel module designed to process various image prompts for seamless integration with state-of-the-art VLN models. Extensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show that incorporating visual prompts significantly boosts navigation performance. While maintaining efficiency with text-only prompts, VLN-MP enables agents to navigate in the pre-explore setting and outperform text-based models, showing its broader applicability.","sentences":["Current Vision-and-Language Navigation (VLN) tasks mainly employ textual instructions to guide agents.","However, being inherently abstract, the same textual instruction can be associated with different visual signals, causing severe ambiguity and limiting the transfer of prior knowledge in the vision domain from the user to the agent.","To fill this gap, we propose Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task augmenting traditional VLN by integrating both natural language and images in instructions.","VLN-MP not only maintains backward compatibility by effectively handling text-only prompts but also consistently shows advantages with different quantities and relevance of visual prompts.","Possible forms of visual prompts include both exact and similar object images, providing adaptability and versatility in diverse navigation scenarios.","To evaluate VLN-MP under a unified framework, we implement a new benchmark that offers: (1) a training-free pipeline to transform textual instructions into multi-modal forms with landmark images; (2) diverse datasets with multi-modal instructions for different downstream tasks; (3) a novel module designed to process various image prompts for seamless integration with state-of-the-art VLN models.","Extensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show that incorporating visual prompts significantly boosts navigation performance.","While maintaining efficiency with text-only prompts, VLN-MP enables agents to navigate in the pre-explore setting and outperform text-based models, showing its broader applicability."],"url":"http://arxiv.org/abs/2406.02208v1","category":"cs.CV"}
{"created":"2024-06-04 11:02:15","title":"Query-Enhanced Adaptive Semantic Path Reasoning for Inductive Knowledge Graph Completion","abstract":"Conventional Knowledge graph completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Query-Enhanced Adaptive Semantic Path Reasoning (QASPR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed QASPR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, QASPR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that QASPR achieves state-of-the-art performance.","sentences":["Conventional Knowledge graph completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities.","Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability.","While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths.","To address these challenges, this paper proposes the Query-Enhanced Adaptive Semantic Path Reasoning (QASPR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task.","Specifically, the proposed QASPR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets.","Additionally, QASPR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs.","The experimental results demonstrate that QASPR achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2406.02205v1","category":"cs.AI"}
{"created":"2024-06-04 10:59:54","title":"The Deep Latent Space Particle Filter for Real-Time Data Assimilation with Uncertainty Quantification","abstract":"In Data Assimilation, observations are fused with simulations to obtain an accurate estimate of the state and parameters for a given physical system. Combining data with a model, however, while accurately estimating uncertainty, is computationally expensive and infeasible to run in real-time for complex systems. Here, we present a novel particle filter methodology, the Deep Latent Space Particle filter or D-LSPF, that uses neural network-based surrogate models to overcome this computational challenge. The D-LSPF enables filtering in the low-dimensional latent space obtained using Wasserstein AEs with modified vision transformer layers for dimensionality reduction and transformers for parameterized latent space time stepping. As we demonstrate on three test cases, including leak localization in multi-phase pipe flow and seabed identification for fully nonlinear water waves, the D-LSPF runs orders of magnitude faster than a high-fidelity particle filter and 3-5 times faster than alternative methods while being up to an order of magnitude more accurate. The D-LSPF thus enables real-time data assimilation with uncertainty quantification for physical systems.","sentences":["In Data Assimilation, observations are fused with simulations to obtain an accurate estimate of the state and parameters for a given physical system.","Combining data with a model, however, while accurately estimating uncertainty, is computationally expensive and infeasible to run in real-time for complex systems.","Here, we present a novel particle filter methodology, the Deep Latent Space Particle filter or D-LSPF, that uses neural network-based surrogate models to overcome this computational challenge.","The D-LSPF enables filtering in the low-dimensional latent space obtained using Wasserstein AEs with modified vision transformer layers for dimensionality reduction and transformers for parameterized latent space time stepping.","As we demonstrate on three test cases, including leak localization in multi-phase pipe flow and seabed identification for fully nonlinear water waves, the D-LSPF runs orders of magnitude faster than a high-fidelity particle filter and 3-5 times faster than alternative methods while being up to an order of magnitude more accurate.","The D-LSPF thus enables real-time data assimilation with uncertainty quantification for physical systems."],"url":"http://arxiv.org/abs/2406.02204v1","category":"cs.CE"}
{"created":"2024-06-04 10:57:59","title":"Can CLIP help CLIP in learning 3D?","abstract":"In this study, we explore an alternative approach to enhance contrastive text-image-3D alignment in the absence of textual descriptions for 3D objects. We introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP knowledge about textual and 2D data to compute the neural perceived similarity between two 3D samples. We employ the proposed methods to mine 3D hard negatives, establishing a multimodal contrastive pipeline with hard negative weighting via a custom loss function. We train on different configurations of the proposed hard negative mining approach, and we evaluate the accuracy of our models in 3D classification and on the cross-modal retrieval benchmark, testing image-to-shape and shape-to-image retrieval. Results demonstrate that our approach, even without explicit text alignment, achieves comparable or superior performance on zero-shot and standard 3D classification, while significantly improving both image-to-shape and shape-to-image retrieval compared to previous methods.","sentences":["In this study, we explore an alternative approach to enhance contrastive text-image-3D alignment in the absence of textual descriptions for 3D objects.","We introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP knowledge about textual and 2D data to compute the neural perceived similarity between two 3D samples.","We employ the proposed methods to mine 3D hard negatives, establishing a multimodal contrastive pipeline with hard negative weighting via a custom loss function.","We train on different configurations of the proposed hard negative mining approach, and we evaluate the accuracy of our models in 3D classification and on the cross-modal retrieval benchmark, testing image-to-shape and shape-to-image retrieval.","Results demonstrate that our approach, even without explicit text alignment, achieves comparable or superior performance on zero-shot and standard 3D classification, while significantly improving both image-to-shape and shape-to-image retrieval compared to previous methods."],"url":"http://arxiv.org/abs/2406.02202v1","category":"cs.CV"}
{"created":"2024-06-04 10:53:16","title":"Communication Complexity of Graph Isomorphism, Coloring, and Distance Games","abstract":"In quantum information, nonlocal games are particularly useful for differentiating classical, quantum, and non-signalling correlations. An example of differentiation is given by the principle of no-collapse of communication complexity, which is often interpreted as necessary for a feasible physical theory. It is satisfied by quantum correlations but violated by some non-signalling ones.   In this work, we investigate this principle in the context of three nonlocal games related to graph theory, starting from the well-known graph isomorphism and graph coloring games, and introducing a new game, the vertex distance game, with a parameter $D\\in\\mathbb N$, that generalizes the former two to some extent. For these three games, we prove that perfect non-signalling strategies collapse communication complexity under favorable conditions. We also define a refinement of fractional isomorphism of graphs, namely D-fractional isomorphisms, and we show that this characterizes perfect non-signalling strategies for the vertex distance game. Surprisingly, we observe that non-signalling strategies provide a finer distinction for the new game compared to classical and quantum strategies since the parameter D is visible only in the non-signalling setting.","sentences":["In quantum information, nonlocal games are particularly useful for differentiating classical, quantum, and non-signalling correlations.","An example of differentiation is given by the principle of no-collapse of communication complexity, which is often interpreted as necessary for a feasible physical theory.","It is satisfied by quantum correlations but violated by some non-signalling ones.   ","In this work, we investigate this principle in the context of three nonlocal games related to graph theory, starting from the well-known graph isomorphism and graph coloring games, and introducing a new game, the vertex distance game, with a parameter $D\\in\\mathbb N$, that generalizes the former two to some extent.","For these three games, we prove that perfect non-signalling strategies collapse communication complexity under favorable conditions.","We also define a refinement of fractional isomorphism of graphs, namely D-fractional isomorphisms, and we show that this characterizes perfect non-signalling strategies for the vertex distance game.","Surprisingly, we observe that non-signalling strategies provide a finer distinction for the new game compared to classical and quantum strategies since the parameter D is visible only in the non-signalling setting."],"url":"http://arxiv.org/abs/2406.02199v1","category":"quant-ph"}
{"created":"2024-06-04 10:52:12","title":"Nonlinear Model Predictive Control for Enhanced Path Tracking and Autonomous Drifting through Direct Yaw Moment Control and Rear-Wheel-Steering","abstract":"Path tracking (PT) controllers capable of replicating race driving techniques, such as drifting beyond the limits of handling, have the potential of enhancing active safety in critical conditions. This paper presents a nonlinear model predictive control (NMPC) approach that integrates multiple actuation methods, namely four-wheel-steering, longitudinal tyre force distribution, and direct yaw moment control, to execute drifting when this is beneficial for PT in emergency scenarios. Simulation results of challenging manoeuvres, based on an experimentally validated vehicle model, highlight the substantial PT performance improvements brought by: i) vehicle operation outside the envelope enforced by the current generation of stability controllers; and ii) the integrated control of multiple actuators.","sentences":["Path tracking (PT) controllers capable of replicating race driving techniques, such as drifting beyond the limits of handling, have the potential of enhancing active safety in critical conditions.","This paper presents a nonlinear model predictive control (NMPC) approach that integrates multiple actuation methods, namely four-wheel-steering, longitudinal tyre force distribution, and direct yaw moment control, to execute drifting when this is beneficial for PT in emergency scenarios.","Simulation results of challenging manoeuvres, based on an experimentally validated vehicle model, highlight the substantial PT performance improvements brought by: i) vehicle operation outside the envelope enforced by the current generation of stability controllers; and ii) the integrated control of multiple actuators."],"url":"http://arxiv.org/abs/2406.02198v1","category":"eess.SY"}
{"created":"2024-06-04 10:45:07","title":"A simple characterization of Quillen adjunctions","abstract":"We observe that an enriched right adjoint functor between model categories which preserves acyclic fibrations and fibrant objects is quite generically a right Quillen functor.","sentences":["We observe that an enriched right adjoint functor between model categories which preserves acyclic fibrations and fibrant objects is quite generically a right Quillen functor."],"url":"http://arxiv.org/abs/2406.02194v1","category":"math.AT"}
{"created":"2024-06-04 10:37:26","title":"Uniform Resolvent Estimates for Subwavelength Resonators: The Minnaert Bubble Case","abstract":"Subwavelength resonators are small scaled objects that exhibit contrasting medium properties (eigher in intensity or sign) while compared to the ones of a uniform background. Such contrasts allow them to resonate at specific frequencies. There are two ways to mathematically define these resonances. First, as the frequencies for which the related system of integral equations is not injective. Second, as the frequencies for which the related resolvent operator of the natural Hamiltonian has a pole. In this work, we consider, as the subwavelength resonator, the Minneart bubble. We show that these two mentioned definitions are equivalent. Most importantly,   1. we derive the related resolvent estimates which are uniform in terms of the size/contrast of the resonators. As a by product, we show that the resolvent operators have no resonances in the upper half complex plane while they exhibit two resonances in the lower half plane which converge to the real axis, as the size of the bubble tends to zero. These resonances are related to the Minnaert frequency (which constitutes their dominating real part).   2. we derive the asymptotic estimates of the generated scattered fields which are uniform in terms of the incident frequency and which are valid everywhere in space (i.e. inside or outside the bubble).   \\end{enumerate} The dominating parts, for both the resolvent operator and the scattered fields, are given by the ones of the point-scatterer supported at the location of the bubble. In particular, these dominant parts are non trivial (not the same as those of the background medium) only if the used incident frequency identifies with the Minnaert one.","sentences":["Subwavelength resonators are small scaled objects that exhibit contrasting medium properties (eigher in intensity or sign) while compared to the ones of a uniform background.","Such contrasts allow them to resonate at specific frequencies.","There are two ways to mathematically define these resonances.","First, as the frequencies for which the related system of integral equations is not injective.","Second, as the frequencies for which the related resolvent operator of the natural Hamiltonian has a pole.","In this work, we consider, as the subwavelength resonator, the Minneart bubble.","We show that these two mentioned definitions are equivalent.","Most importantly,   1.","we derive the related resolvent estimates which are uniform in terms of the size/contrast of the resonators.","As a by product, we show that the resolvent operators have no resonances in the upper half complex plane while they exhibit two resonances in the lower half plane which converge to the real axis, as the size of the bubble tends to zero.","These resonances are related to the Minnaert frequency (which constitutes their dominating real part).   ","2.","we derive the asymptotic estimates of the generated scattered fields which are uniform in terms of the incident frequency and which are valid everywhere in space (i.e. inside or outside the bubble).   ","\\end{enumerate} The dominating parts, for both the resolvent operator and the scattered fields, are given by the ones of the point-scatterer supported at the location of the bubble.","In particular, these dominant parts are non trivial (not the same as those of the background medium) only if the used incident frequency identifies with the Minnaert one."],"url":"http://arxiv.org/abs/2406.02192v1","category":"math.AP"}
{"created":"2024-06-04 10:35:16","title":"On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data","abstract":"We consider the effect of temporal aggregation on instantaneous (non-temporal) causal discovery in general setting. This is motivated by the observation that the true causal time lag is often considerably shorter than the observational interval. This discrepancy leads to high aggregation, causing time-delay causality to vanish and instantaneous dependence to manifest. Although we expect such instantaneous dependence has consistency with the true causal relation in certain sense to make the discovery results meaningful, it remains unclear what type of consistency we need and when will such consistency be satisfied. We proposed functional consistency and conditional independence consistency in formal way correspond functional causal model-based methods and conditional independence-based methods respectively and provide the conditions under which these consistencies will hold. We show theoretically and experimentally that causal discovery results may be seriously distorted by aggregation especially in complete nonlinear case and we also find causal relationship still recoverable from aggregated data if we have partial linearity or appropriate prior. Our findings suggest community should take a cautious and meticulous approach when interpreting causal discovery results from such data and show why and when aggregation will distort the performance of causal discovery methods.","sentences":["We consider the effect of temporal aggregation on instantaneous (non-temporal) causal discovery in general setting.","This is motivated by the observation that the true causal time lag is often considerably shorter than the observational interval.","This discrepancy leads to high aggregation, causing time-delay causality to vanish and instantaneous dependence to manifest.","Although we expect such instantaneous dependence has consistency with the true causal relation in certain sense to make the discovery results meaningful, it remains unclear what type of consistency we need and when will such consistency be satisfied.","We proposed functional consistency and conditional independence consistency in formal way correspond functional causal model-based methods and conditional independence-based methods respectively and provide the conditions under which these consistencies will hold.","We show theoretically and experimentally that causal discovery results may be seriously distorted by aggregation especially in complete nonlinear case and we also find causal relationship still recoverable from aggregated data if we have partial linearity or appropriate prior.","Our findings suggest community should take a cautious and meticulous approach when interpreting causal discovery results from such data and show why and when aggregation will distort the performance of causal discovery methods."],"url":"http://arxiv.org/abs/2406.02191v1","category":"stat.ML"}
{"created":"2024-06-04 10:34:40","title":"Fast and Scalable Multi-Kernel Encoder Classifier","abstract":"This paper introduces a new kernel-based classifier by viewing kernel matrices as generalized graphs and leveraging recent progress in graph embedding techniques. The proposed method facilitates fast and scalable kernel matrix embedding, and seamlessly integrates multiple kernels to enhance the learning process. Our theoretical analysis offers a population-level characterization of this approach using random variables. Empirically, our method demonstrates superior running time compared to standard approaches such as support vector machines and two-layer neural network, while achieving comparable classification accuracy across various simulated and real datasets.","sentences":["This paper introduces a new kernel-based classifier by viewing kernel matrices as generalized graphs and leveraging recent progress in graph embedding techniques.","The proposed method facilitates fast and scalable kernel matrix embedding, and seamlessly integrates multiple kernels to enhance the learning process.","Our theoretical analysis offers a population-level characterization of this approach using random variables.","Empirically, our method demonstrates superior running time compared to standard approaches such as support vector machines and two-layer neural network, while achieving comparable classification accuracy across various simulated and real datasets."],"url":"http://arxiv.org/abs/2406.02189v1","category":"cs.LG"}
{"created":"2024-06-04 10:33:29","title":"Polyadic supersymmetry","abstract":"We introduce a polyadic analog of supersymmetry by considering the polyadization procedure (proposed by the author) applied to the toy model of one-dimensional supersymmetric quantum mechanics. The supercharges are generalized to polyadic ones using the $n$-ary sigma matrices defined in earlier work. In this way, polyadic analogs of supercharges and Hamiltonians take the cyclic shift block matrix form, and they can describe multidegenerated quantum states in a way that is different from the $N$-extended and multigraded SQM. While constructing the corresponding supersymmetry as an $n$-ary Lie superalgebra ($n$ is the arity of the initial associative multiplication), we have found new brackets with a reduced arity of $2\\leq m<n$ and a related series of $m$-ary superalgebras (which is impossible for binary superalgebras). In the case of even reduced arity $m$ we obtain a tower of higher order (as differential operators) even Hamiltonians, while for $m$ odd we get a tower of higher order odd supercharges, and the corresponding algebra consists of the odd sector only.","sentences":["We introduce a polyadic analog of supersymmetry by considering the polyadization procedure (proposed by the author) applied to the toy model of one-dimensional supersymmetric quantum mechanics.","The supercharges are generalized to polyadic ones using the $n$-ary sigma matrices defined in earlier work.","In this way, polyadic analogs of supercharges and Hamiltonians take the cyclic shift block matrix form, and they can describe multidegenerated quantum states in a way that is different from the $N$-extended and multigraded SQM.","While constructing the corresponding supersymmetry as an $n$-ary Lie superalgebra ($n$ is the arity of the initial associative multiplication), we have found new brackets with a reduced arity of $2\\leq m<n$ and a related series of $m$-ary superalgebras (which is impossible for binary superalgebras).","In the case of even reduced arity $m$ we obtain a tower of higher order (as differential operators) even Hamiltonians, while for $m$ odd we get a tower of higher order odd supercharges, and the corresponding algebra consists of the odd sector only."],"url":"http://arxiv.org/abs/2406.02188v1","category":"hep-th"}
{"created":"2024-06-04 10:31:03","title":"DNCs Require More Planning Steps","abstract":"Many recent works use machine learning models to solve various complex algorithmic problems. However, these models attempt to reach a solution without considering the problem's required computational complexity, which can be detrimental to their ability to solve it correctly. In this work we investigate the effect of computational time and memory on generalization of implicit algorithmic solvers. To do so, we focus on the Differentiable Neural Computer (DNC), a general problem solver that also lets us reason directly about its usage of time and memory. In this work, we argue that the number of planning steps the model is allowed to take, which we call \"planning budget\", is a constraint that can cause the model to generalize poorly and hurt its ability to fully utilize its external memory. We evaluate our method on Graph Shortest Path, Convex Hull, Graph MinCut and Associative Recall, and show how the planning budget can drastically change the behavior of the learned algorithm, in terms of learned time complexity, training time, stability and generalization to inputs larger than those seen during training.","sentences":["Many recent works use machine learning models to solve various complex algorithmic problems.","However, these models attempt to reach a solution without considering the problem's required computational complexity, which can be detrimental to their ability to solve it correctly.","In this work we investigate the effect of computational time and memory on generalization of implicit algorithmic solvers.","To do so, we focus on the Differentiable Neural Computer (DNC), a general problem solver that also lets us reason directly about its usage of time and memory.","In this work, we argue that the number of planning steps the model is allowed to take, which we call \"planning budget\", is a constraint that can cause the model to generalize poorly and hurt its ability to fully utilize its external memory.","We evaluate our method on Graph Shortest Path, Convex Hull, Graph MinCut and Associative Recall, and show how the planning budget can drastically change the behavior of the learned algorithm, in terms of learned time complexity, training time, stability and generalization to inputs larger than those seen during training."],"url":"http://arxiv.org/abs/2406.02187v1","category":"cs.LG"}
{"created":"2024-06-04 10:29:18","title":"GraVITON: Graph based garment warping with attention guided inversion for Virtual-tryon","abstract":"Virtual try-on, a rapidly evolving field in computer vision, is transforming e-commerce by improving customer experiences through precise garment warping and seamless integration onto the human body. While existing methods such as TPS and flow address the garment warping but overlook the finer contextual details. In this paper, we introduce a novel graph based warping technique which emphasizes the value of context in garment flow. Our graph based warping module generates warped garment as well as a coarse person image, which is utilised by a simple refinement network to give a coarse virtual tryon image. The proposed work exploits latent diffusion model to generate the final tryon, treating garment transfer as an inpainting task. The diffusion model is conditioned with decoupled cross attention based inversion of visual and textual information. We introduce an occlusion aware warping constraint that generates dense warped garment, without any holes and occlusion. Our method, validated on VITON-HD and Dresscode datasets, showcases substantial state-of-the-art qualitative and quantitative results showing considerable improvement in garment warping, texture preservation, and overall realism.","sentences":["Virtual try-on, a rapidly evolving field in computer vision, is transforming e-commerce by improving customer experiences through precise garment warping and seamless integration onto the human body.","While existing methods such as TPS and flow address the garment warping but overlook the finer contextual details.","In this paper, we introduce a novel graph based warping technique which emphasizes the value of context in garment flow.","Our graph based warping module generates warped garment as well as a coarse person image, which is utilised by a simple refinement network to give a coarse virtual tryon image.","The proposed work exploits latent diffusion model to generate the final tryon, treating garment transfer as an inpainting task.","The diffusion model is conditioned with decoupled cross attention based inversion of visual and textual information.","We introduce an occlusion aware warping constraint that generates dense warped garment, without any holes and occlusion.","Our method, validated on VITON-HD and Dresscode datasets, showcases substantial state-of-the-art qualitative and quantitative results showing considerable improvement in garment warping, texture preservation, and overall realism."],"url":"http://arxiv.org/abs/2406.02184v1","category":"cs.CV"}
{"created":"2024-06-04 10:22:21","title":"Plasmonic properties of electrochromic doped metal oxides investigated through Kubelka Munk formalism","abstract":"Materials with broadband tunable optical properties are looked for in smart windows applications. Doped metal oxides presenting dual band visible (VIS) near infrared (NIR) electrochromic properties can be used for solving such a challenge, and their accurate optical characterization is therefore of prime importance. Kubelka Munk model is a state of the art way to optically quantify the absorption properties of materials and is occasionally applied to plasmonic materials, even if great care should be taken to meet the formalism hypotheses. In the present work, Kubelka Munk theory is discussed in the context of particles of indium tin oxide and molybdenum tungsten oxide formulations that are used as single NIR and both VIS and NIR active advanced electrochromic materials, respectively. An analytical model is derived for particles of much smaller dimensions than the incident wavelength and is experimentally verified. A dilution method is applied to verify the plasmonic characteristics of the particles. This study is key for efficient characterization of optical properties of metal oxides, and plasmonic materials in general, from diffuse reflectance measurements.","sentences":["Materials with broadband tunable optical properties are looked for in smart windows applications.","Doped metal oxides presenting dual band visible (VIS) near infrared (NIR) electrochromic properties can be used for solving such a challenge, and their accurate optical characterization is therefore of prime importance.","Kubelka Munk model is a state of the art way to optically quantify the absorption properties of materials and is occasionally applied to plasmonic materials, even if great care should be taken to meet the formalism hypotheses.","In the present work, Kubelka Munk theory is discussed in the context of particles of indium tin oxide and molybdenum tungsten oxide formulations that are used as single NIR and both VIS and NIR active advanced electrochromic materials, respectively.","An analytical model is derived for particles of much smaller dimensions than the incident wavelength and is experimentally verified.","A dilution method is applied to verify the plasmonic characteristics of the particles.","This study is key for efficient characterization of optical properties of metal oxides, and plasmonic materials in general, from diffuse reflectance measurements."],"url":"http://arxiv.org/abs/2406.02181v1","category":"physics.optics"}
{"created":"2024-06-04 10:22:12","title":"On The Statistical Representation Properties Of The Perturb-Softmax And The Perturb-Argmax Probability Distributions","abstract":"The Gumbel-Softmax probability distribution allows learning discrete tokens in generative learning, while the Gumbel-Argmax probability distribution is useful in learning discrete structures in discriminative learning. Despite the efforts invested in optimizing these probability models, their statistical properties are under-explored. In this work, we investigate their representation properties and determine for which families of parameters these probability distributions are complete, i.e., can represent any probability distribution, and minimal, i.e., can represent a probability distribution uniquely. We rely on convexity and differentiability to determine these statistical conditions and extend this framework to general probability models, such as Gaussian-Softmax and Gaussian-Argmax. We experimentally validate the qualities of these extensions, which enjoy a faster convergence rate. We conclude the analysis by identifying two sets of parameters that satisfy these assumptions and thus admit a complete and minimal representation. Our contribution is theoretical with supporting practical evaluation.","sentences":["The Gumbel-Softmax probability distribution allows learning discrete tokens in generative learning, while the Gumbel-Argmax probability distribution is useful in learning discrete structures in discriminative learning.","Despite the efforts invested in optimizing these probability models, their statistical properties are under-explored.","In this work, we investigate their representation properties and determine for which families of parameters these probability distributions are complete, i.e., can represent any probability distribution, and minimal, i.e., can represent a probability distribution uniquely.","We rely on convexity and differentiability to determine these statistical conditions and extend this framework to general probability models, such as Gaussian-Softmax and Gaussian-Argmax.","We experimentally validate the qualities of these extensions, which enjoy a faster convergence rate.","We conclude the analysis by identifying two sets of parameters that satisfy these assumptions and thus admit a complete and minimal representation.","Our contribution is theoretical with supporting practical evaluation."],"url":"http://arxiv.org/abs/2406.02180v1","category":"cs.LG"}
{"created":"2024-06-04 10:20:16","title":"Abundances of trace constituents in Jupiter's atmosphere inferred from Herschel/PACS observations","abstract":"$Context.$ On October 31, 2009, the Photodetector Array Camera and Spectrometer (PACS) on board the Herschel Space Observatory observed far-infrared spectra of Jupiter between 50 and 220$\\,\\mu$m as part of the program \"Water and Related Chemistry in the Solar System\". $Aims.$ We investigate the disk-averaged chemical composition of Jupiter's atmosphere as a function of height using these observations. $Methods.$ We used the Planetary Spectrum Generator (PSG) and the least-squares fitting technique to infer the abundances of trace constituents. $Results.$ The PACS data include numerous spectral lines attributable to ammonia (NH$_3$), methane (CH$_4$), phosphine (PH$_3$), water (H$_2$O), and deuterated hydrogen (HD) in the Jovian atmosphere. We infer an ammonia abundance profile that decreases from a mole fraction of $(1.7\\pm 0.8)\\times 10^{-4}$ at $p\\sim 900\\,$mbar to $(1.7\\pm 0.9)\\times 10^{-8}$ at $p\\sim 275\\,$mbar, following a fractional scale height of about 0.114. For phosphine, we find a mole fraction of $(7.2\\pm 1.2)\\times 10^{-7}$ at pressures higher than $(550\\pm 100)\\,$mbar and a decrease of its abundance at lower pressures following a fractional scale height of $(0.09\\pm 0.02)$. Our analysis delivers a methane mole fraction of $(1.49\\pm 0.09)\\times 10^{-3}$. Analyzing the HD $R(0)$ line at $112.1\\,\\mu$m yields a new measurement of Jupiter's D/H ratio, $\\text{D/H}=(1.5\\pm 0.6)\\times 10^{-5}$. Finally, the PACS data allow us to put the most stringent $3\\sigma$ upper limits yet on the mole fractions of hydrogen halides in the Jovian troposphere. These new upper limits are $<1.1\\times 10^{-11}$ for hydrogen fluoride (HF), $<6.0\\times 10^{-11}$ for hydrogen chloride (HCl), $<2.3\\times 10^{-10}$ for hydrogen bromide (HBr) and $<1.2\\times 10^{-9}$ for hydrogen iodide (HI) and support the proposed condensation of hydrogen halides into ammonium halide salts in the Jovian troposphere.","sentences":["$Context.$ On October 31, 2009, the Photodetector Array Camera and Spectrometer (PACS) on board the Herschel Space Observatory observed far-infrared spectra of Jupiter between 50 and 220$\\,\\mu$m as part of the program \"Water and Related Chemistry in the Solar System\".","$Aims.$ We investigate the disk-averaged chemical composition of Jupiter's atmosphere as a function of height using these observations.","$Methods.$ We used the Planetary Spectrum Generator (PSG) and the least-squares fitting technique to infer the abundances of trace constituents.","$Results.$ The PACS data include numerous spectral lines attributable to ammonia (NH$_3$), methane (CH$_4$), phosphine (PH$_3$), water (H$_2$O), and deuterated hydrogen (HD) in the Jovian atmosphere.","We infer an ammonia abundance profile that decreases from a mole fraction of $(1.7\\pm 0.8)\\times 10^{-4}$ at $p\\sim 900\\,$mbar to $(1.7\\pm 0.9)\\times 10^{-8}$ at $p\\sim 275\\,$mbar, following a fractional scale height of about 0.114.","For phosphine, we find a mole fraction of $(7.2\\pm 1.2)\\times 10^{-7}$ at pressures higher than $(550\\pm 100)\\,$mbar and a decrease of its abundance at lower pressures following a fractional scale height of $(0.09\\pm 0.02)$. Our analysis delivers a methane mole fraction of $(1.49\\pm 0.09)\\times 10^{-3}$.","Analyzing the HD $R(0)$ line at $112.1\\,\\mu$m yields a new measurement of Jupiter's D/H ratio, $\\text{D/H}=(1.5\\pm 0.6)\\times 10^{-5}$. Finally, the PACS data allow us to put the most stringent $3\\sigma$ upper limits yet on the mole fractions of hydrogen halides in the Jovian troposphere.","These new upper limits are $<1.1\\times 10^{-11}$ for hydrogen fluoride (HF), $<6.0\\times 10^{-11}$ for hydrogen chloride (HCl), $<2.3\\times 10^{-10}$ for hydrogen bromide (HBr) and $<1.2\\times 10^{-9}$ for hydrogen iodide (HI) and support the proposed condensation of hydrogen halides into ammonium halide salts in the Jovian troposphere."],"url":"http://arxiv.org/abs/2406.02179v1","category":"astro-ph.EP"}
{"created":"2024-06-04 10:19:14","title":"Audio Mamba: Selective State Spaces for Self-Supervised Audio Representations","abstract":"Despite its widespread adoption as the prominent neural architecture, the Transformer has spurred several independent lines of work to address its limitations. One such approach is selective state space models, which have demonstrated promising results for language modelling. However, their feasibility for learning self-supervised, general-purpose audio representations is yet to be investigated. This work proposes Audio Mamba, a selective state space model for learning general-purpose audio representations from randomly masked spectrogram patches through self-supervision. Empirical results on ten diverse audio recognition downstream tasks show that the proposed models, pretrained on the AudioSet dataset, consistently outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by a considerable margin and demonstrate better performance in dataset size, sequence length and model size comparisons.","sentences":["Despite its widespread adoption as the prominent neural architecture, the Transformer has spurred several independent lines of work to address its limitations.","One such approach is selective state space models, which have demonstrated promising results for language modelling.","However, their feasibility for learning self-supervised, general-purpose audio representations is yet to be investigated.","This work proposes Audio Mamba, a selective state space model for learning general-purpose audio representations from randomly masked spectrogram patches through self-supervision.","Empirical results on ten diverse audio recognition downstream tasks show that the proposed models, pretrained on the AudioSet dataset, consistently outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by a considerable margin and demonstrate better performance in dataset size, sequence length and model size comparisons."],"url":"http://arxiv.org/abs/2406.02178v1","category":"cs.SD"}
{"created":"2024-06-04 10:14:39","title":"One-Shot Federated Learning with Bayesian Pseudocoresets","abstract":"Optimization-based techniques for federated learning (FL) often come with prohibitive communication cost, as high dimensional model parameters need to be communicated repeatedly between server and clients. In this paper, we follow a Bayesian approach allowing to perform FL with one-shot communication, by solving the global inference problem as a product of local client posteriors. For models with multi-modal likelihoods, such as neural networks, a naive application of this scheme is hampered, since clients will capture different posterior modes, causing a destructive collapse of the posterior on the server side. Consequently, we explore approximate inference in the function-space representation of client posteriors, hence suffering less or not at all from multi-modality. We show that distributed function-space inference is tightly related to learning Bayesian pseudocoresets and develop a tractable Bayesian FL algorithm on this insight. We show that this approach achieves prediction performance competitive to state-of-the-art while showing a striking reduction in communication cost of up to two orders of magnitude. Moreover, due to its Bayesian nature, our method also delivers well-calibrated uncertainty estimates.","sentences":["Optimization-based techniques for federated learning (FL) often come with prohibitive communication cost, as high dimensional model parameters need to be communicated repeatedly between server and clients.","In this paper, we follow a Bayesian approach allowing to perform FL with one-shot communication, by solving the global inference problem as a product of local client posteriors.","For models with multi-modal likelihoods, such as neural networks, a naive application of this scheme is hampered, since clients will capture different posterior modes, causing a destructive collapse of the posterior on the server side.","Consequently, we explore approximate inference in the function-space representation of client posteriors, hence suffering less or not at all from multi-modality.","We show that distributed function-space inference is tightly related to learning Bayesian pseudocoresets and develop a tractable Bayesian FL algorithm on this insight.","We show that this approach achieves prediction performance competitive to state-of-the-art while showing a striking reduction in communication cost of up to two orders of magnitude.","Moreover, due to its Bayesian nature, our method also delivers well-calibrated uncertainty estimates."],"url":"http://arxiv.org/abs/2406.02177v1","category":"cs.LG"}
{"created":"2024-06-04 10:09:48","title":"Incremental units-of-measure verification","abstract":"Despite an abundance of proposed systems, the verification of units-of-measure within programs remains rare in scientific computing. We attempt to address this issue by providing a lightweight static verification system for units-of-measure in Fortran programs which supports incremental annotation of large projects. We take the opposite approach to the most mainstream existing deployment of units-of-measure typing (in F#) and generate a global, rather than local, constraints system for a program. We show that such a system can infer (and check) polymorphic units specifications for under-determined parts of the program. Not only does this ability allow checking of partially annotated programs but it also allows the global constraint problem to be partitioned. This partitioning means we can scale to large programs by solving constraints for each program module independently and storing inferred units at module boundaries (separate verification). We provide an implementation of our approach as an extension to an open-source Fortran analysis tool.","sentences":["Despite an abundance of proposed systems, the verification of units-of-measure within programs remains rare in scientific computing.","We attempt to address this issue by providing a lightweight static verification system for units-of-measure in Fortran programs which supports incremental annotation of large projects.","We take the opposite approach to the most mainstream existing deployment of units-of-measure typing (in F#) and generate a global, rather than local, constraints system for a program.","We show that such a system can infer (and check) polymorphic units specifications for under-determined parts of the program.","Not only does this ability allow checking of partially annotated programs but it also allows the global constraint problem to be partitioned.","This partitioning means we can scale to large programs by solving constraints for each program module independently and storing inferred units at module boundaries (separate verification).","We provide an implementation of our approach as an extension to an open-source Fortran analysis tool."],"url":"http://arxiv.org/abs/2406.02174v1","category":"cs.PL"}
{"created":"2024-06-04 10:04:54","title":"Learning the Hodgkin-Huxley Model with Operator Learning Techniques","abstract":"We construct and compare three operator learning architectures, DeepONet, Fourier Neural Operator, and Wavelet Neural Operator, in order to learn the operator mapping a time-dependent applied current to the transmembrane potential of the Hodgkin- Huxley ionic model. The underlying non-linearity of the Hodgkin-Huxley dynamical system, the stiffness of its solutions, and the threshold dynamics depending on the intensity of the applied current, are some of the challenges to address when exploiting artificial neural networks to learn this class of complex operators. By properly designing these operator learning techniques, we demonstrate their ability to effectively address these challenges, achieving a relative L2 error as low as 1.4% in learning the solutions of the Hodgkin-Huxley ionic model.","sentences":["We construct and compare three operator learning architectures, DeepONet, Fourier Neural Operator, and Wavelet Neural Operator, in order to learn the operator mapping a time-dependent applied current to the transmembrane potential of the Hodgkin- Huxley ionic model.","The underlying non-linearity of the Hodgkin-Huxley dynamical system, the stiffness of its solutions, and the threshold dynamics depending on the intensity of the applied current, are some of the challenges to address when exploiting artificial neural networks to learn this class of complex operators.","By properly designing these operator learning techniques, we demonstrate their ability to effectively address these challenges, achieving a relative L2 error as low as 1.4% in learning the solutions of the Hodgkin-Huxley ionic model."],"url":"http://arxiv.org/abs/2406.02173v1","category":"math.NA"}
{"created":"2024-06-04 09:58:54","title":"MIMO Capacity Maximization with Beyond-Diagonal RIS","abstract":"This paper addresses the problem of maximizing the capacity of a multiple-input multiple-output (MIMO) link assisted by a beyond-diagonal reconfigurable intelligent surface (BD-RIS). We maximize the capacity by alternately optimizing the transmit covariance matrix, and the BD-RIS scattering matrix, which, according to network theory, should be unitary and symmetric. These constraints make the optimization of BD-RIS more challenging than that of diagonal RIS. To find a stationary point of the capacity we maximize a sequence of quadratic problems in the manifold of unitary matrices. This leads to an efficient algorithm that always improves the capacity obtained by a diagonal RIS. Through simulation examples, we study the capacity improvement provided by a passive BD-RIS architecture over the conventional RIS model in which the phase shift matrix is diagonal.","sentences":["This paper addresses the problem of maximizing the capacity of a multiple-input multiple-output (MIMO) link assisted by a beyond-diagonal reconfigurable intelligent surface (BD-RIS).","We maximize the capacity by alternately optimizing the transmit covariance matrix, and the BD-RIS scattering matrix, which, according to network theory, should be unitary and symmetric.","These constraints make the optimization of BD-RIS more challenging than that of diagonal RIS.","To find a stationary point of the capacity we maximize a sequence of quadratic problems in the manifold of unitary matrices.","This leads to an efficient algorithm that always improves the capacity obtained by a diagonal RIS.","Through simulation examples, we study the capacity improvement provided by a passive BD-RIS architecture over the conventional RIS model in which the phase shift matrix is diagonal."],"url":"http://arxiv.org/abs/2406.02170v1","category":"cs.IT"}
{"created":"2024-06-04 09:54:31","title":"Sparse Recovery for Holographic MIMO Channels: Leveraging the Clustered Sparsity","abstract":"Envisioned as the next-generation transceiver technology, the holographic multiple-input-multiple-output (HMIMO) garners attention for its superior capabilities of fabricating electromagnetic (EM) waves. However, the densely packed antenna elements significantly increase the dimension of the HMIMO channel matrix, rendering traditional channel estimation methods inefficient. While the dimension curse can be relieved to avoid the proportional increase with the antenna density using the state-of-the-art wavenumber-domain sparse representation, the sparse recovery complexity remains tied to the order of non-zero elements in the sparse channel, which still considerably exceeds the number of scatterers. By modeling the inherent clustered sparsity using a Gaussian mixed model (GMM)-based von Mises-Fisher (vMF) distribution, the to-be-estimated channel characteristics can be compressed to the scatterer level. Upon the sparsity extraction, a novel wavenumber-domain expectation-maximization (WD-EM) algorithm is proposed to implement the cluster-by-cluster variational inference, thus significantly reducing the computational complexity. Simulation results verify the robustness of the proposed scheme across overheads and signal-to-noise ratio (SNR).","sentences":["Envisioned as the next-generation transceiver technology, the holographic multiple-input-multiple-output (HMIMO) garners attention for its superior capabilities of fabricating electromagnetic (EM) waves.","However, the densely packed antenna elements significantly increase the dimension of the HMIMO channel matrix, rendering traditional channel estimation methods inefficient.","While the dimension curse can be relieved to avoid the proportional increase with the antenna density using the state-of-the-art wavenumber-domain sparse representation, the sparse recovery complexity remains tied to the order of non-zero elements in the sparse channel, which still considerably exceeds the number of scatterers.","By modeling the inherent clustered sparsity using a Gaussian mixed model (GMM)-based von Mises-Fisher (vMF) distribution, the to-be-estimated channel characteristics can be compressed to the scatterer level.","Upon the sparsity extraction, a novel wavenumber-domain expectation-maximization (WD-EM) algorithm is proposed to implement the cluster-by-cluster variational inference, thus significantly reducing the computational complexity.","Simulation results verify the robustness of the proposed scheme across overheads and signal-to-noise ratio (SNR)."],"url":"http://arxiv.org/abs/2406.02164v1","category":"cs.IT"}
{"created":"2024-06-04 09:51:02","title":"BiVocoder: A Bidirectional Neural Vocoder Integrating Feature Extraction and Waveform Generation","abstract":"This paper proposes a novel bidirectional neural vocoder, named BiVocoder, capable both of feature extraction and reverse waveform generation within the short-time Fourier transform (STFT) domain. For feature extraction, the BiVocoder takes amplitude and phase spectra derived from STFT as inputs, transforms them into long-frame-shift and low-dimensional features through convolutional neural networks. The extracted features are demonstrated suitable for direct prediction by acoustic models, supporting its application in text-to-speech (TTS) task. For waveform generation, the BiVocoder restores amplitude and phase spectra from the features by a symmetric network, followed by inverse STFT to reconstruct the speech waveform. Experimental results show that our proposed BiVocoder achieves better performance compared to some baseline vocoders, by comprehensively considering both synthesized speech quality and inference speed for both analysis-synthesis and TTS tasks.","sentences":["This paper proposes a novel bidirectional neural vocoder, named BiVocoder, capable both of feature extraction and reverse waveform generation within the short-time Fourier transform (STFT) domain.","For feature extraction, the BiVocoder takes amplitude and phase spectra derived from STFT as inputs, transforms them into long-frame-shift and low-dimensional features through convolutional neural networks.","The extracted features are demonstrated suitable for direct prediction by acoustic models, supporting its application in text-to-speech (TTS) task.","For waveform generation, the BiVocoder restores amplitude and phase spectra from the features by a symmetric network, followed by inverse STFT to reconstruct the speech waveform.","Experimental results show that our proposed BiVocoder achieves better performance compared to some baseline vocoders, by comprehensively considering both synthesized speech quality and inference speed for both analysis-synthesis and TTS tasks."],"url":"http://arxiv.org/abs/2406.02162v1","category":"eess.AS"}
{"created":"2024-06-04 09:41:40","title":"Analyzing the Feature Extractor Networks for Face Image Synthesis","abstract":"Advancements like Generative Adversarial Networks have attracted the attention of researchers toward face image synthesis to generate ever more realistic images. Thereby, the need for the evaluation criteria to assess the realism of the generated images has become apparent. While FID utilized with InceptionV3 is one of the primary choices for benchmarking, concerns about InceptionV3's limitations for face images have emerged. This study investigates the behavior of diverse feature extractors -- InceptionV3, CLIP, DINOv2, and ArcFace -- considering a variety of metrics -- FID, KID, Precision\\&Recall. While the FFHQ dataset is used as the target domain, as the source domains, the CelebA-HQ dataset and the synthetic datasets generated using StyleGAN2 and Projected FastGAN are used. Experiments include deep-down analysis of the features: $L_2$ normalization, model attention during extraction, and domain distributions in the feature space. We aim to give valuable insights into the behavior of feature extractors for evaluating face image synthesis methodologies. The code is publicly available at https://github.com/ThEnded32/AnalyzingFeatureExtractors.","sentences":["Advancements like Generative Adversarial Networks have attracted the attention of researchers toward face image synthesis to generate ever more realistic images.","Thereby, the need for the evaluation criteria to assess the realism of the generated images has become apparent.","While FID utilized with InceptionV3 is one of the primary choices for benchmarking, concerns about InceptionV3's limitations for face images have emerged.","This study investigates the behavior of diverse feature extractors -- InceptionV3, CLIP, DINOv2, and ArcFace -- considering a variety of metrics -- FID, KID, Precision\\&Recall.","While the FFHQ dataset is used as the target domain, as the source domains, the CelebA-HQ dataset and the synthetic datasets generated using StyleGAN2 and Projected FastGAN are used.","Experiments include deep-down analysis of the features: $L_2$ normalization, model attention during extraction, and domain distributions in the feature space.","We aim to give valuable insights into the behavior of feature extractors for evaluating face image synthesis methodologies.","The code is publicly available at https://github.com/ThEnded32/AnalyzingFeatureExtractors."],"url":"http://arxiv.org/abs/2406.02153v1","category":"cs.CV"}
{"created":"2024-06-04 09:35:47","title":"Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models","abstract":"Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events. Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions. However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences. Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks. In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM. The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting. Then, the SLM refines its learning of event representations based on these insights during fine-tuning. Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage. Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios.","sentences":["Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events.","Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions.","However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences.","Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks.","In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM.","The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting.","Then, the SLM refines its learning of event representations based on these insights during fine-tuning.","Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage.","Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios."],"url":"http://arxiv.org/abs/2406.02148v1","category":"cs.CL"}
{"created":"2024-06-04 09:31:18","title":"Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models","abstract":"Learning multi-task models for jointly detecting stance and verifying rumors poses challenges due to the need for training data of stance at post level and rumor veracity at claim level, which are difficult to obtain. To address this issue, we leverage large language models (LLMs) as the foundation annotators for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed as JSDRV. We introduce a novel reinforcement tuning framework to enhance the joint predictive capabilities of LLM-based SD and RV components. Specifically, we devise a policy for selecting LLM-annotated data at the two levels, employing a hybrid reward mechanism to choose high-quality labels for effective LLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the capabilities of LLMs in the joint tasks, not only outperforming state-of-the-art methods but also generalizing to non-LLMs accommodated as task models.","sentences":["Learning multi-task models for jointly detecting stance and verifying rumors poses challenges due to the need for training data of stance at post level and rumor veracity at claim level, which are difficult to obtain.","To address this issue, we leverage large language models (LLMs) as the foundation annotators for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed as JSDRV.","We introduce a novel reinforcement tuning framework to enhance the joint predictive capabilities of LLM-based SD and RV components.","Specifically, we devise a policy for selecting LLM-annotated data at the two levels, employing a hybrid reward mechanism to choose high-quality labels for effective LLM fine-tuning on both tasks.","Results demonstrate that JSDRV improves the capabilities of LLMs in the joint tasks, not only outperforming state-of-the-art methods but also generalizing to non-LLMs accommodated as task models."],"url":"http://arxiv.org/abs/2406.02143v1","category":"cs.CL"}
{"created":"2024-06-04 09:29:06","title":"Leveraging deterministic weather forecasts for in-situ probabilistic temperature predictions via deep learning","abstract":"We propose a neural network approach to produce probabilistic weather forecasts from a deterministic numerical weather prediction. Our approach is applied to operational surface temperature outputs from the Global Deterministic Prediction System up to ten-day lead times, targeting METAR observations in Canada and the United States. We show how postprocessing performance is improved by training a single model for multiple lead times. Multiple strategies to condition the network for the lead time are studied, including a supplementary predictor and an embedding. The proposed model is evaluated for accuracy, spread, distribution calibration, and its behavior under extremes. The neural network approach decreases CRPS by 15% and has improved distribution calibration compared to a naive probabilistic model based on past forecast errors. Our approach increases the value of a deterministic forecast by adding information about the uncertainty, without incurring the cost of simulating multiple trajectories. It applies to any gridded forecast including the recent machine learning-based weather prediction models. It requires no information regarding forecast spread and can be trained to generate probabilistic predictions from any deterministic forecast.","sentences":["We propose a neural network approach to produce probabilistic weather forecasts from a deterministic numerical weather prediction.","Our approach is applied to operational surface temperature outputs from the Global Deterministic Prediction System up to ten-day lead times, targeting METAR observations in Canada and the United States.","We show how postprocessing performance is improved by training a single model for multiple lead times.","Multiple strategies to condition the network for the lead time are studied, including a supplementary predictor and an embedding.","The proposed model is evaluated for accuracy, spread, distribution calibration, and its behavior under extremes.","The neural network approach decreases CRPS by 15% and has improved distribution calibration compared to a naive probabilistic model based on past forecast errors.","Our approach increases the value of a deterministic forecast by adding information about the uncertainty, without incurring the cost of simulating multiple trajectories.","It applies to any gridded forecast including the recent machine learning-based weather prediction models.","It requires no information regarding forecast spread and can be trained to generate probabilistic predictions from any deterministic forecast."],"url":"http://arxiv.org/abs/2406.02141v1","category":"physics.ao-ph"}
{"created":"2024-06-04 09:27:35","title":"Optimality of Matrix Mechanism on $\\ell_p^p$-metric","abstract":"In this paper, we introduce the $\\ell_p^p$-error metric (for $p \\geq 2$) when answering linear queries under the constraint of differential privacy. We characterize such an error under $(\\epsilon,\\delta)$-differential privacy. Before this paper, tight characterization in the hardness of privately answering linear queries was known under $\\ell_2^2$-error metric (Edmonds et al., STOC 2020) and $\\ell_p^2$-error metric for unbiased mechanisms (Nikolov and Tang, ITCS 2024). As a direct consequence of our results, we give tight bounds on answering prefix sum and parity queries under differential privacy for all constant $p$ in terms of the $\\ell_p^p$ error, generalizing the bounds in Henzinger et al. (SODA 2023) for $p=2$.","sentences":["In this paper, we introduce the $\\ell_p^p$-error metric (for $p \\geq 2$) when answering linear queries under the constraint of differential privacy.","We characterize such an error under $(\\epsilon,\\delta)$-differential privacy.","Before this paper, tight characterization in the hardness of privately answering linear queries was known under $\\ell_2^2$-error metric (Edmonds et al., STOC 2020) and $\\ell_p^2$-error metric for unbiased mechanisms (Nikolov and Tang, ITCS 2024).","As a direct consequence of our results, we give tight bounds on answering prefix sum and parity queries under differential privacy for all constant $p$ in terms of the $\\ell_p^p$ error, generalizing the bounds in Henzinger et al. (SODA 2023) for $p=2$."],"url":"http://arxiv.org/abs/2406.02140v1","category":"cs.CR"}
{"created":"2024-06-04 09:25:09","title":"Enhanced Nonlinear Frequency Conversion Bandwidth through Birefringence induced Mode Hybridization","abstract":"On-chip quantum information network requires qubit transfer between different wavelengths while preserving quantum coherence and entanglement, which needs broadband up-conversion available. Herein, we demonstrate a mode-hybridization based broadband nonlinear frequency conversion on X-cut thin film lithium niobate. With the spontaneous quasi-phase matching and quasi groupvelocity matching being simultaneously satisfied, broadband second harmonic generation with a 3-dB bandwidth up to 13 nm has been achieved in a micro-racetrack resonator. The same mechanism can work on the frequency conversion of the ultra-short pulse in the bent waveguide structure. This work will be beneficial to on-chip tunable frequency conversion and quantum light source generation on integrated photonic platforms, and further enable on-chip large-capacity multiplexing, multichannel optical information processing, and large quantum information networks.","sentences":["On-chip quantum information network requires qubit transfer between different wavelengths while preserving quantum coherence and entanglement, which needs broadband up-conversion available.","Herein, we demonstrate a mode-hybridization based broadband nonlinear frequency conversion on X-cut thin film lithium niobate.","With the spontaneous quasi-phase matching and quasi groupvelocity matching being simultaneously satisfied, broadband second harmonic generation with a 3-dB bandwidth up to 13 nm has been achieved in a micro-racetrack resonator.","The same mechanism can work on the frequency conversion of the ultra-short pulse in the bent waveguide structure.","This work will be beneficial to on-chip tunable frequency conversion and quantum light source generation on integrated photonic platforms, and further enable on-chip large-capacity multiplexing, multichannel optical information processing, and large quantum information networks."],"url":"http://arxiv.org/abs/2406.02137v1","category":"physics.optics"}
{"created":"2024-06-04 09:24:34","title":"Rabi Oscillation of High Partial Wave Interacting Atoms in Deep Optical Lattice","abstract":"Motivated by the recent experiment of p-wave interacting $^{40}\\mathrm{K}$ atom gases in a deep optical lattice, we investigated the Rabi oscillation for any partial wave interacting quantum gases in a deep optical lattice. We first review the solution for two particles interacting by any partial wave in a harmonic trap by using pseudopotential. We generalize the model used in recent work to any partial wave cases, repeated some of the theoretic and experiment results for p-wave interacting $^{40}\\mathrm{K}$ atoms in a deep optical lattice and show the results for the d-wave case. Our results may be useful for future experiments and may stimulate further theoretic investigation.","sentences":["Motivated by the recent experiment of p-wave interacting $^{40}\\mathrm{K}$ atom gases in a deep optical lattice, we investigated the Rabi oscillation for any partial wave interacting quantum gases in a deep optical lattice.","We first review the solution for two particles interacting by any partial wave in a harmonic trap by using pseudopotential.","We generalize the model used in recent work to any partial wave cases, repeated some of the theoretic and experiment results for p-wave interacting $^{40}\\mathrm{K}$ atoms in a deep optical lattice and show the results for the d-wave case.","Our results may be useful for future experiments and may stimulate further theoretic investigation."],"url":"http://arxiv.org/abs/2406.02136v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-04 09:24:04","title":"Robust Interaction-based Relevance Modeling for Online E-Commerce and LLM-based Retrieval","abstract":"Semantic relevance calculation is crucial for e-commerce search engines, as it ensures that the items selected closely align with customer intent. Inadequate attention to this aspect can detrimentally affect user experience and engagement. Traditional text-matching techniques are prevalent but often fail to capture the nuances of search intent accurately, so neural networks now have become a preferred solution to processing such complex text matching. Existing methods predominantly employ representation-based architectures, which strike a balance between high traffic capacity and low latency. However, they exhibit significant shortcomings in generalization and robustness when compared to interaction-based architectures. In this work, we introduce a robust interaction-based modeling paradigm to address these shortcomings. It encompasses 1) a dynamic length representation scheme for expedited inference, 2) a professional terms recognition method to identify subjects and core attributes from complex sentence structures, and 3) a contrastive adversarial training protocol to bolster the model's robustness and matching capabilities. Extensive offline evaluations demonstrate the superior robustness and effectiveness of our approach, and online A/B testing confirms its ability to improve relevance in the same exposure position, resulting in more clicks and conversions. To the best of our knowledge, this method is the first interaction-based approach for large e-commerce search relevance calculation. Notably, we have deployed it for the entire search traffic on alibaba.com, the largest B2B e-commerce platform in the world.","sentences":["Semantic relevance calculation is crucial for e-commerce search engines, as it ensures that the items selected closely align with customer intent.","Inadequate attention to this aspect can detrimentally affect user experience and engagement.","Traditional text-matching techniques are prevalent but often fail to capture the nuances of search intent accurately, so neural networks now have become a preferred solution to processing such complex text matching.","Existing methods predominantly employ representation-based architectures, which strike a balance between high traffic capacity and low latency.","However, they exhibit significant shortcomings in generalization and robustness when compared to interaction-based architectures.","In this work, we introduce a robust interaction-based modeling paradigm to address these shortcomings.","It encompasses 1) a dynamic length representation scheme for expedited inference, 2) a professional terms recognition method to identify subjects and core attributes from complex sentence structures, and 3) a contrastive adversarial training protocol to bolster the model's robustness and matching capabilities.","Extensive offline evaluations demonstrate the superior robustness and effectiveness of our approach, and online A/B testing confirms its ability to improve relevance in the same exposure position, resulting in more clicks and conversions.","To the best of our knowledge, this method is the first interaction-based approach for large e-commerce search relevance calculation.","Notably, we have deployed it for the entire search traffic on alibaba.com, the largest B2B e-commerce platform in the world."],"url":"http://arxiv.org/abs/2406.02135v1","category":"cs.IR"}
{"created":"2024-06-04 09:23:30","title":"The current status of large language models in summarizing radiology report impressions","abstract":"Large language models (LLMs) like ChatGPT show excellent capabilities in various natural language processing tasks, especially for text generation. The effectiveness of LLMs in summarizing radiology report impressions remains unclear. In this study, we explore the capability of eight LLMs on the radiology report impression summarization. Three types of radiology reports, i.e., CT, PET-CT, and Ultrasound reports, are collected from Peking University Cancer Hospital and Institute. We use the report findings to construct the zero-shot, one-shot, and three-shot prompts with complete example reports to generate the impressions. Besides the automatic quantitative evaluation metrics, we define five human evaluation metrics, i.e., completeness, correctness, conciseness, verisimilitude, and replaceability, to evaluate the semantics of the generated impressions. Two thoracic surgeons (ZSY and LB) and one radiologist (LQ) compare the generated impressions with the reference impressions and score each impression under the five human evaluation metrics. Experimental results show that there is a gap between the generated impressions and reference impressions. Although the LLMs achieve comparable performance in completeness and correctness, the conciseness and verisimilitude scores are not very high. Using few-shot prompts can improve the LLMs' performance in conciseness and verisimilitude, but the clinicians still think the LLMs can not replace the radiologists in summarizing the radiology impressions.","sentences":["Large language models (LLMs) like ChatGPT show excellent capabilities in various natural language processing tasks, especially for text generation.","The effectiveness of LLMs in summarizing radiology report impressions remains unclear.","In this study, we explore the capability of eight LLMs on the radiology report impression summarization.","Three types of radiology reports, i.e., CT, PET-CT, and Ultrasound reports, are collected from Peking University Cancer Hospital and Institute.","We use the report findings to construct the zero-shot, one-shot, and three-shot prompts with complete example reports to generate the impressions.","Besides the automatic quantitative evaluation metrics, we define five human evaluation metrics, i.e., completeness, correctness, conciseness, verisimilitude, and replaceability, to evaluate the semantics of the generated impressions.","Two thoracic surgeons (ZSY and LB) and one radiologist (LQ) compare the generated impressions with the reference impressions and score each impression under the five human evaluation metrics.","Experimental results show that there is a gap between the generated impressions and reference impressions.","Although the LLMs achieve comparable performance in completeness and correctness, the conciseness and verisimilitude scores are not very high.","Using few-shot prompts can improve the LLMs' performance in conciseness and verisimilitude, but the clinicians still think the LLMs can not replace the radiologists in summarizing the radiology impressions."],"url":"http://arxiv.org/abs/2406.02134v1","category":"cs.CL"}
{"created":"2024-06-04 09:18:20","title":"CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting","abstract":"Dataset condensation is a newborn technique that generates a small dataset that can be used in training deep neural networks to lower training costs. The objective of dataset condensation is to ensure that the model trained with the synthetic dataset can perform comparably to the model trained with full datasets. However, existing methods predominantly concentrate on classification tasks, posing challenges in their adaptation to time series forecasting (TS-forecasting). This challenge arises from disparities in the evaluation of synthetic data. In classification, the synthetic data is considered well-distilled if the model trained with the full dataset and the model trained with the synthetic dataset yield identical labels for the same input, regardless of variations in output logits distribution. Conversely, in TS-forecasting, the effectiveness of synthetic data distillation is determined by the distance between predictions of the two models. The synthetic data is deemed well-distilled only when all data points within the predictions are similar. Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification. To mitigate this gap, we theoretically analyze the optimization objective of dataset condensation for TS-forecasting and propose a new one-line plugin of dataset condensation designated as Dataset Condensation for Time Series Forecasting (CondTSF) based on our analysis. Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance. We conduct extensive experiments on eight commonly used time series datasets. CondTSF consistently improves the performance of all previous dataset condensation methods across all datasets, particularly at low condensing ratios.","sentences":["Dataset condensation is a newborn technique that generates a small dataset that can be used in training deep neural networks to lower training costs.","The objective of dataset condensation is to ensure that the model trained with the synthetic dataset can perform comparably to the model trained with full datasets.","However, existing methods predominantly concentrate on classification tasks, posing challenges in their adaptation to time series forecasting (TS-forecasting).","This challenge arises from disparities in the evaluation of synthetic data.","In classification, the synthetic data is considered well-distilled if the model trained with the full dataset and the model trained with the synthetic dataset yield identical labels for the same input, regardless of variations in output logits distribution.","Conversely, in TS-forecasting, the effectiveness of synthetic data distillation is determined by the distance between predictions of the two models.","The synthetic data is deemed well-distilled only when all data points within the predictions are similar.","Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification.","To mitigate this gap, we theoretically analyze the optimization objective of dataset condensation for TS-forecasting and propose a new one-line plugin of dataset condensation designated as Dataset Condensation for Time Series Forecasting (CondTSF) based on our analysis.","Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance.","We conduct extensive experiments on eight commonly used time series datasets.","CondTSF consistently improves the performance of all previous dataset condensation methods across all datasets, particularly at low condensing ratios."],"url":"http://arxiv.org/abs/2406.02131v1","category":"cs.LG"}
{"created":"2024-06-04 09:17:43","title":"Spin Orbit and Hyperfine Simulations with Two-Species Ultracold Atoms in a Ring","abstract":"A collective spin model is used to describe two species of mutually interacting ultracold bosonic atoms confined to a toroidal trap. The system is modeled by a Hamiltonian that can be split into two components, a linear part and a quadratic part, which may be controlled independently. We show the linear component is an analog of a Zeeman Hamiltonian, and the quadratic component presents a macroscopic simulator for spin-orbit and hyperfine interactions. We determine a complete set of commuting observables for both the linear and quadratic Hamiltonians, and derive analytical expressions for their respective spectra and density of states. We determine the conditions for generating maximal entanglement between the two species of atoms with a view to applications involving quantum correlations among spin degrees of freedom, such as in the area of quantum information.","sentences":["A collective spin model is used to describe two species of mutually interacting ultracold bosonic atoms confined to a toroidal trap.","The system is modeled by a Hamiltonian that can be split into two components, a linear part and a quadratic part, which may be controlled independently.","We show the linear component is an analog of a Zeeman Hamiltonian, and the quadratic component presents a macroscopic simulator for spin-orbit and hyperfine interactions.","We determine a complete set of commuting observables for both the linear and quadratic Hamiltonians, and derive analytical expressions for their respective spectra and density of states.","We determine the conditions for generating maximal entanglement between the two species of atoms with a view to applications involving quantum correlations among spin degrees of freedom, such as in the area of quantum information."],"url":"http://arxiv.org/abs/2406.02130v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-04 09:14:51","title":"Slice diameter two property in ultrapowers","abstract":"In this note we study the inheritance of the slice diameter two property by ultrapowers. Given a Banach space $X$, we give a characterisation of when $(X)_\\mathcal U$ has the slice diameter two property obtaining that this is the case for many Banach spaces which are known to enjoy the slice diameter two property. We also provide an example of a Banach space $X$ with the Daugavet property so that $(X)_\\mathcal U$ fails the slice diameter two property for every free ultrafilter $\\mathcal U$ over $\\mathbb N$ which proves that, in general, this property is not inherited by taking ultraproduct spaces.","sentences":["In this note we study the inheritance of the slice diameter two property by ultrapowers.","Given a Banach space $X$, we give a characterisation of when $(X)_\\mathcal U$ has the slice diameter two property obtaining that this is the case for many Banach spaces which are known to enjoy the slice diameter two property.","We also provide an example of a Banach space $X$ with the Daugavet property so that $(X)_\\mathcal U$ fails the slice diameter two property for every free ultrafilter $\\mathcal U$ over $\\mathbb N$ which proves that, in general, this property is not inherited by taking ultraproduct spaces."],"url":"http://arxiv.org/abs/2406.02129v1","category":"math.FA"}
{"created":"2024-06-04 09:11:46","title":"Iteration Head: A Mechanistic Study of Chain-of-Thought","abstract":"Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power. However, our understanding of the inner workings and conditions of apparition of CoT capabilities remains limited. This paper helps fill this gap by demonstrating how CoT reasoning emerges in transformers in a controlled and interpretable setting. In particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined \"iteration heads\". We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.","sentences":["Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power.","However, our understanding of the inner workings and conditions of apparition of CoT capabilities remains limited.","This paper helps fill this gap by demonstrating how CoT reasoning emerges in transformers in a controlled and interpretable setting.","In particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined \"iteration heads\".","We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks."],"url":"http://arxiv.org/abs/2406.02128v1","category":"cs.LG"}
{"created":"2024-06-04 09:10:14","title":"CityLight: A Universal Model Towards Real-world City-scale Traffic Signal Control Coordination","abstract":"Traffic signal control (TSC) is a promising low-cost measure to enhance transportation efficiency without affecting existing road infrastructure. While various reinforcement learning-based TSC methods have been proposed and experimentally outperform conventional rule-based methods, none of them has been deployed in the real world. An essential gap lies in the oversimplification of the scenarios in terms of intersection heterogeneity and road network intricacy. To make TSC applicable in urban traffic management, we target TSC coordination in city-scale high-authenticity road networks, aiming to solve the three unique and important challenges: city-level scalability, heterogeneity of real-world intersections, and effective coordination among intricate neighbor connections. Since optimizing multiple agents in a parameter-sharing paradigm can boost the training efficiency and help achieve scalability, we propose our method, CityLight, based on the well-acknowledged optimization framework, parameter-sharing MAPPO. To ensure the unified policy network can learn to fit large-scale heterogeneous intersections and tackle the intricate between-neighbor coordination, CityLight proposes a universal representation module that consists of two key designs: heterogeneous intersection alignment and neighborhood impact alignment for coordination. To further boost coordination, CityLight adopts neighborhood-integrated rewards to transition from achieving local optimal to global optimal. Extensive experiments on datasets with hundreds to tens of thousands of real-world intersections and authentic traffic demands validate the surprising effectiveness and generalizability of CityLight, with an overall performance gain of 11.66% and a 22.59% improvement in transfer scenarios in terms of throughput.","sentences":["Traffic signal control (TSC) is a promising low-cost measure to enhance transportation efficiency without affecting existing road infrastructure.","While various reinforcement learning-based TSC methods have been proposed and experimentally outperform conventional rule-based methods, none of them has been deployed in the real world.","An essential gap lies in the oversimplification of the scenarios in terms of intersection heterogeneity and road network intricacy.","To make TSC applicable in urban traffic management, we target TSC coordination in city-scale high-authenticity road networks, aiming to solve the three unique and important challenges: city-level scalability, heterogeneity of real-world intersections, and effective coordination among intricate neighbor connections.","Since optimizing multiple agents in a parameter-sharing paradigm can boost the training efficiency and help achieve scalability, we propose our method, CityLight, based on the well-acknowledged optimization framework, parameter-sharing MAPPO.","To ensure the unified policy network can learn to fit large-scale heterogeneous intersections and tackle the intricate between-neighbor coordination, CityLight proposes a universal representation module that consists of two key designs: heterogeneous intersection alignment and neighborhood impact alignment for coordination.","To further boost coordination, CityLight adopts neighborhood-integrated rewards to transition from achieving local optimal to global optimal.","Extensive experiments on datasets with hundreds to tens of thousands of real-world intersections and authentic traffic demands validate the surprising effectiveness and generalizability of CityLight, with an overall performance gain of 11.66% and a 22.59% improvement in transfer scenarios in terms of throughput."],"url":"http://arxiv.org/abs/2406.02126v1","category":"eess.SY"}
{"created":"2024-06-04 09:10:02","title":"Domain Game: Disentangle Anatomical Feature for Single Domain Generalized Segmentation","abstract":"Single domain generalization aims to address the challenge of out-of-distribution generalization problem with only one source domain available. Feature distanglement is a classic solution to this purpose, where the extracted task-related feature is presumed to be resilient to domain shift. However, the absence of references from other domains in a single-domain scenario poses significant uncertainty in feature disentanglement (ill-posedness). In this paper, we propose a new framework, named \\textit{Domain Game}, to perform better feature distangling for medical image segmentation, based on the observation that diagnostic relevant features are more sensitive to geometric transformations, whilist domain-specific features probably will remain invariant to such operations. In domain game, a set of randomly transformed images derived from a singular source image is strategically encoded into two separate feature sets to represent diagnostic features and domain-specific features, respectively, and we apply forces to pull or repel them in the feature space, accordingly. Results from cross-site test domain evaluation showcase approximately an ~11.8% performance boost in prostate segmentation and around ~10.5% in brain tumor segmentation compared to the second-best method.","sentences":["Single domain generalization aims to address the challenge of out-of-distribution generalization problem with only one source domain available.","Feature distanglement is a classic solution to this purpose, where the extracted task-related feature is presumed to be resilient to domain shift.","However, the absence of references from other domains in a single-domain scenario poses significant uncertainty in feature disentanglement (ill-posedness).","In this paper, we propose a new framework, named \\textit{Domain Game}, to perform better feature distangling for medical image segmentation, based on the observation that diagnostic relevant features are more sensitive to geometric transformations, whilist domain-specific features probably will remain invariant to such operations.","In domain game, a set of randomly transformed images derived from a singular source image is strategically encoded into two separate feature sets to represent diagnostic features and domain-specific features, respectively, and we apply forces to pull or repel them in the feature space, accordingly.","Results from cross-site test domain evaluation showcase approximately an ~11.8% performance boost in prostate segmentation and around ~10.5% in brain tumor segmentation compared to the second-best method."],"url":"http://arxiv.org/abs/2406.02125v1","category":"cs.CV"}
{"created":"2024-06-04 17:55:22","title":"Enhancing 2D Representation Learning with a 3D Prior","abstract":"Learning robust and effective representations of visual data is a fundamental task in computer vision. Traditionally, this is achieved by training models with labeled data which can be expensive to obtain. Self-supervised learning attempts to circumvent the requirement for labeled data by learning representations from raw unlabeled visual data alone. However, unlike humans who obtain rich 3D information from their binocular vision and through motion, the majority of current self-supervised methods are tasked with learning from monocular 2D image collections. This is noteworthy as it has been demonstrated that shape-centric visual processing is more robust compared to texture-biased automated methods. Inspired by this, we propose a new approach for strengthening existing self-supervised methods by explicitly enforcing a strong 3D structural prior directly into the model during training. Through experiments, across a range of datasets, we demonstrate that our 3D aware representations are more robust compared to conventional self-supervised baselines.","sentences":["Learning robust and effective representations of visual data is a fundamental task in computer vision.","Traditionally, this is achieved by training models with labeled data which can be expensive to obtain.","Self-supervised learning attempts to circumvent the requirement for labeled data by learning representations from raw unlabeled visual data alone.","However, unlike humans who obtain rich 3D information from their binocular vision and through motion, the majority of current self-supervised methods are tasked with learning from monocular 2D image collections.","This is noteworthy as it has been demonstrated that shape-centric visual processing is more robust compared to texture-biased automated methods.","Inspired by this, we propose a new approach for strengthening existing self-supervised methods by explicitly enforcing a strong 3D structural prior directly into the model during training.","Through experiments, across a range of datasets, we demonstrate that our 3D aware representations are more robust compared to conventional self-supervised baselines."],"url":"http://arxiv.org/abs/2406.02535v1","category":"cs.CV"}
{"created":"2024-06-04 16:31:19","title":"Click Without Compromise: Online Advertising Measurement via Per User Differential Privacy","abstract":"Online advertising is a cornerstone of the Internet ecosystem, with advertising measurement playing a crucial role in optimizing efficiency. Ad measurement entails attributing desired behaviors, such as purchases, to ad exposures across various platforms, necessitating the collection of user activities across these platforms. As this practice faces increasing restrictions due to rising privacy concerns, safeguarding user privacy in this context is imperative. Our work is the first to formulate the real-world challenge of advertising measurement systems with real-time reporting of streaming data in advertising campaigns. We introduce Ads-BPC, a novel user-level differential privacy protection scheme for advertising measurement results. This approach optimizes global noise power and results in a non-identically distributed noise distribution that preserves differential privacy while enhancing measurement accuracy. Through experiments on both real-world advertising campaigns and synthetic datasets, Ads-BPC achieves a 25% to 50% increase in accuracy over existing streaming DP mechanisms applied to advertising measurement. This highlights our method's effectiveness in achieving superior accuracy alongside a formal privacy guarantee, thereby advancing the state-of-the-art in privacy-preserving advertising measurement.","sentences":["Online advertising is a cornerstone of the Internet ecosystem, with advertising measurement playing a crucial role in optimizing efficiency.","Ad measurement entails attributing desired behaviors, such as purchases, to ad exposures across various platforms, necessitating the collection of user activities across these platforms.","As this practice faces increasing restrictions due to rising privacy concerns, safeguarding user privacy in this context is imperative.","Our work is the first to formulate the real-world challenge of advertising measurement systems with real-time reporting of streaming data in advertising campaigns.","We introduce Ads-BPC, a novel user-level differential privacy protection scheme for advertising measurement results.","This approach optimizes global noise power and results in a non-identically distributed noise distribution that preserves differential privacy while enhancing measurement accuracy.","Through experiments on both real-world advertising campaigns and synthetic datasets, Ads-BPC achieves a 25% to 50% increase in accuracy over existing streaming DP mechanisms applied to advertising measurement.","This highlights our method's effectiveness in achieving superior accuracy alongside a formal privacy guarantee, thereby advancing the state-of-the-art in privacy-preserving advertising measurement."],"url":"http://arxiv.org/abs/2406.02463v1","category":"cs.CR"}
{"created":"2024-06-04 16:01:26","title":"The Multi-Commodity Flow Problem with Outsourcing Decisions","abstract":"We address a new prize-collecting problem of routing commodities in a given network with hub and non-hub nodes, in which the service of the non-hub nodes will be outsourced to third-party carriers. The problem is modeled as a Stackelberg game: there is a major firm (leader) that decides to serve a subset of commodities. The leader aims to outsource first and third legs of transportation services to smaller carriers (who act as followers) by allocating at most one carrier to each non-hub node. The carriers try to maximize their own profits, which are influenced by the leader's offers. The goal of the leader is to determine the optimal outsourcing fees, along with the allocation of carriers to the non-hub nodes, so that the profit from the routed commodities is maximized. The optimal response of the followers must be taken into account, as the followers might refuse to serve some legs in case they are negative or do not maximize their profit. We also study two alternative settings: one in which the outsourcing fees are fixed, and the other one in which the carriers accept any offer, as long as the resulting profit is non-negative. We prove that the set of possible outsourcing fees can be discretized and formulate the problem as a single-level mixed-integer nonlinear program. For all considered problem variants, we prove NP-hardness and propose and computationally investigate several MIP formulations. We study the computational scalability of these MIP formulations and analyze solutions obtained by varying the reservation prices of the carriers. Finally, by comparing the introduced problem variants, we derive some interesting managerial insights.","sentences":["We address a new prize-collecting problem of routing commodities in a given network with hub and non-hub nodes, in which the service of the non-hub nodes will be outsourced to third-party carriers.","The problem is modeled as a Stackelberg game: there is a major firm (leader) that decides to serve a subset of commodities.","The leader aims to outsource first and third legs of transportation services to smaller carriers (who act as followers) by allocating at most one carrier to each non-hub node.","The carriers try to maximize their own profits, which are influenced by the leader's offers.","The goal of the leader is to determine the optimal outsourcing fees, along with the allocation of carriers to the non-hub nodes, so that the profit from the routed commodities is maximized.","The optimal response of the followers must be taken into account, as the followers might refuse to serve some legs in case they are negative or do not maximize their profit.","We also study two alternative settings: one in which the outsourcing fees are fixed, and the other one in which the carriers accept any offer, as long as the resulting profit is non-negative.","We prove that the set of possible outsourcing fees can be discretized and formulate the problem as a single-level mixed-integer nonlinear program.","For all considered problem variants, we prove NP-hardness and propose and computationally investigate several MIP formulations.","We study the computational scalability of these MIP formulations and analyze solutions obtained by varying the reservation prices of the carriers.","Finally, by comparing the introduced problem variants, we derive some interesting managerial insights."],"url":"http://arxiv.org/abs/2406.02439v1","category":"math.OC"}
{"created":"2024-06-04 15:17:37","title":"WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo Collections","abstract":"Novel View Synthesis (NVS) from unconstrained photo collections is challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has shown promise for photorealistic and real-time NVS of static scenes. Building on 3DGS, we propose an efficient point-based differentiable rendering framework for scene reconstruction from photo collections. Our key innovation is a residual-based spherical harmonic coefficients transfer module that adapts 3DGS to varying lighting conditions and photometric post-processing. This lightweight module can be pre-computed and ensures efficient gradient propagation from rendered images to 3D Gaussian attributes. Additionally, we observe that the appearance encoder and the transient mask predictor, the two most critical parts of NVS from unconstrained photo collections, can be mutually beneficial. We introduce a plug-and-play lightweight spatial attention module to simultaneously predict transient occluders and latent appearance representation for each image. After training and preprocessing, our method aligns with the standard 3DGS format and rendering pipeline, facilitating seamlessly integration into various 3DGS applications. Extensive experiments on diverse datasets show our approach outperforms existing approaches on the rendering quality of novel view and appearance synthesis with high converge and rendering speed.","sentences":["Novel View Synthesis (NVS) from unconstrained photo collections is challenging in computer graphics.","Recently, 3D Gaussian Splatting (3DGS) has shown promise for photorealistic and real-time NVS of static scenes.","Building on 3DGS, we propose an efficient point-based differentiable rendering framework for scene reconstruction from photo collections.","Our key innovation is a residual-based spherical harmonic coefficients transfer module that adapts 3DGS to varying lighting conditions and photometric post-processing.","This lightweight module can be pre-computed and ensures efficient gradient propagation from rendered images to 3D Gaussian attributes.","Additionally, we observe that the appearance encoder and the transient mask predictor, the two most critical parts of NVS from unconstrained photo collections, can be mutually beneficial.","We introduce a plug-and-play lightweight spatial attention module to simultaneously predict transient occluders and latent appearance representation for each image.","After training and preprocessing, our method aligns with the standard 3DGS format and rendering pipeline, facilitating seamlessly integration into various 3DGS applications.","Extensive experiments on diverse datasets show our approach outperforms existing approaches on the rendering quality of novel view and appearance synthesis with high converge and rendering speed."],"url":"http://arxiv.org/abs/2406.02407v1","category":"cs.CV"}
{"created":"2024-06-04 13:09:12","title":"A deep-learning-based MAC for integrating channel access, rate adaptation and channel switch","abstract":"With increasing density and heterogeneity in unlicensed wireless networks, traditional MAC protocols, such as carrier-sense multiple access with collision avoidance (CSMA/CA) in Wi-Fi networks, are experiencing performance degradation. This is manifested in increased collisions and extended backoff times, leading to diminished spectrum efficiency and protocol coordination. Addressing these issues, this paper proposes a deep-learning-based MAC paradigm, dubbed DL-MAC, which leverages spectrum sensing data readily available from energy detection modules in wireless devices to achieve the MAC functionalities of channel access, rate adaptation and channel switch. First, we utilize DL-MAC to realize a joint design of channel access and rate adaptation. Subsequently, we integrate the capability of channel switch into DL-MAC, enhancing its functionality from single-channel to multi-channel operation. Specifically, the DL-MAC protocol incorporates a deep neural network (DNN) for channel selection and a recurrent neural network (RNN) for the joint design of channel access and rate adaptation. We conducted real-world data collection within the 2.4 GHz frequency band to validate the effectiveness of DL-MAC, and our experiments reveal that DL-MAC exhibits superior performance over traditional algorithms in both single and multi-channel environments and also outperforms single-function approaches in terms of overall performance. Additionally, the performance of DL-MAC remains robust, unaffected by channel switch overhead within the evaluated range.","sentences":["With increasing density and heterogeneity in unlicensed wireless networks, traditional MAC protocols, such as carrier-sense multiple access with collision avoidance (CSMA/CA) in Wi-Fi networks, are experiencing performance degradation.","This is manifested in increased collisions and extended backoff times, leading to diminished spectrum efficiency and protocol coordination.","Addressing these issues, this paper proposes a deep-learning-based MAC paradigm, dubbed DL-MAC, which leverages spectrum sensing data readily available from energy detection modules in wireless devices to achieve the MAC functionalities of channel access, rate adaptation and channel switch.","First, we utilize DL-MAC to realize a joint design of channel access and rate adaptation.","Subsequently, we integrate the capability of channel switch into DL-MAC, enhancing its functionality from single-channel to multi-channel operation.","Specifically, the DL-MAC protocol incorporates a deep neural network (DNN) for channel selection and a recurrent neural network (RNN) for the joint design of channel access and rate adaptation.","We conducted real-world data collection within the 2.4 GHz frequency band to validate the effectiveness of DL-MAC, and our experiments reveal that DL-MAC exhibits superior performance over traditional algorithms in both single and multi-channel environments and also outperforms single-function approaches in terms of overall performance.","Additionally, the performance of DL-MAC remains robust, unaffected by channel switch overhead within the evaluated range."],"url":"http://arxiv.org/abs/2406.02291v1","category":"cs.NI"}
{"created":"2024-06-04 12:29:51","title":"Reinforcement Learning with Lookahead Information","abstract":"We study reinforcement learning (RL) problems in which agents observe the reward or transition realizations at their current state before deciding which action to take. Such observations are available in many applications, including transactions, navigation and more. When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward. However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations. In this work, we close this gap and design provably-efficient learning algorithms able to incorporate lookahead information. To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations. We prove that our algorithms achieve tight regret versus a baseline that also has access to lookahead information - linearly increasing the amount of collected reward compared to agents that cannot handle lookahead information.","sentences":["We study reinforcement learning (RL) problems in which agents observe the reward or transition realizations at their current state before deciding which action to take.","Such observations are available in many applications, including transactions, navigation and more.","When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward.","However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations.","In this work, we close this gap and design provably-efficient learning algorithms able to incorporate lookahead information.","To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations.","We prove that our algorithms achieve tight regret versus a baseline that also has access to lookahead information - linearly increasing the amount of collected reward compared to agents that cannot handle lookahead information."],"url":"http://arxiv.org/abs/2406.02258v1","category":"cs.LG"}
{"created":"2024-06-04 12:06:31","title":"Multiplicative largeness of $\\textit{de Polignac numbers}$","abstract":"A number $m$ is said to be a $\\textit{de Polignac number}$, if infinitely many pairs of consecutive primes exist, such that $m$ can be written as the difference of those consecutive prime numbers. Recently in [ W. D. Banks: Consecutive primes and IP sets, arXiv:2403.10637.], using arguments from the Ramsey theory, W. D. Banks proved that the collection of $\\textit{de Polignac number}$ is an $IP^\\star$ set (Though his original statement is relatively weaker, an iterative application of pigeonhole principle/ theory of ultrafilters shows that this statement is sufficient to conclude the set is $IP^\\star$). As a consequence, we have this collection as an additively syndetic set. In this article, we show that this collection is also a multiplicative syndetic set. In our proof, we use combinatorial arguments and the tools from the algebra of the Stone-\\v{C}ech compactification of discrete semigroups (for details see [N. Hindman, and D. Strauss: Algebra in the Stone-\\v{C}ech Compactification: Theory and Applications, second edition, de Gruyter, Berlin,2012.]).","sentences":["A number $m$ is said to be a $\\textit{de Polignac number}$, if infinitely many pairs of consecutive primes exist, such that $m$ can be written as the difference of those consecutive prime numbers.","Recently in [ W. D. Banks: Consecutive primes and IP sets, arXiv:2403.10637.], using arguments from the Ramsey theory, W. D. Banks proved that the collection of $\\textit{de Polignac number}$ is an $IP^\\star$ set (Though his original statement is relatively weaker, an iterative application of pigeonhole principle/ theory of ultrafilters shows that this statement is sufficient to conclude the set is $IP^\\star$).","As a consequence, we have this collection as an additively syndetic set.","In this article, we show that this collection is also a multiplicative syndetic set.","In our proof, we use combinatorial arguments and the tools from the algebra of the Stone-\\v{C}ech compactification of discrete semigroups (for details see [N. Hindman, and D. Strauss: Algebra in the Stone-\\v{C}ech Compactification: Theory and Applications, second edition, de Gruyter, Berlin,2012.])."],"url":"http://arxiv.org/abs/2406.02243v1","category":"math.NT"}
{"created":"2024-06-04 09:58:29","title":"A multilingual dataset for offensive language and hate speech detection for hausa, yoruba and igbo languages","abstract":"The proliferation of online offensive language necessitates the development of effective detection mechanisms, especially in multilingual contexts. This study addresses the challenge by developing and introducing novel datasets for offensive language detection in three major Nigerian languages: Hausa, Yoruba, and Igbo. We collected data from Twitter and manually annotated it to create datasets for each of the three languages, using native speakers. We used pre-trained language models to evaluate their efficacy in detecting offensive language in our datasets. The best-performing model achieved an accuracy of 90\\%. To further support research in offensive language detection, we plan to make the dataset and our models publicly available.","sentences":["The proliferation of online offensive language necessitates the development of effective detection mechanisms, especially in multilingual contexts.","This study addresses the challenge by developing and introducing novel datasets for offensive language detection in three major Nigerian languages: Hausa, Yoruba, and Igbo.","We collected data from Twitter and manually annotated it to create datasets for each of the three languages, using native speakers.","We used pre-trained language models to evaluate their efficacy in detecting offensive language in our datasets.","The best-performing model achieved an accuracy of 90\\%.","To further support research in offensive language detection, we plan to make the dataset and our models publicly available."],"url":"http://arxiv.org/abs/2406.02169v1","category":"cs.CL"}
{"created":"2024-06-04 09:54:55","title":"SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular MDP","abstract":"In this paper, we study safe data collection for the purpose of policy evaluation in tabular Markov decision processes (MDPs). In policy evaluation, we are given a \\textit{target} policy and asked to estimate the expected cumulative reward it will obtain. Policy evaluation requires data and we are interested in the question of what \\textit{behavior} policy should collect the data for the most accurate evaluation of the target policy. While prior work has considered behavior policy selection, in this paper, we additionally consider a safety constraint on the behavior policy. Namely, we assume there exists a known default policy that incurs a particular expected cost when run and we enforce that the cumulative cost of all behavior policies ran is better than a constant factor of the cost that would be incurred had we always run the default policy. We first show that there exists a class of intractable MDPs where no safe oracle algorithm with knowledge about problem parameters can efficiently collect data and satisfy the safety constraints. We then define the tractability condition for an MDP such that a safe oracle algorithm can efficiently collect data and using that we prove the first lower bound for this setting. We then introduce an algorithm SaVeR for this problem that approximates the safe oracle algorithm and bound the finite-sample mean squared error of the algorithm while ensuring it satisfies the safety constraint. Finally, we show in simulations that SaVeR produces low MSE policy evaluation while satisfying the safety constraint.","sentences":["In this paper, we study safe data collection for the purpose of policy evaluation in tabular Markov decision processes (MDPs).","In policy evaluation, we are given a \\textit{target} policy and asked to estimate the expected cumulative reward it will obtain.","Policy evaluation requires data and we are interested in the question of what \\textit{behavior} policy should collect the data for the most accurate evaluation of the target policy.","While prior work has considered behavior policy selection, in this paper, we additionally consider a safety constraint on the behavior policy.","Namely, we assume there exists a known default policy that incurs a particular expected cost when run and we enforce that the cumulative cost of all behavior policies ran is better than a constant factor of the cost that would be incurred had we always run the default policy.","We first show that there exists a class of intractable MDPs where no safe oracle algorithm with knowledge about problem parameters can efficiently collect data and satisfy the safety constraints.","We then define the tractability condition for an MDP such that a safe oracle algorithm can efficiently collect data and using that we prove the first lower bound for this setting.","We then introduce an algorithm SaVeR for this problem that approximates the safe oracle algorithm and bound the finite-sample mean squared error of the algorithm while ensuring it satisfies the safety constraint.","Finally, we show in simulations that SaVeR produces low MSE policy evaluation while satisfying the safety constraint."],"url":"http://arxiv.org/abs/2406.02165v1","category":"cs.LG"}
{"created":"2024-06-04 09:29:59","title":"Analyzing the Effect of Combined Degradations on Face Recognition","abstract":"A face recognition model is typically trained on large datasets of images that may be collected from controlled environments. This results in performance discrepancies when applied to real-world scenarios due to the domain gap between clean and in-the-wild images. Therefore, some researchers have investigated the robustness of these models by analyzing synthetic degradations. Yet, existing studies have mostly focused on single degradation factors, which may not fully capture the complexity of real-world degradations. This work addresses this problem by analyzing the impact of both single and combined degradations using a real-world degradation pipeline extended with under/over-exposure conditions. We use the LFW dataset for our experiments and assess the model's performance based on verification accuracy. Results reveal that single and combined degradations show dissimilar model behavior. The combined effect of degradation significantly lowers performance even if its single effect is negligible. This work emphasizes the importance of accounting for real-world complexity to assess the robustness of face recognition models in real-world settings. The code is publicly available at https://github.com/ThEnded32/AnalyzingCombinedDegradations.","sentences":["A face recognition model is typically trained on large datasets of images that may be collected from controlled environments.","This results in performance discrepancies when applied to real-world scenarios due to the domain gap between clean and in-the-wild images.","Therefore, some researchers have investigated the robustness of these models by analyzing synthetic degradations.","Yet, existing studies have mostly focused on single degradation factors, which may not fully capture the complexity of real-world degradations.","This work addresses this problem by analyzing the impact of both single and combined degradations using a real-world degradation pipeline extended with under/over-exposure conditions.","We use the LFW dataset for our experiments and assess the model's performance based on verification accuracy.","Results reveal that single and combined degradations show dissimilar model behavior.","The combined effect of degradation significantly lowers performance even if its single effect is negligible.","This work emphasizes the importance of accounting for real-world complexity to assess the robustness of face recognition models in real-world settings.","The code is publicly available at https://github.com/ThEnded32/AnalyzingCombinedDegradations."],"url":"http://arxiv.org/abs/2406.02142v1","category":"cs.CV"}
{"created":"2024-06-04 08:36:39","title":"UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models","abstract":"OwnThink stands as the most extensive Chinese open-domain knowledge graph introduced in recent times. Despite prior attempts in question answering over OwnThink (OQA), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering. In this paper, we introduce UniOQA, a unified framework that integrates two complementary parallel workflows. Unlike conventional approaches, UniOQA harnesses large language models (LLMs) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement. Initially, to bolster representation capacity, we fine-tune an LLM to translate questions into the Cypher query language (CQL), tackling issues associated with restricted semantic understanding and hallucinations. Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated CQL. Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation (RAG) process to the knowledge graph. Ultimately, we optimize answer accuracy through a dynamic decision algorithm. Experimental findings illustrate that UniOQA notably advances SpCQL Logical Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new state-of-the-art results on this benchmark. Through ablation experiments, we delve into the superior representation capacity of UniOQA and quantify its performance breakthrough.","sentences":["OwnThink stands as the most extensive Chinese open-domain knowledge graph introduced in recent times.","Despite prior attempts in question answering over OwnThink (OQA), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering.","In this paper, we introduce UniOQA, a unified framework that integrates two complementary parallel workflows.","Unlike conventional approaches, UniOQA harnesses large language models (LLMs) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement.","Initially, to bolster representation capacity, we fine-tune an LLM to translate questions into the Cypher query language (CQL), tackling issues associated with restricted semantic understanding and hallucinations.","Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated CQL.","Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation (RAG) process to the knowledge graph.","Ultimately, we optimize answer accuracy through a dynamic decision algorithm.","Experimental findings illustrate that UniOQA notably advances SpCQL Logical Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new state-of-the-art results on this benchmark.","Through ablation experiments, we delve into the superior representation capacity of UniOQA and quantify its performance breakthrough."],"url":"http://arxiv.org/abs/2406.02110v1","category":"cs.CL"}
{"created":"2024-06-04 08:33:56","title":"Kernel vs. Kernel: Exploring How the Data Structure Affects Neural Collapse","abstract":"Recently, a vast amount of literature has focused on the \"Neural Collapse\" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within class variability of the network's deepest features, dubbed as NC1. The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse. In this paper, we provide a kernel-based analysis that does not suffer from this limitation. First, given a kernel function, we establish expressions for the traces of the within- and between-class covariance matrices of the samples' features (and consequently an NC1 metric). Then, we turn to focus on kernels associated with shallow NNs. First, we consider the NN Gaussian Process kernel (NNGP), associated with the network at initialization, and the complement Neural Tangent Kernel (NTK), associated with its training in the \"lazy regime\". Interestingly, we show that the NTK does not represent more collapsed features than the NNGP for prototypical data models. As NC emerges from training, we then consider an alternative to NTK: the recently proposed adaptive kernel, which generalizes NNGP to model the feature mapping learned from the training data. Contrasting our NC1 analysis for these two kernels enables gaining insights into the effect of data distribution on the extent of collapse, which are empirically aligned with the behavior observed with practical training of NNs.","sentences":["Recently, a vast amount of literature has focused on the \"Neural Collapse\" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point.","The core component of NC is the decrease in the within class variability of the network's deepest features, dubbed as NC1.","The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse.","In this paper, we provide a kernel-based analysis that does not suffer from this limitation.","First, given a kernel function, we establish expressions for the traces of the within- and between-class covariance matrices of the samples' features (and consequently an NC1 metric).","Then, we turn to focus on kernels associated with shallow NNs.","First, we consider the NN Gaussian Process kernel (NNGP), associated with the network at initialization, and the complement Neural Tangent Kernel (NTK), associated with its training in the \"lazy regime\".","Interestingly, we show that the NTK does not represent more collapsed features than the NNGP for prototypical data models.","As NC emerges from training, we then consider an alternative to NTK: the recently proposed adaptive kernel, which generalizes NNGP to model the feature mapping learned from the training data.","Contrasting our NC1 analysis for these two kernels enables gaining insights into the effect of data distribution on the extent of collapse, which are empirically aligned with the behavior observed with practical training of NNs."],"url":"http://arxiv.org/abs/2406.02105v1","category":"cs.LG"}
{"created":"2024-06-04 08:33:17","title":"A Bayesian Approach to Online Planning","abstract":"The combination of Monte Carlo tree search and neural networks has revolutionized online planning. As neural network approximations are often imperfect, we ask whether uncertainty estimates about the network outputs could be used to improve planning. We develop a Bayesian planning approach that facilitates such uncertainty quantification, inspired by classical ideas from the meta-reasoning literature. We propose a Thompson sampling based algorithm for searching the tree of possible actions, for which we prove the first (to our knowledge) finite time Bayesian regret bound, and propose an efficient implementation for a restricted family of posterior distributions. In addition we propose a variant of the Bayes-UCB method applied to trees. Empirically, we demonstrate that on the ProcGen Maze and Leaper environments, when the uncertainty estimates are accurate but the neural network output is inaccurate, our Bayesian approach searches the tree much more effectively. In addition, we investigate whether popular uncertainty estimation methods are accurate enough to yield significant gains in planning. Our code is available at: https://github.com/nirgreshler/bayesian-online-planning.","sentences":["The combination of Monte Carlo tree search and neural networks has revolutionized online planning.","As neural network approximations are often imperfect, we ask whether uncertainty estimates about the network outputs could be used to improve planning.","We develop a Bayesian planning approach that facilitates such uncertainty quantification, inspired by classical ideas from the meta-reasoning literature.","We propose a Thompson sampling based algorithm for searching the tree of possible actions, for which we prove the first (to our knowledge) finite time Bayesian regret bound, and propose an efficient implementation for a restricted family of posterior distributions.","In addition we propose a variant of the Bayes-UCB method applied to trees.","Empirically, we demonstrate that on the ProcGen Maze and Leaper environments, when the uncertainty estimates are accurate but the neural network output is inaccurate, our Bayesian approach searches the tree much more effectively.","In addition, we investigate whether popular uncertainty estimation methods are accurate enough to yield significant gains in planning.","Our code is available at: https://github.com/nirgreshler/bayesian-online-planning."],"url":"http://arxiv.org/abs/2406.02103v1","category":"cs.AI"}
{"created":"2024-06-04 08:27:06","title":"MS-Mapping: Multi-session LiDAR Mapping with Wasserstein-based Keyframe Selection","abstract":"Large-scale multi-session LiDAR mapping plays a crucial role in various applications but faces significant challenges in data redundancy and pose graph scalability. This paper present MS-Mapping, a novel multi-session LiDAR mapping system that combines an incremental mapping scheme with support for various LiDAR-based odometry, enabling high-precision and consistent map assembly in large-scale environments. Our approach introduces a real-time keyframe selection method based on the Wasserstein distance, which effectively reduces data redundancy and pose graph complexity. We formulate the LiDAR point cloud keyframe selection problem using a similarity method based on Gaussian mixture models (GMM) and tackle the real-time challenge by employing an incremental voxel update method. Extensive experiments on large-scale campus scenes and over \\SI{12.8}{km} of public and self-collected datasets demonstrate the efficiency, accuracy, and consistency of our map assembly approach. To facilitate further research and development in the community, we make our code https://github.com/JokerJohn/MS-Mapping and datasets publicly available.","sentences":["Large-scale multi-session LiDAR mapping plays a crucial role in various applications but faces significant challenges in data redundancy and pose graph scalability.","This paper present MS-Mapping, a novel multi-session LiDAR mapping system that combines an incremental mapping scheme with support for various LiDAR-based odometry, enabling high-precision and consistent map assembly in large-scale environments.","Our approach introduces a real-time keyframe selection method based on the Wasserstein distance, which effectively reduces data redundancy and pose graph complexity.","We formulate the LiDAR point cloud keyframe selection problem using a similarity method based on Gaussian mixture models (GMM) and tackle the real-time challenge by employing an incremental voxel update method.","Extensive experiments on large-scale campus scenes and over \\SI{12.8}{km} of public and self-collected datasets demonstrate the efficiency, accuracy, and consistency of our map assembly approach.","To facilitate further research and development in the community, we make our code https://github.com/JokerJohn/MS-Mapping and datasets publicly available."],"url":"http://arxiv.org/abs/2406.02096v1","category":"cs.RO"}
{"created":"2024-06-04 08:23:57","title":"MaskSR: Masked Language Model for Full-band Speech Restoration","abstract":"Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.","sentences":["Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions.","Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored.","In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth.","MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec.","During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions.","During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling.","Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models."],"url":"http://arxiv.org/abs/2406.02092v1","category":"cs.SD"}
{"created":"2024-06-04 08:17:47","title":"How Western, Educated, Industrialized, Rich, and Democratic is Social Computing Research?","abstract":"Much of the research in social computing analyzes data from social media platforms, which may inherently carry biases. An overlooked source of such bias is the over-representation of WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations, which might not accurately mirror the global demographic diversity. We evaluated the dependence on WEIRD populations in research presented at the AAAI ICWSM conference; the only venue whose proceedings are fully dedicated to social computing research. We did so by analyzing 494 papers published from 2018 to 2022, which included full research papers, dataset papers and posters. After filtering out papers that analyze synthetic datasets or those lacking clear country of origin, we were left with 420 papers from which 188 participants in a crowdsourcing study with full manual validation extracted data for the WEIRD scores computation. This data was then used to adapt existing WEIRD metrics to be applicable for social media data. We found that 37% of these papers focused solely on data from Western countries. This percentage is significantly less than the percentages observed in research from CHI (76%) and FAccT (84%) conferences, suggesting a greater diversity of dataset origins within ICWSM. However, the studies at ICWSM still predominantly examine populations from countries that are more Educated, Industrialized, and Rich in comparison to those in FAccT, with a special note on the 'Democratic' variable reflecting political freedoms and rights. This points out the utility of social media data in shedding light on findings from countries with restricted political freedoms. Based on these insights, we recommend extensions of current \"paper checklists\" to include considerations about the WEIRD bias and call for the community to broaden research inclusivity by encouraging the use of diverse datasets from underrepresented regions.","sentences":["Much of the research in social computing analyzes data from social media platforms, which may inherently carry biases.","An overlooked source of such bias is the over-representation of WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations, which might not accurately mirror the global demographic diversity.","We evaluated the dependence on WEIRD populations in research presented at the AAAI ICWSM conference; the only venue whose proceedings are fully dedicated to social computing research.","We did so by analyzing 494 papers published from 2018 to 2022, which included full research papers, dataset papers and posters.","After filtering out papers that analyze synthetic datasets or those lacking clear country of origin, we were left with 420 papers from which 188 participants in a crowdsourcing study with full manual validation extracted data for the WEIRD scores computation.","This data was then used to adapt existing WEIRD metrics to be applicable for social media data.","We found that 37% of these papers focused solely on data from Western countries.","This percentage is significantly less than the percentages observed in research from CHI (76%) and FAccT (84%) conferences, suggesting a greater diversity of dataset origins within ICWSM.","However, the studies at ICWSM still predominantly examine populations from countries that are more Educated, Industrialized, and Rich in comparison to those in FAccT, with a special note on the 'Democratic' variable reflecting political freedoms and rights.","This points out the utility of social media data in shedding light on findings from countries with restricted political freedoms.","Based on these insights, we recommend extensions of current \"paper checklists\" to include considerations about the WEIRD bias and call for the community to broaden research inclusivity by encouraging the use of diverse datasets from underrepresented regions."],"url":"http://arxiv.org/abs/2406.02090v1","category":"cs.HC"}
{"created":"2024-06-04 08:04:23","title":"FightLadder: A Benchmark for Competitive Multi-Agent Reinforcement Learning","abstract":"Recent advances in reinforcement learning (RL) heavily rely on a variety of well-designed benchmarks, which provide environmental platforms and consistent criteria to evaluate existing and novel algorithms. Specifically, in multi-agent RL (MARL), a plethora of benchmarks based on cooperative games have spurred the development of algorithms that improve the scalability of cooperative multi-agent systems. However, for the competitive setting, a lightweight and open-sourced benchmark with challenging gaming dynamics and visual inputs has not yet been established. In this work, we present FightLadder, a real-time fighting game platform, to empower competitive MARL research. Along with the platform, we provide implementations of state-of-the-art MARL algorithms for competitive games, as well as a set of evaluation metrics to characterize the performance and exploitability of agents. We demonstrate the feasibility of this platform by training a general agent that consistently defeats 12 built-in characters in single-player mode, and expose the difficulty of training a non-exploitable agent without human knowledge and demonstrations in two-player mode. FightLadder provides meticulously designed environments to address critical challenges in competitive MARL research, aiming to catalyze a new era of discovery and advancement in the field. Videos and code at https://sites.google.com/view/fightladder/home.","sentences":["Recent advances in reinforcement learning (RL) heavily rely on a variety of well-designed benchmarks, which provide environmental platforms and consistent criteria to evaluate existing and novel algorithms.","Specifically, in multi-agent RL (MARL), a plethora of benchmarks based on cooperative games have spurred the development of algorithms that improve the scalability of cooperative multi-agent systems.","However, for the competitive setting, a lightweight and open-sourced benchmark with challenging gaming dynamics and visual inputs has not yet been established.","In this work, we present FightLadder, a real-time fighting game platform, to empower competitive MARL research.","Along with the platform, we provide implementations of state-of-the-art MARL algorithms for competitive games, as well as a set of evaluation metrics to characterize the performance and exploitability of agents.","We demonstrate the feasibility of this platform by training a general agent that consistently defeats 12 built-in characters in single-player mode, and expose the difficulty of training a non-exploitable agent without human knowledge and demonstrations in two-player mode.","FightLadder provides meticulously designed environments to address critical challenges in competitive MARL research, aiming to catalyze a new era of discovery and advancement in the field.","Videos and code at https://sites.google.com/view/fightladder/home."],"url":"http://arxiv.org/abs/2406.02081v1","category":"cs.MA"}
{"created":"2024-06-04 08:02:39","title":"LongSSM: On the Length Extension of State-space Models in Language Modelling","abstract":"In this paper, we investigate the length-extension of state-space models (SSMs) in language modeling. Length extension involves training models on short sequences and testing them on longer ones. We show that state-space models trained with zero hidden states initialization have difficulty doing length extension. We explain this difficulty by pointing out the length extension is equivalent to polynomial extrapolation. Based on the theory, we propose a simple yet effective method - changing the hidden states initialization scheme - to improve the length extension. Moreover, our method shows that using long training sequence length is beneficial but not necessary to length extension. Changing the hidden state initialization enables the efficient training of long-memory model with a smaller training context length.","sentences":["In this paper, we investigate the length-extension of state-space models (SSMs) in language modeling.","Length extension involves training models on short sequences and testing them on longer ones.","We show that state-space models trained with zero hidden states initialization have difficulty doing length extension.","We explain this difficulty by pointing out the length extension is equivalent to polynomial extrapolation.","Based on the theory, we propose a simple yet effective method - changing the hidden states initialization scheme - to improve the length extension.","Moreover, our method shows that using long training sequence length is beneficial but not necessary to length extension.","Changing the hidden state initialization enables the efficient training of long-memory model with a smaller training context length."],"url":"http://arxiv.org/abs/2406.02080v1","category":"cs.CL"}
{"created":"2024-06-04 07:58:19","title":"A Toolbox for Supporting Research on AI in Water Distribution Networks","abstract":"Drinking water is a vital resource for humanity, and thus, Water Distribution Networks (WDNs) are considered critical infrastructures in modern societies. The operation of WDNs is subject to diverse challenges such as water leakages and contamination, cyber/physical attacks, high energy consumption during pump operation, etc. With model-based methods reaching their limits due to various uncertainty sources, AI methods offer promising solutions to those challenges. In this work, we introduce a Python toolbox for complex scenario modeling \\& generation such that AI researchers can easily access challenging problems from the drinking water domain. Besides providing a high-level interface for the easy generation of hydraulic and water quality scenario data, it also provides easy access to popular event detection benchmarks and an environment for developing control algorithms.","sentences":["Drinking water is a vital resource for humanity, and thus, Water Distribution Networks (WDNs) are considered critical infrastructures in modern societies.","The operation of WDNs is subject to diverse challenges such as water leakages and contamination, cyber/physical attacks, high energy consumption during pump operation, etc.","With model-based methods reaching their limits due to various uncertainty sources, AI methods offer promising solutions to those challenges.","In this work, we introduce a Python toolbox for complex scenario modeling \\& generation such that AI researchers can easily access challenging problems from the drinking water domain.","Besides providing a high-level interface for the easy generation of hydraulic and water quality scenario data, it also provides easy access to popular event detection benchmarks and an environment for developing control algorithms."],"url":"http://arxiv.org/abs/2406.02078v1","category":"cs.AI"}
{"created":"2024-06-04 07:57:34","title":"Multi-target stain normalization for histology slides","abstract":"Traditional staining normalization approaches, e.g. Macenko, typically rely on the choice of a single representative reference image, which may not adequately account for the diverse staining patterns of datasets collected in practical scenarios. In this study, we introduce a novel approach that leverages multiple reference images to enhance robustness against stain variation. Our method is parameter-free and can be adopted in existing computational pathology pipelines with no significant changes. We evaluate the effectiveness of our method through experiments using a deep-learning pipeline for automatic nuclei segmentation on colorectal images. Our results show that by leveraging multiple reference images, better results can be achieved when generalizing to external data, where the staining can widely differ from the training set.","sentences":["Traditional staining normalization approaches, e.g. Macenko, typically rely on the choice of a single representative reference image, which may not adequately account for the diverse staining patterns of datasets collected in practical scenarios.","In this study, we introduce a novel approach that leverages multiple reference images to enhance robustness against stain variation.","Our method is parameter-free and can be adopted in existing computational pathology pipelines with no significant changes.","We evaluate the effectiveness of our method through experiments using a deep-learning pipeline for automatic nuclei segmentation on colorectal images.","Our results show that by leveraging multiple reference images, better results can be achieved when generalizing to external data, where the staining can widely differ from the training set."],"url":"http://arxiv.org/abs/2406.02077v1","category":"eess.IV"}
{"created":"2024-06-04 07:51:30","title":"PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling","abstract":"In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusin on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques achieving up to a 20.5 absolute accuracy improvement on TREC.","sentences":["In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing.","Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusin on critical tokens (a.k.a massive activation or attention sink) in higher layers.","Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method.","This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size.","Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage.","In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques achieving up to a 20.5 absolute accuracy improvement on TREC."],"url":"http://arxiv.org/abs/2406.02069v1","category":"cs.CL"}
{"created":"2024-06-04 07:43:33","title":"Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models","abstract":"Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical \"reasoning\"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW","sentences":["Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale.","These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models.","We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans.","The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical \"reasoning\"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible.","Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail.","We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks.","Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW"],"url":"http://arxiv.org/abs/2406.02061v1","category":"cs.LG"}
{"created":"2024-06-04 07:43:12","title":"I've got the \"Answer\"! Interpretation of LLMs Hidden States in Question Answering","abstract":"Interpretability and explainability of AI are becoming increasingly important in light of the rapid development of large language models (LLMs). This paper investigates the interpretation of LLMs in the context of the knowledge-based question answering. The main hypothesis of the study is that correct and incorrect model behavior can be distinguished at the level of hidden states. The quantized models LLaMA-2-7B-Chat, Mistral-7B, Vicuna-7B and the MuSeRC question-answering dataset are used to test this hypothesis. The results of the analysis support the proposed hypothesis. We also identify the layers which have a negative effect on the model's behavior. As a prospect of practical application of the hypothesis, we propose to train such \"weak\" layers additionally in order to improve the quality of the task solution.","sentences":["Interpretability and explainability of AI are becoming increasingly important in light of the rapid development of large language models (LLMs).","This paper investigates the interpretation of LLMs in the context of the knowledge-based question answering.","The main hypothesis of the study is that correct and incorrect model behavior can be distinguished at the level of hidden states.","The quantized models LLaMA-2-7B-Chat, Mistral-7B, Vicuna-7B and the MuSeRC question-answering dataset are used to test this hypothesis.","The results of the analysis support the proposed hypothesis.","We also identify the layers which have a negative effect on the model's behavior.","As a prospect of practical application of the hypothesis, we propose to train such \"weak\" layers additionally in order to improve the quality of the task solution."],"url":"http://arxiv.org/abs/2406.02060v1","category":"cs.CL"}
{"created":"2024-06-04 07:41:15","title":"Tabular and Deep Learning for the Whittle Index","abstract":"The Whittle index policy is a heuristic that has shown remarkably good performance (with guaranteed asymptotic optimality) when applied to the class of problems known as Restless Multi-Armed Bandit Problems (RMABPs). In this paper we present QWI and QWINN, two reinforcement learning algorithms, respectively tabular and deep, to learn the Whittle index for the total discounted criterion. The key feature is the use of two time-scales, a faster one to update the state-action Q -values, and a relatively slower one to update the Whittle indices. In our main theoretical result we show that QWI, which is a tabular implementation, converges to the real Whittle indices. We then present QWINN, an adaptation of QWI algorithm using neural networks to compute the Q -values on the faster time-scale, which is able to extrapolate information from one state to another and scales naturally to large state-space environments. For QWINN, we show that all local minima of the Bellman error are locally stable equilibria, which is the first result of its kind for DQN-based schemes. Numerical computations show that QWI and QWINN converge faster than the standard Q -learning algorithm, neural-network based approximate Q-learning and other state of the art algorithms.","sentences":["The Whittle index policy is a heuristic that has shown remarkably good performance (with guaranteed asymptotic optimality) when applied to the class of problems known as Restless Multi-Armed Bandit Problems (RMABPs).","In this paper we present QWI and QWINN, two reinforcement learning algorithms, respectively tabular and deep, to learn the Whittle index for the total discounted criterion.","The key feature is the use of two time-scales, a faster one to update the state-action Q -values, and a relatively slower one to update the Whittle indices.","In our main theoretical result we show that QWI, which is a tabular implementation, converges to the real Whittle indices.","We then present QWINN, an adaptation of QWI algorithm using neural networks to compute the Q -values on the faster time-scale, which is able to extrapolate information from one state to another and scales naturally to large state-space environments.","For QWINN, we show that all local minima of the Bellman error are locally stable equilibria, which is the first result of its kind for DQN-based schemes.","Numerical computations show that QWI and QWINN converge faster than the standard Q -learning algorithm, neural-network based approximate Q-learning and other state of the art algorithms."],"url":"http://arxiv.org/abs/2406.02057v1","category":"cs.AI"}
{"created":"2024-06-04 07:28:40","title":"WHOIS Right? An Analysis of WHOIS and RDAP Consistency","abstract":"Public registration information on domain names, such as the accredited registrar, the domain name expiration date, or the abusecontact is crucial for many security tasks, from automated abuse notifications to botnet or phishing detection and classification systems. Various domain registration data is usually accessible through the WHOIS or RDAP protocols-a priori they provide the same data but use distinct formats and communication protocols. While WHOIS aims to provide human-readable data, RDAP uses a machine-readable format. Therefore, deciding which protocol to use is generally considered a straightforward technical choice, depending on the use case and the required automation and security level. In this paper, we examine the core assumption that WHOIS and RDAP offer the same data and that users can query them interchangeably. By collecting, processing, and comparing 164 million WHOIS and RDAP records for a sample of 55 million domain names, we reveal that while the data obtained through WHOIS and RDAP is generally consistent, 7.6% of the observed domains still present inconsistent data on important fields like IANA ID, creation date, or nameservers. Such variances should receive careful consideration from security stakeholders reliant on the accuracy of these fields.","sentences":["Public registration information on domain names, such as the accredited registrar, the domain name expiration date, or the abusecontact is crucial for many security tasks, from automated abuse notifications to botnet or phishing detection and classification systems.","Various domain registration data is usually accessible through the WHOIS or RDAP protocols-a priori they provide the same data but use distinct formats and communication protocols.","While WHOIS aims to provide human-readable data, RDAP uses a machine-readable format.","Therefore, deciding which protocol to use is generally considered a straightforward technical choice, depending on the use case and the required automation and security level.","In this paper, we examine the core assumption that WHOIS and RDAP offer the same data and that users can query them interchangeably.","By collecting, processing, and comparing 164 million WHOIS and RDAP records for a sample of 55 million domain names, we reveal that while the data obtained through WHOIS and RDAP is generally consistent, 7.6% of the observed domains still present inconsistent data on important fields like IANA ID, creation date, or nameservers.","Such variances should receive careful consideration from security stakeholders reliant on the accuracy of these fields."],"url":"http://arxiv.org/abs/2406.02046v1","category":"cs.NI"}
{"created":"2024-06-04 07:24:51","title":"DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback Alignment","abstract":"Graph neural networks are recognized for their strong performance across various applications, with the backpropagation algorithm playing a central role in the development of most GNN models. However, despite its effectiveness, BP has limitations that challenge its biological plausibility and affect the efficiency, scalability and parallelism of training neural networks for graph-based tasks. While several non-BP training algorithms, such as the direct feedback alignment, have been successfully applied to fully-connected and convolutional network components for handling Euclidean data, directly adapting these non-BP frameworks to manage non-Euclidean graph data in GNN models presents significant challenges. These challenges primarily arise from the violation of the i.i.d. assumption in graph data and the difficulty in accessing prediction errors for all samples (nodes) within the graph. To overcome these obstacles, in this paper we propose DFA-GNN, a novel forward learning framework tailored for GNNs with a case study of semi-supervised learning. The proposed method breaks the limitations of BP by using a dedicated forward training mechanism. Specifically, DFA-GNN extends the principles of DFA to adapt to graph data and unique architecture of GNNs, which incorporates the information of graph topology into the feedback links to accommodate the non-Euclidean characteristics of graph data. Additionally, for semi-supervised graph learning tasks, we developed a pseudo error generator that spreads residual errors from training data to create a pseudo error for each unlabeled node. These pseudo errors are then utilized to train GNNs using DFA. Extensive experiments on 10 public benchmarks reveal that our learning framework outperforms not only previous non-BP methods but also the standard BP methods, and it exhibits excellent robustness against various types of noise and attacks.","sentences":["Graph neural networks are recognized for their strong performance across various applications, with the backpropagation algorithm playing a central role in the development of most GNN models.","However, despite its effectiveness, BP has limitations that challenge its biological plausibility and affect the efficiency, scalability and parallelism of training neural networks for graph-based tasks.","While several non-BP training algorithms, such as the direct feedback alignment, have been successfully applied to fully-connected and convolutional network components for handling Euclidean data, directly adapting these non-BP frameworks to manage non-Euclidean graph data in GNN models presents significant challenges.","These challenges primarily arise from the violation of the i.i.d. assumption in graph data and the difficulty in accessing prediction errors for all samples (nodes) within the graph.","To overcome these obstacles, in this paper we propose DFA-GNN, a novel forward learning framework tailored for GNNs with a case study of semi-supervised learning.","The proposed method breaks the limitations of BP by using a dedicated forward training mechanism.","Specifically, DFA-GNN extends the principles of DFA to adapt to graph data and unique architecture of GNNs, which incorporates the information of graph topology into the feedback links to accommodate the non-Euclidean characteristics of graph data.","Additionally, for semi-supervised graph learning tasks, we developed a pseudo error generator that spreads residual errors from training data to create a pseudo error for each unlabeled node.","These pseudo errors are then utilized to train GNNs using DFA.","Extensive experiments on 10 public benchmarks reveal that our learning framework outperforms not only previous non-BP methods but also the standard BP methods, and it exhibits excellent robustness against various types of noise and attacks."],"url":"http://arxiv.org/abs/2406.02040v1","category":"cs.LG"}
{"created":"2024-06-04 07:22:12","title":"A Unifying Framework for Action-Conditional Self-Predictive Reinforcement Learning","abstract":"Learning a good representation is a crucial challenge for Reinforcement Learning (RL) agents. Self-predictive learning provides means to jointly learn a latent representation and dynamics model by bootstrapping from future latent representations (BYOL). Recent work has developed theoretical insights into these algorithms by studying a continuous-time ODE model for self-predictive representation learning under the simplifying assumption that the algorithm depends on a fixed policy (BYOL-$\\Pi$); this assumption is at odds with practical instantiations of such algorithms, which explicitly condition their predictions on future actions. In this work, we take a step towards bridging the gap between theory and practice by analyzing an action-conditional self-predictive objective (BYOL-AC) using the ODE framework, characterizing its convergence properties and highlighting important distinctions between the limiting solutions of the BYOL-$\\Pi$ and BYOL-AC dynamics. We show how the two representations are related by a variance equation. This connection leads to a novel variance-like action-conditional objective (BYOL-VAR) and its corresponding ODE. We unify the study of all three objectives through two complementary lenses; a model-based perspective, where each objective is shown to be equivalent to a low-rank approximation of certain dynamics, and a model-free perspective, which establishes relationships between the objectives and their respective value, Q-value, and advantage function. Our empirical investigations, encompassing both linear function approximation and Deep RL environments, demonstrates that BYOL-AC is better overall in a variety of different settings.","sentences":["Learning a good representation is a crucial challenge for Reinforcement Learning (RL) agents.","Self-predictive learning provides means to jointly learn a latent representation and dynamics model by bootstrapping from future latent representations (BYOL).","Recent work has developed theoretical insights into these algorithms by studying a continuous-time ODE model for self-predictive representation learning under the simplifying assumption that the algorithm depends on a fixed policy (BYOL-$\\Pi$); this assumption is at odds with practical instantiations of such algorithms, which explicitly condition their predictions on future actions.","In this work, we take a step towards bridging the gap between theory and practice by analyzing an action-conditional self-predictive objective (BYOL-AC) using the ODE framework, characterizing its convergence properties and highlighting important distinctions between the limiting solutions of the BYOL-$\\Pi$ and BYOL-AC dynamics.","We show how the two representations are related by a variance equation.","This connection leads to a novel variance-like action-conditional objective (BYOL-VAR) and its corresponding ODE.","We unify the study of all three objectives through two complementary lenses; a model-based perspective, where each objective is shown to be equivalent to a low-rank approximation of certain dynamics, and a model-free perspective, which establishes relationships between the objectives and their respective value, Q-value, and advantage function.","Our empirical investigations, encompassing both linear function approximation and Deep RL environments, demonstrates that BYOL-AC is better overall in a variety of different settings."],"url":"http://arxiv.org/abs/2406.02035v1","category":"cs.LG"}
{"created":"2024-06-04 07:13:23","title":"Multimodal Reasoning with Multimodal Knowledge Graph","abstract":"Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM's parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.","sentences":["Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs.","Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding.","In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs.","In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment.","A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining.","Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM's parameter size.","Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models."],"url":"http://arxiv.org/abs/2406.02030v1","category":"cs.CL"}
{"created":"2024-06-04 07:06:06","title":"Inference Attacks in Machine Learning as a Service: A Taxonomy, Review, and Promising Directions","abstract":"The prosperity of machine learning has also brought people's concerns about data privacy. Among them, inference attacks can implement privacy breaches in various MLaaS scenarios and model training/prediction phases. Specifically, inference attacks can perform privacy inference on undisclosed target training sets based on outputs of the target model, including but not limited to statistics, membership, semantics, data representation, etc. For instance, infer whether the target data has the characteristics of AIDS. In addition, the rapid development of the machine learning community in recent years, especially the surge of model types and application scenarios, has further stimulated the inference attacks' research. Thus, studying inference attacks and analyzing them in depth is urgent and significant. However, there is still a gap in the systematic discussion of inference attacks from taxonomy, global perspective, attack, and defense perspectives. This survey provides an in-depth and comprehensive inference of attacks and corresponding countermeasures in ML-as-a-service based on taxonomy and the latest researches. Without compromising researchers' intuition, we first propose the 3MP taxonomy based on the community research status, trying to normalize the confusing naming system of inference attacks. Also, we analyze the pros and cons of each type of inference attack, their workflow, countermeasure, and how they interact with other attacks. In the end, we point out several promising directions for researchers from a more comprehensive and novel perspective.","sentences":["The prosperity of machine learning has also brought people's concerns about data privacy.","Among them, inference attacks can implement privacy breaches in various MLaaS scenarios and model training/prediction phases.","Specifically, inference attacks can perform privacy inference on undisclosed target training sets based on outputs of the target model, including but not limited to statistics, membership, semantics, data representation, etc.","For instance, infer whether the target data has the characteristics of AIDS.","In addition, the rapid development of the machine learning community in recent years, especially the surge of model types and application scenarios, has further stimulated the inference attacks' research.","Thus, studying inference attacks and analyzing them in depth is urgent and significant.","However, there is still a gap in the systematic discussion of inference attacks from taxonomy, global perspective, attack, and defense perspectives.","This survey provides an in-depth and comprehensive inference of attacks and corresponding countermeasures in ML-as-a-service based on taxonomy and the latest researches.","Without compromising researchers' intuition, we first propose the 3MP taxonomy based on the community research status, trying to normalize the confusing naming system of inference attacks.","Also, we analyze the pros and cons of each type of inference attack, their workflow, countermeasure, and how they interact with other attacks.","In the end, we point out several promising directions for researchers from a more comprehensive and novel perspective."],"url":"http://arxiv.org/abs/2406.02027v1","category":"cs.LG"}
{"created":"2024-06-04 07:00:14","title":"MetaMixer Is All You Need","abstract":"Transformer, composed of self-attention and Feed-Forward Network, has revolutionized the landscape of network design across various vision tasks. FFN is a versatile operator seamlessly integrated into nearly all AI models to effectively harness rich representations. Recent works also show that FFN functions like key-value memories. Thus, akin to the query-key-value mechanism within self-attention, FFN can be viewed as a memory network, where the input serves as query and the two projection weights operate as keys and values, respectively. We hypothesize that the importance lies in query-key-value framework itself rather than in self-attention. To verify this, we propose converting self-attention into a more FFN-like efficient token mixer with only convolutions while retaining query-key-value framework, namely FFNification. Specifically, FFNification replaces query-key and attention coefficient-value interactions with large kernel convolutions and adopts GELU activation function instead of softmax. The derived token mixer, FFNified attention, serves as key-value memories for detecting locally distributed spatial patterns, and operates in the opposite dimension to the ConvNeXt block within each corresponding sub-operation of the query-key-value framework. Building upon the above two modules, we present a family of Fast-Forward Networks. Our FFNet achieves remarkable performance improvements over previous state-of-the-art methods across a wide range of tasks. The strong and general performance of our proposed method validates our hypothesis and leads us to introduce MetaMixer, a general mixer architecture that does not specify sub-operations within the query-key-value framework. We show that using only simple operations like convolution and GELU in the MetaMixer can achieve superior performance.","sentences":["Transformer, composed of self-attention and Feed-Forward Network, has revolutionized the landscape of network design across various vision tasks.","FFN is a versatile operator seamlessly integrated into nearly all AI models to effectively harness rich representations.","Recent works also show that FFN functions like key-value memories.","Thus, akin to the query-key-value mechanism within self-attention, FFN can be viewed as a memory network, where the input serves as query and the two projection weights operate as keys and values, respectively.","We hypothesize that the importance lies in query-key-value framework itself rather than in self-attention.","To verify this, we propose converting self-attention into a more FFN-like efficient token mixer with only convolutions while retaining query-key-value framework, namely FFNification.","Specifically, FFNification replaces query-key and attention coefficient-value interactions with large kernel convolutions and adopts GELU activation function instead of softmax.","The derived token mixer, FFNified attention, serves as key-value memories for detecting locally distributed spatial patterns, and operates in the opposite dimension to the ConvNeXt block within each corresponding sub-operation of the query-key-value framework.","Building upon the above two modules, we present a family of Fast-Forward Networks.","Our FFNet achieves remarkable performance improvements over previous state-of-the-art methods across a wide range of tasks.","The strong and general performance of our proposed method validates our hypothesis and leads us to introduce MetaMixer, a general mixer architecture that does not specify sub-operations within the query-key-value framework.","We show that using only simple operations like convolution and GELU in the MetaMixer can achieve superior performance."],"url":"http://arxiv.org/abs/2406.02021v1","category":"cs.CV"}
{"created":"2024-06-04 06:57:47","title":"Why Would You Suggest That? Human Trust in Language Model Responses","abstract":"The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.","sentences":["The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount.","Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance.","Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses.","Position and faithfulness of these explanations are also important factors.","However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation.","Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems."],"url":"http://arxiv.org/abs/2406.02018v1","category":"cs.CL"}
{"created":"2024-06-04 06:44:00","title":"Review of searches for new physics at CMS","abstract":"A review of recent results from searches for new physics is presented. The analyses exploit data sets collected by the CMS experiment during Run 2 of the CERN LHC. Searches for exotic particles decaying into two bosons and for heavy neutral leptons, as well as a model-agnostic search based on anomaly detection are summarised. No significant sign for the presence of new physics was found. Prospects for the CMS parking strategy during Run 3 are also discussed.","sentences":["A review of recent results from searches for new physics is presented.","The analyses exploit data sets collected by the CMS experiment during Run 2 of the CERN LHC.","Searches for exotic particles decaying into two bosons and for heavy neutral leptons, as well as a model-agnostic search based on anomaly detection are summarised.","No significant sign for the presence of new physics was found.","Prospects for the CMS parking strategy during Run 3 are also discussed."],"url":"http://arxiv.org/abs/2406.02010v1","category":"hep-ex"}
{"created":"2024-06-04 06:39:45","title":"ODE-based Learning to Optimize","abstract":"Recent years have seen a growing interest in understanding acceleration methods through the lens of ordinary differential equations (ODEs). Despite the theoretical advancements, translating the rapid convergence observed in continuous-time models to discrete-time iterative methods poses significant challenges. In this paper, we present a comprehensive framework integrating the inertial systems with Hessian-driven damping equation (ISHD) and learning-based approaches for developing optimization methods through a deep synergy of theoretical insights. We first establish the convergence condition for ensuring the convergence of the solution trajectory of ISHD. Then, we show that provided the stability condition, another relaxed requirement on the coefficients of ISHD, the sequence generated through the explicit Euler discretization of ISHD converges, which gives a large family of practical optimization methods. In order to select the best optimization method in this family for certain problems, we introduce the stopping time, the time required for an optimization method derived from ISHD to achieve a predefined level of suboptimality. Then, we formulate a novel learning to optimize (L2O) problem aimed at minimizing the stopping time subject to the convergence and stability condition. To navigate this learning problem, we present an algorithm combining stochastic optimization and the penalty method (StoPM). The convergence of StoPM using the conservative gradient is proved. Empirical validation of our framework is conducted through extensive numerical experiments across a diverse set of optimization problems. These experiments showcase the superior performance of the learned optimization methods.","sentences":["Recent years have seen a growing interest in understanding acceleration methods through the lens of ordinary differential equations (ODEs).","Despite the theoretical advancements, translating the rapid convergence observed in continuous-time models to discrete-time iterative methods poses significant challenges.","In this paper, we present a comprehensive framework integrating the inertial systems with Hessian-driven damping equation (ISHD) and learning-based approaches for developing optimization methods through a deep synergy of theoretical insights.","We first establish the convergence condition for ensuring the convergence of the solution trajectory of ISHD.","Then, we show that provided the stability condition, another relaxed requirement on the coefficients of ISHD, the sequence generated through the explicit Euler discretization of ISHD converges, which gives a large family of practical optimization methods.","In order to select the best optimization method in this family for certain problems, we introduce the stopping time, the time required for an optimization method derived from ISHD to achieve a predefined level of suboptimality.","Then, we formulate a novel learning to optimize (L2O) problem aimed at minimizing the stopping time subject to the convergence and stability condition.","To navigate this learning problem, we present an algorithm combining stochastic optimization and the penalty method (StoPM).","The convergence of StoPM using the conservative gradient is proved.","Empirical validation of our framework is conducted through extensive numerical experiments across a diverse set of optimization problems.","These experiments showcase the superior performance of the learned optimization methods."],"url":"http://arxiv.org/abs/2406.02006v1","category":"math.OC"}
{"created":"2024-06-04 06:33:13","title":"Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue","abstract":"The core of the dialogue system is to generate relevant, informative, and human-like responses based on extensive dialogue history. Recently, dialogue generation domain has seen mainstream adoption of large language models (LLMs), due to its powerful capability in generating utterances. However, there is a natural deficiency for such models, that is, inherent position bias, which may lead them to pay more attention to the nearby utterances instead of causally relevant ones, resulting in generating irrelevant and generic responses in long-term dialogue. To alleviate such problem, in this paper, we propose a novel method, named Causal Perception long-term Dialogue framework (CPD), which employs perturbation-based causal variable discovery method to extract casually relevant utterances from the dialogue history and enhances model causal perception during fine-tuning. Specifically, a local-position awareness method is proposed in CPD for inter-sentence position correlation elimination, which helps models extract causally relevant utterances based on perturbations. Then, a casual-perception fine-tuning strategy is also proposed, to enhance the capability of discovering the causal invariant factors, by differently perturbing causally relevant and non-casually relevant ones for response generation. Experimental results on two datasets prove that our proposed method can effectively alleviate the position bias for multiple LLMs and achieve significant progress compared with existing baselines.","sentences":["The core of the dialogue system is to generate relevant, informative, and human-like responses based on extensive dialogue history.","Recently, dialogue generation domain has seen mainstream adoption of large language models (LLMs), due to its powerful capability in generating utterances.","However, there is a natural deficiency for such models, that is, inherent position bias, which may lead them to pay more attention to the nearby utterances instead of causally relevant ones, resulting in generating irrelevant and generic responses in long-term dialogue.","To alleviate such problem, in this paper, we propose a novel method, named Causal Perception long-term Dialogue framework (CPD), which employs perturbation-based causal variable discovery method to extract casually relevant utterances from the dialogue history and enhances model causal perception during fine-tuning.","Specifically, a local-position awareness method is proposed in CPD for inter-sentence position correlation elimination, which helps models extract causally relevant utterances based on perturbations.","Then, a casual-perception fine-tuning strategy is also proposed, to enhance the capability of discovering the causal invariant factors, by differently perturbing causally relevant and non-casually relevant ones for response generation.","Experimental results on two datasets prove that our proposed method can effectively alleviate the position bias for multiple LLMs and achieve significant progress compared with existing baselines."],"url":"http://arxiv.org/abs/2406.02002v1","category":"cs.CL"}
{"created":"2024-06-04 06:27:48","title":"Bayesian Mesh Optimization for Graph Neural Networks to Enhance Engineering Performance Prediction","abstract":"In engineering design, surrogate models are widely employed to replace computationally expensive simulations by leveraging design variables and geometric parameters from computer-aided design (CAD) models. However, these models often lose critical information when simplified to lower dimensions and face challenges in parameter definition, especially with the complex 3D shapes commonly found in industrial datasets. To address these limitations, we propose a Bayesian graph neural network (GNN) framework for a 3D deep-learning-based surrogate model that predicts engineering performance by directly learning geometric features from CAD using mesh representation. Our framework determines the optimal size of mesh elements through Bayesian optimization, resulting in a high-accuracy surrogate model. Additionally, it effectively handles the irregular and complex structures of 3D CADs, which differ significantly from the regular and uniform pixel structures of 2D images typically used in deep learning. Experimental results demonstrate that the quality of the mesh significantly impacts the prediction accuracy of the surrogate model, with an optimally sized mesh achieving superior performance. We compare the performance of models based on various 3D representations such as voxel, point cloud, and graph, and evaluate the computational costs of Monte Carlo simulation and Bayesian optimization methods to find the optimal mesh size. We anticipate that our proposed framework has the potential to be applied to mesh-based simulations across various engineering fields, leveraging physics-based information commonly used in computer-aided engineering.","sentences":["In engineering design, surrogate models are widely employed to replace computationally expensive simulations by leveraging design variables and geometric parameters from computer-aided design (CAD) models.","However, these models often lose critical information when simplified to lower dimensions and face challenges in parameter definition, especially with the complex 3D shapes commonly found in industrial datasets.","To address these limitations, we propose a Bayesian graph neural network (GNN) framework for a 3D deep-learning-based surrogate model that predicts engineering performance by directly learning geometric features from CAD using mesh representation.","Our framework determines the optimal size of mesh elements through Bayesian optimization, resulting in a high-accuracy surrogate model.","Additionally, it effectively handles the irregular and complex structures of 3D CADs, which differ significantly from the regular and uniform pixel structures of 2D images typically used in deep learning.","Experimental results demonstrate that the quality of the mesh significantly impacts the prediction accuracy of the surrogate model, with an optimally sized mesh achieving superior performance.","We compare the performance of models based on various 3D representations such as voxel, point cloud, and graph, and evaluate the computational costs of Monte Carlo simulation and Bayesian optimization methods to find the optimal mesh size.","We anticipate that our proposed framework has the potential to be applied to mesh-based simulations across various engineering fields, leveraging physics-based information commonly used in computer-aided engineering."],"url":"http://arxiv.org/abs/2406.01996v1","category":"cs.LG"}
{"created":"2024-06-04 06:09:49","title":"Personalized Topic Selection Model for Topic-Grounded Dialogue","abstract":"Recently, the topic-grounded dialogue (TGD) system has become increasingly popular as its powerful capability to actively guide users to accomplish specific tasks through topic-guided conversations. Most existing works utilize side information (\\eg topics or personas) in isolation to enhance the topic selection ability. However, due to disregarding the noise within these auxiliary information sources and their mutual influence, current models tend to predict user-uninteresting and contextually irrelevant topics. To build user-engaging and coherent dialogue agent, we propose a \\textbf{P}ersonalized topic s\\textbf{E}lection model for \\textbf{T}opic-grounded \\textbf{D}ialogue, named \\textbf{PETD}, which takes account of the interaction of side information to selectively aggregate such information for more accurately predicting subsequent topics. Specifically, we evaluate the correlation between global topics and personas and selectively incorporate the global topics aligned with user personas. Furthermore, we propose a contrastive learning based persona selector to filter out irrelevant personas under the constraint of lacking pertinent persona annotations. Throughout the selection and generation, diverse relevant side information is considered. Extensive experiments demonstrate that our proposed method can generate engaging and diverse responses, outperforming state-of-the-art baselines across various evaluation metrics.","sentences":["Recently, the topic-grounded dialogue (TGD) system has become increasingly popular as its powerful capability to actively guide users to accomplish specific tasks through topic-guided conversations.","Most existing works utilize side information (\\eg topics or personas) in isolation to enhance the topic selection ability.","However, due to disregarding the noise within these auxiliary information sources and their mutual influence, current models tend to predict user-uninteresting and contextually irrelevant topics.","To build user-engaging and coherent dialogue agent, we propose a \\textbf{P}ersonalized topic s\\textbf{E}lection model for \\textbf{T}opic-grounded \\textbf{D}ialogue, named \\textbf{PETD}, which takes account of the interaction of side information to selectively aggregate such information for more accurately predicting subsequent topics.","Specifically, we evaluate the correlation between global topics and personas and selectively incorporate the global topics aligned with user personas.","Furthermore, we propose a contrastive learning based persona selector to filter out irrelevant personas under the constraint of lacking pertinent persona annotations.","Throughout the selection and generation, diverse relevant side information is considered.","Extensive experiments demonstrate that our proposed method can generate engaging and diverse responses, outperforming state-of-the-art baselines across various evaluation metrics."],"url":"http://arxiv.org/abs/2406.01988v1","category":"cs.CL"}
{"created":"2024-06-04 05:47:17","title":"Zyda: A 1.3T Dataset for Open Language Modeling","abstract":"The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.","sentences":["The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly.","State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens.","This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining.","In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus.","We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets.","Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite.","Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently."],"url":"http://arxiv.org/abs/2406.01981v1","category":"cs.CL"}
{"created":"2024-06-04 05:06:00","title":"The Crystal Ball Hypothesis in diffusion models: Anticipating object positions from initial noise","abstract":"Diffusion models have achieved remarkable success in text-to-image generation tasks; however, the role of initial noise has been rarely explored. In this study, we identify specific regions within the initial noise image, termed trigger patches, that play a key role for object generation in the resulting images. Notably, these patches are ``universal'' and can be generalized across various positions, seeds, and prompts. To be specific, extracting these patches from one noise and injecting them into another noise leads to object generation in targeted areas. We identify these patches by analyzing the dispersion of object bounding boxes across generated images, leading to the development of a posterior analysis technique. Furthermore, we create a dataset consisting of Gaussian noises labeled with bounding boxes corresponding to the objects appearing in the generated images and train a detector that identifies these patches from the initial noise. To explain the formation of these patches, we reveal that they are outliers in Gaussian noise, and follow distinct distributions through two-sample tests. Finally, we find the misalignment between prompts and the trigger patch patterns can result in unsuccessful image generations. The study proposes a reject-sampling strategy to obtain optimal noise, aiming to improve prompt adherence and positional diversity in image generation.","sentences":["Diffusion models have achieved remarkable success in text-to-image generation tasks; however, the role of initial noise has been rarely explored.","In this study, we identify specific regions within the initial noise image, termed trigger patches, that play a key role for object generation in the resulting images.","Notably, these patches are ``universal'' and can be generalized across various positions, seeds, and prompts.","To be specific, extracting these patches from one noise and injecting them into another noise leads to object generation in targeted areas.","We identify these patches by analyzing the dispersion of object bounding boxes across generated images, leading to the development of a posterior analysis technique.","Furthermore, we create a dataset consisting of Gaussian noises labeled with bounding boxes corresponding to the objects appearing in the generated images and train a detector that identifies these patches from the initial noise.","To explain the formation of these patches, we reveal that they are outliers in Gaussian noise, and follow distinct distributions through two-sample tests.","Finally, we find the misalignment between prompts and the trigger patch patterns can result in unsuccessful image generations.","The study proposes a reject-sampling strategy to obtain optimal noise, aiming to improve prompt adherence and positional diversity in image generation."],"url":"http://arxiv.org/abs/2406.01970v1","category":"cs.CV"}
{"created":"2024-06-04 05:00:24","title":"Cross-Embodiment Robot Manipulation Skill Transfer using Latent Space Alignment","abstract":"This paper focuses on transferring control policies between robot manipulators with different morphology. While reinforcement learning (RL) methods have shown successful results in robot manipulation tasks, transferring a trained policy from simulation to a real robot or deploying it on a robot with different states, actions, or kinematics is challenging. To achieve cross-embodiment policy transfer, our key insight is to project the state and action spaces of the source and target robots to a common latent space representation. We first introduce encoders and decoders to associate the states and actions of the source robot with a latent space. The encoders, decoders, and a latent space control policy are trained simultaneously using loss functions measuring task performance, latent dynamics consistency, and encoder-decoder ability to reconstruct the original states and actions. To transfer the learned control policy, we only need to train target encoders and decoders that align a new target domain to the latent space. We use generative adversarial training with cycle consistency and latent dynamics losses without access to the task reward or reward tuning in the target domain. We demonstrate sim-to-sim and sim-to-real manipulation policy transfer with source and target robots of different states, actions, and embodiments. The source code is available at \\url{https://github.com/ExistentialRobotics/cross_embodiment_transfer}.","sentences":["This paper focuses on transferring control policies between robot manipulators with different morphology.","While reinforcement learning (RL) methods have shown successful results in robot manipulation tasks, transferring a trained policy from simulation to a real robot or deploying it on a robot with different states, actions, or kinematics is challenging.","To achieve cross-embodiment policy transfer, our key insight is to project the state and action spaces of the source and target robots to a common latent space representation.","We first introduce encoders and decoders to associate the states and actions of the source robot with a latent space.","The encoders, decoders, and a latent space control policy are trained simultaneously using loss functions measuring task performance, latent dynamics consistency, and encoder-decoder ability to reconstruct the original states and actions.","To transfer the learned control policy, we only need to train target encoders and decoders that align a new target domain to the latent space.","We use generative adversarial training with cycle consistency and latent dynamics losses without access to the task reward or reward tuning in the target domain.","We demonstrate sim-to-sim and sim-to-real manipulation policy transfer with source and target robots of different states, actions, and embodiments.","The source code is available at \\url{https://github.com/ExistentialRobotics/cross_embodiment_transfer}."],"url":"http://arxiv.org/abs/2406.01968v1","category":"cs.RO"}
{"created":"2024-06-04 04:53:05","title":"DrEureka: Language Model Guided Sim-To-Real Transfer","abstract":"Transferring policies learned in simulation to the real world is a promising strategy for acquiring robot skills at scale. However, sim-to-real approaches typically rely on manual design and tuning of the task reward function as well as the simulation physics parameters, rendering the process slow and human-labor intensive. In this paper, we investigate using Large Language Models (LLMs) to automate and accelerate sim-to-real design. Our LLM-guided sim-to-real approach, DrEureka, requires only the physics simulation for the target task and automatically constructs suitable reward functions and domain randomization distributions to support real-world transfer. We first demonstrate that our approach can discover sim-to-real configurations that are competitive with existing human-designed ones on quadruped locomotion and dexterous manipulation tasks. Then, we showcase that our approach is capable of solving novel robot tasks, such as quadruped balancing and walking atop a yoga ball, without iterative manual design.","sentences":["Transferring policies learned in simulation to the real world is a promising strategy for acquiring robot skills at scale.","However, sim-to-real approaches typically rely on manual design and tuning of the task reward function as well as the simulation physics parameters, rendering the process slow and human-labor intensive.","In this paper, we investigate using Large Language Models (LLMs) to automate and accelerate sim-to-real design.","Our LLM-guided sim-to-real approach, DrEureka, requires only the physics simulation for the target task and automatically constructs suitable reward functions and domain randomization distributions to support real-world transfer.","We first demonstrate that our approach can discover sim-to-real configurations that are competitive with existing human-designed ones on quadruped locomotion and dexterous manipulation tasks.","Then, we showcase that our approach is capable of solving novel robot tasks, such as quadruped balancing and walking atop a yoga ball, without iterative manual design."],"url":"http://arxiv.org/abs/2406.01967v1","category":"cs.RO"}
{"created":"2024-06-04 04:43:30","title":"Certifiably Byzantine-Robust Federated Conformal Prediction","abstract":"Conformal prediction has shown impressive capacity in constructing statistically rigorous prediction sets for machine learning models with exchangeable data samples. The siloed datasets, coupled with the escalating privacy concerns related to local data sharing, have inspired recent innovations extending conformal prediction into federated environments with distributed data samples. However, this framework for distributed uncertainty quantification is susceptible to Byzantine failures. A minor subset of malicious clients can significantly compromise the practicality of coverage guarantees. To address this vulnerability, we introduce a novel framework Rob-FCP, which executes robust federated conformal prediction, effectively countering malicious clients capable of reporting arbitrary statistics with the conformal calibration process. We theoretically provide the conformal coverage bound of Rob-FCP in the Byzantine setting and show that the coverage of Rob-FCP is asymptotically close to the desired coverage level. We also propose a malicious client number estimator to tackle a more challenging setting where the number of malicious clients is unknown to the defender and theoretically shows its effectiveness. We empirically demonstrate the robustness of Rob-FCP against diverse proportions of malicious clients under a variety of Byzantine attacks on five standard benchmark and real-world healthcare datasets.","sentences":["Conformal prediction has shown impressive capacity in constructing statistically rigorous prediction sets for machine learning models with exchangeable data samples.","The siloed datasets, coupled with the escalating privacy concerns related to local data sharing, have inspired recent innovations extending conformal prediction into federated environments with distributed data samples.","However, this framework for distributed uncertainty quantification is susceptible to Byzantine failures.","A minor subset of malicious clients can significantly compromise the practicality of coverage guarantees.","To address this vulnerability, we introduce a novel framework Rob-FCP, which executes robust federated conformal prediction, effectively countering malicious clients capable of reporting arbitrary statistics with the conformal calibration process.","We theoretically provide the conformal coverage bound of Rob-FCP in the Byzantine setting and show that the coverage of Rob-FCP is asymptotically close to the desired coverage level.","We also propose a malicious client number estimator to tackle a more challenging setting where the number of malicious clients is unknown to the defender and theoretically shows its effectiveness.","We empirically demonstrate the robustness of Rob-FCP against diverse proportions of malicious clients under a variety of Byzantine attacks on five standard benchmark and real-world healthcare datasets."],"url":"http://arxiv.org/abs/2406.01960v1","category":"cs.LG"}
{"created":"2024-06-04 04:16:38","title":"Improving Generalization in Aerial and Terrestrial Mobile Robots Control Through Delayed Policy Learning","abstract":"Deep Reinforcement Learning (DRL) has emerged as a promising approach to enhancing motion control and decision-making through a wide range of robotic applications. While prior research has demonstrated the efficacy of DRL algorithms in facilitating autonomous mapless navigation for aerial and terrestrial mobile robots, these methods often grapple with poor generalization when faced with unknown tasks and environments. This paper explores the impact of the Delayed Policy Updates (DPU) technique on fostering generalization to new situations, and bolstering the overall performance of agents. Our analysis of DPU in aerial and terrestrial mobile robots reveals that this technique significantly curtails the lack of generalization and accelerates the learning process for agents, enhancing their efficiency across diverse tasks and unknown scenarios.","sentences":["Deep Reinforcement Learning (DRL) has emerged as a promising approach to enhancing motion control and decision-making through a wide range of robotic applications.","While prior research has demonstrated the efficacy of DRL algorithms in facilitating autonomous mapless navigation for aerial and terrestrial mobile robots, these methods often grapple with poor generalization when faced with unknown tasks and environments.","This paper explores the impact of the Delayed Policy Updates (DPU) technique on fostering generalization to new situations, and bolstering the overall performance of agents.","Our analysis of DPU in aerial and terrestrial mobile robots reveals that this technique significantly curtails the lack of generalization and accelerates the learning process for agents, enhancing their efficiency across diverse tasks and unknown scenarios."],"url":"http://arxiv.org/abs/2406.01952v1","category":"cs.RO"}
{"created":"2024-06-04 04:03:07","title":"A Comparative Study of Sampling Methods with Cross-Validation in the FedHome Framework","abstract":"This paper presents a comparative study of sampling methods within the FedHome framework, designed for personalized in-home health monitoring. FedHome leverages federated learning (FL) and generative convolutional autoencoders (GCAE) to train models on decentralized edge devices while prioritizing data privacy. A notable challenge in this domain is the class imbalance in health data, where critical events such as falls are underrepresented, adversely affecting model performance. To address this, the research evaluates six oversampling techniques using Stratified K-fold cross-validation: SMOTE, Borderline-SMOTE, Random OverSampler, SMOTE-Tomek, SVM-SMOTE, and SMOTE-ENN. These methods are tested on FedHome's public implementation over 200 training rounds with and without stratified K-fold cross-validation. The findings indicate that SMOTE-ENN achieves the most consistent test accuracy, with a standard deviation range of 0.0167-0.0176, demonstrating stable performance compared to other samplers. In contrast, SMOTE and SVM-SMOTE exhibit higher variability in performance, as reflected by their wider standard deviation ranges of 0.0157-0.0180 and 0.0155-0.0180, respectively. Similarly, the Random OverSampler method shows a significant deviation range of 0.0155-0.0176. SMOTE-Tomek, with a deviation range of 0.0160-0.0175, also shows greater stability but not as much as SMOTE-ENN. This finding highlights the potential of SMOTE-ENN to enhance the reliability and accuracy of personalized health monitoring systems within the FedHome framework.","sentences":["This paper presents a comparative study of sampling methods within the FedHome framework, designed for personalized in-home health monitoring.","FedHome leverages federated learning (FL) and generative convolutional autoencoders (GCAE) to train models on decentralized edge devices while prioritizing data privacy.","A notable challenge in this domain is the class imbalance in health data, where critical events such as falls are underrepresented, adversely affecting model performance.","To address this, the research evaluates six oversampling techniques using Stratified K-fold cross-validation: SMOTE, Borderline-SMOTE, Random OverSampler, SMOTE-Tomek, SVM-SMOTE, and SMOTE-ENN.","These methods are tested on FedHome's public implementation over 200 training rounds with and without stratified K-fold cross-validation.","The findings indicate that SMOTE-ENN achieves the most consistent test accuracy, with a standard deviation range of 0.0167-0.0176, demonstrating stable performance compared to other samplers.","In contrast, SMOTE and SVM-SMOTE exhibit higher variability in performance, as reflected by their wider standard deviation ranges of 0.0157-0.0180 and 0.0155-0.0180, respectively.","Similarly, the Random OverSampler method shows a significant deviation range of 0.0155-0.0176.","SMOTE-Tomek, with a deviation range of 0.0160-0.0175, also shows greater stability but not as much as SMOTE-ENN.","This finding highlights the potential of SMOTE-ENN to enhance the reliability and accuracy of personalized health monitoring systems within the FedHome framework."],"url":"http://arxiv.org/abs/2406.01950v1","category":"cs.LG"}
{"created":"2024-06-04 03:54:53","title":"Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs","abstract":"This paper surveys evaluation techniques to enhance the trustworthiness and understanding of Large Language Models (LLMs). As reliance on LLMs grows, ensuring their reliability, fairness, and transparency is crucial. We explore algorithmic methods and metrics to assess LLM performance, identify weaknesses, and guide development towards more trustworthy applications. Key evaluation metrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR, BERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot Learning Performance, Transfer Learning Evaluation, Adversarial Testing, and Fairness and Bias Evaluation. We introduce innovative approaches like LLMMaps for stratified evaluation, Benchmarking and Leaderboards for competitive assessment, Stratified Analysis for in-depth understanding, Visualization of Blooms Taxonomy for cognitive level accuracy distribution, Hallucination Score for quantifying inaccuracies, Knowledge Stratification Strategy for hierarchical analysis, and Machine Learning Models for Hierarchy Generation. Human Evaluation is highlighted for capturing nuances that automated metrics may miss. These techniques form a framework for evaluating LLMs, aiming to enhance transparency, guide development, and establish user trust. Future papers will describe metric visualization and demonstrate each approach on practical examples.","sentences":["This paper surveys evaluation techniques to enhance the trustworthiness and understanding of Large Language Models (LLMs).","As reliance on LLMs grows, ensuring their reliability, fairness, and transparency is crucial.","We explore algorithmic methods and metrics to assess LLM performance, identify weaknesses, and guide development towards more trustworthy applications.","Key evaluation metrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR, BERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot Learning Performance, Transfer Learning Evaluation, Adversarial Testing, and Fairness and Bias Evaluation.","We introduce innovative approaches like LLMMaps for stratified evaluation, Benchmarking and Leaderboards for competitive assessment, Stratified Analysis for in-depth understanding, Visualization of Blooms Taxonomy for cognitive level accuracy distribution, Hallucination Score for quantifying inaccuracies, Knowledge Stratification Strategy for hierarchical analysis, and Machine Learning Models for Hierarchy Generation.","Human Evaluation is highlighted for capturing nuances that automated metrics may miss.","These techniques form a framework for evaluating LLMs, aiming to enhance transparency, guide development, and establish user trust.","Future papers will describe metric visualization and demonstrate each approach on practical examples."],"url":"http://arxiv.org/abs/2406.01943v1","category":"cs.CL"}
{"created":"2024-06-04 03:48:08","title":"Speeding up Policy Simulation in Supply Chain RL","abstract":"Simulating a single trajectory of a dynamical system under some state-dependent policy is a core bottleneck in policy optimization algorithms. The many inherently serial policy evaluations that must be performed in a single simulation constitute the bulk of this bottleneck. To wit, in applying policy optimization to supply chain optimization (SCO) problems, simulating a single month of a supply chain can take several hours.   We present an iterative algorithm for policy simulation, which we dub Picard Iteration. This scheme carefully assigns policy evaluation tasks to independent processes. Within an iteration, a single process evaluates the policy only on its assigned tasks while assuming a certain 'cached' evaluation for other tasks; the cache is updated at the end of the iteration. Implemented on GPUs, this scheme admits batched evaluation of the policy on a single trajectory. We prove that the structure afforded by many SCO problems allows convergence in a small number of iterations, independent of the horizon. We demonstrate practical speedups of 400x on large-scale SCO problems even with a single GPU, and also demonstrate practical efficacy in other RL environments.","sentences":["Simulating a single trajectory of a dynamical system under some state-dependent policy is a core bottleneck in policy optimization algorithms.","The many inherently serial policy evaluations that must be performed in a single simulation constitute the bulk of this bottleneck.","To wit, in applying policy optimization to supply chain optimization (SCO) problems, simulating a single month of a supply chain can take several hours.   ","We present an iterative algorithm for policy simulation, which we dub Picard Iteration.","This scheme carefully assigns policy evaluation tasks to independent processes.","Within an iteration, a single process evaluates the policy only on its assigned tasks while assuming a certain 'cached' evaluation for other tasks; the cache is updated at the end of the iteration.","Implemented on GPUs, this scheme admits batched evaluation of the policy on a single trajectory.","We prove that the structure afforded by many SCO problems allows convergence in a small number of iterations, independent of the horizon.","We demonstrate practical speedups of 400x on large-scale SCO problems even with a single GPU, and also demonstrate practical efficacy in other RL environments."],"url":"http://arxiv.org/abs/2406.01939v1","category":"cs.AI"}
{"created":"2024-06-04 03:31:42","title":"Detecting Endangered Marine Species in Autonomous Underwater Vehicle Imagery Using Point Annotations and Few-Shot Learning","abstract":"One use of Autonomous Underwater Vehicles (AUVs) is the monitoring of habitats associated with threatened, endangered and protected marine species, such as the handfish of Tasmania, Australia. Seafloor imagery collected by AUVs can be used to identify individuals within their broader habitat context, but the sheer volume of imagery collected can overwhelm efforts to locate rare or cryptic individuals. Machine learning models can be used to identify the presence of a particular species in images using a trained object detector, but the lack of training examples reduces detection performance, particularly for rare species that may only have a small number of examples in the wild. In this paper, inspired by recent work in few-shot learning, images and annotations of common marine species are exploited to enhance the ability of the detector to identify rare and cryptic species. Annotated images of six common marine species are used in two ways. Firstly, the common species are used in a pre-training step to allow the backbone to create rich features for marine species. Secondly, a copy-paste operation is used with the common species images to augment the training data. While annotations for more common marine species are available in public datasets, they are often in point format, which is unsuitable for training an object detector. A popular semantic segmentation model efficiently generates bounding box annotations for training from the available point annotations. Our proposed framework is applied to AUV images of handfish, increasing average precision by up to 48\\% compared to baseline object detection training. This approach can be applied to other objects with low numbers of annotations and promises to increase the ability to actively monitor threatened, endangered and protected species.","sentences":["One use of Autonomous Underwater Vehicles (AUVs) is the monitoring of habitats associated with threatened, endangered and protected marine species, such as the handfish of Tasmania, Australia.","Seafloor imagery collected by AUVs can be used to identify individuals within their broader habitat context, but the sheer volume of imagery collected can overwhelm efforts to locate rare or cryptic individuals.","Machine learning models can be used to identify the presence of a particular species in images using a trained object detector, but the lack of training examples reduces detection performance, particularly for rare species that may only have a small number of examples in the wild.","In this paper, inspired by recent work in few-shot learning, images and annotations of common marine species are exploited to enhance the ability of the detector to identify rare and cryptic species.","Annotated images of six common marine species are used in two ways.","Firstly, the common species are used in a pre-training step to allow the backbone to create rich features for marine species.","Secondly, a copy-paste operation is used with the common species images to augment the training data.","While annotations for more common marine species are available in public datasets, they are often in point format, which is unsuitable for training an object detector.","A popular semantic segmentation model efficiently generates bounding box annotations for training from the available point annotations.","Our proposed framework is applied to AUV images of handfish, increasing average precision by up to 48\\% compared to baseline object detection training.","This approach can be applied to other objects with low numbers of annotations and promises to increase the ability to actively monitor threatened, endangered and protected species."],"url":"http://arxiv.org/abs/2406.01932v1","category":"cs.CV"}
{"created":"2024-06-04 03:26:15","title":"Fast networked data selection via distributed smoothed quantile estimation","abstract":"Collecting the most informative data from a large dataset distributed over a network is a fundamental problem in many fields, including control, signal processing and machine learning. In this paper, we establish a connection between selecting the most informative data and finding the top-$k$ elements of a multiset. The top-$k$ selection in a network can be formulated as a distributed nonsmooth convex optimization problem known as quantile estimation. Unfortunately, the lack of smoothness in the local objective functions leads to extremely slow convergence and poor scalability with respect to the network size. To overcome the deficiency, we propose an accelerated method that employs smoothing techniques. Leveraging the piecewise linearity of the local objective functions in quantile estimation, we characterize the iteration complexity required to achieve top-$k$ selection, a challenging task due to the lack of strong convexity. Several numerical results are provided to validate the effectiveness of the algorithm and the correctness of the theory.","sentences":["Collecting the most informative data from a large dataset distributed over a network is a fundamental problem in many fields, including control, signal processing and machine learning.","In this paper, we establish a connection between selecting the most informative data and finding the top-$k$ elements of a multiset.","The top-$k$ selection in a network can be formulated as a distributed nonsmooth convex optimization problem known as quantile estimation.","Unfortunately, the lack of smoothness in the local objective functions leads to extremely slow convergence and poor scalability with respect to the network size.","To overcome the deficiency, we propose an accelerated method that employs smoothing techniques.","Leveraging the piecewise linearity of the local objective functions in quantile estimation, we characterize the iteration complexity required to achieve top-$k$ selection, a challenging task due to the lack of strong convexity.","Several numerical results are provided to validate the effectiveness of the algorithm and the correctness of the theory."],"url":"http://arxiv.org/abs/2406.01929v1","category":"eess.SY"}
{"created":"2024-06-04 03:04:21","title":"CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models","abstract":"Large Multi-modal Models (LMMs) have recently demonstrated remarkable abilities in visual context understanding and coherent response generation. However, alongside these advancements, the issue of hallucinations has emerged as a significant challenge, producing erroneous responses that are unrelated to the visual contents. In this paper, we introduce a novel contrastive-based decoding method, COuntering DEscription Contrastive Decoding (CODE), which leverages self-generated descriptions as contrasting references during the decoding phase of LMMs to address hallucination issues. CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content. By dynamically adjusting the information flow and distribution of next-token predictions in the LMM's vocabulary, CODE enhances the coherence and informativeness of generated responses. Extensive experiments demonstrate that our method significantly reduces hallucinations and improves cross-modal consistency across various benchmarks and cutting-edge LMMs. Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training.","sentences":["Large Multi-modal Models (LMMs) have recently demonstrated remarkable abilities in visual context understanding and coherent response generation.","However, alongside these advancements, the issue of hallucinations has emerged as a significant challenge, producing erroneous responses that are unrelated to the visual contents.","In this paper, we introduce a novel contrastive-based decoding method, COuntering DEscription Contrastive Decoding (CODE), which leverages self-generated descriptions as contrasting references during the decoding phase of LMMs to address hallucination issues.","CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content.","By dynamically adjusting the information flow and distribution of next-token predictions in the LMM's vocabulary, CODE enhances the coherence and informativeness of generated responses.","Extensive experiments demonstrate that our method significantly reduces hallucinations and improves cross-modal consistency across various benchmarks and cutting-edge LMMs.","Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training."],"url":"http://arxiv.org/abs/2406.01920v1","category":"cs.CV"}
{"created":"2024-06-04 02:59:36","title":"GOMAA-Geo: GOal Modality Agnostic Active Geo-localization","abstract":"We consider the task of active geo-localization (AGL) in which an agent uses a sequence of visual cues observed during aerial navigation to find a target specified through multiple possible modalities. This could emulate a UAV involved in a search-and-rescue operation navigating through an area, observing a stream of aerial images as it goes. The AGL task is associated with two important challenges. Firstly, an agent must deal with a goal specification in one of multiple modalities (e.g., through a natural language description) while the search cues are provided in other modalities (aerial imagery). The second challenge is limited localization time (e.g., limited battery life, urgency) so that the goal must be localized as efficiently as possible, i.e. the agent must effectively leverage its sequentially observed aerial views when searching for the goal. To address these challenges, we propose GOMAA-Geo - a goal modality agnostic active geo-localization agent - for zero-shot generalization between different goal modalities. Our approach combines cross-modality contrastive learning to align representations across modalities with supervised foundation model pretraining and reinforcement learning to obtain highly effective navigation and localization policies. Through extensive evaluations, we show that GOMAA-Geo outperforms alternative learnable approaches and that it generalizes across datasets - e.g., to disaster-hit areas without seeing a single disaster scenario during training - and goal modalities - e.g., to ground-level imagery or textual descriptions, despite only being trained with goals specified as aerial views. Code and models are publicly available at https://github.com/mvrl/GOMAA-Geo/tree/main.","sentences":["We consider the task of active geo-localization (AGL) in which an agent uses a sequence of visual cues observed during aerial navigation to find a target specified through multiple possible modalities.","This could emulate a UAV involved in a search-and-rescue operation navigating through an area, observing a stream of aerial images as it goes.","The AGL task is associated with two important challenges.","Firstly, an agent must deal with a goal specification in one of multiple modalities (e.g., through a natural language description) while the search cues are provided in other modalities (aerial imagery).","The second challenge is limited localization time (e.g., limited battery life, urgency) so that the goal must be localized as efficiently as possible, i.e. the agent must effectively leverage its sequentially observed aerial views when searching for the goal.","To address these challenges, we propose GOMAA-Geo - a goal modality agnostic active geo-localization agent - for zero-shot generalization between different goal modalities.","Our approach combines cross-modality contrastive learning to align representations across modalities with supervised foundation model pretraining and reinforcement learning to obtain highly effective navigation and localization policies.","Through extensive evaluations, we show that GOMAA-Geo outperforms alternative learnable approaches and that it generalizes across datasets - e.g., to disaster-hit areas without seeing a single disaster scenario during training - and goal modalities - e.g., to ground-level imagery or textual descriptions, despite only being trained with goals specified as aerial views.","Code and models are publicly available at https://github.com/mvrl/GOMAA-Geo/tree/main."],"url":"http://arxiv.org/abs/2406.01917v1","category":"cs.CV"}
{"created":"2024-06-04 02:51:26","title":"HPE-CogVLM: New Head Pose Grounding Task Exploration on Vision Language Model","abstract":"Head pose estimation (HPE) task requires a sophisticated understanding of 3D spatial relationships and precise numerical output of yaw, pitch, and roll Euler angles. Previous HPE studies are mainly based on Non-large language models (Non-LLMs), which rely on close-up human heads cropped from the full image as inputs and lack robustness in real-world scenario. In this paper, we present a novel framework to enhance the HPE prediction task by leveraging the visual grounding capability of CogVLM. CogVLM is a vision language model (VLM) with grounding capability of predicting object bounding boxes (BBoxes), which enables HPE training and prediction using full image information input. To integrate the HPE task into the VLM, we first cop with the catastrophic forgetting problem in large language models (LLMs) by investigating the rehearsal ratio in the data rehearsal method. Then, we propose and validate a LoRA layer-based model merging method, which keeps the integrity of parameters, to enhance the HPE performance in the framework. The results show our HPE-CogVLM achieves a 31.5\\% reduction in Mean Absolute Error for HPE prediction over the current Non-LLM based state-of-the-art in cross-dataset evaluation. Furthermore, we compare our LoRA layer-based model merging method with LoRA fine-tuning only and other merging methods in CogVLM. The results demonstrate our framework outperforms them in all HPE metrics.","sentences":["Head pose estimation (HPE) task requires a sophisticated understanding of 3D spatial relationships and precise numerical output of yaw, pitch, and roll Euler angles.","Previous HPE studies are mainly based on Non-large language models (Non-LLMs), which rely on close-up human heads cropped from the full image as inputs and lack robustness in real-world scenario.","In this paper, we present a novel framework to enhance the HPE prediction task by leveraging the visual grounding capability of CogVLM.","CogVLM is a vision language model (VLM) with grounding capability of predicting object bounding boxes (BBoxes), which enables HPE training and prediction using full image information input.","To integrate the HPE task into the VLM, we first cop with the catastrophic forgetting problem in large language models (LLMs) by investigating the rehearsal ratio in the data rehearsal method.","Then, we propose and validate a LoRA layer-based model merging method, which keeps the integrity of parameters, to enhance the HPE performance in the framework.","The results show our HPE-CogVLM achieves a 31.5\\% reduction in Mean Absolute Error for HPE prediction over the current Non-LLM based state-of-the-art in cross-dataset evaluation.","Furthermore, we compare our LoRA layer-based model merging method with LoRA fine-tuning only and other merging methods in CogVLM.","The results demonstrate our framework outperforms them in all HPE metrics."],"url":"http://arxiv.org/abs/2406.01914v1","category":"cs.CV"}
{"created":"2024-06-04 02:50:19","title":"Generating Synthetic Net Load Data with Physics-informed Diffusion Model","abstract":"This paper presents a novel physics-informed diffusion model for generating synthetic net load data, addressing the challenges of data scarcity and privacy concerns. The proposed framework embeds physical models within denoising networks, offering a versatile approach that can be readily generalized to unforeseen scenarios. A conditional denoising neural network is designed to jointly train the parameters of the transition kernel of the diffusion model and the parameters of the physics-informed function. Utilizing the real-world smart meter data from Pecan Street, we validate the proposed method and conduct a thorough numerical study comparing its performance with state-of-the-art generative models, including generative adversarial networks, variational autoencoders, normalizing flows, and a well calibrated baseline diffusion model. A comprehensive set of evaluation metrics is used to assess the accuracy and diversity of the generated synthetic net load data. The numerical study results demonstrate that the proposed physics-informed diffusion model outperforms state-of-the-art models across all quantitative metrics, yielding at least 20% improvement.","sentences":["This paper presents a novel physics-informed diffusion model for generating synthetic net load data, addressing the challenges of data scarcity and privacy concerns.","The proposed framework embeds physical models within denoising networks, offering a versatile approach that can be readily generalized to unforeseen scenarios.","A conditional denoising neural network is designed to jointly train the parameters of the transition kernel of the diffusion model and the parameters of the physics-informed function.","Utilizing the real-world smart meter data from Pecan Street, we validate the proposed method and conduct a thorough numerical study comparing its performance with state-of-the-art generative models, including generative adversarial networks, variational autoencoders, normalizing flows, and a well calibrated baseline diffusion model.","A comprehensive set of evaluation metrics is used to assess the accuracy and diversity of the generated synthetic net load data.","The numerical study results demonstrate that the proposed physics-informed diffusion model outperforms state-of-the-art models across all quantitative metrics, yielding at least 20% improvement."],"url":"http://arxiv.org/abs/2406.01913v1","category":"cs.LG"}
{"created":"2024-06-04 01:57:37","title":"Large Language Model-Enabled Multi-Agent Manufacturing Systems","abstract":"Traditional manufacturing faces challenges adapting to dynamic environments and quickly responding to manufacturing changes. The use of multi-agent systems has improved adaptability and coordination but requires further advancements in rapid human instruction comprehension, operational adaptability, and coordination through natural language integration. Large language models like GPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents to communicate in natural language and interpret human instructions for decision-making. This research introduces a novel framework where large language models enhance the capabilities of agents in manufacturing, making them more adaptable, and capable of processing context-specific instructions. A case study demonstrates the practical application of this framework, showing how agents can effectively communicate, understand tasks, and execute manufacturing processes, including precise G-code allocation among agents. The findings highlight the importance of continuous large language model integration into multi-agent manufacturing systems and the development of sophisticated agent communication protocols for a more flexible manufacturing system.","sentences":["Traditional manufacturing faces challenges adapting to dynamic environments and quickly responding to manufacturing changes.","The use of multi-agent systems has improved adaptability and coordination but requires further advancements in rapid human instruction comprehension, operational adaptability, and coordination through natural language integration.","Large language models like GPT-3.5 and GPT-4 enhance multi-agent manufacturing systems by enabling agents to communicate in natural language and interpret human instructions for decision-making.","This research introduces a novel framework where large language models enhance the capabilities of agents in manufacturing, making them more adaptable, and capable of processing context-specific instructions.","A case study demonstrates the practical application of this framework, showing how agents can effectively communicate, understand tasks, and execute manufacturing processes, including precise G-code allocation among agents.","The findings highlight the importance of continuous large language model integration into multi-agent manufacturing systems and the development of sophisticated agent communication protocols for a more flexible manufacturing system."],"url":"http://arxiv.org/abs/2406.01893v1","category":"cs.MA"}
{"created":"2024-06-04 01:48:15","title":"Windex: Realtime Neural Whittle Indexing for Scalable Service Guarantees in NextG Cellular Networks","abstract":"We address the resource allocation challenges in NextG cellular radio access networks (RAN), where heterogeneous user applications demand guarantees on throughput and service regularity. We leverage the Whittle indexability property to decompose the resource allocation problem, enabling the independent computation of relative priorities for each user. By simply allocating resources in decreasing order of these indices, we transform the combinatorial complexity of resource allocation into a linear one. We propose Windex, a lightweight approach for training neural networks to compute Whittle indices, considering constraint violation, channel quality, and system load. Implemented on a real-time RAN intelligent controller (RIC), our approach enables resource allocation decision times of less than 20$\\mu$s per user and efficiently allocates resources in each 1ms scheduling time slot. Evaluation across standardized 3GPP service classes demonstrates significant improvements in service guarantees compared to existing schedulers, validated through simulations and emulations with over-the-air channel traces on a 5G testbed.","sentences":["We address the resource allocation challenges in NextG cellular radio access networks (RAN), where heterogeneous user applications demand guarantees on throughput and service regularity.","We leverage the Whittle indexability property to decompose the resource allocation problem, enabling the independent computation of relative priorities for each user.","By simply allocating resources in decreasing order of these indices, we transform the combinatorial complexity of resource allocation into a linear one.","We propose Windex, a lightweight approach for training neural networks to compute Whittle indices, considering constraint violation, channel quality, and system load.","Implemented on a real-time RAN intelligent controller (RIC), our approach enables resource allocation decision times of less than 20$\\mu$s per user and efficiently allocates resources in each 1ms scheduling time slot.","Evaluation across standardized 3GPP service classes demonstrates significant improvements in service guarantees compared to existing schedulers, validated through simulations and emulations with over-the-air channel traces on a 5G testbed."],"url":"http://arxiv.org/abs/2406.01888v1","category":"eess.SY"}
{"created":"2024-06-04 01:31:20","title":"HoneyGPT: Breaking the Trilemma in Terminal Honeypots with Large Language Model","abstract":"Honeypots, as a strategic cyber-deception mechanism designed to emulate authentic interactions and bait unauthorized entities, continue to struggle with balancing flexibility, interaction depth, and deceptive capability despite their evolution over decades. Often they also lack the capability of proactively adapting to an attacker's evolving tactics, which restricts the depth of engagement and subsequent information gathering. Under this context, the emergent capabilities of large language models, in tandem with pioneering prompt-based engineering techniques, offer a transformative shift in the design and deployment of honeypot technologies. In this paper, we introduce HoneyGPT, a pioneering honeypot architecture based on ChatGPT, heralding a new era of intelligent honeypot solutions characterized by their cost-effectiveness, high adaptability, and enhanced interactivity, coupled with a predisposition for proactive attacker engagement. Furthermore, we present a structured prompt engineering framework that augments long-term interaction memory and robust security analytics. This framework, integrating thought of chain tactics attuned to honeypot contexts, enhances interactivity and deception, deepens security analytics, and ensures sustained engagement.   The evaluation of HoneyGPT includes two parts: a baseline comparison based on a collected dataset and a field evaluation in real scenarios for four weeks. The baseline comparison demonstrates HoneyGPT's remarkable ability to strike a balance among flexibility, interaction depth, and deceptive capability. The field evaluation further validates HoneyGPT's efficacy, showing its marked superiority in enticing attackers into more profound interactive engagements and capturing a wider array of novel attack vectors in comparison to existing honeypot technologies.","sentences":["Honeypots, as a strategic cyber-deception mechanism designed to emulate authentic interactions and bait unauthorized entities, continue to struggle with balancing flexibility, interaction depth, and deceptive capability despite their evolution over decades.","Often they also lack the capability of proactively adapting to an attacker's evolving tactics, which restricts the depth of engagement and subsequent information gathering.","Under this context, the emergent capabilities of large language models, in tandem with pioneering prompt-based engineering techniques, offer a transformative shift in the design and deployment of honeypot technologies.","In this paper, we introduce HoneyGPT, a pioneering honeypot architecture based on ChatGPT, heralding a new era of intelligent honeypot solutions characterized by their cost-effectiveness, high adaptability, and enhanced interactivity, coupled with a predisposition for proactive attacker engagement.","Furthermore, we present a structured prompt engineering framework that augments long-term interaction memory and robust security analytics.","This framework, integrating thought of chain tactics attuned to honeypot contexts, enhances interactivity and deception, deepens security analytics, and ensures sustained engagement.   ","The evaluation of HoneyGPT includes two parts: a baseline comparison based on a collected dataset and a field evaluation in real scenarios for four weeks.","The baseline comparison demonstrates HoneyGPT's remarkable ability to strike a balance among flexibility, interaction depth, and deceptive capability.","The field evaluation further validates HoneyGPT's efficacy, showing its marked superiority in enticing attackers into more profound interactive engagements and capturing a wider array of novel attack vectors in comparison to existing honeypot technologies."],"url":"http://arxiv.org/abs/2406.01882v1","category":"cs.CR"}
{"created":"2024-06-04 01:08:00","title":"GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security","abstract":"Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems. Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table. This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights. Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s. This study revisits this foundational problem within the context of large language models. Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table. We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy. The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain.","sentences":["Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems.","Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table.","This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights.","Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s.","This study revisits this foundational problem within the context of large language models.","Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table.","We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy.","The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain."],"url":"http://arxiv.org/abs/2406.01876v1","category":"cs.DB"}
{"created":"2024-06-04 00:41:47","title":"Fruit Classification System with Deep Learning and Neural Architecture Search","abstract":"The fruit identification process involves analyzing and categorizing different types of fruits based on their visual characteristics. This activity can be achieved using a range of methodologies, encompassing manual examination, conventional computer vision methodologies, and more sophisticated methodologies employing machine learning and deep learning. Our study identified a total of 15 distinct categories of fruit, consisting of class Avocado, Banana, Cherry, Apple Braeburn, Apple golden 1, Apricot, Grape, Kiwi, Mango, Orange, Papaya, Peach, Pineapple, Pomegranate and Strawberry. Neural Architecture Search (NAS) is a technological advancement employed within the realm of deep learning and artificial intelligence, to automate conceptualizing and refining neural network topologies. NAS aims to identify neural network structures that are highly suitable for tasks, such as the detection of fruits. Our suggested model with 99.98% mAP increased the detection performance of the preceding research study that used Fruit datasets. In addition, after the completion of the study, a comparative analysis was carried out to assess the findings in conjunction with those of another research that is connected to the topic. When compared to the findings of earlier studies, the detector that was proposed exhibited higher performance in terms of both its accuracy and its precision.","sentences":["The fruit identification process involves analyzing and categorizing different types of fruits based on their visual characteristics.","This activity can be achieved using a range of methodologies, encompassing manual examination, conventional computer vision methodologies, and more sophisticated methodologies employing machine learning and deep learning.","Our study identified a total of 15 distinct categories of fruit, consisting of class Avocado, Banana, Cherry, Apple Braeburn, Apple golden 1, Apricot, Grape, Kiwi, Mango, Orange, Papaya, Peach, Pineapple, Pomegranate and Strawberry.","Neural Architecture Search (NAS) is a technological advancement employed within the realm of deep learning and artificial intelligence, to automate conceptualizing and refining neural network topologies.","NAS aims to identify neural network structures that are highly suitable for tasks, such as the detection of fruits.","Our suggested model with 99.98% mAP increased the detection performance of the preceding research study that used Fruit datasets.","In addition, after the completion of the study, a comparative analysis was carried out to assess the findings in conjunction with those of another research that is connected to the topic.","When compared to the findings of earlier studies, the detector that was proposed exhibited higher performance in terms of both its accuracy and its precision."],"url":"http://arxiv.org/abs/2406.01869v1","category":"cs.CV"}
{"created":"2024-06-04 00:30:37","title":"Towards Effective Time-Aware Language Representation: Exploring Enhanced Temporal Understanding in Language Models","abstract":"In the evolving field of Natural Language Processing, understanding the temporal context of text is increasingly crucial. This study investigates methods to incorporate temporal information during pre-training, aiming to achieve effective time-aware language representation for improved performance on time-related tasks. In contrast to common pre-trained models like BERT, which rely on synchronic document collections such as BookCorpus and Wikipedia, our research introduces BiTimeBERT 2.0, a novel language model pre-trained on a temporal news article collection. BiTimeBERT 2.0 utilizes this temporal news collection, focusing on three innovative pre-training objectives: Time-Aware Masked Language Modeling (TAMLM), Document Dating (DD), and Time-Sensitive Entity Replacement (TSER). Each objective targets a unique aspect of temporal information. TAMLM is designed to enhance the understanding of temporal contexts and relations, DD integrates document timestamps as chronological markers, and TSER focuses on the temporal dynamics of \"Person\" entities, recognizing their inherent temporal significance. The experimental results consistently demonstrate that BiTimeBERT 2.0 outperforms models like BERT and other existing pre-trained models, achieving substantial gains across a variety of downstream NLP tasks and applications where time plays a pivotal role.","sentences":["In the evolving field of Natural Language Processing, understanding the temporal context of text is increasingly crucial.","This study investigates methods to incorporate temporal information during pre-training, aiming to achieve effective time-aware language representation for improved performance on time-related tasks.","In contrast to common pre-trained models like BERT, which rely on synchronic document collections such as BookCorpus and Wikipedia, our research introduces BiTimeBERT 2.0, a novel language model pre-trained on a temporal news article collection.","BiTimeBERT 2.0 utilizes this temporal news collection, focusing on three innovative pre-training objectives: Time-Aware Masked Language Modeling (TAMLM), Document Dating (DD), and Time-Sensitive Entity Replacement (TSER).","Each objective targets a unique aspect of temporal information.","TAMLM is designed to enhance the understanding of temporal contexts and relations, DD integrates document timestamps as chronological markers, and TSER focuses on the temporal dynamics of \"Person\" entities, recognizing their inherent temporal significance.","The experimental results consistently demonstrate that BiTimeBERT 2.0 outperforms models like BERT and other existing pre-trained models, achieving substantial gains across a variety of downstream NLP tasks and applications where time plays a pivotal role."],"url":"http://arxiv.org/abs/2406.01863v1","category":"cs.CL"}
{"created":"2024-06-04 00:26:12","title":"Charting the Landscape of Nefarious Uses of Generative Artificial Intelligence for Online Election Interference","abstract":"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) pose significant risks, particularly in the realm of online election interference. This paper explores the nefarious applications of GenAI, highlighting their potential to disrupt democratic processes through deepfakes, botnets, targeted misinformation campaigns, and synthetic identities.","sentences":["Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) pose significant risks, particularly in the realm of online election interference.","This paper explores the nefarious applications of GenAI, highlighting their potential to disrupt democratic processes through deepfakes, botnets, targeted misinformation campaigns, and synthetic identities."],"url":"http://arxiv.org/abs/2406.01862v1","category":"cs.CY"}
{"created":"2024-06-04 00:01:35","title":"TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability","abstract":"Large Language Model (LLM) evaluation is currently one of the most important areas of research, with existing benchmarks proving to be insufficient and not completely representative of LLMs' various capabilities. We present a curated collection of challenging statements on sensitive topics for LLM benchmarking called TruthEval. These statements were curated by hand and contain known truth values. The categories were chosen to distinguish LLMs' abilities from their stochastic nature. We perform some initial analyses using this dataset and find several instances of LLMs failing in simple tasks showing their inability to understand simple questions.","sentences":["Large Language Model (LLM) evaluation is currently one of the most important areas of research, with existing benchmarks proving to be insufficient and not completely representative of LLMs' various capabilities.","We present a curated collection of challenging statements on sensitive topics for LLM benchmarking called TruthEval.","These statements were curated by hand and contain known truth values.","The categories were chosen to distinguish LLMs' abilities from their stochastic nature.","We perform some initial analyses using this dataset and find several instances of LLMs failing in simple tasks showing their inability to understand simple questions."],"url":"http://arxiv.org/abs/2406.01855v1","category":"cs.CL"}
{"created":"2024-06-03 23:56:18","title":"Exploring Memristive Biosensing Dynamics: A COMSOL Multiphysics Approach","abstract":"This paper presents a novel methodology for modeling memristive biosensing within COMSOL Multiphysics, focusing on critical performance metrics such as antigen-antibody binding concentration and output resistive states. By studying the impact of increasing inlet concentrations, insights into binding concentration curve and output resistance variations are uncovered. The resultant simulation data effectively trains a support vector machine classifier (SVMC), achieving a remarkable accuracy rate of 97%. The incorporation of artificial intelligence (AI) through SVM demonstrates promising strides in advancing AI-based memristive biosensing modeling, potentially elevating their performance standards and applicability across diverse domains.","sentences":["This paper presents a novel methodology for modeling memristive biosensing within COMSOL Multiphysics, focusing on critical performance metrics such as antigen-antibody binding concentration and output resistive states.","By studying the impact of increasing inlet concentrations, insights into binding concentration curve and output resistance variations are uncovered.","The resultant simulation data effectively trains a support vector machine classifier (SVMC), achieving a remarkable accuracy rate of 97%.","The incorporation of artificial intelligence (AI) through SVM demonstrates promising strides in advancing AI-based memristive biosensing modeling, potentially elevating their performance standards and applicability across diverse domains."],"url":"http://arxiv.org/abs/2406.01854v1","category":"physics.app-ph"}
{"created":"2024-06-03 23:55:20","title":"Multi-Agent Reinforcement Learning Meets Leaf Sequencing in Radiotherapy","abstract":"In contemporary radiotherapy planning (RTP), a key module leaf sequencing is predominantly addressed by optimization-based approaches. In this paper, we propose a novel deep reinforcement learning (DRL) model termed as Reinforced Leaf Sequencer (RLS) in a multi-agent framework for leaf sequencing. The RLS model offers improvements to time-consuming iterative optimization steps via large-scale training and can control movement patterns through the design of reward mechanisms. We have conducted experiments on four datasets with four metrics and compared our model with a leading optimization sequencer. Our findings reveal that the proposed RLS model can achieve reduced fluence reconstruction errors, and potential faster convergence when integrated in an optimization planner. Additionally, RLS has shown promising results in a full artificial intelligence RTP pipeline. We hope this pioneer multi-agent RL leaf sequencer can foster future research on machine learning for RTP.","sentences":["In contemporary radiotherapy planning (RTP), a key module leaf sequencing is predominantly addressed by optimization-based approaches.","In this paper, we propose a novel deep reinforcement learning (DRL) model termed as Reinforced Leaf Sequencer (RLS) in a multi-agent framework for leaf sequencing.","The RLS model offers improvements to time-consuming iterative optimization steps via large-scale training and can control movement patterns through the design of reward mechanisms.","We have conducted experiments on four datasets with four metrics and compared our model with a leading optimization sequencer.","Our findings reveal that the proposed RLS model can achieve reduced fluence reconstruction errors, and potential faster convergence when integrated in an optimization planner.","Additionally, RLS has shown promising results in a full artificial intelligence RTP pipeline.","We hope this pioneer multi-agent RL leaf sequencer can foster future research on machine learning for RTP."],"url":"http://arxiv.org/abs/2406.01853v1","category":"cs.LG"}
{"created":"2024-06-03 23:28:05","title":"GraphWeaver: Billion-Scale Cybersecurity Incident Correlation","abstract":"In the dynamic landscape of large enterprise cybersecurity, accurately and efficiently correlating billions of security alerts into comprehensive incidents is a substantial challenge. Traditional correlation techniques often struggle with maintenance, scaling, and adapting to emerging threats and novel sources of telemetry. We introduce GraphWeaver, an industry-scale framework that shifts the traditional incident correlation process to a data-optimized, geo-distributed graph based approach. GraphWeaver introduces a suite of innovations tailored to handle the complexities of correlating billions of shared evidence alerts across hundreds of thousands of enterprises. Key among these innovations are a geo-distributed database and PySpark analytics engine for large-scale data processing, a minimum spanning tree algorithm to optimize correlation storage, integration of security domain knowledge and threat intelligence, and a human-in-the-loop feedback system to continuously refine key correlation processes and parameters. GraphWeaver is integrated into the Microsoft Defender XDR product and deployed worldwide, handling billions of correlations with a 99% accuracy rate, as confirmed by customer feedback and extensive investigations by security experts. This integration has not only maintained high correlation accuracy but reduces traditional correlation storage requirements by 7.4x. We provide an in-depth overview of the key design and operational features of GraphWeaver, setting a precedent as the first cybersecurity company to openly discuss these critical capabilities at this level of depth.","sentences":["In the dynamic landscape of large enterprise cybersecurity, accurately and efficiently correlating billions of security alerts into comprehensive incidents is a substantial challenge.","Traditional correlation techniques often struggle with maintenance, scaling, and adapting to emerging threats and novel sources of telemetry.","We introduce GraphWeaver, an industry-scale framework that shifts the traditional incident correlation process to a data-optimized, geo-distributed graph based approach.","GraphWeaver introduces a suite of innovations tailored to handle the complexities of correlating billions of shared evidence alerts across hundreds of thousands of enterprises.","Key among these innovations are a geo-distributed database and PySpark analytics engine for large-scale data processing, a minimum spanning tree algorithm to optimize correlation storage, integration of security domain knowledge and threat intelligence, and a human-in-the-loop feedback system to continuously refine key correlation processes and parameters.","GraphWeaver is integrated into the Microsoft Defender XDR product and deployed worldwide, handling billions of correlations with a 99% accuracy rate, as confirmed by customer feedback and extensive investigations by security experts.","This integration has not only maintained high correlation accuracy but reduces traditional correlation storage requirements by 7.4x.","We provide an in-depth overview of the key design and operational features of GraphWeaver, setting a precedent as the first cybersecurity company to openly discuss these critical capabilities at this level of depth."],"url":"http://arxiv.org/abs/2406.01842v1","category":"cs.CR"}
{"created":"2024-06-03 23:10:35","title":"Learning the Target Network in Function Space","abstract":"We focus on the task of learning the value function in the reinforcement learning (RL) setting. This task is often solved by updating a pair of online and target networks while ensuring that the parameters of these two networks are equivalent. We propose Lookahead-Replicate (LR), a new value-function approximation algorithm that is agnostic to this parameter-space equivalence. Instead, the LR algorithm is designed to maintain an equivalence between the two networks in the function space. This value-based equivalence is obtained by employing a new target-network update. We show that LR leads to a convergent behavior in learning the value function. We also present empirical results demonstrating that LR-based target-network updates significantly improve deep RL on the Atari benchmark.","sentences":["We focus on the task of learning the value function in the reinforcement learning (RL) setting.","This task is often solved by updating a pair of online and target networks while ensuring that the parameters of these two networks are equivalent.","We propose Lookahead-Replicate (LR), a new value-function approximation algorithm that is agnostic to this parameter-space equivalence.","Instead, the LR algorithm is designed to maintain an equivalence between the two networks in the function space.","This value-based equivalence is obtained by employing a new target-network update.","We show that LR leads to a convergent behavior in learning the value function.","We also present empirical results demonstrating that LR-based target-network updates significantly improve deep RL on the Atari benchmark."],"url":"http://arxiv.org/abs/2406.01838v1","category":"cs.LG"}
{"created":"2024-06-03 23:07:18","title":"An Open Multilingual System for Scoring Readability of Wikipedia","abstract":"With over 60M articles, Wikipedia has become the largest platform for open and freely accessible knowledge. While it has more than 15B monthly visits, its content is believed to be inaccessible to many readers due to the lack of readability of its text. However, previous investigations of the readability of Wikipedia have been restricted to English only, and there are currently no systems supporting the automatic readability assessment of the 300+ languages in Wikipedia. To bridge this gap, we develop a multilingual model to score the readability of Wikipedia articles. To train and evaluate this model, we create a novel multilingual dataset spanning 14 languages, by matching articles from Wikipedia to simplified Wikipedia and online children encyclopedias. We show that our model performs well in a zero-shot scenario, yielding a ranking accuracy of more than 80% across 14 languages and improving upon previous benchmarks. These results demonstrate the applicability of the model at scale for languages in which there is no ground-truth data available for model fine-tuning. Furthermore, we provide the first overview on the state of readability in Wikipedia beyond English.","sentences":["With over 60M articles, Wikipedia has become the largest platform for open and freely accessible knowledge.","While it has more than 15B monthly visits, its content is believed to be inaccessible to many readers due to the lack of readability of its text.","However, previous investigations of the readability of Wikipedia have been restricted to English only, and there are currently no systems supporting the automatic readability assessment of the 300+ languages in Wikipedia.","To bridge this gap, we develop a multilingual model to score the readability of Wikipedia articles.","To train and evaluate this model, we create a novel multilingual dataset spanning 14 languages, by matching articles from Wikipedia to simplified Wikipedia and online children encyclopedias.","We show that our model performs well in a zero-shot scenario, yielding a ranking accuracy of more than 80% across 14 languages and improving upon previous benchmarks.","These results demonstrate the applicability of the model at scale for languages in which there is no ground-truth data available for model fine-tuning.","Furthermore, we provide the first overview on the state of readability in Wikipedia beyond English."],"url":"http://arxiv.org/abs/2406.01835v1","category":"cs.CL"}
{"created":"2024-06-03 23:06:45","title":"CAFO: Feature-Centric Explanation on Time Series Classification","abstract":"In multivariate time series (MTS) classification, finding the important features (e.g., sensors) for model performance is crucial yet challenging due to the complex, high-dimensional nature of MTS data, intricate temporal dynamics, and the necessity for domain-specific interpretations. Current explanation methods for MTS mostly focus on time-centric explanations, apt for pinpointing important time periods but less effective in identifying key features. This limitation underscores the pressing need for a feature-centric approach, a vital yet often overlooked perspective that complements time-centric analysis. To bridge this gap, our study introduces a novel feature-centric explanation and evaluation framework for MTS, named CAFO (Channel Attention and Feature Orthgonalization). CAFO employs a convolution-based approach with channel attention mechanisms, incorporating a depth-wise separable channel attention module (DepCA) and a QR decomposition-based loss for promoting feature-wise orthogonality. We demonstrate that this orthogonalization enhances the separability of attention distributions, thereby refining and stabilizing the ranking of feature importance. This improvement in feature-wise ranking enhances our understanding of feature explainability in MTS. Furthermore, we develop metrics to evaluate global and class-specific feature importance. Our framework's efficacy is validated through extensive empirical analyses on two major public benchmarks and real-world datasets, both synthetic and self-collected, specifically designed to highlight class-wise discriminative features. The results confirm CAFO's robustness and informative capacity in assessing feature importance in MTS classification tasks. This study not only advances the understanding of feature-centric explanations in MTS but also sets a foundation for future explorations in feature-centric explanations.","sentences":["In multivariate time series (MTS) classification, finding the important features (e.g., sensors) for model performance is crucial yet challenging due to the complex, high-dimensional nature of MTS data, intricate temporal dynamics, and the necessity for domain-specific interpretations.","Current explanation methods for MTS mostly focus on time-centric explanations, apt for pinpointing important time periods but less effective in identifying key features.","This limitation underscores the pressing need for a feature-centric approach, a vital yet often overlooked perspective that complements time-centric analysis.","To bridge this gap, our study introduces a novel feature-centric explanation and evaluation framework for MTS, named CAFO (Channel Attention and Feature Orthgonalization).","CAFO employs a convolution-based approach with channel attention mechanisms, incorporating a depth-wise separable channel attention module (DepCA) and a QR decomposition-based loss for promoting feature-wise orthogonality.","We demonstrate that this orthogonalization enhances the separability of attention distributions, thereby refining and stabilizing the ranking of feature importance.","This improvement in feature-wise ranking enhances our understanding of feature explainability in MTS.","Furthermore, we develop metrics to evaluate global and class-specific feature importance.","Our framework's efficacy is validated through extensive empirical analyses on two major public benchmarks and real-world datasets, both synthetic and self-collected, specifically designed to highlight class-wise discriminative features.","The results confirm CAFO's robustness and informative capacity in assessing feature importance in MTS classification tasks.","This study not only advances the understanding of feature-centric explanations in MTS but also sets a foundation for future explorations in feature-centric explanations."],"url":"http://arxiv.org/abs/2406.01833v1","category":"cs.LG"}
{"created":"2024-06-03 22:59:53","title":"A Robust Filter for Marker-less Multi-person Tracking in Human-Robot Interaction Scenarios","abstract":"Pursuing natural and marker-less human-robot interaction (HRI) has been a long-standing robotics research focus, driven by the vision of seamless collaboration without physical markers. Marker-less approaches promise an improved user experience, but state-of-the-art struggles with the challenges posed by intrinsic errors in human pose estimation (HPE) and depth cameras. These errors can lead to issues such as robot jittering, which can significantly impact the trust users have in collaborative systems. We propose a filtering pipeline that refines incomplete 3D human poses from an HPE backbone and a single RGB-D camera to address these challenges, solving for occlusions that can degrade the interaction. Experimental results show that using the proposed filter leads to more consistent and noise-free motion representation, reducing unexpected robot movements and enabling smoother interaction.","sentences":["Pursuing natural and marker-less human-robot interaction (HRI) has been a long-standing robotics research focus, driven by the vision of seamless collaboration without physical markers.","Marker-less approaches promise an improved user experience, but state-of-the-art struggles with the challenges posed by intrinsic errors in human pose estimation (HPE) and depth cameras.","These errors can lead to issues such as robot jittering, which can significantly impact the trust users have in collaborative systems.","We propose a filtering pipeline that refines incomplete 3D human poses from an HPE backbone and a single RGB-D camera to address these challenges, solving for occlusions that can degrade the interaction.","Experimental results show that using the proposed filter leads to more consistent and noise-free motion representation, reducing unexpected robot movements and enabling smoother interaction."],"url":"http://arxiv.org/abs/2406.01832v1","category":"cs.RO"}
{"created":"2024-06-03 22:56:40","title":"FacAID: A Transformer Model for Neuro-Symbolic Facade Reconstruction","abstract":"We introduce a neuro-symbolic transformer-based model that converts flat, segmented facade structures into procedural definitions using a custom-designed split grammar. To facilitate this, we first develop a semi-complex split grammar tailored for architectural facades and then generate a dataset comprising of facades alongside their corresponding procedural representations. This dataset is used to train our transformer model to convert segmented, flat facades into the procedural language of our grammar. During inference, the model applies this learned transformation to new facade segmentations, providing a procedural representation that users can adjust to generate varied facade designs. This method not only automates the conversion of static facade images into dynamic, editable procedural formats but also enhances the design flexibility, allowing for easy modifications and variations by architects and designers. Our approach sets a new standard in facade design by combining the precision of procedural generation with the adaptability of neuro-symbolic learning.","sentences":["We introduce a neuro-symbolic transformer-based model that converts flat, segmented facade structures into procedural definitions using a custom-designed split grammar.","To facilitate this, we first develop a semi-complex split grammar tailored for architectural facades and then generate a dataset comprising of facades alongside their corresponding procedural representations.","This dataset is used to train our transformer model to convert segmented, flat facades into the procedural language of our grammar.","During inference, the model applies this learned transformation to new facade segmentations, providing a procedural representation that users can adjust to generate varied facade designs.","This method not only automates the conversion of static facade images into dynamic, editable procedural formats but also enhances the design flexibility, allowing for easy modifications and variations by architects and designers.","Our approach sets a new standard in facade design by combining the precision of procedural generation with the adaptability of neuro-symbolic learning."],"url":"http://arxiv.org/abs/2406.01829v1","category":"cs.NE"}
{"created":"2024-06-03 22:37:45","title":"EMOE: Expansive Matching of Experts for Robust Uncertainty Based Rejection","abstract":"Expansive Matching of Experts (EMOE) is a novel method that utilizes support-expanding, extrapolatory pseudo-labeling to improve prediction and uncertainty based rejection on out-of-distribution (OOD) points. We propose an expansive data augmentation technique that generates OOD instances in a latent space, and an empirical trial based approach to filter out augmented expansive points for pseudo-labeling. EMOE utilizes a diverse set of multiple base experts as pseudo-labelers on the augmented data to improve OOD performance through a shared MLP with multiple heads (one per expert). We demonstrate that EMOE achieves superior performance compared to state-of-the-art methods on tabular data.","sentences":["Expansive Matching of Experts (EMOE) is a novel method that utilizes support-expanding, extrapolatory pseudo-labeling to improve prediction and uncertainty based rejection on out-of-distribution (OOD) points.","We propose an expansive data augmentation technique that generates OOD instances in a latent space, and an empirical trial based approach to filter out augmented expansive points for pseudo-labeling.","EMOE utilizes a diverse set of multiple base experts as pseudo-labelers on the augmented data to improve OOD performance through a shared MLP with multiple heads (one per expert).","We demonstrate that EMOE achieves superior performance compared to state-of-the-art methods on tabular data."],"url":"http://arxiv.org/abs/2406.01825v1","category":"cs.LG"}
{"created":"2024-06-03 22:27:09","title":"Causal Discovery with Fewer Conditional Independence Tests","abstract":"Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an exponential number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a polynomial number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions.","sentences":["Many questions in science center around the fundamental problem of understanding causal relationships.","However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an exponential number of conditional independence (CI) tests, posing limitations in various applications.","Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests.","We show that it is possible to a learn a coarser representation of the hidden causal graph with a polynomial number of tests.","This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components.","CCPG satisfies consistency of orientations and additional constraints which favor finer partitions.","Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable.","As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions."],"url":"http://arxiv.org/abs/2406.01823v1","category":"cs.LG"}
{"created":"2024-06-03 22:24:12","title":"GPU-Accelerated Rule Evaluation and Evolution","abstract":"This paper introduces an innovative approach to boost the efficiency and scalability of Evolutionary Rule-based machine Learning (ERL), a key technique in explainable AI. While traditional ERL systems can distribute processes across multiple CPUs, fitness evaluation of candidate rules is a bottleneck, especially with large datasets. The method proposed in this paper, AERL (Accelerated ERL) solves this problem in two ways. First, by adopting GPU-optimized rule sets through a tensorized representation within the PyTorch framework, AERL mitigates the bottleneck and accelerates fitness evaluation significantly. Second, AERL takes further advantage of the GPUs by fine-tuning the rule coefficients via back-propagation, thereby improving search space exploration. Experimental evidence confirms that AERL search is faster and more effective, thus empowering explainable artificial intelligence.","sentences":["This paper introduces an innovative approach to boost the efficiency and scalability of Evolutionary Rule-based machine Learning (ERL), a key technique in explainable AI.","While traditional ERL systems can distribute processes across multiple CPUs, fitness evaluation of candidate rules is a bottleneck, especially with large datasets.","The method proposed in this paper, AERL (Accelerated ERL) solves this problem in two ways.","First, by adopting GPU-optimized rule sets through a tensorized representation within the PyTorch framework, AERL mitigates the bottleneck and accelerates fitness evaluation significantly.","Second, AERL takes further advantage of the GPUs by fine-tuning the rule coefficients via back-propagation, thereby improving search space exploration.","Experimental evidence confirms that AERL search is faster and more effective, thus empowering explainable artificial intelligence."],"url":"http://arxiv.org/abs/2406.01821v1","category":"cs.NE"}
{"created":"2024-06-03 22:19:42","title":"Finding Lottery Tickets in Vision Models via Data-driven Spectral Foresight Pruning","abstract":"Recent advances in neural network pruning have shown how it is possible to reduce the computational costs and memory demands of deep learning models before training. We focus on this framework and propose a new pruning at initialization algorithm that leverages the Neural Tangent Kernel (NTK) theory to align the training dynamics of the sparse network with that of the dense one. Specifically, we show how the usually neglected data-dependent component in the NTK's spectrum can be taken into account by providing an analytical upper bound to the NTK's trace obtained by decomposing neural networks into individual paths. This leads to our Path eXclusion (PX), a foresight pruning method designed to preserve the parameters that mostly influence the NTK's trace. PX is able to find lottery tickets (i.e. good paths) even at high sparsity levels and largely reduces the need for additional training. When applied to pre-trained models it extracts subnetworks directly usable for several downstream tasks, resulting in performance comparable to those of the dense counterpart but with substantial cost and computational savings. Code available at: https://github.com/iurada/px-ntk-pruning","sentences":["Recent advances in neural network pruning have shown how it is possible to reduce the computational costs and memory demands of deep learning models before training.","We focus on this framework and propose a new pruning at initialization algorithm that leverages the Neural Tangent Kernel (NTK) theory to align the training dynamics of the sparse network with that of the dense one.","Specifically, we show how the usually neglected data-dependent component in the NTK's spectrum can be taken into account by providing an analytical upper bound to the NTK's trace obtained by decomposing neural networks into individual paths.","This leads to our Path eXclusion (PX), a foresight pruning method designed to preserve the parameters that mostly influence the NTK's trace.","PX is able to find lottery tickets (i.e. good paths) even at high sparsity levels and largely reduces the need for additional training.","When applied to pre-trained models it extracts subnetworks directly usable for several downstream tasks, resulting in performance comparable to those of the dense counterpart but with substantial cost and computational savings.","Code available at: https://github.com/iurada/px-ntk-pruning"],"url":"http://arxiv.org/abs/2406.01820v1","category":"cs.CV"}
{"created":"2024-06-03 22:11:38","title":"Diffusion Boosted Trees","abstract":"Combining the merits of both denoising diffusion probabilistic models and gradient boosting, the diffusion boosting paradigm is introduced for tackling supervised learning problems. We develop Diffusion Boosted Trees (DBT), which can be viewed as both a new denoising diffusion generative model parameterized by decision trees (one single tree for each diffusion timestep), and a new boosting algorithm that combines the weak learners into a strong learner of conditional distributions without making explicit parametric assumptions on their density forms. We demonstrate through experiments the advantages of DBT over deep neural network-based diffusion models as well as the competence of DBT on real-world regression tasks, and present a business application (fraud detection) of DBT for classification on tabular data with the ability of learning to defer.","sentences":["Combining the merits of both denoising diffusion probabilistic models and gradient boosting, the diffusion boosting paradigm is introduced for tackling supervised learning problems.","We develop Diffusion Boosted Trees (DBT), which can be viewed as both a new denoising diffusion generative model parameterized by decision trees (one single tree for each diffusion timestep), and a new boosting algorithm that combines the weak learners into a strong learner of conditional distributions without making explicit parametric assumptions on their density forms.","We demonstrate through experiments the advantages of DBT over deep neural network-based diffusion models as well as the competence of DBT on real-world regression tasks, and present a business application (fraud detection) of DBT for classification on tabular data with the ability of learning to defer."],"url":"http://arxiv.org/abs/2406.01813v1","category":"stat.ML"}
{"created":"2024-06-03 22:10:25","title":"Memory Capacity Analysis of Time-delay Reservoir Computing Based on Silicon Microring Resonator Nonlinearities","abstract":"Silicon microring resonators (MRRs) have shown strong potential in acting as the nonlinear nodes of photonic reservoir computing (RC) schemes. By using nonlinearities within a silicon MRR, such as the ones caused by free-carrier dispersion (FCD) and thermo-optic (TO) effects, it is possible to map the input data of the RC to a higher dimensional space. Furthermore, by adding an external waveguide between the through and add ports of the MRR, it is possible to implement a time-delay RC (TDRC) with enhanced memory. The input from the through port is fed back into the add port of the ring with the delay applied by the external waveguide effectively adding memory. In a TDRC, the nodes are multiplexed in time, and their respective time evolutions are detected at the drop port. The performance of MRR-based TDRC is highly dependent on the amount of nonlinearity in the MRR. The nonlinear effects, in turn, are dependent on the physical properties of the MRR as they determine the lifetime of the effects. Another factor to take into account is the stability of the MRR response, as strong time-domain discontinuities at the drop port are known to emerge from FCD nonlinearities due to self-pulsing (high nonlinear behaviour). However, quantifying the right amount of nonlinearity that RC needs for a certain task in order to achieve optimum performance is challenging. Therefore, further analysis is required to fully understand the nonlinear dynamics of this TDRC setup. Here, we quantify the nonlinear and linear memory capacity of the previously described microring-based TDRC scheme, as a function of the time constants of the generated carriers and the thermal of the TO effects. We analyze the properties of the TDRC dynamics that generate the parameter space, in terms of input signal power and frequency detuning range, over which conventional RC tasks can be satisfactorily performed by the TDRC scheme.","sentences":["Silicon microring resonators (MRRs) have shown strong potential in acting as the nonlinear nodes of photonic reservoir computing (RC) schemes.","By using nonlinearities within a silicon MRR, such as the ones caused by free-carrier dispersion (FCD) and thermo-optic (TO) effects, it is possible to map the input data of the RC to a higher dimensional space.","Furthermore, by adding an external waveguide between the through and add ports of the MRR, it is possible to implement a time-delay RC (TDRC) with enhanced memory.","The input from the through port is fed back into the add port of the ring with the delay applied by the external waveguide effectively adding memory.","In a TDRC, the nodes are multiplexed in time, and their respective time evolutions are detected at the drop port.","The performance of MRR-based TDRC is highly dependent on the amount of nonlinearity in the MRR.","The nonlinear effects, in turn, are dependent on the physical properties of the MRR as they determine the lifetime of the effects.","Another factor to take into account is the stability of the MRR response, as strong time-domain discontinuities at the drop port are known to emerge from FCD nonlinearities due to self-pulsing (high nonlinear behaviour).","However, quantifying the right amount of nonlinearity that RC needs for a certain task in order to achieve optimum performance is challenging.","Therefore, further analysis is required to fully understand the nonlinear dynamics of this TDRC setup.","Here, we quantify the nonlinear and linear memory capacity of the previously described microring-based TDRC scheme, as a function of the time constants of the generated carriers and the thermal of the TO effects.","We analyze the properties of the TDRC dynamics that generate the parameter space, in terms of input signal power and frequency detuning range, over which conventional RC tasks can be satisfactorily performed by the TDRC scheme."],"url":"http://arxiv.org/abs/2406.01812v1","category":"cs.NE"}
{"created":"2024-06-03 21:55:07","title":"Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation","abstract":"The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks. For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence. Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components. For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction. Additionally, different tokens should be weighted differently depending on the context. In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM. By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure. We refer to this new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts. Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC.","sentences":["The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks.","For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence.","Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components.","For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction.","Additionally, different tokens should be weighted differently depending on the context.","In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM.","By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure.","We refer to this new score as the Contextualized Sequence Likelihood (CSL).","CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts.","Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC."],"url":"http://arxiv.org/abs/2406.01806v1","category":"cs.CL"}
{"created":"2024-06-03 21:51:13","title":"TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting","abstract":"Tabular data is prevalent in many critical domains, yet it is often challenging to acquire in large quantities. This scarcity usually results in poor performance of machine learning models on such data. Data augmentation, a common strategy for performance improvement in vision and language tasks, typically underperforms for tabular data due to the lack of explicit symmetries in the input space. To overcome this challenge, we introduce TabMDA, a novel method for manifold data augmentation on tabular data. This method utilises a pre-trained in-context model, such as TabPFN, to map the data into a manifold space. TabMDA performs label-invariant transformations by encoding the data multiple times with varied contexts. This process explores the manifold of the underlying in-context models, thereby enlarging the training dataset. TabMDA is a training-free method, making it applicable to any classifier. We evaluate TabMDA on five standard classifiers and observe significant performance improvements across various tabular datasets. Our results demonstrate that TabMDA provides an effective way to leverage information from pre-trained in-context models to enhance the performance of downstream classifiers.","sentences":["Tabular data is prevalent in many critical domains, yet it is often challenging to acquire in large quantities.","This scarcity usually results in poor performance of machine learning models on such data.","Data augmentation, a common strategy for performance improvement in vision and language tasks, typically underperforms for tabular data due to the lack of explicit symmetries in the input space.","To overcome this challenge, we introduce TabMDA, a novel method for manifold data augmentation on tabular data.","This method utilises a pre-trained in-context model, such as TabPFN, to map the data into a manifold space.","TabMDA performs label-invariant transformations by encoding the data multiple times with varied contexts.","This process explores the manifold of the underlying in-context models, thereby enlarging the training dataset.","TabMDA is a training-free method, making it applicable to any classifier.","We evaluate TabMDA on five standard classifiers and observe significant performance improvements across various tabular datasets.","Our results demonstrate that TabMDA provides an effective way to leverage information from pre-trained in-context models to enhance the performance of downstream classifiers."],"url":"http://arxiv.org/abs/2406.01805v1","category":"cs.LG"}
{"created":"2024-06-03 21:18:08","title":"Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning","abstract":"Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.","sentences":["Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task","[Ng et al., 2000].","However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward.","Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition","[Rolland et al., 2022].","In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert.","Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws.","Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert.","Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts."],"url":"http://arxiv.org/abs/2406.01793v1","category":"cs.LG"}
{"created":"2024-06-03 21:14:53","title":"Hybrid-Learning Video Moment Retrieval across Multi-Domain Labels","abstract":"Video moment retrieval (VMR) is to search for a visual temporal moment in an untrimmed raw video by a given text query description (sentence). Existing studies either start from collecting exhaustive frame-wise annotations on the temporal boundary of target moments (fully-supervised), or learn with only the video-level video-text pairing labels (weakly-supervised). The former is poor in generalisation to unknown concepts and/or novel scenes due to restricted dataset scale and diversity under expensive annotation costs; the latter is subject to visual-textual mis-correlations from incomplete labels. In this work, we introduce a new approach called hybrid-learning video moment retrieval to solve the problem by knowledge transfer through adapting the video-text matching relationships learned from a fully-supervised source domain to a weakly-labelled target domain when they do not share a common label space. Our aim is to explore shared universal knowledge between the two domains in order to improve model learning in the weakly-labelled target domain. Specifically, we introduce a multiplE branch Video-text Alignment model (EVA) that performs cross-modal (visual-textual) matching information sharing and multi-modal feature alignment to optimise domain-invariant visual and textual features as well as per-task discriminative joint video-text representations. Experiments show EVA's effectiveness in exploring temporal segment annotations in a source domain to help learn video moment retrieval without temporal labels in a target domain.","sentences":["Video moment retrieval (VMR) is to search for a visual temporal moment in an untrimmed raw video by a given text query description (sentence).","Existing studies either start from collecting exhaustive frame-wise annotations on the temporal boundary of target moments (fully-supervised), or learn with only the video-level video-text pairing labels (weakly-supervised).","The former is poor in generalisation to unknown concepts and/or novel scenes due to restricted dataset scale and diversity under expensive annotation costs; the latter is subject to visual-textual mis-correlations from incomplete labels.","In this work, we introduce a new approach called hybrid-learning video moment retrieval to solve the problem by knowledge transfer through adapting the video-text matching relationships learned from a fully-supervised source domain to a weakly-labelled target domain when they do not share a common label space.","Our aim is to explore shared universal knowledge between the two domains in order to improve model learning in the weakly-labelled target domain.","Specifically, we introduce a multiplE branch Video-text Alignment model (EVA) that performs cross-modal (visual-textual) matching information sharing and multi-modal feature alignment to optimise domain-invariant visual and textual features as well as per-task discriminative joint video-text representations.","Experiments show EVA's effectiveness in exploring temporal segment annotations in a source domain to help learn video moment retrieval without temporal labels in a target domain."],"url":"http://arxiv.org/abs/2406.01791v1","category":"cs.CV"}
{"created":"2024-06-03 21:13:02","title":"AI-based Classification of Customer Support Tickets: State of the Art and Implementation with AutoML","abstract":"Automation of support ticket classification is crucial to improve customer support performance and shortening resolution time for customer inquiries. This research aims to test the applicability of automated machine learning (AutoML) as a technology to train a machine learning model (ML model) that can classify support tickets. The model evaluation conducted in this research shows that AutoML can be used to train ML models with good classification performance. Moreover, this paper fills a research gap by providing new insights into developing AI solutions without a dedicated professional by utilizing AutoML, which makes this technology more accessible for companies without specialized AI departments and staff.","sentences":["Automation of support ticket classification is crucial to improve customer support performance and shortening resolution time for customer inquiries.","This research aims to test the applicability of automated machine learning (AutoML) as a technology to train a machine learning model (ML model) that can classify support tickets.","The model evaluation conducted in this research shows that AutoML can be used to train ML models with good classification performance.","Moreover, this paper fills a research gap by providing new insights into developing AI solutions without a dedicated professional by utilizing AutoML, which makes this technology more accessible for companies without specialized AI departments and staff."],"url":"http://arxiv.org/abs/2406.01789v1","category":"cs.LG"}
{"created":"2024-06-03 21:05:59","title":"Recent Advances in Data-Driven Business Process Management","abstract":"The rapid development of cutting-edge technologies, the increasing volume of data and also the availability and processability of new types of data sources has led to a paradigm shift in data-based management and decision-making. Since business processes are at the core of organizational work, these developments heavily impact BPM as a crucial success factor for organizations. In view of this emerging potential, data-driven business process management has become a relevant and vibrant research area. Given the complexity and interdisciplinarity of the research field, this position paper therefore presents research insights regarding data-driven BPM.","sentences":["The rapid development of cutting-edge technologies, the increasing volume of data and also the availability and processability of new types of data sources has led to a paradigm shift in data-based management and decision-making.","Since business processes are at the core of organizational work, these developments heavily impact BPM as a crucial success factor for organizations.","In view of this emerging potential, data-driven business process management has become a relevant and vibrant research area.","Given the complexity and interdisciplinarity of the research field, this position paper therefore presents research insights regarding data-driven BPM."],"url":"http://arxiv.org/abs/2406.01786v1","category":"cs.DB"}
{"created":"2024-06-03 20:56:12","title":"Multi-agent assignment via state augmented reinforcement learning","abstract":"We address the conflicting requirements of a multi-agent assignment problem through constrained reinforcement learning, emphasizing the inadequacy of standard regularization techniques for this purpose. Instead, we recur to a state augmentation approach in which the oscillation of dual variables is exploited by agents to alternate between tasks. In addition, we coordinate the actions of the multiple agents acting on their local states through these multipliers, which are gossiped through a communication network, eliminating the need to access other agent states. By these means, we propose a distributed multi-agent assignment protocol with theoretical feasibility guarantees that we corroborate in a monitoring numerical experiment.","sentences":["We address the conflicting requirements of a multi-agent assignment problem through constrained reinforcement learning, emphasizing the inadequacy of standard regularization techniques for this purpose.","Instead, we recur to a state augmentation approach in which the oscillation of dual variables is exploited by agents to alternate between tasks.","In addition, we coordinate the actions of the multiple agents acting on their local states through these multipliers, which are gossiped through a communication network, eliminating the need to access other agent states.","By these means, we propose a distributed multi-agent assignment protocol with theoretical feasibility guarantees that we corroborate in a monitoring numerical experiment."],"url":"http://arxiv.org/abs/2406.01782v1","category":"eess.SY"}
{"created":"2024-06-03 20:25:12","title":"LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback","abstract":"To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.","sentences":["To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones.","While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages.","Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English.","In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages.","To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages.","We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset.","We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks.","Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages."],"url":"http://arxiv.org/abs/2406.01771v1","category":"cs.CL"}
{"created":"2024-06-03 20:11:27","title":"An approximation-based approach versus an AI one for the study of CT images of abdominal aorta aneurysms","abstract":"This study evaluates two approaches applied to computed tomography (CT) images of patients with abdominal aortic aneurysm: one deterministic, based on tools of Approximation Theory, and one based on Artificial Intelligence. Both aim to segment the basal CT images to extract the patent area of the aortic vessel, in order to propose an alternative to nephrotoxic contrast agents for diagnosing this pathology. While the deterministic approach employs sampling Kantorovich operators and the theory behind, leveraging the reconstruction and enhancement capabilities of these operators applied to images, the artificial intelligence-based approach lays on a U-net neural network. The results obtained from testing the two methods have been compared numerically and visually to assess their performances, demonstrating that both models yield accurate results.","sentences":["This study evaluates two approaches applied to computed tomography (CT) images of patients with abdominal aortic aneurysm: one deterministic, based on tools of Approximation Theory, and one based on Artificial Intelligence.","Both aim to segment the basal CT images to extract the patent area of the aortic vessel, in order to propose an alternative to nephrotoxic contrast agents for diagnosing this pathology.","While the deterministic approach employs sampling Kantorovich operators and the theory behind, leveraging the reconstruction and enhancement capabilities of these operators applied to images, the artificial intelligence-based approach lays on a U-net neural network.","The results obtained from testing the two methods have been compared numerically and visually to assess their performances, demonstrating that both models yield accurate results."],"url":"http://arxiv.org/abs/2406.01764v1","category":"cs.CV"}
{"created":"2024-06-03 20:05:04","title":"Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation","abstract":"Actor-critic (AC) is a powerful method for learning an optimal policy in reinforcement learning, where the critic uses algorithms, e.g., temporal difference (TD) learning with function approximation, to evaluate the current policy and the actor updates the policy along an approximate gradient direction using information from the critic. This paper provides the \\textit{tightest} non-asymptotic convergence bounds for both the AC and natural AC (NAC) algorithms. Specifically, existing studies show that AC converges to an $\\epsilon+\\varepsilon_{\\text{critic}}$ neighborhood of stationary points with the best known sample complexity of $\\mathcal{O}(\\epsilon^{-2})$ (up to a log factor), and NAC converges to an $\\epsilon+\\varepsilon_{\\text{critic}}+\\sqrt{\\varepsilon_{\\text{actor}}}$ neighborhood of the global optimum with the best known sample complexity of $\\mathcal{O}(\\epsilon^{-3})$, where $\\varepsilon_{\\text{critic}}$ is the approximation error of the critic and $\\varepsilon_{\\text{actor}}$ is the approximation error induced by the insufficient expressive power of the parameterized policy class. This paper analyzes the convergence of both AC and NAC algorithms with compatible function approximation. Our analysis eliminates the term $\\varepsilon_{\\text{critic}}$ from the error bounds while still achieving the best known sample complexities. Moreover, we focus on the challenging single-loop setting with a single Markovian sample trajectory. Our major technical novelty lies in analyzing the stochastic bias due to policy-dependent and time-varying compatible function approximation in the critic, and handling the non-ergodicity of the MDP due to the single Markovian sample trajectory. Numerical results are also provided in the appendix.","sentences":["Actor-critic (AC) is a powerful method for learning an optimal policy in reinforcement learning, where the critic uses algorithms, e.g., temporal difference (TD) learning with function approximation, to evaluate the current policy and the actor updates the policy along an approximate gradient direction using information from the critic.","This paper provides the \\textit{tightest} non-asymptotic convergence bounds for both the AC and natural AC (NAC) algorithms.","Specifically, existing studies show that AC converges to an $\\epsilon+\\varepsilon_{\\text{critic}}$ neighborhood of stationary points with the best known sample complexity of $\\mathcal{O}(\\epsilon^{-2})$ (up to a log factor), and NAC converges to an $\\epsilon+\\varepsilon_{\\text{critic}}+\\sqrt{\\varepsilon_{\\text{actor}}}$ neighborhood of the global optimum with the best known sample complexity of $\\mathcal{O}(\\epsilon^{-3})$, where $\\varepsilon_{\\text{critic}}$ is the approximation error of the critic and $\\varepsilon_{\\text{actor}}$ is the approximation error induced by the insufficient expressive power of the parameterized policy class.","This paper analyzes the convergence of both AC and NAC algorithms with compatible function approximation.","Our analysis eliminates the term $\\varepsilon_{\\text{critic}}$ from the error bounds while still achieving the best known sample complexities.","Moreover, we focus on the challenging single-loop setting with a single Markovian sample trajectory.","Our major technical novelty lies in analyzing the stochastic bias due to policy-dependent and time-varying compatible function approximation in the critic, and handling the non-ergodicity of the MDP due to the single Markovian sample trajectory.","Numerical results are also provided in the appendix."],"url":"http://arxiv.org/abs/2406.01762v1","category":"cs.LG"}
{"created":"2024-06-03 19:54:11","title":"From Latent to Lucid: Transforming Knowledge Graph Embeddings into Interpretable Structures","abstract":"This paper introduces a post-hoc explainable AI method tailored for Knowledge Graph Embedding models. These models are essential to Knowledge Graph Completion yet criticized for their opaque, black-box nature. Despite their significant success in capturing the semantics of knowledge graphs through high-dimensional latent representations, their inherent complexity poses substantial challenges to explainability. Unlike existing methods, our approach directly decodes the latent representations encoded by Knowledge Graph Embedding models, leveraging the principle that similar embeddings reflect similar behaviors within the Knowledge Graph. By identifying distinct structures within the subgraph neighborhoods of similarly embedded entities, our method identifies the statistical regularities on which the models rely and translates these insights into human-understandable symbolic rules and facts. This bridges the gap between the abstract representations of Knowledge Graph Embedding models and their predictive outputs, offering clear, interpretable insights. Key contributions include a novel post-hoc explainable AI method for Knowledge Graph Embedding models that provides immediate, faithful explanations without retraining, facilitating real-time application even on large-scale knowledge graphs. The method's flexibility enables the generation of rule-based, instance-based, and analogy-based explanations, meeting diverse user needs. Extensive evaluations show our approach's effectiveness in delivering faithful and well-localized explanations, enhancing the transparency and trustworthiness of Knowledge Graph Embedding models.","sentences":["This paper introduces a post-hoc explainable AI method tailored for Knowledge Graph Embedding models.","These models are essential to Knowledge Graph Completion yet criticized for their opaque, black-box nature.","Despite their significant success in capturing the semantics of knowledge graphs through high-dimensional latent representations, their inherent complexity poses substantial challenges to explainability.","Unlike existing methods, our approach directly decodes the latent representations encoded by Knowledge Graph Embedding models, leveraging the principle that similar embeddings reflect similar behaviors within the Knowledge Graph.","By identifying distinct structures within the subgraph neighborhoods of similarly embedded entities, our method identifies the statistical regularities on which the models rely and translates these insights into human-understandable symbolic rules and facts.","This bridges the gap between the abstract representations of Knowledge Graph Embedding models and their predictive outputs, offering clear, interpretable insights.","Key contributions include a novel post-hoc explainable AI method for Knowledge Graph Embedding models that provides immediate, faithful explanations without retraining, facilitating real-time application even on large-scale knowledge graphs.","The method's flexibility enables the generation of rule-based, instance-based, and analogy-based explanations, meeting diverse user needs.","Extensive evaluations show our approach's effectiveness in delivering faithful and well-localized explanations, enhancing the transparency and trustworthiness of Knowledge Graph Embedding models."],"url":"http://arxiv.org/abs/2406.01759v1","category":"cs.AI"}
{"created":"2024-06-03 19:52:41","title":"Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities","abstract":"The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind. In this position paper, we discuss that disparities towards marginalized communities - performance, representation, privacy, robustness, interpretability and safety - are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon. We contrast foundation models with traditional models and highlight the potential for exacerbated disparity against marginalized communities. Moreover, we emphasize the unique threat of cascading impacts in foundation models, where interconnected disparities can trigger long-lasting negative consequences, specifically to the people on the margin. We define marginalized communities within the machine learning context and explore the multifaceted nature of disparities. We analyze the sources of these disparities, tracing them from data creation, training and deployment procedures to highlight the complex technical and socio-technical landscape. To mitigate the pressing crisis, we conclude with a set of calls to action to mitigate disparity at its source.","sentences":["The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind.","In this position paper, we discuss that disparities towards marginalized communities - performance, representation, privacy, robustness, interpretability and safety - are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon.","We contrast foundation models with traditional models and highlight the potential for exacerbated disparity against marginalized communities.","Moreover, we emphasize the unique threat of cascading impacts in foundation models, where interconnected disparities can trigger long-lasting negative consequences, specifically to the people on the margin.","We define marginalized communities within the machine learning context and explore the multifaceted nature of disparities.","We analyze the sources of these disparities, tracing them from data creation, training and deployment procedures to highlight the complex technical and socio-technical landscape.","To mitigate the pressing crisis, we conclude with a set of calls to action to mitigate disparity at its source."],"url":"http://arxiv.org/abs/2406.01757v1","category":"cs.LG"}
{"created":"2024-06-03 19:44:47","title":"Sparser, Better, Deeper, Stronger: Improving Sparse Training with Exact Orthogonal Initialization","abstract":"Static sparse training aims to train sparse models from scratch, achieving remarkable results in recent years. A key design choice is given by the sparse initialization, which determines the trainable sub-network through a binary mask. Existing methods mainly select such mask based on a predefined dense initialization. Such an approach may not efficiently leverage the mask's potential impact on the optimization. An alternative direction, inspired by research into dynamical isometry, is to introduce orthogonality in the sparse subnetwork, which helps in stabilizing the gradient signal. In this work, we propose Exact Orthogonal Initialization (EOI), a novel sparse orthogonal initialization scheme based on composing random Givens rotations. Contrary to other existing approaches, our method provides exact (not approximated) orthogonality and enables the creation of layers with arbitrary densities. We demonstrate the superior effectiveness and efficiency of EOI through experiments, consistently outperforming common sparse initialization techniques. Our method enables training highly sparse 1000-layer MLP and CNN networks without residual connections or normalization techniques, emphasizing the crucial role of weight initialization in static sparse training alongside sparse mask selection. The code is available at https://github.com/woocash2/sparser-better-deeper-stronger","sentences":["Static sparse training aims to train sparse models from scratch, achieving remarkable results in recent years.","A key design choice is given by the sparse initialization, which determines the trainable sub-network through a binary mask.","Existing methods mainly select such mask based on a predefined dense initialization.","Such an approach may not efficiently leverage the mask's potential impact on the optimization.","An alternative direction, inspired by research into dynamical isometry, is to introduce orthogonality in the sparse subnetwork, which helps in stabilizing the gradient signal.","In this work, we propose Exact Orthogonal Initialization (EOI), a novel sparse orthogonal initialization scheme based on composing random Givens rotations.","Contrary to other existing approaches, our method provides exact (not approximated) orthogonality and enables the creation of layers with arbitrary densities.","We demonstrate the superior effectiveness and efficiency of EOI through experiments, consistently outperforming common sparse initialization techniques.","Our method enables training highly sparse 1000-layer MLP and CNN networks without residual connections or normalization techniques, emphasizing the crucial role of weight initialization in static sparse training alongside sparse mask selection.","The code is available at https://github.com/woocash2/sparser-better-deeper-stronger"],"url":"http://arxiv.org/abs/2406.01755v1","category":"cs.LG"}
{"created":"2024-06-03 19:03:26","title":"Quantum speed limit of open quantum system models using the Wigner function","abstract":"The quantum speed limit time of open system models is explored using the Wasserstein-1-distance and the Wigner function. Use is made of the phase covariant and a two-qubit model interacting with a squeezed thermal bath via position-dependent coupling. The dependence of the coupling on the position of the qubits allowed the study of the dynamics in the collective regime, which is conducive to speeding up the evolution. The use of the Wigner function naturally allows the study of the quantumness of the systems studied. An interesting interplay is observed between non-Markovian behavior, quantumness, and the quantum speed limit time. The presence of quantum correlations is seen to speed up the evolution.","sentences":["The quantum speed limit time of open system models is explored using the Wasserstein-1-distance and the Wigner function.","Use is made of the phase covariant and a two-qubit model interacting with a squeezed thermal bath via position-dependent coupling.","The dependence of the coupling on the position of the qubits allowed the study of the dynamics in the collective regime, which is conducive to speeding up the evolution.","The use of the Wigner function naturally allows the study of the quantumness of the systems studied.","An interesting interplay is observed between non-Markovian behavior, quantumness, and the quantum speed limit time.","The presence of quantum correlations is seen to speed up the evolution."],"url":"http://arxiv.org/abs/2406.01741v1","category":"quant-ph"}
{"created":"2024-06-03 18:43:17","title":"The Parameterized Complexity of Terminal Monitoring Set","abstract":"In Terminal Monitoring Set (TMS), the input is an undirected graph $G=(V,E)$, together with a collection $T$ of terminal pairs and the goal is to find a subset $S$ of minimum size that hits a shortest path between every pair of terminals. We show that this problem is W[2]-hard with respect to solution size. On the positive side, we show that TMS is fixed parameter tractable with respect to solution size plus distance to cluster, solution size plus neighborhood diversity, and feedback edge number. For the weighted version of the problem, we obtain a FPT algorithm with respect to vertex cover number, and for a relaxed version of the problem, we show that it is W[1]-hard with respect to solution size plus feedback vertex number.","sentences":["In Terminal Monitoring Set (TMS), the input is an undirected graph $G=(V,E)$, together with a collection $T$ of terminal pairs and the goal is to find a subset $S$ of minimum size that hits a shortest path between every pair of terminals.","We show that this problem is W[2]-hard with respect to solution size.","On the positive side, we show that TMS is fixed parameter tractable with respect to solution size plus distance to cluster, solution size plus neighborhood diversity, and feedback edge number.","For the weighted version of the problem, we obtain a FPT algorithm with respect to vertex cover number, and for a relaxed version of the problem, we show that it is W[1]-hard with respect to solution size plus feedback vertex number."],"url":"http://arxiv.org/abs/2406.01730v1","category":"cs.DM"}
{"created":"2024-06-03 18:23:06","title":"CHEOPS in-flight performance: A comprehensive look at the first 3.5 years of operations","abstract":"CHEOPS is a space telescope specifically designed to monitor transiting exoplanets orbiting bright stars. In September 2023, CHEOPS completed its nominal mission and remains in excellent operational conditions. The mission has been extended until the end of 2026. Scientific and instrumental data have been collected throughout in-orbit commissioning and nominal operations, enabling a comprehensive analysis of the mission's performance. In this article, we present the results of this analysis with a twofold goal. First, we aim to inform the scientific community about the present status of the mission and what can be expected as the instrument ages. Secondly, we intend for this publication to serve as a legacy document for future missions, providing insights and lessons learned from the successful operation of CHEOPS. To evaluate the instrument performance in flight, we developed a comprehensive monitoring and characterisation programme. It consists of dedicated observations that allow us to characterise the instrument's response. In addition to the standard collection of nominal science and housekeeping data, these observations provide input for detecting, modelling, and correcting instrument systematics, discovering and addressing anomalies, and comparing the instrument's actual performance with expectations. The precision of the CHEOPS measurements has enabled the mission objectives to be met and exceeded. Careful modelling of the instrumental systematics allows the data quality to be significantly improved during the light curve analysis phase, resulting in more precise scientific measurements. CHEOPS is compliant with the driving scientific requirements of the mission. Although visible, the ageing of the instrument has not affected the mission's performance.","sentences":["CHEOPS is a space telescope specifically designed to monitor transiting exoplanets orbiting bright stars.","In September 2023, CHEOPS completed its nominal mission and remains in excellent operational conditions.","The mission has been extended until the end of 2026.","Scientific and instrumental data have been collected throughout in-orbit commissioning and nominal operations, enabling a comprehensive analysis of the mission's performance.","In this article, we present the results of this analysis with a twofold goal.","First, we aim to inform the scientific community about the present status of the mission and what can be expected as the instrument ages.","Secondly, we intend for this publication to serve as a legacy document for future missions, providing insights and lessons learned from the successful operation of CHEOPS.","To evaluate the instrument performance in flight, we developed a comprehensive monitoring and characterisation programme.","It consists of dedicated observations that allow us to characterise the instrument's response.","In addition to the standard collection of nominal science and housekeeping data, these observations provide input for detecting, modelling, and correcting instrument systematics, discovering and addressing anomalies, and comparing the instrument's actual performance with expectations.","The precision of the CHEOPS measurements has enabled the mission objectives to be met and exceeded.","Careful modelling of the instrumental systematics allows the data quality to be significantly improved during the light curve analysis phase, resulting in more precise scientific measurements.","CHEOPS is compliant with the driving scientific requirements of the mission.","Although visible, the ageing of the instrument has not affected the mission's performance."],"url":"http://arxiv.org/abs/2406.01716v1","category":"astro-ph.IM"}
{"created":"2024-06-03 18:00:50","title":"Demystifying Platform Requirements for Diverse LLM Inference Use Cases","abstract":"Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these parameter-heavy models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With LLM deployment scenarios and models evolving at breakneck speed, the hardware requirements to meet SLOs remains an open research question. In this work, we present an analytical tool, GenZ, to study the relationship between LLM inference performance and various platform design parameters. Our analysis provides insights into configuring platforms for different LLM workloads and use cases. We quantify the platform requirements to support SOTA LLMs models like LLaMA and GPT-4 under diverse serving settings. Furthermore, we project the hardware capabilities needed to enable future LLMs potentially exceeding hundreds of trillions of parameters. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms. Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications. The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer .","sentences":["Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts.","However, deploying these parameter-heavy models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources.","With LLM deployment scenarios and models evolving at breakneck speed, the hardware requirements to meet SLOs remains an open research question.","In this work, we present an analytical tool, GenZ, to study the relationship between LLM inference performance and various platform design parameters.","Our analysis provides insights into configuring platforms for different LLM workloads and use cases.","We quantify the platform requirements to support SOTA LLMs models like LLaMA and GPT-4 under diverse serving settings.","Furthermore, we project the hardware capabilities needed to enable future LLMs potentially exceeding hundreds of trillions of parameters.","The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms.","Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications.","The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer ."],"url":"http://arxiv.org/abs/2406.01698v1","category":"cs.AR"}
{"created":"2024-06-03 17:59:55","title":"Few-Shot Classification of Interactive Activities of Daily Living (InteractADL)","abstract":"Understanding Activities of Daily Living (ADLs) is a crucial step for different applications including assistive robots, smart homes, and healthcare. However, to date, few benchmarks and methods have focused on complex ADLs, especially those involving multi-person interactions in home environments. In this paper, we propose a new dataset and benchmark, InteractADL, for understanding complex ADLs that involve interaction between humans (and objects). Furthermore, complex ADLs occurring in home environments comprise a challenging long-tailed distribution due to the rarity of multi-person interactions, and pose fine-grained visual recognition tasks due to the presence of semantically and visually similar classes. To address these issues, we propose a novel method for fine-grained few-shot video classification called Name Tuning that enables greater semantic separability by learning optimal class name vectors. We show that Name Tuning can be combined with existing prompt tuning strategies to learn the entire input text (rather than only learning the prompt or class names) and demonstrate improved performance for few-shot classification on InteractADL and 4 other fine-grained visual classification benchmarks. For transparency and reproducibility, we release our code at https://github.com/zanedurante/vlm_benchmark.","sentences":["Understanding Activities of Daily Living (ADLs) is a crucial step for different applications including assistive robots, smart homes, and healthcare.","However, to date, few benchmarks and methods have focused on complex ADLs, especially those involving multi-person interactions in home environments.","In this paper, we propose a new dataset and benchmark, InteractADL, for understanding complex ADLs that involve interaction between humans (and objects).","Furthermore, complex ADLs occurring in home environments comprise a challenging long-tailed distribution due to the rarity of multi-person interactions, and pose fine-grained visual recognition tasks due to the presence of semantically and visually similar classes.","To address these issues, we propose a novel method for fine-grained few-shot video classification called Name Tuning that enables greater semantic separability by learning optimal class name vectors.","We show that Name Tuning can be combined with existing prompt tuning strategies to learn the entire input text (rather than only learning the prompt or class names) and demonstrate improved performance for few-shot classification on InteractADL and 4 other fine-grained visual classification benchmarks.","For transparency and reproducibility, we release our code at https://github.com/zanedurante/vlm_benchmark."],"url":"http://arxiv.org/abs/2406.01662v1","category":"cs.CV"}
{"created":"2024-06-03 17:59:43","title":"Text-guided Controllable Mesh Refinement for Interactive 3D Modeling","abstract":"We propose a novel technique for adding geometric details to an input coarse 3D mesh guided by a text prompt. Our method is composed of three stages. First, we generate a single-view RGB image conditioned on the input coarse geometry and the input text prompt. This single-view image generation step allows the user to pre-visualize the result and offers stronger conditioning for subsequent multi-view generation. Second, we use our novel multi-view normal generation architecture to jointly generate six different views of the normal images. The joint view generation reduces inconsistencies and leads to sharper details. Third, we optimize our mesh with respect to all views and generate a fine, detailed geometry as output. The resulting method produces an output within seconds and offers explicit user control over the coarse structure, pose, and desired details of the resulting 3D mesh. Project page: https://text-mesh-refinement.github.io.","sentences":["We propose a novel technique for adding geometric details to an input coarse 3D mesh guided by a text prompt.","Our method is composed of three stages.","First, we generate a single-view RGB image conditioned on the input coarse geometry and the input text prompt.","This single-view image generation step allows the user to pre-visualize the result and offers stronger conditioning for subsequent multi-view generation.","Second, we use our novel multi-view normal generation architecture to jointly generate six different views of the normal images.","The joint view generation reduces inconsistencies and leads to sharper details.","Third, we optimize our mesh with respect to all views and generate a fine, detailed geometry as output.","The resulting method produces an output within seconds and offers explicit user control over the coarse structure, pose, and desired details of the resulting 3D mesh.","Project page: https://text-mesh-refinement.github.io."],"url":"http://arxiv.org/abs/2406.01592v1","category":"cs.CV"}
{"created":"2024-06-03 17:59:30","title":"nn2poly: An R Package for Converting Neural Networks into Interpretable Polynomials","abstract":"The nn2poly package provides the implementation in R of the NN2Poly method to explain and interpret feed-forward neural networks by means of polynomial representations that predict in an equivalent manner as the original network.Through the obtained polynomial coefficients, the effect and importance of each variable and their interactions on the output can be represented. This capabiltiy of capturing interactions is a key aspect usually missing from most Explainable Artificial Intelligence (XAI) methods, specially if they rely on expensive computations that can be amplified when used on large neural networks. The package provides integration with the main deep learning framework packages in R (tensorflow and torch), allowing an user-friendly application of the NN2Poly algorithm. Furthermore, nn2poly provides implementation of the required weight constraints to be used during the network training in those same frameworks. Other neural networks packages can also be used by including their weights in list format. Polynomials obtained with nn2poly can also be used to predict with new data or be visualized through its own plot method. Simulations are provided exemplifying the usage of the package alongside with a comparison with other approaches available in R to interpret neural networks.","sentences":["The nn2poly package provides the implementation in R of the NN2Poly method to explain and interpret feed-forward neural networks by means of polynomial representations that predict in an equivalent manner as the original network.","Through the obtained polynomial coefficients, the effect and importance of each variable and their interactions on the output can be represented.","This capabiltiy of capturing interactions is a key aspect usually missing from most Explainable Artificial Intelligence (XAI) methods, specially if they rely on expensive computations that can be amplified when used on large neural networks.","The package provides integration with the main deep learning framework packages in R (tensorflow and torch), allowing an user-friendly application of the NN2Poly algorithm.","Furthermore, nn2poly provides implementation of the required weight constraints to be used during the network training in those same frameworks.","Other neural networks packages can also be used by including their weights in list format.","Polynomials obtained with nn2poly can also be used to predict with new data or be visualized through its own plot method.","Simulations are provided exemplifying the usage of the package alongside with a comparison with other approaches available in R to interpret neural networks."],"url":"http://arxiv.org/abs/2406.01588v1","category":"cs.LG"}
{"created":"2024-06-03 17:59:23","title":"ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation","abstract":"Diffusion models have been verified to be effective in generating complex distributions from natural images to motion trajectories. Recent diffusion-based methods show impressive performance in 3D robotic manipulation tasks, whereas they suffer from severe runtime inefficiency due to multiple denoising steps, especially with high-dimensional observations. To this end, we propose a real-time robotic manipulation model named ManiCM that imposes the consistency constraint to the diffusion process, so that the model can generate robot actions in only one-step inference. Specifically, we formulate a consistent diffusion process in the robot action space conditioned on the point cloud input, where the original action is required to be directly denoised from any point along the ODE trajectory. To model this process, we design a consistency distillation technique to predict the action sample directly instead of predicting the noise within the vision community for fast convergence in the low-dimensional action manifold. We evaluate ManiCM on 31 robotic manipulation tasks from Adroit and Metaworld, and the results demonstrate that our approach accelerates the state-of-the-art method by 10 times in average inference speed while maintaining competitive average success rate.","sentences":["Diffusion models have been verified to be effective in generating complex distributions from natural images to motion trajectories.","Recent diffusion-based methods show impressive performance in 3D robotic manipulation tasks, whereas they suffer from severe runtime inefficiency due to multiple denoising steps, especially with high-dimensional observations.","To this end, we propose a real-time robotic manipulation model named ManiCM that imposes the consistency constraint to the diffusion process, so that the model can generate robot actions in only one-step inference.","Specifically, we formulate a consistent diffusion process in the robot action space conditioned on the point cloud input, where the original action is required to be directly denoised from any point along the ODE trajectory.","To model this process, we design a consistency distillation technique to predict the action sample directly instead of predicting the noise within the vision community for fast convergence in the low-dimensional action manifold.","We evaluate ManiCM on 31 robotic manipulation tasks from Adroit and Metaworld, and the results demonstrate that our approach accelerates the state-of-the-art method by 10 times in average inference speed while maintaining competitive average success rate."],"url":"http://arxiv.org/abs/2406.01586v1","category":"cs.RO"}
{"created":"2024-06-03 17:55:13","title":"Towards Measuring the Impact of Technical Debt on Lead Time: An Industrial Case Study","abstract":"Background: Software companies must balance fast value delivery with quality, a trade-off that can introduce technical debt and potentially waste developers' time. As software systems evolve, technical debt tends to increase. However, estimating its impact on lead time still requires more empirical and experimental evidence.   Objective: We conduct an empirical study investigating whether technical debt impacts lead time in resolving Jira issues. Furthermore, our aim is to measure the extent to which variance in lead time is explainable by the technical debt.   Method: We conducted an industrial case study to examine the relationship in six components, each of which was analyzed individually. Technical debt was measured using SonarQube and normalized with the component's size, while lead time to resolve Jira issues was collected directly from Jira.   Results: We found a set of mixed results. Technical debt had a moderate positive impact on lead time in two components, while we did not see a meaningful impact on two others. A moderate negative impact was found in the remaining two components.   Conclusion: The findings show that technical debt alone can not explain all the variance in lead time, which ranges from 5% up to 41% across components. So, there should be some other variables (e.g., size of the changes made, complexity, number of teams involved, component ownership) impacting lead time, or it might have a residual effect that might manifest later on. Further investigation into those confounding variables is essential.","sentences":["Background: Software companies must balance fast value delivery with quality, a trade-off that can introduce technical debt and potentially waste developers' time.","As software systems evolve, technical debt tends to increase.","However, estimating its impact on lead time still requires more empirical and experimental evidence.   ","Objective: We conduct an empirical study investigating whether technical debt impacts lead time in resolving Jira issues.","Furthermore, our aim is to measure the extent to which variance in lead time is explainable by the technical debt.   ","Method: We conducted an industrial case study to examine the relationship in six components, each of which was analyzed individually.","Technical debt was measured using SonarQube and normalized with the component's size, while lead time to resolve Jira issues was collected directly from Jira.   ","Results:","We found a set of mixed results.","Technical debt had a moderate positive impact on lead time in two components, while we did not see a meaningful impact on two others.","A moderate negative impact was found in the remaining two components.   ","Conclusion: The findings show that technical debt alone can not explain all the variance in lead time, which ranges from 5% up to 41% across components.","So, there should be some other variables (e.g., size of the changes made, complexity, number of teams involved, component ownership) impacting lead time, or it might have a residual effect that might manifest later on.","Further investigation into those confounding variables is essential."],"url":"http://arxiv.org/abs/2406.01578v1","category":"cs.SE"}
{"created":"2024-06-03 17:55:02","title":"A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization","abstract":"Learning to sample from intractable distributions over discrete sets without relying on corresponding training data is a central problem in a wide range of fields, including Combinatorial Optimization. Currently, popular deep learning-based approaches rely primarily on generative models that yield exact sample likelihoods. This work introduces a method that lifts this restriction and opens the possibility to employ highly expressive latent variable models like diffusion models. Our approach is conceptually based on a loss that upper bounds the reverse Kullback-Leibler divergence and evades the requirement of exact sample likelihoods. We experimentally validate our approach in data-free Combinatorial Optimization and demonstrate that our method achieves a new state-of-the-art on a wide range of benchmark problems.","sentences":["Learning to sample from intractable distributions over discrete sets without relying on corresponding training data is a central problem in a wide range of fields, including Combinatorial Optimization.","Currently, popular deep learning-based approaches rely primarily on generative models that yield exact sample likelihoods.","This work introduces a method that lifts this restriction and opens the possibility to employ highly expressive latent variable models like diffusion models.","Our approach is conceptually based on a loss that upper bounds the reverse Kullback-Leibler divergence and evades the requirement of exact sample likelihoods.","We experimentally validate our approach in data-free Combinatorial Optimization and demonstrate that our method achieves a new state-of-the-art on a wide range of benchmark problems."],"url":"http://arxiv.org/abs/2406.01661v1","category":"cs.LG"}
{"created":"2024-06-03 17:54:39","title":"Stochastic Bilevel Optimization with Lower-Level Contextual Markov Decision Processes","abstract":"In various applications, the optimal policy in a strategic decision-making problem depends both on the environmental configuration and exogenous events. For these settings, we introduce Bilevel Optimization with Contextual Markov Decision Processes (BO-CMDP), a stochastic bilevel decision-making model, where the lower level consists of solving a contextual Markov Decision Process (CMDP). BO-CMDP can be viewed as a Stackelberg Game where the leader and a random context beyond the leader's control together decide the setup of (many) MDPs that (potentially multiple) followers best respond to. This framework extends beyond traditional bilevel optimization and finds relevance in diverse fields such as model design for MDPs, tax design, reward shaping and dynamic mechanism design. We propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm to solve BO-CMDP, and demonstrate its convergence. Notably, HPGD only utilizes observations of the followers' trajectories. Therefore, it allows followers to use any training procedure and the leader to be agnostic of the specific algorithm used, which aligns with various real-world scenarios. We further consider the setting when the leader can influence the training of followers and propose an accelerated algorithm. We empirically demonstrate the performance of our algorithm.","sentences":["In various applications, the optimal policy in a strategic decision-making problem depends both on the environmental configuration and exogenous events.","For these settings, we introduce Bilevel Optimization with Contextual Markov Decision Processes (BO-CMDP), a stochastic bilevel decision-making model, where the lower level consists of solving a contextual Markov Decision Process (CMDP).","BO-CMDP can be viewed as a Stackelberg Game where the leader and a random context beyond the leader's control together decide the setup of (many) MDPs that (potentially multiple) followers best respond to.","This framework extends beyond traditional bilevel optimization and finds relevance in diverse fields such as model design for MDPs, tax design, reward shaping and dynamic mechanism design.","We propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm to solve BO-CMDP, and demonstrate its convergence.","Notably, HPGD only utilizes observations of the followers' trajectories.","Therefore, it allows followers to use any training procedure and the leader to be agnostic of the specific algorithm used, which aligns with various real-world scenarios.","We further consider the setting when the leader can influence the training of followers and propose an accelerated algorithm.","We empirically demonstrate the performance of our algorithm."],"url":"http://arxiv.org/abs/2406.01575v1","category":"math.OC"}
{"created":"2024-06-03 17:53:25","title":"Self-Improving Robust Preference Optimization","abstract":"Both online and offline RLHF methods such as PPO and DPO have been extremely successful in aligning AI with human preferences. Despite their success, the existing methods suffer from a fundamental problem that their optimal solution is highly task-dependent (i.e., not robust to out-of-distribution (OOD) tasks). Here we address this challenge by proposing Self-Improving Robust Preference Optimization SRPO, a practical and mathematically principled offline RLHF framework that is completely robust to the changes in the task. The key idea of SRPO is to cast the problem of learning from human preferences as a self-improvement process, which can be mathematically expressed in terms of a min-max objective that aims at joint optimization of self-improvement policy and the generative policy in an adversarial fashion. The solution for this optimization problem is independent of the training task and thus it is robust to its changes. We then show that this objective can be re-expressed in the form of a non-adversarial offline loss which can be optimized using standard supervised optimization techniques at scale without any need for reward model and online inference. We show the effectiveness of SRPO in terms of AI Win-Rate (WR) against human (GOLD) completions. In particular, when SRPO is evaluated on the OOD XSUM dataset, it outperforms the celebrated DPO by a clear margin of 15% after 5 self-revisions, achieving WR of 90%.","sentences":["Both online and offline RLHF methods such as PPO and DPO have been extremely successful in aligning AI with human preferences.","Despite their success, the existing methods suffer from a fundamental problem that their optimal solution is highly task-dependent (i.e., not robust to out-of-distribution (OOD) tasks).","Here we address this challenge by proposing Self-Improving Robust Preference Optimization SRPO, a practical and mathematically principled offline RLHF framework that is completely robust to the changes in the task.","The key idea of SRPO is to cast the problem of learning from human preferences as a self-improvement process, which can be mathematically expressed in terms of a min-max objective that aims at joint optimization of self-improvement policy and the generative policy in an adversarial fashion.","The solution for this optimization problem is independent of the training task and thus it is robust to its changes.","We then show that this objective can be re-expressed in the form of a non-adversarial offline loss which can be optimized using standard supervised optimization techniques at scale without any need for reward model and online inference.","We show the effectiveness of SRPO in terms of AI Win-Rate (WR) against human (GOLD) completions.","In particular, when SRPO is evaluated on the OOD XSUM dataset, it outperforms the celebrated DPO by a clear margin of 15% after 5 self-revisions, achieving WR of 90%."],"url":"http://arxiv.org/abs/2406.01660v1","category":"cs.LG"}
{"created":"2024-06-03 17:45:19","title":"A New View on Planning in Online Reinforcement Learning","abstract":"This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can propagate value from an abstract space in a manner that helps a variety of base learners learn significantly faster in different domains.","sentences":["This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture.","Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation.","The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps.","In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models.","This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely.","We show that our GSP algorithm can propagate value from an abstract space in a manner that helps a variety of base learners learn significantly faster in different domains."],"url":"http://arxiv.org/abs/2406.01562v1","category":"cs.LG"}
{"created":"2024-06-03 17:44:11","title":"Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation","abstract":"Diffusion-based text-to-image generation models trained on extensive text-image pairs have shown the capacity to generate photorealistic images consistent with textual descriptions. However, a significant limitation of these models is their slow sample generation, which requires iterative refinement through the same network. In this paper, we enhance Score identity Distillation (SiD) by developing long and short classifier-free guidance (LSG) to efficiently distill pretrained Stable Diffusion models without using real training data. SiD aims to optimize a model-based explicit score matching loss, utilizing a score-identity-based approximation alongside the proposed LSG for practical computation. By training exclusively with fake images synthesized with its one-step generator, SiD equipped with LSG rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score. Specifically, its data-free distillation of Stable Diffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation set, with a CLIP score of 0.304 at an LSG scale of 1.5, and a FID of 9.56 with a CLIP score of 0.313 at an LSG scale of 2. We will make our PyTorch implementation and distilled Stable Diffusion one-step generators available at https://github.com/mingyuanzhou/SiD-LSG","sentences":["Diffusion-based text-to-image generation models trained on extensive text-image pairs have shown the capacity to generate photorealistic images consistent with textual descriptions.","However, a significant limitation of these models is their slow sample generation, which requires iterative refinement through the same network.","In this paper, we enhance Score identity Distillation (SiD) by developing long and short classifier-free guidance (LSG) to efficiently distill pretrained Stable Diffusion models without using real training data.","SiD aims to optimize a model-based explicit score matching loss, utilizing a score-identity-based approximation alongside the proposed LSG for practical computation.","By training exclusively with fake images synthesized with its one-step generator, SiD equipped with LSG rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score.","Specifically, its data-free distillation of Stable Diffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation set, with a CLIP score of 0.304 at an LSG scale of 1.5, and a FID of 9.56 with a CLIP score of 0.313 at an LSG scale of 2.","We will make our PyTorch implementation and distilled Stable Diffusion one-step generators available at https://github.com/mingyuanzhou/SiD-LSG"],"url":"http://arxiv.org/abs/2406.01561v1","category":"cs.CV"}
{"created":"2024-06-03 17:32:43","title":"Learning equivariant tensor functions with applications to sparse vector recovery","abstract":"This work characterizes equivariant polynomial functions from tuples of tensor inputs to tensor outputs. Loosely motivated by physics, we focus on equivariant functions with respect to the diagonal action of the orthogonal group on tensors. We show how to extend this characterization to other linear algebraic groups, including the Lorentz and symplectic groups.   Our goal behind these characterizations is to define equivariant machine learning models. In particular, we focus on the sparse vector estimation problem. This problem has been broadly studied in the theoretical computer science literature, and explicit spectral methods, derived by techniques from sum-of-squares, can be shown to recover sparse vectors under certain assumptions. Our numerical results show that the proposed equivariant machine learning models can learn spectral methods that outperform the best theoretically known spectral methods in some regimes. The experiments also suggest that learned spectral methods can solve the problem in settings that have not yet been theoretically analyzed.   This is an example of a promising direction in which theory can inform machine learning models and machine learning models could inform theory.","sentences":["This work characterizes equivariant polynomial functions from tuples of tensor inputs to tensor outputs.","Loosely motivated by physics, we focus on equivariant functions with respect to the diagonal action of the orthogonal group on tensors.","We show how to extend this characterization to other linear algebraic groups, including the Lorentz and symplectic groups.   ","Our goal behind these characterizations is to define equivariant machine learning models.","In particular, we focus on the sparse vector estimation problem.","This problem has been broadly studied in the theoretical computer science literature, and explicit spectral methods, derived by techniques from sum-of-squares, can be shown to recover sparse vectors under certain assumptions.","Our numerical results show that the proposed equivariant machine learning models can learn spectral methods that outperform the best theoretically known spectral methods in some regimes.","The experiments also suggest that learned spectral methods can solve the problem in settings that have not yet been theoretically analyzed.   ","This is an example of a promising direction in which theory can inform machine learning models and machine learning models could inform theory."],"url":"http://arxiv.org/abs/2406.01552v1","category":"stat.ML"}
{"created":"2024-06-03 17:32:23","title":"ELSA: Evaluating Localization of Social Activities in Urban Streets","abstract":"Why do some streets attract more social activities than others? Is it due to street design, or do land use patterns in neighborhoods create opportunities for businesses where people gather? These questions have intrigued urban sociologists, designers, and planners for decades. Yet, most research in this area has remained limited in scale, lacking a comprehensive perspective on the various factors influencing social interactions in urban settings. Exploring these issues requires fine-level data on the frequency and variety of social interactions on urban street. Recent advances in computer vision and the emergence of the open-vocabulary detection models offer a unique opportunity to address this long-standing issue on a scale that was previously impossible using traditional observational methods. In this paper, we propose a new benchmark dataset for Evaluating Localization of Social Activities (ELSA) in urban street images. ELSA draws on theoretical frameworks in urban sociology and design. While majority of action recognition datasets are collected in controlled settings, we use in-the-wild street-level imagery, where the size of social groups and the types of activities can vary significantly. ELSA includes 937 manually annotated images with more than 4,300 multi-labeled bounding boxes for individual and group activities, categorized into three primary groups: Condition, State, and Action. Each category contains various sub-categories, e.g., alone or group under Condition category, standing or walking, which fall under the State category, and talking or dining with regards to the Action category. ELSA is publicly available for the research community.","sentences":["Why do some streets attract more social activities than others?","Is it due to street design, or do land use patterns in neighborhoods create opportunities for businesses where people gather?","These questions have intrigued urban sociologists, designers, and planners for decades.","Yet, most research in this area has remained limited in scale, lacking a comprehensive perspective on the various factors influencing social interactions in urban settings.","Exploring these issues requires fine-level data on the frequency and variety of social interactions on urban street.","Recent advances in computer vision and the emergence of the open-vocabulary detection models offer a unique opportunity to address this long-standing issue on a scale that was previously impossible using traditional observational methods.","In this paper, we propose a new benchmark dataset for Evaluating Localization of Social Activities (ELSA) in urban street images.","ELSA draws on theoretical frameworks in urban sociology and design.","While majority of action recognition datasets are collected in controlled settings, we use in-the-wild street-level imagery, where the size of social groups and the types of activities can vary significantly.","ELSA includes 937 manually annotated images with more than 4,300 multi-labeled bounding boxes for individual and group activities, categorized into three primary groups: Condition, State, and Action.","Each category contains various sub-categories, e.g., alone or group under Condition category, standing or walking, which fall under the State category, and talking or dining with regards to the Action category.","ELSA is publicly available for the research community."],"url":"http://arxiv.org/abs/2406.01551v1","category":"cs.CV"}
{"created":"2024-06-03 17:31:06","title":"An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation","abstract":"Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\\%$ compression rate.","sentences":["Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data.","One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression.","In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation.","Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage.","In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards.","Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\\%$ compression rate."],"url":"http://arxiv.org/abs/2406.01549v1","category":"cs.CL"}
{"created":"2024-06-03 17:30:42","title":"How to discretize continuous state-action spaces in Q-learning: A symbolic control approach","abstract":"Q-learning is widely recognized as an effective approach for synthesizing controllers to achieve specific goals. However, handling challenges posed by continuous state-action spaces remains an ongoing research focus. This paper presents a systematic analysis that highlights a major drawback in space discretization methods. To address this challenge, the paper proposes a symbolic model that represents behavioral relations, such as alternating simulation from abstraction to the controlled system. This relation allows for seamless application of the synthesized controller based on abstraction to the original system. Introducing a novel Q-learning technique for symbolic models, the algorithm yields two Q-tables encoding optimal policies. Theoretical analysis demonstrates that these Q-tables serve as both upper and lower bounds on the Q-values of the original system with continuous spaces. Additionally, the paper explores the correlation between the parameters of the space abstraction and the loss in Q-values. The resulting algorithm facilitates achieving optimality within an arbitrary accuracy, providing control over the trade-off between accuracy and computational complexity. The obtained results provide valuable insights for selecting appropriate learning parameters and refining the controller. The engineering relevance of the proposed Q-learning based symbolic model is illustrated through two case studies.","sentences":["Q-learning is widely recognized as an effective approach for synthesizing controllers to achieve specific goals.","However, handling challenges posed by continuous state-action spaces remains an ongoing research focus.","This paper presents a systematic analysis that highlights a major drawback in space discretization methods.","To address this challenge, the paper proposes a symbolic model that represents behavioral relations, such as alternating simulation from abstraction to the controlled system.","This relation allows for seamless application of the synthesized controller based on abstraction to the original system.","Introducing a novel Q-learning technique for symbolic models, the algorithm yields two Q-tables encoding optimal policies.","Theoretical analysis demonstrates that these Q-tables serve as both upper and lower bounds on the Q-values of the original system with continuous spaces.","Additionally, the paper explores the correlation between the parameters of the space abstraction and the loss in Q-values.","The resulting algorithm facilitates achieving optimality within an arbitrary accuracy, providing control over the trade-off between accuracy and computational complexity.","The obtained results provide valuable insights for selecting appropriate learning parameters and refining the controller.","The engineering relevance of the proposed Q-learning based symbolic model is illustrated through two case studies."],"url":"http://arxiv.org/abs/2406.01548v2","category":"eess.SY"}
{"created":"2024-06-03 17:27:40","title":"TinySV: Speaker Verification in TinyML with On-device Learning","abstract":"TinyML is a novel area of machine learning that gained huge momentum in the last few years thanks to the ability to execute machine learning algorithms on tiny devices (such as Internet-of-Things or embedded systems). Interestingly, research in this area focused on the efficient execution of the inference phase of TinyML models on tiny devices, while very few solutions for on-device learning of TinyML models are available in the literature due to the relevant overhead introduced by the learning algorithms.   The aim of this paper is to introduce a new type of adaptive TinyML solution that can be used in tasks, such as the presented \\textit{Tiny Speaker Verification} (TinySV), that require to be tackled with an on-device learning algorithm. Achieving this goal required (i) reducing the memory and computational demand of TinyML learning algorithms, and (ii) designing a TinyML learning algorithm operating with few and possibly unlabelled training data. The proposed TinySV solution relies on a two-layer hierarchical TinyML solution comprising Keyword Spotting and Adaptive Speaker Verification module. We evaluated the effectiveness and efficiency of the proposed TinySV solution on a dataset collected expressly for the task and tested the proposed solution on a real-world IoT device (Infineon PSoC 62S2 Wi-Fi BT Pioneer Kit).","sentences":["TinyML is a novel area of machine learning that gained huge momentum in the last few years thanks to the ability to execute machine learning algorithms on tiny devices (such as Internet-of-Things or embedded systems).","Interestingly, research in this area focused on the efficient execution of the inference phase of TinyML models on tiny devices, while very few solutions for on-device learning of TinyML models are available in the literature due to the relevant overhead introduced by the learning algorithms.   ","The aim of this paper is to introduce a new type of adaptive TinyML solution that can be used in tasks, such as the presented \\textit{Tiny Speaker Verification} (TinySV), that require to be tackled with an on-device learning algorithm.","Achieving this goal required (i) reducing the memory and computational demand of TinyML learning algorithms, and (ii) designing a TinyML learning algorithm operating with few and possibly unlabelled training data.","The proposed TinySV solution relies on a two-layer hierarchical TinyML solution comprising Keyword Spotting and Adaptive Speaker Verification module.","We evaluated the effectiveness and efficiency of the proposed TinySV solution on a dataset collected expressly for the task and tested the proposed solution on a real-world IoT device (Infineon PSoC","62S2","Wi-Fi BT Pioneer Kit)."],"url":"http://arxiv.org/abs/2406.01655v1","category":"cs.SD"}
{"created":"2024-06-03 17:25:18","title":"Learning from Mistakes: a Weakly-supervised Method for Mitigating the Distribution Shift in Autonomous Vehicle Planning","abstract":"The planning problem constitutes a fundamental aspect of the autonomous driving framework. Recent strides in representation learning have empowered vehicles to comprehend their surrounding environments, thereby facilitating the integration of learning-based planning strategies. Among these approaches, Imitation Learning stands out due to its notable training efficiency. However, traditional Imitation Learning methodologies encounter challenges associated with the co-variate shift phenomenon. We propose Learn from Mistakes (LfM) as a remedy to address this issue. The essence of LfM lies in deploying a pre-trained planner across diverse scenarios. Instances where the planner deviates from its immediate objectives, such as maintaining a safe distance from obstacles or adhering to traffic rules, are flagged as mistakes. The environments corresponding to these mistakes are categorized as out-of-distribution states and compiled into a new dataset termed closed-loop mistakes dataset. Notably, the absence of expert annotations for the closed-loop data precludes the applicability of standard imitation learning approaches. To facilitate learning from the closed-loop mistakes, we introduce Validity Learning, a weakly supervised method, which aims to discern valid trajectories within the current environmental context. Experimental evaluations conducted on the InD and Nuplan datasets reveal substantial enhancements in closed-loop metrics such as Progress and Collision Rate, underscoring the effectiveness of the proposed methodology.","sentences":["The planning problem constitutes a fundamental aspect of the autonomous driving framework.","Recent strides in representation learning have empowered vehicles to comprehend their surrounding environments, thereby facilitating the integration of learning-based planning strategies.","Among these approaches, Imitation Learning stands out due to its notable training efficiency.","However, traditional Imitation Learning methodologies encounter challenges associated with the co-variate shift phenomenon.","We propose Learn from Mistakes (LfM) as a remedy to address this issue.","The essence of LfM lies in deploying a pre-trained planner across diverse scenarios.","Instances where the planner deviates from its immediate objectives, such as maintaining a safe distance from obstacles or adhering to traffic rules, are flagged as mistakes.","The environments corresponding to these mistakes are categorized as out-of-distribution states and compiled into a new dataset termed closed-loop mistakes dataset.","Notably, the absence of expert annotations for the closed-loop data precludes the applicability of standard imitation learning approaches.","To facilitate learning from the closed-loop mistakes, we introduce Validity Learning, a weakly supervised method, which aims to discern valid trajectories within the current environmental context.","Experimental evaluations conducted on the InD and Nuplan datasets reveal substantial enhancements in closed-loop metrics such as Progress and Collision Rate, underscoring the effectiveness of the proposed methodology."],"url":"http://arxiv.org/abs/2406.01544v1","category":"cs.RO"}
{"created":"2024-06-03 17:13:27","title":"What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores","abstract":"Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called \"brain score\". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore use contiguous splits moving forward. Second, we explain the surprisingly high brain scores of untrained LLMs by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.","sentences":["Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain.","One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called \"brain score\".","Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing.","This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing.","Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages.","We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain.","We therefore use contiguous splits moving forward.","Second, we explain the surprisingly high brain scores of untrained LLMs by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position.","This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like.","Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure.","We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals."],"url":"http://arxiv.org/abs/2406.01538v1","category":"cs.CL"}
{"created":"2024-06-03 16:53:37","title":"Coarse Grained Molecular Dynamics with Normalizing Flows","abstract":"We propose a sampling algorithm relying on a collective variable (CV) of mid-size dimension modelled by a normalizing flow and using non-equilibrium dynamics to propose full configurational moves from the proposition of a refreshed value of the CV made by the flow. The algorithm takes the form of a Markov chain with non-local updates, allowing jumps through energy barriers across metastable states. The flow is trained throughout the algorithm to reproduce the free energy landscape of the CV. The output of the algorithm are a sample of thermalized configurations and the trained network that can be used to efficiently produce more configurations. We show the functioning of the algorithm first on a test case with a mixture of Gaussians. Then we successfully test it on a higher dimensional system consisting in a polymer in solution with a compact and an extended stable state separated by a high free energy barrier.","sentences":["We propose a sampling algorithm relying on a collective variable (CV) of mid-size dimension modelled by a normalizing flow and using non-equilibrium dynamics to propose full configurational moves from the proposition of a refreshed value of the CV made by the flow.","The algorithm takes the form of a Markov chain with non-local updates, allowing jumps through energy barriers across metastable states.","The flow is trained throughout the algorithm to reproduce the free energy landscape of the CV.","The output of the algorithm are a sample of thermalized configurations and the trained network that can be used to efficiently produce more configurations.","We show the functioning of the algorithm first on a test case with a mixture of Gaussians.","Then we successfully test it on a higher dimensional system consisting in a polymer in solution with a compact and an extended stable state separated by a high free energy barrier."],"url":"http://arxiv.org/abs/2406.01524v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-03 16:46:18","title":"Decoupled Alignment for Robust Plug-and-Play Adaptation","abstract":"We introduce a low-resource safety enhancement method for aligning large language models (LLMs) without the need for supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF). Our main idea is to exploit knowledge distillation to extract the alignment information from existing well-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play fashion. Methodology, we employ delta debugging to identify the critical components of knowledge necessary for effective distillation. On the harmful question dataset, our method significantly enhances the average defense success rate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned pre-trained LLMs, without compromising performance.","sentences":["We introduce a low-resource safety enhancement method for aligning large language models (LLMs) without the need for supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF).","Our main idea is to exploit knowledge distillation to extract the alignment information from existing well-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play fashion.","Methodology, we employ delta debugging to identify the critical components of knowledge necessary for effective distillation.","On the harmful question dataset, our method significantly enhances the average defense success rate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned pre-trained LLMs, without compromising performance."],"url":"http://arxiv.org/abs/2406.01514v2","category":"cs.CL"}
{"created":"2024-06-03 16:45:06","title":"Enhancing the efficiency of quantum measurement-based engines with entangling measurements","abstract":"We study the impact of entangling measurements on the efficiency of quantum measurement- based engines. We first show that for engines comprising many subsystems their efficiency can be enhanced by performing entangling measurements, as opposed to local measurements over each subsystem. When the collective measurement produces the same local state for the subsystems as individual local measurements, the improvement in the efficiency is proportional to the amount of correlations. Finally, we show that for two level systems these type of engine can operate at perfect efficiency while yielding a finite amount of work, in the limit large the number of subsystems.","sentences":["We study the impact of entangling measurements on the efficiency of quantum measurement- based engines.","We first show that for engines comprising many subsystems their efficiency can be enhanced by performing entangling measurements, as opposed to local measurements over each subsystem.","When the collective measurement produces the same local state for the subsystems as individual local measurements, the improvement in the efficiency is proportional to the amount of correlations.","Finally, we show that for two level systems these type of engine can operate at perfect efficiency while yielding a finite amount of work, in the limit large the number of subsystems."],"url":"http://arxiv.org/abs/2406.01513v1","category":"quant-ph"}
{"created":"2024-06-03 16:34:01","title":"The Geometry of Categorical and Hierarchical Concepts in Large Language Models","abstract":"Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as {'mammal', 'bird', 'reptile', 'fish'}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.","sentences":["Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability.","In this paper, we study the two foundational questions in this area.","First, how are categorical concepts, such as {'mammal', 'bird', 'reptile', 'fish'}, represented?","Second, how are hierarchical relations between concepts encoded?","For example, how is the fact that 'dog' is a kind of 'mammal' encoded?","We show how to extend the linear representation hypothesis to answer these questions.","We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure.","We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet."],"url":"http://arxiv.org/abs/2406.01506v1","category":"cs.CL"}
{"created":"2024-06-03 16:13:33","title":"DA-HFNet: Progressive Fine-Grained Forgery Image Detection and Localization Based on Dual Attention","abstract":"The increasing difficulty in accurately detecting forged images generated by AIGC(Artificial Intelligence Generative Content) poses many risks, necessitating the development of effective methods to identify and further locate forged areas. In this paper, to facilitate research efforts, we construct a DA-HFNet forged image dataset guided by text or image-assisted GAN and Diffusion model. Our goal is to utilize a hierarchical progressive network to capture forged artifacts at different scales for detection and localization. Specifically, it relies on a dual-attention mechanism to adaptively fuse multi-modal image features in depth, followed by a multi-branch interaction network to thoroughly interact image features at different scales and improve detector performance by leveraging dependencies between layers. Additionally, we extract more sensitive noise fingerprints to obtain more prominent forged artifact features in the forged areas. Extensive experiments validate the effectiveness of our approach, demonstrating significant performance improvements compared to state-of-the-art methods for forged image detection and localization.The code and dataset will be released in the future.","sentences":["The increasing difficulty in accurately detecting forged images generated by AIGC(Artificial Intelligence Generative Content) poses many risks, necessitating the development of effective methods to identify and further locate forged areas.","In this paper, to facilitate research efforts, we construct a DA-HFNet forged image dataset guided by text or image-assisted GAN and Diffusion model.","Our goal is to utilize a hierarchical progressive network to capture forged artifacts at different scales for detection and localization.","Specifically, it relies on a dual-attention mechanism to adaptively fuse multi-modal image features in depth, followed by a multi-branch interaction network to thoroughly interact image features at different scales and improve detector performance by leveraging dependencies between layers.","Additionally, we extract more sensitive noise fingerprints to obtain more prominent forged artifact features in the forged areas.","Extensive experiments validate the effectiveness of our approach, demonstrating significant performance improvements compared to state-of-the-art methods for forged image detection and localization.","The code and dataset will be released in the future."],"url":"http://arxiv.org/abs/2406.01489v2","category":"cs.CV"}
{"created":"2024-06-03 16:11:39","title":"Differentiable Task Graph Learning: Procedural Activity Representation and Online Mistake Detection from Egocentric Videos","abstract":"Procedural activities are sequences of key-steps aimed at achieving specific goals. They are crucial to build intelligent agents able to assist users effectively. In this context, task graphs have emerged as a human-understandable representation of procedural activities, encoding a partial ordering over the key-steps. While previous works generally relied on hand-crafted procedures to extract task graphs from videos, in this paper, we propose an approach based on direct maximum likelihood optimization of edges' weights, which allows gradient-based learning of task graphs and can be naturally plugged into neural network architectures. Experiments on the CaptainCook4D dataset demonstrate the ability of our approach to predict accurate task graphs from the observation of action sequences, with an improvement of +16.7% over previous approaches. Owing to the differentiability of the proposed framework, we also introduce a feature-based approach, aiming to predict task graphs from key-step textual or video embeddings, for which we observe emerging video understanding abilities. Task graphs learned with our approach are also shown to significantly enhance online mistake detection in procedural egocentric videos, achieving notable gains of +19.8% and +7.5% on the Assembly101 and EPIC-Tent datasets. Code for replicating experiments is available at https://github.com/fpv-iplab/Differentiable-Task-Graph-Learning.","sentences":["Procedural activities are sequences of key-steps aimed at achieving specific goals.","They are crucial to build intelligent agents able to assist users effectively.","In this context, task graphs have emerged as a human-understandable representation of procedural activities, encoding a partial ordering over the key-steps.","While previous works generally relied on hand-crafted procedures to extract task graphs from videos, in this paper, we propose an approach based on direct maximum likelihood optimization of edges' weights, which allows gradient-based learning of task graphs and can be naturally plugged into neural network architectures.","Experiments on the CaptainCook4D dataset demonstrate the ability of our approach to predict accurate task graphs from the observation of action sequences, with an improvement of +16.7% over previous approaches.","Owing to the differentiability of the proposed framework, we also introduce a feature-based approach, aiming to predict task graphs from key-step textual or video embeddings, for which we observe emerging video understanding abilities.","Task graphs learned with our approach are also shown to significantly enhance online mistake detection in procedural egocentric videos, achieving notable gains of +19.8% and","+7.5% on the Assembly101 and EPIC-Tent datasets.","Code for replicating experiments is available at https://github.com/fpv-iplab/Differentiable-Task-Graph-Learning."],"url":"http://arxiv.org/abs/2406.01486v1","category":"cs.CV"}
{"created":"2024-06-03 16:07:52","title":"Learning from Streaming Data when Users Choose","abstract":"In digital markets comprised of many competing services, each user chooses between multiple service providers according to their preferences, and the chosen service makes use of the user data to incrementally improve its model. The service providers' models influence which service the user will choose at the next time step, and the user's choice, in return, influences the model update, leading to a feedback loop. In this paper, we formalize the above dynamics and develop a simple and efficient decentralized algorithm to locally minimize the overall user loss. Theoretically, we show that our algorithm asymptotically converges to stationary points of of the overall loss almost surely. We also experimentally demonstrate the utility of our algorithm with real world data.","sentences":["In digital markets comprised of many competing services, each user chooses between multiple service providers according to their preferences, and the chosen service makes use of the user data to incrementally improve its model.","The service providers' models influence which service the user will choose at the next time step, and the user's choice, in return, influences the model update, leading to a feedback loop.","In this paper, we formalize the above dynamics and develop a simple and efficient decentralized algorithm to locally minimize the overall user loss.","Theoretically, we show that our algorithm asymptotically converges to stationary points of of the overall loss almost surely.","We also experimentally demonstrate the utility of our algorithm with real world data."],"url":"http://arxiv.org/abs/2406.01481v1","category":"cs.LG"}
{"created":"2024-06-03 15:57:29","title":"Understanding Token Probability Encoding in Output Embeddings","abstract":"In this paper, we investigate the output token probability information in the output embedding of language models. We provide an approximate common log-linear encoding of output token probabilities within the output embedding vectors and demonstrate that it is accurate and sparse when the output space is large and output logits are concentrated. Based on such findings, we edit the encoding in output embedding to modify the output probability distribution accurately. Moreover, the sparsity we find in output probability encoding suggests that a large number of dimensions in the output embedding do not contribute to causal language modeling. Therefore, we attempt to delete the output-unrelated dimensions and find more than 30% of the dimensions can be deleted without significant movement in output distribution and degeneration on sequence generation. Additionally, in training dynamics, we use such encoding as a probe and find that the output embeddings capture token frequency information in early steps, even before an obvious convergence starts.","sentences":["In this paper, we investigate the output token probability information in the output embedding of language models.","We provide an approximate common log-linear encoding of output token probabilities within the output embedding vectors and demonstrate that it is accurate and sparse when the output space is large and output logits are concentrated.","Based on such findings, we edit the encoding in output embedding to modify the output probability distribution accurately.","Moreover, the sparsity we find in output probability encoding suggests that a large number of dimensions in the output embedding do not contribute to causal language modeling.","Therefore, we attempt to delete the output-unrelated dimensions and find more than 30% of the dimensions can be deleted without significant movement in output distribution and degeneration on sequence generation.","Additionally, in training dynamics, we use such encoding as a probe and find that the output embeddings capture token frequency information in early steps, even before an obvious convergence starts."],"url":"http://arxiv.org/abs/2406.01468v1","category":"cs.CL"}
{"created":"2024-06-03 15:55:10","title":"AIFS - ECMWF's data-driven forecasting system","abstract":"Machine learning-based weather forecasting models have quickly emerged as a promising methodology for accurate medium-range global weather forecasting. Here, we introduce the Artificial Intelligence Forecasting System (AIFS), a data driven forecast model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS is based on a graph neural network (GNN) encoder and decoder, and a sliding window transformer processor, and is trained on ECMWF's ERA5 re-analysis and ECMWF's operational numerical weather prediction (NWP) analyses. It has a flexible and modular design and supports several levels of parallelism to enable training on high-resolution input data. AIFS forecast skill is assessed by comparing its forecasts to NWP analyses and direct observational data. We show that AIFS produces highly skilled forecasts for upper-air variables, surface weather parameters and tropical cyclone tracks. AIFS is run four times daily alongside ECMWF's physics-based NWP model and forecasts are available to the public under ECMWF's open data policy.","sentences":["Machine learning-based weather forecasting models have quickly emerged as a promising methodology for accurate medium-range global weather forecasting.","Here, we introduce the Artificial Intelligence Forecasting System (AIFS), a data driven forecast model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF).","AIFS is based on a graph neural network (GNN) encoder and decoder, and a sliding window transformer processor, and is trained on ECMWF's ERA5 re-analysis and ECMWF's operational numerical weather prediction (NWP) analyses.","It has a flexible and modular design and supports several levels of parallelism to enable training on high-resolution input data.","AIFS forecast skill is assessed by comparing its forecasts to NWP analyses and direct observational data.","We show that AIFS produces highly skilled forecasts for upper-air variables, surface weather parameters and tropical cyclone tracks.","AIFS is run four times daily alongside ECMWF's physics-based NWP model and forecasts are available to the public under ECMWF's open data policy."],"url":"http://arxiv.org/abs/2406.01465v1","category":"physics.ao-ph"}
{"created":"2024-06-03 15:49:11","title":"MLIP: Efficient Multi-Perspective Language-Image Pretraining with Exhaustive Data Utilization","abstract":"Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success, leading to rapid advancements in multimodal studies. However, CLIP faces a notable challenge in terms of inefficient data utilization. It relies on a single contrastive supervision for each image-text pair during representation learning, disregarding a substantial amount of valuable information that could offer richer supervision. Additionally, the retention of non-informative tokens leads to increased computational demands and time costs, particularly in CLIP's ViT image encoder. To address these issues, we propose Multi-Perspective Language-Image Pretraining (MLIP). In MLIP, we leverage the frequency transform's sensitivity to both high and low-frequency variations, which complements the spatial domain's sensitivity limited to low-frequency variations only. By incorporating frequency transforms and token-level alignment, we expand CILP's single supervision into multi-domain and multi-level supervision, enabling a more thorough exploration of informative image features. Additionally, we introduce a token merging method guided by comprehensive semantics from the frequency and spatial domains. This allows us to merge tokens to multi-granularity tokens with a controllable compression rate to accelerate CLIP. Extensive experiments validate the effectiveness of our design.","sentences":["Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success, leading to rapid advancements in multimodal studies.","However, CLIP faces a notable challenge in terms of inefficient data utilization.","It relies on a single contrastive supervision for each image-text pair during representation learning, disregarding a substantial amount of valuable information that could offer richer supervision.","Additionally, the retention of non-informative tokens leads to increased computational demands and time costs, particularly in CLIP's ViT image encoder.","To address these issues, we propose Multi-Perspective Language-Image Pretraining (MLIP).","In MLIP, we leverage the frequency transform's sensitivity to both high and low-frequency variations, which complements the spatial domain's sensitivity limited to low-frequency variations only.","By incorporating frequency transforms and token-level alignment, we expand CILP's single supervision into multi-domain and multi-level supervision, enabling a more thorough exploration of informative image features.","Additionally, we introduce a token merging method guided by comprehensive semantics from the frequency and spatial domains.","This allows us to merge tokens to multi-granularity tokens with a controllable compression rate to accelerate CLIP.","Extensive experiments validate the effectiveness of our design."],"url":"http://arxiv.org/abs/2406.01460v2","category":"cs.CV"}
{"created":"2024-06-03 15:43:29","title":"Automatic Fused Multimodal Deep Learning for Plant Identification","abstract":"Plant classification is vital for ecological conservation and agricultural productivity, enhancing our understanding of plant growth dynamics and aiding species preservation. The advent of deep learning (DL) techniques has revolutionized this field by enabling autonomous feature extraction, significantly reducing the dependence on manual expertise. However, conventional DL models often rely solely on single data sources, failing to capture the full biological diversity of plant species comprehensively. Recent research has turned to multimodal learning to overcome this limitation by integrating multiple data types, which enriches the representation of plant characteristics. This shift introduces the challenge of determining the optimal point for modality fusion. In this paper, we introduce a pioneering multimodal DL-based approach for plant classification with automatic modality fusion. Utilizing the multimodal fusion architecture search, our method integrates images from multiple plant organs-flowers, leaves, fruits, and stems-into a cohesive model. Our method achieves 83.48% accuracy on 956 classes of the PlantCLEF2015 dataset, surpassing state-of-the-art methods. It outperforms late fusion by 11.07% and is more robust to missing modalities. We validate our model against established benchmarks using standard performance metrics and McNemar's test, further underscoring its superiority.","sentences":["Plant classification is vital for ecological conservation and agricultural productivity, enhancing our understanding of plant growth dynamics and aiding species preservation.","The advent of deep learning (DL) techniques has revolutionized this field by enabling autonomous feature extraction, significantly reducing the dependence on manual expertise.","However, conventional DL models often rely solely on single data sources, failing to capture the full biological diversity of plant species comprehensively.","Recent research has turned to multimodal learning to overcome this limitation by integrating multiple data types, which enriches the representation of plant characteristics.","This shift introduces the challenge of determining the optimal point for modality fusion.","In this paper, we introduce a pioneering multimodal DL-based approach for plant classification with automatic modality fusion.","Utilizing the multimodal fusion architecture search, our method integrates images from multiple plant organs-flowers, leaves, fruits, and stems-into a cohesive model.","Our method achieves 83.48% accuracy on 956 classes of the PlantCLEF2015 dataset, surpassing state-of-the-art methods.","It outperforms late fusion by 11.07% and is more robust to missing modalities.","We validate our model against established benchmarks using standard performance metrics and McNemar's test, further underscoring its superiority."],"url":"http://arxiv.org/abs/2406.01455v1","category":"cs.CV"}
{"created":"2024-06-03 15:42:31","title":"From high-dimensional committors to reactive insights","abstract":"Transition path theory (TPT) offers a powerful formalism for extracting the rate and mechanism of rare dynamical transitions between metastable states. Most applications of TPT either focus on systems with modestly sized state spaces or use collective variables to try to tame the curse of dimensionality. Increasingly, expressive function approximators like neural networks and tensor networks have shown promise in computing the central object of TPT, the committor function, even in very high dimensional systems. That progress prompts our consideration of how one could use such a high dimensional function to extract mechanistic insight. Here, we present and illustrate a straightforward but powerful way to track how individual dynamical coordinates evolve during a reactive event. The strategy, which involves marginalizing the reactive ensemble, naturally captures the evolution of the dynamical coordinate's distribution, not just its mean reactive behavior.","sentences":["Transition path theory (TPT) offers a powerful formalism for extracting the rate and mechanism of rare dynamical transitions between metastable states.","Most applications of TPT either focus on systems with modestly sized state spaces or use collective variables to try to tame the curse of dimensionality.","Increasingly, expressive function approximators like neural networks and tensor networks have shown promise in computing the central object of TPT, the committor function, even in very high dimensional systems.","That progress prompts our consideration of how one could use such a high dimensional function to extract mechanistic insight.","Here, we present and illustrate a straightforward but powerful way to track how individual dynamical coordinates evolve during a reactive event.","The strategy, which involves marginalizing the reactive ensemble, naturally captures the evolution of the dynamical coordinate's distribution, not just its mean reactive behavior."],"url":"http://arxiv.org/abs/2406.01452v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-03 15:30:36","title":"LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation","abstract":"The fine-tuning of open-source large language models (LLMs) for machine translation has recently received considerable attention, marking a shift towards data-centric research from traditional neural machine translation. However, the area of data collection for instruction fine-tuning in machine translation remains relatively underexplored. In this paper, we present LexMatcher, a simple yet effective method for data collection that leverages bilingual dictionaries to generate a dataset, the design of which is driven by the coverage of senses found in these dictionaries. The dataset comprises a subset retrieved from an existing corpus and a smaller synthesized subset which supplements the infrequent senses of polysemous words. Utilizing LLaMA2 as our base model, our approach outperforms the established baselines on the WMT2022 test sets and also exhibits significant performance improvements in tasks related to word sense disambiguation and specialized terminology translation. These results underscore the effectiveness of LexMatcher in enhancing LLM-based machine translation.","sentences":["The fine-tuning of open-source large language models (LLMs) for machine translation has recently received considerable attention, marking a shift towards data-centric research from traditional neural machine translation.","However, the area of data collection for instruction fine-tuning in machine translation remains relatively underexplored.","In this paper, we present LexMatcher, a simple yet effective method for data collection that leverages bilingual dictionaries to generate a dataset, the design of which is driven by the coverage of senses found in these dictionaries.","The dataset comprises a subset retrieved from an existing corpus and a smaller synthesized subset which supplements the infrequent senses of polysemous words.","Utilizing LLaMA2 as our base model, our approach outperforms the established baselines on the WMT2022 test sets and also exhibits significant performance improvements in tasks related to word sense disambiguation and specialized terminology translation.","These results underscore the effectiveness of LexMatcher in enhancing LLM-based machine translation."],"url":"http://arxiv.org/abs/2406.01441v1","category":"cs.CL"}
{"created":"2024-06-03 15:29:38","title":"Asynchronous Byzantine Federated Learning","abstract":"Federated learning (FL) enables a set of geographically distributed clients to collectively train a model through a server. Classically, the training process is synchronous, but can be made asynchronous to maintain its speed in presence of slow clients and in heterogeneous networks. The vast majority of Byzantine fault-tolerant FL systems however rely on a synchronous training process. Our solution is one of the first Byzantine-resilient and asynchronous FL algorithms that does not require an auxiliary server dataset and is not delayed by stragglers, which are shortcomings of previous works. Intuitively, the server in our solution waits to receive a minimum number of updates from clients on its latest model to safely update it, and is later able to safely leverage the updates that late clients might send. We compare the performance of our solution with state-of-the-art algorithms on both image and text datasets under gradient inversion, perturbation, and backdoor attacks. Our results indicate that our solution trains a model faster than previous synchronous FL solution, and maintains a higher accuracy, up to 1.54x and up to 1.75x for perturbation and gradient inversion attacks respectively, in the presence of Byzantine clients than previous asynchronous FL solutions.","sentences":["Federated learning (FL) enables a set of geographically distributed clients to collectively train a model through a server.","Classically, the training process is synchronous, but can be made asynchronous to maintain its speed in presence of slow clients and in heterogeneous networks.","The vast majority of Byzantine fault-tolerant FL systems however rely on a synchronous training process.","Our solution is one of the first Byzantine-resilient and asynchronous FL algorithms that does not require an auxiliary server dataset and is not delayed by stragglers, which are shortcomings of previous works.","Intuitively, the server in our solution waits to receive a minimum number of updates from clients on its latest model to safely update it, and is later able to safely leverage the updates that late clients might send.","We compare the performance of our solution with state-of-the-art algorithms on both image and text datasets under gradient inversion, perturbation, and backdoor attacks.","Our results indicate that our solution trains a model faster than previous synchronous FL solution, and maintains a higher accuracy, up to 1.54x and up to 1.75x for perturbation and gradient inversion attacks respectively, in the presence of Byzantine clients than previous asynchronous FL solutions."],"url":"http://arxiv.org/abs/2406.01438v1","category":"cs.LG"}
{"created":"2024-06-03 15:28:21","title":"Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models","abstract":"Knowledge editing is a rising technique for efficiently updating factual knowledge in Large Language Models (LLMs) with minimal alteration of parameters. However, recent studies have identified concerning side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing. This survey presents a comprehensive study of these side effects, providing a unified view of the challenges associated with knowledge editing in LLMs. We discuss related works and summarize potential research directions to overcome these limitations. Our work highlights the limitations of current knowledge editing methods, emphasizing the need for deeper understanding of inner knowledge structures of LLMs and improved knowledge editing methods. To foster future research, we have released the complementary materials such as paper collection publicly at https://github.com/MiuLab/EditLLM-Survey","sentences":["Knowledge editing is a rising technique for efficiently updating factual knowledge in Large Language Models (LLMs) with minimal alteration of parameters.","However, recent studies have identified concerning side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing.","This survey presents a comprehensive study of these side effects, providing a unified view of the challenges associated with knowledge editing in LLMs.","We discuss related works and summarize potential research directions to overcome these limitations.","Our work highlights the limitations of current knowledge editing methods, emphasizing the need for deeper understanding of inner knowledge structures of LLMs and improved knowledge editing methods.","To foster future research, we have released the complementary materials such as paper collection publicly at https://github.com/MiuLab/EditLLM-Survey"],"url":"http://arxiv.org/abs/2406.01436v1","category":"cs.CL"}
{"created":"2024-06-03 15:26:58","title":"The azimuthal correlation between the leading jet and the scattered lepton in deep inelastic scattering at HERA","abstract":"The azimuthal correlation angle, $\\Delta\\phi$, between the scattered lepton and the leading jet in deep inelastic $e^{\\pm}p$ scattering at HERA has been studied using data collected with the ZEUS detector at a centre-of-mass energy of $\\sqrt{s} = 318 \\;\\mathrm{GeV}$, corresponding to an integrated luminosity of $326 \\;\\mathrm{pb}^{-1}$. A measurement of jet cross sections in the laboratory frame was made in a fiducial region corresponding to photon virtuality $10 \\;\\mathrm{GeV}^2 < Q^2 < 350 \\;\\mathrm{GeV}^2$, inelasticity $0.04 < y < 0.7$, outgoing lepton energy $E_e > 10 \\;\\mathrm{GeV}$, lepton polar angle $140^\\circ < \\theta_e < 180^\\circ$, jet transverse momentum $2.5 \\;\\mathrm{GeV} < p_\\mathrm{T,jet} < 30 \\;\\mathrm{GeV}$, and jet pseudorapidity $-1.5 < \\eta_\\mathrm{jet} < 1.8$. Jets were reconstructed using the $k_\\mathrm{T}$ algorithm with the radius parameter $R = 1$. The leading jet in an event is defined as the jet that carries the highest $p_\\mathrm{T,jet}$. Differential cross sections, $d\\sigma/d\\Delta\\phi$, were measured as a function of the azimuthal correlation angle in various ranges of leading-jet transverse momentum, photon virtuality and jet multiplicity. Perturbative calculations at $\\mathcal{O}(\\alpha_{s}^2)$ accuracy successfully describe the data within the fiducial region, although a lower level of agreement is observed near $\\Delta\\phi \\rightarrow \\pi$ for events with high jet multiplicity, due to limitations of the perturbative approach in describing soft phenomena in QCD. The data are equally well described by Monte Carlo predictions that supplement leading-order matrix elements with parton showering.","sentences":["The azimuthal correlation angle, $\\Delta\\phi$, between the scattered lepton and the leading jet in deep inelastic $e^{\\pm}p$ scattering at HERA has been studied using data collected with the ZEUS detector at a centre-of-mass energy of $\\sqrt{s} = 318 \\;\\mathrm{GeV}$, corresponding to an integrated luminosity of $326 \\;\\mathrm{pb}^{-1}$.","A measurement of jet cross sections in the laboratory frame was made in a fiducial region corresponding to photon virtuality $10 \\;\\mathrm{GeV}^2 < Q^2 < 350 \\;\\mathrm{GeV}^2$, inelasticity $0.04 <","y < 0.7$, outgoing lepton energy $E_e > 10 \\;\\mathrm{GeV}$, lepton polar angle $140^\\circ <","\\theta_e < 180^\\circ$, jet transverse momentum $2.5 \\;\\mathrm{GeV} < p_\\mathrm{T,jet} < 30 \\;\\mathrm{GeV}$, and jet pseudorapidity $-1.5 < \\eta_\\mathrm{jet} <","1.8$. Jets were reconstructed using the $k_\\mathrm{T}$ algorithm with the radius parameter $R = 1$.","The leading jet in an event is defined as the jet that carries the highest $p_\\mathrm{T,jet}$. Differential cross sections, $d\\sigma/d\\Delta\\phi$, were measured as a function of the azimuthal correlation angle in various ranges of leading-jet transverse momentum, photon virtuality and jet multiplicity.","Perturbative calculations at $\\mathcal{O}(\\alpha_{s}^2)$ accuracy successfully describe the data within the fiducial region, although a lower level of agreement is observed near $\\Delta\\phi \\rightarrow \\pi$ for events with high jet multiplicity, due to limitations of the perturbative approach in describing soft phenomena in QCD.","The data are equally well described by Monte Carlo predictions that supplement leading-order matrix elements with parton showering."],"url":"http://arxiv.org/abs/2406.01430v1","category":"hep-ex"}
{"created":"2024-06-03 15:25:13","title":"Universal In-Context Approximation By Prompting Fully Recurrent Models","abstract":"Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions. Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator. While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism. Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve as universal in-context approximators. To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures. LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks. We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation.","sentences":["Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions.","Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator.","While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism.","Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs.","We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve as universal in-context approximators.","To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures.","LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks.","We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation."],"url":"http://arxiv.org/abs/2406.01424v1","category":"cs.LG"}
{"created":"2024-06-03 15:24:15","title":"Value Improved Actor Critic Algorithms","abstract":"Many modern reinforcement learning algorithms build on the actor-critic (AC) framework: iterative improvement of a policy (the actor) using policy improvement operators and iterative approximation of the policy's value (the critic). In contrast, the popular value-based algorithm family employs improvement operators in the value update, to iteratively improve the value function directly. In this work, we propose a general extension to the AC framework that employs two separate improvement operators: one applied to the policy in the spirit of policy-based algorithms and one applied to the value in the spirit of value-based algorithms, which we dub Value-Improved AC (VI-AC). We design two practical VI-AC algorithms based in the popular online off-policy AC algorithms TD3 and DDPG. We evaluate VI-TD3 and VI-DDPG in the Mujoco benchmark and find that both improve upon or match the performance of their respective baselines in all environments tested.","sentences":["Many modern reinforcement learning algorithms build on the actor-critic (AC) framework: iterative improvement of a policy (the actor) using policy improvement operators and iterative approximation of the policy's value (the critic).","In contrast, the popular value-based algorithm family employs improvement operators in the value update, to iteratively improve the value function directly.","In this work, we propose a general extension to the AC framework that employs two separate improvement operators: one applied to the policy in the spirit of policy-based algorithms and one applied to the value in the spirit of value-based algorithms, which we dub Value-Improved AC (VI-AC).","We design two practical VI-AC algorithms based in the popular online off-policy AC algorithms TD3 and DDPG.","We evaluate VI-TD3 and VI-DDPG in the Mujoco benchmark and find that both improve upon or match the performance of their respective baselines in all environments tested."],"url":"http://arxiv.org/abs/2406.01423v1","category":"cs.LG"}
{"created":"2024-06-03 15:20:05","title":"Problematizing AI Omnipresence in Landscape Architecture","abstract":"This position paper argues for, and offers, a critical lens through which to examine the current AI frenzy in the landscape architecture profession. In it, the authors propose five archetypes or mental modes that landscape architects might inhabit when thinking about AI. Rather than limiting judgments of AI use to a single axis of acceleration, these archetypes and corresponding narratives exist along a relational spectrum and are permeable, allowing LAs to take on and switch between them according to context. We model these relationships between the archetypes and their contributions to AI advancement using a causal loop diagram (CLD), and with those interactions argue that more nuanced ways of approaching AI might also open new modes of practice in the new digital economy.","sentences":["This position paper argues for, and offers, a critical lens through which to examine the current AI frenzy in the landscape architecture profession.","In it, the authors propose five archetypes or mental modes that landscape architects might inhabit when thinking about AI.","Rather than limiting judgments of AI use to a single axis of acceleration, these archetypes and corresponding narratives exist along a relational spectrum and are permeable, allowing LAs to take on and switch between them according to context.","We model these relationships between the archetypes and their contributions to AI advancement using a causal loop diagram (CLD), and with those interactions argue that more nuanced ways of approaching AI might also open new modes of practice in the new digital economy."],"url":"http://arxiv.org/abs/2406.01421v1","category":"cs.AI"}
{"created":"2024-06-03 15:04:47","title":"Mixture of Rationale: Multi-Modal Reasoning Mixture for Visual Question Answering","abstract":"Zero-shot visual question answering (VQA) is a challenging task that requires reasoning across modalities. While some existing methods rely on a single rationale within the Chain of Thoughts (CoT) framework, they may fall short of capturing the complexity of the VQA problem. On the other hand, some other methods that use multiple rationales may still suffer from low diversity, poor modality alignment, and inefficient retrieval and fusion. In response to these challenges, we propose \\emph{Mixture of Rationales (MoR)}, a novel multi-modal reasoning method that mixes multiple rationales for VQA. MoR uses a single frozen Vision-and-Language Pre-trained Models (VLPM) model to {dynamically generate, retrieve and fuse multi-modal thoughts}. We evaluate MoR on two challenging VQA datasets, i.e. NLVR2 and OKVQA, with two representative backbones OFA and VL-T5. MoR achieves a 12.43\\% accuracy improvement on NLVR2, and a 2.45\\% accuracy improvement on OKVQA-S( the science and technology category of OKVQA).","sentences":["Zero-shot visual question answering (VQA) is a challenging task that requires reasoning across modalities.","While some existing methods rely on a single rationale within the Chain of Thoughts (CoT) framework, they may fall short of capturing the complexity of the VQA problem.","On the other hand, some other methods that use multiple rationales may still suffer from low diversity, poor modality alignment, and inefficient retrieval and fusion.","In response to these challenges, we propose \\emph{Mixture of Rationales (MoR)}, a novel multi-modal reasoning method that mixes multiple rationales for VQA.","MoR uses a single frozen Vision-and-Language Pre-trained Models (VLPM) model to {dynamically generate, retrieve and fuse multi-modal thoughts}.","We evaluate MoR on two challenging VQA datasets, i.e. NLVR2 and OKVQA, with two representative backbones OFA and VL-T5.","MoR achieves a 12.43\\% accuracy improvement on NLVR2, and a 2.45\\% accuracy improvement on OKVQA-S( the science and technology category of OKVQA)."],"url":"http://arxiv.org/abs/2406.01402v1","category":"cs.CV"}
{"created":"2024-06-03 14:58:49","title":"TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for Traversability Estimation","abstract":"This paper presents TE-NeXt, a novel and efficient architecture for Traversability Estimation (TE) from sparse LiDAR point clouds based on a residual convolution block. TE-NeXt block fuses notions of current trends such as attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate high capacity for generalisation in a variety of urban and natural environments, using well-known and accessible datasets such as SemanticKITTI, Rellis-3D and SemanticUSL. Thus, the designed architecture ouperforms state-of-the-art methods in the problem of semantic segmentation, demonstrating better results in unstructured environments and maintaining high reliability and robustness in urbans environments, which leads to better abstraction. Implementation is available in a open repository to the scientific community with the aim of ensuring the reproducibility of results.","sentences":["This paper presents TE-NeXt, a novel and efficient architecture for Traversability Estimation (TE) from sparse LiDAR point clouds based on a residual convolution block.","TE-NeXt block fuses notions of current trends such as attention mechanisms and 3D sparse convolutions.","TE-NeXt aims to demonstrate high capacity for generalisation in a variety of urban and natural environments, using well-known and accessible datasets such as SemanticKITTI, Rellis-3D and SemanticUSL.","Thus, the designed architecture ouperforms state-of-the-art methods in the problem of semantic segmentation, demonstrating better results in unstructured environments and maintaining high reliability and robustness in urbans environments, which leads to better abstraction.","Implementation is available in a open repository to the scientific community with the aim of ensuring the reproducibility of results."],"url":"http://arxiv.org/abs/2406.01395v1","category":"cs.CV"}
{"created":"2024-06-04 17:59:57","title":"VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors","abstract":"We present a stereo-matching method for depth estimation from high-resolution images using visual hulls as priors, and a memory-efficient technique for the correlation computation. Our method uses object masks extracted from supplementary views of the scene to guide the disparity estimation, effectively reducing the search space for matches. This approach is specifically tailored to stereo rigs in volumetric capture systems, where an accurate depth plays a key role in the downstream reconstruction task. To enable training and regression at high resolutions targeted by recent systems, our approach extends a sparse correlation computation into a hybrid sparse-dense scheme suitable for application in leading recurrent network architectures. We evaluate the performance-efficiency trade-off of our method compared to state-of-the-art methods, and demonstrate the efficacy of the visual hull guidance. In addition, we propose a training scheme for a further reduction of memory requirements during optimization, facilitating training on high-resolution data.","sentences":["We present a stereo-matching method for depth estimation from high-resolution images using visual hulls as priors, and a memory-efficient technique for the correlation computation.","Our method uses object masks extracted from supplementary views of the scene to guide the disparity estimation, effectively reducing the search space for matches.","This approach is specifically tailored to stereo rigs in volumetric capture systems, where an accurate depth plays a key role in the downstream reconstruction task.","To enable training and regression at high resolutions targeted by recent systems, our approach extends a sparse correlation computation into a hybrid sparse-dense scheme suitable for application in leading recurrent network architectures.","We evaluate the performance-efficiency trade-off of our method compared to state-of-the-art methods, and demonstrate the efficacy of the visual hull guidance.","In addition, we propose a training scheme for a further reduction of memory requirements during optimization, facilitating training on high-resolution data."],"url":"http://arxiv.org/abs/2406.02552v1","category":"cs.CV"}
{"created":"2024-06-04 17:59:45","title":"Local control and mixed dimensions: Exploring high-temperature superconductivity in optical lattices","abstract":"The simulation of high-temperature superconducting materials by implementing strongly correlated fermionic models in optical lattices is one of the major objectives in the field of analog quantum simulation. Here we show that local control and optical bilayer capabilities create a versatile toolbox to study both nickelate and cuprate high-temperature superconductors. On the one hand, we present a scheme to implement a mixed-dimensional (mixD) bilayer model that has been proposed to capture the essential pairing physics of pressurized bilayer nickelates. This allows for the long-sought realization of a state with long-range superconducting order in current lattice quantum simulation machines. In particular, we show how coherent pairing correlations can be accessed in a partially particle-hole transformed and rotated basis. On the other hand, we demonstrate that control of local gates enables the observation of $d$-wave pairing order in the two-dimensional (single-layer) repulsive Fermi-Hubbard model through the simulation of a system with attractive interactions. Lastly, we introduce a scheme to measure momentum-resolved dopant densities, providing access to observables complementary to solid-state experiments -- which is of particular interest for future studies of the enigmatic pseudogap phase appearing in cuprates.","sentences":["The simulation of high-temperature superconducting materials by implementing strongly correlated fermionic models in optical lattices is one of the major objectives in the field of analog quantum simulation.","Here we show that local control and optical bilayer capabilities create a versatile toolbox to study both nickelate and cuprate high-temperature superconductors.","On the one hand, we present a scheme to implement a mixed-dimensional (mixD) bilayer model that has been proposed to capture the essential pairing physics of pressurized bilayer nickelates.","This allows for the long-sought realization of a state with long-range superconducting order in current lattice quantum simulation machines.","In particular, we show how coherent pairing correlations can be accessed in a partially particle-hole transformed and rotated basis.","On the other hand, we demonstrate that control of local gates enables the observation of $d$-wave pairing order in the two-dimensional (single-layer) repulsive Fermi-Hubbard model through the simulation of a system with attractive interactions.","Lastly, we introduce a scheme to measure momentum-resolved dopant densities, providing access to observables complementary to solid-state experiments -- which is of particular interest for future studies of the enigmatic pseudogap phase appearing in cuprates."],"url":"http://arxiv.org/abs/2406.02551v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-04 17:58:32","title":"Asymmetry, Gap Opening and High Accretion Rate on DM Tau: A Hypothesis Based on Interaction of Magnetized Disk Wind with Planet","abstract":"Over two hundred protoplanetary disk systems have been resolved by ALMA, and the vast majority suggest the presence of planets. The dust gaps in transition disks are considered evidence of giant planets sculpting gas and dust under appropriate disk viscosity. However, the unusually high accretion rates in many T Tauri stars hosting transition disks challenge this theory. As the only disk currently observed with high turbulence, the high accretion rate ($\\sim10^{-8.3}M_{\\odot}/yr$) observed in DM Tau indicates the presence of strong turbulence may within the system. Considering the recent theoretical advancements in magnetized disk winds is challenging the traditional gap-opening theories and viscosity-driven accretion models, our study presents a pioneering simulation incorporating a simplified magnetized disk wind model to explain the observed features in DM Tau. Employing multi-fluid simulations with an embedded medium mass planet, we successfully replicate the gap formation and asymmetric structures evident in ALMA Band 6 and the recently JVLA 7 mm observations. Our results suggest that when magnetized disk wind dominate the accretion mode of the system, it's entirely possible for a planet with a medium mass to exist within the gap inside 20 au of DM Tau. This means that DM Tau may not be as turbulence as imagined. However, viscosity within the disk should also contribute a few turbulence to maintain disk stability.","sentences":["Over two hundred protoplanetary disk systems have been resolved by ALMA, and the vast majority suggest the presence of planets.","The dust gaps in transition disks are considered evidence of giant planets sculpting gas and dust under appropriate disk viscosity.","However, the unusually high accretion rates in many T Tauri stars hosting transition disks challenge this theory.","As the only disk currently observed with high turbulence, the high accretion rate ($\\sim10^{-8.3}M_{\\odot}/yr$) observed in DM Tau indicates the presence of strong turbulence may within the system.","Considering the recent theoretical advancements in magnetized disk winds is challenging the traditional gap-opening theories and viscosity-driven accretion models, our study presents a pioneering simulation incorporating a simplified magnetized disk wind model to explain the observed features in DM Tau.","Employing multi-fluid simulations with an embedded medium mass planet, we successfully replicate the gap formation and asymmetric structures evident in ALMA Band 6 and the recently JVLA 7 mm observations.","Our results suggest that when magnetized disk wind dominate the accretion mode of the system, it's entirely possible for a planet with a medium mass to exist within the gap inside 20 au of DM Tau.","This means that DM Tau may not be as turbulence as imagined.","However, viscosity within the disk should also contribute a few turbulence to maintain disk stability."],"url":"http://arxiv.org/abs/2406.02544v1","category":"astro-ph.EP"}
{"created":"2024-06-04 17:55:43","title":"TopViewRS: Vision-Language Models as Top-View Spatial Reasoners","abstract":"Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored. In this work, we thus study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.","sentences":["Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs).","Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored.","In this work, we thus study their capability to understand and reason over spatial relations from the top view.","The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions).","We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input.","We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity.","Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases.","Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited.","Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks."],"url":"http://arxiv.org/abs/2406.02537v1","category":"cs.CL"}
{"created":"2024-06-04 17:42:21","title":"CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks","abstract":"Large Language Models (LLMs) are revolutionizing various domains, yet verifying their answers remains a significant challenge, especially for intricate open-ended tasks such as consolidation, summarization, and extraction of knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and simple LLM verification approach. CheckEmbed is driven by a straightforward yet powerful idea: in order to compare LLM solutions to one another or to the ground-truth, compare their corresponding answer-level embeddings obtained with a model such as GPT Text Embedding Large. This reduces a complex textual answer to a single embedding, facilitating straightforward, fast, and meaningful verification. We develop a comprehensive verification pipeline implementing the CheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for assessing the truthfulness of the LLM answers, such as embedding heatmaps and their summaries. We show how to use these metrics for deploying practical engines that decide whether an LLM answer is satisfactory or not. We apply the pipeline to real-world document analysis tasks, including term extraction and document summarization, showcasing significant improvements in accuracy, cost-effectiveness, and runtime performance compared to existing token-, sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.","sentences":["Large Language Models (LLMs) are revolutionizing various domains, yet verifying their answers remains a significant challenge, especially for intricate open-ended tasks such as consolidation, summarization, and extraction of knowledge.","In this work, we propose CheckEmbed: an accurate, scalable, and simple LLM verification approach.","CheckEmbed is driven by a straightforward yet powerful idea: in order to compare LLM solutions to one another or to the ground-truth, compare their corresponding answer-level embeddings obtained with a model such as GPT Text Embedding Large.","This reduces a complex textual answer to a single embedding, facilitating straightforward, fast, and meaningful verification.","We develop a comprehensive verification pipeline implementing the CheckEmbed methodology.","The CheckEmbed pipeline also comes with metrics for assessing the truthfulness of the LLM answers, such as embedding heatmaps and their summaries.","We show how to use these metrics for deploying practical engines that decide whether an LLM answer is satisfactory or not.","We apply the pipeline to real-world document analysis tasks, including term extraction and document summarization, showcasing significant improvements in accuracy, cost-effectiveness, and runtime performance compared to existing token-, sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT."],"url":"http://arxiv.org/abs/2406.02524v1","category":"cs.CL"}
{"created":"2024-06-04 17:41:23","title":"Superconducting magic-angle twisted trilayer graphene hosts competing magnetic order and moir\u00e9 inhomogeneities","abstract":"The microscopic mechanism of superconductivity in the magic-angle twisted graphene family, including magic-angle twisted trilayer graphene (MATTG), is poorly understood. Properties of MATTG, like Pauli limit violation, suggest unconventional superconductivity. Theoretical studies propose proximal magnetic states in the phase diagram, but direct experimental evidence is lacking. We show direct evidence for an in-plane magnetic order proximal to the superconducting state using two complementary electrical transport measurements. First, we probe the superconducting phase by using statistically significant switching events from superconducting to the dissipative state of MATTG. The system behaves like a network of Josephson junctions due to lattice relaxation-induced moir\\'e inhomogeneity in the system. We observe non-monotonic and hysteretic responses in the switching distributions as a function of temperature and in-plane magnetic field. Second, in normal regions doped slightly away from the superconducting regime, we observe hysteresis in magnetoresistance with an in-plane magnetic field; showing evidence for in-plane magnetic order that vanishes $\\sim$900 mK. Additionally, we show a broadened Berezinskii-Kosterlitz-Thouless transition due to relaxation-induced moir\\'e inhomogeneity. We find superfluid stiffness $J_{\\mathrm{s}}$$\\sim$0.15 K with strong temperature dependence. Theoretically, the magnetic and superconducting order arising from the magnetic order's fluctuations have been proposed - we show direct evidence for both. Our observation that the hysteretic magnetoresistance is sensitive to the in-plane field may constrain possible intervalley-coherent magnetic orders and the resulting superconductivity that arises from its fluctuations.","sentences":["The microscopic mechanism of superconductivity in the magic-angle twisted graphene family, including magic-angle twisted trilayer graphene (MATTG), is poorly understood.","Properties of MATTG, like Pauli limit violation, suggest unconventional superconductivity.","Theoretical studies propose proximal magnetic states in the phase diagram, but direct experimental evidence is lacking.","We show direct evidence for an in-plane magnetic order proximal to the superconducting state using two complementary electrical transport measurements.","First, we probe the superconducting phase by using statistically significant switching events from superconducting to the dissipative state of MATTG.","The system behaves like a network of Josephson junctions due to lattice relaxation-induced moir\\'e inhomogeneity in the system.","We observe non-monotonic and hysteretic responses in the switching distributions as a function of temperature and in-plane magnetic field.","Second, in normal regions doped slightly away from the superconducting regime, we observe hysteresis in magnetoresistance with an in-plane magnetic field; showing evidence for in-plane magnetic order that vanishes $\\sim$900 mK. Additionally, we show a broadened Berezinskii-Kosterlitz-Thouless transition due to relaxation-induced moir\\'e inhomogeneity.","We find superfluid stiffness $J_{\\mathrm{s}}$$\\sim$0.15 K with strong temperature dependence.","Theoretically, the magnetic and superconducting order arising from the magnetic order's fluctuations have been proposed - we show direct evidence for both.","Our observation that the hysteretic magnetoresistance is sensitive to the in-plane field may constrain possible intervalley-coherent magnetic orders and the resulting superconductivity that arises from its fluctuations."],"url":"http://arxiv.org/abs/2406.02521v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-04 17:38:24","title":"Uncertainty of Joint Neural Contextual Bandit","abstract":"Contextual bandit learning is increasingly favored in modern large-scale recommendation systems. To better utlize the contextual information and available user or item features, the integration of neural networks have been introduced to enhance contextual bandit learning and has triggered significant interest from both academia and industry. However, a major challenge arises when implementing a disjoint neural contextual bandit solution in large-scale recommendation systems, where each item or user may correspond to a separate bandit arm. The huge number of items to recommend poses a significant hurdle for real world production deployment. This paper focuses on a joint neural contextual bandit solution which serves all recommending items in one single model. The output consists of a predicted reward $\\mu$, an uncertainty $\\sigma$ and a hyper-parameter $\\alpha$ which balances exploitation and exploration, e.g., $\\mu + \\alpha \\sigma$.   The tuning of the parameter $\\alpha$ is typically heuristic and complex in practice due to its stochastic nature. To address this challenge, we provide both theoretical analysis and experimental findings regarding the uncertainty $\\sigma$ of the joint neural contextual bandit model. Our analysis reveals that $\\alpha$ demonstrates an approximate square root relationship with the size of the last hidden layer $F$ and inverse square root relationship with the amount of training data $N$, i.e., $\\sigma \\propto \\sqrt{\\frac{F}{N}}$. The experiments, conducted with real industrial data, align with the theoretical analysis, help understanding model behaviors and assist the hyper-parameter tuning during both offline training and online deployment.","sentences":["Contextual bandit learning is increasingly favored in modern large-scale recommendation systems.","To better utlize the contextual information and available user or item features, the integration of neural networks have been introduced to enhance contextual bandit learning and has triggered significant interest from both academia and industry.","However, a major challenge arises when implementing a disjoint neural contextual bandit solution in large-scale recommendation systems, where each item or user may correspond to a separate bandit arm.","The huge number of items to recommend poses a significant hurdle for real world production deployment.","This paper focuses on a joint neural contextual bandit solution which serves all recommending items in one single model.","The output consists of a predicted reward $\\mu$, an uncertainty $\\sigma$ and a hyper-parameter $\\alpha$ which balances exploitation and exploration, e.g., $\\mu + \\alpha \\sigma$.   The tuning of the parameter $\\alpha$ is typically heuristic and complex in practice due to its stochastic nature.","To address this challenge, we provide both theoretical analysis and experimental findings regarding the uncertainty $\\sigma$ of the joint neural contextual bandit model.","Our analysis reveals that $\\alpha$ demonstrates an approximate square root relationship with the size of the last hidden layer $F$ and inverse square root relationship with the amount of training data $N$,","i.e., $\\sigma \\propto \\sqrt{\\frac{F}{N}}$. The experiments, conducted with real industrial data, align with the theoretical analysis, help understanding model behaviors and assist the hyper-parameter tuning during both offline training and online deployment."],"url":"http://arxiv.org/abs/2406.02515v1","category":"cs.LG"}
{"created":"2024-06-04 17:35:20","title":"Transient infrared nanoscopy resolves the millisecond photoswitching dynamics of single lipid vesicles in water","abstract":"Understanding the biophysical and biochemical properties of molecular nanocarriers under physiological conditions and with minimal interference is crucial for advancing nanomedicine, photopharmacology, drug delivery, nanotheranostics and synthetic biology. Yet, analytical methods struggle to combine precise chemical imaging and measurements without perturbative labeling. This challenge is exemplified for azobenzene-based photoswitchable lipids, which are intriguing reagents for controlling nanocarrier properties on fast timescales, enabling, e.g., precise light-induced drug release processes. Here, we leverage the chemical recognition and high spatio-temporal resolution of scattering-type scanning near-field optical microscopy (s-SNOM) to demonstrate non-destructive, label-free mid-infrared (MIR) imaging and spectroscopy of photoswitchable liposomes below the diffraction limit and the tracking of their dynamics down to 50 ms resolution. The vesicles are adsorbed on an ultrathin 10-nm SiN membrane, which separates the sample space from the tip space for stable and hour-long observations. By implementing a transient nanoscopy approach, we accurately resolve, for the first time, photoinduced changes in both the shape and the MIR spectral signature of individual vesicles and reveal abrupt change dynamics of the underlying photoisomerization process. Our findings highlight the methods potential for future studies on the complex dynamics of unlabeled nanoscale soft matter, as well as, in a broader context, for host-guest systems, energy materials or drugs.","sentences":["Understanding the biophysical and biochemical properties of molecular nanocarriers under physiological conditions and with minimal interference is crucial for advancing nanomedicine, photopharmacology, drug delivery, nanotheranostics and synthetic biology.","Yet, analytical methods struggle to combine precise chemical imaging and measurements without perturbative labeling.","This challenge is exemplified for azobenzene-based photoswitchable lipids, which are intriguing reagents for controlling nanocarrier properties on fast timescales, enabling, e.g., precise light-induced drug release processes.","Here, we leverage the chemical recognition and high spatio-temporal resolution of scattering-type scanning near-field optical microscopy (s-SNOM) to demonstrate non-destructive, label-free mid-infrared (MIR) imaging and spectroscopy of photoswitchable liposomes below the diffraction limit and the tracking of their dynamics down to 50 ms resolution.","The vesicles are adsorbed on an ultrathin 10-nm SiN membrane, which separates the sample space from the tip space for stable and hour-long observations.","By implementing a transient nanoscopy approach, we accurately resolve, for the first time, photoinduced changes in both the shape and the MIR spectral signature of individual vesicles and reveal abrupt change dynamics of the underlying photoisomerization process.","Our findings highlight the methods potential for future studies on the complex dynamics of unlabeled nanoscale soft matter, as well as, in a broader context, for host-guest systems, energy materials or drugs."],"url":"http://arxiv.org/abs/2406.02513v1","category":"physics.optics"}
{"created":"2024-06-04 17:34:43","title":"Existence, Uniqueness and Asymptotic Dynamics of Nonlinear Schr\u00f6dinger Equations With Quasi-Periodic Initial Data: II. The Derivative NLS","abstract":"This is the second part of a two-paper series studying the nonlinear Schr\\\"odinger equation with quasi-periodic initial data. In this paper, we focus on the quasi-periodic Cauchy problem for the derivative nonlinear Schr\\\"odinger equation. Under the assumption that the Fourier coefficients of the initial data obey an exponential upper bound, we establish local existence of a solution that retains quasi-periodicity in space with a slightly weaker Fourier decay. Moreover, the solution is shown to be unique within this class of quasi-periodic functions. Also, we prove that, for the derivative nonlinear Schr\\\"odinger equation in a weakly nonlinear setting, within the time scale, as the small parameter of nonlinearity tends to zero, the nonlinear solution converges asymptotically to the linear solution in the sense of both sup-norm and analytic Sobolev-norm.   The proof proceeds via a consideration of an associated infinite system of coupled ordinary differential equations for the Fourier coefficients and an explicit combinatorial analysis for the Picard iteration with the help of Feynman diagrams and the power of $\\ast^{[\\cdot]}$ labelling the complex conjugate.","sentences":["This is the second part of a two-paper series studying the nonlinear Schr\\\"odinger equation with quasi-periodic initial data.","In this paper, we focus on the quasi-periodic Cauchy problem for the derivative nonlinear Schr\\\"odinger equation.","Under the assumption that the Fourier coefficients of the initial data obey an exponential upper bound, we establish local existence of a solution that retains quasi-periodicity in space with a slightly weaker Fourier decay.","Moreover, the solution is shown to be unique within this class of quasi-periodic functions.","Also, we prove that, for the derivative nonlinear Schr\\\"odinger equation in a weakly nonlinear setting, within the time scale, as the small parameter of nonlinearity tends to zero, the nonlinear solution converges asymptotically to the linear solution in the sense of both sup-norm and analytic Sobolev-norm.   ","The proof proceeds via a consideration of an associated infinite system of coupled ordinary differential equations for the Fourier coefficients and an explicit combinatorial analysis for the Picard iteration with the help of Feynman diagrams and the power of $\\ast^{[\\cdot]}$ labelling the complex conjugate."],"url":"http://arxiv.org/abs/2406.02512v1","category":"math.AP"}
{"created":"2024-06-04 16:56:51","title":"Investigating the Online Recruitment and Selection Journey of Novice Software Engineers: Anti-patterns and Recommendations","abstract":"[Context] The growing software development market has increased the demand for qualified professionals in Software Engineering (SE). To this end, companies must enhance their Recruitment and Selection (R&S) processes to maintain high quality teams, including opening opportunities for beginners, such as trainees and interns. However, given the various judgments and sociotechnical factors involved, this complex process of R&S poses a challenge for recent graduates seeking to enter the market. [Objective] This paper aims to identify a set of anti-patterns and recommendations for early career SE professionals concerning R&S processes. [Method] Under an exploratory and qualitative methodological approach, we conducted six online Focus Groups with 18 recruiters with experience in R&S in the software industry. [Results] After completing our qualitative analysis, we identified 12 anti-patterns and 31 actionable recommendations regarding the hiring process focused on entry level SE professionals. The identified anti-patterns encompass behavioral and technical dimensions innate to R&S processes. [Conclusion] These findings provide a rich opportunity for reflection in the SE industry and offer valuable guidance for early-career candidates and organizations. From an academic perspective, this work also raises awareness of the intersection of Human Resources and SE, an area with considerable potential to be expanded in the context of cooperative and human aspects of SE.","sentences":["[Context] The growing software development market has increased the demand for qualified professionals in Software Engineering (SE).","To this end, companies must enhance their Recruitment and Selection (R&S) processes to maintain high quality teams, including opening opportunities for beginners, such as trainees and interns.","However, given the various judgments and sociotechnical factors involved, this complex process of R&S poses a challenge for recent graduates seeking to enter the market.","[Objective]","This paper aims to identify a set of anti-patterns and recommendations for early career SE professionals concerning R&S processes.","[Method] Under an exploratory and qualitative methodological approach, we conducted six online Focus Groups with 18 recruiters with experience in R&S in the software industry.","[Results] After completing our qualitative analysis, we identified 12 anti-patterns and 31 actionable recommendations regarding the hiring process focused on entry level SE professionals.","The identified anti-patterns encompass behavioral and technical dimensions innate to R&S processes.","[Conclusion] These findings provide a rich opportunity for reflection in the SE industry and offer valuable guidance for early-career candidates and organizations.","From an academic perspective, this work also raises awareness of the intersection of Human Resources and SE, an area with considerable potential to be expanded in the context of cooperative and human aspects of SE."],"url":"http://arxiv.org/abs/2406.02487v1","category":"cs.SE"}
{"created":"2024-06-04 16:55:42","title":"A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting","abstract":"Capturing complex temporal patterns and relationships within multivariate data streams is a difficult task. We propose the Temporal Kolmogorov-Arnold Transformer (TKAT), a novel attention-based architecture designed to address this task using Temporal Kolmogorov-Arnold Networks (TKANs). Inspired by the Temporal Fusion Transformer (TFT), TKAT emerges as a powerful encoder-decoder model tailored to handle tasks in which the observed part of the features is more important than the a priori known part. This new architecture combined the theoretical foundation of the Kolmogorov-Arnold representation with the power of transformers. TKAT aims to simplify the complex dependencies inherent in time series, making them more \"interpretable\". The use of transformer architecture in this framework allows us to capture long-range dependencies through self-attention mechanisms.","sentences":["Capturing complex temporal patterns and relationships within multivariate data streams is a difficult task.","We propose the Temporal Kolmogorov-Arnold Transformer (TKAT), a novel attention-based architecture designed to address this task using Temporal Kolmogorov-Arnold Networks (TKANs).","Inspired by the Temporal Fusion Transformer (TFT), TKAT emerges as a powerful encoder-decoder model tailored to handle tasks in which the observed part of the features is more important than the a priori known part.","This new architecture combined the theoretical foundation of the Kolmogorov-Arnold representation with the power of transformers.","TKAT aims to simplify the complex dependencies inherent in time series, making them more \"interpretable\".","The use of transformer architecture in this framework allows us to capture long-range dependencies through self-attention mechanisms."],"url":"http://arxiv.org/abs/2406.02486v1","category":"cs.LG"}
{"created":"2024-06-04 16:35:02","title":"Bi-Filar Coil Winding for Fast Quench Protection","abstract":"As superconducting magnet technology is pushed towards higher performance, energy density and total stored energy follow exponentially. Protecting magnets becomes substantially more challenging with traditional methods being stretched to their limits. New technologies such as CLIQ (Coupling Loss Induced Quench) promise to provide a robust method to protect advanced magnets, however they become inductance limited in large magnet strings or at low field, leading to more complex configurations. A technique to substantially reduce this limitation and improve response time is presented, by winding coils in a bifilar fashion and connecting them in series for typical operation, while providing an anti-parallel connection for quasi-zero-inductance in a protection case. This allows for extremely high di/dt. The concept is then demonstrated on a small REBCO coil.","sentences":["As superconducting magnet technology is pushed towards higher performance, energy density and total stored energy follow exponentially.","Protecting magnets becomes substantially more challenging with traditional methods being stretched to their limits.","New technologies such as CLIQ (Coupling Loss Induced Quench) promise to provide a robust method to protect advanced magnets, however they become inductance limited in large magnet strings or at low field, leading to more complex configurations.","A technique to substantially reduce this limitation and improve response time is presented, by winding coils in a bifilar fashion and connecting them in series for typical operation, while providing an anti-parallel connection for quasi-zero-inductance in a protection case.","This allows for extremely high di/dt.","The concept is then demonstrated on a small REBCO coil."],"url":"http://arxiv.org/abs/2406.02467v1","category":"physics.acc-ph"}
{"created":"2024-06-04 16:25:03","title":"Modified scattering for the three dimensional Maxwell-Dirac system","abstract":"In this work we prove global well-posedness for the massive Maxwell-Dirac system in the Lorenz gauge in $\\mathbb{R}^{1+3}$, for small, sufficiently smooth and decaying initial data, as well as modified scattering for the solutions. Heuristically we exploit the close connection between the massive Maxwell-Dirac and the wave-Klein-Gordon equations, while developing a novel approach which applies directly at the level of the Dirac equations. The modified scattering result follows from a precise description of the asymptotic behavior of the solutions inside the light cone, which we derive via the method of testing with wave packets of Ifrim-Tataru.","sentences":["In this work we prove global well-posedness for the massive Maxwell-Dirac system in the Lorenz gauge in $\\mathbb{R}^{1+3}$, for small, sufficiently smooth and decaying initial data, as well as modified scattering for the solutions.","Heuristically we exploit the close connection between the massive Maxwell-Dirac and the wave-Klein-Gordon equations, while developing a novel approach which applies directly at the level of the Dirac equations.","The modified scattering result follows from a precise description of the asymptotic behavior of the solutions inside the light cone, which we derive via the method of testing with wave packets of Ifrim-Tataru."],"url":"http://arxiv.org/abs/2406.02460v1","category":"math.AP"}
{"created":"2024-06-04 16:21:24","title":"Machine learning Hubbard parameters with equivariant neural networks","abstract":"Density-functional theory with extended Hubbard functionals (DFT+$U$+$V$) provides a robust framework to accurately describe complex materials containing transition-metal or rare-earth elements. It does so by mitigating self-interaction errors inherent to semi-local functionals which are particularly pronounced in systems with partially-filled $d$ and $f$ electronic states. However, achieving accuracy in this approach hinges upon the accurate determination of the on-site $U$ and inter-site $V$ Hubbard parameters. In practice, these are obtained either by semi-empirical tuning, requiring prior knowledge, or, more correctly, by using predictive but expensive first-principles calculations. Here, we present a machine learning model based on equivariant neural networks which uses atomic occupation matrices as descriptors, directly capturing the electronic structure, local chemical environment, and oxidation states of the system at hand. We target here the prediction of Hubbard parameters computed self-consistently with iterative linear-response calculations, as implemented in density-functional perturbation theory (DFPT), and structural relaxations. Remarkably, when trained on data from 11 materials spanning various crystal structures and compositions, our model achieves mean absolute relative errors of 3% and 5% for Hubbard $U$ and $V$ parameters, respectively. By circumventing computationally expensive DFT or DFPT self-consistent protocols, our model significantly expedites the prediction of Hubbard parameters with negligible computational overhead, while approaching the accuracy of DFPT. Moreover, owing to its robust transferability, the model facilitates accelerated materials discovery and design via high-throughput calculations, with relevance for various technological applications.","sentences":["Density-functional theory with extended Hubbard functionals (DFT+$U$+$V$) provides a robust framework to accurately describe complex materials containing transition-metal or rare-earth elements.","It does so by mitigating self-interaction errors inherent to semi-local functionals which are particularly pronounced in systems with partially-filled $d$ and $f$ electronic states.","However, achieving accuracy in this approach hinges upon the accurate determination of the on-site $U$ and inter-site $V$ Hubbard parameters.","In practice, these are obtained either by semi-empirical tuning, requiring prior knowledge, or, more correctly, by using predictive but expensive first-principles calculations.","Here, we present a machine learning model based on equivariant neural networks which uses atomic occupation matrices as descriptors, directly capturing the electronic structure, local chemical environment, and oxidation states of the system at hand.","We target here the prediction of Hubbard parameters computed self-consistently with iterative linear-response calculations, as implemented in density-functional perturbation theory (DFPT), and structural relaxations.","Remarkably, when trained on data from 11 materials spanning various crystal structures and compositions, our model achieves mean absolute relative errors of 3% and 5% for Hubbard $U$ and $V$ parameters, respectively.","By circumventing computationally expensive DFT or DFPT self-consistent protocols, our model significantly expedites the prediction of Hubbard parameters with negligible computational overhead, while approaching the accuracy of DFPT.","Moreover, owing to its robust transferability, the model facilitates accelerated materials discovery and design via high-throughput calculations, with relevance for various technological applications."],"url":"http://arxiv.org/abs/2406.02457v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 16:16:58","title":"Quantum states from normalizing flows","abstract":"We introduce an architecture for neural quantum states for many-body quantum-mechanical systems, based on normalizing flows. The use of normalizing flows enables efficient uncorrelated sampling of configurations from the probability distribution defined by the wavefunction, mitigating a major cost of using neural states in simulation. We demonstrate the use of this architecture for both ground-state preparation (for self-interacting particles in a harmonic trap) and real-time evolution (for one-dimensional tunneling). Finally, we detail a procedure for obtaining rigorous estimates of the systematic error when using neural states to approximate quantum evolution.","sentences":["We introduce an architecture for neural quantum states for many-body quantum-mechanical systems, based on normalizing flows.","The use of normalizing flows enables efficient uncorrelated sampling of configurations from the probability distribution defined by the wavefunction, mitigating a major cost of using neural states in simulation.","We demonstrate the use of this architecture for both ground-state preparation (for self-interacting particles in a harmonic trap) and real-time evolution (for one-dimensional tunneling).","Finally, we detail a procedure for obtaining rigorous estimates of the systematic error when using neural states to approximate quantum evolution."],"url":"http://arxiv.org/abs/2406.02451v1","category":"quant-ph"}
{"created":"2024-06-04 16:07:26","title":"Noise-adapted qudit codes for amplitude-damping noise","abstract":"Quantum error correction (QEC) plays a critical role in preventing information loss in quantum systems and provides a framework for reliable quantum computation. Identifying quantum codes with nice code parameters for physically motivated noise models remains an interesting challenge. Going beyond qubit codes, here we propose a class of qudit error correcting codes tailored to protect against amplitude-damping noise. Specifically, we construct a class of four-qudit codes that satisfies the error correction conditions for all single-qudit and a few two-qudit damping errors up to the leading order in the damping parameter $\\gamma$. We devise a protocol to extract syndromes that identify this set of errors unambiguously, leading to a noise-adapted recovery scheme that achieves a fidelity loss of $\\cO(\\gamma^{2})$. For the $d=2$ case, our QEC scheme is identical to the known example of the $4$-qubit code and the associated syndrome-based recovery.   We also assess the performance of our class of codes using the Petz recovery map and note some interesting deviations from the qubit case.","sentences":["Quantum error correction (QEC) plays a critical role in preventing information loss in quantum systems and provides a framework for reliable quantum computation.","Identifying quantum codes with nice code parameters for physically motivated noise models remains an interesting challenge.","Going beyond qubit codes, here we propose a class of qudit error correcting codes tailored to protect against amplitude-damping noise.","Specifically, we construct a class of four-qudit codes that satisfies the error correction conditions for all single-qudit and a few two-qudit damping errors up to the leading order in the damping parameter $\\gamma$.","We devise a protocol to extract syndromes that identify this set of errors unambiguously, leading to a noise-adapted recovery scheme that achieves a fidelity loss of $\\cO(\\gamma^{2})$. For the $d=2$ case, our QEC scheme is identical to the known example of the $4$-qubit code and the associated syndrome-based recovery.   ","We also assess the performance of our class of codes using the Petz recovery map and note some interesting deviations from the qubit case."],"url":"http://arxiv.org/abs/2406.02444v1","category":"quant-ph"}
{"created":"2024-06-04 16:01:57","title":"Simplicial complexes and matroids with vanishing $T^2$","abstract":"We investigate quotients by radical monomial ideals for which $T^2$, the second cotangent cohomology module, vanishes. The dimension of the graded components of $T^2$, and thus their vanishing, depends only on the combinatorics of the corresponding simplicial complex. We give both a complete characterization and a full list of one dimensional complexes with $T^2=0$. We characterize the graded components of $T^2$ when the simplicial complex is a uniform matroid. Finally, we show that $T^2$ vanishes for all matroids of corank at most two and conjecture that all connected matroids with vanishing $T^2$ are of corank at most two.","sentences":["We investigate quotients by radical monomial ideals for which $T^2$, the second cotangent cohomology module, vanishes.","The dimension of the graded components of $T^2$, and thus their vanishing, depends only on the combinatorics of the corresponding simplicial complex.","We give both a complete characterization and a full list of one dimensional complexes with $T^2=0$.","We characterize the graded components of $T^2$ when the simplicial complex is a uniform matroid.","Finally, we show that $T^2$ vanishes for all matroids of corank at most two and conjecture that all connected matroids with vanishing $T^2$ are of corank at most two."],"url":"http://arxiv.org/abs/2406.02440v1","category":"math.CO"}
{"created":"2024-06-04 15:58:14","title":"Out-of-Distribution Runtime Adaptation with Conformalized Neural Network Ensembles","abstract":"We present a method to integrate real-time out-of-distribution (OOD) detection for neural network trajectory predictors, and to adapt the control strategy of a robot (e.g., a self-driving car or drone) to preserve safety while operating in OOD regimes. Specifically, we use a neural network ensemble to predict the trajectory for a dynamic obstacle (such as a pedestrian), and use the maximum singular value of the empirical covariance among the ensemble as a signal for OOD detection. We calibrate this signal with a small fraction of held-out training data using the methodology of conformal prediction, to derive an OOD detector with probabilistic guarantees on the false-positive rate of the detector, given a user-specified confidence level. During in-distribution operation, we use an MPC controller to avoid collisions with the obstacle based on the trajectory predicted by the neural network ensemble. When OOD conditions are detected, we switch to a reachability-based controller to guarantee safety under the worst-case actions of the obstacle. We verify our method in extensive autonomous driving simulations in a pedestrian crossing scenario, showing that our OOD detector obtains the desired accuracy rate within a theoretically-predicted range. We also demonstrate the effectiveness of our method with real pedestrian data. We show improved safety and less conservatism in comparison with two state-of-the-art methods that also use conformal prediction, but without OOD adaptation.","sentences":["We present a method to integrate real-time out-of-distribution (OOD) detection for neural network trajectory predictors, and to adapt the control strategy of a robot (e.g., a self-driving car or drone) to preserve safety while operating in OOD regimes.","Specifically, we use a neural network ensemble to predict the trajectory for a dynamic obstacle (such as a pedestrian), and use the maximum singular value of the empirical covariance among the ensemble as a signal for OOD detection.","We calibrate this signal with a small fraction of held-out training data using the methodology of conformal prediction, to derive an OOD detector with probabilistic guarantees on the false-positive rate of the detector, given a user-specified confidence level.","During in-distribution operation, we use an MPC controller to avoid collisions with the obstacle based on the trajectory predicted by the neural network ensemble.","When OOD conditions are detected, we switch to a reachability-based controller to guarantee safety under the worst-case actions of the obstacle.","We verify our method in extensive autonomous driving simulations in a pedestrian crossing scenario, showing that our OOD detector obtains the desired accuracy rate within a theoretically-predicted range.","We also demonstrate the effectiveness of our method with real pedestrian data.","We show improved safety and less conservatism in comparison with two state-of-the-art methods that also use conformal prediction, but without OOD adaptation."],"url":"http://arxiv.org/abs/2406.02436v1","category":"cs.RO"}
{"created":"2024-06-04 15:50:35","title":"Reweighted Solutions for Weighted Low Rank Approximation","abstract":"Weighted low rank approximation (WLRA) is an important yet computationally challenging primitive with applications ranging from statistical analysis, model compression, and signal processing. To cope with the NP-hardness of this problem, prior work considers heuristics, bicriteria, or fixed parameter tractable algorithms to solve this problem. In this work, we introduce a new relaxed solution to WLRA which outputs a matrix that is not necessarily low rank, but can be stored using very few parameters and gives provable approximation guarantees when the weight matrix has low rank. Our central idea is to use the weight matrix itself to reweight a low rank solution, which gives an extremely simple algorithm with remarkable empirical performance in applications to model compression and on synthetic datasets. Our algorithm also gives nearly optimal communication complexity bounds for a natural distributed problem associated with this problem, for which we show matching communication lower bounds. Together, our communication complexity bounds show that the rank of the weight matrix provably parameterizes the communication complexity of WLRA. We also obtain the first relative error guarantees for feature selection with a weighted objective.","sentences":["Weighted low rank approximation (WLRA) is an important yet computationally challenging primitive with applications ranging from statistical analysis, model compression, and signal processing.","To cope with the NP-hardness of this problem, prior work considers heuristics, bicriteria, or fixed parameter tractable algorithms to solve this problem.","In this work, we introduce a new relaxed solution to WLRA which outputs a matrix that is not necessarily low rank, but can be stored using very few parameters and gives provable approximation guarantees when the weight matrix has low rank.","Our central idea is to use the weight matrix itself to reweight a low rank solution, which gives an extremely simple algorithm with remarkable empirical performance in applications to model compression and on synthetic datasets.","Our algorithm also gives nearly optimal communication complexity bounds for a natural distributed problem associated with this problem, for which we show matching communication lower bounds.","Together, our communication complexity bounds show that the rank of the weight matrix provably parameterizes the communication complexity of WLRA.","We also obtain the first relative error guarantees for feature selection with a weighted objective."],"url":"http://arxiv.org/abs/2406.02431v1","category":"cs.DS"}
{"created":"2024-06-04 15:47:03","title":"Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental Learning","abstract":"Class-incremental learning (CIL) aims to train a model to learn new classes from non-stationary data streams without forgetting old ones. In this paper, we propose a new kind of connectionist model by tailoring neural unit dynamics that adapt the behavior of neural networks for CIL. In each training session, it introduces a supervisory mechanism to guide network expansion whose growth size is compactly commensurate with the intrinsic complexity of a newly arriving task. This constructs a near-minimal network while allowing the model to expand its capacity when cannot sufficiently hold new classes. At inference time, it automatically reactivates the required neural units to retrieve knowledge and leaves the remaining inactivated to prevent interference. We name our model AutoActivator, which is effective and scalable. To gain insights into the neural unit dynamics, we theoretically analyze the model's convergence property via a universal approximation theorem on learning sequential mappings, which is under-explored in the CIL community. Experiments show that our method achieves strong CIL performance in rehearsal-free and minimal-expansion settings with different backbones.","sentences":["Class-incremental learning (CIL) aims to train a model to learn new classes from non-stationary data streams without forgetting old ones.","In this paper, we propose a new kind of connectionist model by tailoring neural unit dynamics that adapt the behavior of neural networks for CIL.","In each training session, it introduces a supervisory mechanism to guide network expansion whose growth size is compactly commensurate with the intrinsic complexity of a newly arriving task.","This constructs a near-minimal network while allowing the model to expand its capacity when cannot sufficiently hold new classes.","At inference time, it automatically reactivates the required neural units to retrieve knowledge and leaves the remaining inactivated to prevent interference.","We name our model AutoActivator, which is effective and scalable.","To gain insights into the neural unit dynamics, we theoretically analyze the model's convergence property via a universal approximation theorem on learning sequential mappings, which is under-explored in the CIL community.","Experiments show that our method achieves strong CIL performance in rehearsal-free and minimal-expansion settings with different backbones."],"url":"http://arxiv.org/abs/2406.02428v1","category":"cs.LG"}
{"created":"2024-06-04 15:41:54","title":"Periodically modulated solitary waves of the CH-KP-I equation","abstract":"We consider the CH-KP-I equation. For this equation we prove the existence of steady solutions, which are solitary in one horizontal direction and periodic in the other. We show that such waves bifurcate from the line solitary wave solutions, i.e. solitary wave solutions to the Camassa-Holm equation, in a dimension-breaking bifurcation. This is achieved through reformulating the problem as a dynamical system for a perturbation of the line solitary wave solutions, where the periodic direction takes the role of time, then applying the Lyapunov-Iooss theorem.","sentences":["We consider the CH-KP-I equation.","For this equation we prove the existence of steady solutions, which are solitary in one horizontal direction and periodic in the other.","We show that such waves bifurcate from the line solitary wave solutions, i.e. solitary wave solutions to the Camassa-Holm equation, in a dimension-breaking bifurcation.","This is achieved through reformulating the problem as a dynamical system for a perturbation of the line solitary wave solutions, where the periodic direction takes the role of time, then applying the Lyapunov-Iooss theorem."],"url":"http://arxiv.org/abs/2406.02423v1","category":"math.AP"}
{"created":"2024-06-04 15:23:29","title":"Accelerated Variance-Reduced Forward-Reflected Methods for Root-Finding Problems","abstract":"We propose a novel class of Nesterov's stochastic accelerated forward-reflected-based methods with variance reduction to solve root-finding problems under $\\frac{1}{L}$-co-coerciveness. Our algorithm is single-loop and leverages a new family of unbiased variance-reduced estimators specifically designed for root-finding problems. It achieves both $\\mathcal{O}(L^2/k^2)$ and $o(1/k^2)$-last-iterate convergence rates in terms of expected operator squared norm, where $k$ denotes the iteration counter. We instantiate our framework for two prominent estimators: SVRG and SAGA. By an appropriate choice of parameters, both variants attain an oracle complexity of $\\mathcal{O}( n + Ln^{2/3}\\epsilon^{-1})$ to reach an $\\epsilon$-solution, where $n$ represents the number of summands in the finite-sum operator. Furthermore, under $\\mu$-strong quasi-monotonicity, our method achieves a linear convergence rate and an oracle complexity of $\\mathcal{O}(n+ \\kappa n^{2/3}\\log(\\epsilon^{-1}))$, where $\\kappa := \\frac{L}{\\mu}$. We extend our approach to solve a class of finite-sum monotone inclusions, demonstrating that our schemes retain the same theoretical guarantees as in the equation setting. Finally, numerical experiments validate our algorithms and demonstrate their promising performance compared to state-of-the-art methods.","sentences":["We propose a novel class of Nesterov's stochastic accelerated forward-reflected-based methods with variance reduction to solve root-finding problems under $\\frac{1}{L}$-co-coerciveness.","Our algorithm is single-loop and leverages a new family of unbiased variance-reduced estimators specifically designed for root-finding problems.","It achieves both $\\mathcal{O}(L^2/k^2)$ and $o(1/k^2)$-last-iterate convergence rates in terms of expected operator squared norm, where $k$ denotes the iteration counter.","We instantiate our framework for two prominent estimators: SVRG and SAGA.","By an appropriate choice of parameters, both variants attain an oracle complexity of $\\mathcal{O}( n + Ln^{2/3}\\epsilon^{-1})$ to reach an $\\epsilon$-solution, where $n$ represents the number of summands in the finite-sum operator.","Furthermore, under $\\mu$-strong quasi-monotonicity, our method achieves a linear convergence rate and an oracle complexity of $\\mathcal{O}(n+ \\kappa n^{2/3}\\log(\\epsilon^{-1}))$, where $\\kappa := \\frac{L}{\\mu}$.","We extend our approach to solve a class of finite-sum monotone inclusions, demonstrating that our schemes retain the same theoretical guarantees as in the equation setting.","Finally, numerical experiments validate our algorithms and demonstrate their promising performance compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.02413v1","category":"math.OC"}
{"created":"2024-06-04 15:19:29","title":"Optimization of Rate-Splitting Multiple Access with Integrated Sensing and Backscatter Communication","abstract":"An integrated sensing and backscatter communication (ISABC) system is introduced herein. This system features a full-duplex (FD) base station (BS) that seamlessly merges sensing with backscatter communication and supports multiple users. Multiple access (MA) for the user is provided by employing rate-splitting multiple access (RSMA). RSMA, unlike other classical orthogonal and non-orthogonal MA schemes, splits messages into common and private streams. With RSMA, the set of common rate forms can be optimized to reduce interference. Optimized formulas are thus derived for communication rates for users, tags, and the BS's sensing rate, with the primary goal of enhancing the transmission efficiency of the BS. The optimization task involves minimizing the BS's overall transmission power by jointly optimizing the BS's beamforming vectors, the tag reflection coefficients, and user common rates. The alternating optimization method is employed to address this challenge. Concrete solutions are provided for the received beamformers, and semi-definite relaxation and slack-optimization techniques are adopted for transmit beamformers and reflection coefficients, respectively. For example, the proposed RSMA-assisted ISABC system achieves a 350% communication rate boost over a nonorthogonal multiple access-assisted ISABC, with only a 24% increase in transmit power, leveraging ten transmit/reception antennas at the BS.","sentences":["An integrated sensing and backscatter communication (ISABC) system is introduced herein.","This system features a full-duplex (FD) base station (BS) that seamlessly merges sensing with backscatter communication and supports multiple users.","Multiple access (MA) for the user is provided by employing rate-splitting multiple access (RSMA).","RSMA, unlike other classical orthogonal and non-orthogonal MA schemes, splits messages into common and private streams.","With RSMA, the set of common rate forms can be optimized to reduce interference.","Optimized formulas are thus derived for communication rates for users, tags, and the BS's sensing rate, with the primary goal of enhancing the transmission efficiency of the BS.","The optimization task involves minimizing the BS's overall transmission power by jointly optimizing the BS's beamforming vectors, the tag reflection coefficients, and user common rates.","The alternating optimization method is employed to address this challenge.","Concrete solutions are provided for the received beamformers, and semi-definite relaxation and slack-optimization techniques are adopted for transmit beamformers and reflection coefficients, respectively.","For example, the proposed RSMA-assisted ISABC system achieves a 350% communication rate boost over a nonorthogonal multiple access-assisted ISABC, with only a 24% increase in transmit power, leveraging ten transmit/reception antennas at the BS."],"url":"http://arxiv.org/abs/2406.02410v1","category":"eess.SP"}
{"created":"2024-06-04 15:18:16","title":"Universal limiting behaviour of reaction-diffusion systems with conservation laws","abstract":"Making sense of complex inhomogeneous systems composed of many interacting species is a grand challenge that pervades basically all natural sciences. Phase separation and pattern formation in reaction-diffusion systems have largely been studied as two separate paradigms. Here we show that in reaction-diffusion systems composed of many species, the presence of a conservation law constrains the evolution of the conserved quantity to be governed by a Cahn-Hilliard-like equation. This establishes a direct link with the paradigm of coexistence and recent \"active\" field theories. Hence, even for complex many-species systems a dramatically simplified but accurate description emerges over coarse spatio-temporal scales. Using the nullcline (the line of homogeneous steady states) as the central motif, we develop a geometrical framework through endowing chemical space with a basis and suitable coordinates. This framework allows us to capture and understand the effect of eliminating fast non-conserved degrees of freedom, and to explicitly construct coefficients of the coarse field theory. We expect that the theory we develop here will be particularly relevant to advance our understanding of biomolecular condensates.","sentences":["Making sense of complex inhomogeneous systems composed of many interacting species is a grand challenge that pervades basically all natural sciences.","Phase separation and pattern formation in reaction-diffusion systems have largely been studied as two separate paradigms.","Here we show that in reaction-diffusion systems composed of many species, the presence of a conservation law constrains the evolution of the conserved quantity to be governed by a Cahn-Hilliard-like equation.","This establishes a direct link with the paradigm of coexistence and recent \"active\" field theories.","Hence, even for complex many-species systems a dramatically simplified but accurate description emerges over coarse spatio-temporal scales.","Using the nullcline (the line of homogeneous steady states) as the central motif, we develop a geometrical framework through endowing chemical space with a basis and suitable coordinates.","This framework allows us to capture and understand the effect of eliminating fast non-conserved degrees of freedom, and to explicitly construct coefficients of the coarse field theory.","We expect that the theory we develop here will be particularly relevant to advance our understanding of biomolecular condensates."],"url":"http://arxiv.org/abs/2406.02409v1","category":"cond-mat.soft"}
{"created":"2024-06-04 15:17:42","title":"Anomalous 4$f$ fine structure in TmSe$_{1-x}$Te$_x$ across the metal-insulator transition","abstract":"Hybridization between localized 4$f$ and itinerant 5$d$6$s$ states in heavy fermion compounds is a well-studied phenomenon and commonly captured by the paradigmatic Anderson model. However, the investigation of additional electronic interactions, beyond the standard Anderson model, has been limited, despite their predicted important role in the exotic quasiparticle formation in mixed-valence systems. We investigate the 4$f$ states in TmSe$_{1-x}$Te$_x$ throughout a semimetal-insulator phase transition, which drastically varies the interactions related to the 4$f$ states. Using synchrotron-based hard x-ray and extreme ultraviolet photoemission spectroscopy, we resolve subtle peak splitting in the 4$f$ peaks near the Fermi level in the mixed-valent semimetal phase. The separation is enhanced by several tens of meV by increasing the lattice parameter by a few percent. Our results elucidate the evolving nature of the 4$f$ state across the phase transition, and provide direct experimental evidence for electronic interactions beyond the standard Anderson model in mixed-valence systems.","sentences":["Hybridization between localized 4$f$ and itinerant 5$d$6$s$ states in heavy fermion compounds is a well-studied phenomenon and commonly captured by the paradigmatic Anderson model.","However, the investigation of additional electronic interactions, beyond the standard Anderson model, has been limited, despite their predicted important role in the exotic quasiparticle formation in mixed-valence systems.","We investigate the 4$f$ states in TmSe$_{1-x}$Te$_x$ throughout a semimetal-insulator phase transition, which drastically varies the interactions related to the 4$f$ states.","Using synchrotron-based hard x-ray and extreme ultraviolet photoemission spectroscopy, we resolve subtle peak splitting in the 4$f$ peaks near the Fermi level in the mixed-valent semimetal phase.","The separation is enhanced by several tens of meV by increasing the lattice parameter by a few percent.","Our results elucidate the evolving nature of the 4$f$ state across the phase transition, and provide direct experimental evidence for electronic interactions beyond the standard Anderson model in mixed-valence systems."],"url":"http://arxiv.org/abs/2406.02408v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 15:17:04","title":"Demonstration of two-dimensional connectivity for a scalable error-corrected ion-trap quantum processor architecture","abstract":"A major hurdle for building a large-scale quantum computer is to scale up the number of qubits while maintaining connectivity between them. In trapped-ion devices, this connectivity can be provided by physically moving subregisters consisting of a few ions across the processor. The topology of the connectivity is given by the layout of the ion trap where one-dimensional and two-dimensional arrangements are possible. Here, we focus on an architecture based on a rectangular two-dimensional lattice, where each lattice site contains a subregister with a linear string of ions. We refer to this architecture as the Quantum Spring Array (QSA). Subregisters placed in neighboring lattice sites can be coupled by bringing the respective ion strings close to each other while avoiding merging them into a single trapping potential. Control of the separation of subregisters along one axis of the lattice, known as the axial direction, uses quasi-static voltages, while the second axis, the radial, requires control of radio frequency signals. In this work, we investigate key elements of the 2D lattice quantum computation architecture along both axes: We show that the coupling rate between neighboring lattice sites increases with the number of ions per site and the motion of the coupled system can be resilient to noise. The coherence of the coupling is assessed, and an entangled state of qubits in separate trapping regions along the radial axis is demonstrated. Moreover, we demonstrate control over radio frequency signals to adjust radial separation between strings, and thus tune their coupling rate. We further map the 2D lattice architecture to code primitives for fault-tolerant quantum error correction, providing a step towards a quantum processor architecture that is optimized for large-scale fault-tolerant operation.","sentences":["A major hurdle for building a large-scale quantum computer is to scale up the number of qubits while maintaining connectivity between them.","In trapped-ion devices, this connectivity can be provided by physically moving subregisters consisting of a few ions across the processor.","The topology of the connectivity is given by the layout of the ion trap where one-dimensional and two-dimensional arrangements are possible.","Here, we focus on an architecture based on a rectangular two-dimensional lattice, where each lattice site contains a subregister with a linear string of ions.","We refer to this architecture as the Quantum Spring Array (QSA).","Subregisters placed in neighboring lattice sites can be coupled by bringing the respective ion strings close to each other while avoiding merging them into a single trapping potential.","Control of the separation of subregisters along one axis of the lattice, known as the axial direction, uses quasi-static voltages, while the second axis, the radial, requires control of radio frequency signals.","In this work, we investigate key elements of the 2D lattice quantum computation architecture along both axes: We show that the coupling rate between neighboring lattice sites increases with the number of ions per site and the motion of the coupled system can be resilient to noise.","The coherence of the coupling is assessed, and an entangled state of qubits in separate trapping regions along the radial axis is demonstrated.","Moreover, we demonstrate control over radio frequency signals to adjust radial separation between strings, and thus tune their coupling rate.","We further map the 2D lattice architecture to code primitives for fault-tolerant quantum error correction, providing a step towards a quantum processor architecture that is optimized for large-scale fault-tolerant operation."],"url":"http://arxiv.org/abs/2406.02406v1","category":"quant-ph"}
{"created":"2024-06-04 15:16:39","title":"Emergence of Newtonian Deterministic Causality from Stochastic Motions in Continuous Space and Time","abstract":"Since Newton's time, deterministic causality has been considered a crucial prerequisite in any fundamental theory in physics. In contrast, the present work investigates stochastic dynamical models for motion in one spatial dimension, in which Newtonian mechanics becomes an emergent property: We present a coherent theory in which a Hamilton-Jacobi equation (HJE) emerges in a description of the evolution of entropy $-\\phi(x,t)=\\epsilon \\log$(Probability) of a system under observation and in the limit of large information extent $\\epsilon^{-1}$ in homogeneous space and time. The variable $\\phi$ represents a non-random high-order statistical concept that is distinct from probability itself as $\\epsilon=0$; the HJE embodies an emergent law of deterministic causality in continuous space and time with an Imaginary Scale symmetry $(t,x,\\phi)\\leftrightarrow (it,ix,i\\phi)$. $\\phi(x,t)$ exhibits a nonlinear wave phenomenon with a mathematical singularity in finite time, overcoming which we introduce viscosity $\\epsilon(\\partial^2\\phi/\\partial x^2)$ and wave $i\\epsilon(\\partial^2 \\phi/\\partial x^2)$ perturbations, articulating dissipation and conservation, which break the Imaginary Scale symmetry: They lead to the Brownian motion and Schr\\\"{o}dinger's equation of motion, respectively. Last but not least, Lagrange's action in classical mechanics acquires an entropic interpretation and Hamilton's principle is established.","sentences":["Since Newton's time, deterministic causality has been considered a crucial prerequisite in any fundamental theory in physics.","In contrast, the present work investigates stochastic dynamical models for motion in one spatial dimension, in which Newtonian mechanics becomes an emergent property: We present a coherent theory in which a Hamilton-Jacobi equation (HJE) emerges in a description of the evolution of entropy $-\\phi(x,t)=\\epsilon \\log$(Probability) of a system under observation and in the limit of large information extent $\\epsilon^{-1}$ in homogeneous space and time.","The variable $\\phi$ represents a non-random high-order statistical concept that is distinct from probability itself as $\\epsilon=0$; the HJE embodies an emergent law of deterministic causality in continuous space and time with an Imaginary Scale symmetry $(t,x,\\phi)\\leftrightarrow (it,ix,i\\phi)$. $\\phi(x,t)$ exhibits a nonlinear wave phenomenon with a mathematical singularity in finite time, overcoming which we introduce viscosity $\\epsilon(\\partial^2\\phi/\\partial x^2)$ and wave $i\\epsilon(\\partial^2 \\phi/\\partial x^2)$","perturbations, articulating dissipation and conservation, which break the Imaginary Scale symmetry: They lead to the Brownian motion and Schr\\\"{o}dinger's equation of motion, respectively.","Last but not least, Lagrange's action in classical mechanics acquires an entropic interpretation and Hamilton's principle is established."],"url":"http://arxiv.org/abs/2406.02405v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-04 15:13:57","title":"Spatial models for boolean actions in the infinite measure-preserving setup","abstract":"We show that up to a null set, every infinite measure-preserving action of a locally compact Polish group can be turned into a continuous measure-preserving action on a locally compact Polish space where the underlying measure is Radon. We also investigate the distinction between spatial and boolean actions in the infinite measure-preserving setup. We finally obtain a streamlined proof of a recent result of Avraham-Re'em and Roy: Levy groups cannot admit nontrivial continous measure-preserving actions on Polish spaces when the measure is locally finite.","sentences":["We show that up to a null set, every infinite measure-preserving action of a locally compact Polish group can be turned into a continuous measure-preserving action on a locally compact Polish space where the underlying measure is Radon.","We also investigate the distinction between spatial and boolean actions in the infinite measure-preserving setup.","We finally obtain a streamlined proof of a recent result of Avraham-Re'em and Roy:","Levy groups cannot admit nontrivial continous measure-preserving actions on Polish spaces when the measure is locally finite."],"url":"http://arxiv.org/abs/2406.02401v1","category":"math.DS"}
{"created":"2024-06-04 15:08:07","title":"Note on $Spin(3,1)$ tensors, the Dirac field and $GL(k, \\mathbb{R})$ symmetry","abstract":"We show that the rank decomposition of a real matrix $r$, which is a $Spin(3,1)$ tensor, leads to $2k$ Majorana bispinors, where $k= rank\\: r$. The Majorana bispinors are determined up to local $GL(k, \\mathbb{R})$ transformations. The bispinors are combined in pairs to form $k$ complex Dirac fields. We analyze in detail the case $k=1$, in which there is just one Dirac field with the standard Lagrangian. The $GL(1, \\mathbb{R})$ symmetry gives rise to a new conserved current, different from the well known $U(1)$ current. The $U(1)$ symmetry is present too. All global continuous internal symmetries in the $k=1$ case form the $SO(2,1)$ group. As a side result, we clarify the discussed in literature issue whether there exist algebraic constraints for the matrix $r$ which would be equivalent to the condition $rank\\: r=1$.","sentences":["We show that the rank decomposition of a real matrix $r$, which is a $Spin(3,1)$ tensor, leads to $2k$ Majorana bispinors, where $k= rank\\: r$.","The Majorana bispinors are determined up to local $GL(k, \\mathbb{R})$ transformations.","The bispinors are combined in pairs to form $k$ complex Dirac fields.","We analyze in detail the case $k=1$, in which there is just one Dirac field with the standard Lagrangian.","The $GL(1, \\mathbb{R})$ symmetry gives rise to a new conserved current, different from the well known $U(1)$ current.","The $U(1)$ symmetry is present too.","All global continuous internal symmetries in the $k=1$ case form the $SO(2,1)$ group.","As a side result, we clarify the discussed in literature issue whether there exist algebraic constraints for the matrix $r$ which would be equivalent to the condition $rank\\: r=1$."],"url":"http://arxiv.org/abs/2406.02392v1","category":"hep-th"}
{"created":"2024-06-04 15:07:24","title":"Demonstration of Erasure Conversion in a Molecular Tweezer Array","abstract":"Programmable optical tweezer arrays of molecules are an emerging platform for quantum simulation and quantum information science. For these applications, reducing and mitigating errors that arise during initial state preparation and subsequent evolution remain major challenges. In this paper, we present work on site-resolved detection of internal state errors and quantum erasures, which are qubit errors with known locations. First, using a new site-resolved detection scheme, we demonstrate robust and enhanced tweezer array preparation fidelities. This enables creating molecular arrays with low defect rates, opening the door to high-fidelity simulation of quantum many-body systems. Second, for the first time in molecules, we demonstrate mid-circuit detection of erasures using a composite detection scheme that minimally affects error-free qubits. We also demonstrate mid-circuit conversion of blackbody-induced errors into detectable erasures. Our demonstration of erasure conversion, which has been shown to significantly reduce overheads for fault-tolerant quantum error correction, could be useful for quantum information processing in molecular tweezer arrays.","sentences":["Programmable optical tweezer arrays of molecules are an emerging platform for quantum simulation and quantum information science.","For these applications, reducing and mitigating errors that arise during initial state preparation and subsequent evolution remain major challenges.","In this paper, we present work on site-resolved detection of internal state errors and quantum erasures, which are qubit errors with known locations.","First, using a new site-resolved detection scheme, we demonstrate robust and enhanced tweezer array preparation fidelities.","This enables creating molecular arrays with low defect rates, opening the door to high-fidelity simulation of quantum many-body systems.","Second, for the first time in molecules, we demonstrate mid-circuit detection of erasures using a composite detection scheme that minimally affects error-free qubits.","We also demonstrate mid-circuit conversion of blackbody-induced errors into detectable erasures.","Our demonstration of erasure conversion, which has been shown to significantly reduce overheads for fault-tolerant quantum error correction, could be useful for quantum information processing in molecular tweezer arrays."],"url":"http://arxiv.org/abs/2406.02391v1","category":"quant-ph"}
{"created":"2024-06-04 15:04:41","title":"Utilizing Voronoi Tessellations to Determine Radial Density Profiles","abstract":"We have developed a novel method of determining 2D radial density profiles for astronomical systems of discrete objects using Voronoi tessellations. This Voronoi-based method was tested against the standard annulus-based method on 5 simulated systems of objects, following known Hubble density profiles of varying parameters and sizes. It was found that the Voronoi-based method returned radial density fits with lower uncertainties on the fitting parameters across all 5 systems compared to the annulus-based method. The Voronoi-based method also consistently returned more accurate estimates of the total number of objects in each system than the annulus-based method, and this accuracy increased with increasing system size. Finally, the Voronoi-based method was applied to two observed globular cluster systems around brightest cluster galaxies ESO 444-G046 and 2MASX J13272961-3123237 and the results were compared to previous results for these galaxies obtained with the annulus-based method. Again, it was found that the Voronoi-based method returned fits with lower uncertainties on the fitting parameters, and the total number of globular clusters returned are within errors of the annulus-based method estimates, however also with lower uncertainties.","sentences":["We have developed a novel method of determining 2D radial density profiles for astronomical systems of discrete objects using Voronoi tessellations.","This Voronoi-based method was tested against the standard annulus-based method on 5 simulated systems of objects, following known Hubble density profiles of varying parameters and sizes.","It was found that the Voronoi-based method returned radial density fits with lower uncertainties on the fitting parameters across all 5 systems compared to the annulus-based method.","The Voronoi-based method also consistently returned more accurate estimates of the total number of objects in each system than the annulus-based method, and this accuracy increased with increasing system size.","Finally, the Voronoi-based method was applied to two observed globular cluster systems around brightest cluster galaxies ESO 444-G046 and 2MASX J13272961-3123237 and the results were compared to previous results for these galaxies obtained with the annulus-based method.","Again, it was found that the Voronoi-based method returned fits with lower uncertainties on the fitting parameters, and the total number of globular clusters returned are within errors of the annulus-based method estimates, however also with lower uncertainties."],"url":"http://arxiv.org/abs/2406.02390v1","category":"astro-ph.IM"}
{"created":"2024-06-04 15:01:20","title":"A fast neural emulator for interstellar chemistry","abstract":"Astrochemical models are important tools to interpret observations of molecular and atomic species in different environments. However, these models are time-consuming, precluding a thorough exploration of the parameter space, leading to uncertainties and biased results. Using neural networks to simulate the behavior of astrochemical models is a way to circumvent this problem, providing fast calculations that are based on real astrochemical models. In this paper, we present a fast neural emulator of the astrochemical code Nautilus based on conditional neural fields. The resulting model produces the abundance of 192 species for arbitrary times between 1 and 10$^7$ years. Uncertainties well below 0.2 dex are found for all species, while the computing time is of the order of 10$^4$ smaller than Nautilus. This will open up the possibility of performing much more complex forward models to better understand the physical properties of the interstellar medium. As an example of the power of these models, we ran a feature importance analysis on the electron abundance predicted by Nautilus. We found that the electron density is coupled to the initial sulphur abundance in a low density gas. Increasing the initial sulphur abundance from a depleted scenario to the cosmic abundance leads to an enhancement of an order of magnitude of the electron density. This enhancement can potentially influence the dynamics of the gas in star formation sites.","sentences":["Astrochemical models are important tools to interpret observations of molecular and atomic species in different environments.","However, these models are time-consuming, precluding a thorough exploration of the parameter space, leading to uncertainties and biased results.","Using neural networks to simulate the behavior of astrochemical models is a way to circumvent this problem, providing fast calculations that are based on real astrochemical models.","In this paper, we present a fast neural emulator of the astrochemical code Nautilus based on conditional neural fields.","The resulting model produces the abundance of 192 species for arbitrary times between 1 and 10$^7$ years.","Uncertainties well below 0.2 dex are found for all species, while the computing time is of the order of 10$^4$ smaller than Nautilus.","This will open up the possibility of performing much more complex forward models to better understand the physical properties of the interstellar medium.","As an example of the power of these models, we ran a feature importance analysis on the electron abundance predicted by Nautilus.","We found that the electron density is coupled to the initial sulphur abundance in a low density gas.","Increasing the initial sulphur abundance from a depleted scenario to the cosmic abundance leads to an enhancement of an order of magnitude of the electron density.","This enhancement can potentially influence the dynamics of the gas in star formation sites."],"url":"http://arxiv.org/abs/2406.02387v1","category":"astro-ph.IM"}
{"created":"2024-06-04 14:59:52","title":"Second-order optimality conditions for the sparse optimal control of nonviscous Cahn-Hilliard systems","abstract":"In this paper we study the optimal control of an initial-boundary value problem for the classical nonviscous Cahn-Hilliard system with zero Neumann boundary conditions. Phase field systems of this type govern the evolution of diffusive phase transition processes with conserved order parameter. For such systems, optimal control problems have been studied in the past. We focus here on the situation when the cost functional of the optimal control problem contains a sparsity-enhancing nondifferentiable term like the L1-norm. For such cases, we establish first-order necessary and second-order sufficient optimality conditions for locally optimal controls, where in the approach to second-order sufficient conditions we employ a technique introduced by E. Casas, C. Ryll and F. Tr\\\"oltzsch in the paper [SIAM J. Control Optim. 53 (2015), 2168-2202]. The main novelty of this paper is that this method, which has recently been successfully applied to systems of viscous Cahn-Hilliard type, can be adapted also to the classical nonviscous case. Since in the case without viscosity the solutions to the state and adjoint systems turn out to be considerably less regular than in the viscous case, numerous additional technical difficulties have to be overcome, and additional conditions have to be imposed. In particular, we have to restrict ourselves to the case when the nonlinearity driving the phase separation is regular, while in the presence of a viscosity term also nonlinearities of logarithmic type turn could be admitted. In addition, the implicit function theorem, which was employed to establish the needed differentiability properties of the control-to-state operator in the viscous case, does not apply in our situation and has to be substituted by other arguments.","sentences":["In this paper we study the optimal control of an initial-boundary value problem for the classical nonviscous Cahn-Hilliard system with zero Neumann boundary conditions.","Phase field systems of this type govern the evolution of diffusive phase transition processes with conserved order parameter.","For such systems, optimal control problems have been studied in the past.","We focus here on the situation when the cost functional of the optimal control problem contains a sparsity-enhancing nondifferentiable term like the L1-norm.","For such cases, we establish first-order necessary and second-order sufficient optimality conditions for locally optimal controls, where in the approach to second-order sufficient conditions we employ a technique introduced by E. Casas, C. Ryll and F. Tr\\\"oltzsch in the paper [SIAM J. Control Optim.","53 (2015), 2168-2202].","The main novelty of this paper is that this method, which has recently been successfully applied to systems of viscous Cahn-Hilliard type, can be adapted also to the classical nonviscous case.","Since in the case without viscosity the solutions to the state and adjoint systems turn out to be considerably less regular than in the viscous case, numerous additional technical difficulties have to be overcome, and additional conditions have to be imposed.","In particular, we have to restrict ourselves to the case when the nonlinearity driving the phase separation is regular, while in the presence of a viscosity term also nonlinearities of logarithmic type turn could be admitted.","In addition, the implicit function theorem, which was employed to establish the needed differentiability properties of the control-to-state operator in the viscous case, does not apply in our situation and has to be substituted by other arguments."],"url":"http://arxiv.org/abs/2406.02384v1","category":"math.OC"}
{"created":"2024-06-04 14:58:28","title":"Closed forms for spatiotemporal optical vortices and sagittal skyrmionic pulses","abstract":"Spatiotemporal optical vortices (STOVs) are short pulses that present a vortex whose axis is perpendicular to the main propagation direction. We present analytic expressions for these pulses that satisfy exactly Maxwell's equation, by applying appropriate differential operators to complex focus pulses with Poisson-like frequency spectrum. We also provide a simple ray picture for understanding the deformation of these pulses under propagation. Finally, we use these solutions to propose a type of pulse with sagittal skyrmionic polarization distribution covering all states of transverse polarization.","sentences":["Spatiotemporal optical vortices (STOVs) are short pulses that present a vortex whose axis is perpendicular to the main propagation direction.","We present analytic expressions for these pulses that satisfy exactly Maxwell's equation, by applying appropriate differential operators to complex focus pulses with Poisson-like frequency spectrum.","We also provide a simple ray picture for understanding the deformation of these pulses under propagation.","Finally, we use these solutions to propose a type of pulse with sagittal skyrmionic polarization distribution covering all states of transverse polarization."],"url":"http://arxiv.org/abs/2406.02382v1","category":"physics.optics"}
{"created":"2024-06-04 14:57:21","title":"Entanglement accelerates quantum simulation","abstract":"Quantum entanglement is an essential feature of many-body systems that impacts both quantum information processing and fundamental physics. The growth of entanglement is a major challenge for classical simulation methods. In this work, we investigate the relationship between quantum entanglement and quantum simulation, showing that product-formula approximations can perform better for entangled systems. We establish a tighter upper bound for algorithmic error in terms of entanglement entropy and develop an adaptive simulation algorithm incorporating measurement gadgets to estimate the algorithmic error. This shows that entanglement is not only an obstacle to classical simulation, but also a feature that can accelerate quantum algorithms.","sentences":["Quantum entanglement is an essential feature of many-body systems that impacts both quantum information processing and fundamental physics.","The growth of entanglement is a major challenge for classical simulation methods.","In this work, we investigate the relationship between quantum entanglement and quantum simulation, showing that product-formula approximations can perform better for entangled systems.","We establish a tighter upper bound for algorithmic error in terms of entanglement entropy and develop an adaptive simulation algorithm incorporating measurement gadgets to estimate the algorithmic error.","This shows that entanglement is not only an obstacle to classical simulation, but also a feature that can accelerate quantum algorithms."],"url":"http://arxiv.org/abs/2406.02379v1","category":"quant-ph"}
{"created":"2024-06-04 14:48:45","title":"Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Analysis with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error","abstract":"Power analyses help investigators design robust and reproducible research. Understanding of determinants of statistical power is helpful in interpreting results in publications and in analysis for causal inference. Case-crossover analysis, a matched case-control analysis, is widely used to estimate health effects of short-term exposures. Despite its widespread use, understanding of sample size, statistical power, and the accuracy and precision of the estimator in real-world data settings is very limited. First, the variance of exposures that exhibit spatiotemporal patterns may be heteroskedastic (e.g., air pollution and temperature exposures, impacted by climate change). Second, distributed lags of the exposure variable may be used to identify critical exposure time-windows. Third, exposure measurement error is not uncommon, impacting the accuracy and/or precision of the estimator, depending on the measurement error mechanism. Exposure measurement errors result in covariate measurement errors of distributed lags. All these issues complicate the understanding. Therefore, I developed approximation equations for sample size, estimates of the estimators and standard errors, and identified conditions for applications. I discussed polynomials for non-linear effect estimation. I analyzed air pollution estimates in the United States (U.S.), developed by U.S. Environmental Protection Agency to examine errors, and conducted statistical simulations. Overall, sample size can be calculated based on external information about exposure variable validation, without validation data in hand. For estimators of distributed lags, calculations may perform well if residual confounding due to covariate measurement error is not severe. This condition may sometimes be difficult to identify without validation data, suggesting investigators should consider validation research.","sentences":["Power analyses help investigators design robust and reproducible research.","Understanding of determinants of statistical power is helpful in interpreting results in publications and in analysis for causal inference.","Case-crossover analysis, a matched case-control analysis, is widely used to estimate health effects of short-term exposures.","Despite its widespread use, understanding of sample size, statistical power, and the accuracy and precision of the estimator in real-world data settings is very limited.","First, the variance of exposures that exhibit spatiotemporal patterns may be heteroskedastic (e.g., air pollution and temperature exposures, impacted by climate change).","Second, distributed lags of the exposure variable may be used to identify critical exposure time-windows.","Third, exposure measurement error is not uncommon, impacting the accuracy and/or precision of the estimator, depending on the measurement error mechanism.","Exposure measurement errors result in covariate measurement errors of distributed lags.","All these issues complicate the understanding.","Therefore, I developed approximation equations for sample size, estimates of the estimators and standard errors, and identified conditions for applications.","I discussed polynomials for non-linear effect estimation.","I analyzed air pollution estimates in the United States (U.S.), developed by U.S. Environmental Protection Agency to examine errors, and conducted statistical simulations.","Overall, sample size can be calculated based on external information about exposure variable validation, without validation data in hand.","For estimators of distributed lags, calculations may perform well if residual confounding due to covariate measurement error is not severe.","This condition may sometimes be difficult to identify without validation data, suggesting investigators should consider validation research."],"url":"http://arxiv.org/abs/2406.02369v1","category":"stat.ME"}
{"created":"2024-06-04 14:43:50","title":"Exploiting Chordal Sparsity for Fast Global Optimality with Application to Localization","abstract":"In recent years, many estimation problems in robotics have been shown to be solvable to global optimality using their semidefinite relaxations. However, the runtime complexity of off-the-shelve semidefinite programming solvers is up to cubic in problem size, which inhibits real-time solutions of problems involving large state dimensions. We show that for a large class of problems, namely those with chordal sparsity, we can reduce the complexity of these solvers to linear in problem size. In particular, we show how to replace the large positive-semidefinite variable by a number of smaller interconnected ones using the well-known chordal decomposition. This formulation also allows for the straightforward application of the alternating direction method of multipliers (ADMM), which can exploit parallelism for increased scalability. We show in simulation that the algorithms provide a significant speed up for two example problems: matrix-weighted and range-only localization.","sentences":["In recent years, many estimation problems in robotics have been shown to be solvable to global optimality using their semidefinite relaxations.","However, the runtime complexity of off-the-shelve semidefinite programming solvers is up to cubic in problem size, which inhibits real-time solutions of problems involving large state dimensions.","We show that for a large class of problems, namely those with chordal sparsity, we can reduce the complexity of these solvers to linear in problem size.","In particular, we show how to replace the large positive-semidefinite variable by a number of smaller interconnected ones using the well-known chordal decomposition.","This formulation also allows for the straightforward application of the alternating direction method of multipliers (ADMM), which can exploit parallelism for increased scalability.","We show in simulation that the algorithms provide a significant speed up for two example problems: matrix-weighted and range-only localization."],"url":"http://arxiv.org/abs/2406.02365v1","category":"cs.RO"}
{"created":"2024-06-04 14:42:09","title":"Learning dynamical models from stochastic trajectories","abstract":"The dynamics of biological systems, from proteins to cells to organisms, is complex and stochastic. To decipher their physical laws, we need to bridge between experimental observations and theoretical modeling. Thanks to progress in microscopy and tracking, there is today an abundance of experimental trajectories reflecting these dynamical laws. Inferring physical models from noisy and imperfect experimental data, however, is challenging. Because there are no inference methods that are robust and efficient, model reconstruction from experimental trajectories is a bottleneck to data-driven biophysics. In this Thesis, I present a set of tools developed to bridge this gap and permit robust and universal inference of stochastic dynamical models from experimental trajectories. These methods are rooted in an information-theoretical framework that quantifies how much can be inferred from trajectories that are short, partial and noisy. They permit the efficient inference of dynamical models for overdamped and underdamped Langevin systems, as well as the inference of entropy production rates. I finally present early applications of these techniques, as well as future research directions.","sentences":["The dynamics of biological systems, from proteins to cells to organisms, is complex and stochastic.","To decipher their physical laws, we need to bridge between experimental observations and theoretical modeling.","Thanks to progress in microscopy and tracking, there is today an abundance of experimental trajectories reflecting these dynamical laws.","Inferring physical models from noisy and imperfect experimental data, however, is challenging.","Because there are no inference methods that are robust and efficient, model reconstruction from experimental trajectories is a bottleneck to data-driven biophysics.","In this Thesis, I present a set of tools developed to bridge this gap and permit robust and universal inference of stochastic dynamical models from experimental trajectories.","These methods are rooted in an information-theoretical framework that quantifies how much can be inferred from trajectories that are short, partial and noisy.","They permit the efficient inference of dynamical models for overdamped and underdamped Langevin systems, as well as the inference of entropy production rates.","I finally present early applications of these techniques, as well as future research directions."],"url":"http://arxiv.org/abs/2406.02363v1","category":"cond-mat.soft"}
{"created":"2024-06-04 14:38:23","title":"A Practical Approach for Exploring Granger Connectivity in High-Dimensional Networks of Time Series","abstract":"This manuscript presents a novel method for discovering effective connectivity between specified pairs of nodes in a high-dimensional network of time series. To accurately perform Granger causality analysis from the first node to the second node, it is essential to eliminate the influence of all other nodes within the network. The approach proposed is to create a low-dimensional representation of all other nodes in the network using frequency-domain-based dynamic principal component analysis (spectral DPCA). The resulting scores are subsequently removed from the first and second nodes of interest, thus eliminating the confounding effect of other nodes within the high-dimensional network. To conduct hypothesis testing on Granger causality, we propose a permutation-based causality test. This test enhances the accuracy of our findings when the error structures are non-Gaussian. The approach has been validated in extensive simulation studies, which demonstrate the efficacy of the methodology as a tool for causality analysis in complex time series networks. The proposed methodology has also been demonstrated to be both expedient and viable on real datasets, with particular success observed on multichannel EEG networks.","sentences":["This manuscript presents a novel method for discovering effective connectivity between specified pairs of nodes in a high-dimensional network of time series.","To accurately perform Granger causality analysis from the first node to the second node, it is essential to eliminate the influence of all other nodes within the network.","The approach proposed is to create a low-dimensional representation of all other nodes in the network using frequency-domain-based dynamic principal component analysis (spectral DPCA).","The resulting scores are subsequently removed from the first and second nodes of interest, thus eliminating the confounding effect of other nodes within the high-dimensional network.","To conduct hypothesis testing on Granger causality, we propose a permutation-based causality test.","This test enhances the accuracy of our findings when the error structures are non-Gaussian.","The approach has been validated in extensive simulation studies, which demonstrate the efficacy of the methodology as a tool for causality analysis in complex time series networks.","The proposed methodology has also been demonstrated to be both expedient and viable on real datasets, with particular success observed on multichannel EEG networks."],"url":"http://arxiv.org/abs/2406.02360v1","category":"stat.AP"}
{"created":"2024-06-04 14:28:36","title":"System-Aware Neural ODE Processes for Few-Shot Bayesian Optimization","abstract":"We consider the problem of optimizing initial conditions and timing in dynamical systems governed by unknown ordinary differential equations (ODEs), where evaluating different initial conditions is costly and there are constraints on observation times. To identify the optimal conditions within several trials, we introduce a few-shot Bayesian Optimization (BO) framework based on the system's prior information. At the core of our approach is the System-Aware Neural ODE Processes (SANODEP), an extension of Neural ODE Processes (NODEP) designed to meta-learn ODE systems from multiple trajectories using a novel context embedding block. Additionally, we propose a multi-scenario loss function specifically for optimization purposes. Our two-stage BO framework effectively incorporates search space constraints, enabling efficient optimization of both initial conditions and observation timings. We conduct extensive experiments showcasing SANODEP's potential for few-shot BO. We also explore SANODEP's adaptability to varying levels of prior information, highlighting the trade-off between prior flexibility and model fitting accuracy.","sentences":["We consider the problem of optimizing initial conditions and timing in dynamical systems governed by unknown ordinary differential equations (ODEs), where evaluating different initial conditions is costly and there are constraints on observation times.","To identify the optimal conditions within several trials, we introduce a few-shot Bayesian Optimization (BO) framework based on the system's prior information.","At the core of our approach is the System-Aware Neural ODE Processes (SANODEP), an extension of Neural ODE Processes (NODEP) designed to meta-learn ODE systems from multiple trajectories using a novel context embedding block.","Additionally, we propose a multi-scenario loss function specifically for optimization purposes.","Our two-stage BO framework effectively incorporates search space constraints, enabling efficient optimization of both initial conditions and observation timings.","We conduct extensive experiments showcasing SANODEP's potential for few-shot BO.","We also explore SANODEP's adaptability to varying levels of prior information, highlighting the trade-off between prior flexibility and model fitting accuracy."],"url":"http://arxiv.org/abs/2406.02352v1","category":"cs.LG"}
{"created":"2024-06-04 14:12:02","title":"Train Localization During GNSS Outages: A Minimalist Approach Using Track Geometry And IMU Sensor Data","abstract":"Train localization during Global Navigation Satellite Systems (GNSS) outages presents challenges for ensuring failsafe and accurate positioning in railway networks. This paper proposes a minimalist approach exploiting track geometry and Inertial Measurement Unit (IMU) sensor data. By integrating a discrete track map as a Look-Up Table (LUT) into a Particle Filter (PF) based solution, accurate train positioning is achieved with only an IMU sensor and track map data. The approach is tested on an open railway positioning data set, showing that accurate positioning (absolute errors below 10 m) can be maintained during GNSS outages up to 30 s in the given data. We simulate outages on different track segments and show that accurate positioning is reached during track curves and curvy railway lines. The approach can be used as a redundant complement to established positioning solutions to increase the position estimate's reliability and robustness.","sentences":["Train localization during Global Navigation Satellite Systems (GNSS) outages presents challenges for ensuring failsafe and accurate positioning in railway networks.","This paper proposes a minimalist approach exploiting track geometry and Inertial Measurement Unit (IMU) sensor data.","By integrating a discrete track map as a Look-Up Table (LUT) into a Particle Filter (PF) based solution, accurate train positioning is achieved with only an IMU sensor and track map data.","The approach is tested on an open railway positioning data set, showing that accurate positioning (absolute errors below 10 m) can be maintained during GNSS outages up to 30 s in the given data.","We simulate outages on different track segments and show that accurate positioning is reached during track curves and curvy railway lines.","The approach can be used as a redundant complement to established positioning solutions to increase the position estimate's reliability and robustness."],"url":"http://arxiv.org/abs/2406.02339v1","category":"cs.RO"}
{"created":"2024-06-04 14:00:02","title":"Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering","abstract":"Building a reliable visual question answering~(VQA) system across different languages is a challenging problem, primarily due to the lack of abundant samples for training. To address this challenge, recent studies have employed machine translation systems for the cross-lingual VQA task. This involves translating the evaluation samples into a source language (usually English) and using monolingual models (i.e., translate-test). However, our analysis reveals that translated texts contain unique characteristics distinct from human-written ones, referred to as translation artifacts. We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes. In light of this, we present a simple data augmentation strategy that can alleviate the adverse impacts of translation artifacts.","sentences":["Building a reliable visual question answering~(VQA) system across different languages is a challenging problem, primarily due to the lack of abundant samples for training.","To address this challenge, recent studies have employed machine translation systems for the cross-lingual VQA task.","This involves translating the evaluation samples into a source language (usually English) and using monolingual models (i.e., translate-test).","However, our analysis reveals that translated texts contain unique characteristics distinct from human-written ones, referred to as translation artifacts.","We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes.","In light of this, we present a simple data augmentation strategy that can alleviate the adverse impacts of translation artifacts."],"url":"http://arxiv.org/abs/2406.02331v1","category":"cs.CL"}
{"created":"2024-06-04 13:59:50","title":"Universality arising from invertible weighted composition operators","abstract":"A linear operator $U$ acting boundedly on an infinite-dimensional separable complex Hilbert space $H$ is universal if every linear bounded operator acting on $H$ is similar to a scalar multiple of a restriction of $U$ to one of its invariant subspaces. It turns out that characterizing the lattice of closed invariant subspaces of a universal operator is equivalent to solve the Invariant Subspace Problem for Hilbert spaces. In this paper, we consider invertible weighted hyperbolic composition operators and we prove the universality of the translations by eigenvalues of such operators, acting on Hardy and weighted Bergman spaces. Some consequences for the Banach space case are also discussed.","sentences":["A linear operator $U$ acting boundedly on an infinite-dimensional separable complex Hilbert space $H$ is universal if every linear bounded operator acting on $H$ is similar to a scalar multiple of a restriction of $U$ to one of its invariant subspaces.","It turns out that characterizing the lattice of closed invariant subspaces of a universal operator is equivalent to solve the Invariant Subspace Problem for Hilbert spaces.","In this paper, we consider invertible weighted hyperbolic composition operators and we prove the universality of the translations by eigenvalues of such operators, acting on Hardy and weighted Bergman spaces.","Some consequences for the Banach space case are also discussed."],"url":"http://arxiv.org/abs/2406.02330v1","category":"math.FA"}
{"created":"2024-06-04 13:45:12","title":"Fast and Secure Decentralized Optimistic Rollups Using Setchain","abstract":"Modern blockchains face a scalability challenge due to the intrinsic throughput limitations of consensus protocols. Layer 2 optimistic rollups (L2) are a faster alternative that offer the same interface in terms of smart contract development and user interaction. Optimistic rollups perform most computations offchain and make light use of an underlying blockchain (L1) to guarantee correct behavior, implementing a cheaper blockchain on a blockchain solution. With optimistic rollups, a sequencer calculates offchain batches of L2 transactions and commits batches (compressed or hashed) to the L1 blockchain. The use of hashes requires a data service to translate hashes into their corresponding batches. Current L2 implementations consist of a centralized sequencer (central authority) and an optional data availability committee (DAC).   In this paper, we propose a decentralized L2 optimistic rollup based on Setchain, a decentralized Byzantine-tolerant implementation of sets. The main contribution is a fully decentralized \"arranger\" where arrangers are a formal definition combining sequencers and DACs. We prove our implementation correct and show empirical evidence that our solution scales. A final contribution is a system of incentives (payments) for servers that implement the sequencer and data availability committee protocols correctly, and a fraud-proof mechanism to detect violations of the protocol.","sentences":["Modern blockchains face a scalability challenge due to the intrinsic throughput limitations of consensus protocols.","Layer 2 optimistic rollups (L2) are a faster alternative that offer the same interface in terms of smart contract development and user interaction.","Optimistic rollups perform most computations offchain and make light use of an underlying blockchain (L1) to guarantee correct behavior, implementing a cheaper blockchain on a blockchain solution.","With optimistic rollups, a sequencer calculates offchain batches of L2 transactions and commits batches (compressed or hashed) to the L1 blockchain.","The use of hashes requires a data service to translate hashes into their corresponding batches.","Current L2 implementations consist of a centralized sequencer (central authority) and an optional data availability committee (DAC).   ","In this paper, we propose a decentralized L2 optimistic rollup based on Setchain, a decentralized Byzantine-tolerant implementation of sets.","The main contribution is a fully decentralized \"arranger\" where arrangers are a formal definition combining sequencers and DACs.","We prove our implementation correct and show empirical evidence that our solution scales.","A final contribution is a system of incentives (payments) for servers that implement the sequencer and data availability committee protocols correctly, and a fraud-proof mechanism to detect violations of the protocol."],"url":"http://arxiv.org/abs/2406.02316v1","category":"cs.CR"}
{"created":"2024-06-04 13:42:42","title":"Neural Thermodynamic Integration: Free Energies from Energy-based Diffusion Models","abstract":"Thermodynamic integration (TI) offers a rigorous method for estimating free-energy differences by integrating over a sequence of interpolating conformational ensembles. However, TI calculations are computationally expensive and typically limited to coupling a small number of degrees of freedom due to the need to sample numerous intermediate ensembles with sufficient conformational-space overlap. In this work, we propose to perform TI along an alchemical pathway represented by a trainable neural network, which we term Neural TI. Critically, we parametrize a time-dependent Hamiltonian interpolating between the interacting and non-interacting systems, and optimize its gradient using a denoising-diffusion objective. The ability of the resulting energy-based diffusion model to sample all intermediate ensembles, allows us to perform TI from a single reference calculation. We apply our method to Lennard-Jones fluids, where we report accurate calculations of the excess chemical potential, demonstrating that Neural TI is capable of coupling hundreds of degrees of freedom at once.","sentences":["Thermodynamic integration (TI) offers a rigorous method for estimating free-energy differences by integrating over a sequence of interpolating conformational ensembles.","However, TI calculations are computationally expensive and typically limited to coupling a small number of degrees of freedom due to the need to sample numerous intermediate ensembles with sufficient conformational-space overlap.","In this work, we propose to perform TI along an alchemical pathway represented by a trainable neural network, which we term Neural TI.","Critically, we parametrize a time-dependent Hamiltonian interpolating between the interacting and non-interacting systems, and optimize its gradient using a denoising-diffusion objective.","The ability of the resulting energy-based diffusion model to sample all intermediate ensembles, allows us to perform TI from a single reference calculation.","We apply our method to Lennard-Jones fluids, where we report accurate calculations of the excess chemical potential, demonstrating that Neural TI is capable of coupling hundreds of degrees of freedom at once."],"url":"http://arxiv.org/abs/2406.02313v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-04 13:42:24","title":"Multimodal Resonance in Strongly Coupled Inductor Arrays","abstract":"Magnetic resonance coupling (MRC) is widely used for wireless power transfer (WPT) applications, but little work has explored how MRC phenomena could be exploited for sensing applications. This paper introduces, validates and evaluates the unique multi-resonant phenomena predicted by circuit theory for over-coupled inductive arrays, and presents eigen-formulae for calculating resonant frequencies and voltage modes within passively excited arrays. Finite-element simulations and experimental results demonstrate the validity of the multi-modal resonant principles for strongly-coupled inductor arrays. The results confirm the distinctive multi-modal resonant frequencies these arrays exhibit, corresponding to the specific magnetic excitation \"modes\" (comparable to vibrational modes in multi-degree-of-freedom systems). The theoretical and finite element models presented offer a framework for designing and optimizing novel inductive sensing arrays, capitalizing on the unique resonant effects of over-coupling and exploiting their potential magnetic field shaping.","sentences":["Magnetic resonance coupling (MRC) is widely used for wireless power transfer (WPT) applications, but little work has explored how MRC phenomena could be exploited for sensing applications.","This paper introduces, validates and evaluates the unique multi-resonant phenomena predicted by circuit theory for over-coupled inductive arrays, and presents eigen-formulae for calculating resonant frequencies and voltage modes within passively excited arrays.","Finite-element simulations and experimental results demonstrate the validity of the multi-modal resonant principles for strongly-coupled inductor arrays.","The results confirm the distinctive multi-modal resonant frequencies these arrays exhibit, corresponding to the specific magnetic excitation \"modes\" (comparable to vibrational modes in multi-degree-of-freedom systems).","The theoretical and finite element models presented offer a framework for designing and optimizing novel inductive sensing arrays, capitalizing on the unique resonant effects of over-coupling and exploiting their potential magnetic field shaping."],"url":"http://arxiv.org/abs/2406.02312v1","category":"eess.SY"}
{"created":"2024-06-04 13:41:41","title":"Development and Validation of a Proximity-based Wearable Computing Testbed for Community-oriented Wearable Systems","abstract":"In the rapidly evolving digital technology landscape, community-oriented wearable computing systems are emerging as a key tool for enhancing connectivity and interaction within communal spaces. This paper contributes to this burgeoning field by presenting the development and implementation of a proximity-based wearable computing testbed designed to forge stronger links within communities. The testbed exploits Ultra-Wideband (UWB) position sensors, 9-axis motion sensors, edge nodes, and a centralized server, forming a cohesive network that actively facilitates community interactions and engagements. By employing anchors and targets within the UWB sensors, the system achieves high precision in location and distance measurements, laying the groundwork for various proximity-based applications. Integrating 9-axis motion sensors and advanced edge nodes further underscores the system's versatility and robustness in wearable and edge computing. This paper delves into an in-depth exploration and evaluation of the proposed system's architecture, design, and implementation processes. It provides a comprehensive analysis of experimental results and discusses the system's potential impact on enhancing community networks, along with the future directions this technology could take.","sentences":["In the rapidly evolving digital technology landscape, community-oriented wearable computing systems are emerging as a key tool for enhancing connectivity and interaction within communal spaces.","This paper contributes to this burgeoning field by presenting the development and implementation of a proximity-based wearable computing testbed designed to forge stronger links within communities.","The testbed exploits Ultra-Wideband (UWB) position sensors, 9-axis motion sensors, edge nodes, and a centralized server, forming a cohesive network that actively facilitates community interactions and engagements.","By employing anchors and targets within the UWB sensors, the system achieves high precision in location and distance measurements, laying the groundwork for various proximity-based applications.","Integrating 9-axis motion sensors and advanced edge nodes further underscores the system's versatility and robustness in wearable and edge computing.","This paper delves into an in-depth exploration and evaluation of the proposed system's architecture, design, and implementation processes.","It provides a comprehensive analysis of experimental results and discusses the system's potential impact on enhancing community networks, along with the future directions this technology could take."],"url":"http://arxiv.org/abs/2406.02311v1","category":"cs.NI"}
{"created":"2024-06-04 13:35:33","title":"Debris Disks can Contaminate Mid-Infrared Exoplanet Spectra: Evidence for a Circumstellar Debris Disk around Exoplanet Host WASP-39","abstract":"The signal from a transiting planet can be diluted by astrophysical contamination. In the case of circumstellar debris disks, this contamination could start in the mid-infrared and vary as a function of wavelength, which would then change the observed transmission spectrum for any planet in the system. The MIRI/LRS WASP-39b transmission spectrum shows an unexplained dip starting at $\\sim$10 $\\mu$m that could be caused by astrophysical contamination. The spectral energy distribution displays excess flux at similar levels to that which are needed to create the dip in the transmission spectrum. In this article, we show that this dip is consistent with the presence of a bright circumstellar debris disk, at a distance of $>$2 au. We discuss how a circumstellar debris disk like that could affect the atmosphere of WASP-39b. We also show that even faint debris disks can be a source of contamination in MIRI exoplanet spectra.","sentences":["The signal from a transiting planet can be diluted by astrophysical contamination.","In the case of circumstellar debris disks, this contamination could start in the mid-infrared and vary as a function of wavelength, which would then change the observed transmission spectrum for any planet in the system.","The MIRI/LRS WASP-39b transmission spectrum shows an unexplained dip starting at $\\sim$10 $\\mu$m that could be caused by astrophysical contamination.","The spectral energy distribution displays excess flux at similar levels to that which are needed to create the dip in the transmission spectrum.","In this article, we show that this dip is consistent with the presence of a bright circumstellar debris disk, at a distance of $>$2 au.","We discuss how a circumstellar debris disk like that could affect the atmosphere of WASP-39b.","We also show that even faint debris disks can be a source of contamination in MIRI exoplanet spectra."],"url":"http://arxiv.org/abs/2406.02305v1","category":"astro-ph.EP"}
{"created":"2024-06-04 13:29:12","title":"Node-Level Topological Representation Learning on Point Clouds","abstract":"Topological Data Analysis (TDA) allows us to extract powerful topological and higher-order information on the global shape of a data set or point cloud. Tools like Persistent Homology or the Euler Transform give a single complex description of the global structure of the point cloud. However, common machine learning applications like classification require point-level information and features to be available. In this paper, we bridge this gap and propose a novel method to extract node-level topological features from complex point clouds using discrete variants of concepts from algebraic topology and differential geometry. We verify the effectiveness of these topological point features (TOPF) on both synthetic and real-world data and study their robustness under noise.","sentences":["Topological Data Analysis (TDA) allows us to extract powerful topological and higher-order information on the global shape of a data set or point cloud.","Tools like Persistent Homology or the Euler Transform give a single complex description of the global structure of the point cloud.","However, common machine learning applications like classification require point-level information and features to be available.","In this paper, we bridge this gap and propose a novel method to extract node-level topological features from complex point clouds using discrete variants of concepts from algebraic topology and differential geometry.","We verify the effectiveness of these topological point features (TOPF) on both synthetic and real-world data and study their robustness under noise."],"url":"http://arxiv.org/abs/2406.02300v1","category":"math.AT"}
{"created":"2024-06-04 13:16:08","title":"Smaller Batches, Bigger Gains? Investigating the Impact of Batch Sizes on Reinforcement Learning Based Real-World Production Scheduling","abstract":"Production scheduling is an essential task in manufacturing, with Reinforcement Learning (RL) emerging as a key solution. In a previous work, RL was utilized to solve an extended permutation flow shop scheduling problem (PFSSP) for a real-world production line with two stages, linked by a central buffer. The RL agent was trained to sequence equallysized product batches to minimize setup efforts and idle times. However, the substantial impact caused by varying the size of these product batches has not yet been explored. In this follow-up study, we investigate the effects of varying batch sizes, exploring both the quality of solutions and the training dynamics of the RL agent. The results demonstrate that it is possible to methodically identify reasonable boundaries for the batch size. These boundaries are determined on one side by the increasing sample complexity associated with smaller batch sizes, and on the other side by the decreasing flexibility of the agent when dealing with larger batch sizes. This provides the practitioner the ability to make an informed decision regarding the selection of an appropriate batch size. Moreover, we introduce and investigate two new curriculum learning strategies to enable the training with small batch sizes. The findings of this work offer the potential for application in several industrial use cases with comparable scheduling problems.","sentences":["Production scheduling is an essential task in manufacturing, with Reinforcement Learning (RL) emerging as a key solution.","In a previous work, RL was utilized to solve an extended permutation flow shop scheduling problem (PFSSP) for a real-world production line with two stages, linked by a central buffer.","The RL agent was trained to sequence equallysized product batches to minimize setup efforts and idle times.","However, the substantial impact caused by varying the size of these product batches has not yet been explored.","In this follow-up study, we investigate the effects of varying batch sizes, exploring both the quality of solutions and the training dynamics of the RL agent.","The results demonstrate that it is possible to methodically identify reasonable boundaries for the batch size.","These boundaries are determined on one side by the increasing sample complexity associated with smaller batch sizes, and on the other side by the decreasing flexibility of the agent when dealing with larger batch sizes.","This provides the practitioner the ability to make an informed decision regarding the selection of an appropriate batch size.","Moreover, we introduce and investigate two new curriculum learning strategies to enable the training with small batch sizes.","The findings of this work offer the potential for application in several industrial use cases with comparable scheduling problems."],"url":"http://arxiv.org/abs/2406.02294v1","category":"cs.LG"}
{"created":"2024-06-04 13:00:22","title":"Optimised ProPainter for Video Diminished Reality Inpainting","abstract":"In this paper, part of the DREAMING Challenge - Diminished Reality for Emerging Applications in Medicine through Inpainting, we introduce a refined video inpainting technique optimised from the ProPainter method to meet the specialised demands of medical imaging, specifically in the context of oral and maxillofacial surgery. Our enhanced algorithm employs the zero-shot ProPainter, featuring optimized parameters and pre-processing, to adeptly manage the complex task of inpainting surgical video sequences, without requiring any training process. It aims to produce temporally coherent and detail-rich reconstructions of occluded regions, facilitating clearer views of operative fields. The efficacy of our approach is evaluated using comprehensive metrics, positioning it as a significant advancement in the application of diminished reality for medical purposes.","sentences":["In this paper, part of the DREAMING Challenge - Diminished Reality for Emerging Applications in Medicine through Inpainting, we introduce a refined video inpainting technique optimised from the ProPainter method to meet the specialised demands of medical imaging, specifically in the context of oral and maxillofacial surgery.","Our enhanced algorithm employs the zero-shot ProPainter, featuring optimized parameters and pre-processing, to adeptly manage the complex task of inpainting surgical video sequences, without requiring any training process.","It aims to produce temporally coherent and detail-rich reconstructions of occluded regions, facilitating clearer views of operative fields.","The efficacy of our approach is evaluated using comprehensive metrics, positioning it as a significant advancement in the application of diminished reality for medical purposes."],"url":"http://arxiv.org/abs/2406.02287v1","category":"cs.CV"}
{"created":"2024-06-04 12:59:29","title":"How pure can we go with adiabatic state manipulation?","abstract":"Dissipative systems with decoherence free subspaces, a.k.a. dark spaces (DSs), can be used to protect quantum information. At the same time, dissipation is expected to give rise to coherent information degradation outside the DS. Employed to support quantum information platforms, DSs can be adiabatically modified in a way that resembles adiabatic control of coherent systems. Here we study the slow evolution of a purely dissipative system with a spectral gap $\\gamma$, characterized by a strong symmetry, under a cyclic protocol with period $T$. Non-adiabatic corrections to the state evolution give rise to decoherence: the evolution within the instantaneous DS is described by a time-local effective Liouvillian operator that leads to purity degradation over a period, of order $1/\\gamma T$. We obtain a closed form of the latter to order $1/(\\gamma T)^2$. Our analysis underlines speed limitations in quantum information processing in the absence of corrective measures.","sentences":["Dissipative systems with decoherence free subspaces, a.k.a. dark spaces (DSs), can be used to protect quantum information.","At the same time, dissipation is expected to give rise to coherent information degradation outside the DS.","Employed to support quantum information platforms, DSs can be adiabatically modified in a way that resembles adiabatic control of coherent systems.","Here we study the slow evolution of a purely dissipative system with a spectral gap $\\gamma$, characterized by a strong symmetry, under a cyclic protocol with period $T$. Non-adiabatic corrections to the state evolution give rise to decoherence: the evolution within the instantaneous DS is described by a time-local effective Liouvillian operator that leads to purity degradation over a period, of order $1/\\gamma","T$. We obtain a closed form of the latter to order $1/(\\gamma","T)^2$. Our analysis underlines speed limitations in quantum information processing in the absence of corrective measures."],"url":"http://arxiv.org/abs/2406.02286v1","category":"quant-ph"}
{"created":"2024-06-04 12:58:19","title":"Towards Supervised Performance on Speaker Verification with Self-Supervised Learning by Leveraging Large-Scale ASR Models","abstract":"Recent advancements in Self-Supervised Learning (SSL) have shown promising results in Speaker Verification (SV). However, narrowing the performance gap with supervised systems remains an ongoing challenge. Several studies have observed that speech representations from large-scale ASR models contain valuable speaker information. This work explores the limitations of fine-tuning these models for SV using an SSL contrastive objective in an end-to-end approach. Then, we propose a framework to learn speaker representations in an SSL context by fine-tuning a pre-trained WavLM with a supervised loss using pseudo-labels. Initial pseudo-labels are derived from an SSL DINO-based model and are iteratively refined by clustering the model embeddings. Our method achieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art on self-supervised SV. As this performance is close to our supervised baseline of 0.94% EER, this contribution is a step towards supervised performance on SV with SSL.","sentences":["Recent advancements in Self-Supervised Learning (SSL) have shown promising results in Speaker Verification (SV).","However, narrowing the performance gap with supervised systems remains an ongoing challenge.","Several studies have observed that speech representations from large-scale ASR models contain valuable speaker information.","This work explores the limitations of fine-tuning these models for SV using an SSL contrastive objective in an end-to-end approach.","Then, we propose a framework to learn speaker representations in an SSL context by fine-tuning a pre-trained WavLM with a supervised loss using pseudo-labels.","Initial pseudo-labels are derived from an SSL DINO-based model and are iteratively refined by clustering the model embeddings.","Our method achieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art on self-supervised SV.","As this performance is close to our supervised baseline of 0.94% EER, this contribution is a step towards supervised performance on SV with SSL."],"url":"http://arxiv.org/abs/2406.02285v1","category":"eess.AS"}
{"created":"2024-06-04 12:57:25","title":"Broadcasting Support Relations Recursively from Local Dynamics for Object Retrieval in Clutters","abstract":"In our daily life, cluttered objects are everywhere, from scattered stationery and books cluttering the table to bowls and plates filling the kitchen sink. Retrieving a target object from clutters is an essential while challenging skill for robots, for the difficulty of safely manipulating an object without disturbing others, which requires the robot to plan a manipulation sequence and first move away a few other objects supported by the target object step by step. However, due to the diversity of object configurations (e.g., categories, geometries, locations and poses) and their combinations in clutters, it is difficult for a robot to accurately infer the support relations between objects faraway with various objects in between. In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the complexity of the support relation inference and improves the accuracy. Experiments in both simulation and the real world demonstrate the efficiency and effectiveness of our method.","sentences":["In our daily life, cluttered objects are everywhere, from scattered stationery and books cluttering the table to bowls and plates filling the kitchen sink.","Retrieving a target object from clutters is an essential while challenging skill for robots, for the difficulty of safely manipulating an object without disturbing others, which requires the robot to plan a manipulation sequence and first move away a few other objects supported by the target object step by step.","However, due to the diversity of object configurations (e.g., categories, geometries, locations and poses) and their combinations in clutters, it is difficult for a robot to accurately infer the support relations between objects faraway with various objects in between.","In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the complexity of the support relation inference and improves the accuracy.","Experiments in both simulation and the real world demonstrate the efficiency and effectiveness of our method."],"url":"http://arxiv.org/abs/2406.02283v1","category":"cs.RO"}
{"created":"2024-06-04 12:49:10","title":"Computation-Aware Learning for Stable Control with Gaussian Process","abstract":"In Gaussian Process (GP) dynamical model learning for robot control, particularly for systems constrained by computational resources like small quadrotors equipped with low-end processors, analyzing stability and designing a stable controller present significant challenges. This paper distinguishes between two types of uncertainty within the posteriors of GP dynamical models: the well-documented mathematical uncertainty stemming from limited data and computational uncertainty arising from constrained computational capabilities, which has been largely overlooked in prior research. Our work demonstrates that computational uncertainty, quantified through a probabilistic approximation of the inverse covariance matrix in GP dynamical models, is essential for stable control under computational constraints. We show that incorporating computational uncertainty can prevent overestimating the region of attraction, a safe subset of the state space with asymptotic stability, thus improving system safety. Building on these insights, we propose an innovative controller design methodology that integrates computational uncertainty within a second-order cone programming framework. Simulations of canonical stable control tasks and experiments of quadrotor tracking exhibit the effectiveness of our method under computational constraints.","sentences":["In Gaussian Process (GP) dynamical model learning for robot control, particularly for systems constrained by computational resources like small quadrotors equipped with low-end processors, analyzing stability and designing a stable controller present significant challenges.","This paper distinguishes between two types of uncertainty within the posteriors of GP dynamical models: the well-documented mathematical uncertainty stemming from limited data and computational uncertainty arising from constrained computational capabilities, which has been largely overlooked in prior research.","Our work demonstrates that computational uncertainty, quantified through a probabilistic approximation of the inverse covariance matrix in GP dynamical models, is essential for stable control under computational constraints.","We show that incorporating computational uncertainty can prevent overestimating the region of attraction, a safe subset of the state space with asymptotic stability, thus improving system safety.","Building on these insights, we propose an innovative controller design methodology that integrates computational uncertainty within a second-order cone programming framework.","Simulations of canonical stable control tasks and experiments of quadrotor tracking exhibit the effectiveness of our method under computational constraints."],"url":"http://arxiv.org/abs/2406.02272v1","category":"eess.SY"}
{"created":"2024-06-04 12:33:02","title":"A DAFT Based Unified Waveform Design Framework for High-Mobility Communications","abstract":"With the increasing demand for multi-carrier communication in high-mobility scenarios, it is urgent to design new multi-carrier communication waveforms that can resist large delay-Doppler spreads. Various multi-carrier waveforms in the transform domain were proposed for the fast time-varying channels, including orthogonal time frequency space (OTFS), orthogonal chirp division multiplexing (OCDM), and affine frequency division multiplexing (AFDM). Among these, the AFDM is a strong candidate for its low implementation complexity and ability to achieve optimal diversity. This paper unifies the waveforms based on the discrete affine Fourier transform (DAFT) by using the chirp slope factor \"k\" in the time-frequency representation to construct a unified design framework for high-mobility communications. The design framework is employed to verify that the bit error rate performance of the DAFT-based waveform can be enhanced when the signal-to-noise ratio (SNR) is sufficiently high by adjusting the chirp slope factor \"k\".","sentences":["With the increasing demand for multi-carrier communication in high-mobility scenarios, it is urgent to design new multi-carrier communication waveforms that can resist large delay-Doppler spreads.","Various multi-carrier waveforms in the transform domain were proposed for the fast time-varying channels, including orthogonal time frequency space (OTFS), orthogonal chirp division multiplexing (OCDM), and affine frequency division multiplexing (AFDM).","Among these, the AFDM is a strong candidate for its low implementation complexity and ability to achieve optimal diversity.","This paper unifies the waveforms based on the discrete affine Fourier transform (DAFT) by using the chirp slope factor \"k\" in the time-frequency representation to construct a unified design framework for high-mobility communications.","The design framework is employed to verify that the bit error rate performance of the DAFT-based waveform can be enhanced when the signal-to-noise ratio (SNR) is sufficiently high by adjusting the chirp slope factor \"k\"."],"url":"http://arxiv.org/abs/2406.02262v1","category":"eess.SP"}
{"created":"2024-06-04 12:32:19","title":"Near-Room-Temperature Field-Controllable Exchange Bias in 2D van der Waals Ferromagnet Fe3GaTe2","abstract":"Exchange bias (EB) is a cornerstone of modern magnetic memory and sensing technologies. Its extension to the realm of two-dimensional (2D) van der Waals (vdW) magnets holds promise for revolutionary advancements in miniaturized and efficient atomic spintronic devices. However, the blocking temperature of EB in 2D vdW magnets is currently well below room temperature ~130 K. This study reports a robust EB phenomenon in Fe3GaTe2 thin-layer devices, which significantly increases the blocking temperature to a near-room-temperature record of 280 K. Both the bias direction and magnitude can be isothermally tuned by adjusting the field sweep range, in striking contrast to the conventional EB in ferromagnetic/antiferromagnetic (FM/AFM) bilayers. We propose an exchange spring model in which crystal defects with higher coercivity act as the pivotal pinning source for the observed EB phenomenon, deviating from the conventional FM/AFM interface mechanism. Cumulative growth of minor loops and multiple magnetization reversal paths are observed in field cycles below the saturation field, consistent with the hard FM defects behavior of our exchange spring model. These findings provide insights into the complex magnetic order in 2D ferromagnets and open new avenues for developing practical ultrathin vdW spintronic devices with EB-like properties at room temperature.","sentences":["Exchange bias (EB) is a cornerstone of modern magnetic memory and sensing technologies.","Its extension to the realm of two-dimensional (2D) van der Waals (vdW) magnets holds promise for revolutionary advancements in miniaturized and efficient atomic spintronic devices.","However, the blocking temperature of EB in 2D vdW magnets is currently well below room temperature ~130","K. This study reports a robust EB phenomenon in Fe3GaTe2 thin-layer devices, which significantly increases the blocking temperature to a near-room-temperature record of 280 K. Both the bias direction and magnitude can be isothermally tuned by adjusting the field sweep range, in striking contrast to the conventional EB in ferromagnetic/antiferromagnetic (FM/AFM) bilayers.","We propose an exchange spring model in which crystal defects with higher coercivity act as the pivotal pinning source for the observed EB phenomenon, deviating from the conventional FM/AFM interface mechanism.","Cumulative growth of minor loops and multiple magnetization reversal paths are observed in field cycles below the saturation field, consistent with the hard FM defects behavior of our exchange spring model.","These findings provide insights into the complex magnetic order in 2D ferromagnets and open new avenues for developing practical ultrathin vdW spintronic devices with EB-like properties at room temperature."],"url":"http://arxiv.org/abs/2406.02260v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-04 12:17:00","title":"A novel measurement method for SiPM external crosstalk probability at low temperature","abstract":"Silicon photomultipliers (SiPMs) are being considered as potential replacements for conventional photomultiplier tubes (PMTs). However, a significant disadvantage of SiPMs is crosstalk (CT), wherein photons propagate through other pixels, resulting in secondary avalanches. CT can be categorized into internal crosstalk and external crosstalk based on whether the secondary avalanche occurs within the same SiPM or a different one. Numerous methods exist for quantitatively estimating the percentage of internal crosstalk (iCT). However, external crosstalk (eCT) has not been extensively studied.   This article presents a novel measurement method for the probability of emitting an external crosstalk photon during a single pixel avalanche, using a setup involving two identical SiPMs facing each other, and without the need for complex optical designs. The entire apparatus is enclosed within a stainless steel chamber, functioning as a light-tight enclosure, and maintained at liquid nitrogen temperature. The experimental setup incorporates two Sensl J-60035 SiPM chips along with two 0.5-inch Hamamatsu Photonics (HPK) VUV4 S13370-6050CN SiPM arrays. The findings show a linear relationship between the probability of emitting an external crosstalk photon and the SiPM overvoltage for both SiPM samples. Surprisingly, this novel measurement method also rovides measurements of the SiPM photon detection efficiency (PDE) for eCT photons at low temperature.","sentences":["Silicon photomultipliers (SiPMs) are being considered as potential replacements for conventional photomultiplier tubes (PMTs).","However, a significant disadvantage of SiPMs is crosstalk (CT), wherein photons propagate through other pixels, resulting in secondary avalanches.","CT can be categorized into internal crosstalk and external crosstalk based on whether the secondary avalanche occurs within the same SiPM or a different one.","Numerous methods exist for quantitatively estimating the percentage of internal crosstalk (iCT).","However, external crosstalk (eCT) has not been extensively studied.   ","This article presents a novel measurement method for the probability of emitting an external crosstalk photon during a single pixel avalanche, using a setup involving two identical SiPMs facing each other, and without the need for complex optical designs.","The entire apparatus is enclosed within a stainless steel chamber, functioning as a light-tight enclosure, and maintained at liquid nitrogen temperature.","The experimental setup incorporates two Sensl J-60035 SiPM chips along with two 0.5-inch Hamamatsu Photonics (HPK) VUV4 S13370-6050CN SiPM arrays.","The findings show a linear relationship between the probability of emitting an external crosstalk photon and the SiPM overvoltage for both SiPM samples.","Surprisingly, this novel measurement method also rovides measurements of the SiPM photon detection efficiency (PDE) for eCT photons at low temperature."],"url":"http://arxiv.org/abs/2406.02249v1","category":"physics.ins-det"}
{"created":"2024-06-04 12:03:41","title":"Femtoscopy at NA61/SHINE using symmetric L\u00e9vy sources in central $^{40}$Ar+$^{45}$Sc from 40$A$ GeV/$c$ to 150$A$ GeV/$c$","abstract":"In the recent decades of high energy physics research, it was demonstrated that strongly interacting quark-gluon plasma (sQGP) is created in ultra-relativistic nucleus-nucleus collisions. Investigation and understanding of properties of the hadronic matter is among the important goals of NA61/SHINE collaboration at CERN SPS. Mapping of the phase diagram is achieved by varying the collision energy (5 GeV $<\\sqrt{s_{\\textrm{NN}}}<$17 GeV) and by changing the collision system (p+p, p+Pb, Be+Be, Ar+Sc, Xe+La, Pb+Pb). We report on the measurement of femtoscopic correlations in intermediate system at intermediate SPS energies. Interpreting the results of measurements within the symmetric L\\'evy source formalism, we discuss the values of L\\'evy source parameters as a function of average pair transverse mass. One of the physical parameters is particularly important, the L\\'evy exponent $\\alpha$, which describes the shape of the source and may be related to the critical exponent $\\eta$ in the proximity of the critical point. Therefore, measuring it may shed light on the location of the critical endpoint of the QCD phase diagram.","sentences":["In the recent decades of high energy physics research, it was demonstrated that strongly interacting quark-gluon plasma (sQGP) is created in ultra-relativistic nucleus-nucleus collisions.","Investigation and understanding of properties of the hadronic matter is among the important goals of NA61/SHINE collaboration at CERN SPS.","Mapping of the phase diagram is achieved by varying the collision energy (5 GeV $<\\sqrt{s_{\\textrm{NN}}}<$17 GeV) and by changing the collision system (p+p, p+Pb, Be+Be, Ar+Sc, Xe+La, Pb+Pb).","We report on the measurement of femtoscopic correlations in intermediate system at intermediate SPS energies.","Interpreting the results of measurements within the symmetric L\\'evy source formalism, we discuss the values of L\\'evy source parameters as a function of average pair transverse mass.","One of the physical parameters is particularly important, the L\\'evy exponent $\\alpha$, which describes the shape of the source and may be related to the critical exponent $\\eta$ in the proximity of the critical point.","Therefore, measuring it may shed light on the location of the critical endpoint of the QCD phase diagram."],"url":"http://arxiv.org/abs/2406.02242v1","category":"nucl-ex"}
{"created":"2024-06-04 11:59:12","title":"Local Properties of Reed-Solomon Codes and Random Linear Codes","abstract":"We study the list-decodability and list-recoverability of two code ensembles: random linear codes and random Reed-Solomon codes. Inspired by the existing research about local properties of random code ensembles over small alphabets, we develop a new framework to study a similar family of properties over larger alphabets, such as in the case of Reed-Solomon codes. We introduce the notion of local coordinate-wise linear (LCL) properties, which encompasses various natural properties including list-decodability and list-recoverability.   Our main contributions are twofold: (1) we establish a threshold theorem for LCL properties of random linear codes, identifying a critical rate below which these codes almost surely satisfy a given property and above which they almost surely do not; and (2) we demonstrate a reduction from random linear codes to random Reed-Solomon codes, showing that Reed-Solomon codes inherit the LCL properties of linear codes with similar rates.   Our results imply that conjectures about the list-recoverability of random linear codes can be extended to random Reed-Solomon codes, potentially up to optimal bounds. Additionally, they provide a potential avenue to prove these list-recovery conjectures for random linear codes. Furthermore, our approach provides a more elementary proof of recent theorems on list-decodability for both random linear codes and random Reed-Solomon codes, avoiding reliance on complex external results.","sentences":["We study the list-decodability and list-recoverability of two code ensembles: random linear codes and random Reed-Solomon codes.","Inspired by the existing research about local properties of random code ensembles over small alphabets, we develop a new framework to study a similar family of properties over larger alphabets, such as in the case of Reed-Solomon codes.","We introduce the notion of local coordinate-wise linear (LCL) properties, which encompasses various natural properties including list-decodability and list-recoverability.   ","Our main contributions are twofold: (1) we establish a threshold theorem for LCL properties of random linear codes, identifying a critical rate below which these codes almost surely satisfy a given property and above which they almost surely do not; and (2) we demonstrate a reduction from random linear codes to random Reed-Solomon codes, showing that Reed-Solomon codes inherit the LCL properties of linear codes with similar rates.   ","Our results imply that conjectures about the list-recoverability of random linear codes can be extended to random Reed-Solomon codes, potentially up to optimal bounds.","Additionally, they provide a potential avenue to prove these list-recovery conjectures for random linear codes.","Furthermore, our approach provides a more elementary proof of recent theorems on list-decodability for both random linear codes and random Reed-Solomon codes, avoiding reliance on complex external results."],"url":"http://arxiv.org/abs/2406.02238v1","category":"cs.IT"}
{"created":"2024-06-04 11:56:37","title":"Demonstration of superior communication through thermodynamically free channels in an optical quantum switch","abstract":"The release of causal structure of physical events from a well-defined order to an indefinite one stimulates remarkable enhancements in various quantum information tasks. Some of these advantages, however, are questioned for the ambiguous role of the control system in the quantum switch that is an experimentally realized process with indefinite causal structure. In communications, for example, not only the superposition of alternative causal orders, but also the superposition of alternative trajectories can accelerate information transmissions. Here, we follow the proposal of Liu et al. [Phys. Rev. Lett. 129, 230604 (2022)], and examine the information enhancement effect of indefinite causal orders with the toolkit of thermodynamics in a photonic platform. Specifically, we simulate the thermal interaction between a system qubit and two heat baths embedded in a quantum switch by implementing the corresponding switched thermal channels. Although its action on the system qubit only is thermally free, our results suggest that the quantum switch should be seen as a resource when the control qubit is also considered. Moreover, we characterize the non-Markovian property in this scenario by measuring the information backflows from the heat baths to the system qubit.","sentences":["The release of causal structure of physical events from a well-defined order to an indefinite one stimulates remarkable enhancements in various quantum information tasks.","Some of these advantages, however, are questioned for the ambiguous role of the control system in the quantum switch that is an experimentally realized process with indefinite causal structure.","In communications, for example, not only the superposition of alternative causal orders, but also the superposition of alternative trajectories can accelerate information transmissions.","Here, we follow the proposal of Liu et al.","[Phys. Rev. Lett.","129, 230604 (2022)], and examine the information enhancement effect of indefinite causal orders with the toolkit of thermodynamics in a photonic platform.","Specifically, we simulate the thermal interaction between a system qubit and two heat baths embedded in a quantum switch by implementing the corresponding switched thermal channels.","Although its action on the system qubit only is thermally free, our results suggest that the quantum switch should be seen as a resource when the control qubit is also considered.","Moreover, we characterize the non-Markovian property in this scenario by measuring the information backflows from the heat baths to the system qubit."],"url":"http://arxiv.org/abs/2406.02236v1","category":"quant-ph"}
{"created":"2024-06-04 11:55:11","title":"Towards Out-of-Distribution Detection in Vocoder Recognition via Latent Feature Reconstruction","abstract":"Advancements in synthesized speech have created a growing threat of impersonation, making it crucial to develop deepfake algorithm recognition. One significant aspect is out-of-distribution (OOD) detection, which has gained notable attention due to its important role in deepfake algorithm recognition. However, most of the current approaches for detecting OOD in deepfake algorithm recognition rely on probability-score or classified-distance, which may lead to limitations in the accuracy of the sample at the edge of the threshold. In this study, we propose a reconstruction-based detection approach that employs an autoencoder architecture to compress and reconstruct the acoustic feature extracted from a pre-trained WavLM model. Each acoustic feature belonging to a specific vocoder class is only aptly reconstructed by its corresponding decoder. When none of the decoders can satisfactorily reconstruct a feature, it is classified as an OOD sample. To enhance the distinctiveness of the reconstructed features by each decoder, we incorporate contrastive learning and an auxiliary classifier to further constrain the reconstructed feature. Experiments demonstrate that our proposed approach surpasses baseline systems by a relative margin of 10\\% in the evaluation dataset. Ablation studies further validate the effectiveness of both the contrastive constraint and the auxiliary classifier within our proposed approach.","sentences":["Advancements in synthesized speech have created a growing threat of impersonation, making it crucial to develop deepfake algorithm recognition.","One significant aspect is out-of-distribution (OOD) detection, which has gained notable attention due to its important role in deepfake algorithm recognition.","However, most of the current approaches for detecting OOD in deepfake algorithm recognition rely on probability-score or classified-distance, which may lead to limitations in the accuracy of the sample at the edge of the threshold.","In this study, we propose a reconstruction-based detection approach that employs an autoencoder architecture to compress and reconstruct the acoustic feature extracted from a pre-trained WavLM model.","Each acoustic feature belonging to a specific vocoder class is only aptly reconstructed by its corresponding decoder.","When none of the decoders can satisfactorily reconstruct a feature, it is classified as an OOD sample.","To enhance the distinctiveness of the reconstructed features by each decoder, we incorporate contrastive learning and an auxiliary classifier to further constrain the reconstructed feature.","Experiments demonstrate that our proposed approach surpasses baseline systems by a relative margin of 10\\% in the evaluation dataset.","Ablation studies further validate the effectiveness of both the contrastive constraint and the auxiliary classifier within our proposed approach."],"url":"http://arxiv.org/abs/2406.02233v1","category":"eess.AS"}
{"created":"2024-06-04 11:46:56","title":"Hybrid Quantum-Classical Neural Network for LAB Color Space Image Classification","abstract":"Parameterized quantum circuits (PQCs) are essential in many variational quantum algorithms. Quantum convolutional neural networks (QCNNs), which are structurally similar to classical convolutional neural networks, possess the capability to extract informative features and have shown significant effectiveness in image classification tasks. To achieve higher image classification accuracy and reduce the number of training parameters, modifications to the structure of PQCs or hybrid quantum-classical convolutional neural network (HQCCNN) models are typically employed. The image datasets used in these learning models are usually RGB images. We investigate the effects of different color spaces to explore the potential for reducing the resources required for quantum computation. By utilizing a simple HQCCNN model with existing PQCs, we analyze the performance of each channel in various color space images. Experimental results reveal that two channels consistently exhibit higher classification accuracy in images from different color spaces than the third. Specifically, the L channel of LAB color space images achieves superior classification accuracy when employing a more complex PQC. Additionally, PQCs utilizing controlled rotation X-gates outperform those using controlled selection Z-gates in this classification task.","sentences":["Parameterized quantum circuits (PQCs) are essential in many variational quantum algorithms.","Quantum convolutional neural networks (QCNNs), which are structurally similar to classical convolutional neural networks, possess the capability to extract informative features and have shown significant effectiveness in image classification tasks.","To achieve higher image classification accuracy and reduce the number of training parameters, modifications to the structure of PQCs or hybrid quantum-classical convolutional neural network (HQCCNN) models are typically employed.","The image datasets used in these learning models are usually RGB images.","We investigate the effects of different color spaces to explore the potential for reducing the resources required for quantum computation.","By utilizing a simple HQCCNN model with existing PQCs, we analyze the performance of each channel in various color space images.","Experimental results reveal that two channels consistently exhibit higher classification accuracy in images from different color spaces than the third.","Specifically, the L channel of LAB color space images achieves superior classification accuracy when employing a more complex PQC.","Additionally, PQCs utilizing controlled rotation X-gates outperform those using controlled selection Z-gates in this classification task."],"url":"http://arxiv.org/abs/2406.02229v1","category":"quant-ph"}
{"created":"2024-06-04 11:31:00","title":"Towards an Extensible Model-Based Digital Twin Framework for Space Launch Vehicles","abstract":"The concept of Digital Twin (DT) is increasingly applied to systems on different levels of abstraction across domains, to support monitoring, analysis, diagnosis, decision making and automated control. Whilst the interest in applying DT is growing, the definition of DT is unclear, neither is there a clear pathway to develop DT to fully realise its capacities. In this paper, we revise the concept of DT and its categorisation. We propose a DT maturity matrix, based on which we propose a model-based DT development methodology. We also discuss how model-based tools can be used to support the methodology and present our own supporting tool. We report our preliminary findings with a discussion on a case study, in which we use our proposed methodology and our supporting tool to develop an extensible DT platform for the assurance of Electrical and Electronics systems of space launch vehicles.","sentences":["The concept of Digital Twin (DT) is increasingly applied to systems on different levels of abstraction across domains, to support monitoring, analysis, diagnosis, decision making and automated control.","Whilst the interest in applying DT is growing, the definition of DT is unclear, neither is there a clear pathway to develop DT to fully realise its capacities.","In this paper, we revise the concept of DT and its categorisation.","We propose a DT maturity matrix, based on which we propose a model-based DT development methodology.","We also discuss how model-based tools can be used to support the methodology and present our own supporting tool.","We report our preliminary findings with a discussion on a case study, in which we use our proposed methodology and our supporting tool to develop an extensible DT platform for the assurance of Electrical and Electronics systems of space launch vehicles."],"url":"http://arxiv.org/abs/2406.02222v1","category":"cs.SE"}
{"created":"2024-06-04 11:24:07","title":"Stochastic Thermodynamics of Micromagnetics with Spin Torque","abstract":"In this work, we study the stochastic dynamics of micro-magnetics interacting with a spin-current torque. We extend the previously constructed stochastic Landau-Lifshitz equation to the case with spin-current torque, and verify the conditions of detailed balance. Then we construct various thermodynamics quantities such as work and heat, and prove the second law of thermodynamics. Due to the existence of spin-torque and the asymmetry of the kinetic matrix, a novel effect of entropy pumping shows up. As a consequence, the system may behave as a heat engine which constantly transforms heat into magnetic work. Finally, we derive a fluctuation theorem for the joint probability density function of the pumped entropy and the total work, and verify it using numerical simulations.","sentences":["In this work, we study the stochastic dynamics of micro-magnetics interacting with a spin-current torque.","We extend the previously constructed stochastic Landau-Lifshitz equation to the case with spin-current torque, and verify the conditions of detailed balance.","Then we construct various thermodynamics quantities such as work and heat, and prove the second law of thermodynamics.","Due to the existence of spin-torque and the asymmetry of the kinetic matrix, a novel effect of entropy pumping shows up.","As a consequence, the system may behave as a heat engine which constantly transforms heat into magnetic work.","Finally, we derive a fluctuation theorem for the joint probability density function of the pumped entropy and the total work, and verify it using numerical simulations."],"url":"http://arxiv.org/abs/2406.02220v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-04 11:19:02","title":"Projection scheme for a perfect plasticity model with a time-dependent constraint set","abstract":"This paper introduces a new numerical scheme for a system that includes evolution equations describing a perfect plasticity model with a time-dependent yield surface. We demonstrate that the solution to the proposed scheme is stable under suitable norms. Moreover, the stability leads to the existence of an exact solution, and we also prove that the solution to the proposed scheme converges strongly to the exact solution under suitable norms.","sentences":["This paper introduces a new numerical scheme for a system that includes evolution equations describing a perfect plasticity model with a time-dependent yield surface.","We demonstrate that the solution to the proposed scheme is stable under suitable norms.","Moreover, the stability leads to the existence of an exact solution, and we also prove that the solution to the proposed scheme converges strongly to the exact solution under suitable norms."],"url":"http://arxiv.org/abs/2406.02218v1","category":"math.AP"}
{"created":"2024-06-04 11:17:02","title":"NordIQuEst: the Nordic-Estonian Quantum Computing e-Infrastructure Quest","abstract":"This paper presents the Nordic-Estonian Quantum Computing e-Infrastructure Quest - NordIQuEst - an international collaboration of scientific and academic organizations from Denmark, Estonia, Finland, Norway, and Sweden, working together to develop a hybrid High-Performance and Quantum Computing (HPC+QC) infrastructure. The project leverages existing and upcoming classical high-performance computing and quantum computing systems, facilitating the development of interconnected systems. Our effort pioneers a forward-looking architecture for both hardware and software capabilities, representing an early-stage development in hybrid computing infrastructure. Here, we detail the outline of the initiative, summarizing the progress since the project outset, and describing the framework established. Moreover, we identify the crucial challenges encountered, and potential strategies employed to address them.","sentences":["This paper presents the Nordic-Estonian Quantum Computing e-Infrastructure Quest - NordIQuEst - an international collaboration of scientific and academic organizations from Denmark, Estonia, Finland, Norway, and Sweden, working together to develop a hybrid High-Performance and Quantum Computing (HPC+QC) infrastructure.","The project leverages existing and upcoming classical high-performance computing and quantum computing systems, facilitating the development of interconnected systems.","Our effort pioneers a forward-looking architecture for both hardware and software capabilities, representing an early-stage development in hybrid computing infrastructure.","Here, we detail the outline of the initiative, summarizing the progress since the project outset, and describing the framework established.","Moreover, we identify the crucial challenges encountered, and potential strategies employed to address them."],"url":"http://arxiv.org/abs/2406.02216v1","category":"cs.DC"}
{"created":"2024-06-04 11:07:31","title":"An Open and Reconfigurable User Interface to Manage Complex ROS-based Robotic Systems","abstract":"The Robot Operating System (ROS) has significantly gained popularity among robotic engineers and researchers over the past five years, primarily due to its powerful infrastructure for node communication, which enables developers to build modular and large robotic applications. However, ROS presents a steep learning curve and lacks the intuitive usability of vendor-specific robotic Graphical User Interfaces (GUIs). Moreover, its modular and distributed nature complicates the control and monitoring of extensive systems, even for advanced users. To address these challenges, this paper proposes a highly adaptable and reconfigurable web-based GUI for intuitively controlling, monitoring, and configuring complex ROS-based robotic systems. The GUI leverages ROSBridge and roslibjs to ensure seamless communication with ROS systems via topics and services. Designed as a versatile platform, the GUI allows for the selective incorporation of modular features to accommodate diverse robotic systems and applications. An initial set of commonly used features in robotic applications is presented. To demonstrate its reconfigurability, the GUI was customized and tested for four industrial use cases, receiving positive feedback. The project's repository has been made publicly available to support the robotics community and lower the entry barrier for ROS in industrial applications.","sentences":["The Robot Operating System (ROS) has significantly gained popularity among robotic engineers and researchers over the past five years, primarily due to its powerful infrastructure for node communication, which enables developers to build modular and large robotic applications.","However, ROS presents a steep learning curve and lacks the intuitive usability of vendor-specific robotic Graphical User Interfaces (GUIs).","Moreover, its modular and distributed nature complicates the control and monitoring of extensive systems, even for advanced users.","To address these challenges, this paper proposes a highly adaptable and reconfigurable web-based GUI for intuitively controlling, monitoring, and configuring complex ROS-based robotic systems.","The GUI leverages ROSBridge and roslibjs to ensure seamless communication with ROS systems via topics and services.","Designed as a versatile platform, the GUI allows for the selective incorporation of modular features to accommodate diverse robotic systems and applications.","An initial set of commonly used features in robotic applications is presented.","To demonstrate its reconfigurability, the GUI was customized and tested for four industrial use cases, receiving positive feedback.","The project's repository has been made publicly available to support the robotics community and lower the entry barrier for ROS in industrial applications."],"url":"http://arxiv.org/abs/2406.02210v1","category":"cs.RO"}
{"created":"2024-06-04 11:02:39","title":"Nonlinear Model Predictive Control for Preview-Based Traction Control","abstract":"This study presents a nonlinear model predictive control (NMPC) formulation for preview-based traction control, which uses the information on the expected tire-road friction coefficient ahead to enhance the wheel slip control performance, in the context of connected vehicles with V2X features. Proof-of-concept experiments on an electric vehicle prototype highlight the real-time capability of the controller, and the wheel slip control performance improvement brought by the tire-road friction coefficient preview. Finally, an experimentally validated simulation model is used in sensitivity analyses, to evaluate the performance benefit of the preview-based controller for different dynamic characteristics (e.g., time constant and pure time delays) of the electric powertrains.","sentences":["This study presents a nonlinear model predictive control (NMPC) formulation for preview-based traction control, which uses the information on the expected tire-road friction coefficient ahead to enhance the wheel slip control performance, in the context of connected vehicles with V2X features.","Proof-of-concept experiments on an electric vehicle prototype highlight the real-time capability of the controller, and the wheel slip control performance improvement brought by the tire-road friction coefficient preview.","Finally, an experimentally validated simulation model is used in sensitivity analyses, to evaluate the performance benefit of the preview-based controller for different dynamic characteristics (e.g., time constant and pure time delays) of the electric powertrains."],"url":"http://arxiv.org/abs/2406.02206v1","category":"eess.SY"}
{"created":"2024-06-04 10:56:30","title":"Dynamics and non-integrability of the double spring pendulum","abstract":"This paper investigates the dynamics and integrability of the double spring pendulum, which has great importance in studying nonlinear dynamics, chaos, and bifurcations. Being a Hamiltonian system with three degrees of freedom, its analysis presents a significant challenge. To gain insight into the system's dynamics, we employ various numerical methods, including Lyapunov exponents spectra, phase-parametric diagrams, and Poincar\\'e cross-sections. The novelty of our work lies in the integration of these three numerical methods into one powerful tool. We provide a comprehensive understanding of the system's dynamics by identifying parameter values or initial conditions that lead to hyper-chaotic, chaotic, quasi-periodic, and periodic motion, which is a novel contribution in the context of Hamiltonian systems. In the absence of gravitational potential, the system exhibits $S^1$ symmetry, and the presence of an additional first integral was identified using Lyapunov exponents diagrams. We demonstrate the effective utilisation of Lyapunov exponents as a potential indicator of first integrals and integrable dynamics. The numerical analysis is complemented by an analytical proof regarding the non-integrability of the system. This proof relies on the analysis of properties of the differential Galois group of variational equations along specific solutions of the system. To facilitate this analysis, we utilised a newly developed extension of the Kovacic algorithm specifically designed for fourth-order differential equations. Overall, our study sheds light on the intricate dynamics and integrability of the double spring pendulum, offering new insights and methodologies for further research in this field.   The article has been published in JSV, and the final version is available at this link: https://doi.org/10.1016/j.jsv.2024.118550","sentences":["This paper investigates the dynamics and integrability of the double spring pendulum, which has great importance in studying nonlinear dynamics, chaos, and bifurcations.","Being a Hamiltonian system with three degrees of freedom, its analysis presents a significant challenge.","To gain insight into the system's dynamics, we employ various numerical methods, including Lyapunov exponents spectra, phase-parametric diagrams, and Poincar\\'e cross-sections.","The novelty of our work lies in the integration of these three numerical methods into one powerful tool.","We provide a comprehensive understanding of the system's dynamics by identifying parameter values or initial conditions that lead to hyper-chaotic, chaotic, quasi-periodic, and periodic motion, which is a novel contribution in the context of Hamiltonian systems.","In the absence of gravitational potential, the system exhibits $S^1$ symmetry, and the presence of an additional first integral was identified using Lyapunov exponents diagrams.","We demonstrate the effective utilisation of Lyapunov exponents as a potential indicator of first integrals and integrable dynamics.","The numerical analysis is complemented by an analytical proof regarding the non-integrability of the system.","This proof relies on the analysis of properties of the differential Galois group of variational equations along specific solutions of the system.","To facilitate this analysis, we utilised a newly developed extension of the Kovacic algorithm specifically designed for fourth-order differential equations.","Overall, our study sheds light on the intricate dynamics and integrability of the double spring pendulum, offering new insights and methodologies for further research in this field.   ","The article has been published in JSV, and the final version is available at this link: https://doi.org/10.1016/j.jsv.2024.118550"],"url":"http://arxiv.org/abs/2406.02200v1","category":"nlin.CD"}
{"created":"2024-06-04 10:51:12","title":"A Pipelined Memristive Neural Network Analog-to-Digital Converter","abstract":"With the advent of high-speed, high-precision, and low-power mixed-signal systems, there is an ever-growing demand for accurate, fast, and energy-efficient analog-to-digital (ADCs) and digital-to-analog converters (DACs). Unfortunately, with the downscaling of CMOS technology, modern ADCs trade off speed, power and accuracy. Recently, memristive neuromorphic architectures of four-bit ADC/DAC have been proposed. Such converters can be trained in real-time using machine learning algorithms, to break through the speedpower-accuracy trade-off while optimizing the conversion performance for different applications. However, scaling such architectures above four bits is challenging. This paper proposes a scalable and modular neural network ADC architecture based on a pipeline of four-bit converters, preserving their inherent advantages in application reconfiguration, mismatch selfcalibration, noise tolerance, and power optimization, while approaching higher resolution and throughput in penalty of latency. SPICE evaluation shows that an 8-bit pipelined ADC achieves 0.18 LSB INL, 0.20 LSB DNL, 7.6 ENOB, and 0.97 fJ/conv FOM. This work presents a significant step towards the realization of large-scale neuromorphic data converters.","sentences":["With the advent of high-speed, high-precision, and low-power mixed-signal systems, there is an ever-growing demand for accurate, fast, and energy-efficient analog-to-digital (ADCs) and digital-to-analog converters (DACs).","Unfortunately, with the downscaling of CMOS technology, modern ADCs trade off speed, power and accuracy.","Recently, memristive neuromorphic architectures of four-bit ADC/DAC have been proposed.","Such converters can be trained in real-time using machine learning algorithms, to break through the speedpower-accuracy trade-off while optimizing the conversion performance for different applications.","However, scaling such architectures above four bits is challenging.","This paper proposes a scalable and modular neural network ADC architecture based on a pipeline of four-bit converters, preserving their inherent advantages in application reconfiguration, mismatch selfcalibration, noise tolerance, and power optimization, while approaching higher resolution and throughput in penalty of latency.","SPICE evaluation shows that an 8-bit pipelined ADC achieves 0.18 LSB INL, 0.20 LSB DNL, 7.6 ENOB, and 0.97 fJ/conv FOM.","This work presents a significant step towards the realization of large-scale neuromorphic data converters."],"url":"http://arxiv.org/abs/2406.02197v1","category":"eess.SY"}
{"created":"2024-06-04 10:47:43","title":"Local versus global environment: the suppression of star formation in the vicinity of galaxy clusters","abstract":"In order to examine where, how and why the quenching of star formation begins in the outskirts of galaxy clusters, we investigate the de-projected radial distribution of a large sample of quenched and star-forming galaxies (SFGs) out to $30R_{500}$ around clusters. We identify the SFG sample using radio continuum emission from the Low-Frequency Array Two-metre Sky Survey. We find that the SFG fraction starts to decrease from the field fraction as far out as $10R_{500}$, well outside the virial radius of the clusters. We investigate how the SFG fraction depends on both large-scale and local environments, using radial distance from a cluster to characterise the former, and distance from 5th nearest neighbour for the latter. The fraction of SFGs in high-density local environments is consistently lower than that found in low-density local environments, indicating that galaxies' immediate surroundings have a significant impact on star formation. However, for high-mass galaxies -- and low mass galaxies to a lesser extent -- high-density local environments appear to act as a protective barrier for those SFGs that survived this pre-processing, shielding them from the external quenching mechanisms of the cluster outskirts. For those galaxies that are not in a dense local environment, the global environment causes the fraction of SFGs to decrease toward the cluster centre in a manner that is independent of galaxy mass. Thus, the fraction of SFGs depends on quite a complex interplay between the galaxies' mass, their local environment, and their more global cluster-centric distance.","sentences":["In order to examine where, how and why the quenching of star formation begins in the outskirts of galaxy clusters, we investigate the de-projected radial distribution of a large sample of quenched and star-forming galaxies (SFGs) out to $30R_{500}$ around clusters.","We identify the SFG sample using radio continuum emission from the Low-Frequency Array Two-metre Sky Survey.","We find that the SFG fraction starts to decrease from the field fraction as far out as $10R_{500}$, well outside the virial radius of the clusters.","We investigate how the SFG fraction depends on both large-scale and local environments, using radial distance from a cluster to characterise the former, and distance from 5th nearest neighbour for the latter.","The fraction of SFGs in high-density local environments is consistently lower than that found in low-density local environments, indicating that galaxies' immediate surroundings have a significant impact on star formation.","However, for high-mass galaxies -- and low mass galaxies to a lesser extent -- high-density local environments appear to act as a protective barrier for those SFGs that survived this pre-processing, shielding them from the external quenching mechanisms of the cluster outskirts.","For those galaxies that are not in a dense local environment, the global environment causes the fraction of SFGs to decrease toward the cluster centre in a manner that is independent of galaxy mass.","Thus, the fraction of SFGs depends on quite a complex interplay between the galaxies' mass, their local environment, and their more global cluster-centric distance."],"url":"http://arxiv.org/abs/2406.02196v1","category":"astro-ph.GA"}
{"created":"2024-06-04 10:45:12","title":"Electromechanical response of saddle points in twisted hBN moir\u00e9 superlattices","abstract":"In twisted layered materials (t-LMs), an inter-layer rotation can break inversion symmetry and create an interfacial array of staggered out-of-plane polarization due to AB/BA stacking registries. This symmetry breaking can also trigger the formation of edge in-plane polarizations localized along the perimeter of AB/BA regions (i.e., saddle point domains). However, a comprehensive experimental investigation of these features is still lacking. Here, we use piezo force microscopy to probe the electromechanical behavior of twisted hexagonal boron nitride (t-hBN). For a parallel stacking alignment of t-hBN, we reveal very narrow (width ~ 20 nm) saddle point polarizations, which we also measure in the anti-parallel configuration. These localized polarizations can still be found on a multiply-stacked t-hBN structure, determining the formation of a double moir\\'e. We also visualize a t-hBN moir\\'e superlattice in the topography maps with atomic force microscopy, related to the strain accumulated at the saddle point domains. Our findings imply that polarizations in t-hBN do not only point in the out-of-plane direction, but also show an in-plane component, giving rise to a much more complex 3D polarization field.","sentences":["In twisted layered materials (t-LMs), an inter-layer rotation can break inversion symmetry and create an interfacial array of staggered out-of-plane polarization due to AB/BA stacking registries.","This symmetry breaking can also trigger the formation of edge in-plane polarizations localized along the perimeter of AB/BA regions (i.e., saddle point domains).","However, a comprehensive experimental investigation of these features is still lacking.","Here, we use piezo force microscopy to probe the electromechanical behavior of twisted hexagonal boron nitride (t-hBN).","For a parallel stacking alignment of t-hBN, we reveal very narrow (width ~ 20 nm) saddle point polarizations, which we also measure in the anti-parallel configuration.","These localized polarizations can still be found on a multiply-stacked t-hBN structure, determining the formation of a double moir\\'e.","We also visualize a t-hBN moir\\'e superlattice in the topography maps with atomic force microscopy, related to the strain accumulated at the saddle point domains.","Our findings imply that polarizations in t-hBN do not only point in the out-of-plane direction, but also show an in-plane component, giving rise to a much more complex 3D polarization field."],"url":"http://arxiv.org/abs/2406.02195v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 10:34:40","title":"Age of Trust (AoT): A Continuous Verification Framework for Wireless Networks","abstract":"Zero Trust is a new security vision for 6G networks that emphasises the philosophy of never trust and always verify. However, there is a fundamental trade-off between the wireless transmission efficiency and the trust level, which is reflected by the verification interval and its adaptation strategy. More importantly, the mathematical framework to characterise the trust level of the adaptive verification strategy is still missing. Inspired by this vision, we propose a concept called age of trust (AoT) to capture the characteristics of the trust level degrading over time, with the definition of the time elapsed since the last verification of the target user's trust plus the initial age, which depends on the trust level evaluated at that verification. The higher the trust level, the lower the initial age. To evaluate the trust level in the long term, the average AoT is used. We then investigate how to find a compromise between average AoT and wireless transmission efficiency with limited resources. In particular, we address the bi-objective optimization (BOO) problem between average AoT and throughput over a single link with arbitrary service process, where the identity of the receiver is constantly verified, and we devise a periodic verification scheme and a Q-learning-based scheme for constant process and random process, respectively. We also tackle the BOO problem in a multiple random access scenario, where a trust-enhanced frame-slotted ALOHA is designed. Finally, the numerical results show that our proposals can achieve a fair compromise between trust level and wireless transmission efficiency, and thus have a wide application prospect in various zero-trust architectures.","sentences":["Zero Trust is a new security vision for 6G networks that emphasises the philosophy of never trust and always verify.","However, there is a fundamental trade-off between the wireless transmission efficiency and the trust level, which is reflected by the verification interval and its adaptation strategy.","More importantly, the mathematical framework to characterise the trust level of the adaptive verification strategy is still missing.","Inspired by this vision, we propose a concept called age of trust (AoT) to capture the characteristics of the trust level degrading over time, with the definition of the time elapsed since the last verification of the target user's trust plus the initial age, which depends on the trust level evaluated at that verification.","The higher the trust level, the lower the initial age.","To evaluate the trust level in the long term, the average AoT is used.","We then investigate how to find a compromise between average AoT and wireless transmission efficiency with limited resources.","In particular, we address the bi-objective optimization (BOO) problem between average AoT and throughput over a single link with arbitrary service process, where the identity of the receiver is constantly verified, and we devise a periodic verification scheme and a Q-learning-based scheme for constant process and random process, respectively.","We also tackle the BOO problem in a multiple random access scenario, where a trust-enhanced frame-slotted ALOHA is designed.","Finally, the numerical results show that our proposals can achieve a fair compromise between trust level and wireless transmission efficiency, and thus have a wide application prospect in various zero-trust architectures."],"url":"http://arxiv.org/abs/2406.02190v1","category":"eess.SY"}
{"created":"2024-06-04 10:12:09","title":"AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields","abstract":"We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors.","sentences":["We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields.","Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds.","This versatility eliminates the need for patching and allows efficient processing of diverse geometries.","The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs.","By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training.","AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors."],"url":"http://arxiv.org/abs/2406.02176v1","category":"cs.LG"}
{"created":"2024-06-04 10:11:46","title":"Branches: A Fast Dynamic Programming and Branch & Bound Algorithm for Optimal Decision Trees","abstract":"Decision Tree Learning is a fundamental problem for Interpretable Machine Learning, yet it poses a formidable optimization challenge. Despite numerous efforts dating back to the early 1990's, practical algorithms have only recently emerged, primarily leveraging Dynamic Programming (DP) and Branch & Bound (B&B) techniques. These breakthroughs led to the development of two distinct approaches. Algorithms like DL8.5 and MurTree operate on the space of nodes (or branches), they are very fast, but do not penalise complex Decision Trees, i.e. they do not solve for sparsity. On the other hand, algorithms like OSDT and GOSDT operate on the space of Decision Trees, they solve for sparsity but at the detriment of speed. In this work, we introduce Branches, a novel algorithm that integrates the strengths of both paradigms. Leveraging DP and B&B, Branches achieves exceptional speed while also solving for sparsity. Central to its efficiency is a novel analytical bound enabling substantial pruning of the search space. Theoretical analysis demonstrates that Branches has lower complexity compared to state-of-the-art methods, a claim validated through extensive empirical evaluation. Our results illustrate that Branches not only greatly outperforms existing approaches in terms of speed and number of iterations, it also consistently yields optimal Decision Trees.","sentences":["Decision Tree Learning is a fundamental problem for Interpretable Machine Learning, yet it poses a formidable optimization challenge.","Despite numerous efforts dating back to the early 1990's, practical algorithms have only recently emerged, primarily leveraging Dynamic Programming (DP) and Branch & Bound (B&B) techniques.","These breakthroughs led to the development of two distinct approaches.","Algorithms like DL8.5 and MurTree operate on the space of nodes (or branches), they are very fast, but do not penalise complex Decision Trees, i.e. they do not solve for sparsity.","On the other hand, algorithms like OSDT and GOSDT operate on the space of Decision Trees, they solve for sparsity but at the detriment of speed.","In this work, we introduce Branches, a novel algorithm that integrates the strengths of both paradigms.","Leveraging DP and B&B, Branches achieves exceptional speed while also solving for sparsity.","Central to its efficiency is a novel analytical bound enabling substantial pruning of the search space.","Theoretical analysis demonstrates that Branches has lower complexity compared to state-of-the-art methods, a claim validated through extensive empirical evaluation.","Our results illustrate that Branches not only greatly outperforms existing approaches in terms of speed and number of iterations, it also consistently yields optimal Decision Trees."],"url":"http://arxiv.org/abs/2406.02175v1","category":"cs.LG"}
{"created":"2024-06-04 10:00:46","title":"A Multipurpose Interface for Close- and Far-Proximity Control of Mobile Collaborative Robots","abstract":"This letter introduces an innovative visuo-haptic interface to control Mobile Collaborative Robots (MCR). Thanks to a passive detachable mechanism, the interface can be attached/detached from a robot, offering two control modes: local control (attached) and teleoperation (detached). These modes are integrated with a robot whole-body controller and presented in a unified close- and far-proximity control framework for MCR. The earlier introduction of the haptic component in this interface enabled users to execute intricate loco-manipulation tasks via admittance-type control, effectively decoupling task dynamics and enhancing human capabilities. In contrast, this ongoing work proposes a novel design that integrates a visual component. This design utilizes Visual-Inertial Odometry (VIO) for teleoperation, estimating the interface's pose through stereo cameras and an Inertial Measurement Unit (IMU). The estimated pose serves as the reference for the robot's end-effector in teleoperation mode. Hence, the interface offers complete flexibility and adaptability, enabling any user to operate an MCR seamlessly without needing expert knowledge. In this letter, we primarily focus on the new visual feature, and first present a performance evaluation of different VIO-based methods for teleoperation. Next, the interface's usability is analyzed in a home-care application and compared to an alternative designed by a commercial MoCap system. Results show comparable performance in terms of accuracy, completion time, and usability. Nevertheless, the proposed interface is low-cost, poses minimal wearability constraints, and can be used anywhere and anytime without needing external devices or additional equipment, offering a versatile and accessible solution for teleoperation.","sentences":["This letter introduces an innovative visuo-haptic interface to control Mobile Collaborative Robots (MCR).","Thanks to a passive detachable mechanism, the interface can be attached/detached from a robot, offering two control modes: local control (attached) and teleoperation (detached).","These modes are integrated with a robot whole-body controller and presented in a unified close- and far-proximity control framework for MCR.","The earlier introduction of the haptic component in this interface enabled users to execute intricate loco-manipulation tasks via admittance-type control, effectively decoupling task dynamics and enhancing human capabilities.","In contrast, this ongoing work proposes a novel design that integrates a visual component.","This design utilizes Visual-Inertial Odometry (VIO) for teleoperation, estimating the interface's pose through stereo cameras and an Inertial Measurement Unit (IMU).","The estimated pose serves as the reference for the robot's end-effector in teleoperation mode.","Hence, the interface offers complete flexibility and adaptability, enabling any user to operate an MCR seamlessly without needing expert knowledge.","In this letter, we primarily focus on the new visual feature, and first present a performance evaluation of different VIO-based methods for teleoperation.","Next, the interface's usability is analyzed in a home-care application and compared to an alternative designed by a commercial MoCap system.","Results show comparable performance in terms of accuracy, completion time, and usability.","Nevertheless, the proposed interface is low-cost, poses minimal wearability constraints, and can be used anywhere and anytime without needing external devices or additional equipment, offering a versatile and accessible solution for teleoperation."],"url":"http://arxiv.org/abs/2406.02171v1","category":"cs.RO"}
{"created":"2024-06-04 09:56:39","title":"ERes2NetV2: Boosting Short-Duration Speaker Verification Performance with Computational Efficiency","abstract":"Speaker verification systems experience significant performance degradation when tasked with short-duration trial recordings. To address this challenge, a multi-scale feature fusion approach has been proposed to effectively capture speaker characteristics from short utterances. Constrained by the model's size, a robust backbone Enhanced Res2Net (ERes2Net) combining global and local feature fusion demonstrates sub-optimal performance in short-duration speaker verification. To further improve the short-duration feature extraction capability of ERes2Net, we expand the channel dimension within each stage. However, this modification also increases the number of model parameters and computational complexity. To alleviate this problem, we propose an improved ERes2NetV2 by pruning redundant structures, ultimately reducing both the model parameters and its computational cost. A range of experiments conducted on the VoxCeleb datasets exhibits the superiority of ERes2NetV2, which achieves EER of 0.61% for the full-duration trial, 0.98% for the 3s-duration trial, and 1.48% for the 2s-duration trial on VoxCeleb1-O, respectively.","sentences":["Speaker verification systems experience significant performance degradation when tasked with short-duration trial recordings.","To address this challenge, a multi-scale feature fusion approach has been proposed to effectively capture speaker characteristics from short utterances.","Constrained by the model's size, a robust backbone Enhanced Res2Net (ERes2Net) combining global and local feature fusion demonstrates sub-optimal performance in short-duration speaker verification.","To further improve the short-duration feature extraction capability of ERes2Net, we expand the channel dimension within each stage.","However, this modification also increases the number of model parameters and computational complexity.","To alleviate this problem, we propose an improved ERes2NetV2 by pruning redundant structures, ultimately reducing both the model parameters and its computational cost.","A range of experiments conducted on the VoxCeleb datasets exhibits the superiority of ERes2NetV2, which achieves EER of 0.61% for the full-duration trial, 0.98% for the 3s-duration trial, and 1.48% for the 2s-duration trial on VoxCeleb1-O, respectively."],"url":"http://arxiv.org/abs/2406.02167v1","category":"eess.AS"}
{"created":"2024-06-04 09:52:41","title":"Pairwise Ranking Loss for Multi-Task Learning in Recommender Systems","abstract":"Multi-Task Learning (MTL) plays a crucial role in real-world advertising applications such as recommender systems, aiming to achieve robust representations while minimizing resource consumption. MTL endeavors to simultaneously optimize multiple tasks to construct a unified model serving diverse objectives. In online advertising systems, tasks like Click-Through Rate (CTR) and Conversion Rate (CVR) are often treated as MTL problems concurrently. However, it has been overlooked that a conversion ($y_{cvr}=1$) necessitates a preceding click ($y_{ctr}=1$). In other words, while certain CTR tasks are associated with corresponding conversions, others lack such associations. Moreover, the likelihood of noise is significantly higher in CTR tasks where conversions do not occur compared to those where they do, and existing methods lack the ability to differentiate between these two scenarios. In this study, exposure labels corresponding to conversions are regarded as definitive indicators, and a novel task-specific loss is introduced by calculating a \\textbf{p}air\\textbf{wise} \\textbf{r}anking (PWiseR) loss between model predictions, manifesting as pairwise ranking loss, to encourage the model to rely more on them. To demonstrate the effect of the proposed loss function, experiments were conducted on different MTL and Single-Task Learning (STL) models using four distinct public MTL datasets, namely Alibaba FR, NL, US, and CCP, along with a proprietary industrial dataset. The results indicate that our proposed loss function outperforms the BCE loss function in most cases in terms of the AUC metric.","sentences":["Multi-Task Learning (MTL) plays a crucial role in real-world advertising applications such as recommender systems, aiming to achieve robust representations while minimizing resource consumption.","MTL endeavors to simultaneously optimize multiple tasks to construct a unified model serving diverse objectives.","In online advertising systems, tasks like Click-Through Rate (CTR) and Conversion Rate (CVR) are often treated as MTL problems concurrently.","However, it has been overlooked that a conversion ($y_{cvr}=1$) necessitates a preceding click ($y_{ctr}=1$).","In other words, while certain CTR tasks are associated with corresponding conversions, others lack such associations.","Moreover, the likelihood of noise is significantly higher in CTR tasks where conversions do not occur compared to those where they do, and existing methods lack the ability to differentiate between these two scenarios.","In this study, exposure labels corresponding to conversions are regarded as definitive indicators, and a novel task-specific loss is introduced by calculating a \\textbf{p}air\\textbf{wise} \\textbf{r}anking (PWiseR) loss between model predictions, manifesting as pairwise ranking loss, to encourage the model to rely more on them.","To demonstrate the effect of the proposed loss function, experiments were conducted on different MTL and Single-Task Learning (STL) models using four distinct public MTL datasets, namely Alibaba FR, NL, US, and CCP, along with a proprietary industrial dataset.","The results indicate that our proposed loss function outperforms the BCE loss function in most cases in terms of the AUC metric."],"url":"http://arxiv.org/abs/2406.02163v1","category":"cs.IR"}
{"created":"2024-06-04 09:50:51","title":"An Observability-Constrained Magnetic-Field-Aided Inertial Navigation System","abstract":"A method to construct an observability-constrained magnetic-field-aided inertial navigation system is proposed. The proposed method builds upon the previously proposed observability-constrained extended Kalman filter and extends it to work with a magnetic-field-based odometry-aided inertial navigation system. The proposed method is evaluated using simulation and real-world data, showing that (i) the system observability properties are preserved, (ii) the estimation accuracy increases, and (iii) the perceived uncertainty calculated by the EKF is more consistent with the true uncertainty of the filter estimates.","sentences":["A method to construct an observability-constrained magnetic-field-aided inertial navigation system is proposed.","The proposed method builds upon the previously proposed observability-constrained extended Kalman filter and extends it to work with a magnetic-field-based odometry-aided inertial navigation system.","The proposed method is evaluated using simulation and real-world data, showing that (i) the system observability properties are preserved, (ii) the estimation accuracy increases, and (iii) the perceived uncertainty calculated by the EKF is more consistent with the true uncertainty of the filter estimates."],"url":"http://arxiv.org/abs/2406.02161v1","category":"cs.RO"}
{"created":"2024-06-04 09:44:49","title":"Online Learning and Information Exponents: On The Importance of Batch size, and Time/Complexity Tradeoffs","abstract":"We study the impact of the batch size $n_b$ on the iteration time $T$ of training two-layer neural networks with one-pass stochastic gradient descent (SGD) on multi-index target functions of isotropic covariates. We characterize the optimal batch size minimizing the iteration time as a function of the hardness of the target, as characterized by the information exponents. We show that performing gradient updates with large batches $n_b \\lesssim d^{\\frac{\\ell}{2}}$ minimizes the training time without changing the total sample complexity, where $\\ell$ is the information exponent of the target to be learned \\citep{arous2021online} and $d$ is the input dimension. However, larger batch sizes than $n_b \\gg d^{\\frac{\\ell}{2}}$ are detrimental for improving the time complexity of SGD. We provably overcome this fundamental limitation via a different training protocol, \\textit{Correlation loss SGD}, which suppresses the auto-correlation terms in the loss function. We show that one can track the training progress by a system of low-dimensional ordinary differential equations (ODEs). Finally, we validate our theoretical results with numerical experiments.","sentences":["We study the impact of the batch size $n_b$ on the iteration time $T$ of training two-layer neural networks with one-pass stochastic gradient descent (SGD) on multi-index target functions of isotropic covariates.","We characterize the optimal batch size minimizing the iteration time as a function of the hardness of the target, as characterized by the information exponents.","We show that performing gradient updates with large batches $n_b \\lesssim d^{\\frac{\\ell}{2}}$ minimizes the training time without changing the total sample complexity, where $\\ell$ is the information exponent of the target to be learned \\citep{arous2021online} and $d$ is the input dimension.","However, larger batch sizes than $n_b \\gg d^{\\frac{\\ell}{2}}$ are detrimental for improving the time complexity of SGD.","We provably overcome this fundamental limitation via a different training protocol, \\textit{Correlation loss SGD}, which suppresses the auto-correlation terms in the loss function.","We show that one can track the training progress by a system of low-dimensional ordinary differential equations (ODEs).","Finally, we validate our theoretical results with numerical experiments."],"url":"http://arxiv.org/abs/2406.02157v1","category":"stat.ML"}
{"created":"2024-06-04 09:44:24","title":"Almost linear time differentially private release of synthetic graphs","abstract":"In this paper, we give an almost linear time and space algorithms to sample from an exponential mechanism with an $\\ell_1$-score function defined over an exponentially large non-convex set. As a direct result, on input an $n$ vertex $m$ edges graph $G$, we present the \\textit{first} $\\widetilde{O}(m)$ time and $O(m)$ space algorithms for differentially privately outputting an $n$ vertex $O(m)$ edges synthetic graph that approximates all the cuts and the spectrum of $G$. These are the \\emph{first} private algorithms for releasing synthetic graphs that nearly match this task's time and space complexity in the non-private setting while achieving the same (or better) utility as the previous works in the more practical sparse regime. Additionally, our algorithms can be extended to private graph analysis under continual observation.","sentences":["In this paper, we give an almost linear time and space algorithms to sample from an exponential mechanism with an $\\ell_1$-score function defined over an exponentially large non-convex set.","As a direct result, on input an $n$ vertex $m$ edges graph $G$, we present the \\textit{first} $\\widetilde{O}(m)$ time and $O(m)$ space algorithms for differentially privately outputting an $n$ vertex $O(m)$ edges synthetic graph that approximates all the cuts and the spectrum of $G$. These are the \\emph{first} private algorithms for releasing synthetic graphs that nearly match this task's time and space complexity in the non-private setting while achieving the same (or better) utility as the previous works in the more practical sparse regime.","Additionally, our algorithms can be extended to private graph analysis under continual observation."],"url":"http://arxiv.org/abs/2406.02156v1","category":"cs.CR"}
{"created":"2024-06-04 09:42:34","title":"Learning Hamiltonian neural Koopman operator and simultaneously sustaining and discovering conservation law","abstract":"Accurately finding and predicting dynamics based on the observational data with noise perturbations is of paramount significance but still a major challenge presently. Here, for the Hamiltonian mechanics, we propose the Hamiltonian Neural Koopman Operator (HNKO), integrating the knowledge of mathematical physics in learning the Koopman operator, and making it automatically sustain and even discover the conservation laws. We demonstrate the outperformance of the HNKO and its extension using a number of representative physical systems even with hundreds or thousands of freedoms. Our results suggest that feeding the prior knowledge of the underlying system and the mathematical theory appropriately to the learning framework can reinforce the capability of machine learning in solving physical problems.","sentences":["Accurately finding and predicting dynamics based on the observational data with noise perturbations is of paramount significance but still a major challenge presently.","Here, for the Hamiltonian mechanics, we propose the Hamiltonian Neural Koopman Operator (HNKO), integrating the knowledge of mathematical physics in learning the Koopman operator, and making it automatically sustain and even discover the conservation laws.","We demonstrate the outperformance of the HNKO and its extension using a number of representative physical systems even with hundreds or thousands of freedoms.","Our results suggest that feeding the prior knowledge of the underlying system and the mathematical theory appropriately to the learning framework can reinforce the capability of machine learning in solving physical problems."],"url":"http://arxiv.org/abs/2406.02154v1","category":"math-ph"}
{"created":"2024-06-04 09:36:13","title":"Analysis and Simulation of a Coupled Fluid-Heat System in a Thin, Rough Layer","abstract":"We investigate the effective coupling between heat and fluid dynamics within a thin fluid layer in contact with a solid structure via a rough surface. Moreover, the opposing vertical surfaces of the thin layer are in relative motion. This setup is particularly relevant to grinding processes, where cooling lubricants interact with the rough surface of a rotating grinding wheel. The resulting model is non-linearly coupled through(i) temperature-dependent viscosity and (ii) convective heat transport. The underlying geometry is highly heterogeneous due to the thin, rough surface characterized by a small parameter representing both the height of the layer and the periodicity of the roughness. We analyze this non-linear system for existence, uniqueness, and energy estimates and study the limit behavior within the framework of two-scale convergence in thin domains. In this limit, we derive an effective interface model in 3D (a line in 2D) for the heat and fluid interactions inside the fluid. We implement the system numerically and validate the limit problem through direct comparison with the micromodel. Additionally, we investigate the influence of the temperature-dependent viscosity and various geometrical configurations via simulation experiments. The corresponding numerical code is freely available on GitHub.","sentences":["We investigate the effective coupling between heat and fluid dynamics within a thin fluid layer in contact with a solid structure via a rough surface.","Moreover, the opposing vertical surfaces of the thin layer are in relative motion.","This setup is particularly relevant to grinding processes, where cooling lubricants interact with the rough surface of a rotating grinding wheel.","The resulting model is non-linearly coupled through(i) temperature-dependent viscosity and (ii) convective heat transport.","The underlying geometry is highly heterogeneous due to the thin, rough surface characterized by a small parameter representing both the height of the layer and the periodicity of the roughness.","We analyze this non-linear system for existence, uniqueness, and energy estimates and study the limit behavior within the framework of two-scale convergence in thin domains.","In this limit, we derive an effective interface model in 3D (a line in 2D) for the heat and fluid interactions inside the fluid.","We implement the system numerically and validate the limit problem through direct comparison with the micromodel.","Additionally, we investigate the influence of the temperature-dependent viscosity and various geometrical configurations via simulation experiments.","The corresponding numerical code is freely available on GitHub."],"url":"http://arxiv.org/abs/2406.02150v1","category":"math.AP"}
{"created":"2024-06-04 09:31:27","title":"Room-temperature entanglement of the nickel-radical molecular complex (Et3NH)[Ni(hfac)2L]","abstract":"The bipartite entanglement is comprehensively investigated in the mononuclear molecular complex (Et3NH)[Ni(hfac)2L]L, where HL denotes 2-(2-hydroxy-3-methoxy-5-nitrophenyl)-4,4,5,5-tetramethyl-4,5-dihydro-1H-imidazol-3-oxide-1-oxyl and hfacH stands for hexafluoroacetylacetone. From the magnetic point of view, the molecular compound (Et3NH)[Ni(hfac)2L] consists of an exchange-coupled spin-1 Ni2+ magnetic ion and a spin-1/2 nitronyl-nitroxide radical substituted nitrophenol. The nickel-radical molecular complex affords an experimental realization of a mixed spin-(1/2, 1) Heisenberg dimer with a strong antiferromagnetic exchange coupling J/kB = 505 K and two distinct g-factors gRad=2.005 and gNi=2.275. By adopting this set of magnetic parameters we demonstrate that the Zeeman splitting of a quantum ferrimagnetic ground-state doublet due to a weak magnetic field may substantially reinforce the strength of bipartite entanglement at low temperatures. The molecular compound (Et3NH)[Ni(hfac)2L] maintains sufficiently strong thermal entanglement even at room temperature, vanishing only above 546 K. Specifically, the thermal entanglement in the nickel-radical molecular complex retains approximately 40% of the maximum value corresponding to perfectly entangled Bell states at room temperature, which implies that this magnetic compound provides suitable platform of a molecular qubit with potential implications for room-temperature quantum computation and quantum information processing.","sentences":["The bipartite entanglement is comprehensively investigated in the mononuclear molecular complex (Et3NH)[Ni(hfac)2L]L, where HL denotes 2-(2-hydroxy-3-methoxy-5-nitrophenyl)-4,4,5,5-tetramethyl-4,5-dihydro-1H-imidazol-3-oxide-1-oxyl and hfacH stands for hexafluoroacetylacetone.","From the magnetic point of view, the molecular compound (Et3NH)[Ni(hfac)2L] consists of an exchange-coupled spin-1 Ni2+ magnetic ion and a spin-1/2 nitronyl-nitroxide radical substituted nitrophenol.","The nickel-radical molecular complex affords an experimental realization of a mixed spin-(1/2, 1) Heisenberg dimer with a strong antiferromagnetic exchange coupling J/kB = 505 K and two distinct g-factors gRad=2.005 and gNi=2.275.","By adopting this set of magnetic parameters we demonstrate that the Zeeman splitting of a quantum ferrimagnetic ground-state doublet due to a weak magnetic field may substantially reinforce the strength of bipartite entanglement at low temperatures.","The molecular compound (Et3NH)[Ni(hfac)2L] maintains sufficiently strong thermal entanglement even at room temperature, vanishing only above 546 K. Specifically, the thermal entanglement in the nickel-radical molecular complex retains approximately 40% of the maximum value corresponding to perfectly entangled Bell states at room temperature, which implies that this magnetic compound provides suitable platform of a molecular qubit with potential implications for room-temperature quantum computation and quantum information processing."],"url":"http://arxiv.org/abs/2406.02144v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 09:27:23","title":"Statistical Age of Information: A Risk-Aware Metric and Its Applications in Status Updates","abstract":"Age of information (AoI) is an effective measure to quantify the information freshness in wireless status update systems. It has been further validated that the peak AoI has the potential to capture the core characteristics of the aging process, and thus the average peak AoI is widely used to evaluate the long-term performance of information freshness. However, the average peak AoI is a risk-insensitive metric and therefore may not be well suited for evaluating critical status update services. Motivated by this concern, and following the spirit of entropic value-at-risk (EVaR) in the field of risk analysis, in this paper we present a concept, termed Statistical AoI, for providing a unified framework to guarantee various requirements of risk-sensitive status-update services with the demand on the violation probability of the peak age. In particular, as the constraint on the violation probability of the peak age varies from loose to strict, the statistical AoI evolves from the average peak AoI to the maximum peak AoI. We then investigate the statistical AoI minimization problem for status updates over wireless fading channels. It is interesting to note that the corresponding optimal sampling scheme varies from step to constant functions of the channel power gain with the peak age violation probability from one to zero. We also address the maximum statistical AoI minimization problem for multi-status updates with time division multiple access (TDMA), where longer transmission time can improve reliability but may also cause the larger age. By solving this problem, we derive the optimal transmission time allocation scheme. Numerical results show that our proposals can better satisfy the diverse requirements of various risk-sensitive status update services, and demonstrate the great potential of improving information freshness compared to baseline approaches.","sentences":["Age of information (AoI) is an effective measure to quantify the information freshness in wireless status update systems.","It has been further validated that the peak AoI has the potential to capture the core characteristics of the aging process, and thus the average peak AoI is widely used to evaluate the long-term performance of information freshness.","However, the average peak AoI is a risk-insensitive metric and therefore may not be well suited for evaluating critical status update services.","Motivated by this concern, and following the spirit of entropic value-at-risk (EVaR) in the field of risk analysis, in this paper we present a concept, termed Statistical AoI, for providing a unified framework to guarantee various requirements of risk-sensitive status-update services with the demand on the violation probability of the peak age.","In particular, as the constraint on the violation probability of the peak age varies from loose to strict, the statistical AoI evolves from the average peak AoI to the maximum peak AoI.","We then investigate the statistical AoI minimization problem for status updates over wireless fading channels.","It is interesting to note that the corresponding optimal sampling scheme varies from step to constant functions of the channel power gain with the peak age violation probability from one to zero.","We also address the maximum statistical AoI minimization problem for multi-status updates with time division multiple access (TDMA), where longer transmission time can improve reliability but may also cause the larger age.","By solving this problem, we derive the optimal transmission time allocation scheme.","Numerical results show that our proposals can better satisfy the diverse requirements of various risk-sensitive status update services, and demonstrate the great potential of improving information freshness compared to baseline approaches."],"url":"http://arxiv.org/abs/2406.02139v1","category":"eess.SY"}
{"created":"2024-06-04 09:02:39","title":"Surface groups among cubulated hyperbolic and one-relator groups","abstract":"Let $X$ be a non-positively curved cube complex with hyperbolic fundamental group. We prove that $\\pi_1(X)$ has a non-free subgroup of infinite index unless $\\pi_1(X)$ is either free or a surface group, answering a question of Wise. A similar result for one-relator groups follows, answering a question posed by several authors. The proof relies on a careful analysis of free and cyclic splittings of cubulated groups.","sentences":["Let $X$ be a non-positively curved cube complex with hyperbolic fundamental group.","We prove that $\\pi_1(X)$ has a non-free subgroup of infinite index unless $\\pi_1(X)$ is either free or a surface group, answering a question of Wise.","A similar result for one-relator groups follows, answering a question posed by several authors.","The proof relies on a careful analysis of free and cyclic splittings of cubulated groups."],"url":"http://arxiv.org/abs/2406.02121v1","category":"math.GR"}
{"created":"2024-06-04 08:59:58","title":"A novel model reduction method to solve inverse problems of parabolic type","abstract":"In this paper, we propose novel proper orthogonal decomposition (POD)--based model reduction methods that effectively address the issue of inverse crime in solving parabolic inverse problems. Both the inverse initial value problems and inverse source problems are studied. By leveraging the inherent low-dimensional structures present in the data, our approach enables a reduction in the forward model complexity without compromising the accuracy of the inverse problem solution. Besides, we prove the convergence analysis of the proposed methods for solving parabolic inverse problems. Through extensive experimentation and comparative analysis, we demonstrate the effectiveness of our method in overcoming inverse crime and achieving improved inverse problem solutions. The proposed POD model reduction method offers a promising direction for improving the reliability and applicability of inverse problem-solving techniques in various domains.","sentences":["In this paper, we propose novel proper orthogonal decomposition (POD)--based model reduction methods that effectively address the issue of inverse crime in solving parabolic inverse problems.","Both the inverse initial value problems and inverse source problems are studied.","By leveraging the inherent low-dimensional structures present in the data, our approach enables a reduction in the forward model complexity without compromising the accuracy of the inverse problem solution.","Besides, we prove the convergence analysis of the proposed methods for solving parabolic inverse problems.","Through extensive experimentation and comparative analysis, we demonstrate the effectiveness of our method in overcoming inverse crime and achieving improved inverse problem solutions.","The proposed POD model reduction method offers a promising direction for improving the reliability and applicability of inverse problem-solving techniques in various domains."],"url":"http://arxiv.org/abs/2406.02119v1","category":"math.NA"}
{"created":"2024-06-04 08:57:44","title":"Spectral line fluorescence in moving envelopes of stars","abstract":"The formation of optical fluorescent lines in moving media has not yet been studied in detail, so this work represents a first step in investigating the fluorescence process in different types of macroscopic velocity fields: (a) accelerated outflows, (b) accelerated infalls, and (c) non-monotonic velocity fields (such as an accelerating outflow followed by a deceleration region or an accretion shock front). We solve the radiative transfer equations for the lines involved in the fluorescent process, assuming spherical symmetry and a simplified atomic model. We use the framework of the generalized Sobolev theory for computing the interacting, non-local source functions. The emergent line fluxes are then integrated exactly. Because of Doppler shifts in the moving gaseous envelope, photons of the three lines involved in TTS FeI fluorescence CaII H, FeI 3969, and H_epsilon interact with each other in a complex way, so that fluorescent amplification of the line flux occurs not only for FeI 3969, but also for the other two lines, in all velocity fields that we investigated. With the assumption of LTE populations, the line source functions of moderately optically thick lines are not strongly affected by line interactions, while they are depressed in the inner envelope for optically thick lines because of stellar photon absorption in the interaction regions. Fluorescent amplification takes place mainly in the observer's reference frame during the flux integration. Further comparison with observations will require solving the rate equations for the atomic populations involved, along with the radiation field computed with the method presented here. The main product of this research is the open-source computer code SLIM2 (Spectral Line Interactions in Moving Media), written in Python/Numpy, which numerically solves the fluorescence problem for arbitrary 2D velocities.","sentences":["The formation of optical fluorescent lines in moving media has not yet been studied in detail, so this work represents a first step in investigating the fluorescence process in different types of macroscopic velocity fields: (a) accelerated outflows, (b) accelerated infalls, and (c) non-monotonic velocity fields (such as an accelerating outflow followed by a deceleration region or an accretion shock front).","We solve the radiative transfer equations for the lines involved in the fluorescent process, assuming spherical symmetry and a simplified atomic model.","We use the framework of the generalized Sobolev theory for computing the interacting, non-local source functions.","The emergent line fluxes are then integrated exactly.","Because of Doppler shifts in the moving gaseous envelope, photons of the three lines involved in TTS FeI fluorescence CaII H, FeI 3969, and H_epsilon interact with each other in a complex way, so that fluorescent amplification of the line flux occurs not only for FeI 3969, but also for the other two lines, in all velocity fields that we investigated.","With the assumption of LTE populations, the line source functions of moderately optically thick lines are not strongly affected by line interactions, while they are depressed in the inner envelope for optically thick lines because of stellar photon absorption in the interaction regions.","Fluorescent amplification takes place mainly in the observer's reference frame during the flux integration.","Further comparison with observations will require solving the rate equations for the atomic populations involved, along with the radiation field computed with the method presented here.","The main product of this research is the open-source computer code SLIM2 (Spectral Line Interactions in Moving Media), written in Python/Numpy, which numerically solves the fluorescence problem for arbitrary 2D velocities."],"url":"http://arxiv.org/abs/2406.02117v1","category":"astro-ph.SR"}
{"created":"2024-06-04 08:57:31","title":"A clock is just a way to tell the time: gravitational algebras in cosmological spacetimes","abstract":"We study the algebra of observables in semiclassical quantum gravity for cosmological backgrounds, focusing on two key examples: slow-roll inflation and evaporating Schwarzschild-de Sitter black holes. In both cases, we demonstrate the existence of a nontrivial algebra of diffeomorphism-invariant observables \\emph{without} the introduction of an external clock system or the presence of any asymptotic gravitational charges. Instead, the rolling inflaton field and the evaporating black hole act as physical clocks that allow a definition of gauge-invariant observables at $G = 0$. The resulting algebras are both Type II$_\\infty$ factors, but neither is manifestly a crossed product algebra. We establish a connection between the Type II entropy of these algebras and generalized entropies for appropriate states. Our work extends previous results on Type II gravitational algebras and highlights the crucial role of out-of-equilibrium dynamics for defining gauge-invariant observables in semiclassical canonically quantised gravity. We also briefly discuss the construction of gauge-invariant algebras for compact wedges bounded by extremal surfaces in generic spacetimes (i.e. in the absence of any Killing symmetry). In contrast to the inflaton and black hole cases, this algebra does end up being a simple crossed product. No clock or asymptotic charges are required because of the absence of any symmetry in the classical background.","sentences":["We study the algebra of observables in semiclassical quantum gravity for cosmological backgrounds, focusing on two key examples: slow-roll inflation and evaporating Schwarzschild-de Sitter black holes.","In both cases, we demonstrate the existence of a nontrivial algebra of diffeomorphism-invariant observables \\emph{without} the introduction of an external clock system or the presence of any asymptotic gravitational charges.","Instead, the rolling inflaton field and the evaporating black hole act as physical clocks that allow a definition of gauge-invariant observables at $G = 0$.","The resulting algebras are both Type II$_\\infty$ factors, but neither is manifestly a crossed product algebra.","We establish a connection between the Type II entropy of these algebras and generalized entropies for appropriate states.","Our work extends previous results on Type II gravitational algebras and highlights the crucial role of out-of-equilibrium dynamics for defining gauge-invariant observables in semiclassical canonically quantised gravity.","We also briefly discuss the construction of gauge-invariant algebras for compact wedges bounded by extremal surfaces in generic spacetimes (i.e. in the absence of any Killing symmetry).","In contrast to the inflaton and black hole cases, this algebra does end up being a simple crossed product.","No clock or asymptotic charges are required because of the absence of any symmetry in the classical background."],"url":"http://arxiv.org/abs/2406.02116v1","category":"hep-th"}
{"created":"2024-06-04 08:38:46","title":"Timescale bridging in atomistic simulations of epoxy polymer mechanics using non-affine deformation theory","abstract":"Developing a deep understanding of macroscopic mechanical properties of amorphous systems which lack structural periodicity, has posed a key challenge, not only at the level of theory but also in molecular simulations. Despite significant advancements in computational resources, there is a vast timescale disparity, more than 6 orders of magnitude, between mechanical properties probed in simulations compared to experiments. Using the theoretical framework of non-affine lattice dynamics (NALD), based on the instantaneous normal modes analysis determined through the dynamical matrix of the system, we study the viscoelastic response of a cross-linked epoxy system of diglycidyl ether of bisphenol A (DGEBA) and poly(oxypropylene) diamine, over many orders of magnitude in deformation frequency, below the glass transition temperature. Predictions of the elastic modulus are satisfactorily validated against the non-equilibrium molecular dynamics simulations in the high-frequency regime, and against experimental data from dynamic mechanical analysis at frequencies $ \\sim 1 {\\rm Hz}$, hence successfully bridging the timescale gap. The comparison shows that non-affine displacements at the atomic level account for nearly two orders of magnitude reduction in the low-frequency elastic modulus of the polymer glass, compared to affine elasticity estimates. The analysis also reveals the role of internal stresses (as reflected in the instantaneous normal modes), which act as to strengthen the mechanical response.","sentences":["Developing a deep understanding of macroscopic mechanical properties of amorphous systems which lack structural periodicity, has posed a key challenge, not only at the level of theory but also in molecular simulations.","Despite significant advancements in computational resources, there is a vast timescale disparity, more than 6 orders of magnitude, between mechanical properties probed in simulations compared to experiments.","Using the theoretical framework of non-affine lattice dynamics (NALD), based on the instantaneous normal modes analysis determined through the dynamical matrix of the system, we study the viscoelastic response of a cross-linked epoxy system of diglycidyl ether of bisphenol A (DGEBA) and poly(oxypropylene) diamine, over many orders of magnitude in deformation frequency, below the glass transition temperature.","Predictions of the elastic modulus are satisfactorily validated against the non-equilibrium molecular dynamics simulations in the high-frequency regime, and against experimental data from dynamic mechanical analysis at frequencies $ \\sim 1 {\\rm Hz}$, hence successfully bridging the timescale gap.","The comparison shows that non-affine displacements at the atomic level account for nearly two orders of magnitude reduction in the low-frequency elastic modulus of the polymer glass, compared to affine elasticity estimates.","The analysis also reveals the role of internal stresses (as reflected in the instantaneous normal modes), which act as to strengthen the mechanical response."],"url":"http://arxiv.org/abs/2406.02113v1","category":"cond-mat.soft"}
{"created":"2024-06-04 08:37:17","title":"Symmetry-Governed Dynamics of Magnetic Skyrmions Under Field Pulses","abstract":"Topological magnetic solitons, such as skyrmions, exhibit intriguing particle-like properties that make them attractive for fundamental research and practical applications. While many magnetic systems can host skyrmions as statically stable configurations, chiral magnets stand out for their ability to accommodate a wide diversity of skyrmions with arbitrary topological charges and varied morphologies. Despite extensive investigation, a complete understanding of chiral magnetic skyrmions has remained elusive. We present a classification of all chiral skyrmions, demonstrating three classes based on their response to external magnetic field pulses: stationary, translating, and rotating. We highlight the role of magnetic texture symmetry in this classification. Skyrmions with varied dynamics offer avenues for exploring phenomena like skyrmion-skyrmion scattering that might be crucial for future applications.","sentences":["Topological magnetic solitons, such as skyrmions, exhibit intriguing particle-like properties that make them attractive for fundamental research and practical applications.","While many magnetic systems can host skyrmions as statically stable configurations, chiral magnets stand out for their ability to accommodate a wide diversity of skyrmions with arbitrary topological charges and varied morphologies.","Despite extensive investigation, a complete understanding of chiral magnetic skyrmions has remained elusive.","We present a classification of all chiral skyrmions, demonstrating three classes based on their response to external magnetic field pulses: stationary, translating, and rotating.","We highlight the role of magnetic texture symmetry in this classification.","Skyrmions with varied dynamics offer avenues for exploring phenomena like skyrmion-skyrmion scattering that might be crucial for future applications."],"url":"http://arxiv.org/abs/2406.02112v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 08:36:14","title":"Description Complexity of Unary Structures in First-Order Logic with Links to Entropy","abstract":"The description complexity of a model is the length of the shortest formula that defines the model. We study the description complexity of unary structures in first-order logic FO, also drawing links to semantic complexity in the form of entropy. The class of unary structures provides a simple way to represent tabular Boolean data sets as relational structures. We define structures with FO-formulas that are strictly linear in the size of the model as opposed to using the naive quadratic ones, and we use arguments based on formula size games to obtain related lower bounds for description complexity. We also obtain a precise asymptotic result on the expected description complexity of a randomly selected structure. We then give bounds on the relationship between Shannon entropy and description complexity. We extend this relationship also to Boltzmann entropy by establishing an asymptotic match between the two entropies. Despite the simplicity of unary structures, our arguments require the use of formula size games, Stirling's approximation and Chernoff bounds.","sentences":["The description complexity of a model is the length of the shortest formula that defines the model.","We study the description complexity of unary structures in first-order logic FO, also drawing links to semantic complexity in the form of entropy.","The class of unary structures provides a simple way to represent tabular Boolean data sets as relational structures.","We define structures with FO-formulas that are strictly linear in the size of the model as opposed to using the naive quadratic ones, and we use arguments based on formula size games to obtain related lower bounds for description complexity.","We also obtain a precise asymptotic result on the expected description complexity of a randomly selected structure.","We then give bounds on the relationship between Shannon entropy and description complexity.","We extend this relationship also to Boltzmann entropy by establishing an asymptotic match between the two entropies.","Despite the simplicity of unary structures, our arguments require the use of formula size games, Stirling's approximation and Chernoff bounds."],"url":"http://arxiv.org/abs/2406.02108v1","category":"math.LO"}
{"created":"2024-06-04 08:35:04","title":"MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset","abstract":"To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents. Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning. We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step. These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities. Our data and models are publicly accessible at https://github.com/HKUST-KnowComp/MARS.","sentences":["To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents.","Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions.","Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning.","We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step.","These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action.","Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning.","Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities.","Our data and models are publicly accessible at https://github.com/HKUST-KnowComp/MARS."],"url":"http://arxiv.org/abs/2406.02106v1","category":"cs.CL"}
{"created":"2024-06-04 08:32:19","title":"Universal gravitational self-force for a point mass orbiting around a compact star","abstract":"In this work, we study the gravitational back-reaction (i.e., the \"self-force\") of a point mass moving around a non-rotating, compact star on a circular orbit. We find that the additional self-force, comparing with the case with a point mass orbiting around a Schwarzschild black hole, can be well characterized by an universal frequency-dependent function multiplied by the (dynamical) tidal deformability of the compact star. This finding provides the foundation for building the waveform model for an extreme mass-ratio inspiral system around a star-like black hole mimicker, which is relevant for testing General Relativity and exotic compact objects with space-borne gravitational wave detectors.","sentences":["In this work, we study the gravitational back-reaction (i.e., the \"self-force\") of a point mass moving around a non-rotating, compact star on a circular orbit.","We find that the additional self-force, comparing with the case with a point mass orbiting around a Schwarzschild black hole, can be well characterized by an universal frequency-dependent function multiplied by the (dynamical) tidal deformability of the compact star.","This finding provides the foundation for building the waveform model for an extreme mass-ratio inspiral system around a star-like black hole mimicker, which is relevant for testing General Relativity and exotic compact objects with space-borne gravitational wave detectors."],"url":"http://arxiv.org/abs/2406.02101v1","category":"gr-qc"}
{"created":"2024-06-04 08:30:37","title":"Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data","abstract":"Large Language Models (LLMs) have shown excellent performance in language understanding, text generation, code synthesis, and many other tasks, while they still struggle in complex multi-step reasoning problems, such as mathematical reasoning. In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine-tuning on high-quality synthetic data. Experimental results with the open-llama-3B model on three different test datasets show that not only the model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also demonstrates certain generalization capabilities on the out-of-domain datasets. Specifically, this paper has designed two out-of-domain datasets in the form of extending the numerical range and the composing components of the arithmetical puzzle problem separately. The fine-tuned models have shown encouraging performance on these two far more difficult tasks with the zero-shot pass@1 at 0.33 and 0.35, respectively.","sentences":["Large Language Models (LLMs) have shown excellent performance in language understanding, text generation, code synthesis, and many other tasks, while they still struggle in complex multi-step reasoning problems, such as mathematical reasoning.","In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine-tuning on high-quality synthetic data.","Experimental results with the open-llama-3B model on three different test datasets show that not only the model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also demonstrates certain generalization capabilities on the out-of-domain datasets.","Specifically, this paper has designed two out-of-domain datasets in the form of extending the numerical range and the composing components of the arithmetical puzzle problem separately.","The fine-tuned models have shown encouraging performance on these two far more difficult tasks with the zero-shot pass@1 at 0.33 and 0.35, respectively."],"url":"http://arxiv.org/abs/2406.02100v1","category":"cs.CL"}
{"created":"2024-06-04 08:29:13","title":"Homogeneous nucleation for two-dimensional Kawasaki dynamics","abstract":"This is the third in a series of three papers in which we study a lattice gas subject to Kawasaki dynamics at inverse temperature $\\beta>0$ in a large finite box $\\Lambda_\\beta \\subset \\mathbb{Z}^2$ whose volume depends on $\\beta$. Each pair of neighbouring particles has a negative binding energy $-U<0$, while each particle has a positive activation energy $\\Delta>0$. The initial configuration is drawn from the grand-canonical ensemble restricted to the set of configurations where all the droplets are subcritical. Our goal is to describe, in the metastable regime $\\Delta \\in (U,2U)$ and in the limit as $\\beta\\to\\infty$, how and when the system nucleates, i.e., creates a critical droplet somewhere in $\\Lambda_\\beta$ that subsequently grows by absorbing particles from the surrounding gas.   In the first paper we showed that subcritical droplets behave as quasi-random walks. In the second paper we used the results in the first paper to analyse how subcritical droplets form and dissolve on multiple space-time scales when the volume is moderately large, namely, $|\\Lambda_\\beta| = \\mathrm{e}^{\\theta\\beta}$ with $\\Delta < \\theta < 2\\Delta-U$. In the present paper we consider the setting where the volume is very large, namely, $|\\Lambda_\\beta| = \\mathrm{e}^{\\Theta\\beta}$ with $\\Delta < \\Theta < \\Gamma-(2\\Delta-U)$, where $\\Gamma$ is the energy of the critical droplet in the local model with fixed volume, and use the results in the first two papers to identify the nucleation time and the tube of typical trajectories towards nucleation. We will see that in a very large volume critical droplets appear more or less independently in boxes of moderate volume, a phenomenon referred to as homogeneous nucleation. One of the key ingredients in the proof is an estimate showing that no information can travel between these boxes on relevant time scales.","sentences":["This is the third in a series of three papers in which we study a lattice gas subject to Kawasaki dynamics at inverse temperature $\\beta>0$ in a large finite box $\\Lambda_\\beta \\subset","\\mathbb{Z}^2$ whose volume depends on $\\beta$. Each pair of neighbouring particles has a negative binding energy $-U<0$, while each particle has a positive activation energy $\\Delta>0$. The initial configuration is drawn from the grand-canonical ensemble restricted to the set of configurations where all the droplets are subcritical.","Our goal is to describe, in the metastable regime $\\Delta \\in (U,2U)$ and in the limit as $\\beta\\to\\infty$, how and when the system nucleates, i.e., creates a critical droplet somewhere in $\\Lambda_\\beta$ that subsequently grows by absorbing particles from the surrounding gas.   ","In the first paper we showed that subcritical droplets behave as quasi-random walks.","In the second paper we used the results in the first paper to analyse how subcritical droplets form and dissolve on multiple space-time scales when the volume is moderately large, namely, $|\\Lambda_\\beta| = \\mathrm{e}^{\\theta\\beta}$ with $\\Delta <","\\theta < 2\\Delta-U$.","In the present paper we consider the setting where the volume is very large, namely, $|\\Lambda_\\beta| = \\mathrm{e}^{\\Theta\\beta}$ with $\\Delta < \\Theta < \\Gamma-(2\\Delta-U)$, where $\\Gamma$ is the energy of the critical droplet in the local model with fixed volume, and use the results in the first two papers to identify the nucleation time and the tube of typical trajectories towards nucleation.","We will see that in a very large volume critical droplets appear more or less independently in boxes of moderate volume, a phenomenon referred to as homogeneous nucleation.","One of the key ingredients in the proof is an estimate showing that no information can travel between these boxes on relevant time scales."],"url":"http://arxiv.org/abs/2406.02099v1","category":"math.PR"}
{"created":"2024-06-04 08:29:07","title":"Blow-up of solutions to semilinear wave equations with spatial derivatives","abstract":"For small-amplitude semilinear wave equations with power type nonlinearity on the first-order spatial derivative, the expected sharp upper bound on the lifespan of solutions is obtained for both critical cases and subcritical cases, for all spatial dimensions $n>1$. It is achieved uniformly by constructing the integral equations, deriving the ordinary differential inequality system, and iteration argument. Combined with the former works, the sharp lifespan estimates for this problem are completely established, at least for the spherical symmetric case.","sentences":["For small-amplitude semilinear wave equations with power type nonlinearity on the first-order spatial derivative, the expected sharp upper bound on the lifespan of solutions is obtained for both critical cases and subcritical cases, for all spatial dimensions $n>1$. It is achieved uniformly by constructing the integral equations, deriving the ordinary differential inequality system, and iteration argument.","Combined with the former works, the sharp lifespan estimates for this problem are completely established, at least for the spherical symmetric case."],"url":"http://arxiv.org/abs/2406.02098v1","category":"math.AP"}
{"created":"2024-06-04 08:26:54","title":"Brightened emission of dark trions in transition-metal dichalcogenide monolayers","abstract":"The optical emission spectra of semiconducting transition-metal dichalcogenide monolayers highlight fascinating recombination processes of charged excitons (trions). When charge tunable WSe2 monolayers are moderately doped with electrons, a strong luminescence peak emerges just below the well-understood spectral lines associated with the recombination of negatively charged bright and dark trions. Despite previous investigations, its origin remains elusive. Here, we demonstrate that this luminescence peak is the result of electron-electron assisted recombination that brightens the dark trion emission. Supporting evidence for this second-order recombination process comes from identifying the equivalent brightened emission of positively charged dark trions when the monolayer is electrostatically doped with holes. Remarkably, the discovered hole-hole assisted luminescence peak emerges in the near infrared, about 500 meV below the well-studied spectral region of excitons and trions. In addition to identifying new recombination channels of these excitonic complexes, our findings accurately determine the spin-split energies of the conduction and valence bands. Both of which play crucial roles in understanding the optical properties of WSe2 based homo- and hetero-structures.","sentences":["The optical emission spectra of semiconducting transition-metal dichalcogenide monolayers highlight fascinating recombination processes of charged excitons (trions).","When charge tunable WSe2 monolayers are moderately doped with electrons, a strong luminescence peak emerges just below the well-understood spectral lines associated with the recombination of negatively charged bright and dark trions.","Despite previous investigations, its origin remains elusive.","Here, we demonstrate that this luminescence peak is the result of electron-electron assisted recombination that brightens the dark trion emission.","Supporting evidence for this second-order recombination process comes from identifying the equivalent brightened emission of positively charged dark trions when the monolayer is electrostatically doped with holes.","Remarkably, the discovered hole-hole assisted luminescence peak emerges in the near infrared, about 500 meV below the well-studied spectral region of excitons and trions.","In addition to identifying new recombination channels of these excitonic complexes, our findings accurately determine the spin-split energies of the conduction and valence bands.","Both of which play crucial roles in understanding the optical properties of WSe2 based homo- and hetero-structures."],"url":"http://arxiv.org/abs/2406.02095v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-04 08:26:11","title":"Hybrid-Dynamic Ehrenfeucht-Fra\u00efss\u00e9 Games","abstract":"Ehrenfeucht-Fra\\\"iss\\'e games provide means to characterize elementary equivalence for first-order logic, and by standard translation also for modal logics. We propose a novel generalization of Ehrenfeucht- Fra\\\"iss\\'e games to hybrid-dynamic logics which is direct and fully modular: parameterized by the features of the hybrid language we wish to include, for instance, the modal and hybrid language operators as well as first-order existential quantification. We use these games to establish a new modular Fra\\\"iss\\'e-Hintikka Theorem for hybrid-dynamic propositional logic and its various fragments. We study the relationship between countable game equivalence (determined by countable Ehrenfeucht- Fra\\\"iss\\'e games) and bisimulation (determined by countable back-and-forth systems). In general, the former turns out to be weaker than the latter, but under certain conditions on the language, the two coincide. We also use games to prove that for reachable image-finite Kripke structures elementary equivalence implies isomorphism.","sentences":["Ehrenfeucht-Fra\\\"iss\\'e games provide means to characterize elementary equivalence for first-order logic, and by standard translation also for modal logics.","We propose a novel generalization of Ehrenfeucht- Fra\\\"iss\\'e games to hybrid-dynamic logics which is direct and fully modular: parameterized by the features of the hybrid language we wish to include, for instance, the modal and hybrid language operators as well as first-order existential quantification.","We use these games to establish a new modular Fra\\\"iss\\'e-Hintikka Theorem for hybrid-dynamic propositional logic and its various fragments.","We study the relationship between countable game equivalence (determined by countable Ehrenfeucht- Fra\\\"iss\\'e games) and bisimulation (determined by countable back-and-forth systems).","In general, the former turns out to be weaker than the latter, but under certain conditions on the language, the two coincide.","We also use games to prove that for reachable image-finite Kripke structures elementary equivalence implies isomorphism."],"url":"http://arxiv.org/abs/2406.02094v1","category":"cs.LO"}
{"created":"2024-06-04 08:24:24","title":"Multi-state Chiral Switching Through Adiabaticity Control in Encircling Exceptional Points","abstract":"Dynamic encircling of exceptional points has attracted significant interest in recent years, as it can facilitate chiral transmission selectivity due to a nontrivial eigenstate evolution. Recently, multi-state systems have been explored, associated with more complex topologies supporting a larger number of exceptional points, but chiral switching among multiple eigenstates has remained elusive in experiments. Here, we overcome this challenge by dividing the eigenstate space into multiple subspaces by controlling the adiabaticity. The eigenstates in different subspaces can evolve without crosstalk, and chiral switching occurs as the eigenstates within each subspace are subject to a non-adiabatic transition while they encircle exceptional points. We experimentally demonstrate this phenomenon by reporting chiral switching for two groups of optical modes at telecom wavelengths in a four-state optical system, and theoretically demonstrate that our approach can be extended to higher-order systems. Our findings pave new avenues for studying chiral dynamics based on exceptional-point physics in multi-state systems, and offer opportunities to develop multiplexed photonic devices.","sentences":["Dynamic encircling of exceptional points has attracted significant interest in recent years, as it can facilitate chiral transmission selectivity due to a nontrivial eigenstate evolution.","Recently, multi-state systems have been explored, associated with more complex topologies supporting a larger number of exceptional points, but chiral switching among multiple eigenstates has remained elusive in experiments.","Here, we overcome this challenge by dividing the eigenstate space into multiple subspaces by controlling the adiabaticity.","The eigenstates in different subspaces can evolve without crosstalk, and chiral switching occurs as the eigenstates within each subspace are subject to a non-adiabatic transition while they encircle exceptional points.","We experimentally demonstrate this phenomenon by reporting chiral switching for two groups of optical modes at telecom wavelengths in a four-state optical system, and theoretically demonstrate that our approach can be extended to higher-order systems.","Our findings pave new avenues for studying chiral dynamics based on exceptional-point physics in multi-state systems, and offer opportunities to develop multiplexed photonic devices."],"url":"http://arxiv.org/abs/2406.02093v1","category":"physics.optics"}
{"created":"2024-06-04 08:22:43","title":"Giant cluster formation in a simple model for globular colloidal aggregates","abstract":"A simple model of fluid whose constituent particles interact via a short range attractive and long range repulsive potential is used to model the aggregation of giant clusters made up of hundreds of particles. The model can be thought of as a simplistic representation membraneless organelles or a straightforward rendition of colloid flocculation into large spherical aggregates. We illustrate how temperature and particle density influence the cluster size distribution and vary specially the inter-cluster dynamics. The system is shown to exhibit two well separated length and time scales, which can be tuned by the balance between repulsive and attractive forces.","sentences":["A simple model of fluid whose constituent particles interact via a short range attractive and long range repulsive potential is used to model the aggregation of giant clusters made up of hundreds of particles.","The model can be thought of as a simplistic representation membraneless organelles or a straightforward rendition of colloid flocculation into large spherical aggregates.","We illustrate how temperature and particle density influence the cluster size distribution and vary specially the inter-cluster dynamics.","The system is shown to exhibit two well separated length and time scales, which can be tuned by the balance between repulsive and attractive forces."],"url":"http://arxiv.org/abs/2406.02091v1","category":"cond-mat.soft"}
{"created":"2024-06-04 08:13:43","title":"Fast and Practical Strassen's Matrix Multiplication using FPGAs","abstract":"Matrix multiplication is a cornerstone operation in a wide array of scientific fields, including machine learning and computer graphics. The standard algorithm for matrix multiplication has a complexity of $\\mathcal{O}(n^3)$ for $n\\times n$ matrices. Strassen's algorithm improves this to $\\mathcal{O}(n^{2.807})$, but its practicality is limited for small to medium matrix sizes due to the large number of additions it introduces. This paper presents a novel FPGA-based implementation of Strassen's algorithm that achieves superior speed over an optimized General Matrix Multiply (GeMM) implementation for matrices as small as $n=256$. Our design, tested extensively on two high-performance FPGA accelerators (Alveo U50 and U280) across various data types, matches or surpasses the performance of a highly optimized baseline across a range of matrix sizes.","sentences":["Matrix multiplication is a cornerstone operation in a wide array of scientific fields, including machine learning and computer graphics.","The standard algorithm for matrix multiplication has a complexity of $\\mathcal{O}(n^3)$ for $n\\times n$ matrices.","Strassen's algorithm improves this to $\\mathcal{O}(n^{2.807})$, but its practicality is limited for small to medium matrix sizes due to the large number of additions it introduces.","This paper presents a novel FPGA-based implementation of Strassen's algorithm that achieves superior speed over an optimized General Matrix Multiply (GeMM) implementation for matrices as small as $n=256$. Our design, tested extensively on two high-performance FPGA accelerators (Alveo U50 and U280) across various data types, matches or surpasses the performance of a highly optimized baseline across a range of matrix sizes."],"url":"http://arxiv.org/abs/2406.02088v1","category":"cs.AR"}
{"created":"2024-06-04 08:05:59","title":"On the Computation of 2-Dimensional Recurrence Equations","abstract":"The paper demonstrates how a 2-dimensional recurrence problem can be reduced to a mono-dimensional recurrence problem where the Kogge and Stone algorithm is applicable, with the computation time - excluding the reduction step - becoming proportional to $log_2(2n-1)$.","sentences":["The paper demonstrates how a 2-dimensional recurrence problem can be reduced to a mono-dimensional recurrence problem where the Kogge and Stone algorithm is applicable, with the computation time - excluding the reduction step - becoming proportional to $log_2(2n-1)$."],"url":"http://arxiv.org/abs/2406.02082v1","category":"cs.DS"}
{"created":"2024-06-04 07:54:31","title":"ReLU-KAN: New Kolmogorov-Arnold Networks that Only Need Matrix Addition, Dot Multiplication, and ReLU","abstract":"Limited by the complexity of basis function (B-spline) calculations, Kolmogorov-Arnold Networks (KAN) suffer from restricted parallel computing capability on GPUs. This paper proposes a novel ReLU-KAN implementation that inherits the core idea of KAN. By adopting ReLU (Rectified Linear Unit) and point-wise multiplication, we simplify the design of KAN's basis function and optimize the computation process for efficient CUDA computing. The proposed ReLU-KAN architecture can be readily implemented on existing deep learning frameworks (e.g., PyTorch) for both inference and training. Experimental results demonstrate that ReLU-KAN achieves a 20x speedup compared to traditional KAN with 4-layer networks. Furthermore, ReLU-KAN exhibits a more stable training process with superior fitting ability while preserving the \"catastrophic forgetting avoidance\" property of KAN. You can get the code in https://github.com/quiqi/relu_kan","sentences":["Limited by the complexity of basis function (B-spline) calculations, Kolmogorov-Arnold Networks (KAN) suffer from restricted parallel computing capability on GPUs.","This paper proposes a novel ReLU-KAN implementation that inherits the core idea of KAN.","By adopting ReLU (Rectified Linear Unit) and point-wise multiplication, we simplify the design of KAN's basis function and optimize the computation process for efficient CUDA computing.","The proposed ReLU-KAN architecture can be readily implemented on existing deep learning frameworks (e.g., PyTorch) for both inference and training.","Experimental results demonstrate that ReLU-KAN achieves a 20x speedup compared to traditional KAN with 4-layer networks.","Furthermore, ReLU-KAN exhibits a more stable training process with superior fitting ability while preserving the \"catastrophic forgetting avoidance\" property of KAN.","You can get the code in https://github.com/quiqi/relu_kan"],"url":"http://arxiv.org/abs/2406.02075v1","category":"cs.LG"}
{"created":"2024-06-04 07:53:04","title":"Input-to-state stability of infinite-dimensional systems: Foundations and present-day developments","abstract":"Input-to-state stability (ISS) unifies the stability and robustness in one notion, and serves as a basis for broad areas of nonlinear control theory. In this contribution, we covered the most fundamental facts in the infinite-dimensional ISS theory with a stress on Lyapunov methods. We consider various applications given by different classes of infinite-dimensional systems. Finally, we discuss a Lyapunov-based small-gain theorem for stability analysis of an interconnection of two ISS systems.","sentences":["Input-to-state stability (ISS) unifies the stability and robustness in one notion, and serves as a basis for broad areas of nonlinear control theory.","In this contribution, we covered the most fundamental facts in the infinite-dimensional ISS theory with a stress on Lyapunov methods.","We consider various applications given by different classes of infinite-dimensional systems.","Finally, we discuss a Lyapunov-based small-gain theorem for stability analysis of an interconnection of two ISS systems."],"url":"http://arxiv.org/abs/2406.02071v1","category":"eess.SY"}
{"created":"2024-06-04 07:44:57","title":"An agent-based model of modal choice with perception biases and habits","abstract":"This paper presents an agent-based model of mobility choice, influenced by human factors such as habits and perception biases. It is implemented in a Netlogo simulator, calibrated from results of an online survey about perceptions of mobility. The simulator can be played online. It allows to modify urban infrastructure and observe modal report.","sentences":["This paper presents an agent-based model of mobility choice, influenced by human factors such as habits and perception biases.","It is implemented in a Netlogo simulator, calibrated from results of an online survey about perceptions of mobility.","The simulator can be played online.","It allows to modify urban infrastructure and observe modal report."],"url":"http://arxiv.org/abs/2406.02063v1","category":"cs.CY"}
{"created":"2024-06-04 07:44:18","title":"Towards Railways Remote Driving: Analysis of Video Streaming Latency and Adaptive Rate Control","abstract":"Remote driving aims to improve transport systems by promoting efficiency, sustainability, and accessibility. In the railway sector, remote driving makes it possible to increase flexibility, as the driver no longer has to be in the cab. However, this brings several challenges, as it has to provide at least the same level of safety obtained when the driver is in the cab. To achieve it, wireless networks and video streaming technologies gain importance as they should provide real-time track visualization and obstacle detection capabilities to the remote driver. Low latency camera capture, onboard media processing devices, and streaming protocols adapted for wireless links are the necessary enablers to be developed and integrated into the railway infrastructure. This paper compares video streaming protocols such as Real-Time Streaming Protocol (RTSP) and Web Real-Time Communication (WebRTC), as they are the main alternatives based on Real-time Transport Protocol (RTP) protocol to enable low latency. As latency is the main performance metric, this paper also provides a solution to calculate the End-to-End video streaming latency analytically. Finally, the paper proposes a rate control algorithm to adapt the video stream depending on the network capacity. The objective is to keep the latency as low as possible while avoiding any visual artifacts. The proposed solutions are tested in different setups and scenarios to prove their effectiveness before the planned field testing.","sentences":["Remote driving aims to improve transport systems by promoting efficiency, sustainability, and accessibility.","In the railway sector, remote driving makes it possible to increase flexibility, as the driver no longer has to be in the cab.","However, this brings several challenges, as it has to provide at least the same level of safety obtained when the driver is in the cab.","To achieve it, wireless networks and video streaming technologies gain importance as they should provide real-time track visualization and obstacle detection capabilities to the remote driver.","Low latency camera capture, onboard media processing devices, and streaming protocols adapted for wireless links are the necessary enablers to be developed and integrated into the railway infrastructure.","This paper compares video streaming protocols such as Real-Time Streaming Protocol (RTSP) and Web Real-Time Communication (WebRTC), as they are the main alternatives based on Real-time Transport Protocol (RTP) protocol to enable low latency.","As latency is the main performance metric, this paper also provides a solution to calculate the End-to-End video streaming latency analytically.","Finally, the paper proposes a rate control algorithm to adapt the video stream depending on the network capacity.","The objective is to keep the latency as low as possible while avoiding any visual artifacts.","The proposed solutions are tested in different setups and scenarios to prove their effectiveness before the planned field testing."],"url":"http://arxiv.org/abs/2406.02062v1","category":"cs.NI"}
{"created":"2024-06-04 07:37:44","title":"Stochastic Carbon Footprint Tracing Methods in Power Systems","abstract":"As the penetration of distributed energy resources (DER) and renewable energy sources (RES) increases, carbon footprint tracking requires more granular analysis results. Existing carbon footprint tracking methods focus on deterministic steady-state analysis where the high uncertainties of RES cannot be considered. Considering the deficiency of the existing deterministic method, this paper proposes two stochastic carbon footprint tracking methods to cope with the impact of RES uncertainty on load-side carbon footprint tracing. The first method introduces probabilistic analysis in the framework of carbon emissions flow (CEF) to provide a global reference for the spatial characteristic of the power system component carbon intensity distribution. Considering that the CEF network expands with the increasing penetration of DERs, the second method can effectively improve the computational efficiency over the first method while ensuring the computational accuracy on the large power systems. These proposed models are tested and compared in a synthetic 1004-bus test system in the case study to demonstrate the performance of the two proposed methods","sentences":["As the penetration of distributed energy resources (DER) and renewable energy sources (RES) increases, carbon footprint tracking requires more granular analysis results.","Existing carbon footprint tracking methods focus on deterministic steady-state analysis where the high uncertainties of RES cannot be considered.","Considering the deficiency of the existing deterministic method, this paper proposes two stochastic carbon footprint tracking methods to cope with the impact of RES uncertainty on load-side carbon footprint tracing.","The first method introduces probabilistic analysis in the framework of carbon emissions flow (CEF) to provide a global reference for the spatial characteristic of the power system component carbon intensity distribution.","Considering that the CEF network expands with the increasing penetration of DERs, the second method can effectively improve the computational efficiency over the first method while ensuring the computational accuracy on the large power systems.","These proposed models are tested and compared in a synthetic 1004-bus test system in the case study to demonstrate the performance of the two proposed methods"],"url":"http://arxiv.org/abs/2406.02055v1","category":"eess.SY"}
{"created":"2024-06-04 07:26:00","title":"Phase control of transmission and reflection in a sample of duplicated two-level systems driven by a stationary control field","abstract":"In this article, we investigate the optical response of a duplicated two-level atomic medium submitted to a strong stationnary control field and a weak co-propagating probe field, orthogonally polarized to each other. We show that both reflected and transmitted components of the probe may be absorbed and amplified. Moreover, for low optical depths, reflection and transmission factors are controlled by the relative phase between control and probe fields, which makes the configuration we present here promising for the development of optical devices, such as phase-controlled switches.","sentences":["In this article, we investigate the optical response of a duplicated two-level atomic medium submitted to a strong stationnary control field and a weak co-propagating probe field, orthogonally polarized to each other.","We show that both reflected and transmitted components of the probe may be absorbed and amplified.","Moreover, for low optical depths, reflection and transmission factors are controlled by the relative phase between control and probe fields, which makes the configuration we present here promising for the development of optical devices, such as phase-controlled switches."],"url":"http://arxiv.org/abs/2406.02043v1","category":"quant-ph"}
{"created":"2024-06-04 07:25:36","title":"A New Puzzling Periodic Signal in GeV Energies of the $\u03b3$-Ray Binary LS I +61$^\\circ$303","abstract":"LS I + 61$^\\circ$303 is a high-mass X-ray binary system comprising a massive Be star and a rapidly rotating neutron star. Its spectral energy distribution across multi-wavelengths categorizes it as a $\\gamma$-ray binary system. In our analysis of LS I + 61$^\\circ$303 using Fermi-LAT observations, we not only confirmed the three previously discussed periodicities of orbital, superorbital, and orbital-superorbital beat periods observed in multi-wavelength observations, but also identified an additional periodic signal. This newly discovered signal exhibits a period of $\\sim$26.3 day at a $\\sim7\\sigma$ confidence level. Moreover, the power spectrum peak of the new signal gradually decreases as the energy increases across the energy ranges of 0.1-0.3, 0.3-1.0, and 1.0-500.0 GeV. Interestingly, a potential signal with a similar period was found in data obtained from the Owens Valley Radio Observatory 40 m telescope. We suggest that the newly discovered periodic signal may originate from a coupling between the orbital period and the retrograde stellar precession period.","sentences":["LS I + 61$^\\circ$303 is a high-mass X-ray binary system comprising a massive Be star and a rapidly rotating neutron star.","Its spectral energy distribution across multi-wavelengths categorizes it as a $\\gamma$-ray binary system.","In our analysis of LS I","+ 61$^\\circ$303 using Fermi-LAT observations, we not only confirmed the three previously discussed periodicities of orbital, superorbital, and orbital-superorbital beat periods observed in multi-wavelength observations, but also identified an additional periodic signal.","This newly discovered signal exhibits a period of $\\sim$26.3 day at a $\\sim7\\sigma$ confidence level.","Moreover, the power spectrum peak of the new signal gradually decreases as the energy increases across the energy ranges of 0.1-0.3, 0.3-1.0, and 1.0-500.0 GeV.","Interestingly, a potential signal with a similar period was found in data obtained from the Owens Valley Radio Observatory 40 m telescope.","We suggest that the newly discovered periodic signal may originate from a coupling between the orbital period and the retrograde stellar precession period."],"url":"http://arxiv.org/abs/2406.02042v1","category":"astro-ph.HE"}
{"created":"2024-06-04 07:22:15","title":"First close-coupling study of the excitation of a large cyclic molecule: collision of c-C5H6 with He","abstract":"Recent astronomical observations revealed an increasing molecular complexity in the interstellar medium through the detection of a series of large cyclic carbon species. To correctly interpret these detections, a complex analysis is necessary that takes into account the non-local thermodynamic equilibrium (non-LTE) conditions of the emitting media (e.g. when energy level populations deviate from a Boltzman distribution). This requires proper state-to-state collisional data for the excitation and de-excitation processes of the molecular levels. Cyclopentadiene (c-C5H6), which was recently detected in multiple cold interstellar clouds, is extensively studied in many aspects due to its large importance for chemistry in general. At the same time, there are no collisional data available for this species, which are necessary for a more precise interpretation of the corresponding detections. In this work, we first provide an accurate 3D rigid-rotor interaction potential for the [c-C5H6 + He] complex from high-level of ab initio theories, which has been used to study their inelastic collision by the exact close coupling quantum scattering method. To the best of our knowledge, this is the first study where this method is systematically applied to treat the dynamics of molecular collisions involving more than ten atoms. We also analyse the collisional propensity rules and the differences in contrast to calculations, where the approximate coupled states scattering methods is used.","sentences":["Recent astronomical observations revealed an increasing molecular complexity in the interstellar medium through the detection of a series of large cyclic carbon species.","To correctly interpret these detections, a complex analysis is necessary that takes into account the non-local thermodynamic equilibrium (non-LTE) conditions of the emitting media (e.g. when energy level populations deviate from a Boltzman distribution).","This requires proper state-to-state collisional data for the excitation and de-excitation processes of the molecular levels.","Cyclopentadiene (c-C5H6), which was recently detected in multiple cold interstellar clouds, is extensively studied in many aspects due to its large importance for chemistry in general.","At the same time, there are no collisional data available for this species, which are necessary for a more precise interpretation of the corresponding detections.","In this work, we first provide an accurate 3D rigid-rotor interaction potential for the [c-C5H6 +","He] complex from high-level of ab initio theories, which has been used to study their inelastic collision by the exact close coupling quantum scattering method.","To the best of our knowledge, this is the first study where this method is systematically applied to treat the dynamics of molecular collisions involving more than ten atoms.","We also analyse the collisional propensity rules and the differences in contrast to calculations, where the approximate coupled states scattering methods is used."],"url":"http://arxiv.org/abs/2406.02036v1","category":"physics.atm-clus"}
{"created":"2024-06-04 07:19:02","title":"Method for Verifying Solutions of Sparse Linear Systems with General Coefficients","abstract":"This paper proposes a verification method for sparse linear systems $Ax=b$ with general and nonsingular coefficients. A verification method produces the error bound for a given approximate solution. Conventional methods use one of two approaches. One approach is to verify the computed solution of the normal equation $A^TAx=A^Tb$ by exploiting symmetric and positive definiteness; however, the condition number of $A^TA$ is the square of that for $A$. The other approach uses an approximate inverse matrix of the coefficient; however, the approximate inverse may be dense even if $A$ is sparse. Here, we propose a method for the verification of solutions of sparse linear systems based on $LDL^T$ decomposition. The proposed method can reduce the fill-in and is applicable to many problems. Moreover, an efficient iterative refinement method is proposed for obtaining accurate solutions.","sentences":["This paper proposes a verification method for sparse linear systems $Ax=b$ with general and nonsingular coefficients.","A verification method produces the error bound for a given approximate solution.","Conventional methods use one of two approaches.","One approach is to verify the computed solution of the normal equation $A^TAx=A^Tb$ by exploiting symmetric and positive definiteness; however, the condition number of $A^TA$ is the square of that for $A$.","The other approach uses an approximate inverse matrix of the coefficient; however, the approximate inverse may be dense even if $A$ is sparse.","Here, we propose a method for the verification of solutions of sparse linear systems based on $LDL^T$ decomposition.","The proposed method can reduce the fill-in and is applicable to many problems.","Moreover, an efficient iterative refinement method is proposed for obtaining accurate solutions."],"url":"http://arxiv.org/abs/2406.02033v1","category":"math.NA"}
{"created":"2024-06-04 07:12:23","title":"Electric Vehicle Adoption Modeling in France: A Systematic Literature Review","abstract":"France is one of the pioneer countries in the use of Electric Vehicles (EVs). The French government aims to complete the transition to EVs by 2040. Therefore, modeling related to the adoption of EVs is needed in order to determine the potential policies needed to achieve this goal. This modeling is based on a literature study to identify the factors and the causal relationship between those factors. The systematic literature review (SLR) analysis was performed on 20 journals selected based on PRISMA filtering. From this SLR analysis, five direct factors and four indirect factors were obtained which were then used as the basis for modeling. Based on the model developed, four balancing (B) loops and three reinforcing (R) loops were obtained. From the analysis, it was found that the advertising factor has a goal seeking structure, while the word of mouth, environmentally friendly image, and total cost of ownership factors have an S-shaped structure.","sentences":["France is one of the pioneer countries in the use of Electric Vehicles (EVs).","The French government aims to complete the transition to EVs by 2040.","Therefore, modeling related to the adoption of EVs is needed in order to determine the potential policies needed to achieve this goal.","This modeling is based on a literature study to identify the factors and the causal relationship between those factors.","The systematic literature review (SLR) analysis was performed on 20 journals selected based on PRISMA filtering.","From this SLR analysis, five direct factors and four indirect factors were obtained which were then used as the basis for modeling.","Based on the model developed, four balancing (B) loops and three reinforcing (R) loops were obtained.","From the analysis, it was found that the advertising factor has a goal seeking structure, while the word of mouth, environmentally friendly image, and total cost of ownership factors have an S-shaped structure."],"url":"http://arxiv.org/abs/2406.02029v1","category":"eess.SY"}
{"created":"2024-06-04 07:05:28","title":"Cosmological dynamics of interacting dark energy and dark matter in $f(Q)$ gravity","abstract":"In this work, we explore the behavior of interacting dark energy and dark matter within a model of $f(Q)$ gravity, employing a standard framework of dynamical system analysis. We consider the power-law $f(Q)$ model incorporating with two different forms of interacting dark energy and dark matter: $3\\alpha H\\rho_m$ and $\\frac{\\alpha}{3H}\\rho_m \\rho_{DE}$. The evolution of $\\Omega_m, \\Omega_r, \\Omega_{DE}, q$, and $\\omega$ for different values of the model parameter $n$ and the interaction parameter $\\alpha$ has been examined. Our results show that the universe was dominated by matter in the early stages and will be dominated by dark energy in later stages. Using the observational data, the fixed points are found to be stable and can be represented the de Sitter and quintessence acceleration solutions. We discover that the dynamical profiles of the universe in $f(Q)$ dark energy models are influenced by both the interaction term and the relevant model parameters.","sentences":["In this work, we explore the behavior of interacting dark energy and dark matter within a model of $f(Q)$ gravity, employing a standard framework of dynamical system analysis.","We consider the power-law $f(Q)$ model incorporating with two different forms of interacting dark energy and dark matter: $3\\alpha H\\rho_m$ and $\\frac{\\alpha}{3H}\\rho_m \\rho_{DE}$. The evolution of $\\Omega_m, \\Omega_r, \\Omega_{DE}, q$, and $\\omega$ for different values of the model parameter $n$ and the interaction parameter $\\alpha$ has been examined.","Our results show that the universe was dominated by matter in the early stages and will be dominated by dark energy in later stages.","Using the observational data, the fixed points are found to be stable and can be represented the de Sitter and quintessence acceleration solutions.","We discover that the dynamical profiles of the universe in $f(Q)$ dark energy models are influenced by both the interaction term and the relevant model parameters."],"url":"http://arxiv.org/abs/2406.02026v1","category":"gr-qc"}
{"created":"2024-06-04 07:02:59","title":"Verifying the Generalization of Deep Learning to Out-of-Distribution Domains","abstract":"Deep neural networks (DNNs) play a crucial role in the field of machine learning, demonstrating state-of-the-art performance across various application domains. However, despite their success, DNN-based models may occasionally exhibit challenges with generalization, i.e., may fail to handle inputs that were not encountered during training. This limitation is a significant challenge when it comes to deploying deep learning for safety-critical tasks, as well as in real-world settings characterized by substantial variability. We introduce a novel approach for harnessing DNN verification technology to identify DNN-driven decision rules that exhibit robust generalization to previously unencountered input domains. Our method assesses generalization within an input domain by measuring the level of agreement between independently trained deep neural networks for inputs in this domain. We also efficiently realize our approach by using off-the-shelf DNN verification engines, and extensively evaluate it on both supervised and unsupervised DNN benchmarks, including a deep reinforcement learning (DRL) system for Internet congestion control -- demonstrating the applicability of our approach for real-world settings. Moreover, our research introduces a fresh objective for formal verification, offering the prospect of mitigating the challenges linked to deploying DNN-driven systems in real-world scenarios.","sentences":["Deep neural networks (DNNs) play a crucial role in the field of machine learning, demonstrating state-of-the-art performance across various application domains.","However, despite their success, DNN-based models may occasionally exhibit challenges with generalization, i.e., may fail to handle inputs that were not encountered during training.","This limitation is a significant challenge when it comes to deploying deep learning for safety-critical tasks, as well as in real-world settings characterized by substantial variability.","We introduce a novel approach for harnessing DNN verification technology to identify DNN-driven decision rules that exhibit robust generalization to previously unencountered input domains.","Our method assesses generalization within an input domain by measuring the level of agreement between independently trained deep neural networks for inputs in this domain.","We also efficiently realize our approach by using off-the-shelf DNN verification engines, and extensively evaluate it on both supervised and unsupervised DNN benchmarks, including a deep reinforcement learning (DRL) system for Internet congestion control -- demonstrating the applicability of our approach for real-world settings.","Moreover, our research introduces a fresh objective for formal verification, offering the prospect of mitigating the challenges linked to deploying DNN-driven systems in real-world scenarios."],"url":"http://arxiv.org/abs/2406.02024v1","category":"cs.LG"}
{"created":"2024-06-04 07:02:53","title":"ShadowBound: Efficient Heap Memory Protection Through Advanced Metadata Management and Customized Compiler Optimization","abstract":"In software development, the prevalence of unsafe languages such as C and C++ introduces potential vulnerabilities, especially within the heap, a pivotal component for dynamic memory allocation. Despite its significance, heap management complexities have made heap corruption pervasive, posing severe threats to system security. While prior solutions aiming for temporal and spatial memory safety exhibit overheads deemed impractical, we present ShadowBound, a unique heap memory protection design. At its core, ShadowBound is an efficient out-of-bounds defense that can work with various use-after-free defenses (e.g. MarkUs, FFMalloc, PUMM) without compatibility constraints. We harness a shadow memory-based metadata management mechanism to store heap chunk boundaries and apply customized compiler optimizations tailored for boundary checking. We implemented ShadowBound atop the LLVM framework and integrated three state-of-the-art use-after-free defenses. Our evaluations show that ShadowBound provides robust heap protection with minimal time and memory overhead, suggesting its effectiveness and efficiency in safeguarding real-world programs against prevalent heap vulnerabilities.","sentences":["In software development, the prevalence of unsafe languages such as C and C++ introduces potential vulnerabilities, especially within the heap, a pivotal component for dynamic memory allocation.","Despite its significance, heap management complexities have made heap corruption pervasive, posing severe threats to system security.","While prior solutions aiming for temporal and spatial memory safety exhibit overheads deemed impractical, we present ShadowBound, a unique heap memory protection design.","At its core, ShadowBound is an efficient out-of-bounds defense that can work with various use-after-free defenses (e.g. MarkUs, FFMalloc, PUMM) without compatibility constraints.","We harness a shadow memory-based metadata management mechanism to store heap chunk boundaries and apply customized compiler optimizations tailored for boundary checking.","We implemented ShadowBound atop the LLVM framework and integrated three state-of-the-art use-after-free defenses.","Our evaluations show that ShadowBound provides robust heap protection with minimal time and memory overhead, suggesting its effectiveness and efficiency in safeguarding real-world programs against prevalent heap vulnerabilities."],"url":"http://arxiv.org/abs/2406.02023v1","category":"cs.CR"}
{"created":"2024-06-04 06:59:10","title":"A large population of neutron star low-mass X-ray binaries with long outburst recurrence time ?","abstract":"Low-mass X-ray binaries (LMXBs) with neutron stars show quite different features which depend on the rate of mass transfer from the donor star. With a high transfer rate the Z sources are in a persistent soft spectral state, with a moderate rate the transient Atoll sources have outburst cycles like the black hole X-ray binaries. The observations document very long outburst recurrence times for quite a number of sources. We follow with our computations the evolution of the accretion disc until the onset of the ionization instability. For sources with a low mass transfer rate the accumulation of matter in the disc is essentially reduced due to the continuous evaporation of matter from the disc to the coronal flow. Different mass transfer rates result in nearly the same amount of matter accumulated for the outburst which means the outburst properties are similar for sources with short and sources with long outburst cycles, contrary to some expectations. Then of systems with long recurrence time less sources will be detected and the total population of LMXBs could be larger than it appears. This would relieve the apparent problem that the observed number of LMXBs as progenitors of millisecond pulsars (MSP) is too small compared to the number of MSP. Concerning the few quasi-persistent sources with year-long soft states we argue that these states are not outbursts, but quasi-stationary hot states as in Z sources.","sentences":["Low-mass X-ray binaries (LMXBs) with neutron stars show quite different features which depend on the rate of mass transfer from the donor star.","With a high transfer rate the Z sources are in a persistent soft spectral state, with a moderate rate the transient Atoll sources have outburst cycles like the black hole X-ray binaries.","The observations document very long outburst recurrence times for quite a number of sources.","We follow with our computations the evolution of the accretion disc until the onset of the ionization instability.","For sources with a low mass transfer rate the accumulation of matter in the disc is essentially reduced due to the continuous evaporation of matter from the disc to the coronal flow.","Different mass transfer rates result in nearly the same amount of matter accumulated for the outburst which means the outburst properties are similar for sources with short and sources with long outburst cycles, contrary to some expectations.","Then of systems with long recurrence time less sources will be detected and the total population of LMXBs could be larger than it appears.","This would relieve the apparent problem that the observed number of LMXBs as progenitors of millisecond pulsars (MSP) is too small compared to the number of MSP.","Concerning the few quasi-persistent sources with year-long soft states we argue that these states are not outbursts, but quasi-stationary hot states as in Z sources."],"url":"http://arxiv.org/abs/2406.02020v1","category":"astro-ph.HE"}
{"created":"2024-06-04 06:57:12","title":"On the Mode-Seeking Properties of Langevin Dynamics","abstract":"The Langevin Dynamics framework, which aims to generate samples from the score function of a probability distribution, is widely used for analyzing and interpreting score-based generative modeling. While the convergence behavior of Langevin Dynamics under unimodal distributions has been extensively studied in the literature, in practice the data distribution could consist of multiple distinct modes. In this work, we investigate Langevin Dynamics in producing samples from multimodal distributions and theoretically study its mode-seeking properties. We prove that under a variety of sub-Gaussian mixtures, Langevin Dynamics is unlikely to find all mixture components within a sub-exponential number of steps in the data dimension. To reduce the mode-seeking tendencies of Langevin Dynamics, we propose Chained Langevin Dynamics, which divides the data vector into patches of constant size and generates every patch sequentially conditioned on the previous patches. We perform a theoretical analysis of Chained Langevin Dynamics by reducing it to sampling from a constant-dimensional distribution. We present the results of several numerical experiments on synthetic and real image datasets, supporting our theoretical results on the iteration complexities of sample generation from mixture distributions using the chained and vanilla Langevin Dynamics. The code is available at https://github.com/Xiwei-Cheng/Chained_LD.","sentences":["The Langevin Dynamics framework, which aims to generate samples from the score function of a probability distribution, is widely used for analyzing and interpreting score-based generative modeling.","While the convergence behavior of Langevin Dynamics under unimodal distributions has been extensively studied in the literature, in practice the data distribution could consist of multiple distinct modes.","In this work, we investigate Langevin Dynamics in producing samples from multimodal distributions and theoretically study its mode-seeking properties.","We prove that under a variety of sub-Gaussian mixtures, Langevin Dynamics is unlikely to find all mixture components within a sub-exponential number of steps in the data dimension.","To reduce the mode-seeking tendencies of Langevin Dynamics, we propose Chained Langevin Dynamics, which divides the data vector into patches of constant size and generates every patch sequentially conditioned on the previous patches.","We perform a theoretical analysis of Chained Langevin Dynamics by reducing it to sampling from a constant-dimensional distribution.","We present the results of several numerical experiments on synthetic and real image datasets, supporting our theoretical results on the iteration complexities of sample generation from mixture distributions using the chained and vanilla Langevin Dynamics.","The code is available at https://github.com/Xiwei-Cheng/Chained_LD."],"url":"http://arxiv.org/abs/2406.02017v1","category":"cs.LG"}
{"created":"2024-06-04 06:56:41","title":"Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization","abstract":"We propose adaptive, line search-free second-order methods with optimal rate of convergence for solving convex-concave min-max problems. By means of an adaptive step size, our algorithms feature a simple update rule that requires solving only one linear system per iteration, eliminating the need for line search or backtracking mechanisms. Specifically, we base our algorithms on the optimistic method and appropriately combine it with second-order information. Moreover, distinct from common adaptive schemes, we define the step size recursively as a function of the gradient norm and the prediction error in the optimistic update. We first analyze a variant where the step size requires knowledge of the Lipschitz constant of the Hessian. Under the additional assumption of Lipschitz continuous gradients, we further design a parameter-free version by tracking the Hessian Lipschitz constant locally and ensuring the iterates remain bounded. We also evaluate the practical performance of our algorithm by comparing it to existing second-order algorithms for minimax optimization.","sentences":["We propose adaptive, line search-free second-order methods with optimal rate of convergence for solving convex-concave min-max problems.","By means of an adaptive step size, our algorithms feature a simple update rule that requires solving only one linear system per iteration, eliminating the need for line search or backtracking mechanisms.","Specifically, we base our algorithms on the optimistic method and appropriately combine it with second-order information.","Moreover, distinct from common adaptive schemes, we define the step size recursively as a function of the gradient norm and the prediction error in the optimistic update.","We first analyze a variant where the step size requires knowledge of the Lipschitz constant of the Hessian.","Under the additional assumption of Lipschitz continuous gradients, we further design a parameter-free version by tracking the Hessian Lipschitz constant locally and ensuring the iterates remain bounded.","We also evaluate the practical performance of our algorithm by comparing it to existing second-order algorithms for minimax optimization."],"url":"http://arxiv.org/abs/2406.02016v1","category":"math.OC"}
{"created":"2024-06-04 17:52:07","title":"LongBet: Heterogeneous Treatment Effect Estimation in Panel Data","abstract":"This paper introduces a novel approach for estimating heterogeneous treatment effects of binary treatment in panel data, particularly focusing on short panel data with large cross-sectional data and observed confoundings. In contrast to traditional literature in difference-in-differences method that often relies on the parallel trend assumption, our proposed model does not necessitate such an assumption. Instead, it leverages observed confoundings to impute potential outcomes and identify treatment effects. The method presented is a Bayesian semi-parametric approach based on the Bayesian causal forest model, which is extended here to suit panel data settings. The approach offers the advantage of the Bayesian approach to provides uncertainty quantification on the estimates. Simulation studies demonstrate its performance with and without the presence of parallel trend. Additionally, our proposed model enables the estimation of conditional average treatment effects, a capability that is rarely available in panel data settings.","sentences":["This paper introduces a novel approach for estimating heterogeneous treatment effects of binary treatment in panel data, particularly focusing on short panel data with large cross-sectional data and observed confoundings.","In contrast to traditional literature in difference-in-differences method that often relies on the parallel trend assumption, our proposed model does not necessitate such an assumption.","Instead, it leverages observed confoundings to impute potential outcomes and identify treatment effects.","The method presented is a Bayesian semi-parametric approach based on the Bayesian causal forest model, which is extended here to suit panel data settings.","The approach offers the advantage of the Bayesian approach to provides uncertainty quantification on the estimates.","Simulation studies demonstrate its performance with and without the presence of parallel trend.","Additionally, our proposed model enables the estimation of conditional average treatment effects, a capability that is rarely available in panel data settings."],"url":"http://arxiv.org/abs/2406.02530v1","category":"stat.ME"}
{"created":"2024-06-04 17:21:43","title":"Atomicity in integral domains","abstract":"In algebra, atomicity is the study of divisibility by and factorizations into atoms (also called irreducibles). In one side of the spectrum of atomicity we find the antimatter algebraic structures, inside which there are no atoms and, therefore, divisibility by and factorizations into atoms are not possible. In the other (more interesting) side of the spectrum, we find the atomic algebraic structures, where essentially every element factors into atoms (the study of such objects is known as factorization theory). In this paper, we survey some of the most fundamental results on the atomicity of cancellative commutative monoids and integral domains, putting our emphasis on the latter. We mostly consider the realm of atomic domains. For integral domains, the distinction between being atomic and satisfying the ascending chain condition on principal ideals, or ACCP for short (which is a stronger and better-behaved algebraic condition) is subtle, so atomicity has been often studied in connection with the ACCP: we consider this connection at many parts of this survey. We discuss atomicity under various classical algebraic constructions, including localization, polynomial extensions, $D+M$ constructions, and monoid algebras. Integral domains having all their subrings atomic are also discussed. In the last section, we explore the middle ground of the spectrum of atomicity: some integral domains where some of but not all the elements factor into atoms, which are called quasi-atomic and almost atomic. We conclude providing techniques from homological algebra to measure how far quasi-atomic and almost atomic domains are from being atomic.","sentences":["In algebra, atomicity is the study of divisibility by and factorizations into atoms (also called irreducibles).","In one side of the spectrum of atomicity we find the antimatter algebraic structures, inside which there are no atoms and, therefore, divisibility by and factorizations into atoms are not possible.","In the other (more interesting) side of the spectrum, we find the atomic algebraic structures, where essentially every element factors into atoms (the study of such objects is known as factorization theory).","In this paper, we survey some of the most fundamental results on the atomicity of cancellative commutative monoids and integral domains, putting our emphasis on the latter.","We mostly consider the realm of atomic domains.","For integral domains, the distinction between being atomic and satisfying the ascending chain condition on principal ideals, or ACCP for short (which is a stronger and better-behaved algebraic condition) is subtle, so atomicity has been often studied in connection with the ACCP: we consider this connection at many parts of this survey.","We discuss atomicity under various classical algebraic constructions, including localization, polynomial extensions, $D+M$ constructions, and monoid algebras.","Integral domains having all their subrings atomic are also discussed.","In the last section, we explore the middle ground of the spectrum of atomicity: some integral domains where some of but not all the elements factor into atoms, which are called quasi-atomic and almost atomic.","We conclude providing techniques from homological algebra to measure how far quasi-atomic and almost atomic domains are from being atomic."],"url":"http://arxiv.org/abs/2406.02503v1","category":"math.AC"}
{"created":"2024-06-04 16:47:58","title":"A Schur-Weyl duality analogue based on a commutative bilinear operation","abstract":"Schur-Weyl duality concerns the actions of $\\text{GL}_{n}(\\mathbb{C})$ and $S_{k}$ on tensor powers of the form $V^{\\otimes k}$ for an $n$-dimensional vector space $V$. There are rich histories within representation theory, combinatorics, and statistical mechanics involving the study and use of diagram algebras, which arise through the restriction of the action of $\\text{GL}_{n}(\\mathbb{C})$ to subgroups of $\\text{GL}_{n}(\\mathbb{C})$. This leads us to consider further variants of Schur-Weyl duality, with the use of variants of the tensor space $V^{\\otimes k}$. Instead of taking repeated tensor products of $V$, we make use of a freest commutative bilinear operation in place of $\\otimes$, and this is motivated by an associated invariance property given by the action of $S_{k}$. By then taking the centralizer algebra with respect to the action of the group of permutation matrices in $\\text{GL}_{n}(\\mathbb{C})$, this gives rise to a diagram-like algebra spanned by a new class of combinatorial objects. We construct orbit-type bases for the centralizer algebras introduced in this paper, and we apply these bases to prove a combinatorial formula for the dimensions of our centralizer algebras.","sentences":["Schur-Weyl duality concerns the actions of $\\text{GL}_{n}(\\mathbb{C})$ and $S_{k}$ on tensor powers of the form $V^{\\otimes k}$ for an $n$-dimensional vector space $V$. There are rich histories within representation theory, combinatorics, and statistical mechanics involving the study and use of diagram algebras, which arise through the restriction of the action of $\\text{GL}_{n}(\\mathbb{C})$ to subgroups of $\\text{GL}_{n}(\\mathbb{C})$. This leads us to consider further variants of Schur-Weyl duality, with the use of variants of the tensor space $V^{\\otimes k}$. Instead of taking repeated tensor products of $V$, we make use of a freest commutative bilinear operation in place of $\\otimes$, and this is motivated by an associated invariance property given by the action of $S_{k}$. By then taking the centralizer algebra with respect to the action of the group of permutation matrices in $\\text{GL}_{n}(\\mathbb{C})$, this gives rise to a diagram-like algebra spanned by a new class of combinatorial objects.","We construct orbit-type bases for the centralizer algebras introduced in this paper, and we apply these bases to prove a combinatorial formula for the dimensions of our centralizer algebras."],"url":"http://arxiv.org/abs/2406.02478v1","category":"math.RT"}
{"created":"2024-06-04 16:43:29","title":"Dimension of the deformation space of ordinary representations in the cyclotomic limit","abstract":"The weight two ordinary deformations are unobstructed in the cyclotomic limit under certain assumptions. We show that such an ordinary deformation ring over the cyclotomic tower can have arbitrarily large dimension.","sentences":["The weight two ordinary deformations are unobstructed in the cyclotomic limit under certain assumptions.","We show that such an ordinary deformation ring over the cyclotomic tower can have arbitrarily large dimension."],"url":"http://arxiv.org/abs/2406.02473v1","category":"math.NT"}
{"created":"2024-06-04 16:20:06","title":"Series of combinatorial games","abstract":"We present a tentative definition for the sum of a sequence of combinatorial games. This sum turns out to be invariant under Conway equivalence and coincides with the classical sum in the case of a converging sequence of real numbers. It coincides with the infinitary natural sum in the case of a sequence of ordinal numbers.","sentences":["We present a tentative definition for the sum of a sequence of combinatorial games.","This sum turns out to be invariant under Conway equivalence and coincides with the classical sum in the case of a converging sequence of real numbers.","It coincides with the infinitary natural sum in the case of a sequence of ordinal numbers."],"url":"http://arxiv.org/abs/2406.02453v1","category":"math.CO"}
{"created":"2024-06-04 16:07:49","title":"On a Modified Directional Distortional Hardening Model for Metal Plasticity","abstract":"This study proposes a modification of the yield condition that overcomes the mathematical constraints of the Directional Distortional Hardening models developed by Feigenbaum and Dafalias. This modified model surpasses the mathematical inconsistency of the complete model and the limitation of the r-model of Feigenbaum and Dafalias. In the complete model, the inconsistency of distortional term in the yield surface and the plastic part of free energy appears in the absence of kinematic hardening. In addition, the simplified r-model fails to capture the flattening of the yield surface in the opposite direction of loading due to the absence of a fourth-order internal variable in the distortional term. Hence, this study incorporates a decoupled distortional hardening term in the equation of yield surface that makes it possible to simultaneously capture the flattening of the yield surface and allows isotropic hardening with distortion, even in the absence of kinematic hardening. The mathematical formulation of the proposed modified model is also developed, which sets the stage for future experimental investigations and validation efforts.","sentences":["This study proposes a modification of the yield condition that overcomes the mathematical constraints of the Directional Distortional Hardening models developed by Feigenbaum and Dafalias.","This modified model surpasses the mathematical inconsistency of the complete model and the limitation of the r-model of Feigenbaum and Dafalias.","In the complete model, the inconsistency of distortional term in the yield surface and the plastic part of free energy appears in the absence of kinematic hardening.","In addition, the simplified r-model fails to capture the flattening of the yield surface in the opposite direction of loading due to the absence of a fourth-order internal variable in the distortional term.","Hence, this study incorporates a decoupled distortional hardening term in the equation of yield surface that makes it possible to simultaneously capture the flattening of the yield surface and allows isotropic hardening with distortion, even in the absence of kinematic hardening.","The mathematical formulation of the proposed modified model is also developed, which sets the stage for future experimental investigations and validation efforts."],"url":"http://arxiv.org/abs/2406.02446v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 15:39:49","title":"IterMask2: Iterative Unsupervised Anomaly Segmentation via Spatial and Frequency Masking for Brain Lesions in MRI","abstract":"Unsupervised anomaly segmentation approaches to pathology segmentation train a model on images of healthy subjects, that they define as the 'normal' data distribution. At inference, they aim to segment any pathologies in new images as 'anomalies', as they exhibit patterns that deviate from those in 'normal' training data. Prevailing methods follow the 'corrupt-and-reconstruct' paradigm. They intentionally corrupt an input image, reconstruct it to follow the learned 'normal' distribution, and subsequently segment anomalies based on reconstruction error. Corrupting an input image, however, inevitably leads to suboptimal reconstruction even of normal regions, causing false positives. To alleviate this, we propose a novel iterative spatial mask-refining strategy IterMask2. We iteratively mask areas of the image, reconstruct them, and update the mask based on reconstruction error. This iterative process progressively adds information about areas that are confidently normal as per the model. The increasing content guides reconstruction of nearby masked areas, improving reconstruction of normal tissue under these areas, reducing false positives. We also use high-frequency image content as an auxiliary input to provide additional structural information for masked areas. This further improves reconstruction error of normal in comparison to anomalous areas, facilitating segmentation of the latter. We conduct experiments on several brain lesion datasets and demonstrate effectiveness of our method. Code is available at: https://github.com/ZiyunLiang/IterMasks2","sentences":["Unsupervised anomaly segmentation approaches to pathology segmentation train a model on images of healthy subjects, that they define as the 'normal' data distribution.","At inference, they aim to segment any pathologies in new images as 'anomalies', as they exhibit patterns that deviate from those in 'normal' training data.","Prevailing methods follow the 'corrupt-and-reconstruct' paradigm.","They intentionally corrupt an input image, reconstruct it to follow the learned 'normal' distribution, and subsequently segment anomalies based on reconstruction error.","Corrupting an input image, however, inevitably leads to suboptimal reconstruction even of normal regions, causing false positives.","To alleviate this, we propose a novel iterative spatial mask-refining strategy IterMask2.","We iteratively mask areas of the image, reconstruct them, and update the mask based on reconstruction error.","This iterative process progressively adds information about areas that are confidently normal as per the model.","The increasing content guides reconstruction of nearby masked areas, improving reconstruction of normal tissue under these areas, reducing false positives.","We also use high-frequency image content as an auxiliary input to provide additional structural information for masked areas.","This further improves reconstruction error of normal in comparison to anomalous areas, facilitating segmentation of the latter.","We conduct experiments on several brain lesion datasets and demonstrate effectiveness of our method.","Code is available at: https://github.com/ZiyunLiang/IterMasks2"],"url":"http://arxiv.org/abs/2406.02422v1","category":"eess.IV"}
{"created":"2024-06-04 15:27:53","title":"Improved Modelling of Federated Datasets using Mixtures-of-Dirichlet-Multinomials","abstract":"In practice, training using federated learning can be orders of magnitude slower than standard centralized training. This severely limits the amount of experimentation and tuning that can be done, making it challenging to obtain good performance on a given task. Server-side proxy data can be used to run training simulations, for instance for hyperparameter tuning. This can greatly speed up the training pipeline by reducing the number of tuning runs to be performed overall on the true clients. However, it is challenging to ensure that these simulations accurately reflect the dynamics of the real federated training. In particular, the proxy data used for simulations often comes as a single centralized dataset without a partition into distinct clients, and partitioning this data in a naive way can lead to simulations that poorly reflect real federated training. In this paper we address the challenge of how to partition centralized data in a way that reflects the statistical heterogeneity of the true federated clients. We propose a fully federated, theoretically justified, algorithm that efficiently learns the distribution of the true clients and observe improved server-side simulations when using the inferred distribution to create simulated clients from the centralized data.","sentences":["In practice, training using federated learning can be orders of magnitude slower than standard centralized training.","This severely limits the amount of experimentation and tuning that can be done, making it challenging to obtain good performance on a given task.","Server-side proxy data can be used to run training simulations, for instance for hyperparameter tuning.","This can greatly speed up the training pipeline by reducing the number of tuning runs to be performed overall on the true clients.","However, it is challenging to ensure that these simulations accurately reflect the dynamics of the real federated training.","In particular, the proxy data used for simulations often comes as a single centralized dataset without a partition into distinct clients, and partitioning this data in a naive way can lead to simulations that poorly reflect real federated training.","In this paper we address the challenge of how to partition centralized data in a way that reflects the statistical heterogeneity of the true federated clients.","We propose a fully federated, theoretically justified, algorithm that efficiently learns the distribution of the true clients and observe improved server-side simulations when using the inferred distribution to create simulated clients from the centralized data."],"url":"http://arxiv.org/abs/2406.02416v1","category":"cs.LG"}
{"created":"2024-06-04 15:14:10","title":"Online Fair Allocation of Perishable Resources","abstract":"We consider a practically motivated variant of the canonical online fair allocation problem: a decision-maker has a budget of perishable resources to allocate over a fixed number of rounds. Each round sees a random number of arrivals, and the decision-maker must commit to an allocation for these individuals before moving on to the next round. The goal is to construct a sequence of allocations that is envy-free and efficient. Our work makes two important contributions toward this problem: we first derive strong lower bounds on the optimal envy-efficiency trade-off that demonstrate that a decision-maker is fundamentally limited in what she can hope to achieve relative to the no-perishing setting; we then design an algorithm achieving these lower bounds which takes as input $(i)$ a prediction of the perishing order, and $(ii)$ a desired bound on envy. Given the remaining budget in each period, the algorithm uses forecasts of future demand and perishing to adaptively choose one of two carefully constructed guardrail quantities. We demonstrate our algorithm's strong numerical performance - and state-of-the-art, perishing-agnostic algorithms' inefficacy - on simulations calibrated to a real-world dataset.","sentences":["We consider a practically motivated variant of the canonical online fair allocation problem: a decision-maker has a budget of perishable resources to allocate over a fixed number of rounds.","Each round sees a random number of arrivals, and the decision-maker must commit to an allocation for these individuals before moving on to the next round.","The goal is to construct a sequence of allocations that is envy-free and efficient.","Our work makes two important contributions toward this problem: we first derive strong lower bounds on the optimal envy-efficiency trade-off that demonstrate that a decision-maker is fundamentally limited in what she can hope to achieve relative to the no-perishing setting; we then design an algorithm achieving these lower bounds which takes as input $(i)$ a prediction of the perishing order, and $(ii)$ a desired bound on envy.","Given the remaining budget in each period, the algorithm uses forecasts of future demand and perishing to adaptively choose one of two carefully constructed guardrail quantities.","We demonstrate our algorithm's strong numerical performance - and state-of-the-art, perishing-agnostic algorithms' inefficacy - on simulations calibrated to a real-world dataset."],"url":"http://arxiv.org/abs/2406.02402v1","category":"math.OC"}
{"created":"2024-06-04 15:13:41","title":"Can a Few Decide for Many? The Metric Distortion of Sortition","abstract":"Recent works have studied the design of algorithms for selecting representative sortition panels. However, the most central question remains unaddressed: Do these panels reflect the entire population's opinion? We present a positive answer by adopting the concept of metric distortion from computational social choice, which aims to quantify how much a panel's decision aligns with the ideal decision of the population when preferences and agents lie on a metric space. We show that uniform selection needs only logarithmically many agents in terms of the number of alternatives to achieve almost optimal distortion. We also show that Fair Greedy Capture, a selection algorithm introduced recently by Ebadian & Micha (2024), matches uniform selection's guarantees of almost optimal distortion and also achieves constant ex-post distortion, ensuring a \"best of both worlds\" performance.","sentences":["Recent works have studied the design of algorithms for selecting representative sortition panels.","However, the most central question remains unaddressed: Do these panels reflect the entire population's opinion?","We present a positive answer by adopting the concept of metric distortion from computational social choice, which aims to quantify how much a panel's decision aligns with the ideal decision of the population when preferences and agents lie on a metric space.","We show that uniform selection needs only logarithmically many agents in terms of the number of alternatives to achieve almost optimal distortion.","We also show that Fair Greedy Capture, a selection algorithm introduced recently by Ebadian & Micha (2024), matches uniform selection's guarantees of almost optimal distortion and also achieves constant ex-post distortion, ensuring a \"best of both worlds\" performance."],"url":"http://arxiv.org/abs/2406.02400v1","category":"cs.GT"}
{"created":"2024-06-04 14:53:24","title":"Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs","abstract":"The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can consistently perform well even at high compression ratios, which also offers significant benefits in terms of inference cost and throughput.","sentences":["The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs).","However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level.","This decline can be attributed to the loss of key information during the compression process.","Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios.","As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context.","Additionally, we employ a dynamic compression strategy.","We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets.","Experimental results show that QGC can consistently perform well even at high compression ratios, which also offers significant benefits in terms of inference cost and throughput."],"url":"http://arxiv.org/abs/2406.02376v1","category":"cs.CL"}
{"created":"2024-06-04 14:52:31","title":"Some aspects of the theory of nodal orders","abstract":"In this paper, we elaborate ring theoretic properties of nodal orders. In particular, we prove that they are closed under taking crossed products with finite groups.","sentences":["In this paper, we elaborate ring theoretic properties of nodal orders.","In particular, we prove that they are closed under taking crossed products with finite groups."],"url":"http://arxiv.org/abs/2406.02375v1","category":"math.RT"}
{"created":"2024-06-04 14:06:03","title":"Probing the Category of Verbal Aspect in Transformer Language Models","abstract":"We investigate how pretrained language models (PLM) encode the grammatical category of verbal aspect in Russian. Encoding of aspect in transformer LMs has not been studied previously in any language. A particular challenge is posed by \"alternative contexts\": where either the perfective or the imperfective aspect is suitable grammatically and semantically. We perform probing using BERT and RoBERTa on alternative and non-alternative contexts. First, we assess the models' performance on aspect prediction, via behavioral probing. Next, we examine the models' performance when their contextual representations are substituted with counterfactual representations, via causal probing. These counterfactuals alter the value of the \"boundedness\" feature--a semantic feature, which characterizes the action in the context. Experiments show that BERT and RoBERTa do encode aspect--mostly in their final layers. The counterfactual interventions affect perfective and imperfective in opposite ways, which is consistent with grammar: perfective is positively affected by adding the meaning of boundedness, and vice versa. The practical implications of our probing results are that fine-tuning only the last layers of BERT on predicting aspect is faster and more effective than fine-tuning the whole model. The model has high predictive uncertainty about aspect in alternative contexts, which tend to lack explicit hints about the boundedness of the described action.","sentences":["We investigate how pretrained language models (PLM) encode the grammatical category of verbal aspect in Russian.","Encoding of aspect in transformer LMs has not been studied previously in any language.","A particular challenge is posed by \"alternative contexts\": where either the perfective or the imperfective aspect is suitable grammatically and semantically.","We perform probing using BERT and RoBERTa on alternative and non-alternative contexts.","First, we assess the models' performance on aspect prediction, via behavioral probing.","Next, we examine the models' performance when their contextual representations are substituted with counterfactual representations, via causal probing.","These counterfactuals alter the value of the \"boundedness\" feature--a semantic feature, which characterizes the action in the context.","Experiments show that BERT and RoBERTa do encode aspect--mostly in their final layers.","The counterfactual interventions affect perfective and imperfective in opposite ways, which is consistent with grammar: perfective is positively affected by adding the meaning of boundedness, and vice versa.","The practical implications of our probing results are that fine-tuning only the last layers of BERT on predicting aspect is faster and more effective than fine-tuning the whole model.","The model has high predictive uncertainty about aspect in alternative contexts, which tend to lack explicit hints about the boundedness of the described action."],"url":"http://arxiv.org/abs/2406.02335v1","category":"cs.CL"}
{"created":"2024-06-04 13:52:24","title":"A Bayesian nonlinear stationary model with multiple frequencies for business cycle analysis","abstract":"We design a novel, nonlinear single-source-of-error model for analysis of multiple business cycles. The model's specification is intended to capture key empirical characteristics of business cycle data by allowing for simultaneous cycles of different types and lengths, as well as time-variable amplitude and phase shift. The model is shown to feature relevant theoretical properties, including stationarity and pseudo-cyclical autocovariance function, and enables a decomposition of overall cyclic fluctuations into separate frequency-specific components. We develop a Bayesian framework for estimation and inference in the model, along with an MCMC procedure for posterior sampling, combining the Gibbs sampler and the Metropolis-Hastings algorithm, suitably adapted to address encountered numerical issues. Empirical results obtained from the model applied to the Polish GDP growth rates imply co-existence of two types of economic fluctuations: the investment and inventory cycles, and support the stochastic variability of the amplitude and phase shift, also capturing some business cycle asymmetries. Finally, the Bayesian framework enables a fully probabilistic inference on the business cycle clocks and dating, which seems the most relevant approach in view of economic uncertainties.","sentences":["We design a novel, nonlinear single-source-of-error model for analysis of multiple business cycles.","The model's specification is intended to capture key empirical characteristics of business cycle data by allowing for simultaneous cycles of different types and lengths, as well as time-variable amplitude and phase shift.","The model is shown to feature relevant theoretical properties, including stationarity and pseudo-cyclical autocovariance function, and enables a decomposition of overall cyclic fluctuations into separate frequency-specific components.","We develop a Bayesian framework for estimation and inference in the model, along with an MCMC procedure for posterior sampling, combining the Gibbs sampler and the Metropolis-Hastings algorithm, suitably adapted to address encountered numerical issues.","Empirical results obtained from the model applied to the Polish GDP growth rates imply co-existence of two types of economic fluctuations: the investment and inventory cycles, and support the stochastic variability of the amplitude and phase shift, also capturing some business cycle asymmetries.","Finally, the Bayesian framework enables a fully probabilistic inference on the business cycle clocks and dating, which seems the most relevant approach in view of economic uncertainties."],"url":"http://arxiv.org/abs/2406.02321v1","category":"stat.ME"}
{"created":"2024-06-04 13:41:07","title":"Disentangled Representation via Variational AutoEncoder for Continuous Treatment Effect Estimation","abstract":"Continuous treatment effect estimation holds significant practical importance across various decision-making and assessment domains, such as healthcare and the military. However, current methods for estimating dose-response curves hinge on balancing the entire representation by treating all covariates as confounding variables. Although various approaches disentangle covariates into different factors for treatment effect estimation, they are confined to binary treatment settings. Moreover, observational data are often tainted with non-causal noise information that is imperceptible to the human. Hence, in this paper, we propose a novel Dose-Response curve estimator via Variational AutoEncoder (DRVAE) disentangled covariates representation. Our model is dedicated to disentangling covariates into instrumental factors, confounding factors, adjustment factors, and external noise factors, thereby facilitating the estimation of treatment effects under continuous treatment settings by balancing the disentangled confounding factors. Extensive results on synthetic and semi-synthetic datasets demonstrate that our model outperforms the current state-of-the-art methods.","sentences":["Continuous treatment effect estimation holds significant practical importance across various decision-making and assessment domains, such as healthcare and the military.","However, current methods for estimating dose-response curves hinge on balancing the entire representation by treating all covariates as confounding variables.","Although various approaches disentangle covariates into different factors for treatment effect estimation, they are confined to binary treatment settings.","Moreover, observational data are often tainted with non-causal noise information that is imperceptible to the human.","Hence, in this paper, we propose a novel Dose-Response curve estimator via Variational AutoEncoder (DRVAE) disentangled covariates representation.","Our model is dedicated to disentangling covariates into instrumental factors, confounding factors, adjustment factors, and external noise factors, thereby facilitating the estimation of treatment effects under continuous treatment settings by balancing the disentangled confounding factors.","Extensive results on synthetic and semi-synthetic datasets demonstrate that our model outperforms the current state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.02310v1","category":"cs.LG"}
{"created":"2024-06-04 13:13:29","title":"Composite Quantile Regression With XGBoost Using the Novel Arctan Pinball Loss","abstract":"This paper explores the use of XGBoost for composite quantile regression. XGBoost is a highly popular model renowned for its flexibility, efficiency, and capability to deal with missing data. The optimization uses a second order approximation of the loss function, complicating the use of loss functions with a zero or vanishing second derivative. Quantile regression -- a popular approach to obtain conditional quantiles when point estimates alone are insufficient -- unfortunately uses such a loss function, the pinball loss. Existing workarounds are typically inefficient and can result in severe quantile crossings. In this paper, we present a smooth approximation of the pinball loss, the arctan pinball loss, that is tailored to the needs of XGBoost. Specifically, contrary to other smooth approximations, the arctan pinball loss has a relatively large second derivative, which makes it more suitable to use in the second order approximation. Using this loss function enables the simultaneous prediction of multiple quantiles, which is more efficient and results in far fewer quantile crossings.","sentences":["This paper explores the use of XGBoost for composite quantile regression.","XGBoost is a highly popular model renowned for its flexibility, efficiency, and capability to deal with missing data.","The optimization uses a second order approximation of the loss function, complicating the use of loss functions with a zero or vanishing second derivative.","Quantile regression -- a popular approach to obtain conditional quantiles when point estimates alone are insufficient -- unfortunately uses such a loss function, the pinball loss.","Existing workarounds are typically inefficient and can result in severe quantile crossings.","In this paper, we present a smooth approximation of the pinball loss, the arctan pinball loss, that is tailored to the needs of XGBoost.","Specifically, contrary to other smooth approximations, the arctan pinball loss has a relatively large second derivative, which makes it more suitable to use in the second order approximation.","Using this loss function enables the simultaneous prediction of multiple quantiles, which is more efficient and results in far fewer quantile crossings."],"url":"http://arxiv.org/abs/2406.02293v1","category":"stat.ML"}
{"created":"2024-06-04 13:05:47","title":"A Study of Optimizations for Fine-tuning Large Language Models","abstract":"Fine-tuning large language models is a popular choice among users trying to adapt them for specific applications. However, fine-tuning these models is a demanding task because the user has to examine several factors, such as resource budget, runtime, model size and context length among others. A specific challenge is that fine-tuning is memory intensive, imposing constraints on the required hardware memory and context length of training data that can be handled. In this work, we share a detailed study on a variety of fine-tuning optimizations across different fine-tuning scenarios. In particular, we assess Gradient Checkpointing, Low Rank Adaptation, DeepSpeed's ZeRO Redundancy Optimizer and Flash Attention. With a focus on memory and runtime, we examine the impact of different optimization combinations on GPU memory usage and execution runtime during fine-tuning phase. We provide recommendation on best default optimization for balancing memory and runtime across diverse model sizes. We share effective strategies for fine-tuning very large models with tens or hundreds of billions of parameters and enabling large context lengths during fine-tuning. Furthermore, we propose the appropriate optimization mixtures for fine-tuning under GPU resource limitations.","sentences":["Fine-tuning large language models is a popular choice among users trying to adapt them for specific applications.","However, fine-tuning these models is a demanding task because the user has to examine several factors, such as resource budget, runtime, model size and context length among others.","A specific challenge is that fine-tuning is memory intensive, imposing constraints on the required hardware memory and context length of training data that can be handled.","In this work, we share a detailed study on a variety of fine-tuning optimizations across different fine-tuning scenarios.","In particular, we assess Gradient Checkpointing, Low Rank Adaptation, DeepSpeed's ZeRO Redundancy Optimizer and Flash Attention.","With a focus on memory and runtime, we examine the impact of different optimization combinations on GPU memory usage and execution runtime during fine-tuning phase.","We provide recommendation on best default optimization for balancing memory and runtime across diverse model sizes.","We share effective strategies for fine-tuning very large models with tens or hundreds of billions of parameters and enabling large context lengths during fine-tuning.","Furthermore, we propose the appropriate optimization mixtures for fine-tuning under GPU resource limitations."],"url":"http://arxiv.org/abs/2406.02290v1","category":"cs.LG"}
{"created":"2024-06-04 12:56:10","title":"Test-Time Regret Minimization in Meta Reinforcement Learning","abstract":"Meta reinforcement learning sets a distribution over a set of tasks on which the agent can train at will, then is asked to learn an optimal policy for any test task efficiently. In this paper, we consider a finite set of tasks modeled through Markov decision processes with various dynamics. We assume to have endured a long training phase, from which the set of tasks is perfectly recovered, and we focus on regret minimization against the optimal policy in the unknown test task. Under a separation condition that states the existence of a state-action pair revealing a task against another, Chen et al. (2022) show that $O(M^2 \\log(H))$ regret can be achieved, where $M, H$ are the number of tasks in the set and test episodes, respectively. In our first contribution, we demonstrate that the latter rate is nearly optimal by developing a novel lower bound for test-time regret minimization under separation, showing that a linear dependence with $M$ is unavoidable. Then, we present a family of stronger yet reasonable assumptions beyond separation, which we call strong identifiability, enabling algorithms achieving fast rates $\\log (H)$ and sublinear dependence with $M$ simultaneously. Our paper provides a new understanding of the statistical barriers of test-time regret minimization and when fast rates can be achieved.","sentences":["Meta reinforcement learning sets a distribution over a set of tasks on which the agent can train at will, then is asked to learn an optimal policy for any test task efficiently.","In this paper, we consider a finite set of tasks modeled through Markov decision processes with various dynamics.","We assume to have endured a long training phase, from which the set of tasks is perfectly recovered, and we focus on regret minimization against the optimal policy in the unknown test task.","Under a separation condition that states the existence of a state-action pair revealing a task against another, Chen et al. (2022) show that $O(M^2 \\log(H))$ regret can be achieved, where $M, H$ are the number of tasks in the set and test episodes, respectively.","In our first contribution, we demonstrate that the latter rate is nearly optimal by developing a novel lower bound for test-time regret minimization under separation, showing that a linear dependence with $M$ is unavoidable.","Then, we present a family of stronger yet reasonable assumptions beyond separation, which we call strong identifiability, enabling algorithms achieving fast rates $\\log (H)$ and sublinear dependence with $M$ simultaneously.","Our paper provides a new understanding of the statistical barriers of test-time regret minimization and when fast rates can be achieved."],"url":"http://arxiv.org/abs/2406.02282v1","category":"cs.LG"}
{"created":"2024-06-04 12:49:46","title":"A KL-based Analysis Framework with Applications to Non-Descent Optimization Methods","abstract":"We propose a novel analysis framework for non-descent-type optimization methodologies in nonconvex scenarios based on the Kurdyka-Lojasiewicz property. Our framework allows covering a broad class of algorithms, including those commonly employed in stochastic and distributed optimization. Specifically, it enables the analysis of first-order methods that lack a sufficient descent property and do not require access to full (deterministic) gradient information. We leverage this framework to establish, for the first time, iterate convergence and the corresponding rates for the decentralized gradient method and federated averaging under mild assumptions. Furthermore, based on the new analysis techniques, we show the convergence of the random reshuffling and stochastic gradient descent method without necessitating typical a priori bounded iterates assumptions.","sentences":["We propose a novel analysis framework for non-descent-type optimization methodologies in nonconvex scenarios based on the Kurdyka-Lojasiewicz property.","Our framework allows covering a broad class of algorithms, including those commonly employed in stochastic and distributed optimization.","Specifically, it enables the analysis of first-order methods that lack a sufficient descent property and do not require access to full (deterministic) gradient information.","We leverage this framework to establish, for the first time, iterate convergence and the corresponding rates for the decentralized gradient method and federated averaging under mild assumptions.","Furthermore, based on the new analysis techniques, we show the convergence of the random reshuffling and stochastic gradient descent method without necessitating typical a priori bounded iterates assumptions."],"url":"http://arxiv.org/abs/2406.02273v1","category":"math.OC"}
{"created":"2024-06-04 12:27:57","title":"Intrinsic spin Hall effect from spin quantum metric","abstract":"The intrinsic spin Hall effect (ISHE) proposed in [\\href{https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.92.126603} {Sinova \\textit{et al.} Phys. Rev. Lett. \\textbf{92}, 126603 (2004)}] is driven by the spin Berry curvature. Herein, we establish the concept of \\textit{spin quantum metric}, which is the counterpart of the spin Berry curvature in the \\textit{spin quantum geometric tensor} defined in a similar way to the quantum geometric tensor. Dual to the $\\mathcal{T}$-even ($\\mathcal{T}$, time reversal) spin Berry curvature, the spin quantum metric features a $\\mathcal{T}$-odd characteristic. Notably, we show that the $\\mathcal{T}$-odd spin quantum metric can also drive an ISHE ($\\mathcal{T}$-odd ISHE) under a high-frequency electric field. Guided by symmetry, we evaluate this $\\mathcal{T}$-odd ISHE in the magnetically tilted surface Dirac cone and in ferromagnetic monolayer MnBi$_2$Te$_4$. We find that this $\\mathcal{T}$-odd ISHE dominates when the Fermi level is close to the band (anti)crossing point, where its magnitude can be as large as the $\\mathcal{T}$-even ISHE when an infrared driving field is applied. Our work not only uncovers an indispensable ingredient in the emergent community of quantum geometric physics but also offers a novel mechanism for ultrafast spintronics.","sentences":["The intrinsic spin Hall effect (ISHE) proposed in [\\href{https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.92.126603} {Sinova \\textit{et al.} Phys.","Rev. Lett.","\\textbf{92}, 126603 (2004)}] is driven by the spin Berry curvature.","Herein, we establish the concept of \\textit{spin quantum metric}, which is the counterpart of the spin Berry curvature in the \\textit{spin quantum geometric tensor} defined in a similar way to the quantum geometric tensor.","Dual to the $\\mathcal{T}$-even ($\\mathcal{T}$, time reversal) spin Berry curvature, the spin quantum metric features a $\\mathcal{T}$-odd characteristic.","Notably, we show that the $\\mathcal{T}$-odd spin quantum metric can also drive an ISHE ($\\mathcal{T}$-odd ISHE) under a high-frequency electric field.","Guided by symmetry, we evaluate this $\\mathcal{T}$-odd ISHE in the magnetically tilted surface Dirac cone and in ferromagnetic monolayer MnBi$_2$Te$_4$. We find that this $\\mathcal{T}$-odd ISHE dominates when the Fermi level is close to the band (anti)crossing point, where its magnitude can be as large as the $\\mathcal{T}$-even ISHE when an infrared driving field is applied.","Our work not only uncovers an indispensable ingredient in the emergent community of quantum geometric physics but also offers a novel mechanism for ultrafast spintronics."],"url":"http://arxiv.org/abs/2406.02257v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 12:02:49","title":"Enabling Decision-Making with the Modified Causal Forest: Policy Trees for Treatment Assignment","abstract":"Decision-making plays a pivotal role in shaping outcomes in various disciplines, such as medicine, economics, and business. This paper provides guidance to practitioners on how to implement a decision tree designed to address treatment assignment policies using an interpretable and non-parametric algorithm. Our Policy Tree is motivated on the method proposed by Zhou, Athey, and Wager (2023), distinguishing itself for the policy score calculation, incorporating constraints, and handling categorical and continuous variables. We demonstrate the usage of the Policy Tree for multiple, discrete treatments on data sets from different fields. The Policy Tree is available in Python's open-source package mcf (Modified Causal Forest).","sentences":["Decision-making plays a pivotal role in shaping outcomes in various disciplines, such as medicine, economics, and business.","This paper provides guidance to practitioners on how to implement a decision tree designed to address treatment assignment policies using an interpretable and non-parametric algorithm.","Our Policy Tree is motivated on the method proposed by Zhou, Athey, and Wager (2023), distinguishing itself for the policy score calculation, incorporating constraints, and handling categorical and continuous variables.","We demonstrate the usage of the Policy Tree for multiple, discrete treatments on data sets from different fields.","The Policy Tree is available in Python's open-source package mcf (Modified Causal Forest)."],"url":"http://arxiv.org/abs/2406.02241v1","category":"econ.EM"}
{"created":"2024-06-04 11:18:21","title":"Enhancing the sensitivity of quantum fiber-optical gyroscopes via a non-Gaussian-state probe","abstract":"We propose a theoretical scheme to enhance the sensitivity of a quantum fiber-optical gyroscope (QFOG) via a non-Gaussian-state probe based on quadrature measurements of the optical field. The non-Gaussian-state probe utilizes the product state comprising a photon-added coherent state (PACS) with photon excitations and a coherent state CS. We study the sensitivity of the QFOG, and find that it can be significantly enhanced through increasing the photon excitations in the PACS probe. We investigate the influence of photon loss on the performance of QFOG and demonstrate that the PACS probe exhibits robust resistance to photon loss. Furthermore, we compare the performance of the QFOG using the PACS probe against two Gaussian-state probes: the CS probe and the squeezed state (SS) probe and indicate that the PACS probe offers a significant advantage in terms of sensitivity, regardless of photon loss, under the constraint condition of the same total number of input photons. Particularly, it is found that the sensitivity of the PACS probe can be three orders of magnitude higher than that of two Gaussian-state probes for certain values of the measured parameter. The capabilities of the non-Gaussian state probe on enhancing the sensitivity and resisting photon loss could have a wide-ranging impact on future high-performance QFOGs.","sentences":["We propose a theoretical scheme to enhance the sensitivity of a quantum fiber-optical gyroscope (QFOG) via a non-Gaussian-state probe based on quadrature measurements of the optical field.","The non-Gaussian-state probe utilizes the product state comprising a photon-added coherent state (PACS) with photon excitations and a coherent state CS.","We study the sensitivity of the QFOG, and find that it can be significantly enhanced through increasing the photon excitations in the PACS probe.","We investigate the influence of photon loss on the performance of QFOG and demonstrate that the PACS probe exhibits robust resistance to photon loss.","Furthermore, we compare the performance of the QFOG using the PACS probe against two Gaussian-state probes: the CS probe and the squeezed state (SS) probe and indicate that the PACS probe offers a significant advantage in terms of sensitivity, regardless of photon loss, under the constraint condition of the same total number of input photons.","Particularly, it is found that the sensitivity of the PACS probe can be three orders of magnitude higher than that of two Gaussian-state probes for certain values of the measured parameter.","The capabilities of the non-Gaussian state probe on enhancing the sensitivity and resisting photon loss could have a wide-ranging impact on future high-performance QFOGs."],"url":"http://arxiv.org/abs/2406.02217v1","category":"quant-ph"}
{"created":"2024-06-04 10:30:43","title":"Achieving Stability for Aloha Networks with Multiple Receivers","abstract":"Slotted Aloha has been widely adopted in various communication networks. Yet if the transmission probabilities and traffic input rates of transmitters are not properly regulated, their data queues may easily become unstable. For stability analysis of Aloha networks with multiple receivers, the focus of previous studies has been placed on the maximum input rate of each transmitter, below which the network is guaranteed to be stabilized under any given topology. By assuming a fixed and identical transmission probability across the network, however, network stability is found to be unachievable when the input rate exceeds zero.   As we will demonstrate in this paper, the key to stabilizing the network lies in proper selection of transmission probabilities according to the traffic input rates and locations of all transmitters and receivers. Specifically, for an Aloha network with multiple capture receivers, by establishing and solving the fixed-point equations of the steady-state probabilities of successful transmissions of Head-of-Line (HOL) packets, the exact service rates of all transmitters' queues are obtained, based on which the operating region of transmission probabilities for achieving stability and the stability region of input rates are further characterized. The results are illustrated in various scenarios of multi-cell and ad-hoc networks. Simulation results validate the analysis and corroborate that the network can be stabilized as long as the traffic input rates are within the stability region, and the transmission probabilities are properly adjusted according to the traffic input rates and network topology.","sentences":["Slotted Aloha has been widely adopted in various communication networks.","Yet if the transmission probabilities and traffic input rates of transmitters are not properly regulated, their data queues may easily become unstable.","For stability analysis of Aloha networks with multiple receivers, the focus of previous studies has been placed on the maximum input rate of each transmitter, below which the network is guaranteed to be stabilized under any given topology.","By assuming a fixed and identical transmission probability across the network, however, network stability is found to be unachievable when the input rate exceeds zero.   ","As we will demonstrate in this paper, the key to stabilizing the network lies in proper selection of transmission probabilities according to the traffic input rates and locations of all transmitters and receivers.","Specifically, for an Aloha network with multiple capture receivers, by establishing and solving the fixed-point equations of the steady-state probabilities of successful transmissions of Head-of-Line (HOL) packets, the exact service rates of all transmitters' queues are obtained, based on which the operating region of transmission probabilities for achieving stability and the stability region of input rates are further characterized.","The results are illustrated in various scenarios of multi-cell and ad-hoc networks.","Simulation results validate the analysis and corroborate that the network can be stabilized as long as the traffic input rates are within the stability region, and the transmission probabilities are properly adjusted according to the traffic input rates and network topology."],"url":"http://arxiv.org/abs/2406.02186v1","category":"cs.NI"}
{"created":"2024-06-04 09:56:05","title":"Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition via Weakly Phonetic Supervision","abstract":"There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pre-training with phonetic or graphemic transcription, and self-supervised pre-training. We find that pre-training with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pre-training with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency.It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. To support reproducibility and promote future research along this direction, we will release the code, models and data for the whole pipeline of Whistle at https://github.com/thu-spmi/CAT upon publication.","sentences":["There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pre-training with phonetic or graphemic transcription, and self-supervised pre-training.","We find that pre-training with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages.","This paper explores the approach of pre-training with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle.","We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models.","We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages.","A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR.","Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency.","It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency.","To support reproducibility and promote future research along this direction, we will release the code, models and data for the whole pipeline of Whistle at https://github.com/thu-spmi/CAT upon publication."],"url":"http://arxiv.org/abs/2406.02166v1","category":"cs.SD"}
{"created":"2024-06-04 09:34:46","title":"UA-Track: Uncertainty-Aware End-to-End 3D Multi-Object Tracking","abstract":"3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception. Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task. However, existing methods overlook the uncertainty issue, which refers to the lack of precise confidence about the state and location of tracked objects. Uncertainty arises owing to various factors during motion observation by cameras, especially occlusions and the small size of target objects, resulting in an inaccurate estimation of the object's position, label, and identity. To this end, we propose an Uncertainty-Aware 3D MOT framework, UA-Track, which tackles the uncertainty problem from multiple aspects. Specifically, we first introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty in object prediction with probabilistic attention. Secondly, we propose an Uncertainty-guided Query Denoising strategy to further enhance the training process. We also utilize Uncertainty-reduced Query Initialization, which leverages predicted 2D object location and depth information to reduce query uncertainty. As a result, our UA-Track achieves state-of-the-art performance on the nuScenes benchmark, i.e., 66.3% AMOTA on the test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA.","sentences":["3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception.","Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task.","However, existing methods overlook the uncertainty issue, which refers to the lack of precise confidence about the state and location of tracked objects.","Uncertainty arises owing to various factors during motion observation by cameras, especially occlusions and the small size of target objects, resulting in an inaccurate estimation of the object's position, label, and identity.","To this end, we propose an Uncertainty-Aware 3D MOT framework, UA-Track, which tackles the uncertainty problem from multiple aspects.","Specifically, we first introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty in object prediction with probabilistic attention.","Secondly, we propose an Uncertainty-guided Query Denoising strategy to further enhance the training process.","We also utilize Uncertainty-reduced Query Initialization, which leverages predicted 2D object location and depth information to reduce query uncertainty.","As a result, our UA-Track achieves state-of-the-art performance on the nuScenes benchmark, i.e., 66.3% AMOTA on the test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA."],"url":"http://arxiv.org/abs/2406.02147v1","category":"cs.CV"}
{"created":"2024-06-04 09:05:09","title":"clauseSMT: A NLSAT-Based Clause-Level Framework for Satisfiability Modulo Nonlinear Real Arithmetic Theory","abstract":"Model-constructing satisfiability calculus (MCSAT) framework has been applied to SMT problems on different arithmetic theories. NLSAT, an implementation using cylindrical algebraic decomposition for explanation, is especially competitive among nonlinear real arithmetic constraints. However, current Conflict-Driven Clause Learning (CDCL)-style algorithms only consider literal information for decision, and thus ignore clause-level influence on arithmetic variables. As a consequence, NLSAT encounters unnecessary conflicts caused by improper literal decisions.   In this work, we analyze the literal decision caused conflicts, and introduce clause-level information with a direct effect on arithmetic variables. Two main algorithm improvements are presented: clause-level feasible-set based look-ahead mechanism and arithmetic propagation based branching heuristic. We implement our solver named clauseSMT on our dynamic variable ordering framework. Experiments show that clauseSMT is competitive on nonlinear real arithmetic theory against existing SMT solvers (cvc5, Z3, Yices2), and outperforms all these solvers on satisfiable instances of SMT(QF_NRA) in SMT-LIB. The effectiveness of our proposed methods are also studied.","sentences":["Model-constructing satisfiability calculus (MCSAT) framework has been applied to SMT problems on different arithmetic theories.","NLSAT, an implementation using cylindrical algebraic decomposition for explanation, is especially competitive among nonlinear real arithmetic constraints.","However, current Conflict-Driven Clause Learning (CDCL)-style algorithms only consider literal information for decision, and thus ignore clause-level influence on arithmetic variables.","As a consequence, NLSAT encounters unnecessary conflicts caused by improper literal decisions.   ","In this work, we analyze the literal decision caused conflicts, and introduce clause-level information with a direct effect on arithmetic variables.","Two main algorithm improvements are presented: clause-level feasible-set based look-ahead mechanism and arithmetic propagation based branching heuristic.","We implement our solver named clauseSMT on our dynamic variable ordering framework.","Experiments show that clauseSMT is competitive on nonlinear real arithmetic theory against existing SMT solvers (cvc5, Z3, Yices2), and outperforms all these solvers on satisfiable instances of SMT(QF_NRA) in SMT-LIB.","The effectiveness of our proposed methods are also studied."],"url":"http://arxiv.org/abs/2406.02122v1","category":"cs.SC"}
{"created":"2024-06-04 08:33:38","title":"Pseudo-transition between antiferromagnetic and charge orders in a minimal spin-pseudospin model of one-dimensional cuprates","abstract":"This study rigorously investigates the phenomenon of a pseudo-transition in a minimal spinpseudospin model, serving as a simplified model of one-dimensional cuprate chains [CuO]n, by making use of the transfer-matrix method. The studied spin-pseudospin model of one-dimensional cuprate chains [CuO]n reveals a peculiar pseudo-transition between a charged-ordered phase and an antiferromagnetic phase, which is accompanied with anomalous behavior of magnetic and thermodynamic quantities. While the entropy and short-range correlations undergo near the pseudo-transition steep continuous changes reminiscent of a discontinuous phase transition, the specific heat displays a sizable sharp peak akin to a continuous phase transition.","sentences":["This study rigorously investigates the phenomenon of a pseudo-transition in a minimal spinpseudospin model, serving as a simplified model of one-dimensional cuprate chains [CuO]n, by making use of the transfer-matrix method.","The studied spin-pseudospin model of one-dimensional cuprate chains [CuO]n reveals a peculiar pseudo-transition between a charged-ordered phase and an antiferromagnetic phase, which is accompanied with anomalous behavior of magnetic and thermodynamic quantities.","While the entropy and short-range correlations undergo near the pseudo-transition steep continuous changes reminiscent of a discontinuous phase transition, the specific heat displays a sizable sharp peak akin to a continuous phase transition."],"url":"http://arxiv.org/abs/2406.02104v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-04 08:15:08","title":"Long-Time Behavior of Zero-Sum Linear-Quadratic Stochastic Differential Games","abstract":"The paper investigates the long-time behavior of zero-sum linear-quadratic stochastic differential games, aiming to demonstrate that, under appropriate conditions, both the saddle strategy and the optimal state process exhibit the exponential turnpike property. Namely, for the majority of the time horizon, the distributions of the saddle strategy and the optimal state process closely stay near certain (time-invariant) distributions $\\nu_1^*$, $\\nu_2^*$ and $\\mu^*$, respectively. Additionally, as a byproduct, we solve the infinite horizon version of the differential game and derive closed-loop representations for its open-loop saddle strategy, which has not been proved in the literature.","sentences":["The paper investigates the long-time behavior of zero-sum linear-quadratic stochastic differential games, aiming to demonstrate that, under appropriate conditions, both the saddle strategy and the optimal state process exhibit the exponential turnpike property.","Namely, for the majority of the time horizon, the distributions of the saddle strategy and the optimal state process closely stay near certain (time-invariant) distributions $\\nu_1^*$, $\\nu_2^*$ and $\\mu^*$, respectively.","Additionally, as a byproduct, we solve the infinite horizon version of the differential game and derive closed-loop representations for its open-loop saddle strategy, which has not been proved in the literature."],"url":"http://arxiv.org/abs/2406.02089v1","category":"math.OC"}
{"created":"2024-06-04 08:07:03","title":"Predicting the Curie temperature of magnetic materials with automated calculations across chemistries and structures","abstract":"We develop a technique for predicting the Curie temperature of magnetic materials using density functional theory calculations suitable to include in high-throughput frameworks. We apply four different models, including physically relevant observables and assess numerical constants by studying 32 ferro- and ferrimagnets. With the best-performing model, the Curie temperature can be predicted with a mean absolute error of approximately 126 K. As predictive factors, the models consider either the energy differences between the magnetic ground state and a magnetically disordered paramagnetic state, or the average constraining fields acting on magnetic moments in a disordered local moments calculation. Additionally, the energy differences are refined by incorporating the magnetic entropy of the paramagnetic state and the number of nearest magnetic neighbors of the magnetic atoms. The most advanced model is found to extend well into Fe$_{1-x}$Co$_x$ alloys, indicating the potential efficacy of utilizing our model in designing materials with tailored Curie temperatures by altering alloy compositions. This examination can illuminate the factors influencing magnetic transition temperatures in magnetic materials and provide insights into how they can be employed to make quantitative predictions of Curie temperatures. Our approach is not restricted to specific crystal structures or chemical compositions. It offers a more cost-effective alternative, in terms of human time and need for hands-on oversight, to other density functional theory methods for predicting the Curie temperature. As a result, it provides a practical strategy for conducting high-throughput screening for new technologically applicable magnetic materials. Alternatively, it can complement ML-based screening of magnetic materials by integrating physical principles into such approaches, thereby enhancing their prediction accuracy.","sentences":["We develop a technique for predicting the Curie temperature of magnetic materials using density functional theory calculations suitable to include in high-throughput frameworks.","We apply four different models, including physically relevant observables and assess numerical constants by studying 32 ferro- and ferrimagnets.","With the best-performing model, the Curie temperature can be predicted with a mean absolute error of approximately 126 K. As predictive factors, the models consider either the energy differences between the magnetic ground state and a magnetically disordered paramagnetic state, or the average constraining fields acting on magnetic moments in a disordered local moments calculation.","Additionally, the energy differences are refined by incorporating the magnetic entropy of the paramagnetic state and the number of nearest magnetic neighbors of the magnetic atoms.","The most advanced model is found to extend well into Fe$_{1-x}$Co$_x$ alloys, indicating the potential efficacy of utilizing our model in designing materials with tailored Curie temperatures by altering alloy compositions.","This examination can illuminate the factors influencing magnetic transition temperatures in magnetic materials and provide insights into how they can be employed to make quantitative predictions of Curie temperatures.","Our approach is not restricted to specific crystal structures or chemical compositions.","It offers a more cost-effective alternative, in terms of human time and need for hands-on oversight, to other density functional theory methods for predicting the Curie temperature.","As a result, it provides a practical strategy for conducting high-throughput screening for new technologically applicable magnetic materials.","Alternatively, it can complement ML-based screening of magnetic materials by integrating physical principles into such approaches, thereby enhancing their prediction accuracy."],"url":"http://arxiv.org/abs/2406.02084v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 08:00:40","title":"Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks","abstract":"Information Extraction (IE) plays a crucial role in Natural Language Processing (NLP) by extracting structured information from unstructured text, thereby facilitating seamless integration with various real-world applications that rely on structured data. Despite its significance, recent experiments focusing on English IE tasks have shed light on the challenges faced by Large Language Models (LLMs) in achieving optimal performance, particularly in sub-tasks like Named Entity Recognition (NER). In this paper, we delve into a comprehensive investigation of the performance of mainstream Chinese open-source LLMs in tackling IE tasks, specifically under zero-shot conditions where the models are not fine-tuned for specific tasks. Additionally, we present the outcomes of several few-shot experiments to further gauge the capability of these models. Moreover, our study includes a comparative analysis between these open-source LLMs and ChatGPT, a widely recognized language model, on IE performance. Through meticulous experimentation and analysis, we aim to provide insights into the strengths, limitations, and potential enhancements of existing Chinese open-source LLMs in the domain of Information Extraction within the context of NLP.","sentences":["Information Extraction (IE) plays a crucial role in Natural Language Processing (NLP) by extracting structured information from unstructured text, thereby facilitating seamless integration with various real-world applications that rely on structured data.","Despite its significance, recent experiments focusing on English IE tasks have shed light on the challenges faced by Large Language Models (LLMs) in achieving optimal performance, particularly in sub-tasks like Named Entity Recognition (NER).","In this paper, we delve into a comprehensive investigation of the performance of mainstream Chinese open-source LLMs in tackling IE tasks, specifically under zero-shot conditions where the models are not fine-tuned for specific tasks.","Additionally, we present the outcomes of several few-shot experiments to further gauge the capability of these models.","Moreover, our study includes a comparative analysis between these open-source LLMs and ChatGPT, a widely recognized language model, on IE performance.","Through meticulous experimentation and analysis, we aim to provide insights into the strengths, limitations, and potential enhancements of existing Chinese open-source LLMs in the domain of Information Extraction within the context of NLP."],"url":"http://arxiv.org/abs/2406.02079v1","category":"cs.CL"}
{"created":"2024-06-04 07:54:10","title":"FaceCom: Towards High-fidelity 3D Facial Shape Completion via Optimization and Inpainting Guidance","abstract":"We propose FaceCom, a method for 3D facial shape completion, which delivers high-fidelity results for incomplete facial inputs of arbitrary forms. Unlike end-to-end shape completion methods based on point clouds or voxels, our approach relies on a mesh-based generative network that is easy to optimize, enabling it to handle shape completion for irregular facial scans. We first train a shape generator on a mixed 3D facial dataset containing 2405 identities. Based on the incomplete facial input, we fit complete faces using an optimization approach under image inpainting guidance. The completion results are refined through a post-processing step. FaceCom demonstrates the ability to effectively and naturally complete facial scan data with varying missing regions and degrees of missing areas. Our method can be used in medical prosthetic fabrication and the registration of deficient scanning data. Our experimental results demonstrate that FaceCom achieves exceptional performance in fitting and shape completion tasks. The code is available at https://github.com/dragonylee/FaceCom.git.","sentences":["We propose FaceCom, a method for 3D facial shape completion, which delivers high-fidelity results for incomplete facial inputs of arbitrary forms.","Unlike end-to-end shape completion methods based on point clouds or voxels, our approach relies on a mesh-based generative network that is easy to optimize, enabling it to handle shape completion for irregular facial scans.","We first train a shape generator on a mixed 3D facial dataset containing 2405 identities.","Based on the incomplete facial input, we fit complete faces using an optimization approach under image inpainting guidance.","The completion results are refined through a post-processing step.","FaceCom demonstrates the ability to effectively and naturally complete facial scan data with varying missing regions and degrees of missing areas.","Our method can be used in medical prosthetic fabrication and the registration of deficient scanning data.","Our experimental results demonstrate that FaceCom achieves exceptional performance in fitting and shape completion tasks.","The code is available at https://github.com/dragonylee/FaceCom.git."],"url":"http://arxiv.org/abs/2406.02074v1","category":"cs.CV"}
{"created":"2024-06-04 07:54:08","title":"ZTF SN Ia DR2: Colour standardisation of Type Ia Supernovae and its dependence on environment","abstract":"As Type Ia supernova cosmology transitions from a statistics dominated to a systematics dominated era, it is crucial to understand leftover unexplained uncertainties affecting their luminosity, such as the ones stemming from astrophysical biases. Indeed, SNe Ia are standardisable candles, whose absolute magnitude reach a 0.15~mag scatter once empirical correlations with their lightcurve stretch and colour and with their environment are accounted for. In this paper, we investigate how the standardisation process of SNe Ia depends on environment, to ultimately reduce their scatter in magnitude, focusing on colour standardisation. We use the volume-limited ZTF SN Ia DR2 sample, which offers unprecedented statistics for the low redshift ($z<0.06$) range. We first study the colour distribution, focusing on the effects of dust, to then select a dustless subsample of objects from low stellar mass environments and from the outskirts of their host galaxies. We then look at the colour-residuals relation and its associated parameter $\\beta$. Finally, we investigate the colour dependency of the environment-dependent magnitude offsets (steps), to try to disentangle intrinsic and extrinsic colour origin. Our sample probes well the red tail of the colour distribution, up to $c=0.8$. The dustless sample exhibits a significantly lower red tail ($4.6\\sigma$) in comparison to the whole sample. This suggests that reddening above $c\\geq0.2$ is dominated by host interstellar dust absorption. Looking at the colour-residuals relation, we find it to be linear with lightcurve colour. We show hints of a potential evolution of $\\beta$ with host stellar mass at a $2.5\\sigma$ level. Finally, unlike recent claims from the literature, we see no evolution of steps as a function of lightcurve colour, suggesting that dust may not be the dominating mechanism responsible for the environmental dependency of SNe Ia magnitude.","sentences":["As Type Ia supernova cosmology transitions from a statistics dominated to a systematics dominated era, it is crucial to understand leftover unexplained uncertainties affecting their luminosity, such as the ones stemming from astrophysical biases.","Indeed, SNe Ia are standardisable candles, whose absolute magnitude reach a 0.15~mag scatter once empirical correlations with their lightcurve stretch and colour and with their environment are accounted for.","In this paper, we investigate how the standardisation process of SNe Ia depends on environment, to ultimately reduce their scatter in magnitude, focusing on colour standardisation.","We use the volume-limited ZTF SN Ia DR2 sample, which offers unprecedented statistics for the low redshift ($z<0.06$) range.","We first study the colour distribution, focusing on the effects of dust, to then select a dustless subsample of objects from low stellar mass environments and from the outskirts of their host galaxies.","We then look at the colour-residuals relation and its associated parameter $\\beta$. Finally, we investigate the colour dependency of the environment-dependent magnitude offsets (steps), to try to disentangle intrinsic and extrinsic colour origin.","Our sample probes well the red tail of the colour distribution, up to $c=0.8$. The dustless sample exhibits a significantly lower red tail ($4.6\\sigma$) in comparison to the whole sample.","This suggests that reddening above $c\\geq0.2$ is dominated by host interstellar dust absorption.","Looking at the colour-residuals relation, we find it to be linear with lightcurve colour.","We show hints of a potential evolution of $\\beta$ with host stellar mass at a $2.5\\sigma$ level.","Finally, unlike recent claims from the literature, we see no evolution of steps as a function of lightcurve colour, suggesting that dust may not be the dominating mechanism responsible for the environmental dependency of SNe Ia magnitude."],"url":"http://arxiv.org/abs/2406.02072v1","category":"astro-ph.CO"}
{"created":"2024-06-04 07:36:18","title":"Geometric surgeries of three-dimensional flag structures and non-uniformizable examples","abstract":"In this paper, we introduce a notion of geometric surgery for flag structures, which are geometric structures locally modelled on the three-dimensional flag space under the action of ${\\mathrm{PGL}}_3(\\mathbb{R})$. Using such surgeries we provide examples of flag structures, of both uniformizable and non-uniformizable type.","sentences":["In this paper, we introduce a notion of geometric surgery for flag structures, which are geometric structures locally modelled on the three-dimensional flag space under the action of ${\\mathrm{PGL}}_3(\\mathbb{R})$. Using such surgeries we provide examples of flag structures, of both uniformizable and non-uniformizable type."],"url":"http://arxiv.org/abs/2406.02053v1","category":"math.DG"}
{"created":"2024-06-04 07:29:59","title":"Auto-Encoding or Auto-Regression? A Reality Check on Causality of Self-Attention-Based Sequential Recommenders","abstract":"The comparison between Auto-Encoding (AE) and Auto-Regression (AR) has become an increasingly important topic with recent advances in sequential recommendation. At the heart of this discussion lies the comparison of BERT4Rec and SASRec, which serve as representative AE and AR models for self-attentive sequential recommenders. Yet the conclusion of this debate remains uncertain due to: (1) the lack of fair and controlled environments for experiments and evaluations; and (2) the presence of numerous confounding factors w.r.t. feature selection, modeling choices and optimization algorithms. In this work, we aim to answer this question by conducting a series of controlled experiments. We start by tracing the AE/AR debate back to its origin through a systematic re-evaluation of SASRec and BERT4Rec, discovering that AR models generally surpass AE models in sequential recommendation. In addition, we find that AR models further outperforms AE models when using a customized design space that includes additional features, modeling approaches and optimization techniques. Furthermore, the performance advantage of AR models persists in the broader HuggingFace transformer ecosystems. Lastly, we provide potential explanations and insights into AE/AR performance from two key perspectives: low-rank approximation and inductive bias. We make our code and data available at https://github.com/yueqirex/ModSAR","sentences":["The comparison between Auto-Encoding (AE) and Auto-Regression (AR) has become an increasingly important topic with recent advances in sequential recommendation.","At the heart of this discussion lies the comparison of BERT4Rec and SASRec, which serve as representative AE and AR models for self-attentive sequential recommenders.","Yet the conclusion of this debate remains uncertain due to: (1) the lack of fair and controlled environments for experiments and evaluations; and (2) the presence of numerous confounding factors w.r.t.","feature selection, modeling choices and optimization algorithms.","In this work, we aim to answer this question by conducting a series of controlled experiments.","We start by tracing the AE/AR debate back to its origin through a systematic re-evaluation of SASRec and BERT4Rec, discovering that AR models generally surpass AE models in sequential recommendation.","In addition, we find that AR models further outperforms AE models when using a customized design space that includes additional features, modeling approaches and optimization techniques.","Furthermore, the performance advantage of AR models persists in the broader HuggingFace transformer ecosystems.","Lastly, we provide potential explanations and insights into AE/AR performance from two key perspectives: low-rank approximation and inductive bias.","We make our code and data available at https://github.com/yueqirex/ModSAR"],"url":"http://arxiv.org/abs/2406.02048v1","category":"cs.IR"}
{"created":"2024-06-04 07:24:58","title":"On $S$-injective modules","abstract":"Let $R$ be a commutative ring with an identity, and $S$ a multiplicative subset of $R$. In this paper, we introduce the notion of $S$-injective modules as a weak version of injective modules. Among other results, we provide an $S$-version of the Baer's characterisation of injective modules. We also give an $S$-version of the Lambek's characterization of flat modules: an $R$-module $M$ is $S$-flat if and only if its character, $Hom_{\\mathbb{Z}}(M,\\mathbb{Q}/\\mathbb{Z})$, is an $S$-injective $R$-module. As applications, we establish, under certain conditions, counterparts of Cheatham and Stone's characterizations for $S$-Noetherian rings using the notion of character modules.","sentences":["Let $R$ be a commutative ring with an identity, and $S$ a multiplicative subset of $R$. In this paper, we introduce the notion of $S$-injective modules as a weak version of injective modules.","Among other results, we provide an $S$-version of the Baer's characterisation of injective modules.","We also give an $S$-version of the Lambek's characterization of flat modules: an $R$-module $M$ is $S$-flat if and only if its character, $Hom_{\\mathbb{Z}}(M,\\mathbb{Q}/\\mathbb{Z})$, is an $S$-injective $R$-module.","As applications, we establish, under certain conditions, counterparts of Cheatham and Stone's characterizations for $S$-Noetherian rings using the notion of character modules."],"url":"http://arxiv.org/abs/2406.02041v1","category":"math.AC"}
{"created":"2024-06-04 07:20:13","title":"Generator-Based Fuzzers with Type-Based Targeted Mutation","abstract":"As with any fuzzer, directing Generator-Based Fuzzers (GBF) to reach particular code targets can increase the fuzzer's effectiveness. In previous work, coverage-guided fuzzers used a mix of static analysis, taint analysis, and constraint-solving approaches to address this problem. However, none of these techniques were particularly crafted for GBF where input generators are used to construct program inputs. The observation is that input generators carry information about the input structure that is naturally present through the typing composition of the program input.   In this paper, we introduce a type-based mutation heuristic, along with constant string lookup, for Java GBF. Our key intuition is that if one can identify which sub-part (types) of the input will likely influence the branching decision, then focusing on mutating the choices of the generators constructing these types is likely to achieve the desired coverages. We used our technique to fuzz AWSLambda applications. Results compared to a baseline GBF tool show an almost 20\\% average improvement in application coverage, and larger improvements when third-party code is included.","sentences":["As with any fuzzer, directing Generator-Based Fuzzers (GBF) to reach particular code targets can increase the fuzzer's effectiveness.","In previous work, coverage-guided fuzzers used a mix of static analysis, taint analysis, and constraint-solving approaches to address this problem.","However, none of these techniques were particularly crafted for GBF where input generators are used to construct program inputs.","The observation is that input generators carry information about the input structure that is naturally present through the typing composition of the program input.   ","In this paper, we introduce a type-based mutation heuristic, along with constant string lookup, for Java GBF.","Our key intuition is that if one can identify which sub-part (types) of the input will likely influence the branching decision, then focusing on mutating the choices of the generators constructing these types is likely to achieve the desired coverages.","We used our technique to fuzz AWSLambda applications.","Results compared to a baseline GBF tool show an almost 20\\% average improvement in application coverage, and larger improvements when third-party code is included."],"url":"http://arxiv.org/abs/2406.02034v1","category":"cs.SE"}
{"created":"2024-06-04 07:11:08","title":"How should parallel cluster randomized trials with a baseline period be analyzed? A survey of estimands and common estimators","abstract":"The parallel cluster randomized trial with baseline (PB-CRT) is a common variant of the standard parallel cluster randomized trial (P-CRT) that maintains parallel randomization but additionally allows for both within and between-cluster comparisons. We define two estimands of interest in the context of PB-CRTs, the participant-average treatment effect (pATE) and cluster-average treatment effect (cATE), to address participant and cluster-level hypotheses. Previous work has indicated that under informative cluster sizes, commonly used mixed-effects models may yield inconsistent estimators for the estimands of interest. In this work, we theoretically derive the convergence of the unweighted and inverse cluster-period size weighted (i.) independence estimating equation, (ii.) fixed-effects model, (iii.) exchangeable mixed-effects model, and (iv.) nested-exchangeable mixed-effects model treatment effect estimators in a PB-CRT with continuous outcomes. We report a simulation study to evaluate the bias and inference with these different treatment effect estimators and their corresponding model-based or jackknife variance estimators. We then re-analyze a PB-CRT examining the effects of community youth teams on improving mental health among adolescent girls in rural eastern India. We demonstrate that the unweighted and weighted independence estimating equation and fixed-effects model regularly yield consistent estimators for the pATE and cATE estimands, whereas the mixed-effects models yield inconsistent estimators under informative cluster sizes. However, we demonstrate that unlike the nested-exchangeable mixed-effects model and corresponding analyses in P-CRTs, the exchangeable mixed-effects model is surprisingly robust to bias in many PB-CRT scenarios.","sentences":["The parallel cluster randomized trial with baseline (PB-CRT) is a common variant of the standard parallel cluster randomized trial (P-CRT) that maintains parallel randomization but additionally allows for both within and between-cluster comparisons.","We define two estimands of interest in the context of PB-CRTs, the participant-average treatment effect (pATE) and cluster-average treatment effect (cATE), to address participant and cluster-level hypotheses.","Previous work has indicated that under informative cluster sizes, commonly used mixed-effects models may yield inconsistent estimators for the estimands of interest.","In this work, we theoretically derive the convergence of the unweighted and inverse cluster-period size weighted (i.)","independence estimating equation, (ii.) fixed-effects model, (iii.)","exchangeable mixed-effects model, and (iv.) nested-exchangeable mixed-effects model treatment effect estimators in a PB-CRT with continuous outcomes.","We report a simulation study to evaluate the bias and inference with these different treatment effect estimators and their corresponding model-based or jackknife variance estimators.","We then re-analyze a PB-CRT examining the effects of community youth teams on improving mental health among adolescent girls in rural eastern India.","We demonstrate that the unweighted and weighted independence estimating equation and fixed-effects model regularly yield consistent estimators for the pATE and cATE estimands, whereas the mixed-effects models yield inconsistent estimators under informative cluster sizes.","However, we demonstrate that unlike the nested-exchangeable mixed-effects model and corresponding analyses in P-CRTs, the exchangeable mixed-effects model is surprisingly robust to bias in many PB-CRT scenarios."],"url":"http://arxiv.org/abs/2406.02028v1","category":"stat.ME"}
{"created":"2024-06-04 06:54:53","title":"Parameterizing Federated Continual Learning for Reproducible Research","abstract":"Federated Learning (FL) systems evolve in heterogeneous and ever-evolving environments that challenge their performance. Under real deployments, the learning tasks of clients can also evolve with time, which calls for the integration of methodologies such as Continual Learning. To enable research reproducibility, we propose a set of experimental best practices that precisely capture and emulate complex learning scenarios. Our framework, Freddie, is the first entirely configurable framework for Federated Continual Learning (FCL), and it can be seamlessly deployed on a large number of machines thanks to the use of Kubernetes and containerization. We demonstrate the effectiveness of Freddie on two use cases, (i) large-scale FL on CIFAR100 and (ii) heterogeneous task sequence on FCL, which highlight unaddressed performance challenges in FCL scenarios.","sentences":["Federated Learning (FL) systems evolve in heterogeneous and ever-evolving environments that challenge their performance.","Under real deployments, the learning tasks of clients can also evolve with time, which calls for the integration of methodologies such as Continual Learning.","To enable research reproducibility, we propose a set of experimental best practices that precisely capture and emulate complex learning scenarios.","Our framework, Freddie, is the first entirely configurable framework for Federated Continual Learning (FCL), and it can be seamlessly deployed on a large number of machines thanks to the use of Kubernetes and containerization.","We demonstrate the effectiveness of Freddie on two use cases, (i) large-scale FL on CIFAR100 and (ii) heterogeneous task sequence on FCL, which highlight unaddressed performance challenges in FCL scenarios."],"url":"http://arxiv.org/abs/2406.02015v1","category":"cs.LG"}
{"created":"2024-06-04 06:49:18","title":"Mamba as Decision Maker: Exploring Multi-scale Sequence Modeling in Offline Reinforcement Learning","abstract":"Sequential modeling has demonstrated remarkable capabilities in offline reinforcement learning (RL), with Decision Transformer (DT) being one of the most notable representatives, achieving significant success. However, RL trajectories possess unique properties to be distinguished from the conventional sequence (e.g., text or audio): (1) local correlation, where the next states in RL are theoretically determined solely by current states and actions based on the Markov Decision Process (MDP), and (2) global correlation, where each step's features are related to long-term historical information due to the time-continuous nature of trajectories. In this paper, we propose a novel action sequence predictor, named Mamba Decision Maker (MambaDM), where Mamba is expected to be a promising alternative for sequence modeling paradigms, owing to its efficient modeling of multi-scale dependencies. In particular, we introduce a novel mixer module that proficiently extracts and integrates both global and local features of the input sequence, effectively capturing interrelationships in RL datasets. Extensive experiments demonstrate that MambaDM achieves state-of-the-art performance in Atari and OpenAI Gym datasets. Furthermore, we empirically investigate the scaling laws of MambaDM, finding that increasing model size does not bring performance improvement, but scaling the dataset amount by 2x for MambaDM can obtain up to 33.7% score improvement on Atari dataset. This paper delves into the sequence modeling capabilities of MambaDM in the RL domain, paving the way for future advancements in robust and efficient decision-making systems. Our code will be available at https://github.com/AndyCao1125/MambaDM.","sentences":["Sequential modeling has demonstrated remarkable capabilities in offline reinforcement learning (RL), with Decision Transformer (DT) being one of the most notable representatives, achieving significant success.","However, RL trajectories possess unique properties to be distinguished from the conventional sequence (e.g., text or audio): (1) local correlation, where the next states in RL are theoretically determined solely by current states and actions based on the Markov Decision Process (MDP), and (2) global correlation, where each step's features are related to long-term historical information due to the time-continuous nature of trajectories.","In this paper, we propose a novel action sequence predictor, named Mamba Decision Maker (MambaDM), where Mamba is expected to be a promising alternative for sequence modeling paradigms, owing to its efficient modeling of multi-scale dependencies.","In particular, we introduce a novel mixer module that proficiently extracts and integrates both global and local features of the input sequence, effectively capturing interrelationships in RL datasets.","Extensive experiments demonstrate that MambaDM achieves state-of-the-art performance in Atari and OpenAI Gym datasets.","Furthermore, we empirically investigate the scaling laws of MambaDM, finding that increasing model size does not bring performance improvement, but scaling the dataset amount by 2x for MambaDM can obtain up to 33.7% score improvement on Atari dataset.","This paper delves into the sequence modeling capabilities of MambaDM in the RL domain, paving the way for future advancements in robust and efficient decision-making systems.","Our code will be available at https://github.com/AndyCao1125/MambaDM."],"url":"http://arxiv.org/abs/2406.02013v1","category":"cs.LG"}
{"created":"2024-06-04 06:32:39","title":"Advancing Ultra-Reliable 6G: Transformer and Semantic Localization Empowered Robust Beamforming in Millimeter-Wave Communications","abstract":"Advancements in 6G wireless technology have elevated the importance of beamforming, especially for attaining ultra-high data rates via millimeter-wave (mmWave) frequency deployment. Although promising, mmWave bands require substantial beam training to achieve precise beamforming. While initial deep learning models that use RGB camera images demonstrated promise in reducing beam training overhead, their performance suffers due to sensitivity to lighting and environmental variations. Due to this sensitivity, Quality of Service (QoS) fluctuates, eventually affecting the stability and dependability of networks in dynamic environments. This emphasizes a critical need for more robust solutions. This paper proposes a robust beamforming technique to ensure consistent QoS under varying environmental conditions. An optimization problem has been formulated to maximize users' data rates. To solve the formulated NP-hard optimization problem, we decompose it into two subproblems: the semantic localization problem and the optimal beam selection problem. To solve the semantic localization problem, we propose a novel method that leverages the k-means clustering and YOLOv8 model. To solve the beam selection problem, we propose a novel lightweight hybrid architecture that utilizes various data sources and a weighted entropy-based mechanism to predict the optimal beams. Rapid and accurate beam predictions are needed to maintain QoS. A novel metric, Accuracy-Complexity Efficiency (ACE), has been proposed to quantify this. Six testing scenarios have been developed to evaluate the robustness of the proposed model. Finally, the simulation result demonstrates that the proposed model outperforms several state-of-the-art baselines regarding beam prediction accuracy, received power, and ACE in the developed test scenarios.","sentences":["Advancements in 6G wireless technology have elevated the importance of beamforming, especially for attaining ultra-high data rates via millimeter-wave (mmWave) frequency deployment.","Although promising, mmWave bands require substantial beam training to achieve precise beamforming.","While initial deep learning models that use RGB camera images demonstrated promise in reducing beam training overhead, their performance suffers due to sensitivity to lighting and environmental variations.","Due to this sensitivity, Quality of Service (QoS) fluctuates, eventually affecting the stability and dependability of networks in dynamic environments.","This emphasizes a critical need for more robust solutions.","This paper proposes a robust beamforming technique to ensure consistent QoS under varying environmental conditions.","An optimization problem has been formulated to maximize users' data rates.","To solve the formulated NP-hard optimization problem, we decompose it into two subproblems: the semantic localization problem and the optimal beam selection problem.","To solve the semantic localization problem, we propose a novel method that leverages the k-means clustering and YOLOv8 model.","To solve the beam selection problem, we propose a novel lightweight hybrid architecture that utilizes various data sources and a weighted entropy-based mechanism to predict the optimal beams.","Rapid and accurate beam predictions are needed to maintain QoS.","A novel metric, Accuracy-Complexity Efficiency (ACE), has been proposed to quantify this.","Six testing scenarios have been developed to evaluate the robustness of the proposed model.","Finally, the simulation result demonstrates that the proposed model outperforms several state-of-the-art baselines regarding beam prediction accuracy, received power, and ACE in the developed test scenarios."],"url":"http://arxiv.org/abs/2406.02000v1","category":"cs.NI"}
{"created":"2024-06-04 06:23:27","title":"Choroidal Vessel Segmentation on Indocyanine Green Angiography Images via Human-in-the-Loop Labeling","abstract":"Human-in-the-loop (HITL) strategy has been recently introduced into the field of medical image processing. Indocyanine green angiography (ICGA) stands as a well-established examination for visualizing choroidal vasculature and detecting chorioretinal diseases. However, the intricate nature of choroidal vascular networks makes large-scale manual segmentation of ICGA images challenging. Thus, the study aims to develop a high-precision choroidal vessel segmentation model with limited labor using HITL framework. We utilized a multi-source ICGA dataset, including 55 degree view and ultra-widefield ICGA (UWF-ICGA) images for model development. The choroidal vessel network was pre-segmented by a pre-trained vessel segmentation model, and then manually modified by two ophthalmologists. Choroidal vascular diameter, density, complexity, tortuosity, and branching angle were automatically quantified based on the segmentation. We finally conducted four cycles of HITL. One hundred and fifty 55 degree view ICGA images were used for the first three cycles (50 images per cycle), and twenty UWF-ICGA images for the last cycle. The average time needed to manually correct a pre-segmented ICGA image per cycle reduced from 20 minutes to 1 minute. High segmentation accuracy has been achieved on both 55 degree view ICGA and UWF-ICGA images. Additionally, the multi-dimensional choroidal vascular parameters were significantly associated with various chorioretinal diseases. Our study not only demonstrated the feasibility of the HITL strategy in improving segmentation performance with reduced manual labeling, but also innovatively introduced several risk predictors for choroidal abnormalities.","sentences":["Human-in-the-loop (HITL) strategy has been recently introduced into the field of medical image processing.","Indocyanine green angiography (ICGA) stands as a well-established examination for visualizing choroidal vasculature and detecting chorioretinal diseases.","However, the intricate nature of choroidal vascular networks makes large-scale manual segmentation of ICGA images challenging.","Thus, the study aims to develop a high-precision choroidal vessel segmentation model with limited labor using HITL framework.","We utilized a multi-source ICGA dataset, including 55 degree view and ultra-widefield ICGA (UWF-ICGA) images for model development.","The choroidal vessel network was pre-segmented by a pre-trained vessel segmentation model, and then manually modified by two ophthalmologists.","Choroidal vascular diameter, density, complexity, tortuosity, and branching angle were automatically quantified based on the segmentation.","We finally conducted four cycles of HITL.","One hundred and fifty 55 degree view ICGA images were used for the first three cycles (50 images per cycle), and twenty UWF-ICGA images for the last cycle.","The average time needed to manually correct a pre-segmented ICGA image per cycle reduced from 20 minutes to 1 minute.","High segmentation accuracy has been achieved on both 55 degree view ICGA and UWF-ICGA images.","Additionally, the multi-dimensional choroidal vascular parameters were significantly associated with various chorioretinal diseases.","Our study not only demonstrated the feasibility of the HITL strategy in improving segmentation performance with reduced manual labeling, but also innovatively introduced several risk predictors for choroidal abnormalities."],"url":"http://arxiv.org/abs/2406.01993v1","category":"eess.IV"}
{"created":"2024-06-04 06:22:04","title":"Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions","abstract":"Constrained bilevel optimization tackles nested structures present in constrained learning tasks like constrained meta-learning, adversarial learning, and distributed bilevel optimization. However, existing bilevel optimization methods mostly are typically restricted to specific constraint settings, such as linear lower-level constraints. In this work, we overcome this limitation and develop a new single-loop, Hessian-free constrained bilevel algorithm capable of handling more general lower-level constraints. We achieve this by employing a doubly regularized gap function tailored to the constrained lower-level problem, transforming constrained bilevel optimization into an equivalent single-level optimization problem with a single smooth constraint. We rigorously establish the non-asymptotic convergence analysis of the proposed algorithm under the convexity of lower-level problem, avoiding the need for strong convexity assumptions on the lower-level objective or coupling convexity assumptions on lower-level constraints found in existing literature. Additionally, the generality of our method allows for its extension to bilevel optimization with minimax lower-level problem. We evaluate the effectiveness and efficiency of our algorithm on various synthetic problems, typical hyperparameter learning tasks, and generative adversarial network.","sentences":["Constrained bilevel optimization tackles nested structures present in constrained learning tasks like constrained meta-learning, adversarial learning, and distributed bilevel optimization.","However, existing bilevel optimization methods mostly are typically restricted to specific constraint settings, such as linear lower-level constraints.","In this work, we overcome this limitation and develop a new single-loop, Hessian-free constrained bilevel algorithm capable of handling more general lower-level constraints.","We achieve this by employing a doubly regularized gap function tailored to the constrained lower-level problem, transforming constrained bilevel optimization into an equivalent single-level optimization problem with a single smooth constraint.","We rigorously establish the non-asymptotic convergence analysis of the proposed algorithm under the convexity of lower-level problem, avoiding the need for strong convexity assumptions on the lower-level objective or coupling convexity assumptions on lower-level constraints found in existing literature.","Additionally, the generality of our method allows for its extension to bilevel optimization with minimax lower-level problem.","We evaluate the effectiveness and efficiency of our algorithm on various synthetic problems, typical hyperparameter learning tasks, and generative adversarial network."],"url":"http://arxiv.org/abs/2406.01992v1","category":"math.OC"}
{"created":"2024-06-04 06:07:24","title":"Dealing with All-stage Missing Modality: Towards A Universal Model with Robust Reconstruction and Personalization","abstract":"Addressing missing modalities presents a critical challenge in multimodal learning. Current approaches focus on developing models that can handle modality-incomplete inputs during inference, assuming that the full set of modalities are available for all the data during training. This reliance on full-modality data for training limits the use of abundant modality-incomplete samples that are often encountered in practical settings. In this paper, we propose a robust universal model with modality reconstruction and model personalization, which can effectively tackle the missing modality at both training and testing stages. Our method leverages a multimodal masked autoencoder to reconstruct the missing modality and masked patches simultaneously, incorporating an innovative distribution approximation mechanism to fully utilize both modality-complete and modality-incomplete data. The reconstructed modalities then contributes to our designed data-model co-distillation scheme to guide the model learning in the presence of missing modalities. Moreover, we propose a CLIP-driven hyper-network to personalize partial model parameters, enabling the model to adapt to each distinct missing modality scenario. Our method has been extensively validated on two brain tumor segmentation benchmarks. Experimental results demonstrate the promising performance of our method, which consistently exceeds previous state-of-the-art approaches under the all-stage missing modality settings with different missing ratios. Code will be available.","sentences":["Addressing missing modalities presents a critical challenge in multimodal learning.","Current approaches focus on developing models that can handle modality-incomplete inputs during inference, assuming that the full set of modalities are available for all the data during training.","This reliance on full-modality data for training limits the use of abundant modality-incomplete samples that are often encountered in practical settings.","In this paper, we propose a robust universal model with modality reconstruction and model personalization, which can effectively tackle the missing modality at both training and testing stages.","Our method leverages a multimodal masked autoencoder to reconstruct the missing modality and masked patches simultaneously, incorporating an innovative distribution approximation mechanism to fully utilize both modality-complete and modality-incomplete data.","The reconstructed modalities then contributes to our designed data-model co-distillation scheme to guide the model learning in the presence of missing modalities.","Moreover, we propose a CLIP-driven hyper-network to personalize partial model parameters, enabling the model to adapt to each distinct missing modality scenario.","Our method has been extensively validated on two brain tumor segmentation benchmarks.","Experimental results demonstrate the promising performance of our method, which consistently exceeds previous state-of-the-art approaches under the all-stage missing modality settings with different missing ratios.","Code will be available."],"url":"http://arxiv.org/abs/2406.01987v1","category":"cs.CV"}
{"created":"2024-06-04 05:51:36","title":"Variable importance measure for spatial machine learning models with application to air pollution exposure prediction","abstract":"Exposure assessment is fundamental to air pollution cohort studies. The objective is to predict air pollution exposures for study subjects at locations without data in order to optimize our ability to learn about health effects of air pollution. In addition to generating accurate predictions to minimize exposure measurement error, understanding the mechanism captured by the model is another crucial aspect that may not always be straightforward due to the complex nature of machine learning methods, as well as the lack of unifying notions of variable importance. This is further complicated in air pollution modeling by the presence of spatial correlation. We tackle these challenges in two datasets: sulfur (S) from regulatory United States national PM2.5 sub-species data and ultrafine particles (UFP) from a new Seattle-area traffic-related air pollution dataset. Our key contribution is a leave-one-out approach for variable importance that leads to interpretable and comparable measures for a broad class of models with separable mean and covariance components. We illustrate our approach with several spatial machine learning models, and it clearly highlights the difference in model mechanisms, even for those producing similar predictions. We leverage insights from this variable importance measure to assess the relative utilities of two exposure models for S and UFP that have similar out-of-sample prediction accuracies but appear to draw on different types of spatial information to make predictions.","sentences":["Exposure assessment is fundamental to air pollution cohort studies.","The objective is to predict air pollution exposures for study subjects at locations without data in order to optimize our ability to learn about health effects of air pollution.","In addition to generating accurate predictions to minimize exposure measurement error, understanding the mechanism captured by the model is another crucial aspect that may not always be straightforward due to the complex nature of machine learning methods, as well as the lack of unifying notions of variable importance.","This is further complicated in air pollution modeling by the presence of spatial correlation.","We tackle these challenges in two datasets: sulfur (S) from regulatory United States national PM2.5 sub-species data and ultrafine particles (UFP) from a new Seattle-area traffic-related air pollution dataset.","Our key contribution is a leave-one-out approach for variable importance that leads to interpretable and comparable measures for a broad class of models with separable mean and covariance components.","We illustrate our approach with several spatial machine learning models, and it clearly highlights the difference in model mechanisms, even for those producing similar predictions.","We leverage insights from this variable importance measure to assess the relative utilities of two exposure models for S and UFP that have similar out-of-sample prediction accuracies but appear to draw on different types of spatial information to make predictions."],"url":"http://arxiv.org/abs/2406.01982v1","category":"stat.AP"}
{"created":"2024-06-04 05:30:16","title":"What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding","abstract":"Graph Transformers, which incorporate self-attention and positional encoding, have recently emerged as a powerful architecture for various graph learning tasks. Despite their impressive performance, the complex non-convex interactions across layers and the recursive graph structure have made it challenging to establish a theoretical foundation for learning and generalization. This study introduces the first theoretical investigation of a shallow Graph Transformer for semi-supervised node classification, comprising a self-attention layer with relative positional encoding and a two-layer perceptron. Focusing on a graph data model with discriminative nodes that determine node labels and non-discriminative nodes that are class-irrelevant, we characterize the sample complexity required to achieve a desirable generalization error by training with stochastic gradient descent (SGD). This paper provides the quantitative characterization of the sample complexity and number of iterations for convergence dependent on the fraction of discriminative nodes, the dominant patterns, and the initial model errors. Furthermore, we demonstrate that self-attention and positional encoding enhance generalization by making the attention map sparse and promoting the core neighborhood during training, which explains the superior feature representation of Graph Transformers. Our theoretical results are supported by empirical experiments on synthetic and real-world benchmarks.","sentences":["Graph Transformers, which incorporate self-attention and positional encoding, have recently emerged as a powerful architecture for various graph learning tasks.","Despite their impressive performance, the complex non-convex interactions across layers and the recursive graph structure have made it challenging to establish a theoretical foundation for learning and generalization.","This study introduces the first theoretical investigation of a shallow Graph Transformer for semi-supervised node classification, comprising a self-attention layer with relative positional encoding and a two-layer perceptron.","Focusing on a graph data model with discriminative nodes that determine node labels and non-discriminative nodes that are class-irrelevant, we characterize the sample complexity required to achieve a desirable generalization error by training with stochastic gradient descent (SGD).","This paper provides the quantitative characterization of the sample complexity and number of iterations for convergence dependent on the fraction of discriminative nodes, the dominant patterns, and the initial model errors.","Furthermore, we demonstrate that self-attention and positional encoding enhance generalization by making the attention map sparse and promoting the core neighborhood during training, which explains the superior feature representation of Graph Transformers.","Our theoretical results are supported by empirical experiments on synthetic and real-world benchmarks."],"url":"http://arxiv.org/abs/2406.01977v1","category":"cs.LG"}
{"created":"2024-06-04 05:22:24","title":"Conditional Language Learning with Context","abstract":"Language models can learn sophisticated language understanding skills from fitting raw text. They also unselectively learn useless corpus statistics and biases, especially during finetuning on domain-specific corpora. In this paper, we propose a simple modification to causal language modeling called conditional finetuning, which performs language modeling conditioned on a context. We show that a context can \"explain away\" certain corpus statistics and make the model avoid learning them. In this fashion, conditional finetuning achieves selective learning from a corpus, learning knowledge useful for downstream tasks while avoiding learning useless corpus statistics like topic biases. This selective learning effect leads to less forgetting and better stability-plasticity tradeoff in domain finetuning, potentially benefitting lifelong learning with language models.","sentences":["Language models can learn sophisticated language understanding skills from fitting raw text.","They also unselectively learn useless corpus statistics and biases, especially during finetuning on domain-specific corpora.","In this paper, we propose a simple modification to causal language modeling called conditional finetuning, which performs language modeling conditioned on a context.","We show that a context can \"explain away\" certain corpus statistics and make the model avoid learning them.","In this fashion, conditional finetuning achieves selective learning from a corpus, learning knowledge useful for downstream tasks while avoiding learning useless corpus statistics like topic biases.","This selective learning effect leads to less forgetting and better stability-plasticity tradeoff in domain finetuning, potentially benefitting lifelong learning with language models."],"url":"http://arxiv.org/abs/2406.01976v1","category":"cs.CL"}
{"created":"2024-06-04 05:19:32","title":"Can Dense Connectivity Benefit Outlier Detection? An Odyssey with NAS","abstract":"Recent advances in Out-of-Distribution (OOD) Detection is the driving force behind safe and reliable deployment of Convolutional Neural Networks (CNNs) in real world applications. However, existing studies focus on OOD detection through confidence score and deep generative model-based methods, without considering the impact of DNN structures, especially dense connectivity in architecture fabrications. In addition, existing outlier detection approaches exhibit high variance in generalization performance, lacking stability and confidence in evaluating and ranking different outlier detectors. In this work, we propose a novel paradigm, Dense Connectivity Search of Outlier Detector (DCSOD), that automatically explore the dense connectivity of CNN architectures on near-OOD detection task using Neural Architecture Search (NAS). We introduce a hierarchical search space containing versatile convolution operators and dense connectivity, allowing a flexible exploration of CNN architectures with diverse connectivity patterns. To improve the quality of evaluation on OOD detection during search, we propose evolving distillation based on our multi-view feature learning explanation. Evolving distillation stabilizes training for OOD detection evaluation, thus improves the quality of search. We thoroughly examine DCSOD on CIFAR benchmarks under OOD detection protocol. Experimental results show that DCSOD achieve remarkable performance over widely used architectures and previous NAS baselines. Notably, DCSOD achieves state-of-the-art (SOTA) performance on CIFAR benchmark, with AUROC improvement of $\\sim$1.0%.","sentences":["Recent advances in Out-of-Distribution (OOD) Detection is the driving force behind safe and reliable deployment of Convolutional Neural Networks (CNNs) in real world applications.","However, existing studies focus on OOD detection through confidence score and deep generative model-based methods, without considering the impact of DNN structures, especially dense connectivity in architecture fabrications.","In addition, existing outlier detection approaches exhibit high variance in generalization performance, lacking stability and confidence in evaluating and ranking different outlier detectors.","In this work, we propose a novel paradigm, Dense Connectivity Search of Outlier Detector (DCSOD), that automatically explore the dense connectivity of CNN architectures on near-OOD detection task using Neural Architecture Search (NAS).","We introduce a hierarchical search space containing versatile convolution operators and dense connectivity, allowing a flexible exploration of CNN architectures with diverse connectivity patterns.","To improve the quality of evaluation on OOD detection during search, we propose evolving distillation based on our multi-view feature learning explanation.","Evolving distillation stabilizes training for OOD detection evaluation, thus improves the quality of search.","We thoroughly examine DCSOD on CIFAR benchmarks under OOD detection protocol.","Experimental results show that DCSOD achieve remarkable performance over widely used architectures and previous NAS baselines.","Notably, DCSOD achieves state-of-the-art (SOTA) performance on CIFAR benchmark, with AUROC improvement of $\\sim$1.0%."],"url":"http://arxiv.org/abs/2406.01975v1","category":"cs.LG"}
{"created":"2024-06-04 05:18:11","title":"Adaptive Relaxation based Non-Conservative Chance Constrained Stochastic MPC for Battery Scheduling Under Forecast Uncertainties","abstract":"Chance constrained stochastic model predictive controllers (CC-SMPC) trade off full constraint satisfaction for economical plant performance under uncertainty. Previous CC-SMPC works are over-conservative in constraint violations leading to worse economic performance. Other past works require a-priori information about the uncertainty set, limiting their application to real-world systems. This paper considers a discrete linear time invariant system with hard constraints on inputs and chance constraints on states, with unknown uncertainty distribution, statistics, or samples. This work proposes a novel adaptive online update rule to relax the state constraints based on the time-average of past constraint violations, for the SMPC to achieve reduced conservativeness in closed-loop. Under an ideal control policy assumption, it is proven that the time-average of constraint violations converges to the maximum allowed violation probability. The time-average of constraint violations is also proven to asymptotically converge even without the simplifying assumptions. The proposed method is applied to the optimal battery energy storage system (BESS) dispatch in a grid connected microgrid with PV generation and load demand with chance constraints on BESS state-of-charge (SOC). Realistic simulations show the superior electricity cost saving potential of the proposed method as compared to the traditional MPC (with hard constraints on BESS SOC), by satisfying the chance constraints non-conservatively in closed-loop, thereby effectively trading off increased cost savings with minimal adverse effects on BESS lifetime.","sentences":["Chance constrained stochastic model predictive controllers (CC-SMPC) trade off full constraint satisfaction for economical plant performance under uncertainty.","Previous CC-SMPC works are over-conservative in constraint violations leading to worse economic performance.","Other past works require a-priori information about the uncertainty set, limiting their application to real-world systems.","This paper considers a discrete linear time invariant system with hard constraints on inputs and chance constraints on states, with unknown uncertainty distribution, statistics, or samples.","This work proposes a novel adaptive online update rule to relax the state constraints based on the time-average of past constraint violations, for the SMPC to achieve reduced conservativeness in closed-loop.","Under an ideal control policy assumption, it is proven that the time-average of constraint violations converges to the maximum allowed violation probability.","The time-average of constraint violations is also proven to asymptotically converge even without the simplifying assumptions.","The proposed method is applied to the optimal battery energy storage system (BESS) dispatch in a grid connected microgrid with PV generation and load demand with chance constraints on BESS state-of-charge (SOC).","Realistic simulations show the superior electricity cost saving potential of the proposed method as compared to the traditional MPC (with hard constraints on BESS SOC), by satisfying the chance constraints non-conservatively in closed-loop, thereby effectively trading off increased cost savings with minimal adverse effects on BESS lifetime."],"url":"http://arxiv.org/abs/2406.01973v1","category":"eess.SY"}
{"created":"2024-06-04 05:05:27","title":"Multiway Multislice PHATE: Visualizing Hidden Dynamics of RNNs through Training","abstract":"Recurrent neural networks (RNNs) are a widely used tool for sequential data analysis, however, they are still often seen as black boxes of computation. Understanding the functional principles of these networks is critical to developing ideal model architectures and optimization strategies. Previous studies typically only emphasize the network representation post-training, overlooking their evolution process throughout training. Here, we present Multiway Multislice PHATE (MM-PHATE), a novel method for visualizing the evolution of RNNs' hidden states. MM-PHATE is a graph-based embedding using structured kernels across the multiple dimensions spanned by RNNs: time, training epoch, and units. We demonstrate on various datasets that MM-PHATE uniquely preserves hidden representation community structure among units and identifies information processing and compression phases during training. The embedding allows users to look under the hood of RNNs across training and provides an intuitive and comprehensive strategy to understanding the network's internal dynamics and draw conclusions, e.g., on why and how one model outperforms another or how a specific architecture might impact an RNN's learning ability.","sentences":["Recurrent neural networks (RNNs) are a widely used tool for sequential data analysis, however, they are still often seen as black boxes of computation.","Understanding the functional principles of these networks is critical to developing ideal model architectures and optimization strategies.","Previous studies typically only emphasize the network representation post-training, overlooking their evolution process throughout training.","Here, we present Multiway","Multislice PHATE (MM-PHATE), a novel method for visualizing the evolution of RNNs' hidden states.","MM-PHATE is a graph-based embedding using structured kernels across the multiple dimensions spanned by RNNs: time, training epoch, and units.","We demonstrate on various datasets that MM-PHATE uniquely preserves hidden representation community structure among units and identifies information processing and compression phases during training.","The embedding allows users to look under the hood of RNNs across training and provides an intuitive and comprehensive strategy to understanding the network's internal dynamics and draw conclusions, e.g., on why and how one model outperforms another or how a specific architecture might impact an RNN's learning ability."],"url":"http://arxiv.org/abs/2406.01969v1","category":"cs.LG"}
{"created":"2024-06-04 04:48:40","title":"Measure-Observe-Remeasure: An Interactive Paradigm for Differentially-Private Exploratory Analysis","abstract":"Differential privacy (DP) has the potential to enable privacy-preserving analysis on sensitive data, but requires analysts to judiciously spend a limited ``privacy loss budget'' $\\epsilon$ across queries. Analysts conducting exploratory analyses do not, however, know all queries in advance and seldom have DP expertise. Thus, they are limited in their ability to specify $\\epsilon$ allotments across queries prior to an analysis. To support analysts in spending $\\epsilon$ efficiently, we propose a new interactive analysis paradigm, Measure-Observe-Remeasure, where analysts ``measure'' the database with a limited amount of $\\epsilon$, observe estimates and their errors, and remeasure with more $\\epsilon$ as needed.   We instantiate the paradigm in an interactive visualization interface which allows analysts to spend increasing amounts of $\\epsilon$ under a total budget. To observe how analysts interact with the Measure-Observe-Remeasure paradigm via the interface, we conduct a user study that compares the utility of $\\epsilon$ allocations and findings from sensitive data participants make to the allocations and findings expected of a rational agent who faces the same decision task. We find that participants are able to use the workflow relatively successfully, including using budget allocation strategies that maximize over half of the available utility stemming from $\\epsilon$ allocation. Their loss in performance relative to a rational agent appears to be driven more by their inability to access information and report it than to allocate $\\epsilon$.","sentences":["Differential privacy (DP) has the potential to enable privacy-preserving analysis on sensitive data, but requires analysts to judiciously spend a limited ``privacy loss budget'' $\\epsilon$ across queries.","Analysts conducting exploratory analyses do not, however, know all queries in advance and seldom have DP expertise.","Thus, they are limited in their ability to specify $\\epsilon$ allotments across queries prior to an analysis.","To support analysts in spending $\\epsilon$ efficiently, we propose a new interactive analysis paradigm, Measure-Observe-Remeasure, where analysts ``measure'' the database with a limited amount of $\\epsilon$, observe estimates and their errors, and remeasure with more $\\epsilon$ as needed.   ","We instantiate the paradigm in an interactive visualization interface which allows analysts to spend increasing amounts of $\\epsilon$ under a total budget.","To observe how analysts interact with the Measure-Observe-Remeasure paradigm via the interface, we conduct a user study that compares the utility of $\\epsilon$ allocations and findings from sensitive data participants make to the allocations and findings expected of a rational agent who faces the same decision task.","We find that participants are able to use the workflow relatively successfully, including using budget allocation strategies that maximize over half of the available utility stemming from $\\epsilon$ allocation.","Their loss in performance relative to a rational agent appears to be driven more by their inability to access information and report it than to allocate $\\epsilon$."],"url":"http://arxiv.org/abs/2406.01964v1","category":"cs.CR"}
{"created":"2024-06-04 04:45:30","title":"Diamond molecular balance: Revolutionizing high-accuracy mass spectrometry from MDa to TDa at room temperature","abstract":"The significance of mass spectrometry lies in its unparalleled ability to accurately identify and quantify molecules in complex samples, providing invaluable insights into molecular structures and interactions. Here, we leverage diamond nanostructures as highly sensitive mass sensors by utilizing a self-excitation mechanism under an electron beam in a conventional scanning electron microscope (SEM). The diamond molecular balance (DMB) exhibits exceptional mass accuracy of a few MDa and an extensive dynamic range from MDa to TDa, positioning itself as a forefront molecular balance operating at room temperature. Notably, the DMB measures the mass of a single bacteriophage T4, achieving a mass accuracy of 4.7 MDa for an analyte at 184 MDa, while precisely determining their positional information on the device. These findings highlight the groundbreaking potential of the DMB as a revolutionary tool for mass analysis at room temperature.","sentences":["The significance of mass spectrometry lies in its unparalleled ability to accurately identify and quantify molecules in complex samples, providing invaluable insights into molecular structures and interactions.","Here, we leverage diamond nanostructures as highly sensitive mass sensors by utilizing a self-excitation mechanism under an electron beam in a conventional scanning electron microscope (SEM).","The diamond molecular balance (DMB) exhibits exceptional mass accuracy of a few MDa and an extensive dynamic range from MDa to TDa, positioning itself as a forefront molecular balance operating at room temperature.","Notably, the DMB measures the mass of a single bacteriophage T4, achieving a mass accuracy of 4.7 MDa for an analyte at 184 MDa, while precisely determining their positional information on the device.","These findings highlight the groundbreaking potential of the DMB as a revolutionary tool for mass analysis at room temperature."],"url":"http://arxiv.org/abs/2406.01963v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-04 04:45:29","title":"Exploring coherent dynamics in resonant x-ray scattering of intense ultrafast pulses","abstract":"Intense x-ray free-electron lasers (XFELs) offer unique opportunities to control inner-shell electrons on ultrafast timescales. This study presents a theoretical framework for modeling resonant x-ray scattering under intense ultrafast pulses, focusing on the coherent dynamics of Rabi oscillations. We employ a time-dependent Schr\\\"odinger equation approach to investigate the effects of high-intensity pulses on the single atom response which includes resonant fluorescence and elastic scattering channels, and competing decay processes. Our findings highlight the sensitivity of scattering responses to pulse parameters and initial states, with interference effects playing a significant role.","sentences":["Intense x-ray free-electron lasers (XFELs) offer unique opportunities to control inner-shell electrons on ultrafast timescales.","This study presents a theoretical framework for modeling resonant x-ray scattering under intense ultrafast pulses, focusing on the coherent dynamics of Rabi oscillations.","We employ a time-dependent Schr\\\"odinger equation approach to investigate the effects of high-intensity pulses on the single atom response which includes resonant fluorescence and elastic scattering channels, and competing decay processes.","Our findings highlight the sensitivity of scattering responses to pulse parameters and initial states, with interference effects playing a significant role."],"url":"http://arxiv.org/abs/2406.01962v1","category":"physics.atom-ph"}
{"created":"2024-06-04 04:39:51","title":"Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions","abstract":"This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique. Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional $\\mathcal{O}(\\log T)$ term in the convergence rate. To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of $\\mathcal{O}(T^{-1/3})$ for non-convex functions with our newly designed learning rate strategy. Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional $\\mathcal{O}(\\log T)$ term. We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of $\\mathcal{O}(T^{-1/3})$. Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of $\\mathcal{O}(n^{1/4} T^{-1/2} )$, where $n$ represents the number of component functions. Numerical experiments across various tasks validate the effectiveness of our method.","sentences":["This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique.","Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional $\\mathcal{O}(\\log T)$ term in the convergence rate.","To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of $\\mathcal{O}(T^{-1/3})$ for non-convex functions with our newly designed learning rate strategy.","Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional $\\mathcal{O}(\\log T)$ term.","We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of $\\mathcal{O}(T^{-1/3})$. Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of $\\mathcal{O}(n^{1/4} T^{-1/2} )$, where $n$ represents the number of component functions.","Numerical experiments across various tasks validate the effectiveness of our method."],"url":"http://arxiv.org/abs/2406.01959v1","category":"math.OC"}
{"created":"2024-06-04 03:35:25","title":"Orthogonal Causal Calibration","abstract":"Estimates of causal parameters such as conditional average treatment effects and conditional quantile treatment effects play an important role in real-world decision making. Given this importance, one should ensure these estimators are calibrated. While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters.   In this work, we provide a general framework for calibrating predictors involving nuisance estimation. We consider a notion of calibration defined with respect to an arbitrary, nuisance-dependent loss $\\ell$, under which we say an estimator $\\theta$ is calibrated if its predictions cannot be changed on any level set to decrease loss. We prove generic upper bounds on the calibration error of any causal parameter estimate $\\theta$ with respect to any loss $\\ell$ using a concept called Neyman Orthogonality. Our bounds involve two decoupled terms - one measuring the error in estimating the unknown nuisance parameters, and the other representing the calibration error in a hypothetical world where the learned nuisance estimates were true. We use our bound to analyze the convergence of two sample splitting algorithms for causal calibration. One algorithm, which applies to universally orthogonalizable loss functions, transforms the data into generalized pseudo-outcomes and applies an off-the-shelf calibration procedure. The other algorithm, which applies to conditionally orthogonalizable loss functions, extends the classical uniform mass binning algorithm to include nuisance estimation. Our results are exceedingly general, showing that essentially any existing calibration algorithm can be used in causal settings, with additional loss only arising from errors in nuisance estimation.","sentences":["Estimates of causal parameters such as conditional average treatment effects and conditional quantile treatment effects play an important role in real-world decision making.","Given this importance, one should ensure these estimators are calibrated.","While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters.   ","In this work, we provide a general framework for calibrating predictors involving nuisance estimation.","We consider a notion of calibration defined with respect to an arbitrary, nuisance-dependent loss $\\ell$, under which we say an estimator $\\theta$ is calibrated if its predictions cannot be changed on any level set to decrease loss.","We prove generic upper bounds on the calibration error of any causal parameter estimate $\\theta$ with respect to any loss $\\ell$ using a concept called Neyman Orthogonality.","Our bounds involve two decoupled terms - one measuring the error in estimating the unknown nuisance parameters, and the other representing the calibration error in a hypothetical world where the learned nuisance estimates were true.","We use our bound to analyze the convergence of two sample splitting algorithms for causal calibration.","One algorithm, which applies to universally orthogonalizable loss functions, transforms the data into generalized pseudo-outcomes and applies an off-the-shelf calibration procedure.","The other algorithm, which applies to conditionally orthogonalizable loss functions, extends the classical uniform mass binning algorithm to include nuisance estimation.","Our results are exceedingly general, showing that essentially any existing calibration algorithm can be used in causal settings, with additional loss only arising from errors in nuisance estimation."],"url":"http://arxiv.org/abs/2406.01933v1","category":"stat.ML"}
{"created":"2024-06-04 03:31:09","title":"Dishonesty in Helpful and Harmless Alignment","abstract":"People tell lies when seeking rewards. Large language models (LLMs) are aligned to human values with reinforcement learning where they get rewards if they satisfy human preference. We find that this also induces dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses. Using the latest interpreting tools, we detect dishonesty, show how LLMs can be harmful if their honesty is increased, and analyze such conflicts at the parameter-level. Given these preliminaries and the hypothesis that reward-seeking stimulates dishonesty, we theoretically show that the dishonesty can in-turn decrease the alignment performances and augment reward-seeking alignment with representation regularization. Extensive results, including GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we can train more honest, helpful, and harmless LLMs. We will make all our codes and results be open-sourced upon this paper's acceptance.","sentences":["People tell lies when seeking rewards.","Large language models (LLMs) are aligned to human values with reinforcement learning where they get rewards if they satisfy human preference.","We find that this also induces dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses.","Using the latest interpreting tools, we detect dishonesty, show how LLMs can be harmful if their honesty is increased, and analyze such conflicts at the parameter-level.","Given these preliminaries and the hypothesis that reward-seeking stimulates dishonesty, we theoretically show that the dishonesty can in-turn decrease the alignment performances and augment reward-seeking alignment with representation regularization.","Extensive results, including GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we can train more honest, helpful, and harmless LLMs.","We will make all our codes and results be open-sourced upon this paper's acceptance."],"url":"http://arxiv.org/abs/2406.01931v1","category":"cs.CL"}
{"created":"2024-06-04 03:22:36","title":"Position-based Rogue Access Point Detection","abstract":"Rogue Wi-Fi access point (AP) attacks can lead to data breaches and unauthorized access. Existing rogue AP detection methods and tools often rely on channel state information (CSI) or received signal strength indicator (RSSI), but they require specific hardware or achieve low detection accuracy. On the other hand, AP positions are typically fixed, and Wi-Fi can support indoor positioning of user devices. Based on this position information, the mobile platform can check if one (or more) AP in range is rogue. The inclusion of a rogue AP would in principle result in a wrong estimated position. Thus, the idea to use different subsets of APs: the positions computed based on subsets that include a rogue AP will be significantly different from those that do not. Our scheme contains two components: subset generation and position validation. First, we generate subsets of RSSIs from APs, which are then utilized for positioning, similar to receiver autonomous integrity monitoring (RAIM). Second, the position estimates, along with uncertainties, are combined into a Gaussian mixture, to check for inconsistencies by evaluating the overlap of the Gaussian components. Our comparative analysis, conducted on a real-world dataset with three types of attacks and synthetic RSSIs integrated, demonstrates a substantial improvement in rogue AP detection accuracy.","sentences":["Rogue Wi-Fi access point (AP) attacks can lead to data breaches and unauthorized access.","Existing rogue AP detection methods and tools often rely on channel state information (CSI) or received signal strength indicator (RSSI), but they require specific hardware or achieve low detection accuracy.","On the other hand, AP positions are typically fixed, and Wi-Fi can support indoor positioning of user devices.","Based on this position information, the mobile platform can check if one (or more)","AP in range is rogue.","The inclusion of a rogue AP would in principle result in a wrong estimated position.","Thus, the idea to use different subsets of APs: the positions computed based on subsets that include a rogue AP will be significantly different from those that do not.","Our scheme contains two components: subset generation and position validation.","First, we generate subsets of RSSIs from APs, which are then utilized for positioning, similar to receiver autonomous integrity monitoring (RAIM).","Second, the position estimates, along with uncertainties, are combined into a Gaussian mixture, to check for inconsistencies by evaluating the overlap of the Gaussian components.","Our comparative analysis, conducted on a real-world dataset with three types of attacks and synthetic RSSIs integrated, demonstrates a substantial improvement in rogue AP detection accuracy."],"url":"http://arxiv.org/abs/2406.01927v1","category":"cs.CR"}
{"created":"2024-06-04 02:57:09","title":"FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping","abstract":"The semantically interactive radiance field has always been an appealing task for its potential to facilitate user-friendly and automated real-world 3D scene understanding applications. However, it is a challenging task to achieve high quality, efficiency and zero-shot ability at the same time with semantics in radiance fields. In this work, we present FastLGS, an approach that supports real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high resolution. We propose the semantic feature grid to save multi-view CLIP features which are extracted based on Segment Anything Model (SAM) masks, and map the grids to low dimensional features for semantic field training through 3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through feature grids from rendered features for open-vocabulary queries. Comparisons with other state-of-the-art methods prove that FastLGS can achieve the first place performance concerning both speed and accuracy, where FastLGS is 98x faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that FastLGS is adaptive and compatible with many downstream tasks, such as 3D segmentation and 3D object inpainting, which can be easily applied to other 3D manipulation systems.","sentences":["The semantically interactive radiance field has always been an appealing task for its potential to facilitate user-friendly and automated real-world 3D scene understanding applications.","However, it is a challenging task to achieve high quality, efficiency and zero-shot ability at the same time with semantics in radiance fields.","In this work, we present FastLGS, an approach that supports real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high resolution.","We propose the semantic feature grid to save multi-view CLIP features which are extracted based on Segment Anything Model (SAM) masks, and map the grids to low dimensional features for semantic field training through 3DGS.","Once trained, we can restore pixel-aligned CLIP embeddings through feature grids from rendered features for open-vocabulary queries.","Comparisons with other state-of-the-art methods prove that FastLGS can achieve the first place performance concerning both speed and accuracy, where FastLGS is 98x faster than LERF and 4x faster than LangSplat.","Meanwhile, experiments show that FastLGS is adaptive and compatible with many downstream tasks, such as 3D segmentation and 3D object inpainting, which can be easily applied to other 3D manipulation systems."],"url":"http://arxiv.org/abs/2406.01916v1","category":"cs.CV"}
{"created":"2024-06-04 02:39:48","title":"A Global Geometric Analysis of Maximal Coding Rate Reduction","abstract":"The maximal coding rate reduction (MCR$^2$) objective for learning structured and compact deep representations is drawing increasing attention, especially after its recent usage in the derivation of fully explainable and highly effective deep network architectures. However, it lacks a complete theoretical justification: only the properties of its global optima are known, and its global landscape has not been studied. In this work, we give a complete characterization of the properties of all its local and global optima, as well as other types of critical points. Specifically, we show that each (local or global) maximizer of the MCR$^2$ problem corresponds to a low-dimensional, discriminative, and diverse representation, and furthermore, each critical point of the objective is either a local maximizer or a strict saddle point. Such a favorable landscape makes MCR$^2$ a natural choice of objective for learning diverse and discriminative representations via first-order optimization methods. To validate our theoretical findings, we conduct extensive experiments on both synthetic and real data sets.","sentences":["The maximal coding rate reduction (MCR$^2$) objective for learning structured and compact deep representations is drawing increasing attention, especially after its recent usage in the derivation of fully explainable and highly effective deep network architectures.","However, it lacks a complete theoretical justification: only the properties of its global optima are known, and its global landscape has not been studied.","In this work, we give a complete characterization of the properties of all its local and global optima, as well as other types of critical points.","Specifically, we show that each (local or global) maximizer of the MCR$^2$ problem corresponds to a low-dimensional, discriminative, and diverse representation, and furthermore, each critical point of the objective is either a local maximizer or a strict saddle point.","Such a favorable landscape makes MCR$^2$ a natural choice of objective for learning diverse and discriminative representations via first-order optimization methods.","To validate our theoretical findings, we conduct extensive experiments on both synthetic and real data sets."],"url":"http://arxiv.org/abs/2406.01909v1","category":"cs.LG"}
{"created":"2024-06-04 02:28:51","title":"ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization","abstract":"Visual Geo-localization (VG) refers to the process to identify the location described in query images, which is widely applied in robotics field and computer vision tasks, such as autonomous driving, metaverse, augmented reality, and SLAM. In fine-grained images lacking specific text descriptions, directly applying pure visual methods to represent neighborhood features often leads to the model focusing on overly fine-grained features, unable to fully mine the semantic information in the images. Therefore, we propose a two-stage training method to enhance visual performance and use contrastive learning to mine challenging samples. We first leverage the multi-modal description capability of CLIP (Contrastive Language-Image Pretraining) to create a set of learnable text prompts for each geographic image feature to form vague descriptions. Then, by utilizing dynamic text prompts to assist the training of the image encoder, we enable the image encoder to learn better and more generalizable visual features. This strategy of applying text to purely visual tasks addresses the challenge of using multi-modal models for geographic images, which often suffer from a lack of precise descriptions, making them difficult to utilize widely. We validate the effectiveness of the proposed strategy on several large-scale visual geo-localization datasets, and our method achieves competitive results on multiple visual geo-localization datasets. Our code and model are available at https://github.com/Chain-Mao/ProGEO.","sentences":["Visual Geo-localization (VG) refers to the process to identify the location described in query images, which is widely applied in robotics field and computer vision tasks, such as autonomous driving, metaverse, augmented reality, and SLAM.","In fine-grained images lacking specific text descriptions, directly applying pure visual methods to represent neighborhood features often leads to the model focusing on overly fine-grained features, unable to fully mine the semantic information in the images.","Therefore, we propose a two-stage training method to enhance visual performance and use contrastive learning to mine challenging samples.","We first leverage the multi-modal description capability of CLIP (Contrastive Language-Image Pretraining) to create a set of learnable text prompts for each geographic image feature to form vague descriptions.","Then, by utilizing dynamic text prompts to assist the training of the image encoder, we enable the image encoder to learn better and more generalizable visual features.","This strategy of applying text to purely visual tasks addresses the challenge of using multi-modal models for geographic images, which often suffer from a lack of precise descriptions, making them difficult to utilize widely.","We validate the effectiveness of the proposed strategy on several large-scale visual geo-localization datasets, and our method achieves competitive results on multiple visual geo-localization datasets.","Our code and model are available at https://github.com/Chain-Mao/ProGEO."],"url":"http://arxiv.org/abs/2406.01906v1","category":"cs.CV"}
{"created":"2024-06-04 01:38:22","title":"Nonlinear Eigen-approach ADMM for Sparse Optimization on Stiefel Manifold","abstract":"With the growing interest and applications in machine learning and data science, finding an efficient method to sparse analysis the high-dimensional data and optimizing a dimension reduction model to extract lower dimensional features has becoming more and more important. Orthogonal constraints (Stiefel manifold) is a commonly met constraint in these applications, and the sparsity is usually enforced through the element-wise L1 norm. Many applications can be found on optimization over Stiefel manifold within the area of physics and machine learning. In this paper, we propose a novel idea by tackling the Stiefel manifold through an nonlinear eigen-approach by first using ADMM to split the problem into smooth optimization over manifold and convex non-smooth optimization, and then transforming the former into the form of nonlinear eigenvalue problem with eigenvector dependency (NEPv) which is solved by self-consistent field (SCF) iteration, and the latter can be found to have an closed-form solution through proximal gradient. Compared with existing methods, our proposed algorithm takes the advantage of specific structure of the objective function, and has efficient convergence results under mild assumptions.","sentences":["With the growing interest and applications in machine learning and data science, finding an efficient method to sparse analysis the high-dimensional data and optimizing a dimension reduction model to extract lower dimensional features has becoming more and more important.","Orthogonal constraints (Stiefel manifold) is a commonly met constraint in these applications, and the sparsity is usually enforced through the element-wise L1 norm.","Many applications can be found on optimization over Stiefel manifold within the area of physics and machine learning.","In this paper, we propose a novel idea by tackling the Stiefel manifold through an nonlinear eigen-approach by first using ADMM to split the problem into smooth optimization over manifold and convex non-smooth optimization, and then transforming the former into the form of nonlinear eigenvalue problem with eigenvector dependency (NEPv) which is solved by self-consistent field (SCF) iteration, and the latter can be found to have an closed-form solution through proximal gradient.","Compared with existing methods, our proposed algorithm takes the advantage of specific structure of the objective function, and has efficient convergence results under mild assumptions."],"url":"http://arxiv.org/abs/2406.01885v1","category":"math.OC"}
{"created":"2024-06-04 01:36:29","title":"Rank-based No-reference Quality Assessment for Face Swapping","abstract":"Face swapping has become a prominent research area in computer vision and image processing due to rapid technological advancements. The metric of measuring the quality in most face swapping methods relies on several distances between the manipulated images and the source image, or the target image, i.e., there are suitable known reference face images. Therefore, there is still a gap in accurately assessing the quality of face interchange in reference-free scenarios. In this study, we present a novel no-reference image quality assessment (NR-IQA) method specifically designed for face swapping, addressing this issue by constructing a comprehensive large-scale dataset, implementing a method for ranking image quality based on multiple facial attributes, and incorporating a Siamese network based on interpretable qualitative comparisons. Our model demonstrates the state-of-the-art performance in the quality assessment of swapped faces, providing coarse- and fine-grained. Enhanced by this metric, an improved face-swapping model achieved a more advanced level with respect to expressions and poses. Extensive experiments confirm the superiority of our method over existing general no-reference image quality assessment metrics and the latest metric of facial image quality assessment, making it well suited for evaluating face swapping images in real-world scenarios.","sentences":["Face swapping has become a prominent research area in computer vision and image processing due to rapid technological advancements.","The metric of measuring the quality in most face swapping methods relies on several distances between the manipulated images and the source image, or the target image, i.e., there are suitable known reference face images.","Therefore, there is still a gap in accurately assessing the quality of face interchange in reference-free scenarios.","In this study, we present a novel no-reference image quality assessment (NR-IQA) method specifically designed for face swapping, addressing this issue by constructing a comprehensive large-scale dataset, implementing a method for ranking image quality based on multiple facial attributes, and incorporating a Siamese network based on interpretable qualitative comparisons.","Our model demonstrates the state-of-the-art performance in the quality assessment of swapped faces, providing coarse- and fine-grained.","Enhanced by this metric, an improved face-swapping model achieved a more advanced level with respect to expressions and poses.","Extensive experiments confirm the superiority of our method over existing general no-reference image quality assessment metrics and the latest metric of facial image quality assessment, making it well suited for evaluating face swapping images in real-world scenarios."],"url":"http://arxiv.org/abs/2406.01884v1","category":"cs.CV"}
{"created":"2024-06-04 01:20:14","title":"Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check","abstract":"Chinese Spelling Check (CSC) aims to detect and correct potentially misspelled characters in Chinese sentences. Naturally, it involves the detection and correction subtasks, which interact with each other dynamically. Such interactions are bi-directional, i.e., the detection result would help reduce the risk of over-correction and under-correction while the knowledge learnt from correction would help prevent false detection. Current CSC approaches are of two types: correction-only or single-directional detection-to-correction interactive frameworks. Nonetheless, they overlook the bi-directional interactions between detection and correction. This paper aims to fill the gap by proposing a Bi-directional Detector-Corrector framework for CSC (Bi-DCSpell). Notably, Bi-DCSpell contains separate detection and correction encoders, followed by a novel interactive learning module facilitating bi-directional feature interactions between detection and correction to improve each other's representation learning. Extensive experimental results demonstrate a robust correction performance of Bi-DCSpell on widely used benchmarking datasets while possessing a satisfactory detection ability.","sentences":["Chinese Spelling Check (CSC) aims to detect and correct potentially misspelled characters in Chinese sentences.","Naturally, it involves the detection and correction subtasks, which interact with each other dynamically.","Such interactions are bi-directional, i.e., the detection result would help reduce the risk of over-correction and under-correction while the knowledge learnt from correction would help prevent false detection.","Current CSC approaches are of two types: correction-only or single-directional detection-to-correction interactive frameworks.","Nonetheless, they overlook the bi-directional interactions between detection and correction.","This paper aims to fill the gap by proposing a Bi-directional Detector-Corrector framework for CSC (Bi-DCSpell).","Notably, Bi-DCSpell contains separate detection and correction encoders, followed by a novel interactive learning module facilitating bi-directional feature interactions between detection and correction to improve each other's representation learning.","Extensive experimental results demonstrate a robust correction performance of Bi-DCSpell on widely used benchmarking datasets while possessing a satisfactory detection ability."],"url":"http://arxiv.org/abs/2406.01879v1","category":"cs.CL"}
{"created":"2024-06-04 01:02:22","title":"CR-UTP: Certified Robustness against Universal Text Perturbations","abstract":"It is imperative to ensure the stability of every prediction made by a language model; that is, a language's prediction should remain consistent despite minor input variations, like word substitutions. In this paper, we investigate the problem of certifying a language model's robustness against Universal Text Perturbations (UTPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing certified robustness based on random smoothing has shown considerable promise in certifying the input-specific text perturbations (ISTPs), operating under the assumption that any random alteration of a sample's clean or adversarial words would negate the impact of sample-wise perturbations. However, with UTPs, masking only the adversarial words can eliminate the attack. A naive method is to simply increase the masking ratio and the likelihood of masking attack tokens, but it leads to a significant reduction in both certified accuracy and the certified radius due to input corruption by extensive masking. To solve this challenge, we introduce a novel approach, the superior prompt search method, designed to identify a superior prompt that maintains higher certified accuracy under extensive masking. Additionally, we theoretically motivate why ensembles are a particularly suitable choice as base prompts for random smoothing. The method is denoted by superior prompt ensembling technique. We also empirically confirm this technique, obtaining state-of-the-art results in multiple settings. These methodologies, for the first time, enable high certified accuracy against both UTPs and ISTPs. The source code of CR-UTP is available at https://github.com/UCFML-Research/CR-UTP.","sentences":["It is imperative to ensure the stability of every prediction made by a language model; that is, a language's prediction should remain consistent despite minor input variations, like word substitutions.","In this paper, we investigate the problem of certifying a language model's robustness against Universal Text Perturbations (UTPs), which have been widely used in universal adversarial attacks and backdoor attacks.","Existing certified robustness based on random smoothing has shown considerable promise in certifying the input-specific text perturbations (ISTPs), operating under the assumption that any random alteration of a sample's clean or adversarial words would negate the impact of sample-wise perturbations.","However, with UTPs, masking only the adversarial words can eliminate the attack.","A naive method is to simply increase the masking ratio and the likelihood of masking attack tokens, but it leads to a significant reduction in both certified accuracy and the certified radius due to input corruption by extensive masking.","To solve this challenge, we introduce a novel approach, the superior prompt search method, designed to identify a superior prompt that maintains higher certified accuracy under extensive masking.","Additionally, we theoretically motivate why ensembles are a particularly suitable choice as base prompts for random smoothing.","The method is denoted by superior prompt ensembling technique.","We also empirically confirm this technique, obtaining state-of-the-art results in multiple settings.","These methodologies, for the first time, enable high certified accuracy against both UTPs and ISTPs.","The source code of CR-UTP is available at https://github.com/UCFML-Research/CR-UTP."],"url":"http://arxiv.org/abs/2406.01873v1","category":"cs.CL"}
{"created":"2024-06-04 00:37:29","title":"#EpiTwitter: Public Health Messaging During the COVID-19 Pandemic","abstract":"Effective communication during health crises is critical, with social media serving as a key platform for public health experts (PHEs) to engage with the public. However, it also amplifies pseudo-experts promoting contrarian views. Despite its importance, the role of emotional and moral language in PHEs' communication during COVID-19 remains under explored. This study examines how PHEs and pseudo-experts communicated on Twitter during the pandemic, focusing on emotional and moral language and their engagement with political elites. Analyzing tweets from 489 PHEs and 356 pseudo-experts from January 2020 to January 2021, alongside public responses, we identified key priorities and differences in messaging strategy. PHEs prioritize masking, healthcare, education, and vaccines, using positive emotional language like optimism. In contrast, pseudo-experts discuss therapeutics and lockdowns more frequently, employing negative emotions like pessimism and disgust. Negative emotional and moral language tends to drive engagement, but positive language from PHEs fosters positivity in public responses. PHEs exhibit liberal partisanship, expressing more positivity towards liberals and negativity towards conservative elites, while pseudo-experts show conservative partisanship. These findings shed light on the polarization of COVID-19 discourse and underscore the importance of strategic use of emotional and moral language by experts to mitigate polarization and enhance public trust.","sentences":["Effective communication during health crises is critical, with social media serving as a key platform for public health experts (PHEs) to engage with the public.","However, it also amplifies pseudo-experts promoting contrarian views.","Despite its importance, the role of emotional and moral language in PHEs' communication during COVID-19 remains under explored.","This study examines how PHEs and pseudo-experts communicated on Twitter during the pandemic, focusing on emotional and moral language and their engagement with political elites.","Analyzing tweets from 489 PHEs and 356 pseudo-experts from January 2020 to January 2021, alongside public responses, we identified key priorities and differences in messaging strategy.","PHEs prioritize masking, healthcare, education, and vaccines, using positive emotional language like optimism.","In contrast, pseudo-experts discuss therapeutics and lockdowns more frequently, employing negative emotions like pessimism and disgust.","Negative emotional and moral language tends to drive engagement, but positive language from PHEs fosters positivity in public responses.","PHEs exhibit liberal partisanship, expressing more positivity towards liberals and negativity towards conservative elites, while pseudo-experts show conservative partisanship.","These findings shed light on the polarization of COVID-19 discourse and underscore the importance of strategic use of emotional and moral language by experts to mitigate polarization and enhance public trust."],"url":"http://arxiv.org/abs/2406.01866v1","category":"cs.CL"}
{"created":"2024-06-04 00:37:21","title":"The influence of active agent motility on SIRS epidemiological dynamics","abstract":"Active Brownian disks moving in two dimensions that exchange information about their internal state stochastically are chosen to model epidemic spread in a self-propelled population of agents under the susceptible-infected-recovered-susceptible (SIRS) framework. The state of infection of an agent, or disk, governs its self-propulsion speed; consequently, the activity of the agents in the system varies in time. Two different protocols (one-to-one and one-to-many) are considered for the transmission of disease from the infected to susceptible populations. The effectiveness of the two protocols are practically identical at high values of the infection transmission rate. The one-to-many protocol, however, outperforms the one-to-one protocol at lower values of the infection transmission rate. Salient features of the macroscopic SIRS model are revisited, and compared to predictions from the agent-based model. Lastly, the motility induced phase separation in a population of such agents with a fluctuating fraction of active disks is found to be well-described by theories governing phase separation in a mixture of active and passive particles with a constant fraction of passive disks.","sentences":["Active Brownian disks moving in two dimensions that exchange information about their internal state stochastically are chosen to model epidemic spread in a self-propelled population of agents under the susceptible-infected-recovered-susceptible (SIRS) framework.","The state of infection of an agent, or disk, governs its self-propulsion speed; consequently, the activity of the agents in the system varies in time.","Two different protocols (one-to-one and one-to-many) are considered for the transmission of disease from the infected to susceptible populations.","The effectiveness of the two protocols are practically identical at high values of the infection transmission rate.","The one-to-many protocol, however, outperforms the one-to-one protocol at lower values of the infection transmission rate.","Salient features of the macroscopic SIRS model are revisited, and compared to predictions from the agent-based model.","Lastly, the motility induced phase separation in a population of such agents with a fluctuating fraction of active disks is found to be well-described by theories governing phase separation in a mixture of active and passive particles with a constant fraction of passive disks."],"url":"http://arxiv.org/abs/2406.01865v1","category":"cond-mat.soft"}
{"created":"2024-06-04 00:09:43","title":"Eliciting the Priors of Large Language Models using Iterated In-Context Learning","abstract":"As Large Language Models (LLMs) are increasingly deployed in real-world settings, understanding the knowledge they implicitly use when making decisions is critical. One way to capture this knowledge is in the form of Bayesian prior distributions. We develop a prompt-based workflow for eliciting prior distributions from LLMs. Our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution. We validated our method in settings where iterated learning has previously been used to estimate the priors of human participants -- causal learning, proportion estimation, and predicting everyday quantities. We found that priors elicited from GPT-4 qualitatively align with human priors in these settings. We then used the same method to elicit priors from GPT-4 for a variety of speculative events, such as the timing of the development of superhuman AI.","sentences":["As Large Language Models (LLMs) are increasingly deployed in real-world settings, understanding the knowledge they implicitly use when making decisions is critical.","One way to capture this knowledge is in the form of Bayesian prior distributions.","We develop a prompt-based workflow for eliciting prior distributions from LLMs.","Our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution.","We validated our method in settings where iterated learning has previously been used to estimate the priors of human participants -- causal learning, proportion estimation, and predicting everyday quantities.","We found that priors elicited from GPT-4 qualitatively align with human priors in these settings.","We then used the same method to elicit priors from GPT-4 for a variety of speculative events, such as the timing of the development of superhuman AI."],"url":"http://arxiv.org/abs/2406.01860v1","category":"cs.CL"}
{"created":"2024-06-03 23:51:40","title":"Conditional uncorrelation equals independence","abstract":"It is well known that the independent random variables $X$ and $Y$ are uncorrelated in the sense $E[XY]=E[X]\\cdot E[Y]$ and that the implication may be reversed in very specific cases only. This paper proves that under general assumptions the conditional uncorrelation of random variables, where the conditioning takes place over the suitable class of test sets, is equivalent to the independence. It is also shown that the mutual independence of $X_1,\\dots,X_n$ is equivalent to the fact that any conditional correlation matrix equals to the identity matrix.","sentences":["It is well known that the independent random variables $X$ and $Y$ are uncorrelated in the sense $E[XY]=E[X]\\cdot E[Y]$ and that the implication may be reversed in very specific cases only.","This paper proves that under general assumptions the conditional uncorrelation of random variables, where the conditioning takes place over the suitable class of test sets, is equivalent to the independence.","It is also shown that the mutual independence of $X_1,\\dots,X_n$ is equivalent to the fact that any conditional correlation matrix equals to the identity matrix."],"url":"http://arxiv.org/abs/2406.01849v1","category":"math.ST"}
{"created":"2024-06-03 23:40:32","title":"Dynamic transition and Galilean relativity of current-driven skyrmions","abstract":"The coupling of conduction electrons and magnetic textures leads to quantum transport phenomena described by the language of emergent electromagnetic fields [1-3]. For magnetic skyrmions, spin-swirling particle-like objects, an emergent magnetic field is produced by their topological winding [4-6], resulting in the conduction electrons exhibiting the topological Hall effect (THE) [7]. When the skyrmion lattice (SkL) acquires a drift velocity under conduction electron flow, an emergent electric field is also generated [8,9]. The resulting emergent electrodynamics dictate the magnitude of the THE via the relative motion of SkL and conduction electrons. Here, we report the emergent electrodynamics induced by SkL motion in Gd$_2$PdSi$_3$, facilitated by its giant THE [10,11]. With increasing current excitation, we observe the dynamic transition of the SkL motion from the pinned to creep regime and finally to the flow regime, where the THE is totally suppressed. We argue that the Galilean relativity required for the total cancellation of the THE can be generically recovered in the flow regime, even in complex multiband systems such as the present compound. Moreover, the observed THE voltages are large enough to enable real-time measurement of the SkL velocity-current profile, which reveals the inertial-like motion of the SkL in the creep regime, appearing as current-hysteretic behavior of the skyrmion velocity.","sentences":["The coupling of conduction electrons and magnetic textures leads to quantum transport phenomena described by the language of emergent electromagnetic fields [1-3].","For magnetic skyrmions, spin-swirling particle-like objects, an emergent magnetic field is produced by their topological winding [4-6], resulting in the conduction electrons exhibiting the topological Hall effect (THE)","[7].","When the skyrmion lattice (SkL) acquires a drift velocity under conduction electron flow, an emergent electric field is also generated [8,9].","The resulting emergent electrodynamics dictate the magnitude of the THE via the relative motion of SkL and conduction electrons.","Here, we report the emergent electrodynamics induced by SkL motion in Gd$_2$PdSi$_3$, facilitated by its giant THE [10,11].","With increasing current excitation, we observe the dynamic transition of the SkL motion from the pinned to creep regime and finally to the flow regime, where the THE is totally suppressed.","We argue that the Galilean relativity required for the total cancellation of the THE can be generically recovered in the flow regime, even in complex multiband systems such as the present compound.","Moreover, the observed THE voltages are large enough to enable real-time measurement of the SkL velocity-current profile, which reveals the inertial-like motion of the SkL in the creep regime, appearing as current-hysteretic behavior of the skyrmion velocity."],"url":"http://arxiv.org/abs/2406.01847v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-03 23:06:58","title":"Multi-Task Learning for Arousal and Sleep Stage Detection Using Fully Convolutional Networks","abstract":"Objective. Sleep is a critical physiological process that plays a vital role in maintaining physical and mental health. Accurate detection of arousals and sleep stages is essential for the diagnosis of sleep disorders, as frequent and excessive occurrences of arousals disrupt sleep stage patterns and lead to poor sleep quality, negatively impacting physical and mental health. Polysomnography is a traditional method for arousal and sleep stage detection that is time-consuming and prone to high variability among experts. Approach. In this paper, we propose a novel multi-task learning approach for arousal and sleep stage detection using fully convolutional neural networks. Our model, FullSleepNet, accepts a full-night single-channel EEG signal as input and produces segmentation masks for arousal and sleep stage labels. FullSleepNet comprises four modules: a convolutional module to extract local features, a recurrent module to capture long-range dependencies, an attention mechanism to focus on relevant parts of the input, and a segmentation module to output final predictions. Main results. By unifying the two interrelated tasks as segmentation problems and employing a multi-task learning approach, FullSleepNet achieves state-of-the-art performance for arousal detection with an area under the precision-recall curve of 0.70 on Sleep Heart Health Study and Multi-Ethnic Study of Atherosclerosis datasets. For sleep stage classification, FullSleepNet obtains comparable performance on both datasets, achieving an accuracy of 0.88 on the former and an accuracy of 0.83 on the latter. Significance. Our results demonstrate that FullSleepNet offers improved practicality, efficiency, and accuracy for the detection of arousal and classification of sleep stages using raw EEG signals as input.","sentences":["Objective.","Sleep is a critical physiological process that plays a vital role in maintaining physical and mental health.","Accurate detection of arousals and sleep stages is essential for the diagnosis of sleep disorders, as frequent and excessive occurrences of arousals disrupt sleep stage patterns and lead to poor sleep quality, negatively impacting physical and mental health.","Polysomnography is a traditional method for arousal and sleep stage detection that is time-consuming and prone to high variability among experts.","Approach.","In this paper, we propose a novel multi-task learning approach for arousal and sleep stage detection using fully convolutional neural networks.","Our model, FullSleepNet, accepts a full-night single-channel EEG signal as input and produces segmentation masks for arousal and sleep stage labels.","FullSleepNet comprises four modules: a convolutional module to extract local features, a recurrent module to capture long-range dependencies, an attention mechanism to focus on relevant parts of the input, and a segmentation module to output final predictions.","Main results.","By unifying the two interrelated tasks as segmentation problems and employing a multi-task learning approach, FullSleepNet achieves state-of-the-art performance for arousal detection with an area under the precision-recall curve of 0.70 on Sleep Heart Health Study and Multi-Ethnic Study of Atherosclerosis datasets.","For sleep stage classification, FullSleepNet obtains comparable performance on both datasets, achieving an accuracy of 0.88 on the former and an accuracy of 0.83 on the latter.","Significance.","Our results demonstrate that FullSleepNet offers improved practicality, efficiency, and accuracy for the detection of arousal and classification of sleep stages using raw EEG signals as input."],"url":"http://arxiv.org/abs/2406.01834v1","category":"eess.SP"}
{"created":"2024-06-03 22:59:46","title":"Feedback in Emerging Extragalactic Star Clusters (JWST--FEAST): Calibration of Star Formation Rates in the Mid-Infrared with NGC 628","abstract":"New JWST near-infrared imaging of the nearby galaxy NGC 628 from the Cycle 1 program JWST-FEAST is combined with archival JWST mid-infrared imaging to calibrate the 21 $\\mu$m emission as a star formation rate indicator (SFR) at $\\sim$120 pc scales. The Pa$\\alpha$ ($\\lambda$1.8756 $\\mu$m) hydrogen recombination emission line targeted by FEAST provides a reference SFR indicator that is relatively insensitive to dust attenuation, as demonstrated by combining this tracer with the HST H$\\alpha$ imaging. Our analysis is restricted to regions that appear compact in nebular line emission and are sufficiently bright to mitigate effects of both age and stochastic sampling of the stellar initial mass function. We find that the 21 $\\mu$m emission closely correlates with the nebular line emission, with a power-law with exponent=1.07$\\pm$0.01, in agreement with past results. We calibrate a hybrid SFR indicator using a combination of H$\\alpha$ and 24 $\\mu$m (extrapolated from 21 $\\mu$m) tracers and derive the proportionality constant between the two tracers $b=0.095\\pm0.007$, which is $\\sim$ 3-5 times larger than previous derivations using large regions/entire galaxies. We model these discrepancies as an increasing contribution to the dust heating by progressively older stellar populations for increasing spatial scales, in agreement with earlier findings that star formation is hierarchically distributed in galaxies. Thus, use of hybrid SFR indicators requires prior knowledge of the mean age of the stellar populations dominating the dust heating, which makes their application uncertain. Conversely, non-linear calibrations of SFRs from L(24) alone are more robust, with a factor $\\lesssim$2.5 variation across the entire range of L(24) luminosities from HII regions to galaxies.","sentences":["New JWST near-infrared imaging of the nearby galaxy NGC 628 from the Cycle 1 program JWST-FEAST is combined with archival JWST mid-infrared imaging to calibrate the 21 $\\mu$m emission as a star formation rate indicator (SFR) at $\\sim$120 pc scales.","The Pa$\\alpha$ ($\\lambda$1.8756 $\\mu$m) hydrogen recombination emission line targeted by FEAST provides a reference SFR indicator that is relatively insensitive to dust attenuation, as demonstrated by combining this tracer with the HST H$\\alpha$ imaging.","Our analysis is restricted to regions that appear compact in nebular line emission and are sufficiently bright to mitigate effects of both age and stochastic sampling of the stellar initial mass function.","We find that the 21 $\\mu$m emission closely correlates with the nebular line emission, with a power-law with exponent=1.07$\\pm$0.01, in agreement with past results.","We calibrate a hybrid SFR indicator using a combination of H$\\alpha$ and 24 $\\mu$m (extrapolated from 21 $\\mu$m) tracers and derive the proportionality constant between the two tracers $b=0.095\\pm0.007$, which is $\\sim$ 3-5 times larger than previous derivations using large regions/entire galaxies.","We model these discrepancies as an increasing contribution to the dust heating by progressively older stellar populations for increasing spatial scales, in agreement with earlier findings that star formation is hierarchically distributed in galaxies.","Thus, use of hybrid SFR indicators requires prior knowledge of the mean age of the stellar populations dominating the dust heating, which makes their application uncertain.","Conversely, non-linear calibrations of SFRs from L(24) alone are more robust, with a factor $\\lesssim$2.5 variation across the entire range of L(24) luminosities from HII regions to galaxies."],"url":"http://arxiv.org/abs/2406.01831v1","category":"astro-ph.GA"}
{"created":"2024-06-03 22:54:11","title":"Measuring the magnetic dipole moment and magnetospheric fluctuations of accretion-powered pulsars in the Small Magellanic Cloud with an unscented Kalman filter","abstract":"Many accretion-powered pulsars rotate in magnetocentrifugal disequilibrium, spinning up or down secularly over multi-year intervals. The magnetic dipole moment $\\mu$ of such systems cannot be inferred uniquely from the time-averaged aperiodic X-ray flux $\\langle L(t) \\rangle$ and pulse period $\\langle P(t) \\rangle$, because the radiative efficiency of the accretion is unknown and degenerate with the mass accretion rate. Here we circumvent the degeneracy by tracking the fluctuations in the unaveraged time series $L(t)$ and $P(t)$ using an unscented Kalman filter, whereupon $\\mu$ can be estimated uniquely, up to the uncertainties in the mass, radius and distance of the star. The analysis is performed on Rossi X-ray Timing Explorer observations for $24$ X-ray transients in the Small Magellanic Cloud, which have been monitored regularly for $\\sim 16$ years. As well as independent estimates of $\\mu$, the analysis yields time-resolved histories of the mass accretion rate and the Maxwell stress at the disk-magnetosphere boundary for each star, and hence auto- and cross-correlations involving the latter two state variables. The inferred fluctuation statistics convey important information about the complex accretion physics at the disk-magnetosphere boundary.","sentences":["Many accretion-powered pulsars rotate in magnetocentrifugal disequilibrium, spinning up or down secularly over multi-year intervals.","The magnetic dipole moment $\\mu$ of such systems cannot be inferred uniquely from the time-averaged aperiodic X-ray flux $\\langle L(t) \\rangle$ and pulse period $\\langle P(t) \\rangle$, because the radiative efficiency of the accretion is unknown and degenerate with the mass accretion rate.","Here we circumvent the degeneracy by tracking the fluctuations in the unaveraged time series $L(t)$ and $P(t)$ using an unscented Kalman filter, whereupon $\\mu$ can be estimated uniquely, up to the uncertainties in the mass, radius and distance of the star.","The analysis is performed on Rossi X-ray Timing Explorer observations for $24$ X-ray transients in the Small Magellanic Cloud, which have been monitored regularly for $\\sim 16$ years.","As well as independent estimates of $\\mu$, the analysis yields time-resolved histories of the mass accretion rate and the Maxwell stress at the disk-magnetosphere boundary for each star, and hence auto- and cross-correlations involving the latter two state variables.","The inferred fluctuation statistics convey important information about the complex accretion physics at the disk-magnetosphere boundary."],"url":"http://arxiv.org/abs/2406.01827v1","category":"astro-ph.HE"}
{"created":"2024-06-03 22:41:32","title":"Global existence of solutions to the fully parabolic chemotaxis system with logistic source under nonlinear Neumann boundary conditions","abstract":"We study the existence of global boundedness solutions to the fully parabolic chemotaxis systems with logistic sources, $ru- \\mu u^2$, under nonlinear Neumann boundary conditions, $\\frac{\\partial u}{\\partial \\nu }= |u|^p$ where $p >1 $ in smooth bounded domain $\\Omega \\subset \\mathbb{R}^n$ with $n \\geq 2$. A recent study by Le (2023) has shown that the logistic sources can ensure that solutions are global and bounded when $n =2$ with $p < \\frac{3}{2}$ and $n=3$ with $p <\\frac{7}{5}$. In this paper, we extend the previous findings by demonstrating the existence of global bounded solutions when $p< \\frac{3}{2}$ in any spatial dimension $n \\geq 2$.","sentences":["We study the existence of global boundedness solutions to the fully parabolic chemotaxis systems with logistic sources, $ru- \\mu u^2$, under nonlinear Neumann boundary conditions, $\\frac{\\partial u}{\\partial \\nu }= |u|^p$ where $p >1 $ in smooth bounded domain $\\Omega \\subset \\mathbb{R}^n$ with $n \\geq 2$.","A recent study by Le (2023) has shown that the logistic sources can ensure that solutions are global and bounded when","$n =2$ with $p < \\frac{3}{2}$ and $n=3$ with $p <\\frac{7}{5}$.","In this paper, we extend the previous findings by demonstrating the existence of global bounded solutions when $p< \\frac{3}{2}$ in any spatial dimension $n \\geq 2$."],"url":"http://arxiv.org/abs/2406.01826v1","category":"math.AP"}
{"created":"2024-06-03 22:26:52","title":"Infection fronts in randomly varying transmission-rate media","abstract":"We numerically investigate the geometry and transport properties of infection fronts within the spatial SIR model in two dimensions. The model incorporates short-range correlated quenched random transmission rates. Our findings reveal that the critical average transmission rate for the steady-state propagation of the infection is overestimated by the naive mean-field homogenization. Furthermore, we observe that the velocity, profile, and harmfulness of the fronts, given a specific average transmission, are sensitive to the details of randomness. In particular, we find that the harmfulness of the front is larger the more uniform the transmission-rate is, suggesting potential optimization in vaccination strategies under constraints like fixed average-transmission-rates or limited vaccine resources. The large-scale geometry of the advancing fronts presents nevertheless robust universal features and, for a statistically isotropic and short-range correlated disorder, we get a roughness exponent $\\alpha\\approx 0.42 \\pm 0.10$ and a dynamical exponent $z\\approx 1.6 \\pm 0.10$, which are roughly compatible with the one-dimensional Kardar-Parisi-Zhang (KPZ) universality class. We find that the KPZ term and the disorder-induced effective noise are present and have a kinematic origin.","sentences":["We numerically investigate the geometry and transport properties of infection fronts within the spatial SIR model in two dimensions.","The model incorporates short-range correlated quenched random transmission rates.","Our findings reveal that the critical average transmission rate for the steady-state propagation of the infection is overestimated by the naive mean-field homogenization.","Furthermore, we observe that the velocity, profile, and harmfulness of the fronts, given a specific average transmission, are sensitive to the details of randomness.","In particular, we find that the harmfulness of the front is larger the more uniform the transmission-rate is, suggesting potential optimization in vaccination strategies under constraints like fixed average-transmission-rates or limited vaccine resources.","The large-scale geometry of the advancing fronts presents nevertheless robust universal features and, for a statistically isotropic and short-range correlated disorder, we get a roughness exponent $\\alpha\\approx 0.42 \\pm 0.10$ and a dynamical exponent $z\\approx 1.6 \\pm 0.10$, which are roughly compatible with the one-dimensional Kardar-Parisi-Zhang (KPZ) universality class.","We find that the KPZ term and the disorder-induced effective noise are present and have a kinematic origin."],"url":"http://arxiv.org/abs/2406.01822v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-03 22:15:57","title":"Long-term foehn reconstruction combining unsupervised and supervised learning","abstract":"Foehn winds, characterized by abrupt temperature increases and wind speed changes, significantly impact regions on the leeward side of mountain ranges, e.g., by spreading wildfires. Understanding how foehn occurrences change under climate change is crucial. Unfortunately, foehn cannot be measured directly but has to be inferred from meteorological measurements employing suitable classification schemes. Hence, this approach is typically limited to specific periods for which the necessary data are available. We present a novel approach for reconstructing historical foehn occurrences using a combination of unsupervised and supervised probabilistic statistical learning methods. We utilize in-situ measurements (available for recent decades) to train an unsupervised learner (finite mixture model) for automatic foehn classification. These labeled data are then linked to reanalysis data (covering longer periods) using a supervised learner (lasso or boosting). This allows to reconstruct past foehn probabilities based solely on reanalysis data. Applying this method to ERA5 reanalysis data for six stations across Switzerland and Austria achieves accurate hourly reconstructions of north and south foehn occurrence, respectively, dating back to 1940. This paves the way for investigating how seasonal foehn patterns have evolved over the past 83 years, providing valuable insights into climate change impacts on these critical wind events.","sentences":["Foehn winds, characterized by abrupt temperature increases and wind speed changes, significantly impact regions on the leeward side of mountain ranges, e.g., by spreading wildfires.","Understanding how foehn occurrences change under climate change is crucial.","Unfortunately, foehn cannot be measured directly but has to be inferred from meteorological measurements employing suitable classification schemes.","Hence, this approach is typically limited to specific periods for which the necessary data are available.","We present a novel approach for reconstructing historical foehn occurrences using a combination of unsupervised and supervised probabilistic statistical learning methods.","We utilize in-situ measurements (available for recent decades) to train an unsupervised learner (finite mixture model) for automatic foehn classification.","These labeled data are then linked to reanalysis data (covering longer periods) using a supervised learner (lasso or boosting).","This allows to reconstruct past foehn probabilities based solely on reanalysis data.","Applying this method to ERA5 reanalysis data for six stations across Switzerland and Austria achieves accurate hourly reconstructions of north and south foehn occurrence, respectively, dating back to 1940.","This paves the way for investigating how seasonal foehn patterns have evolved over the past 83 years, providing valuable insights into climate change impacts on these critical wind events."],"url":"http://arxiv.org/abs/2406.01818v1","category":"stat.AP"}
{"created":"2024-06-03 22:11:39","title":"ZAPP! Zonotope Agreement of Prediction and Planning for Continuous-Time Collision Avoidance with Discrete-Time Dynamics","abstract":"The past few years have seen immense progress on two fronts that are critical to safe, widespread mobile robot deployment: predicting uncertain motion of multiple agents, and planning robot motion under uncertainty. However, the numerical methods required on each front have resulted in a mismatch of representation for prediction and planning. In prediction, numerical tractability is usually achieved by coarsely discretizing time, and by representing multimodal multi-agent interactions as distributions with infinite support. On the other hand, safe planning typically requires very fine time discretization, paired with distributions with compact support, to reduce conservativeness and ensure numerical tractability. The result is, when existing predictors are coupled with planning and control, one may often find unsafe motion plans. This paper proposes ZAPP (Zonotope Agreement of Prediction and Planning) to resolve the representation mismatch. ZAPP unites a prediction-friendly coarse time discretization and a planning-friendly zonotope uncertainty representation; the method also enables differentiating through a zonotope collision check, allowing one to integrate prediction and planning within a gradient-based optimization framework. Numerical examples show how ZAPP can produce safer trajectories compared to baselines in interactive scenes.","sentences":["The past few years have seen immense progress on two fronts that are critical to safe, widespread mobile robot deployment: predicting uncertain motion of multiple agents, and planning robot motion under uncertainty.","However, the numerical methods required on each front have resulted in a mismatch of representation for prediction and planning.","In prediction, numerical tractability is usually achieved by coarsely discretizing time, and by representing multimodal multi-agent interactions as distributions with infinite support.","On the other hand, safe planning typically requires very fine time discretization, paired with distributions with compact support, to reduce conservativeness and ensure numerical tractability.","The result is, when existing predictors are coupled with planning and control, one may often find unsafe motion plans.","This paper proposes ZAPP (Zonotope Agreement of Prediction and Planning) to resolve the representation mismatch.","ZAPP unites a prediction-friendly coarse time discretization and a planning-friendly zonotope uncertainty representation; the method also enables differentiating through a zonotope collision check, allowing one to integrate prediction and planning within a gradient-based optimization framework.","Numerical examples show how ZAPP can produce safer trajectories compared to baselines in interactive scenes."],"url":"http://arxiv.org/abs/2406.01814v1","category":"cs.RO"}
{"created":"2024-06-03 22:09:47","title":"A Game-Theoretic Approach to Privacy-Utility Tradeoff in Sharing Genomic Summary Statistics","abstract":"The advent of online genomic data-sharing services has sought to enhance the accessibility of large genomic datasets by allowing queries about genetic variants, such as summary statistics, aiding care providers in distinguishing between spurious genomic variations and those with clinical significance. However, numerous studies have demonstrated that even sharing summary genomic information exposes individual members of such datasets to a significant privacy risk due to membership inference attacks. While several approaches have emerged that reduce privacy risks by adding noise or reducing the amount of information shared, these typically assume non-adaptive attacks that use likelihood ratio test (LRT) statistics. We propose a Bayesian game-theoretic framework for optimal privacy-utility tradeoff in the sharing of genomic summary statistics. Our first contribution is to prove that a very general Bayesian attacker model that anchors our game-theoretic approach is more powerful than the conventional LRT-based threat models in that it induces worse privacy loss for the defender who is modeled as a von Neumann-Morgenstern (vNM) decision-maker. We show this to be true even when the attacker uses a non-informative subjective prior. Next, we present an analytically tractable approach to compare the Bayesian attacks with arbitrary subjective priors and the Neyman-Pearson optimal LRT attacks under the Gaussian mechanism common in differential privacy frameworks. Finally, we propose an approach for approximating Bayes-Nash equilibria of the game using deep neural network generators to implicitly represent player mixed strategies. Our experiments demonstrate that the proposed game-theoretic framework yields both stronger attacks and stronger defense strategies than the state of the art.","sentences":["The advent of online genomic data-sharing services has sought to enhance the accessibility of large genomic datasets by allowing queries about genetic variants, such as summary statistics, aiding care providers in distinguishing between spurious genomic variations and those with clinical significance.","However, numerous studies have demonstrated that even sharing summary genomic information exposes individual members of such datasets to a significant privacy risk due to membership inference attacks.","While several approaches have emerged that reduce privacy risks by adding noise or reducing the amount of information shared, these typically assume non-adaptive attacks that use likelihood ratio test (LRT) statistics.","We propose a Bayesian game-theoretic framework for optimal privacy-utility tradeoff in the sharing of genomic summary statistics.","Our first contribution is to prove that a very general Bayesian attacker model that anchors our game-theoretic approach is more powerful than the conventional LRT-based threat models in that it induces worse privacy loss for the defender who is modeled as a von Neumann-Morgenstern (vNM) decision-maker.","We show this to be true even when the attacker uses a non-informative subjective prior.","Next, we present an analytically tractable approach to compare the Bayesian attacks with arbitrary subjective priors and the Neyman-Pearson optimal LRT attacks under the Gaussian mechanism common in differential privacy frameworks.","Finally, we propose an approach for approximating Bayes-Nash equilibria of the game using deep neural network generators to implicitly represent player mixed strategies.","Our experiments demonstrate that the proposed game-theoretic framework yields both stronger attacks and stronger defense strategies than the state of the art."],"url":"http://arxiv.org/abs/2406.01811v1","category":"cs.CR"}
{"created":"2024-06-03 21:42:07","title":"Motion Planning for Hybrid Dynamical Systems: Framework, Algorithm Template, and a Sampling-based Approach","abstract":"This paper focuses on the motion planning problem for the systems exhibiting both continuous and discrete behaviors, which we refer to as hybrid dynamical systems. Firstly, the motion planning problem for hybrid systems is formulated using the hybrid equation framework, which is general to capture most hybrid systems. Secondly, a propagation algorithm template is proposed that describes a general framework to solve the motion planning problem for hybrid systems. Thirdly, a rapidly-exploring random trees (RRT) implementation of the proposed algorithm template is designed to solve the motion planning problem for hybrid systems. At each iteration, the proposed algorithm, called HyRRT, randomly picks a state sample and extends the search tree by flow or jump, which is also chosen randomly when both regimes are possible. Through a definition of concatenation of functions defined on hybrid time domains, we show that HyRRT is probabilistically complete, namely, the probability of failing to find a motion plan approaches zero as the number of iterations of the algorithm increases. This property is guaranteed under mild conditions on the data defining the motion plan, which include a relaxation of the usual positive clearance assumption imposed in the literature of classical systems. The motion plan is computed through the solution of two optimization problems, one associated with the flow and the other with the jumps of the system. The proposed algorithm is applied to an actuated bouncing ball system and a walking robot system so as to highlight its generality and computational features.","sentences":["This paper focuses on the motion planning problem for the systems exhibiting both continuous and discrete behaviors, which we refer to as hybrid dynamical systems.","Firstly, the motion planning problem for hybrid systems is formulated using the hybrid equation framework, which is general to capture most hybrid systems.","Secondly, a propagation algorithm template is proposed that describes a general framework to solve the motion planning problem for hybrid systems.","Thirdly, a rapidly-exploring random trees (RRT) implementation of the proposed algorithm template is designed to solve the motion planning problem for hybrid systems.","At each iteration, the proposed algorithm, called HyRRT, randomly picks a state sample and extends the search tree by flow or jump, which is also chosen randomly when both regimes are possible.","Through a definition of concatenation of functions defined on hybrid time domains, we show that HyRRT is probabilistically complete, namely, the probability of failing to find a motion plan approaches zero as the number of iterations of the algorithm increases.","This property is guaranteed under mild conditions on the data defining the motion plan, which include a relaxation of the usual positive clearance assumption imposed in the literature of classical systems.","The motion plan is computed through the solution of two optimization problems, one associated with the flow and the other with the jumps of the system.","The proposed algorithm is applied to an actuated bouncing ball system and a walking robot system so as to highlight its generality and computational features."],"url":"http://arxiv.org/abs/2406.01802v1","category":"cs.RO"}
{"created":"2024-06-03 21:32:50","title":"The Empirical Impact of Forgetting and Transfer in Continual Visual Odometry","abstract":"As robotics continues to advance, the need for adaptive and continuously-learning embodied agents increases, particularly in the realm of assistance robotics. Quick adaptability and long-term information retention are essential to operate in dynamic environments typical of humans' everyday lives. A lifelong learning paradigm is thus required, but it is scarcely addressed by current robotics literature. This study empirically investigates the impact of catastrophic forgetting and the effectiveness of knowledge transfer in neural networks trained continuously in an embodied setting. We focus on the task of visual odometry, which holds primary importance for embodied agents in enabling their self-localization. We experiment on the simple continual scenario of discrete transitions between indoor locations, akin to a robot navigating different apartments. In this regime, we observe initial satisfactory performance with high transferability between environments, followed by a specialization phase where the model prioritizes current environment-specific knowledge at the expense of generalization. Conventional regularization strategies and increased model capacity prove ineffective in mitigating this phenomenon. Rehearsal is instead mildly beneficial but with the addition of a substantial memory cost. Incorporating action information, as commonly done in embodied settings, facilitates quicker convergence but exacerbates specialization, making the model overly reliant on its motion expectations and less adept at correctly interpreting visual cues. These findings emphasize the open challenges of balancing adaptation and memory retention in lifelong robotics and contribute valuable insights into the application of a lifelong paradigm on embodied agents.","sentences":["As robotics continues to advance, the need for adaptive and continuously-learning embodied agents increases, particularly in the realm of assistance robotics.","Quick adaptability and long-term information retention are essential to operate in dynamic environments typical of humans' everyday lives.","A lifelong learning paradigm is thus required, but it is scarcely addressed by current robotics literature.","This study empirically investigates the impact of catastrophic forgetting and the effectiveness of knowledge transfer in neural networks trained continuously in an embodied setting.","We focus on the task of visual odometry, which holds primary importance for embodied agents in enabling their self-localization.","We experiment on the simple continual scenario of discrete transitions between indoor locations, akin to a robot navigating different apartments.","In this regime, we observe initial satisfactory performance with high transferability between environments, followed by a specialization phase where the model prioritizes current environment-specific knowledge at the expense of generalization.","Conventional regularization strategies and increased model capacity prove ineffective in mitigating this phenomenon.","Rehearsal is instead mildly beneficial but with the addition of a substantial memory cost.","Incorporating action information, as commonly done in embodied settings, facilitates quicker convergence but exacerbates specialization, making the model overly reliant on its motion expectations and less adept at correctly interpreting visual cues.","These findings emphasize the open challenges of balancing adaptation and memory retention in lifelong robotics and contribute valuable insights into the application of a lifelong paradigm on embodied agents."],"url":"http://arxiv.org/abs/2406.01797v1","category":"cs.CV"}
{"created":"2024-06-03 21:27:56","title":"Video Coding with Cross-Component Sample Offset","abstract":"Beyond the exploration of traditional spatial, temporal and subjective visual signal redundancy in image and video compression, recent research has focused on leveraging cross-color component redundancy to enhance coding efficiency. Cross-component coding approaches are motivated by the statistical correlations among different color components, such as those in the Y'CbCr color space, where luma (Y) color component typically exhibits finer details than chroma (Cb/Cr) color components. Inspired by previous cross-component coding algorithms, this paper introduces a novel in-loop filtering approach named Cross-Component Sample Offset (CCSO). CCSO utilizes co-located and neighboring luma samples to generate correction signals for both luma and chroma reconstructed samples. It is a multiplication-free, non-linear mapping process implemented using a look-up-table. The input to the mapping is a group of reconstructed luma samples, and the output is an offset value applied on the center luma or co-located chroma sample. Experimental results demonstrate that the proposed CCSO can be applied to both image and video coding, resulting in improved coding efficiency and visual quality. The method has been adopted into an experimental next-generation video codec beyond AV1 developed by the Alliance for Open Media (AOMedia), achieving significant objective coding gains up to 3.5\\,\\% and 1.8\\,\\% for PSNR and VMAF quality metrics, respectively, under random access configuration. Additionally, CCSO notably improves the subjective visual quality.","sentences":["Beyond the exploration of traditional spatial, temporal and subjective visual signal redundancy in image and video compression, recent research has focused on leveraging cross-color component redundancy to enhance coding efficiency.","Cross-component coding approaches are motivated by the statistical correlations among different color components, such as those in the Y'CbCr color space, where luma (Y) color component typically exhibits finer details than chroma (Cb/Cr) color components.","Inspired by previous cross-component coding algorithms, this paper introduces a novel in-loop filtering approach named Cross-Component Sample Offset (CCSO).","CCSO utilizes co-located and neighboring luma samples to generate correction signals for both luma and chroma reconstructed samples.","It is a multiplication-free, non-linear mapping process implemented using a look-up-table.","The input to the mapping is a group of reconstructed luma samples, and the output is an offset value applied on the center luma or co-located chroma sample.","Experimental results demonstrate that the proposed CCSO can be applied to both image and video coding, resulting in improved coding efficiency and visual quality.","The method has been adopted into an experimental next-generation video codec beyond AV1 developed by the Alliance for Open Media (AOMedia), achieving significant objective coding gains up to 3.5\\,\\% and 1.8\\,\\% for PSNR and VMAF quality metrics, respectively, under random access configuration.","Additionally, CCSO notably improves the subjective visual quality."],"url":"http://arxiv.org/abs/2406.01795v1","category":"eess.IV"}
{"created":"2024-06-03 21:21:17","title":"It Takes Two: A Peer-Prediction Solution for Blockchain Verifier's Dilemma","abstract":"The security of blockchain systems is fundamentally based on the decentralized consensus in which the majority of parties behave honestly, and the process of content verification is essential to keep the robustness of blockchain systems. However, the phenomenon that a secure blockchain system with few or no cheaters could not provide sufficient incentive for verifiers to honestly perform the costly verification, referred to as the Verifier's Dilemma, could severely undermine the fundamental security of blockchain systems. While existing works have attempted to insert deliberate errors to disincentivize lazy verification, the decentralized environment makes it impossible to judge the correctness of verification or detect malicious verifiers directly.   In this paper, we initiate the research that leverages the peer prediction approach towards the design of Bayesian truthful mechanisms for the decentralized verification game among multiple verifiers, incentivizing all verifiers to perform honest verification without access to the ground truth even in the presence of noisy observations in the verification process. With theoretically guaranteed truthfulness of our mechanism for the verification game, our work provides a framework of verification mechanisms that enhances the security and robustness of the blockchain and potentially other decentralized systems.","sentences":["The security of blockchain systems is fundamentally based on the decentralized consensus in which the majority of parties behave honestly, and the process of content verification is essential to keep the robustness of blockchain systems.","However, the phenomenon that a secure blockchain system with few or no cheaters could not provide sufficient incentive for verifiers to honestly perform the costly verification, referred to as the Verifier's Dilemma, could severely undermine the fundamental security of blockchain systems.","While existing works have attempted to insert deliberate errors to disincentivize lazy verification, the decentralized environment makes it impossible to judge the correctness of verification or detect malicious verifiers directly.   ","In this paper, we initiate the research that leverages the peer prediction approach towards the design of Bayesian truthful mechanisms for the decentralized verification game among multiple verifiers, incentivizing all verifiers to perform honest verification without access to the ground truth even in the presence of noisy observations in the verification process.","With theoretically guaranteed truthfulness of our mechanism for the verification game, our work provides a framework of verification mechanisms that enhances the security and robustness of the blockchain and potentially other decentralized systems."],"url":"http://arxiv.org/abs/2406.01794v1","category":"cs.CR"}
{"created":"2024-06-04 16:38:57","title":"Landscape-Aware Growing: The Power of a Little LAG","abstract":"Recently, there has been increasing interest in efficient pretraining paradigms for training Transformer-based models. Several recent approaches use smaller models to initialize larger models in order to save computation (e.g., stacking and fusion). In this work, we study the fundamental question of how to select the best growing strategy from a given pool of growing strategies. Prior works have extensively focused on loss- and/or function-preserving behavior at initialization or simply performance at the end of training. Instead, we identify that behavior at initialization can be misleading as a predictor of final performance and present an alternative perspective based on early training dynamics, which we call \"landscape-aware growing (LAG)\". We perform extensive analysis of correlation of the final performance with performance in the initial steps of training and find early and more accurate predictions of the optimal growing strategy (i.e., with only a small \"lag\" after initialization). This perspective also motivates an adaptive strategy for gradual stacking.","sentences":["Recently, there has been increasing interest in efficient pretraining paradigms for training Transformer-based models.","Several recent approaches use smaller models to initialize larger models in order to save computation (e.g., stacking and fusion).","In this work, we study the fundamental question of how to select the best growing strategy from a given pool of growing strategies.","Prior works have extensively focused on loss- and/or function-preserving behavior at initialization or simply performance at the end of training.","Instead, we identify that behavior at initialization can be misleading as a predictor of final performance and present an alternative perspective based on early training dynamics, which we call \"landscape-aware growing (LAG)\".","We perform extensive analysis of correlation of the final performance with performance in the initial steps of training and find early and more accurate predictions of the optimal growing strategy (i.e., with only a small \"lag\" after initialization).","This perspective also motivates an adaptive strategy for gradual stacking."],"url":"http://arxiv.org/abs/2406.02469v1","category":"cs.LG"}
{"created":"2024-06-04 16:03:23","title":"Ultra-thin transistors and circuits for conformable electronics","abstract":"Adapting electronics to perfectly conform to non-planar and rough surfaces, such as human skin, is a very challenging task which, if solved, could open up new applications in fields of high economic and scientific interest ranging from health to robotics, wearable electronics, human-machine interface and Internet of Things. The key to success lies in defining a technology that can lead to the fabrication of ultra-thin devices while exploiting materials that are ultimately thin, with high mechanical flexibility and excellent electrical properties. Here, we report a hybrid approach for the definition of high-performance, ultra-thin and conformable electronic devices and circuits, based on the integration of ultimately thin semiconducting transition metal dichalcogenides (TMDC), i.e., MoS2, with organic gate dielectric material, i.e., polyvinyl formal (PVF) combined with the ink-jet printing of conductive PEDOT:PSS ink for electrodes definition. Through this cost-effective, fully bottom-up and solution-based approach, transistors and simple digital and analogue circuits are fabricated by a sequential stacking of ultrathin (nanometer) layers on a few micron thick polyimide substrate, which guarantees the high flexibility mandatory for the targeted applications.","sentences":["Adapting electronics to perfectly conform to non-planar and rough surfaces, such as human skin, is a very challenging task which, if solved, could open up new applications in fields of high economic and scientific interest ranging from health to robotics, wearable electronics, human-machine interface and Internet of Things.","The key to success lies in defining a technology that can lead to the fabrication of ultra-thin devices while exploiting materials that are ultimately thin, with high mechanical flexibility and excellent electrical properties.","Here, we report a hybrid approach for the definition of high-performance, ultra-thin and conformable electronic devices and circuits, based on the integration of ultimately thin semiconducting transition metal dichalcogenides (TMDC), i.e., MoS2, with organic gate dielectric material, i.e., polyvinyl formal (PVF) combined with the ink-jet printing of conductive PEDOT:","PSS ink for electrodes definition.","Through this cost-effective, fully bottom-up and solution-based approach, transistors and simple digital and analogue circuits are fabricated by a sequential stacking of ultrathin (nanometer) layers on a few micron thick polyimide substrate, which guarantees the high flexibility mandatory for the targeted applications."],"url":"http://arxiv.org/abs/2406.02442v1","category":"physics.app-ph"}
{"created":"2024-06-04 14:06:15","title":"Polynomial-Augmented Neural Networks (PANNs) with Weak Orthogonality Constraints for Enhanced Function and PDE Approximation","abstract":"We present polynomial-augmented neural networks (PANNs), a novel machine learning architecture that combines deep neural networks (DNNs) with a polynomial approximant. PANNs combine the strengths of DNNs (flexibility and efficiency in higher-dimensional approximation) with those of polynomial approximation (rapid convergence rates for smooth functions). To aid in both stable training and enhanced accuracy over a variety of problems, we present (1) a family of orthogonality constraints that impose mutual orthogonality between the polynomial and the DNN within a PANN; (2) a simple basis pruning approach to combat the curse of dimensionality introduced by the polynomial component; and (3) an adaptation of a polynomial preconditioning strategy to both DNNs and polynomials. We test the resulting architecture for its polynomial reproduction properties, ability to approximate both smooth functions and functions of limited smoothness, and as a method for the solution of partial differential equations (PDEs). Through these experiments, we demonstrate that PANNs offer superior approximation properties to DNNs for both regression and the numerical solution of PDEs, while also offering enhanced accuracy over both polynomial and DNN-based regression (each) when regressing functions with limited smoothness.","sentences":["We present polynomial-augmented neural networks (PANNs), a novel machine learning architecture that combines deep neural networks (DNNs) with a polynomial approximant.","PANNs combine the strengths of DNNs (flexibility and efficiency in higher-dimensional approximation) with those of polynomial approximation (rapid convergence rates for smooth functions).","To aid in both stable training and enhanced accuracy over a variety of problems, we present (1) a family of orthogonality constraints that impose mutual orthogonality between the polynomial and the DNN within a PANN; (2) a simple basis pruning approach to combat the curse of dimensionality introduced by the polynomial component; and (3) an adaptation of a polynomial preconditioning strategy to both DNNs and polynomials.","We test the resulting architecture for its polynomial reproduction properties, ability to approximate both smooth functions and functions of limited smoothness, and as a method for the solution of partial differential equations (PDEs).","Through these experiments, we demonstrate that PANNs offer superior approximation properties to DNNs for both regression and the numerical solution of PDEs, while also offering enhanced accuracy over both polynomial and DNN-based regression (each) when regressing functions with limited smoothness."],"url":"http://arxiv.org/abs/2406.02336v1","category":"cs.LG"}
{"created":"2024-06-04 13:51:12","title":"Compositional dynamic modelling for causal prediction in multivariate time series","abstract":"Theoretical developments in sequential Bayesian analysis of multivariate dynamic models underlie new methodology for causal prediction. This extends the utility of existing models with computationally efficient methodology, enabling routine exploration of Bayesian counterfactual analyses with multiple selected time series as synthetic controls. Methodological contributions also define the concept of outcome adaptive modelling to monitor and inferentially respond to changes in experimental time series following interventions designed to explore causal effects. The benefits of sequential analyses with time-varying parameter models for causal investigations are inherited in this broader setting. A case study in commercial causal analysis-- involving retail revenue outcomes related to marketing interventions-- highlights the methodological advances.","sentences":["Theoretical developments in sequential Bayesian analysis of multivariate dynamic models underlie new methodology for causal prediction.","This extends the utility of existing models with computationally efficient methodology, enabling routine exploration of Bayesian counterfactual analyses with multiple selected time series as synthetic controls.","Methodological contributions also define the concept of outcome adaptive modelling to monitor and inferentially respond to changes in experimental time series following interventions designed to explore causal effects.","The benefits of sequential analyses with time-varying parameter models for causal investigations are inherited in this broader setting.","A case study in commercial causal analysis-- involving retail revenue outcomes related to marketing interventions-- highlights the methodological advances."],"url":"http://arxiv.org/abs/2406.02320v1","category":"stat.ME"}
{"created":"2024-06-04 10:56:56","title":"Porting the grid-based 3D+3V hybrid-Vlasov kinetic plasma simulation Vlasiator to heterogeneous GPU architectures","abstract":"Vlasiator is a space plasma simulation code which models near-Earth ion-kinetic dynamics in three spatial and three velocity dimensions. It is highly parallelized, modeling the Vlasov equation directly through the distribution function, discretized on a Cartesian grid, instead of the more common particle-in-cell approach. Modeling near-Earth space, plasma properties span several orders of magnitude in temperature, density, and magnetic field strength. In order to fit the required six-dimensional grids in memory, Vlasiator utilizes a sparse block-based velocity mesh, where chunks of velocity space are added or deleted based on the advection requirements of the Vlasov solver. In addition, the spatial mesh is adaptively refined through cell-based octree refinement. In this paper, we describe the design choices of porting Vlasiator to heterogeneous CPU/GPU architectures. We detail the memory management, algorithmic changes, and kernel construction as well as our unified codebase approach, resulting in portability to both NVIDIA and AMD hardware (CUDA and HIP languages, respectively). In particular, we showcase a highly parallel block adjustment approach allowing efficient re-ordering of a sparse velocity mesh. We detail pitfalls we have overcome and lay out a plan for optimization to facilitate future exascale simulations using multi-node GPU supercomputing.","sentences":["Vlasiator is a space plasma simulation code which models near-Earth ion-kinetic dynamics in three spatial and three velocity dimensions.","It is highly parallelized, modeling the Vlasov equation directly through the distribution function, discretized on a Cartesian grid, instead of the more common particle-in-cell approach.","Modeling near-Earth space, plasma properties span several orders of magnitude in temperature, density, and magnetic field strength.","In order to fit the required six-dimensional grids in memory, Vlasiator utilizes a sparse block-based velocity mesh, where chunks of velocity space are added or deleted based on the advection requirements of the Vlasov solver.","In addition, the spatial mesh is adaptively refined through cell-based octree refinement.","In this paper, we describe the design choices of porting Vlasiator to heterogeneous CPU/GPU architectures.","We detail the memory management, algorithmic changes, and kernel construction as well as our unified codebase approach, resulting in portability to both NVIDIA and AMD hardware (CUDA and HIP languages, respectively).","In particular, we showcase a highly parallel block adjustment approach allowing efficient re-ordering of a sparse velocity mesh.","We detail pitfalls we have overcome and lay out a plan for optimization to facilitate future exascale simulations using multi-node GPU supercomputing."],"url":"http://arxiv.org/abs/2406.02201v1","category":"physics.comp-ph"}
{"created":"2024-06-04 09:32:08","title":"The continuity equation in the Heisenberg-periodic case: a representation formula and an application to Mean Field Games","abstract":"We provide a representation of the weak solution of the continuity equation on the Heisenberg group $\\mathbb H^1$ with periodic data (the periodicity is suitably adapted to the group law). This solution is the push forward of a measure concentrated on the flux associated with the drift of the continuity equation. Furthermore, we shall use this interpretation for proving that weak solutions to first order Mean Field Games on $\\mathbb H^1$ are also mild solutions.","sentences":["We provide a representation of the weak solution of the continuity equation on the Heisenberg group $\\mathbb H^1$ with periodic data (the periodicity is suitably adapted to the group law).","This solution is the push forward of a measure concentrated on the flux associated with the drift of the continuity equation.","Furthermore, we shall use this interpretation for proving that weak solutions to first order Mean Field Games on $\\mathbb H^1$ are also mild solutions."],"url":"http://arxiv.org/abs/2406.02145v1","category":"math.AP"}
{"created":"2024-06-04 09:09:54","title":"Measuring the Dispersion of Discrete Distributions","abstract":"Measuring dispersion is among the most fundamental and ubiquitous concepts in statistics, both in applied and theoretical contexts. In order to ensure that dispersion measures like the standard deviation indeed capture the dispersion of any given distribution, they are by definition required to preserve a stochastic order of dispersion. The most basic order that functions as a foundation underneath the concept of dispersion measures is the so-called dispersive order. However, that order is incompatible with almost all discrete distributions, including all lattice distributions and most empirical distributions. Thus, there is no guarantee that popular measures properly capture the dispersion of these distributions.   In this paper, discrete adaptations of the dispersive order are derived and analyzed. Their derivation is directly informed by key properties of the dispersive order in order to obtain a foundation for the measurement of discrete dispersion that is as similar as possible to the continuous setting. Two slightly different orders are obtained that both have numerous properties that the original dispersive order also has. Their behaviour on well-known families of lattice distribution is generally as expected if the parameter differences are large enough. Most popular dispersion measures preserve both discrete dispersive orders, which rigorously ensures that they are also meaningful in discrete settings. However, the interquantile range preserves neither discrete order, yielding that it should not be used to measure the dispersion of discrete distributions.","sentences":["Measuring dispersion is among the most fundamental and ubiquitous concepts in statistics, both in applied and theoretical contexts.","In order to ensure that dispersion measures like the standard deviation indeed capture the dispersion of any given distribution, they are by definition required to preserve a stochastic order of dispersion.","The most basic order that functions as a foundation underneath the concept of dispersion measures is the so-called dispersive order.","However, that order is incompatible with almost all discrete distributions, including all lattice distributions and most empirical distributions.","Thus, there is no guarantee that popular measures properly capture the dispersion of these distributions.   ","In this paper, discrete adaptations of the dispersive order are derived and analyzed.","Their derivation is directly informed by key properties of the dispersive order in order to obtain a foundation for the measurement of discrete dispersion that is as similar as possible to the continuous setting.","Two slightly different orders are obtained that both have numerous properties that the original dispersive order also has.","Their behaviour on well-known families of lattice distribution is generally as expected if the parameter differences are large enough.","Most popular dispersion measures preserve both discrete dispersive orders, which rigorously ensures that they are also meaningful in discrete settings.","However, the interquantile range preserves neither discrete order, yielding that it should not be used to measure the dispersion of discrete distributions."],"url":"http://arxiv.org/abs/2406.02124v1","category":"stat.ME"}
{"created":"2024-06-04 09:02:22","title":"Diver: Large Language Model Decoding with Span-Level Mutual Information Verification","abstract":"Large language models (LLMs) have shown impressive capabilities in adapting to various tasks when provided with task-specific instructions. However, LLMs using standard decoding strategies often struggle with deviations from the inputs. Intuitively, compliant LLM outputs should reflect the information present in the input, which can be measured by point-wise mutual information (PMI) scores. Therefore, we propose Diver, a novel approach that enhances LLM Decoding through span-level PMI verification. During inference, Diver first identifies divergence steps that may lead to multiple candidate spans. Subsequently, it calculates the PMI scores by assessing the log-likelihood gains of the input if the candidate spans are generated. Finally, the optimal span is selected based on the PMI re-ranked output distributions. We evaluate our method across various downstream tasks, and empirical results demonstrate that Diver significantly outperforms existing decoding methods in both performance and versatility.","sentences":["Large language models (LLMs) have shown impressive capabilities in adapting to various tasks when provided with task-specific instructions.","However, LLMs using standard decoding strategies often struggle with deviations from the inputs.","Intuitively, compliant LLM outputs should reflect the information present in the input, which can be measured by point-wise mutual information (PMI) scores.","Therefore, we propose Diver, a novel approach that enhances LLM Decoding through span-level PMI verification.","During inference, Diver first identifies divergence steps that may lead to multiple candidate spans.","Subsequently, it calculates the PMI scores by assessing the log-likelihood gains of the input if the candidate spans are generated.","Finally, the optimal span is selected based on the PMI re-ranked output distributions.","We evaluate our method across various downstream tasks, and empirical results demonstrate that Diver significantly outperforms existing decoding methods in both performance and versatility."],"url":"http://arxiv.org/abs/2406.02120v1","category":"cs.CL"}
{"created":"2024-06-04 07:30:27","title":"Causal Effect Identification in LiNGAM Models with Latent Confounders","abstract":"We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables. We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown. In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables. Moreover, we propose efficient algorithms to certify the graphical conditions. Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph. Experimental results show the effectiveness of the proposed method in estimating the causal effects.","sentences":["We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables.","We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown.","In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables.","Moreover, we propose efficient algorithms to certify the graphical conditions.","Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph.","Experimental results show the effectiveness of the proposed method in estimating the causal effects."],"url":"http://arxiv.org/abs/2406.02049v1","category":"stat.ML"}
{"created":"2024-06-04 06:53:32","title":"Understanding Auditory Evoked Brain Signal via Physics-informed Embedding Network with Multi-Task Transformer","abstract":"In the fields of brain-computer interaction and cognitive neuroscience, effective decoding of auditory signals from task-based functional magnetic resonance imaging (fMRI) is key to understanding how the brain processes complex auditory information. Although existing methods have enhanced decoding capabilities, limitations remain in information utilization and model representation. To overcome these challenges, we propose an innovative multi-task learning model, Physics-informed Embedding Network with Multi-Task Transformer (PEMT-Net), which enhances decoding performance through physics-informed embedding and deep learning techniques. PEMT-Net consists of two principal components: feature augmentation and classification. For feature augmentation, we propose a novel approach by creating neural embedding graphs via node embedding, utilizing random walks to simulate the physical diffusion of neural information. This method captures both local and non-local information overflow and proposes a position encoding based on relative physical coordinates. In the classification segment, we propose adaptive embedding fusion to maximally capture linear and non-linear characteristics. Furthermore, we propose an innovative parameter-sharing mechanism to optimize the retention and learning of extracted features. Experiments on a specific dataset demonstrate PEMT-Net's significant performance in multi-task auditory signal decoding, surpassing existing methods and offering new insights into the brain's mechanisms for processing complex auditory information.","sentences":["In the fields of brain-computer interaction and cognitive neuroscience, effective decoding of auditory signals from task-based functional magnetic resonance imaging (fMRI) is key to understanding how the brain processes complex auditory information.","Although existing methods have enhanced decoding capabilities, limitations remain in information utilization and model representation.","To overcome these challenges, we propose an innovative multi-task learning model, Physics-informed Embedding Network with Multi-Task Transformer (PEMT-Net), which enhances decoding performance through physics-informed embedding and deep learning techniques.","PEMT-Net consists of two principal components: feature augmentation and classification.","For feature augmentation, we propose a novel approach by creating neural embedding graphs via node embedding, utilizing random walks to simulate the physical diffusion of neural information.","This method captures both local and non-local information overflow and proposes a position encoding based on relative physical coordinates.","In the classification segment, we propose adaptive embedding fusion to maximally capture linear and non-linear characteristics.","Furthermore, we propose an innovative parameter-sharing mechanism to optimize the retention and learning of extracted features.","Experiments on a specific dataset demonstrate PEMT-Net's significant performance in multi-task auditory signal decoding, surpassing existing methods and offering new insights into the brain's mechanisms for processing complex auditory information."],"url":"http://arxiv.org/abs/2406.02014v1","category":"q-bio.NC"}
{"created":"2024-06-04 06:34:33","title":"Efficiently Train ASR Models that Memorize Less and Perform Better with Per-core Clipping","abstract":"Gradient clipping plays a vital role in training large-scale automatic speech recognition (ASR) models. It is typically applied to minibatch gradients to prevent gradient explosion, and to the individual sample gradients to mitigate unintended memorization. This work systematically investigates the impact of a specific granularity of gradient clipping, namely per-core clip-ping (PCC), across training a wide range of ASR models. We empirically demonstrate that PCC can effectively mitigate unintended memorization in ASR models. Surprisingly, we find that PCC positively influences ASR performance metrics, leading to improved convergence rates and reduced word error rates. To avoid tuning the additional hyperparameter introduced by PCC, we further propose a novel variant, adaptive per-core clipping (APCC), for streamlined optimization. Our findings highlight the multifaceted benefits of PCC as a strategy for robust, privacy-forward ASR model training.","sentences":["Gradient clipping plays a vital role in training large-scale automatic speech recognition (ASR) models.","It is typically applied to minibatch gradients to prevent gradient explosion, and to the individual sample gradients to mitigate unintended memorization.","This work systematically investigates the impact of a specific granularity of gradient clipping, namely per-core clip-ping (PCC), across training a wide range of ASR models.","We empirically demonstrate that PCC can effectively mitigate unintended memorization in ASR models.","Surprisingly, we find that PCC positively influences ASR performance metrics, leading to improved convergence rates and reduced word error rates.","To avoid tuning the additional hyperparameter introduced by PCC, we further propose a novel variant, adaptive per-core clipping (APCC), for streamlined optimization.","Our findings highlight the multifaceted benefits of PCC as a strategy for robust, privacy-forward ASR model training."],"url":"http://arxiv.org/abs/2406.02004v1","category":"cs.CR"}
{"created":"2024-06-04 04:22:39","title":"On-Demand Routing in LEO Mega-Constellations with Dynamic Laser Inter-Satellite Links","abstract":"Low Earth orbit (LEO) satellite mega constellations are beginning to include laser inter-satellite links (LISLs) to extend the Internet to the most remote locations on Earth. Since the process of establishing these links incurs a setup delay on the order of seconds, a static network topology is generally established well in advance, which is then used for the routing calculations. However, this involves keeping links active even when they are not being used to forward traffic, leading to poor energy efficiency. Motivated by technological advances that are gradually decreasing the LISL setup delays, we foresee scenarios where it will be possible to compute routes and establish dynamic LISLs on demand. This will require considering setup delays as penalties that will affect the end-to-end latency. In this paper, we present a nonlinear optimization model that considers these penalties in the cost function and propose three heuristic algorithms that solve the problem in a tractable way. The algorithms establish different trade-offs in terms of performance and computational complexity. We extensively analyze metrics including average latency, route change rate, outage probability, and jitter in Starlink's Phase I version 2 constellation. The results show the benefit of adaptive routing schemes according to the link setup delay. In particular, more complex schemes can decrease the average end-to-end latency in exchange for an increase in execution time. On the other hand, depending on the maximum tolerated latency, it is possible to use less computationally complex schemes which will be more scalable for the satellite mega constellations of the future.","sentences":["Low Earth orbit (LEO) satellite mega constellations are beginning to include laser inter-satellite links (LISLs) to extend the Internet to the most remote locations on Earth.","Since the process of establishing these links incurs a setup delay on the order of seconds, a static network topology is generally established well in advance, which is then used for the routing calculations.","However, this involves keeping links active even when they are not being used to forward traffic, leading to poor energy efficiency.","Motivated by technological advances that are gradually decreasing the LISL setup delays, we foresee scenarios where it will be possible to compute routes and establish dynamic LISLs on demand.","This will require considering setup delays as penalties that will affect the end-to-end latency.","In this paper, we present a nonlinear optimization model that considers these penalties in the cost function and propose three heuristic algorithms that solve the problem in a tractable way.","The algorithms establish different trade-offs in terms of performance and computational complexity.","We extensively analyze metrics including average latency, route change rate, outage probability, and jitter in Starlink's Phase I version 2 constellation.","The results show the benefit of adaptive routing schemes according to the link setup delay.","In particular, more complex schemes can decrease the average end-to-end latency in exchange for an increase in execution time.","On the other hand, depending on the maximum tolerated latency, it is possible to use less computationally complex schemes which will be more scalable for the satellite mega constellations of the future."],"url":"http://arxiv.org/abs/2406.01953v1","category":"cs.NI"}
{"created":"2024-06-04 03:49:10","title":"SDS++: Online Situation-Aware Drivable Space Estimation for Automated Driving","abstract":"Autonomous Vehicles (AVs) need an accurate and up-to-date representation of the environment for safe navigation. Traditional methods, which often rely on detailed environmental representations constructed offline, struggle in dynamically changing environments or when dealing with outdated maps. Consequently, there is a pressing need for real-time solutions that can integrate diverse data sources and adapt to the current situation. An existing framework that addresses these challenges is SDS (situation-aware drivable space). However, SDS faces several limitations, including its use of a non-standard output representation, its choice of encoding objects as points, restricting representation of more complex geometries like road lanes, and the fact that its methodology has been validated only with simulated or heavily post-processed data. This work builds upon SDS and introduces SDS++, designed to overcome SDS's shortcomings while preserving its benefits. SDS++ has been rigorously validated not only in simulations but also with unrefined vehicle data, and it is integrated with a model predictive control (MPC)-based planner to verify its advantages for the planning task. The results demonstrate that SDS++ significantly enhances trajectory planning capabilities, providing increased robustness against localization noise, and enabling the planning of trajectories that adapt to the current driving context.","sentences":["Autonomous Vehicles (AVs) need an accurate and up-to-date representation of the environment for safe navigation.","Traditional methods, which often rely on detailed environmental representations constructed offline, struggle in dynamically changing environments or when dealing with outdated maps.","Consequently, there is a pressing need for real-time solutions that can integrate diverse data sources and adapt to the current situation.","An existing framework that addresses these challenges is SDS (situation-aware drivable space).","However, SDS faces several limitations, including its use of a non-standard output representation, its choice of encoding objects as points, restricting representation of more complex geometries like road lanes, and the fact that its methodology has been validated only with simulated or heavily post-processed data.","This work builds upon SDS and introduces SDS++, designed to overcome SDS's shortcomings while preserving its benefits.","SDS++ has been rigorously validated not only in simulations but also with unrefined vehicle data, and it is integrated with a model predictive control (MPC)-based planner to verify its advantages for the planning task.","The results demonstrate that SDS++ significantly enhances trajectory planning capabilities, providing increased robustness against localization noise, and enabling the planning of trajectories that adapt to the current driving context."],"url":"http://arxiv.org/abs/2406.01941v1","category":"cs.RO"}
{"created":"2024-06-04 03:00:55","title":"OTTAWA: Optimal TransporT Adaptive Word Aligner for Hallucination and Omission Translation Errors Detection","abstract":"Recently, there has been considerable attention on detecting hallucinations and omissions in Machine Translation (MT) systems. The two dominant approaches to tackle this task involve analyzing the MT system's internal states or relying on the output of external tools, such as sentence similarity or MT quality estimators. In this work, we introduce OTTAWA, a novel Optimal Transport (OT)-based word aligner specifically designed to enhance the detection of hallucinations and omissions in MT systems. Our approach explicitly models the missing alignments by introducing a \"null\" vector, for which we propose a novel one-side constrained OT setting to allow an adaptive null alignment. Our approach yields competitive results compared to state-of-the-art methods across 18 language pairs on the HalOmi benchmark. In addition, it shows promising features, such as the ability to distinguish between both error types and perform word-level detection without accessing the MT system's internal states.","sentences":["Recently, there has been considerable attention on detecting hallucinations and omissions in Machine Translation (MT) systems.","The two dominant approaches to tackle this task involve analyzing the MT system's internal states or relying on the output of external tools, such as sentence similarity or MT quality estimators.","In this work, we introduce OTTAWA, a novel Optimal Transport (OT)-based word aligner specifically designed to enhance the detection of hallucinations and omissions in MT systems.","Our approach explicitly models the missing alignments by introducing a \"null\" vector, for which we propose a novel one-side constrained OT setting to allow an adaptive null alignment.","Our approach yields competitive results compared to state-of-the-art methods across 18 language pairs on the HalOmi benchmark.","In addition, it shows promising features, such as the ability to distinguish between both error types and perform word-level detection without accessing the MT system's internal states."],"url":"http://arxiv.org/abs/2406.01919v1","category":"cs.CL"}
{"created":"2024-06-04 02:52:26","title":"Enhancing Human-Robot Collaborative Assembly in Manufacturing Systems Using Large Language Models","abstract":"The development of human-robot collaboration has the ability to improve manufacturing system performance by leveraging the unique strengths of both humans and robots. On the shop floor, human operators contribute with their adaptability and flexibility in dynamic situations, while robots provide precision and the ability to perform repetitive tasks. However, the communication gap between human operators and robots limits the collaboration and coordination of human-robot teams in manufacturing systems. Our research presents a human-robot collaborative assembly framework that utilizes a large language model for enhancing communication in manufacturing environments. The framework facilitates human-robot communication by integrating voice commands through natural language for task management. A case study for an assembly task demonstrates the framework's ability to process natural language inputs and address real-time assembly challenges, emphasizing adaptability to language variation and efficiency in error resolution. The results suggest that large language models have the potential to improve human-robot interaction for collaborative manufacturing assembly applications.","sentences":["The development of human-robot collaboration has the ability to improve manufacturing system performance by leveraging the unique strengths of both humans and robots.","On the shop floor, human operators contribute with their adaptability and flexibility in dynamic situations, while robots provide precision and the ability to perform repetitive tasks.","However, the communication gap between human operators and robots limits the collaboration and coordination of human-robot teams in manufacturing systems.","Our research presents a human-robot collaborative assembly framework that utilizes a large language model for enhancing communication in manufacturing environments.","The framework facilitates human-robot communication by integrating voice commands through natural language for task management.","A case study for an assembly task demonstrates the framework's ability to process natural language inputs and address real-time assembly challenges, emphasizing adaptability to language variation and efficiency in error resolution.","The results suggest that large language models have the potential to improve human-robot interaction for collaborative manufacturing assembly applications."],"url":"http://arxiv.org/abs/2406.01915v1","category":"cs.RO"}
{"created":"2024-06-04 02:04:09","title":"Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models","abstract":"Models for natural language and images benefit from data scaling behavior: the more data fed into the model, the better they perform. This 'better with more' phenomenon enables the effectiveness of large-scale pre-training on vast amounts of data. However, current graph pre-training methods struggle to scale up data due to heterogeneity across graphs. To achieve effective data scaling, we aim to develop a general model that is able to capture diverse data patterns of graphs and can be utilized to adaptively help the downstream tasks. To this end, we propose UniAug, a universal graph structure augmentor built on a diffusion model. We first pre-train a discrete diffusion model on thousands of graphs across domains to learn the graph structural patterns. In the downstream phase, we provide adaptive enhancement by conducting graph structure augmentation with the help of the pre-trained diffusion model via guided generation. By leveraging the pre-trained diffusion model for structure augmentation, we consistently achieve performance improvements across various downstream tasks in a plug-and-play manner. To the best of our knowledge, this study represents the first demonstration of a data-scaling graph structure augmentor on graphs across domains.","sentences":["Models for natural language and images benefit from data scaling behavior: the more data fed into the model, the better they perform.","This 'better with more' phenomenon enables the effectiveness of large-scale pre-training on vast amounts of data.","However, current graph pre-training methods struggle to scale up data due to heterogeneity across graphs.","To achieve effective data scaling, we aim to develop a general model that is able to capture diverse data patterns of graphs and can be utilized to adaptively help the downstream tasks.","To this end, we propose UniAug, a universal graph structure augmentor built on a diffusion model.","We first pre-train a discrete diffusion model on thousands of graphs across domains to learn the graph structural patterns.","In the downstream phase, we provide adaptive enhancement by conducting graph structure augmentation with the help of the pre-trained diffusion model via guided generation.","By leveraging the pre-trained diffusion model for structure augmentation, we consistently achieve performance improvements across various downstream tasks in a plug-and-play manner.","To the best of our knowledge, this study represents the first demonstration of a data-scaling graph structure augmentor on graphs across domains."],"url":"http://arxiv.org/abs/2406.01899v1","category":"cs.LG"}
{"created":"2024-06-04 01:29:55","title":"Activity patterns in ring networks of quadratic integrate-and-fire neurons with synaptic and gap junction coupling","abstract":"We consider a ring network of quadratic integrate-and-fire neurons with nonlocal synaptic and gap junction coupling. The corresponding neural field model supports solutions such as standing and travelling waves, and also lurching waves. We show that many of these solutions satisfy self-consistency equations which can be used to follow them as parameters are varied. We perform numerical bifurcation analysis of the neural field model, concentrating on the effects of varying gap junction coupling strength. Our methods are generally applicable to a wide variety of networks of quadratic integrate-and-fire neurons.","sentences":["We consider a ring network of quadratic integrate-and-fire neurons with nonlocal synaptic and gap junction coupling.","The corresponding neural field model supports solutions such as standing and travelling waves, and also lurching waves.","We show that many of these solutions satisfy self-consistency equations which can be used to follow them as parameters are varied.","We perform numerical bifurcation analysis of the neural field model, concentrating on the effects of varying gap junction coupling strength.","Our methods are generally applicable to a wide variety of networks of quadratic integrate-and-fire neurons."],"url":"http://arxiv.org/abs/2406.01881v1","category":"nlin.AO"}
{"created":"2024-06-03 23:54:48","title":"Non-uniformity is All You Need: Efficient and Timely Encrypted Traffic Classification With ECHO","abstract":"With 95% of Internet traffic now encrypted, an effective approach to classifying this traffic is crucial for network security and management. This paper introduces ECHO -- a novel optimization process for ML/DL-based encrypted traffic classification. ECHO targets both classification time and memory utilization and incorporates two innovative techniques.   The first component, HO (Hyperparameter Optimization of binnings), aims at creating efficient traffic representations. While previous research often uses representations that map packet sizes and packet arrival times to fixed-sized bins, we show that non-uniform binnings are significantly more efficient. These non-uniform binnings are derived by employing a hyperparameter optimization algorithm in the training stage. HO significantly improves accuracy given a required representation size, or, equivalently, achieves comparable accuracy using smaller representations.   Then, we introduce EC (Early Classification of traffic), which enables faster classification using a cascade of classifiers adapted for different exit times, where classification is based on the level of confidence. EC reduces the average classification latency by up to 90\\%. Remarkably, this method not only maintains classification accuracy but also, in certain cases, improves it.   Using three publicly available datasets, we demonstrate that the combined method, Early Classification with Hyperparameter Optimization (ECHO), leads to a significant improvement in classification efficiency.","sentences":["With 95% of Internet traffic now encrypted, an effective approach to classifying this traffic is crucial for network security and management.","This paper introduces ECHO -- a novel optimization process for ML/DL-based encrypted traffic classification.","ECHO targets both classification time and memory utilization and incorporates two innovative techniques.   ","The first component, HO (Hyperparameter Optimization of binnings), aims at creating efficient traffic representations.","While previous research often uses representations that map packet sizes and packet arrival times to fixed-sized bins, we show that non-uniform binnings are significantly more efficient.","These non-uniform binnings are derived by employing a hyperparameter optimization algorithm in the training stage.","HO significantly improves accuracy given a required representation size, or, equivalently, achieves comparable accuracy using smaller representations.   ","Then, we introduce EC (Early Classification of traffic), which enables faster classification using a cascade of classifiers adapted for different exit times, where classification is based on the level of confidence.","EC reduces the average classification latency by up to 90\\%.","Remarkably, this method not only maintains classification accuracy but also, in certain cases, improves it.   ","Using three publicly available datasets, we demonstrate that the combined method, Early Classification with Hyperparameter Optimization (ECHO), leads to a significant improvement in classification efficiency."],"url":"http://arxiv.org/abs/2406.01852v1","category":"cs.NI"}
{"created":"2024-06-03 21:59:21","title":"In-Context Learning of Physical Properties: Few-Shot Adaptation to Out-of-Distribution Molecular Graphs","abstract":"Large language models manifest the ability of few-shot adaptation to a sequence of provided examples. This behavior, known as in-context learning, allows for performing nontrivial machine learning tasks during inference only. In this work, we address the question: can we leverage in-context learning to predict out-of-distribution materials properties? However, this would not be possible for structure property prediction tasks unless an effective method is found to pass atomic-level geometric features to the transformer model. To address this problem, we employ a compound model in which GPT-2 acts on the output of geometry-aware graph neural networks to adapt in-context information. To demonstrate our model's capabilities, we partition the QM9 dataset into sequences of molecules that share a common substructure and use them for in-context learning. This approach significantly improves the performance of the model on out-of-distribution examples, surpassing the one of general graph neural network models.","sentences":["Large language models manifest the ability of few-shot adaptation to a sequence of provided examples.","This behavior, known as in-context learning, allows for performing nontrivial machine learning tasks during inference only.","In this work, we address the question: can we leverage in-context learning to predict out-of-distribution materials properties?","However, this would not be possible for structure property prediction tasks unless an effective method is found to pass atomic-level geometric features to the transformer model.","To address this problem, we employ a compound model in which GPT-2 acts on the output of geometry-aware graph neural networks to adapt in-context information.","To demonstrate our model's capabilities, we partition the QM9 dataset into sequences of molecules that share a common substructure and use them for in-context learning.","This approach significantly improves the performance of the model on out-of-distribution examples, surpassing the one of general graph neural network models."],"url":"http://arxiv.org/abs/2406.01808v1","category":"cs.LG"}
{"created":"2024-06-03 21:50:40","title":"Leader-Follower Density Control of Spatial Dynamics in Large-Scale Multi-Agent Systems","abstract":"We address the problem of controlling the density of a large ensemble of follower agents by acting on group of leader agents that interact with them. We formulate the problem as a system of coupled partial integro-differential equations describing for the dynamics of the leaders' and followers' densities. We define feasibility conditions and propose two control architectures for exponential global stability. The first architecture is a feed-forward scheme for the followers. It adjusts the leaders' density via a feedback loop, which leverages information about leaders and a fixed reference density, to direct followers towards a target distribution. The second, dual feedback strategy employs a reference-governor to dynamically adapt the leaders' reference density based on measurements on both leaders and followers. Initially analyzed in one dimension, our methods are expanded to multi-dimensional applications. Numerical validations and an application in continuification-based control of leader-follower multiagent systems confirm the effectiveness of our approaches.","sentences":["We address the problem of controlling the density of a large ensemble of follower agents by acting on group of leader agents that interact with them.","We formulate the problem as a system of coupled partial integro-differential equations describing for the dynamics of the leaders' and followers' densities.","We define feasibility conditions and propose two control architectures for exponential global stability.","The first architecture is a feed-forward scheme for the followers.","It adjusts the leaders' density via a feedback loop, which leverages information about leaders and a fixed reference density, to direct followers towards a target distribution.","The second, dual feedback strategy employs a reference-governor to dynamically adapt the leaders' reference density based on measurements on both leaders and followers.","Initially analyzed in one dimension, our methods are expanded to multi-dimensional applications.","Numerical validations and an application in continuification-based control of leader-follower multiagent systems confirm the effectiveness of our approaches."],"url":"http://arxiv.org/abs/2406.01804v1","category":"eess.SY"}
{"created":"2024-06-03 20:37:27","title":"OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models","abstract":"The advent of large language models (LLMs) has revolutionized natural language processing, enabling unprecedented capabilities in understanding and generating human-like text. However, the computational cost and convergence times associated with fine-tuning these models remain significant challenges. Low-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these issues by introducing efficient fine-tuning techniques with a reduced number of trainable parameters. In this paper, we present OLoRA, an enhancement to the LoRA method that leverages orthonormal matrix initialization through QR decomposition. OLoRA significantly accelerates the convergence of LLM training while preserving the efficiency benefits of LoRA, such as the number of trainable parameters and GPU memory footprint. Our empirical evaluations demonstrate that OLoRA not only converges faster but also exhibits improved performance compared to standard LoRA across a variety of language modeling tasks. This advancement opens new avenues for more efficient and accessible fine-tuning of LLMs, potentially enabling broader adoption and innovation in natural language applications.","sentences":["The advent of large language models (LLMs) has revolutionized natural language processing, enabling unprecedented capabilities in understanding and generating human-like text.","However, the computational cost and convergence times associated with fine-tuning these models remain significant challenges.","Low-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these issues by introducing efficient fine-tuning techniques with a reduced number of trainable parameters.","In this paper, we present OLoRA, an enhancement to the LoRA method that leverages orthonormal matrix initialization through QR decomposition.","OLoRA significantly accelerates the convergence of LLM training while preserving the efficiency benefits of LoRA, such as the number of trainable parameters and GPU memory footprint.","Our empirical evaluations demonstrate that OLoRA not only converges faster but also exhibits improved performance compared to standard LoRA across a variety of language modeling tasks.","This advancement opens new avenues for more efficient and accessible fine-tuning of LLMs, potentially enabling broader adoption and innovation in natural language applications."],"url":"http://arxiv.org/abs/2406.01775v1","category":"cs.CL"}
{"created":"2024-06-03 19:28:01","title":"3D transcranial Dynamic Ultrasound Localization Microscopy in the mouse brain using a Row-Column Array","abstract":"The role of brain hemodynamics in neurodegenerative diseases cannot be fully assessed using existing imaging technologies. Recently, 2D Dynamic Ultrasound Localization Microscopy (DULM) has allowed for the quantitative mapping of the pulsatile flow at sub-wavelength resolution. However, to obtain accurate velocity estimates, 3D imaging is more adapted, especially for complex vascularized organs like the brain. 3D+t DULM is achievable using matrix array probes, but suffers from limitations in terms of cost, device complexity associated with the high channel count, and operating frequencies. Alternatively, Row Column Arrays (RCA) can reduce the number of elements while maintaining a large field of view and high frame rate. Herein, we demonstrate the feasibility of performing 3D+t blood flow measurements in the mouse brain using an RCA and a DULM sequence with a high spatiotemporal resolution. Transcranial images of anesthetized mice (n=7) were acquired at a volume rate of 750 Hz using 42 tilted plane waves. After microbubbles localization and tracking, super-resolved dynamic density and velocity maps of the 3D brain vascular network were obtained. Cortical vessels were segmented and pulsatility in the arteries was significantly higher than in veins for all mice, in accordance with the literature. Our results demonstrate the feasibility and reproducibility of achieving high spatiotemporal resolution volumes of the mouse brain vasculature with DULM using a RCA.","sentences":["The role of brain hemodynamics in neurodegenerative diseases cannot be fully assessed using existing imaging technologies.","Recently, 2D Dynamic Ultrasound Localization Microscopy (DULM) has allowed for the quantitative mapping of the pulsatile flow at sub-wavelength resolution.","However, to obtain accurate velocity estimates, 3D imaging is more adapted, especially for complex vascularized organs like the brain.","3D+t DULM is achievable using matrix array probes, but suffers from limitations in terms of cost, device complexity associated with the high channel count, and operating frequencies.","Alternatively, Row Column Arrays (RCA) can reduce the number of elements while maintaining a large field of view and high frame rate.","Herein, we demonstrate the feasibility of performing 3D+t blood flow measurements in the mouse brain using an RCA and a DULM sequence with a high spatiotemporal resolution.","Transcranial images of anesthetized mice (n=7) were acquired at a volume rate of 750 Hz using 42 tilted plane waves.","After microbubbles localization and tracking, super-resolved dynamic density and velocity maps of the 3D brain vascular network were obtained.","Cortical vessels were segmented and pulsatility in the arteries was significantly higher than in veins for all mice, in accordance with the literature.","Our results demonstrate the feasibility and reproducibility of achieving high spatiotemporal resolution volumes of the mouse brain vasculature with DULM using a RCA."],"url":"http://arxiv.org/abs/2406.01746v1","category":"physics.med-ph"}
{"created":"2024-06-03 18:45:34","title":"The density conjecture for activated random walk","abstract":"In the late 1990s, Dickman, Mu\\~noz, Vespignani, and Zapperi explained the self-organized criticality observed by Bak, Tang, and Wiesenfeld as an external force pushing a hidden parameter toward the critical value of a traditional absorbing-state phase transition. As evidence, they observed empirically that for various sandpile models the particle density in a finite box under driven-dissipative dynamics converges to the critical density of an infinite-volume version of the model. We give the first proof of this well-known density conjecture in any setting by establishing it for activated random walk in one dimension. We prove that two other natural versions of the model have the same critical value, further establishing activated random walk as a universal model of self-organized criticality.","sentences":["In the late 1990s, Dickman, Mu\\~noz, Vespignani, and Zapperi explained the self-organized criticality observed by Bak, Tang, and Wiesenfeld as an external force pushing a hidden parameter toward the critical value of a traditional absorbing-state phase transition.","As evidence, they observed empirically that for various sandpile models the particle density in a finite box under driven-dissipative dynamics converges to the critical density of an infinite-volume version of the model.","We give the first proof of this well-known density conjecture in any setting by establishing it for activated random walk in one dimension.","We prove that two other natural versions of the model have the same critical value, further establishing activated random walk as a universal model of self-organized criticality."],"url":"http://arxiv.org/abs/2406.01731v1","category":"math.PR"}
{"created":"2024-06-03 18:01:43","title":"Snowflake: A Distributed Streaming Decoder","abstract":"We design Snowflake, a quantum error correction decoder that runs in a streaming fashion and is capable of a simple, local implementation. In doing so we propose a new method for general stream decoding that eliminates the processing overhead due to window overlap in existing windowing methods. As a first study, we test our local implementation of Snowflake on the surface code under circuit-level noise. It recovers roughly 2/3 the accuracy threshold of the Union-Find decoder adapted with a windowing method, with a better mean runtime scaling: subquadratic as opposed to cubic in code distance $d$. We discuss how Snowflake may be implemented on a 2D chip and decode not just quantum memory but lattice surgery-based computation.","sentences":["We design Snowflake, a quantum error correction decoder that runs in a streaming fashion and is capable of a simple, local implementation.","In doing so we propose a new method for general stream decoding that eliminates the processing overhead due to window overlap in existing windowing methods.","As a first study, we test our local implementation of Snowflake on the surface code under circuit-level noise.","It recovers roughly 2/3 the accuracy threshold of the Union-Find decoder adapted with a windowing method, with a better mean runtime scaling: subquadratic as opposed to cubic in code distance $d$.","We discuss how Snowflake may be implemented on a 2D chip and decode not just quantum memory but lattice surgery-based computation."],"url":"http://arxiv.org/abs/2406.01701v1","category":"quant-ph"}
{"created":"2024-06-03 18:00:01","title":"Color Glass Condensate meets High Twist Expansion","abstract":"We establish the correspondence between two well-known frameworks for QCD multiple scattering in nuclear media: the Color Glass Condensate (CGC) and the High-Twist (HT) expansion formalism. We argue that a consistent matching between both frameworks, in their common domain of validity, is achieved by incorporating the sub-eikonal longitudinal momentum phase in the CGC formalism, which mediates the transition between coherent and incoherent scattering. We perform a detailed calculation and analysis of direct photon production in proton-nucleus scattering as a concrete example to establish the matching between HT and CGC up to twist-4, including initial- and final-state interactions, as well as their interferences. The techniques developed in this work can be adapted to other processes in electron-nucleus and proton-nucleus collisions, and they provide a potential avenue for a unified picture of dilute-dense dynamics in nuclear media.","sentences":["We establish the correspondence between two well-known frameworks for QCD multiple scattering in nuclear media: the Color Glass Condensate (CGC) and the High-Twist (HT) expansion formalism.","We argue that a consistent matching between both frameworks, in their common domain of validity, is achieved by incorporating the sub-eikonal longitudinal momentum phase in the CGC formalism, which mediates the transition between coherent and incoherent scattering.","We perform a detailed calculation and analysis of direct photon production in proton-nucleus scattering as a concrete example to establish the matching between HT and CGC up to twist-4, including initial- and final-state interactions, as well as their interferences.","The techniques developed in this work can be adapted to other processes in electron-nucleus and proton-nucleus collisions, and they provide a potential avenue for a unified picture of dilute-dense dynamics in nuclear media."],"url":"http://arxiv.org/abs/2406.01684v1","category":"hep-ph"}
{"created":"2024-06-03 17:59:53","title":"DiffUHaul: A Training-Free Method for Object Dragging in Images","abstract":"Text-to-image diffusion models have proven effective for solving many image editing tasks. However, the seemingly straightforward task of seamlessly relocating objects within a scene remains surprisingly challenging. Existing methods addressing this problem often struggle to function reliably in real-world scenarios due to lacking spatial reasoning. In this work, we propose a training-free method, dubbed DiffUHaul, that harnesses the spatial understanding of a localized text-to-image model, for the object dragging task. Blindly manipulating layout inputs of the localized model tends to cause low editing performance due to the intrinsic entanglement of object representation in the model. To this end, we first apply attention masking in each denoising step to make the generation more disentangled across different objects and adopt the self-attention sharing mechanism to preserve the high-level object appearance. Furthermore, we propose a new diffusion anchoring technique: in the early denoising steps, we interpolate the attention features between source and target images to smoothly fuse new layouts with the original appearance; in the later denoising steps, we pass the localized features from the source images to the interpolated images to retain fine-grained object details. To adapt DiffUHaul to real-image editing, we apply a DDPM self-attention bucketing that can better reconstruct real images with the localized model. Finally, we introduce an automated evaluation pipeline for this task and showcase the efficacy of our method. Our results are reinforced through a user preference study.","sentences":["Text-to-image diffusion models have proven effective for solving many image editing tasks.","However, the seemingly straightforward task of seamlessly relocating objects within a scene remains surprisingly challenging.","Existing methods addressing this problem often struggle to function reliably in real-world scenarios due to lacking spatial reasoning.","In this work, we propose a training-free method, dubbed DiffUHaul, that harnesses the spatial understanding of a localized text-to-image model, for the object dragging task.","Blindly manipulating layout inputs of the localized model tends to cause low editing performance due to the intrinsic entanglement of object representation in the model.","To this end, we first apply attention masking in each denoising step to make the generation more disentangled across different objects and adopt the self-attention sharing mechanism to preserve the high-level object appearance.","Furthermore, we propose a new diffusion anchoring technique: in the early denoising steps, we interpolate the attention features between source and target images to smoothly fuse new layouts with the original appearance; in the later denoising steps, we pass the localized features from the source images to the interpolated images to retain fine-grained object details.","To adapt DiffUHaul to real-image editing, we apply a DDPM self-attention bucketing that can better reconstruct real images with the localized model.","Finally, we introduce an automated evaluation pipeline for this task and showcase the efficacy of our method.","Our results are reinforced through a user preference study."],"url":"http://arxiv.org/abs/2406.01594v1","category":"cs.CV"}
{"created":"2024-06-03 17:59:51","title":"Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting","abstract":"3D reconstruction and simulation, while interrelated, have distinct objectives: reconstruction demands a flexible 3D representation adaptable to diverse scenes, whereas simulation requires a structured representation to model motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to resolve such a dilemma. MaGS constrains 3D Gaussians to hover on the mesh surface, creating a mutual-adsorbed mesh-Gaussian 3D representation that combines the rendering flexibility of 3D Gaussians with the spatial coherence of meshes. Leveraging this representation, we introduce a learnable Relative Deformation Field (RDF) to model the relative displacement between the mesh and 3D Gaussians, extending traditional mesh-driven deformation paradigms that only rely on ARAP prior, thus capturing the motion of each 3D Gaussian more precisely. By joint optimizing meshes, 3D Gaussians, and RDF, MaGS achieves both high rendering accuracy and realistic deformation. Extensive experiments on the D-NeRF and NeRF-DS datasets demonstrate that MaGS can generate competitive results in both reconstruction and simulation.","sentences":["3D reconstruction and simulation, while interrelated, have distinct objectives: reconstruction demands a flexible 3D representation adaptable to diverse scenes, whereas simulation requires a structured representation to model motion principles effectively.","This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to resolve such a dilemma.","MaGS constrains 3D Gaussians to hover on the mesh surface, creating a mutual-adsorbed mesh-Gaussian 3D representation that combines the rendering flexibility of 3D Gaussians with the spatial coherence of meshes.","Leveraging this representation, we introduce a learnable Relative Deformation Field (RDF) to model the relative displacement between the mesh and 3D Gaussians, extending traditional mesh-driven deformation paradigms that only rely on ARAP prior, thus capturing the motion of each 3D Gaussian more precisely.","By joint optimizing meshes, 3D Gaussians, and RDF, MaGS achieves both high rendering accuracy and realistic deformation.","Extensive experiments on the D-NeRF and NeRF-DS datasets demonstrate that MaGS can generate competitive results in both reconstruction and simulation."],"url":"http://arxiv.org/abs/2406.01593v1","category":"cs.CV"}
{"created":"2024-06-03 17:59:34","title":"DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation","abstract":"This paper presents Deformable Neural Vessel Representations (DeNVeR), an unsupervised approach for vessel segmentation in X-ray videos without annotated ground truth. DeNVeR uses optical flow and layer separation, enhancing segmentation accuracy and adaptability through test-time training. A key component of our research is the introduction of the XACV dataset, the first X-ray angiography coronary video dataset with high-quality, manually labeled segmentation ground truth. Our evaluation demonstrates that DeNVeR outperforms current state-of-the-art methods in vessel segmentation. This paper marks an advance in medical imaging, providing a robust, data-efficient tool for disease diagnosis and treatment planning and setting a new standard for future research in video vessel segmentation. See our project page for video results at https://kirito878.github.io/DeNVeR/.","sentences":["This paper presents Deformable Neural Vessel Representations (DeNVeR), an unsupervised approach for vessel segmentation in X-ray videos without annotated ground truth.","DeNVeR uses optical flow and layer separation, enhancing segmentation accuracy and adaptability through test-time training.","A key component of our research is the introduction of the XACV dataset, the first X-ray angiography coronary video dataset with high-quality, manually labeled segmentation ground truth.","Our evaluation demonstrates that DeNVeR outperforms current state-of-the-art methods in vessel segmentation.","This paper marks an advance in medical imaging, providing a robust, data-efficient tool for disease diagnosis and treatment planning and setting a new standard for future research in video vessel segmentation.","See our project page for video results at https://kirito878.github.io/DeNVeR/."],"url":"http://arxiv.org/abs/2406.01591v1","category":"cs.CV"}
{"created":"2024-06-03 17:58:17","title":"Variational time reversal for free energy estimation in nonequilibrium steady states","abstract":"Studying the structure of systems in nonequilibrium steady states necessitates tools that quantify population shifts and associated deformations of equilibrium free energy landscapes under persistent currents. Within the framework of stochastic thermodynamics, we establish a variant of the Kawasaki--Crooks equality that relates nonequilibrium free energy corrections in driven systems to heat dissipation statistics along time-reversed relaxation trajectories computable with molecular simulation. Using stochastic control theory, we arrive at a variational approach to evaluate the Kawasaki--Crooks equality and use it to estimate distribution functions of order parameters in overdamped Langevin models of active matter, attaining substantial improvement in accuracy over simple perturbative methods.","sentences":["Studying the structure of systems in nonequilibrium steady states necessitates tools that quantify population shifts and associated deformations of equilibrium free energy landscapes under persistent currents.","Within the framework of stochastic thermodynamics, we establish a variant of the Kawasaki--Crooks equality that relates nonequilibrium free energy corrections in driven systems to heat dissipation statistics along time-reversed relaxation trajectories computable with molecular simulation.","Using stochastic control theory, we arrive at a variational approach to evaluate the Kawasaki--Crooks equality and use it to estimate distribution functions of order parameters in overdamped Langevin models of active matter, attaining substantial improvement in accuracy over simple perturbative methods."],"url":"http://arxiv.org/abs/2406.01582v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-03 17:54:58","title":"An Equivalence Between Static and Dynamic Regret Minimization","abstract":"We study the problem of dynamic regret minimization in online convex optimization, in which the objective is to minimize the difference between the cumulative loss of an algorithm and that of an arbitrary sequence of comparators. While the literature on this topic is very rich, a unifying framework for the analysis and design of these algorithms is still missing. In this paper, \\emph{we show that dynamic regret minimization is equivalent to static regret minimization in an extended decision space}. Using this simple observation, we show that there is a frontier of lower bounds trading off penalties due to the variance of the losses and penalties due to variability of the comparator sequence, and provide a framework for achieving any of the guarantees along this frontier. As a result, we prove for the first time that adapting to the squared path-length of an arbitrary sequence of comparators to achieve regret $R_{T}(u_{1},\\dots,u_{T})\\le O(\\sqrt{T\\sum_{t} \\|u_{t}-u_{t+1}\\|^{2}})$ is impossible. However, we prove that it is possible to adapt to a new notion of variability based on the locally-smoothed squared path-length of the comparator sequence, and provide an algorithm guaranteeing dynamic regret of the form $R_{T}(u_{1},\\dots,u_{T})\\le \\tilde O(\\sqrt{T\\sum_{i}\\|\\bar u_{i}-\\bar u_{i+1}\\|^{2}})$. Up to polylogarithmic terms, the new notion of variability is never worse than the classic one involving the path-length.","sentences":["We study the problem of dynamic regret minimization in online convex optimization, in which the objective is to minimize the difference between the cumulative loss of an algorithm and that of an arbitrary sequence of comparators.","While the literature on this topic is very rich, a unifying framework for the analysis and design of these algorithms is still missing.","In this paper, \\emph{we show that dynamic regret minimization is equivalent to static regret minimization in an extended decision space}.","Using this simple observation, we show that there is a frontier of lower bounds trading off penalties due to the variance of the losses and penalties due to variability of the comparator sequence, and provide a framework for achieving any of the guarantees along this frontier.","As a result, we prove for the first time that adapting to the squared path-length of an arbitrary sequence of comparators to achieve regret $R_{T}(u_{1},\\dots,u_{T})\\le O(\\sqrt{T\\sum_{t} \\|u_{t}-u_{t+1}\\|^{2}})$ is impossible.","However, we prove that it is possible to adapt to a new notion of variability based on the locally-smoothed squared path-length of the comparator sequence, and provide an algorithm guaranteeing dynamic regret of the form $R_{T}(u_{1},\\dots,u_{T})\\le","\\tilde O(\\sqrt{T\\sum_{i}\\|\\bar u_{i}-\\bar","u_{i+1}\\|^{2}})$. Up to polylogarithmic terms, the new notion of variability is never worse than the classic one involving the path-length."],"url":"http://arxiv.org/abs/2406.01577v1","category":"cs.LG"}
{"created":"2024-06-03 17:45:41","title":"LoFiT: Localized Fine-tuning on LLM Representations","abstract":"Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, for the tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods.","sentences":["Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment.","For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models.","In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods.","We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads.","LoFiT localizes to a sparse set of heads (3%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention.","For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention.","We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task.","Finally, for the tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods."],"url":"http://arxiv.org/abs/2406.01563v1","category":"cs.CL"}
{"created":"2024-06-03 17:38:24","title":"Bayesian compositional regression with flexible microbiome feature aggregation and selection","abstract":"Ongoing advances in microbiome profiling have allowed unprecedented insights into the molecular activities of microbial communities. This has fueled a strong scientific interest in understanding the critical role the microbiome plays in governing human health, by identifying microbial features associated with clinical outcomes of interest. Several aspects of microbiome data limit the applicability of existing variable selection approaches. In particular, microbiome data are high-dimensional, extremely sparse, and compositional. Importantly, many of the observed features, although categorized as different taxa, may play related functional roles. To address these challenges, we propose a novel compositional regression approach that leverages the data-adaptive clustering and variable selection properties of the spiked Dirichlet process to identify taxa that exhibit similar functional roles. Our proposed method, Bayesian Regression with Agglomerated Compositional Effects using a dirichLET process (BRACElet), enables the identification of a sparse set of features with shared impacts on the outcome, facilitating dimension reduction and model interpretation. We demonstrate that BRACElet outperforms existing approaches for microbiome variable selection through simulation studies and an application elucidating the impact of oral microbiome composition on insulin resistance.","sentences":["Ongoing advances in microbiome profiling have allowed unprecedented insights into the molecular activities of microbial communities.","This has fueled a strong scientific interest in understanding the critical role the microbiome plays in governing human health, by identifying microbial features associated with clinical outcomes of interest.","Several aspects of microbiome data limit the applicability of existing variable selection approaches.","In particular, microbiome data are high-dimensional, extremely sparse, and compositional.","Importantly, many of the observed features, although categorized as different taxa, may play related functional roles.","To address these challenges, we propose a novel compositional regression approach that leverages the data-adaptive clustering and variable selection properties of the spiked Dirichlet process to identify taxa that exhibit similar functional roles.","Our proposed method, Bayesian Regression with Agglomerated Compositional Effects using a dirichLET process (BRACElet), enables the identification of a sparse set of features with shared impacts on the outcome, facilitating dimension reduction and model interpretation.","We demonstrate that BRACElet outperforms existing approaches for microbiome variable selection through simulation studies and an application elucidating the impact of oral microbiome composition on insulin resistance."],"url":"http://arxiv.org/abs/2406.01557v1","category":"stat.ME"}
{"created":"2024-06-03 17:36:36","title":"Proxy Denoising for Source-Free Domain Adaptation","abstract":"Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of pre-trained large vision-language (ViL) models in many other applications, the latest SFDA methods have also validated the benefit of ViL models by leveraging their predictions as pseudo supervision. However, we observe that ViL's predictions could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, in this paper, we introduce a novel Proxy Denoising (ProDe) approach. Specifically, we leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Critically, we design a proxy denoising mechanism for correcting ViL's predictions. This is grounded on a novel proxy confidence theory by modeling elegantly the domain adaption effect of the proxy's divergence against the domain-invariant space. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that our ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set and generalized SFDA settings. The code will release soon.","sentences":["Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data.","Inspired by the success of pre-trained large vision-language (ViL) models in many other applications, the latest SFDA methods have also validated the benefit of ViL models by leveraging their predictions as pseudo supervision.","However, we observe that ViL's predictions could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption.","To address this thus-far ignored challenge, in this paper, we introduce a novel Proxy Denoising (ProDe) approach.","Specifically, we leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space.","Critically, we design a proxy denoising mechanism for correcting ViL's predictions.","This is grounded on a novel proxy confidence theory by modeling elegantly the domain adaption effect of the proxy's divergence against the domain-invariant space.","To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization.","Extensive experiments show that our ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set and generalized SFDA settings.","The code will release soon."],"url":"http://arxiv.org/abs/2406.01658v1","category":"cs.CV"}
{"created":"2024-06-03 17:34:37","title":"Towards Flexible Interactive Reflection Removal with Human Guidance","abstract":"Single image reflection removal is inherently ambiguous, as both the reflection and transmission components requiring separation may follow natural image statistics. Existing methods attempt to address the issue by using various types of low-level and physics-based cues as sources of reflection signals. However, these cues are not universally applicable, since they are only observable in specific capture scenarios. This leads to a significant performance drop when test images do not align with their assumptions. In this paper, we aim to explore a novel flexible interactive reflection removal approach that leverages various forms of sparse human guidance, such as points and bounding boxes, as auxiliary high-level prior to achieve robust reflection removal. However, incorporating the raw user guidance naively into the existing reflection removal network does not result in performance gains. To this end, we innovatively transform raw user input into a unified form -- reflection masks using an Interactive Segmentation Foundation Model. Such a design absorbs the quintessence of the foundational segmentation model and flexible human guidance, thereby mitigating the challenges of reflection separations. Furthermore, to fully utilize user guidance and reduce user annotation costs, we design a mask-guided reflection removal network, comprising our proposed self-adaptive prompt block. This block adaptively incorporates user guidance as anchors and refines transmission features via cross-attention mechanisms. Extensive results on real-world images validate that our method demonstrates state-of-the-art performance on various datasets with the help of flexible and sparse user guidance. Our code and dataset will be publicly available here https://github.com/ShawnChenn/FlexibleReflectionRemoval.","sentences":["Single image reflection removal is inherently ambiguous, as both the reflection and transmission components requiring separation may follow natural image statistics.","Existing methods attempt to address the issue by using various types of low-level and physics-based cues as sources of reflection signals.","However, these cues are not universally applicable, since they are only observable in specific capture scenarios.","This leads to a significant performance drop when test images do not align with their assumptions.","In this paper, we aim to explore a novel flexible interactive reflection removal approach that leverages various forms of sparse human guidance, such as points and bounding boxes, as auxiliary high-level prior to achieve robust reflection removal.","However, incorporating the raw user guidance naively into the existing reflection removal network does not result in performance gains.","To this end, we innovatively transform raw user input into a unified form -- reflection masks using an Interactive Segmentation Foundation Model.","Such a design absorbs the quintessence of the foundational segmentation model and flexible human guidance, thereby mitigating the challenges of reflection separations.","Furthermore, to fully utilize user guidance and reduce user annotation costs, we design a mask-guided reflection removal network, comprising our proposed self-adaptive prompt block.","This block adaptively incorporates user guidance as anchors and refines transmission features via cross-attention mechanisms.","Extensive results on real-world images validate that our method demonstrates state-of-the-art performance on various datasets with the help of flexible and sparse user guidance.","Our code and dataset will be publicly available here https://github.com/ShawnChenn/FlexibleReflectionRemoval."],"url":"http://arxiv.org/abs/2406.01555v1","category":"cs.CV"}
{"created":"2024-06-03 17:23:00","title":"CO in the Draco Nebula: The Atomic-Molecular Transition","abstract":"This paper presents maps of the J=2-1 transition of CO toward the Draco Nebula Intermediate Velocity Cloud (IVC). The maps cover 8500 square arcmin with a velocity resolution of 0.33 km~s$^{-1}$ and angular resolution of 38\", or 0.11 pc at the cloud distance of 600 pc. The mapped area includes all the emission detected by the {\\it Herschel} satellite with 250 $\\mu$m intensity >5 MJy/sr. Previously published observations of the far-IR emission and the 21 cm line of HI are used to derive the column density distribution of H$_2$ and the abundance ratio CO/H$_2$, as well as the distribution of the molecular fraction of hydrogen, which approaches 90\\% over much of the brighter parts of the nebula. The CO emission is highly clumpy and closely resembles the structures seen in far-IR images. The kinematics of the CO show supersonic motions between clumps but near-thermal to trans-sonic motions within clumps, consistent with model predictions that the scale length for dissipation of supersonic turbulence should be $\\sim0.1$ pc, mediated by kinematic viscosity and/or ambipolar diffusion. Different parts of the nebula show evidence for a spread of molecular formation timescales of a few 10$^5$ years, comparable to the dynamical timescale of the infalling gas. The IVC will likely merge with the Galactic interstellar medium in $\\sim 10^7$ years, and the densest clumps may form an unbound cluster of low-mass stars.","sentences":["This paper presents maps of the J=2-1 transition of CO toward the Draco Nebula Intermediate Velocity Cloud (IVC).","The maps cover 8500 square arcmin with a velocity resolution of 0.33 km~s$^{-1}$ and angular resolution of 38\", or 0.11 pc at the cloud distance of 600 pc.","The mapped area includes all the emission detected by the {\\it Herschel} satellite with 250 $\\mu$m intensity >5 MJy/sr.","Previously published observations of the far-IR emission and the 21 cm line of HI are used to derive the column density distribution of H$_2$ and the abundance ratio CO/H$_2$, as well as the distribution of the molecular fraction of hydrogen, which approaches 90\\% over much of the brighter parts of the nebula.","The CO emission is highly clumpy and closely resembles the structures seen in far-IR images.","The kinematics of the CO show supersonic motions between clumps but near-thermal to trans-sonic motions within clumps, consistent with model predictions that the scale length for dissipation of supersonic turbulence should be $\\sim0.1$ pc, mediated by kinematic viscosity and/or ambipolar diffusion.","Different parts of the nebula show evidence for a spread of molecular formation timescales of a few 10$^5$ years, comparable to the dynamical timescale of the infalling gas.","The IVC will likely merge with the Galactic interstellar medium in $\\sim 10^7$ years, and the densest clumps may form an unbound cluster of low-mass stars."],"url":"http://arxiv.org/abs/2406.01542v1","category":"astro-ph.GA"}
{"created":"2024-06-03 17:19:30","title":"Adaptive discretization algorithms for locally optimal experimental design","abstract":"We develop adaptive discretization algorithms for locally optimal experimental design of nonlinear prediction models. With these algorithms, we refine and improve a pertinent state-of-the-art algorithm in various respects. We establish novel termination, convergence, and convergence rate results for the proposed algorithms. In particular, we prove a sublinear convergence rate result under very general assumptions on the design criterion and, most notably, a linear convergence result under the additional assumption that the design criterion is strongly convex and the design space is finite. Additionally, we prove the finite termination at approximately optimal designs, including upper bounds on the number of iterations until termination. And finally, we illustrate the practical use of the proposed algorithms by means of two application examples from chemical engineering: one with a stationary model and one with a dynamic model.","sentences":["We develop adaptive discretization algorithms for locally optimal experimental design of nonlinear prediction models.","With these algorithms, we refine and improve a pertinent state-of-the-art algorithm in various respects.","We establish novel termination, convergence, and convergence rate results for the proposed algorithms.","In particular, we prove a sublinear convergence rate result under very general assumptions on the design criterion and, most notably, a linear convergence result under the additional assumption that the design criterion is strongly convex and the design space is finite.","Additionally, we prove the finite termination at approximately optimal designs, including upper bounds on the number of iterations until termination.","And finally, we illustrate the practical use of the proposed algorithms by means of two application examples from chemical engineering: one with a stationary model and one with a dynamic model."],"url":"http://arxiv.org/abs/2406.01541v1","category":"math.OC"}
{"created":"2024-06-03 16:51:57","title":"MOSEAC: Streamlined Variable Time Step Reinforcement Learning","abstract":"Traditional reinforcement learning (RL) methods typically employ a fixed control loop, where each cycle corresponds to an action. This rigidity poses challenges in practical applications, as the optimal control frequency is task-dependent. A suboptimal choice can lead to high computational demands and reduced exploration efficiency. Variable Time Step Reinforcement Learning (VTS-RL) addresses these issues by using adaptive frequencies for the control loop, executing actions only when necessary. This approach, rooted in reactive programming principles, reduces computational load and extends the action space by including action durations. However, VTS-RL's implementation is often complicated by the need to tune multiple hyperparameters that govern exploration in the multi-objective action-duration space (i.e., balancing task performance and number of time steps to achieve a goal). To overcome these challenges, we introduce the Multi-Objective Soft Elastic Actor-Critic (MOSEAC) method. This method features an adaptive reward scheme that adjusts hyperparameters based on observed trends in task rewards during training. This scheme reduces the complexity of hyperparameter tuning, requiring a single hyperparameter to guide exploration, thereby simplifying the learning process and lowering deployment costs. We validate the MOSEAC method through simulations in a Newtonian kinematics environment, demonstrating high task and training performance with fewer time steps, ultimately lowering energy consumption. This validation shows that MOSEAC streamlines RL algorithm deployment by automatically tuning the agent control loop frequency using a single parameter. Its principles can be applied to enhance any RL algorithm, making it a versatile solution for various applications.","sentences":["Traditional reinforcement learning (RL) methods typically employ a fixed control loop, where each cycle corresponds to an action.","This rigidity poses challenges in practical applications, as the optimal control frequency is task-dependent.","A suboptimal choice can lead to high computational demands and reduced exploration efficiency.","Variable Time Step Reinforcement Learning (VTS-RL) addresses these issues by using adaptive frequencies for the control loop, executing actions only when necessary.","This approach, rooted in reactive programming principles, reduces computational load and extends the action space by including action durations.","However, VTS-RL's implementation is often complicated by the need to tune multiple hyperparameters that govern exploration in the multi-objective action-duration space (i.e., balancing task performance and number of time steps to achieve a goal).","To overcome these challenges, we introduce the Multi-Objective Soft Elastic Actor-Critic (MOSEAC) method.","This method features an adaptive reward scheme that adjusts hyperparameters based on observed trends in task rewards during training.","This scheme reduces the complexity of hyperparameter tuning, requiring a single hyperparameter to guide exploration, thereby simplifying the learning process and lowering deployment costs.","We validate the MOSEAC method through simulations in a Newtonian kinematics environment, demonstrating high task and training performance with fewer time steps, ultimately lowering energy consumption.","This validation shows that MOSEAC streamlines RL algorithm deployment by automatically tuning the agent control loop frequency using a single parameter.","Its principles can be applied to enhance any RL algorithm, making it a versatile solution for various applications."],"url":"http://arxiv.org/abs/2406.01521v1","category":"cs.LG"}
{"created":"2024-06-03 16:33:54","title":"Reducing phenotype-structured PDE models of cancer evolution to systems of ODEs: a generalised moment dynamics approach","abstract":"Intratumour phenotypic heterogeneity is nowadays understood to play a critical role in disease progression and treatment failure. Accordingly, there has been increasing interest in the development of mathematical models capable of capturing its role in cancer cell adaptation. This can be systematically achieved by means of models comprising phenotype-structured nonlocal partial differential equations, tracking the evolution of the phenotypic density distribution of the cell population, which may be compared to gene and protein expression distributions obtained experimentally. Nevertheless, given the high analytical and computational cost of solving these models, much is to be gained from reducing them to systems of ordinary differential equations for the moments of the distribution. We propose a generalised method of model-reduction, relying on the use of a moment generating function, Taylor series expansion and truncation closure, to reduce a nonlocal reaction-advection-diffusion equation, with general phenotypic drift and proliferation rate functions, to a system of moment equations up to arbitrary order. Our method extends previous results in the literature, which we address via two examples, by removing any \\textit{a priori} assumption on the shape of the distribution, and provides a flexible framework for mathematical modellers to account for the role of phenotypic heterogeneity in cancer adaptive dynamics, in a simpler mathematical framework.","sentences":["Intratumour phenotypic heterogeneity is nowadays understood to play a critical role in disease progression and treatment failure.","Accordingly, there has been increasing interest in the development of mathematical models capable of capturing its role in cancer cell adaptation.","This can be systematically achieved by means of models comprising phenotype-structured nonlocal partial differential equations, tracking the evolution of the phenotypic density distribution of the cell population, which may be compared to gene and protein expression distributions obtained experimentally.","Nevertheless, given the high analytical and computational cost of solving these models, much is to be gained from reducing them to systems of ordinary differential equations for the moments of the distribution.","We propose a generalised method of model-reduction, relying on the use of a moment generating function, Taylor series expansion and truncation closure, to reduce a nonlocal reaction-advection-diffusion equation, with general phenotypic drift and proliferation rate functions, to a system of moment equations up to arbitrary order.","Our method extends previous results in the literature, which we address via two examples, by removing any \\textit{a priori} assumption on the shape of the distribution, and provides a flexible framework for mathematical modellers to account for the role of phenotypic heterogeneity in cancer adaptive dynamics, in a simpler mathematical framework."],"url":"http://arxiv.org/abs/2406.01505v1","category":"q-bio.PE"}
{"created":"2024-06-03 16:03:04","title":"New limits on warm inflation from pulsar timing arrays","abstract":"In this paper, we investigate scalar-induced gravitational waves (GWs) generated in the post-inflationary universe to infer new limits on warm inflation. We specifically examine the evolution of primordial GWs produced by scalar perturbations produced during the radiation-dominated epoch. For this purpose, we assume a weak regime of warm inflation under the slow-roll approximation, with a dissipation coefficient linearly dependent on the temperature of the radiation bath. We then derive analytical expressions for the curvature power spectrum and the scalar index, in the cases of chaotic and exponential potentials of the inflationary field. Subsequently, we compare the theoretical predictions regarding the relic energy density of GWs with the stochastic GW background signal recently detected by the NANOGrav collaboration through the use of pulsar timing array measurements. In so doing, we obtain numerical constraints on the free parameters of the inflationary models under study. Finally, we conduct a model selection analysis through the Bayesian inference method to measure the statistical performance of the different theoretical scenarios.","sentences":["In this paper, we investigate scalar-induced gravitational waves (GWs) generated in the post-inflationary universe to infer new limits on warm inflation.","We specifically examine the evolution of primordial GWs produced by scalar perturbations produced during the radiation-dominated epoch.","For this purpose, we assume a weak regime of warm inflation under the slow-roll approximation, with a dissipation coefficient linearly dependent on the temperature of the radiation bath.","We then derive analytical expressions for the curvature power spectrum and the scalar index, in the cases of chaotic and exponential potentials of the inflationary field.","Subsequently, we compare the theoretical predictions regarding the relic energy density of GWs with the stochastic GW background signal recently detected by the NANOGrav collaboration through the use of pulsar timing array measurements.","In so doing, we obtain numerical constraints on the free parameters of the inflationary models under study.","Finally, we conduct a model selection analysis through the Bayesian inference method to measure the statistical performance of the different theoretical scenarios."],"url":"http://arxiv.org/abs/2406.01475v1","category":"astro-ph.CO"}
{"created":"2024-06-03 15:57:43","title":"Tomographic Reconstruction and Regularisation with Search Space Expansion and Total Variation","abstract":"The use of ray projections to reconstruct images is a common technique in medical imaging. Dealing with incomplete data is particularly important when a patient is vulnerable to potentially damaging radiation or is unable to cope with the long scanning time. This paper utilises the reformulation of the problem into an optimisation tasks, followed by using a swarm-based reconstruction from highly undersampled data where particles move in image space in an attempt to minimise the reconstruction error. The process is prone to noise and, in addition to the recently introduced search space expansion technique, a further smoothing process, total variation regularisation, is adapted and investigated. The proposed method is shown to produce lower reproduction errors compared to standard tomographic reconstruction toolbox algorithms as well as one of the leading high-dimensional optimisers on the clinically important Shepp-Logan phantom.","sentences":["The use of ray projections to reconstruct images is a common technique in medical imaging.","Dealing with incomplete data is particularly important when a patient is vulnerable to potentially damaging radiation or is unable to cope with the long scanning time.","This paper utilises the reformulation of the problem into an optimisation tasks, followed by using a swarm-based reconstruction from highly undersampled data where particles move in image space in an attempt to minimise the reconstruction error.","The process is prone to noise and, in addition to the recently introduced search space expansion technique, a further smoothing process, total variation regularisation, is adapted and investigated.","The proposed method is shown to produce lower reproduction errors compared to standard tomographic reconstruction toolbox algorithms as well as one of the leading high-dimensional optimisers on the clinically important Shepp-Logan phantom."],"url":"http://arxiv.org/abs/2406.01469v1","category":"cs.NE"}
{"created":"2024-06-03 15:43:04","title":"Flag-like singular integrals and associated Hardy spaces on a kind of nilpotent Lie groups of step two","abstract":"The Cauchy-Szeg\\\"o singular integral is a fundamental tool in the study of holomorphic $H^p$ Hardy space. But for a kind of Siegel domains, the Cauchy-Szeg\\\"o kernels are neither product ones nor flag ones on the Shilov boundaries, which have the structure of nilpotent Lie groups $\\mathscr N $ of step two. We use the lifting method to investigate flag-like singular integrals on $\\mathscr N $, which includes these Cauchy-Szeg\\\"o ones as a special case. The lifting group is the product $\\tilde {\\mathscr N }$ of three Heisenberg groups, and naturally geometric or analytical objects on $\\mathscr N $ are the projection of those on $\\tilde {\\mathscr N } $. As in the flag case, we introduce various notions on $\\mathscr N $ adapted to geometric feature of these kernels, such as tubes, nontangential regions, tube maximal functions, Littlewood-Paley functions, tents, shards and atoms etc. They have the feature of tri-parameters, although the second step of the group $\\mathscr N$ is only $2$-dimensional, i.e. there exists a hidden parameter as in the flag case. We also establish the corresponding Calder\\'on reproducing formula, characterization of $ L ^p (\\mathscr N)$ by Littlewood-Paley functions, $ L ^p $-boundedness of tube maximal functions and flag-like singular integrals and atomic decomposition of $H^1$ Hardy space on $ {\\mathscr N } $.","sentences":["The Cauchy-Szeg\\\"o singular integral is a fundamental tool in the study of holomorphic $H^p$ Hardy space.","But for a kind of Siegel domains, the Cauchy-Szeg\\\"o kernels are neither product ones nor flag ones on the Shilov boundaries, which have the structure of nilpotent Lie groups $\\mathscr N $ of step two.","We use the lifting method to investigate flag-like singular integrals on $\\mathscr N $, which includes these Cauchy-Szeg\\\"o ones as a special case.","The lifting group is the product $\\tilde {\\mathscr N }$ of three Heisenberg groups, and naturally geometric or analytical objects on $\\mathscr N $ are the projection of those on $\\tilde {\\mathscr N } $.","As in the flag case, we introduce various notions on $\\mathscr N $ adapted to geometric feature of these kernels, such as tubes, nontangential regions, tube maximal functions, Littlewood-Paley functions, tents, shards and atoms etc.","They have the feature of tri-parameters, although the second step of the group $\\mathscr N$ is only $2$-dimensional, i.e. there exists a hidden parameter as in the flag case.","We also establish the corresponding Calder\\'on reproducing formula, characterization of $ L ^p (\\mathscr N)$ by Littlewood-Paley functions, $ L ^p $-boundedness of tube maximal functions and flag-like singular integrals and atomic decomposition of $H^1$ Hardy space on $ {\\mathscr N } $."],"url":"http://arxiv.org/abs/2406.01453v1","category":"math.FA"}
{"created":"2024-06-03 15:28:12","title":"Learning Analysis of Kernel Ridgeless Regression with Asymmetric Kernel Learning","abstract":"Ridgeless regression has garnered attention among researchers, particularly in light of the ``Benign Overfitting'' phenomenon, where models interpolating noisy samples demonstrate robust generalization. However, kernel ridgeless regression does not always perform well due to the lack of flexibility. This paper enhances kernel ridgeless regression with Locally-Adaptive-Bandwidths (LAB) RBF kernels, incorporating kernel learning techniques to improve performance in both experiments and theory. For the first time, we demonstrate that functions learned from LAB RBF kernels belong to an integral space of Reproducible Kernel Hilbert Spaces (RKHSs). Despite the absence of explicit regularization in the proposed model, its optimization is equivalent to solving an $\\ell_0$-regularized problem in the integral space of RKHSs, elucidating the origin of its generalization ability. Taking an approximation analysis viewpoint, we introduce an $l_q$-norm analysis technique (with $0<q<1$) to derive the learning rate for the proposed model under mild conditions. This result deepens our theoretical understanding, explaining that our algorithm's robust approximation ability arises from the large capacity of the integral space of RKHSs, while its generalization ability is ensured by sparsity, controlled by the number of support vectors. Experimental results on both synthetic and real datasets validate our theoretical conclusions.","sentences":["Ridgeless regression has garnered attention among researchers, particularly in light of the ``Benign Overfitting'' phenomenon, where models interpolating noisy samples demonstrate robust generalization.","However, kernel ridgeless regression does not always perform well due to the lack of flexibility.","This paper enhances kernel ridgeless regression with Locally-Adaptive-Bandwidths (LAB) RBF kernels, incorporating kernel learning techniques to improve performance in both experiments and theory.","For the first time, we demonstrate that functions learned from LAB RBF kernels belong to an integral space of Reproducible Kernel Hilbert Spaces (RKHSs).","Despite the absence of explicit regularization in the proposed model, its optimization is equivalent to solving an $\\ell_0$-regularized problem in the integral space of RKHSs, elucidating the origin of its generalization ability.","Taking an approximation analysis viewpoint, we introduce an $l_q$-norm analysis technique (with $0<q<1$) to derive the learning rate for the proposed model under mild conditions.","This result deepens our theoretical understanding, explaining that our algorithm's robust approximation ability arises from the large capacity of the integral space of RKHSs, while its generalization ability is ensured by sparsity, controlled by the number of support vectors.","Experimental results on both synthetic and real datasets validate our theoretical conclusions."],"url":"http://arxiv.org/abs/2406.01435v1","category":"cs.LG"}
{"created":"2024-06-03 15:26:33","title":"EAGLE: Efficient Adaptive Geometry-based Learning in Cross-view Understanding","abstract":"Unsupervised Domain Adaptation has been an efficient approach to transferring the semantic segmentation model across data distributions. Meanwhile, the recent Open-vocabulary Semantic Scene understanding based on large-scale vision language models is effective in open-set settings because it can learn diverse concepts and categories. However, these prior methods fail to generalize across different camera views due to the lack of cross-view geometric modeling. At present, there are limited studies analyzing cross-view learning. To address this problem, we introduce a novel Unsupervised Cross-view Adaptation Learning approach to modeling the geometric structural change across views in Semantic Scene Understanding. First, we introduce a novel Cross-view Geometric Constraint on Unpaired Data to model structural changes in images and segmentation masks across cameras. Second, we present a new Geodesic Flow-based Correlation Metric to efficiently measure the geometric structural changes across camera views. Third, we introduce a novel view-condition prompting mechanism to enhance the view-information modeling of the open-vocabulary segmentation network in cross-view adaptation learning. The experiments on different cross-view adaptation benchmarks have shown the effectiveness of our approach in cross-view modeling, demonstrating that we achieve State-of-the-Art (SOTA) performance compared to prior unsupervised domain adaptation and open-vocabulary semantic segmentation methods.","sentences":["Unsupervised Domain Adaptation has been an efficient approach to transferring the semantic segmentation model across data distributions.","Meanwhile, the recent Open-vocabulary Semantic Scene understanding based on large-scale vision language models is effective in open-set settings because it can learn diverse concepts and categories.","However, these prior methods fail to generalize across different camera views due to the lack of cross-view geometric modeling.","At present, there are limited studies analyzing cross-view learning.","To address this problem, we introduce a novel Unsupervised Cross-view Adaptation Learning approach to modeling the geometric structural change across views in Semantic Scene Understanding.","First, we introduce a novel Cross-view Geometric Constraint on Unpaired Data to model structural changes in images and segmentation masks across cameras.","Second, we present a new Geodesic Flow-based Correlation Metric to efficiently measure the geometric structural changes across camera views.","Third, we introduce a novel view-condition prompting mechanism to enhance the view-information modeling of the open-vocabulary segmentation network in cross-view adaptation learning.","The experiments on different cross-view adaptation benchmarks have shown the effectiveness of our approach in cross-view modeling, demonstrating that we achieve State-of-the-Art (SOTA) performance compared to prior unsupervised domain adaptation and open-vocabulary semantic segmentation methods."],"url":"http://arxiv.org/abs/2406.01429v1","category":"cs.CV"}
{"created":"2024-06-03 15:25:45","title":"Sensitivity-Informed Augmentation for Robust Segmentation","abstract":"Segmentation is an integral module in many visual computing applications such as virtual try-on, medical imaging, autonomous driving, and agricultural automation. These applications often involve either widespread consumer use or highly variable environments, both of which can degrade the quality of visual sensor data, whether from a common mobile phone or an expensive satellite imaging camera. In addition to external noises like user difference or weather conditions, internal noises such as variations in camera quality or lens distortion can affect the performance of segmentation models during both development and deployment. In this work, we present an efficient, adaptable, and gradient-free method to enhance the robustness of learning-based segmentation models across training. First, we introduce a novel adaptive sensitivity analysis (ASA) using Kernel Inception Distance (KID) on basis perturbations to benchmark perturbation sensitivity of pre-trained segmentation models. Then, we model the sensitivity curve using the adaptive SA and sample perturbation hyperparameter values accordingly. Finally, we conduct adversarial training with the selected perturbation values and dynamically re-evaluate robustness during online training. Our method, implemented end-to-end with minimal fine-tuning required, consistently outperforms state-of-the-art data augmentation techniques for segmentation. It shows significant improvement in both clean data evaluation and real-world adverse scenario evaluation across various segmentation datasets used in visual computing and computer graphics applications.","sentences":["Segmentation is an integral module in many visual computing applications such as virtual try-on, medical imaging, autonomous driving, and agricultural automation.","These applications often involve either widespread consumer use or highly variable environments, both of which can degrade the quality of visual sensor data, whether from a common mobile phone or an expensive satellite imaging camera.","In addition to external noises like user difference or weather conditions, internal noises such as variations in camera quality or lens distortion can affect the performance of segmentation models during both development and deployment.","In this work, we present an efficient, adaptable, and gradient-free method to enhance the robustness of learning-based segmentation models across training.","First, we introduce a novel adaptive sensitivity analysis (ASA) using Kernel Inception Distance (KID) on basis perturbations to benchmark perturbation sensitivity of pre-trained segmentation models.","Then, we model the sensitivity curve using the adaptive SA and sample perturbation hyperparameter values accordingly.","Finally, we conduct adversarial training with the selected perturbation values and dynamically re-evaluate robustness during online training.","Our method, implemented end-to-end with minimal fine-tuning required, consistently outperforms state-of-the-art data augmentation techniques for segmentation.","It shows significant improvement in both clean data evaluation and real-world adverse scenario evaluation across various segmentation datasets used in visual computing and computer graphics applications."],"url":"http://arxiv.org/abs/2406.01425v2","category":"cs.CV"}
{"created":"2024-06-03 15:16:02","title":"Adapting Conformal Prediction to Distribution Shifts Without Labels","abstract":"Conformal prediction (CP) enables machine learning models to output prediction sets with guaranteed coverage rate, assuming exchangeable data. Unfortunately, the exchangeability assumption is frequently violated due to distribution shifts in practice, and the challenge is often compounded by the lack of ground truth labels at test time. Focusing on classification in this paper, our goal is to improve the quality of CP-generated prediction sets using only unlabeled data from the test domain. This is achieved by two new methods called ECP and EACP, that adjust the score function in CP according to the base model's uncertainty on the unlabeled test data. Through extensive experiments on a number of large-scale datasets and neural network architectures, we show that our methods provide consistent improvement over existing baselines and nearly match the performance of supervised algorithms.","sentences":["Conformal prediction (CP) enables machine learning models to output prediction sets with guaranteed coverage rate, assuming exchangeable data.","Unfortunately, the exchangeability assumption is frequently violated due to distribution shifts in practice, and the challenge is often compounded by the lack of ground truth labels at test time.","Focusing on classification in this paper, our goal is to improve the quality of CP-generated prediction sets using only unlabeled data from the test domain.","This is achieved by two new methods called ECP and EACP, that adjust the score function in CP according to the base model's uncertainty on the unlabeled test data.","Through extensive experiments on a number of large-scale datasets and neural network architectures, we show that our methods provide consistent improvement over existing baselines and nearly match the performance of supervised algorithms."],"url":"http://arxiv.org/abs/2406.01416v1","category":"cs.LG"}
{"created":"2024-06-03 14:40:09","title":"sAirflow: Adopting Serverless in a Legacy Workflow Scheduler","abstract":"Serverless clouds promise efficient scaling, reduced toil and monetary costs. Yet, serverless-ing a complex, legacy application might require major refactoring and thus is risky. As a case study, we use Airflow, an industry-standard workflow system. To reduce migration risk, we propose to limit code modifications by relying on change data capture (CDC) and message queues for internal communication. To achieve serverless efficiency, we rely on Function-as-a-Service (FaaS). Our system, sAirflow, is the first adaptation of the control plane and workers to the serverless cloud - and it maintains the same interface and most of the code. Experimentally, we show that sAirflow delivers the key serverless benefits: scaling and cost reduction. We compare sAirflow to MWAA, a managed (SaaS) Airflow. On Alibaba benchmarks on warm systems, sAirflow performs similarly while halving the monetary cost. On highly parallel workflows on cold systems, sAirflow scales out in seconds to 125 workers, reducing makespan by 2x-7x.","sentences":["Serverless clouds promise efficient scaling, reduced toil and monetary costs.","Yet, serverless-ing a complex, legacy application might require major refactoring and thus is risky.","As a case study, we use Airflow, an industry-standard workflow system.","To reduce migration risk, we propose to limit code modifications by relying on change data capture (CDC) and message queues for internal communication.","To achieve serverless efficiency, we rely on Function-as-a-Service (FaaS).","Our system, sAirflow, is the first adaptation of the control plane and workers to the serverless cloud - and it maintains the same interface and most of the code.","Experimentally, we show that sAirflow delivers the key serverless benefits: scaling and cost reduction.","We compare sAirflow to MWAA, a managed (SaaS) Airflow.","On Alibaba benchmarks on warm systems, sAirflow performs similarly while halving the monetary cost.","On highly parallel workflows on cold systems, sAirflow scales out in seconds to 125 workers, reducing makespan by 2x-7x."],"url":"http://arxiv.org/abs/2406.01374v1","category":"cs.DC"}
{"created":"2024-06-03 14:32:30","title":"BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards","abstract":"Input-output safeguards are used to detect anomalies in the traces produced by Large Language Models (LLMs) systems. These detectors are at the core of diverse safety-critical applications such as real-time monitoring, offline evaluation of traces, and content moderation. However, there is no widely recognized methodology to evaluate them. To fill this gap, we introduce the Benchmarks for the Evaluation of LLM Safeguards (BELLS), a structured collection of tests, organized into three categories: (1) established failure tests, based on already-existing benchmarks for well-defined failure modes, aiming to compare the performance of current input-output safeguards; (2) emerging failure tests, to measure generalization to never-seen-before failure modes and encourage the development of more general safeguards; (3) next-gen architecture tests, for more complex scaffolding (such as LLM-agents and multi-agent systems), aiming to foster the development of safeguards that could adapt to future applications for which no safeguard currently exists. Furthermore, we implement and share the first next-gen architecture test, using the MACHIAVELLI environment, along with an interactive visualization of the dataset.","sentences":["Input-output safeguards are used to detect anomalies in the traces produced by Large Language Models (LLMs) systems.","These detectors are at the core of diverse safety-critical applications such as real-time monitoring, offline evaluation of traces, and content moderation.","However, there is no widely recognized methodology to evaluate them.","To fill this gap, we introduce the Benchmarks for the Evaluation of LLM Safeguards (BELLS), a structured collection of tests, organized into three categories: (1) established failure tests, based on already-existing benchmarks for well-defined failure modes, aiming to compare the performance of current input-output safeguards; (2) emerging failure tests, to measure generalization to never-seen-before failure modes and encourage the development of more general safeguards; (3) next-gen architecture tests, for more complex scaffolding (such as LLM-agents and multi-agent systems), aiming to foster the development of safeguards that could adapt to future applications for which no safeguard currently exists.","Furthermore, we implement and share the first next-gen architecture test, using the MACHIAVELLI environment, along with an interactive visualization of the dataset."],"url":"http://arxiv.org/abs/2406.01364v1","category":"cs.CR"}
{"created":"2024-06-03 13:09:32","title":"pOps: Photo-Inspired Diffusion Operators","abstract":"Text-guided image generation enables the creation of visual content from textual descriptions. However, certain visual concepts cannot be effectively conveyed through language alone. This has sparked a renewed interest in utilizing the CLIP image embedding space for more visually-oriented tasks through methods such as IP-Adapter. Interestingly, the CLIP image embedding space has been shown to be semantically meaningful, where linear operations within this space yield semantically meaningful results. Yet, the specific meaning of these operations can vary unpredictably across different images. To harness this potential, we introduce pOps, a framework that trains specific semantic operators directly on CLIP image embeddings. Each pOps operator is built upon a pretrained Diffusion Prior model. While the Diffusion Prior model was originally trained to map between text embeddings and image embeddings, we demonstrate that it can be tuned to accommodate new input conditions, resulting in a diffusion operator. Working directly over image embeddings not only improves our ability to learn semantic operations but also allows us to directly use a textual CLIP loss as an additional supervision when needed. We show that pOps can be used to learn a variety of photo-inspired operators with distinct semantic meanings, highlighting the semantic diversity and potential of our proposed approach.","sentences":["Text-guided image generation enables the creation of visual content from textual descriptions.","However, certain visual concepts cannot be effectively conveyed through language alone.","This has sparked a renewed interest in utilizing the CLIP image embedding space for more visually-oriented tasks through methods such as IP-Adapter.","Interestingly, the CLIP image embedding space has been shown to be semantically meaningful, where linear operations within this space yield semantically meaningful results.","Yet, the specific meaning of these operations can vary unpredictably across different images.","To harness this potential, we introduce pOps, a framework that trains specific semantic operators directly on CLIP image embeddings.","Each pOps operator is built upon a pretrained Diffusion Prior model.","While the Diffusion Prior model was originally trained to map between text embeddings and image embeddings, we demonstrate that it can be tuned to accommodate new input conditions, resulting in a diffusion operator.","Working directly over image embeddings not only improves our ability to learn semantic operations but also allows us to directly use a textual CLIP loss as an additional supervision when needed.","We show that pOps can be used to learn a variety of photo-inspired operators with distinct semantic meanings, highlighting the semantic diversity and potential of our proposed approach."],"url":"http://arxiv.org/abs/2406.01300v1","category":"cs.CV"}
{"created":"2024-06-03 12:33:27","title":"iKAN: Global Incremental Learning with KAN for Human Activity Recognition Across Heterogeneous Datasets","abstract":"This work proposes an incremental learning (IL) framework for wearable sensor human activity recognition (HAR) that tackles two challenges simultaneously: catastrophic forgetting and non-uniform inputs. The scalable framework, iKAN, pioneers IL with Kolmogorov-Arnold Networks (KAN) to replace multi-layer perceptrons as the classifier that leverages the local plasticity and global stability of splines. To adapt KAN for HAR, iKAN uses task-specific feature branches and a feature redistribution layer. Unlike existing IL methods that primarily adjust the output dimension or the number of classifier nodes to adapt to new tasks, iKAN focuses on expanding the feature extraction branches to accommodate new inputs from different sensor modalities while maintaining consistent dimensions and the number of classifier outputs. Continual learning across six public HAR datasets demonstrated the iKAN framework's incremental learning performance, with a last performance of 84.9\\% (weighted F1 score) and an average incremental performance of 81.34\\%, which significantly outperforms the two existing incremental learning methods, such as EWC (51.42\\%) and experience replay (59.92\\%).","sentences":["This work proposes an incremental learning (IL) framework for wearable sensor human activity recognition (HAR) that tackles two challenges simultaneously: catastrophic forgetting and non-uniform inputs.","The scalable framework, iKAN, pioneers IL with Kolmogorov-Arnold Networks (KAN) to replace multi-layer perceptrons as the classifier that leverages the local plasticity and global stability of splines.","To adapt KAN for HAR, iKAN uses task-specific feature branches and a feature redistribution layer.","Unlike existing IL methods that primarily adjust the output dimension or the number of classifier nodes to adapt to new tasks, iKAN focuses on expanding the feature extraction branches to accommodate new inputs from different sensor modalities while maintaining consistent dimensions and the number of classifier outputs.","Continual learning across six public HAR datasets demonstrated the iKAN framework's incremental learning performance, with a last performance of 84.9\\% (weighted F1 score) and an average incremental performance of 81.34\\%, which significantly outperforms the two existing incremental learning methods, such as EWC (51.42\\%) and experience replay (59.92\\%)."],"url":"http://arxiv.org/abs/2406.01646v1","category":"cs.LG"}
{"created":"2024-06-03 12:20:42","title":"Utilizing and extending superconducting circuit toolbox to simulate analog quantum gravity","abstract":"There has been considerable effort to simulate quantum gravity features in solid state systems, such as analog black holes or wormholes. However, superconducting circuits have so far received only limited attention in this regard. Moreover, for quantum superpositions of spacetime geometries - a highly contentious notion within the quantum gravity community itself - there currently exist no solid state blueprints. We here show how quantum circuit hardware can implement a variety of classical and quantum spacetime geometries on lattices, by both using established circuit elements and introducing new ones. We demonstrate the possibility of a metric sharply changing within a single lattice point, thus entering a regime where the spacetime curvature itself is trans-Planckian, and the Hawking temperature ill-defined. In fact, our approach suggests that stable, thermal event horizons are incompatible with strictly discrete lattice models. We thus propose to directly probe the evaporation of a wormhole by tracking the accumulation of charge and phase quantum fluctuations over short time scales, which are a robust signature even in the presence of a dissipative environment. Moreover, we present a loop-hole for the typical black/while hole ambiguity in lattice simulations: the existence of exceptional points in the dispersion relation allows for the creation of pure black (or white) hole horizons - at the expense of radically changing the interior wormhole dynamics. Finally, based on multistable Josephson junctions, we introduce the notion of quantum inductors: circuit elements that can be prepared in a superposition of different inductance values. Such inductors realise regions with a quantum superposition of analog spacetime. The entanglement of signals with the quantum spacetime can be probed by a type of delayed-choice experiment.","sentences":["There has been considerable effort to simulate quantum gravity features in solid state systems, such as analog black holes or wormholes.","However, superconducting circuits have so far received only limited attention in this regard.","Moreover, for quantum superpositions of spacetime geometries - a highly contentious notion within the quantum gravity community itself - there currently exist no solid state blueprints.","We here show how quantum circuit hardware can implement a variety of classical and quantum spacetime geometries on lattices, by both using established circuit elements and introducing new ones.","We demonstrate the possibility of a metric sharply changing within a single lattice point, thus entering a regime where the spacetime curvature itself is trans-Planckian, and the Hawking temperature ill-defined.","In fact, our approach suggests that stable, thermal event horizons are incompatible with strictly discrete lattice models.","We thus propose to directly probe the evaporation of a wormhole by tracking the accumulation of charge and phase quantum fluctuations over short time scales, which are a robust signature even in the presence of a dissipative environment.","Moreover, we present a loop-hole for the typical black/while hole ambiguity in lattice simulations: the existence of exceptional points in the dispersion relation allows for the creation of pure black (or white) hole horizons - at the expense of radically changing the interior wormhole dynamics.","Finally, based on multistable Josephson junctions, we introduce the notion of quantum inductors: circuit elements that can be prepared in a superposition of different inductance values.","Such inductors realise regions with a quantum superposition of analog spacetime.","The entanglement of signals with the quantum spacetime can be probed by a type of delayed-choice experiment."],"url":"http://arxiv.org/abs/2406.01261v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-03 12:11:01","title":"animal2vec and MeerKAT: A self-supervised transformer for rare-event raw audio input and a large-scale reference dataset for bioacoustics","abstract":"Bioacoustic research provides invaluable insights into the behavior, ecology, and conservation of animals. Most bioacoustic datasets consist of long recordings where events of interest, such as vocalizations, are exceedingly rare. Analyzing these datasets poses a monumental challenge to researchers, where deep learning techniques have emerged as a standard method. Their adaptation remains challenging, focusing on models conceived for computer vision, where the audio waveforms are engineered into spectrographic representations for training and inference. We improve the current state of deep learning in bioacoustics in two ways: First, we present the animal2vec framework: a fully interpretable transformer model and self-supervised training scheme tailored for sparse and unbalanced bioacoustic data. Second, we openly publish MeerKAT: Meerkat Kalahari Audio Transcripts, a large-scale dataset containing audio collected via biologgers deployed on free-ranging meerkats with a length of over 1068h, of which 184h have twelve time-resolved vocalization-type classes, each with ms-resolution, making it the largest publicly-available labeled dataset on terrestrial mammals. Further, we benchmark animal2vec against the NIPS4Bplus birdsong dataset. We report new state-of-the-art results on both datasets and evaluate the few-shot capabilities of animal2vec of labeled training data. Finally, we perform ablation studies to highlight the differences between our architecture and a vanilla transformer baseline for human-produced sounds. animal2vec allows researchers to classify massive amounts of sparse bioacoustic data even with little ground truth information available. In addition, the MeerKAT dataset is the first large-scale, millisecond-resolution corpus for benchmarking bioacoustic models in the pretrain/finetune paradigm. We believe this sets the stage for a new reference point for bioacoustics.","sentences":["Bioacoustic research provides invaluable insights into the behavior, ecology, and conservation of animals.","Most bioacoustic datasets consist of long recordings where events of interest, such as vocalizations, are exceedingly rare.","Analyzing these datasets poses a monumental challenge to researchers, where deep learning techniques have emerged as a standard method.","Their adaptation remains challenging, focusing on models conceived for computer vision, where the audio waveforms are engineered into spectrographic representations for training and inference.","We improve the current state of deep learning in bioacoustics in two ways: First, we present the animal2vec framework: a fully interpretable transformer model and self-supervised training scheme tailored for sparse and unbalanced bioacoustic data.","Second, we openly publish MeerKAT: Meerkat Kalahari Audio Transcripts, a large-scale dataset containing audio collected via biologgers deployed on free-ranging meerkats with a length of over 1068h, of which 184h have twelve time-resolved vocalization-type classes, each with ms-resolution, making it the largest publicly-available labeled dataset on terrestrial mammals.","Further, we benchmark animal2vec against the NIPS4Bplus birdsong dataset.","We report new state-of-the-art results on both datasets and evaluate the few-shot capabilities of animal2vec of labeled training data.","Finally, we perform ablation studies to highlight the differences between our architecture and a vanilla transformer baseline for human-produced sounds.","animal2vec allows researchers to classify massive amounts of sparse bioacoustic data even with little ground truth information available.","In addition, the MeerKAT dataset is the first large-scale, millisecond-resolution corpus for benchmarking bioacoustic models in the pretrain/finetune paradigm.","We believe this sets the stage for a new reference point for bioacoustics."],"url":"http://arxiv.org/abs/2406.01253v1","category":"cs.SD"}
{"created":"2024-06-03 11:24:15","title":"GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer","abstract":"Cross-modal transformers have demonstrated superiority in various vision tasks by effectively integrating different modalities. This paper first critiques prior token exchange methods which replace less informative tokens with inter-modal features, and demonstrate exchange based methods underperform cross-attention mechanisms, while the computational demand of the latter inevitably restricts its use with longer sequences. To surmount the computational challenges, we propose GeminiFusion, a pixel-wise fusion approach that capitalizes on aligned cross-modal representations. GeminiFusion elegantly combines intra-modal and inter-modal attentions, dynamically integrating complementary information across modalities. We employ a layer-adaptive noise to adaptively control their interplay on a per-layer basis, thereby achieving a harmonized fusion process. Notably, GeminiFusion maintains linear complexity with respect to the number of input tokens, ensuring this multimodal framework operates with efficiency comparable to unimodal networks. Comprehensive evaluations across multimodal image-to-image translation, 3D object detection and arbitrary-modal semantic segmentation tasks, including RGB, depth, LiDAR, event data, etc. demonstrate the superior performance of our GeminiFusion against leading-edge techniques. The PyTorch code is available at https://github.com/JiaDingCN/GeminiFusion","sentences":["Cross-modal transformers have demonstrated superiority in various vision tasks by effectively integrating different modalities.","This paper first critiques prior token exchange methods which replace less informative tokens with inter-modal features, and demonstrate exchange based methods underperform cross-attention mechanisms, while the computational demand of the latter inevitably restricts its use with longer sequences.","To surmount the computational challenges, we propose GeminiFusion, a pixel-wise fusion approach that capitalizes on aligned cross-modal representations.","GeminiFusion elegantly combines intra-modal and inter-modal attentions, dynamically integrating complementary information across modalities.","We employ a layer-adaptive noise to adaptively control their interplay on a per-layer basis, thereby achieving a harmonized fusion process.","Notably, GeminiFusion maintains linear complexity with respect to the number of input tokens, ensuring this multimodal framework operates with efficiency comparable to unimodal networks.","Comprehensive evaluations across multimodal image-to-image translation, 3D object detection and arbitrary-modal semantic segmentation tasks, including RGB, depth, LiDAR, event data, etc. demonstrate the superior performance of our GeminiFusion against leading-edge techniques.","The PyTorch code is available at https://github.com/JiaDingCN/GeminiFusion"],"url":"http://arxiv.org/abs/2406.01210v2","category":"cs.CV"}
{"created":"2024-06-03 10:54:58","title":"Sparsity-Agnostic Linear Bandits with Adaptive Adversaries","abstract":"We study stochastic linear bandits where, in each round, the learner receives a set of actions (i.e., feature vectors), from which it chooses an element and obtains a stochastic reward. The expected reward is a fixed but unknown linear function of the chosen action. We study sparse regret bounds, that depend on the number $S$ of non-zero coefficients in the linear reward function. Previous works focused on the case where $S$ is known, or the action sets satisfy additional assumptions. In this work, we obtain the first sparse regret bounds that hold when $S$ is unknown and the action sets are adversarially generated. Our techniques combine online to confidence set conversions with a novel randomized model selection approach over a hierarchy of nested confidence sets. When $S$ is known, our analysis recovers state-of-the-art bounds for adversarial action sets. We also show that a variant of our approach, using Exp3 to dynamically select the confidence sets, can be used to improve the empirical performance of stochastic linear bandits while enjoying a regret bound with optimal dependence on the time horizon.","sentences":["We study stochastic linear bandits where, in each round, the learner receives a set of actions (i.e., feature vectors), from which it chooses an element and obtains a stochastic reward.","The expected reward is a fixed but unknown linear function of the chosen action.","We study sparse regret bounds, that depend on the number $S$ of non-zero coefficients in the linear reward function.","Previous works focused on the case where $S$ is known, or the action sets satisfy additional assumptions.","In this work, we obtain the first sparse regret bounds that hold when $S$ is unknown and the action sets are adversarially generated.","Our techniques combine online to confidence set conversions with a novel randomized model selection approach over a hierarchy of nested confidence sets.","When $S$ is known, our analysis recovers state-of-the-art bounds for adversarial action sets.","We also show that a variant of our approach, using Exp3 to dynamically select the confidence sets, can be used to improve the empirical performance of stochastic linear bandits while enjoying a regret bound with optimal dependence on the time horizon."],"url":"http://arxiv.org/abs/2406.01192v1","category":"cs.LG"}
{"created":"2024-06-03 10:51:43","title":"MultiMax: Sparse and Multi-Modal Attention Learning","abstract":"SoftMax is a ubiquitous ingredient of modern machine learning algorithms. It maps an input vector onto a probability simplex and reweights the input by concentrating the probability mass at large entries. Yet, as a smooth approximation to the Argmax function, a significant amount of probability mass is distributed to other, residual entries, leading to poor interpretability and noise. Although sparsity can be achieved by a family of SoftMax variants, they often require an alternative loss function and do not preserve multi-modality. We show that this trade-off between multi-modality and sparsity limits the expressivity of SoftMax as well as its variants. We provide a solution to this tension between objectives by proposing a piece-wise differentiable function, termed MultiMax, which adaptively modulates the output distribution according to input entry range. Through comprehensive analysis and evaluation, we show that MultiMax successfully produces a distribution that supresses irrelevant entries while preserving multimodality, with benefits in image classification, language modeling and machine translation. The code is available at https://github.com/ZhouYuxuanYX/MultiMax.","sentences":["SoftMax is a ubiquitous ingredient of modern machine learning algorithms.","It maps an input vector onto a probability simplex and reweights the input by concentrating the probability mass at large entries.","Yet, as a smooth approximation to the Argmax function, a significant amount of probability mass is distributed to other, residual entries, leading to poor interpretability and noise.","Although sparsity can be achieved by a family of SoftMax variants, they often require an alternative loss function and do not preserve multi-modality.","We show that this trade-off between multi-modality and sparsity limits the expressivity of SoftMax as well as its variants.","We provide a solution to this tension between objectives by proposing a piece-wise differentiable function, termed MultiMax, which adaptively modulates the output distribution according to input entry range.","Through comprehensive analysis and evaluation, we show that MultiMax successfully produces a distribution that supresses irrelevant entries while preserving multimodality, with benefits in image classification, language modeling and machine translation.","The code is available at https://github.com/ZhouYuxuanYX/MultiMax."],"url":"http://arxiv.org/abs/2406.01189v2","category":"cs.LG"}
{"created":"2024-06-03 10:08:23","title":"Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization","abstract":"Recently, methods investigating how to adapt large language models (LLMs) for specific scenarios have gained great attention. Particularly, the concept of \\textit{persona}, originally adopted in dialogue literature, has re-surged as a promising avenue. However, the growing research on persona is relatively disorganized, lacking a systematic overview. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. To the best of our knowledge, we present the first survey tailored for LLM role-playing and LLM personalization under the unified view of persona, including taxonomy, current challenges, and potential directions. To foster future endeavors, we actively maintain a paper collection available to the community: https://github.com/MiuLab/PersonaLLM-Survey","sentences":["Recently, methods investigating how to adapt large language models (LLMs) for specific scenarios have gained great attention.","Particularly, the concept of \\textit{persona}, originally adopted in dialogue literature, has re-surged as a promising avenue.","However, the growing research on persona is relatively disorganized, lacking a systematic overview.","To close the gap, we present a comprehensive survey to categorize the current state of the field.","We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas.","To the best of our knowledge, we present the first survey tailored for LLM role-playing and LLM personalization under the unified view of persona, including taxonomy, current challenges, and potential directions.","To foster future endeavors, we actively maintain a paper collection available to the community: https://github.com/MiuLab/PersonaLLM-Survey"],"url":"http://arxiv.org/abs/2406.01171v1","category":"cs.CL"}
{"created":"2024-06-03 10:06:45","title":"Current and future applications of PVDF-carbon nanomaterials in energy and sensing","abstract":"The review unveils the diverse applications of concerted polyvinylidene fluoride (PVDF)-carbon nanomaterial (CNM) systems, spanning from electromagnetic interference shielding, including elimination of 5G-interference, to piezoelectrics and a variety of sensing modalities (breathing, movement, health monitoring, structural integrity assessments, home monitoring, and seismic acceleration). These materials also excel in biomaterials with applications like tactile skin and COVID-preventing facemasks through sunlight sterilization. Moreover, PVDF-CNMs demonstrate excellence in radar absorption, solar-assisted electricity generation, triboelectric energy harvesting, 3D-4D printing materials, anti-icing covers, anti-stealth materials, and heat-dissipating solids in electronics. Across diverse scientific disciplines, the research merges materials chemistry and engineering, yielding materials with multimodal functionalities. The demand for a comprehensive review stems from the need to synthesize insights from fundamental sciences and technologies, capturing the cutting-edge nature of these materials.","sentences":["The review unveils the diverse applications of concerted polyvinylidene fluoride (PVDF)-carbon nanomaterial (CNM) systems, spanning from electromagnetic interference shielding, including elimination of 5G-interference, to piezoelectrics and a variety of sensing modalities (breathing, movement, health monitoring, structural integrity assessments, home monitoring, and seismic acceleration).","These materials also excel in biomaterials with applications like tactile skin and COVID-preventing facemasks through sunlight sterilization.","Moreover, PVDF-CNMs demonstrate excellence in radar absorption, solar-assisted electricity generation, triboelectric energy harvesting, 3D-4D printing materials, anti-icing covers, anti-stealth materials, and heat-dissipating solids in electronics.","Across diverse scientific disciplines, the research merges materials chemistry and engineering, yielding materials with multimodal functionalities.","The demand for a comprehensive review stems from the need to synthesize insights from fundamental sciences and technologies, capturing the cutting-edge nature of these materials."],"url":"http://arxiv.org/abs/2406.01169v1","category":"physics.chem-ph"}
{"created":"2024-06-03 09:57:18","title":"When to Sense and Control? A Time-adaptive Approach for Continuous-Time RL","abstract":"Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP). However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice. In many applications, such as greenhouse control or medical treatments, each interaction (measurement or switching of action) involves manual intervention and thus is inherently costly. Therefore, we generally prefer a time-adaptive approach with fewer interactions with the system. In this work, we formalize an RL framework, Time-adaptive Control & Sensing (TaCoS), that tackles this challenge by optimizing over policies that besides control predict the duration of its application. Our formulation results in an extended MDP that any standard RL algorithm can solve. We demonstrate that state-of-the-art RL algorithms trained on TaCoS drastically reduce the interaction amount over their discrete-time counterpart while retaining the same or improved performance, and exhibiting robustness over discretization frequency. Finally, we propose OTaCoS, an efficient model-based algorithm for our setting. We show that OTaCoS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains.","sentences":["Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP).","However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice.","In many applications, such as greenhouse control or medical treatments, each interaction (measurement or switching of action) involves manual intervention and thus is inherently costly.","Therefore, we generally prefer a time-adaptive approach with fewer interactions with the system.","In this work, we formalize an RL framework, Time-adaptive Control & Sensing (TaCoS), that tackles this challenge by optimizing over policies that besides control predict the duration of its application.","Our formulation results in an extended MDP that any standard RL algorithm can solve.","We demonstrate that state-of-the-art RL algorithms trained on TaCoS drastically reduce the interaction amount over their discrete-time counterpart while retaining the same or improved performance, and exhibiting robustness over discretization frequency.","Finally, we propose OTaCoS, an efficient model-based algorithm for our setting.","We show that OTaCoS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains."],"url":"http://arxiv.org/abs/2406.01163v2","category":"cs.LG"}
{"created":"2024-06-03 09:51:59","title":"Dimba: Transformer-Mamba Diffusion Models","abstract":"This paper unveils Dimba, a new text-to-image diffusion model that employs a distinctive hybrid architecture combining Transformer and Mamba elements. Specifically, Dimba sequentially stacked blocks alternate between Transformer and Mamba layers, and integrate conditional information through the cross-attention layer, thus capitalizing on the advantages of both architectural paradigms. We investigate several optimization strategies, including quality tuning, resolution adaption, and identify critical configurations necessary for large-scale image generation. The model's flexible design supports scenarios that cater to specific resource constraints and objectives. When scaled appropriately, Dimba offers substantial throughput and a reduced memory footprint relative to conventional pure Transformers-based benchmarks. Extensive experiments indicate that Dimba achieves comparable performance compared with benchmarks in terms of image quality, artistic rendering, and semantic control. We also report several intriguing properties of architecture discovered during evaluation and release checkpoints in experiments. Our findings emphasize the promise of large-scale hybrid Transformer-Mamba architectures in the foundational stage of diffusion models, suggesting a bright future for text-to-image generation.","sentences":["This paper unveils Dimba, a new text-to-image diffusion model that employs a distinctive hybrid architecture combining Transformer and Mamba elements.","Specifically, Dimba sequentially stacked blocks alternate between Transformer and Mamba layers, and integrate conditional information through the cross-attention layer, thus capitalizing on the advantages of both architectural paradigms.","We investigate several optimization strategies, including quality tuning, resolution adaption, and identify critical configurations necessary for large-scale image generation.","The model's flexible design supports scenarios that cater to specific resource constraints and objectives.","When scaled appropriately, Dimba offers substantial throughput and a reduced memory footprint relative to conventional pure Transformers-based benchmarks.","Extensive experiments indicate that Dimba achieves comparable performance compared with benchmarks in terms of image quality, artistic rendering, and semantic control.","We also report several intriguing properties of architecture discovered during evaluation and release checkpoints in experiments.","Our findings emphasize the promise of large-scale hybrid Transformer-Mamba architectures in the foundational stage of diffusion models, suggesting a bright future for text-to-image generation."],"url":"http://arxiv.org/abs/2406.01159v1","category":"cs.CV"}
{"created":"2024-06-03 09:51:03","title":"Local structural distortions drive magnetic molecular field in a compositionally complex spinel oxide","abstract":"A core challenge in understanding high entropy oxides (HEOs) is how these systems, with five or more cations at a crystallographic site, can withstand local distortions while preserving a uniform structure on a larger scale. We address this for spinel HEO by comparatively examining extended X-ray absorption fine structure (EXAFS) on (Mn$_{0.2}$Co$_{0.2}$Ni$_{0.2}$Cu$_{0.2}$Zn$_{0.2}$)Cr$_2$O$_4$ ($A^5$Cr$_2$O$_4$) and its parent counterparts $A$Cr$_2$O$_4$ ($A$= Mn, Co, Ni, Cu, Zn). Unlike the HEO with rock-salt structure, the element-specific distortions in disordered sublattice go beyond the first neighbor here. Moreover, the tetragonal distortion around the Cu$^{2+}$ ion, known as a textbook example of the Jahn-Teller effect, is highly reduced in $A^5$Cr$_2$O$_4$ compared to CuCr$_2$O$_4$. Despite variations in the A-O bond lengths, the inter-cationic distances remained remarkably similar. This affirms a high level of flexibility in the positioning of oxygen, enabling them to adapt to the overall cubic symmetry. Despite containing multiple magnetic ions, the Curie-Weiss temperature and effective magnetic moments of $A^5$Cr$_2$O$_4$ are similar to those of NiCr$_2$O$_4$. This can be attributed to both materials' comparable local bond lengths around Cr, as evidenced by EXAFS analysis. This study conclusively presents a method for elucidating how local structural distortions influence the macroscopic properties of compositionally complex quantum materials.","sentences":["A core challenge in understanding high entropy oxides (HEOs) is how these systems, with five or more cations at a crystallographic site, can withstand local distortions while preserving a uniform structure on a larger scale.","We address this for spinel HEO by comparatively examining extended X-ray absorption fine structure (EXAFS) on (Mn$_{0.2}$Co$_{0.2}$Ni$_{0.2}$Cu$_{0.2}$Zn$_{0.2}$)Cr$_2$O$_4$ ($A^5$Cr$_2$O$_4$) and its parent counterparts $A$Cr$_2$O$_4$ ($A$= Mn, Co, Ni, Cu, Zn).","Unlike the HEO with rock-salt structure, the element-specific distortions in disordered sublattice go beyond the first neighbor here.","Moreover, the tetragonal distortion around the Cu$^{2+}$ ion, known as a textbook example of the Jahn-Teller effect, is highly reduced in $A^5$Cr$_2$O$_4$ compared to CuCr$_2$O$_4$. Despite variations in the A-O bond lengths, the inter-cationic distances remained remarkably similar.","This affirms a high level of flexibility in the positioning of oxygen, enabling them to adapt to the overall cubic symmetry.","Despite containing multiple magnetic ions, the Curie-Weiss temperature and effective magnetic moments of $A^5$Cr$_2$O$_4$ are similar to those of NiCr$_2$O$_4$.","This can be attributed to both materials' comparable local bond lengths around Cr, as evidenced by EXAFS analysis.","This study conclusively presents a method for elucidating how local structural distortions influence the macroscopic properties of compositionally complex quantum materials."],"url":"http://arxiv.org/abs/2406.01156v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-03 09:11:34","title":"Learning Adaptive Fusion Bank for Multi-modal Salient Object Detection","abstract":"Multi-modal salient object detection (MSOD) aims to boost saliency detection performance by integrating visible sources with depth or thermal infrared ones. Existing methods generally design different fusion schemes to handle certain issues or challenges. Although these fusion schemes are effective at addressing specific issues or challenges, they may struggle to handle multiple complex challenges simultaneously. To solve this problem, we propose a novel adaptive fusion bank that makes full use of the complementary benefits from a set of basic fusion schemes to handle different challenges simultaneously for robust MSOD. We focus on handling five major challenges in MSOD, namely center bias, scale variation, image clutter, low illumination, and thermal crossover or depth ambiguity. The fusion bank proposed consists of five representative fusion schemes, which are specifically designed based on the characteristics of each challenge, respectively. The bank is scalable, and more fusion schemes could be incorporated into the bank for more challenges. To adaptively select the appropriate fusion scheme for multi-modal input, we introduce an adaptive ensemble module that forms the adaptive fusion bank, which is embedded into hierarchical layers for sufficient fusion of different source data. Moreover, we design an indirect interactive guidance module to accurately detect salient hollow objects via the skip integration of high-level semantic information and low-level spatial details. Extensive experiments on three RGBT datasets and seven RGBD datasets demonstrate that the proposed method achieves the outstanding performance compared to the state-of-the-art methods. The code and results are available at https://github.com/Angknpng/LAFB.","sentences":["Multi-modal salient object detection (MSOD) aims to boost saliency detection performance by integrating visible sources with depth or thermal infrared ones.","Existing methods generally design different fusion schemes to handle certain issues or challenges.","Although these fusion schemes are effective at addressing specific issues or challenges, they may struggle to handle multiple complex challenges simultaneously.","To solve this problem, we propose a novel adaptive fusion bank that makes full use of the complementary benefits from a set of basic fusion schemes to handle different challenges simultaneously for robust MSOD.","We focus on handling five major challenges in MSOD, namely center bias, scale variation, image clutter, low illumination, and thermal crossover or depth ambiguity.","The fusion bank proposed consists of five representative fusion schemes, which are specifically designed based on the characteristics of each challenge, respectively.","The bank is scalable, and more fusion schemes could be incorporated into the bank for more challenges.","To adaptively select the appropriate fusion scheme for multi-modal input, we introduce an adaptive ensemble module that forms the adaptive fusion bank, which is embedded into hierarchical layers for sufficient fusion of different source data.","Moreover, we design an indirect interactive guidance module to accurately detect salient hollow objects via the skip integration of high-level semantic information and low-level spatial details.","Extensive experiments on three RGBT datasets and seven RGBD datasets demonstrate that the proposed method achieves the outstanding performance compared to the state-of-the-art methods.","The code and results are available at https://github.com/Angknpng/LAFB."],"url":"http://arxiv.org/abs/2406.01127v1","category":"cs.CV"}
{"created":"2024-06-03 09:10:42","title":"Latent Logic Tree Extraction for Event Sequence Explanation from LLMs","abstract":"Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.","sentences":["Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences.","Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence.","Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees.","We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables.","In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences.","LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution.","We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables.","The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters.","In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations.","Empirical demonstrations showcase the promising performance and adaptability of our framework."],"url":"http://arxiv.org/abs/2406.01124v2","category":"cs.LG"}
{"created":"2024-06-03 08:12:09","title":"FedAdOb: Privacy-Preserving Federated Deep Learning with Adaptive Obfuscation","abstract":"Federated learning (FL) has emerged as a collaborative approach that allows multiple clients to jointly learn a machine learning model without sharing their private data. The concern about privacy leakage, albeit demonstrated under specific conditions, has triggered numerous follow-up research in designing powerful attacking methods and effective defending mechanisms aiming to thwart these attacking methods. Nevertheless, privacy-preserving mechanisms employed in these defending methods invariably lead to compromised model performances due to a fixed obfuscation applied to private data or gradients. In this article, we, therefore, propose a novel adaptive obfuscation mechanism, coined FedAdOb, to protect private data without yielding original model performances. Technically, FedAdOb utilizes passport-based adaptive obfuscation to ensure data privacy in both horizontal and vertical federated learning settings. The privacy-preserving capabilities of FedAdOb, specifically with regard to private features and labels, are theoretically proven through Theorems 1 and 2. Furthermore, extensive experimental evaluations conducted on various datasets and network architectures demonstrate the effectiveness of FedAdOb by manifesting its superior trade-off between privacy preservation and model performance, surpassing existing methods.","sentences":["Federated learning (FL) has emerged as a collaborative approach that allows multiple clients to jointly learn a machine learning model without sharing their private data.","The concern about privacy leakage, albeit demonstrated under specific conditions, has triggered numerous follow-up research in designing powerful attacking methods and effective defending mechanisms aiming to thwart these attacking methods.","Nevertheless, privacy-preserving mechanisms employed in these defending methods invariably lead to compromised model performances due to a fixed obfuscation applied to private data or gradients.","In this article, we, therefore, propose a novel adaptive obfuscation mechanism, coined FedAdOb, to protect private data without yielding original model performances.","Technically, FedAdOb utilizes passport-based adaptive obfuscation to ensure data privacy in both horizontal and vertical federated learning settings.","The privacy-preserving capabilities of FedAdOb, specifically with regard to private features and labels, are theoretically proven through Theorems 1 and 2.","Furthermore, extensive experimental evaluations conducted on various datasets and network architectures demonstrate the effectiveness of FedAdOb by manifesting its superior trade-off between privacy preservation and model performance, surpassing existing methods."],"url":"http://arxiv.org/abs/2406.01085v1","category":"cs.CR"}
{"created":"2024-06-03 08:00:18","title":"Adapting coherent-state superpositions in noisy channels","abstract":"Quantum non-Gaussian states are crucial for the fundamental understanding of non-linear bosonic systems and simultaneously advanced applications in quantum technologies. In many bosonic experiments the important quantum non-Gaussian feature is the negativity of the Wigner function, a cornerstone for quantum computation with bosons. Unfortunately, the negativities present in complex quantum states are extremely vulnerable to the effects of decoherence, such as energy loss, noise and dephasing, caused by the coupling to the environment, which is an unavoidable part of any experimental implementation. An efficient way to mitigate its effects is by adapting quantum states into more resilient forms. We propose an optimal protection of superpositions of coherent states against a sequence of asymmetric thermal lossy channels by suitable squeezing operations.","sentences":["Quantum non-Gaussian states are crucial for the fundamental understanding of non-linear bosonic systems and simultaneously advanced applications in quantum technologies.","In many bosonic experiments the important quantum non-Gaussian feature is the negativity of the Wigner function, a cornerstone for quantum computation with bosons.","Unfortunately, the negativities present in complex quantum states are extremely vulnerable to the effects of decoherence, such as energy loss, noise and dephasing, caused by the coupling to the environment, which is an unavoidable part of any experimental implementation.","An efficient way to mitigate its effects is by adapting quantum states into more resilient forms.","We propose an optimal protection of superpositions of coherent states against a sequence of asymmetric thermal lossy channels by suitable squeezing operations."],"url":"http://arxiv.org/abs/2406.01081v1","category":"quant-ph"}
{"created":"2024-06-03 07:58:40","title":"Object Aware Egocentric Online Action Detection","abstract":"Advancements in egocentric video datasets like Ego4D, EPIC-Kitchens, and Ego-Exo4D have enriched the study of first-person human interactions, which is crucial for applications in augmented reality and assisted living. Despite these advancements, current Online Action Detection methods, which efficiently detect actions in streaming videos, are predominantly designed for exocentric views and thus fail to capitalize on the unique perspectives inherent to egocentric videos. To address this gap, we introduce an Object-Aware Module that integrates egocentric-specific priors into existing OAD frameworks, enhancing first-person footage interpretation. Utilizing object-specific details and temporal dynamics, our module improves scene understanding in detecting actions. Validated extensively on the Epic-Kitchens 100 dataset, our work can be seamlessly integrated into existing models with minimal overhead and bring consistent performance enhancements, marking an important step forward in adapting action detection systems to egocentric video analysis.","sentences":["Advancements in egocentric video datasets like Ego4D, EPIC-Kitchens, and Ego-Exo4D have enriched the study of first-person human interactions, which is crucial for applications in augmented reality and assisted living.","Despite these advancements, current Online Action Detection methods, which efficiently detect actions in streaming videos, are predominantly designed for exocentric views and thus fail to capitalize on the unique perspectives inherent to egocentric videos.","To address this gap, we introduce an Object-Aware Module that integrates egocentric-specific priors into existing OAD frameworks, enhancing first-person footage interpretation.","Utilizing object-specific details and temporal dynamics, our module improves scene understanding in detecting actions.","Validated extensively on the Epic-Kitchens 100 dataset, our work can be seamlessly integrated into existing models with minimal overhead and bring consistent performance enhancements, marking an important step forward in adapting action detection systems to egocentric video analysis."],"url":"http://arxiv.org/abs/2406.01079v1","category":"cs.CV"}
{"created":"2024-06-03 07:58:08","title":"Hybrid Quadratic Programming -- Pullback Bundle Dynamical Systems Control","abstract":"Dynamical System (DS)-based closed-loop control is a simple and effective way to generate reactive motion policies that well generalize to the robotic workspace, while retaining stability guarantees. Lately the formalism has been expanded in order to handle arbitrary geometry curved spaces, namely manifolds, beyond the standard flat Euclidean space. Despite the many different ways proposed to handle DS on manifolds, it is still unclear how to apply such structures on real robotic systems. In this preliminary study, we propose a way to combine modern optimal control techniques with a geometry-based formulation of DS. The advantage of such approach is two fold. First, it yields a torque-based control for compliant and adaptive motions; second, it generates dynamical systems consistent with the controlled system's dynamics. The salient point of the approach is that the complexity of designing a proper constrained-based optimal control problem, to ensure that dynamics move on a manifold while avoiding obstacles or self-collisions, is \"outsourced\" to the geometric DS. Constraints are implicitly embedded into the structure of the space in which the DS evolves. The optimal control, on the other hand, provides a torque-based control interface, and ensures dynamical consistency of the generated output. The whole can be achieved with minimal computational overhead since most of the computational complexity is delegated to the closed-form geometric DS.","sentences":["Dynamical System (DS)-based closed-loop control is a simple and effective way to generate reactive motion policies that well generalize to the robotic workspace, while retaining stability guarantees.","Lately the formalism has been expanded in order to handle arbitrary geometry curved spaces, namely manifolds, beyond the standard flat Euclidean space.","Despite the many different ways proposed to handle DS on manifolds, it is still unclear how to apply such structures on real robotic systems.","In this preliminary study, we propose a way to combine modern optimal control techniques with a geometry-based formulation of DS.","The advantage of such approach is two fold.","First, it yields a torque-based control for compliant and adaptive motions; second, it generates dynamical systems consistent with the controlled system's dynamics.","The salient point of the approach is that the complexity of designing a proper constrained-based optimal control problem, to ensure that dynamics move on a manifold while avoiding obstacles or self-collisions, is \"outsourced\" to the geometric DS.","Constraints are implicitly embedded into the structure of the space in which the DS evolves.","The optimal control, on the other hand, provides a torque-based control interface, and ensures dynamical consistency of the generated output.","The whole can be achieved with minimal computational overhead since most of the computational complexity is delegated to the closed-form geometric DS."],"url":"http://arxiv.org/abs/2406.01077v1","category":"cs.RO"}
{"created":"2024-06-03 07:44:37","title":"Towards Efficient Deep Spiking Neural Networks Construction with Spiking Activity based Pruning","abstract":"The emergence of deep and large-scale spiking neural networks (SNNs) exhibiting high performance across diverse complex datasets has led to a need for compressing network models due to the presence of a significant number of redundant structural units, aiming to more effectively leverage their low-power consumption and biological interpretability advantages. Currently, most model compression techniques for SNNs are based on unstructured pruning of individual connections, which requires specific hardware support. Hence, we propose a structured pruning approach based on the activity levels of convolutional kernels named Spiking Channel Activity-based (SCA) network pruning framework. Inspired by synaptic plasticity mechanisms, our method dynamically adjusts the network's structure by pruning and regenerating convolutional kernels during training, enhancing the model's adaptation to the current target task. While maintaining model performance, this approach refines the network architecture, ultimately reducing computational load and accelerating the inference process. This indicates that structured dynamic sparse learning methods can better facilitate the application of deep SNNs in low-power and high-efficiency scenarios.","sentences":["The emergence of deep and large-scale spiking neural networks (SNNs) exhibiting high performance across diverse complex datasets has led to a need for compressing network models due to the presence of a significant number of redundant structural units, aiming to more effectively leverage their low-power consumption and biological interpretability advantages.","Currently, most model compression techniques for SNNs are based on unstructured pruning of individual connections, which requires specific hardware support.","Hence, we propose a structured pruning approach based on the activity levels of convolutional kernels named Spiking Channel Activity-based (SCA) network pruning framework.","Inspired by synaptic plasticity mechanisms, our method dynamically adjusts the network's structure by pruning and regenerating convolutional kernels during training, enhancing the model's adaptation to the current target task.","While maintaining model performance, this approach refines the network architecture, ultimately reducing computational load and accelerating the inference process.","This indicates that structured dynamic sparse learning methods can better facilitate the application of deep SNNs in low-power and high-efficiency scenarios."],"url":"http://arxiv.org/abs/2406.01072v1","category":"cs.NE"}
{"created":"2024-06-03 07:40:10","title":"UniQA: Unified Vision-Language Pre-training for Image Quality and Aesthetic Assessment","abstract":"Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA) aim to simulate human subjective perception of image visual quality and aesthetic appeal. Existing methods typically address these tasks independently due to distinct learning objectives. However, they neglect the underlying interconnectedness of both tasks, which hinders the learning of task-agnostic shared representations for human subjective perception. To confront this challenge, we propose Unified vision-language pre-training of Quality and Aesthetics (UniQA), to learn general perceptions of two tasks, thereby benefiting them simultaneously. Addressing the absence of text in the IQA datasets and the presence of textual noise in the IAA datasets, (1) we utilize multimodal large language models (MLLMs) to generate high-quality text descriptions; (2) the generated text for IAA serves as metadata to purify noisy IAA data. To effectively adapt the pre-trained UniQA to downstream tasks, we further propose a lightweight adapter that utilizes versatile cues to fully exploit the extensive knowledge of the pre-trained model. Extensive experiments demonstrate that our approach attains a new state-of-the-art performance on both IQA and IAA tasks, while concurrently showcasing exceptional zero-shot and few-label image assessment capabilities. The source code will be available at https://github.com/zht8506/UniQA.","sentences":["Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA) aim to simulate human subjective perception of image visual quality and aesthetic appeal.","Existing methods typically address these tasks independently due to distinct learning objectives.","However, they neglect the underlying interconnectedness of both tasks, which hinders the learning of task-agnostic shared representations for human subjective perception.","To confront this challenge, we propose Unified vision-language pre-training of Quality and Aesthetics (UniQA), to learn general perceptions of two tasks, thereby benefiting them simultaneously.","Addressing the absence of text in the IQA datasets and the presence of textual noise in the IAA datasets, (1) we utilize multimodal large language models (MLLMs) to generate high-quality text descriptions; (2) the generated text for IAA serves as metadata to purify noisy IAA data.","To effectively adapt the pre-trained UniQA to downstream tasks, we further propose a lightweight adapter that utilizes versatile cues to fully exploit the extensive knowledge of the pre-trained model.","Extensive experiments demonstrate that our approach attains a new state-of-the-art performance on both IQA and IAA tasks, while concurrently showcasing exceptional zero-shot and few-label image assessment capabilities.","The source code will be available at https://github.com/zht8506/UniQA."],"url":"http://arxiv.org/abs/2406.01069v1","category":"cs.CV"}
{"created":"2024-06-03 07:15:57","title":"Mechanical dynamics around higher-order exceptional point in magno-optomechanics","abstract":"We theoretically study diverse exceptional points (EPs) in an experimentally feasible magno-optomechanics consisting of an optomechanical subsystem coupled to a magnomechanical subsystem via physically direct contact. By adiabatically eliminating both the cavity and the Kittel mode, dissipative and parity-time symmetric exceptional points can be observed. When only the cavity mode is eliminated, a second (third) -order pseudo-Hermitian EP emerges for nondegenerate (degenerate) mechanical modes. The distinct dynamical behavior of two mechanical modes around these EPs are further studied. Our proposal provides a promising way to engineer diverse EPs and quantify non-Hermitian phase transition with exceptional dynamical behavior in magno-optomechanics.","sentences":["We theoretically study diverse exceptional points (EPs) in an experimentally feasible magno-optomechanics consisting of an optomechanical subsystem coupled to a magnomechanical subsystem via physically direct contact.","By adiabatically eliminating both the cavity and the Kittel mode, dissipative and parity-time symmetric exceptional points can be observed.","When only the cavity mode is eliminated, a second (third) -order pseudo-Hermitian EP emerges for nondegenerate (degenerate) mechanical modes.","The distinct dynamical behavior of two mechanical modes around these EPs are further studied.","Our proposal provides a promising way to engineer diverse EPs and quantify non-Hermitian phase transition with exceptional dynamical behavior in magno-optomechanics."],"url":"http://arxiv.org/abs/2406.01060v1","category":"quant-ph"}
{"created":"2024-06-03 07:08:27","title":"Confidence-Based Task Prediction in Continual Disease Classification Using Probability Distribution","abstract":"Deep learning models are widely recognized for their effectiveness in identifying medical image findings in disease classification. However, their limitations become apparent in the dynamic and ever-changing clinical environment, characterized by the continuous influx of newly annotated medical data from diverse sources. In this context, the need for continual learning becomes particularly paramount, not only to adapt to evolving medical scenarios but also to ensure the privacy of healthcare data. In our research, we emphasize the utilization of a network comprising expert classifiers, where a new expert classifier is added each time a new task is introduced. We present CTP, a task-id predictor that utilizes confidence scores, leveraging the probability distribution (logits) of the classifier to accurately determine the task-id at inference time. Logits are adjusted to ensure that classifiers yield a high-entropy distribution for data associated with tasks other than their own. By defining a noise region in the distribution and computing confidence scores, CTP achieves superior performance when compared to other relevant continual learning methods. Additionally, the performance of CTP can be further improved by providing it with a continuum of data at the time of inference.","sentences":["Deep learning models are widely recognized for their effectiveness in identifying medical image findings in disease classification.","However, their limitations become apparent in the dynamic and ever-changing clinical environment, characterized by the continuous influx of newly annotated medical data from diverse sources.","In this context, the need for continual learning becomes particularly paramount, not only to adapt to evolving medical scenarios but also to ensure the privacy of healthcare data.","In our research, we emphasize the utilization of a network comprising expert classifiers, where a new expert classifier is added each time a new task is introduced.","We present CTP, a task-id predictor that utilizes confidence scores, leveraging the probability distribution (logits) of the classifier to accurately determine the task-id at inference time.","Logits are adjusted to ensure that classifiers yield a high-entropy distribution for data associated with tasks other than their own.","By defining a noise region in the distribution and computing confidence scores, CTP achieves superior performance when compared to other relevant continual learning methods.","Additionally, the performance of CTP can be further improved by providing it with a continuum of data at the time of inference."],"url":"http://arxiv.org/abs/2406.01054v1","category":"cs.LG"}
{"created":"2024-06-03 06:55:10","title":"Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs","abstract":"Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method's superior performance compared to baseline approaches.","sentences":["Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making.","However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content.","This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction.","Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation.","Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method's superior performance compared to baseline approaches."],"url":"http://arxiv.org/abs/2406.01045v1","category":"cs.CL"}
{"created":"2024-06-03 06:07:44","title":"Combining Qualitative and Computational Approaches for Literary Analysis of Finnish Novels","abstract":"What can we learn from the classics of Finnish literature by using computational emotion analysis? This article tries to answer this question by examining how computational methods of sentiment analysis can be used in the study of literary works in conjunction with a qualitative or more 'traditional' approach to literature and affect. We present and develop a simple but robust computational approach of affect analysis that uses a carefully curated emotion lexicon adapted to Finnish turn-of-the-century literary texts combined with word embeddings to map out the semantic emotional spaces of seminal works of Finnish literature. We focus our qualitative analysis on selected case studies: four works by Juhani Aho, Minna Canth, Maria Jotuni, and F. E. Sillanp\\\"a\\\"a, but provide emotion arcs for a total of 975 Finnish novels.   We argue that a computational analysis of a text's lexicon can be valuable in evaluating the large distribution of the emotional valence in a text and provide guidelines to help other researchers replicate our findings. We show that computational approaches have a place in traditional studies on affect in literature as a support tool for close-reading-based analyses, but also allowing for large-scale comparison between, for example, genres or national canons.","sentences":["What can we learn from the classics of Finnish literature by using computational emotion analysis?","This article tries to answer this question by examining how computational methods of sentiment analysis can be used in the study of literary works in conjunction with a qualitative or more 'traditional' approach to literature and affect.","We present and develop a simple but robust computational approach of affect analysis that uses a carefully curated emotion lexicon adapted to Finnish turn-of-the-century literary texts combined with word embeddings to map out the semantic emotional spaces of seminal works of Finnish literature.","We focus our qualitative analysis on selected case studies: four works by Juhani Aho, Minna Canth, Maria Jotuni, and F. E. Sillanp\\\"a\\\"a, but provide emotion arcs for a total of 975 Finnish novels.   ","We argue that a computational analysis of a text's lexicon can be valuable in evaluating the large distribution of the emotional valence in a text and provide guidelines to help other researchers replicate our findings.","We show that computational approaches have a place in traditional studies on affect in literature as a support tool for close-reading-based analyses, but also allowing for large-scale comparison between, for example, genres or national canons."],"url":"http://arxiv.org/abs/2406.01021v1","category":"cs.CL"}
{"created":"2024-06-03 05:46:52","title":"Attention-based Iterative Decomposition for Tensor Product Representation","abstract":"In recent research, Tensor Product Representation (TPR) is applied for the systematic generalization task of deep neural networks by learning the compositional structure of data. However, such prior works show limited performance in discovering and representing the symbolic structure from unseen test data because their decomposition to the structural representations was incomplete. In this work, we propose an Attention-based Iterative Decomposition (AID) module designed to enhance the decomposition operations for the structured representations encoded from the sequential input data with TPR. Our AID can be easily adapted to any TPR-based model and provides enhanced systematic decomposition through a competitive attention mechanism between input features and structured representations. In our experiments, AID shows effectiveness by significantly improving the performance of TPR-based prior works on the series of systematic generalization tasks. Moreover, in the quantitative and qualitative evaluations, AID produces more compositional and well-bound structural representations than other works.","sentences":["In recent research, Tensor Product Representation (TPR) is applied for the systematic generalization task of deep neural networks by learning the compositional structure of data.","However, such prior works show limited performance in discovering and representing the symbolic structure from unseen test data because their decomposition to the structural representations was incomplete.","In this work, we propose an Attention-based Iterative Decomposition (AID) module designed to enhance the decomposition operations for the structured representations encoded from the sequential input data with TPR.","Our AID can be easily adapted to any TPR-based model and provides enhanced systematic decomposition through a competitive attention mechanism between input features and structured representations.","In our experiments, AID shows effectiveness by significantly improving the performance of TPR-based prior works on the series of systematic generalization tasks.","Moreover, in the quantitative and qualitative evaluations, AID produces more compositional and well-bound structural representations than other works."],"url":"http://arxiv.org/abs/2406.01012v1","category":"cs.LG"}
{"created":"2024-06-03 05:25:50","title":"Uni-ISP: Unifying the Learning of ISPs from Multiple Cameras","abstract":"Modern end-to-end image signal processors (ISPs) can learn complex mappings from RAW/XYZ data to sRGB (or inverse), opening new possibilities in image processing. However, as the diversity of camera models continues to expand, developing and maintaining individual ISPs is not sustainable in the long term, which inherently lacks versatility, hindering the adaptability to multiple camera models. In this paper, we propose a novel pipeline, Uni-ISP, which unifies the learning of ISPs from multiple cameras, offering an accurate and versatile processor to multiple camera models. The core of Uni-ISP is leveraging device-aware embeddings through learning inverse/forward ISPs and its special training scheme. By doing so, Uni-ISP not only improves the performance of inverse/forward ISPs but also unlocks a variety of new applications inaccessible to existing learned ISPs. Moreover, since there is no dataset synchronously captured by multiple cameras for training, we construct a real-world 4K dataset, FiveCam, comprising more than 2,400 pairs of sRGB-RAW images synchronously captured by five smartphones. We conducted extensive experiments demonstrating Uni-ISP's accuracy in inverse/forward ISPs (with improvements of +1.5dB/2.4dB PSNR), its versatility in enabling new applications, and its adaptability to new camera models.","sentences":["Modern end-to-end image signal processors (ISPs) can learn complex mappings from RAW/XYZ data to sRGB (or inverse), opening new possibilities in image processing.","However, as the diversity of camera models continues to expand, developing and maintaining individual ISPs is not sustainable in the long term, which inherently lacks versatility, hindering the adaptability to multiple camera models.","In this paper, we propose a novel pipeline, Uni-ISP, which unifies the learning of ISPs from multiple cameras, offering an accurate and versatile processor to multiple camera models.","The core of Uni-ISP is leveraging device-aware embeddings through learning inverse/forward ISPs and its special training scheme.","By doing so, Uni-ISP not only improves the performance of inverse/forward ISPs but also unlocks a variety of new applications inaccessible to existing learned ISPs.","Moreover, since there is no dataset synchronously captured by multiple cameras for training, we construct a real-world 4K dataset, FiveCam, comprising more than 2,400 pairs of sRGB-RAW images synchronously captured by five smartphones.","We conducted extensive experiments demonstrating Uni-ISP's accuracy in inverse/forward ISPs (with improvements of +1.5dB/2.4dB PSNR), its versatility in enabling new applications, and its adaptability to new camera models."],"url":"http://arxiv.org/abs/2406.01003v1","category":"cs.CV"}
{"created":"2024-06-03 05:23:32","title":"Random Subspace Local Projections","abstract":"We show how random subspace methods can be adapted to estimating local projections with many controls. Random subspace methods have their roots in the machine learning literature and are implemented by averaging over regressions estimated over different combinations of subsets of these controls. We document three key results: (i) Our approach can successfully recover the impulse response functions across Monte Carlo experiments representative of different macroeconomic settings and identification schemes. (ii) Our results suggest that random subspace methods are more accurate than other dimension reduction methods if the underlying large dataset has a factor structure similar to typical macroeconomic datasets such as FRED-MD. (iii) Our approach leads to differences in the estimated impulse response functions relative to benchmark methods when applied to two widely studied empirical applications.","sentences":["We show how random subspace methods can be adapted to estimating local projections with many controls.","Random subspace methods have their roots in the machine learning literature and are implemented by averaging over regressions estimated over different combinations of subsets of these controls.","We document three key results: (i) Our approach can successfully recover the impulse response functions across Monte Carlo experiments representative of different macroeconomic settings and identification schemes.","(ii) Our results suggest that random subspace methods are more accurate than other dimension reduction methods if the underlying large dataset has a factor structure similar to typical macroeconomic datasets such as FRED-MD.","(iii) Our approach leads to differences in the estimated impulse response functions relative to benchmark methods when applied to two widely studied empirical applications."],"url":"http://arxiv.org/abs/2406.01002v1","category":"econ.EM"}
{"created":"2024-06-03 04:03:24","title":"Cold-start Recommendation by Personalized Embedding Region Elicitation","abstract":"Rating elicitation is a success element for recommender systems to perform well at cold-starting, in which the systems need to recommend items to a newly arrived user with no prior knowledge about the user's preference. Existing elicitation methods employ a fixed set of items to learn the user's preference and then infer the users' preferences on the remaining items. Using a fixed seed set can limit the performance of the recommendation system since the seed set is unlikely optimal for all new users with potentially diverse preferences. This paper addresses this challenge using a 2-phase, personalized elicitation scheme. First, the elicitation scheme asks users to rate a small set of popular items in a ``burn-in'' phase. Second, it sequentially asks the user to rate adaptive items to refine the preference and the user's representation. Throughout the process, the system represents the user's embedding value not by a point estimate but by a region estimate. The value of information obtained by asking the user's rating on an item is quantified by the distance from the region center embedding space that contains with high confidence the true embedding value of the user. Finally, the recommendations are successively generated by considering the preference region of the user. We show that each subproblem in the elicitation scheme can be efficiently implemented. Further, we empirically demonstrate the effectiveness of the proposed method against existing rating-elicitation methods on several prominent datasets.","sentences":["Rating elicitation is a success element for recommender systems to perform well at cold-starting, in which the systems need to recommend items to a newly arrived user with no prior knowledge about the user's preference.","Existing elicitation methods employ a fixed set of items to learn the user's preference and then infer the users' preferences on the remaining items.","Using a fixed seed set can limit the performance of the recommendation system since the seed set is unlikely optimal for all new users with potentially diverse preferences.","This paper addresses this challenge using a 2-phase, personalized elicitation scheme.","First, the elicitation scheme asks users to rate a small set of popular items in a ``burn-in'' phase.","Second, it sequentially asks the user to rate adaptive items to refine the preference and the user's representation.","Throughout the process, the system represents the user's embedding value not by a point estimate but by a region estimate.","The value of information obtained by asking the user's rating on an item is quantified by the distance from the region center embedding space that contains with high confidence the true embedding value of the user.","Finally, the recommendations are successively generated by considering the preference region of the user.","We show that each subproblem in the elicitation scheme can be efficiently implemented.","Further, we empirically demonstrate the effectiveness of the proposed method against existing rating-elicitation methods on several prominent datasets."],"url":"http://arxiv.org/abs/2406.00973v1","category":"cs.IR"}
{"created":"2024-06-03 03:16:25","title":"Improving Segment Anything on the Fly: Auxiliary Online Learning and Adaptive Fusion for Medical Image Segmentation","abstract":"The current variants of the Segment Anything Model (SAM), which include the original SAM and Medical SAM, still lack the capability to produce sufficiently accurate segmentation for medical images. In medical imaging contexts, it is not uncommon for human experts to rectify segmentations of specific test samples after SAM generates its segmentation predictions. These rectifications typically entail manual or semi-manual corrections employing state-of-the-art annotation tools. Motivated by this process, we introduce a novel approach that leverages the advantages of online machine learning to enhance Segment Anything (SA) during test time. We employ rectified annotations to perform online learning, with the aim of improving the segmentation quality of SA on medical images. To improve the effectiveness and efficiency of online learning when integrated with large-scale vision models like SAM, we propose a new method called Auxiliary Online Learning (AuxOL). AuxOL creates and applies a small auxiliary model (specialist) in conjunction with SAM (generalist), entails adaptive online-batch and adaptive segmentation fusion. Experiments conducted on eight datasets covering four medical imaging modalities validate the effectiveness of the proposed method. Our work proposes and validates a new, practical, and effective approach for enhancing SA on downstream segmentation tasks (e.g., medical image segmentation).","sentences":["The current variants of the Segment Anything Model (SAM), which include the original SAM and Medical SAM, still lack the capability to produce sufficiently accurate segmentation for medical images.","In medical imaging contexts, it is not uncommon for human experts to rectify segmentations of specific test samples after SAM generates its segmentation predictions.","These rectifications typically entail manual or semi-manual corrections employing state-of-the-art annotation tools.","Motivated by this process, we introduce a novel approach that leverages the advantages of online machine learning to enhance Segment Anything (SA) during test time.","We employ rectified annotations to perform online learning, with the aim of improving the segmentation quality of SA on medical images.","To improve the effectiveness and efficiency of online learning when integrated with large-scale vision models like SAM, we propose a new method called Auxiliary Online Learning (AuxOL).","AuxOL creates and applies a small auxiliary model (specialist) in conjunction with SAM (generalist), entails adaptive online-batch and adaptive segmentation fusion.","Experiments conducted on eight datasets covering four medical imaging modalities validate the effectiveness of the proposed method.","Our work proposes and validates a new, practical, and effective approach for enhancing SA on downstream segmentation tasks (e.g., medical image segmentation)."],"url":"http://arxiv.org/abs/2406.00956v1","category":"cs.CV"}
{"created":"2024-06-03 02:24:01","title":"A Synergistic Approach In Network Intrusion Detection By Neurosymbolic AI","abstract":"The prevailing approaches in Network Intrusion Detection Systems (NIDS) are often hampered by issues such as high resource consumption, significant computational demands, and poor interpretability. Furthermore, these systems generally struggle to identify novel, rapidly changing cyber threats. This paper delves into the potential of incorporating Neurosymbolic Artificial Intelligence (NSAI) into NIDS, combining deep learning's data-driven strengths with symbolic AI's logical reasoning to tackle the dynamic challenges in cybersecurity, which also includes detailed NSAI techniques introduction for cyber professionals to explore the potential strengths of NSAI in NIDS. The inclusion of NSAI in NIDS marks potential advancements in both the detection and interpretation of intricate network threats, benefiting from the robust pattern recognition of neural networks and the interpretive prowess of symbolic reasoning. By analyzing network traffic data types and machine learning architectures, we illustrate NSAI's distinctive capability to offer more profound insights into network behavior, thereby improving both detection performance and the adaptability of the system. This merging of technologies not only enhances the functionality of traditional NIDS but also sets the stage for future developments in building more resilient, interpretable, and dynamic defense mechanisms against advanced cyber threats. The continued progress in this area is poised to transform NIDS into a system that is both responsive to known threats and anticipatory of emerging, unseen ones.","sentences":["The prevailing approaches in Network Intrusion Detection Systems (NIDS) are often hampered by issues such as high resource consumption, significant computational demands, and poor interpretability.","Furthermore, these systems generally struggle to identify novel, rapidly changing cyber threats.","This paper delves into the potential of incorporating Neurosymbolic Artificial Intelligence (NSAI) into NIDS, combining deep learning's data-driven strengths with symbolic AI's logical reasoning to tackle the dynamic challenges in cybersecurity, which also includes detailed NSAI techniques introduction for cyber professionals to explore the potential strengths of NSAI in NIDS.","The inclusion of NSAI in NIDS marks potential advancements in both the detection and interpretation of intricate network threats, benefiting from the robust pattern recognition of neural networks and the interpretive prowess of symbolic reasoning.","By analyzing network traffic data types and machine learning architectures, we illustrate NSAI's distinctive capability to offer more profound insights into network behavior, thereby improving both detection performance and the adaptability of the system.","This merging of technologies not only enhances the functionality of traditional NIDS but also sets the stage for future developments in building more resilient, interpretable, and dynamic defense mechanisms against advanced cyber threats.","The continued progress in this area is poised to transform NIDS into a system that is both responsive to known threats and anticipatory of emerging, unseen ones."],"url":"http://arxiv.org/abs/2406.00938v1","category":"cs.CR"}
{"created":"2024-06-03 02:12:27","title":"LanEvil: Benchmarking the Robustness of Lane Detection to Environmental Illusions","abstract":"Lane detection (LD) is an essential component of autonomous driving systems, providing fundamental functionalities like adaptive cruise control and automated lane centering. Existing LD benchmarks primarily focus on evaluating common cases, neglecting the robustness of LD models against environmental illusions such as shadows and tire marks on the road. This research gap poses significant safety challenges since these illusions exist naturally in real-world traffic situations. For the first time, this paper studies the potential threats caused by these environmental illusions to LD and establishes the first comprehensive benchmark LanEvil for evaluating the robustness of LD against this natural corruption. We systematically design 14 prevalent yet critical types of environmental illusions (e.g., shadow, reflection) that cover a wide spectrum of real-world influencing factors in LD tasks. Based on real-world environments, we create 94 realistic and customizable 3D cases using the widely used CARLA simulator, resulting in a dataset comprising 90,292 sampled images. Through extensive experiments, we benchmark the robustness of popular LD methods using LanEvil, revealing substantial performance degradation (-5.37% Accuracy and -10.70% F1-Score on average), with shadow effects posing the greatest risk (-7.39% Accuracy). Additionally, we assess the performance of commercial auto-driving systems OpenPilot and Apollo through collaborative simulations, demonstrating that proposed environmental illusions can lead to incorrect decisions and potential traffic accidents. To defend against environmental illusions, we propose the Attention Area Mixing (AAM) approach using hard examples, which witness significant robustness improvement (+3.76%) under illumination effects. We hope our paper can contribute to advancing more robust auto-driving systems in the future. Website: https://lanevil.github.io/.","sentences":["Lane detection (LD) is an essential component of autonomous driving systems, providing fundamental functionalities like adaptive cruise control and automated lane centering.","Existing LD benchmarks primarily focus on evaluating common cases, neglecting the robustness of LD models against environmental illusions such as shadows and tire marks on the road.","This research gap poses significant safety challenges since these illusions exist naturally in real-world traffic situations.","For the first time, this paper studies the potential threats caused by these environmental illusions to LD and establishes the first comprehensive benchmark LanEvil for evaluating the robustness of LD against this natural corruption.","We systematically design 14 prevalent yet critical types of environmental illusions (e.g., shadow, reflection) that cover a wide spectrum of real-world influencing factors in LD tasks.","Based on real-world environments, we create 94 realistic and customizable 3D cases using the widely used CARLA simulator, resulting in a dataset comprising 90,292 sampled images.","Through extensive experiments, we benchmark the robustness of popular LD methods using LanEvil, revealing substantial performance degradation (-5.37% Accuracy and -10.70% F1-Score on average), with shadow effects posing the greatest risk (-7.39% Accuracy).","Additionally, we assess the performance of commercial auto-driving systems OpenPilot and Apollo through collaborative simulations, demonstrating that proposed environmental illusions can lead to incorrect decisions and potential traffic accidents.","To defend against environmental illusions, we propose the Attention Area Mixing (AAM) approach using hard examples, which witness significant robustness improvement (+3.76%) under illumination effects.","We hope our paper can contribute to advancing more robust auto-driving systems in the future.","Website: https://lanevil.github.io/."],"url":"http://arxiv.org/abs/2406.00934v2","category":"cs.CV"}
{"created":"2024-06-03 02:05:38","title":"Refined Thermodynamic Uncertainty Relation for Chemical Reactions","abstract":"Thermodynamic uncertainty relations elucidate the intricate balance between the precision of current and the thermodynamic costs or dissipation, marking a recent and enthralling advancement at the confluence of statistical mechanics, thermodynamics, and information theory. In this study, we derive a time-energy uncertainty relation tailored for chemical reactions, expressed in terms of the Gibbs free energy and chemical potential. This inequality holds true irrespective of whether the total substance of chemical species is conserved during the reaction. Furthermore, it supports the general thermodynamic framework by ensuring the spontaneous decrease in Gibbs free energy. We present two formulations of the thermodynamic uncertainty relation: one based on chemical species concentrations and the other on molar fractions. The validity of our inequalities is numerically demonstrated using model systems of the Belousov-Zhabotinsky and Michaelis-Menten reactions. Our uncertainty relation may find practical applications in measuring and optimizing thermodynamic properties relevant to chemical reaction systems out of equilibrium.","sentences":["Thermodynamic uncertainty relations elucidate the intricate balance between the precision of current and the thermodynamic costs or dissipation, marking a recent and enthralling advancement at the confluence of statistical mechanics, thermodynamics, and information theory.","In this study, we derive a time-energy uncertainty relation tailored for chemical reactions, expressed in terms of the Gibbs free energy and chemical potential.","This inequality holds true irrespective of whether the total substance of chemical species is conserved during the reaction.","Furthermore, it supports the general thermodynamic framework by ensuring the spontaneous decrease in Gibbs free energy.","We present two formulations of the thermodynamic uncertainty relation: one based on chemical species concentrations and the other on molar fractions.","The validity of our inequalities is numerically demonstrated using model systems of the Belousov-Zhabotinsky and Michaelis-Menten reactions.","Our uncertainty relation may find practical applications in measuring and optimizing thermodynamic properties relevant to chemical reaction systems out of equilibrium."],"url":"http://arxiv.org/abs/2406.00933v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-03 01:32:52","title":"MEDIQ: Question-Asking LLMs for Adaptive and Reliable Clinical Reasoning","abstract":"In high-stakes domains like clinical reasoning, AI assistants powered by large language models (LLMs) are yet to be reliable and safe. We identify a key obstacle towards reliability: existing LLMs are trained to answer any question, even with incomplete context in the prompt or insufficient parametric knowledge. We propose to change this paradigm to develop more careful LLMs that ask follow-up questions to gather necessary and sufficient information and respond reliably. We introduce MEDIQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System. The Patient may provide incomplete information in the beginning; the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details from the Patient via follow-up questions. To evaluate MEDIQ, we convert MEDQA and CRAFT-MD -- medical benchmarks for diagnostic question answering -- into an interactive setup. We develop a reliable Patient system and prototype several Expert systems, first showing that directly prompting state-of-the-art LLMs to ask questions degrades the quality of clinical reasoning, indicating that adapting LLMs to interactive information-seeking settings is nontrivial. We then augment the Expert with a novel abstention module to better estimate model confidence and decide whether to ask more questions, thereby improving diagnostic accuracy by 20.3%; however, performance still lags compared to an (unrealistic in practice) upper bound when full information is given upfront. Further analyses reveal that interactive performance can be improved by filtering irrelevant contexts and reformatting conversations. Overall, our paper introduces a novel problem towards LLM reliability, a novel MEDIQ framework, and highlights important future directions to extend the information-seeking abilities of LLM assistants in critical domains.","sentences":["In high-stakes domains like clinical reasoning, AI assistants powered by large language models (LLMs) are yet to be reliable and safe.","We identify a key obstacle towards reliability: existing LLMs are trained to answer any question, even with incomplete context in the prompt or insufficient parametric knowledge.","We propose to change this paradigm to develop more careful LLMs that ask follow-up questions to gather necessary and sufficient information and respond reliably.","We introduce MEDIQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System.","The Patient may provide incomplete information in the beginning; the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details from the Patient via follow-up questions.","To evaluate MEDIQ, we convert MEDQA and CRAFT-MD -- medical benchmarks for diagnostic question answering -- into an interactive setup.","We develop a reliable Patient system and prototype several Expert systems, first showing that directly prompting state-of-the-art LLMs to ask questions degrades the quality of clinical reasoning, indicating that adapting LLMs to interactive information-seeking settings is nontrivial.","We then augment the Expert with a novel abstention module to better estimate model confidence and decide whether to ask more questions, thereby improving diagnostic accuracy by 20.3%; however, performance still lags compared to an (unrealistic in practice) upper bound when full information is given upfront.","Further analyses reveal that interactive performance can be improved by filtering irrelevant contexts and reformatting conversations.","Overall, our paper introduces a novel problem towards LLM reliability, a novel MEDIQ framework, and highlights important future directions to extend the information-seeking abilities of LLM assistants in critical domains."],"url":"http://arxiv.org/abs/2406.00922v2","category":"cs.CL"}
{"created":"2024-06-03 00:57:27","title":"Relations in Twisted Quantum K-Rings","abstract":"We introduce twisted quantum $K$-rings, defined via twisted $K$-theoretic Gromov-Witten invariants. We develop a toolkit for computing relations by adapting some results about ordinary quantum K rings to our setting, and discuss some applications, including Ruan-Zhang's quantum $K$-theory with level structure, and complete intersections inside projective space, confirming some predictions coming from physics.   In addition, we formulate a ring-theoretic abelian/non-abelian correspondence conjecture, relating the quantum K-ring of a GIT quotient $X//G$ to a certain twist of the quantum K-ring of $X//T$, the quotient by the maximal torus. We prove this conjecture for the case of Grassmanians, and use this to give another proof of the Whitney relations of Mihalcea-Gu-Sharpe-Zhou.","sentences":["We introduce twisted quantum $K$-rings, defined via twisted $K$-theoretic Gromov-Witten invariants.","We develop a toolkit for computing relations by adapting some results about ordinary quantum K rings to our setting, and discuss some applications, including Ruan-Zhang's quantum $K$-theory with level structure, and complete intersections inside projective space, confirming some predictions coming from physics.   ","In addition, we formulate a ring-theoretic abelian/non-abelian correspondence conjecture, relating the quantum K-ring of a GIT quotient $X//G$ to a certain twist of the quantum K-ring of $X//T$, the quotient by the maximal torus.","We prove this conjecture for the case of Grassmanians, and use this to give another proof of the Whitney relations of Mihalcea-Gu-Sharpe-Zhou."],"url":"http://arxiv.org/abs/2406.00916v1","category":"math.AG"}
{"created":"2024-06-03 00:41:37","title":"Characterization and thermometry of dissapatively stabilized steady states","abstract":"In this work we study the properties of dissipatively stabilized steady states of noisy quantum algorithms, exploring the extent to which they can be well approximated as thermal distributions, and proposing methods to extract the effective temperature T. We study an algorithm called the Relaxational Quantum Eigensolver (RQE), which is one of a family of algorithms that attempt to find ground states and balance error in noisy quantum devices. In RQE, we weakly couple a second register of auxiliary \"shadow\" qubits to the primary system in Trotterized evolution, thus engineering an approximate zero-temperature bath by periodically resetting the auxiliary qubits during the algorithm's runtime. Balancing the infinite temperature bath of random gate error, RQE returns states with an average energy equal to a constant fraction of the ground state. We probe the steady states of this algorithm for a range of base error rates, using several methods for estimating both T and deviations from thermal behavior. In particular, we both confirm that the steady states of these systems are often well-approximated by thermal distributions, and show that the same resources used for cooling can be adopted for thermometry, yielding a fairly reliable measure of the temperature. These methods could be readily implemented in near-term quantum hardware, and for stabilizing and probing Hamiltonians where simulating approximate thermal states is hard for classical computers.","sentences":["In this work we study the properties of dissipatively stabilized steady states of noisy quantum algorithms, exploring the extent to which they can be well approximated as thermal distributions, and proposing methods to extract the effective temperature T. We study an algorithm called the Relaxational Quantum Eigensolver (RQE), which is one of a family of algorithms that attempt to find ground states and balance error in noisy quantum devices.","In RQE, we weakly couple a second register of auxiliary \"shadow\" qubits to the primary system in Trotterized evolution, thus engineering an approximate zero-temperature bath by periodically resetting the auxiliary qubits during the algorithm's runtime.","Balancing the infinite temperature bath of random gate error, RQE returns states with an average energy equal to a constant fraction of the ground state.","We probe the steady states of this algorithm for a range of base error rates, using several methods for estimating both T and deviations from thermal behavior.","In particular, we both confirm that the steady states of these systems are often well-approximated by thermal distributions, and show that the same resources used for cooling can be adopted for thermometry, yielding a fairly reliable measure of the temperature.","These methods could be readily implemented in near-term quantum hardware, and for stabilizing and probing Hamiltonians where simulating approximate thermal states is hard for classical computers."],"url":"http://arxiv.org/abs/2406.00911v1","category":"quant-ph"}
{"created":"2024-06-03 00:31:13","title":"ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video Generation","abstract":"Video generation has made remarkable progress in recent years, especially since the advent of the video diffusion models. Many video generation models can produce plausible synthetic videos, e.g., Stable Video Diffusion (SVD). However, most video models can only generate low frame rate videos due to the limited GPU memory as well as the difficulty of modeling a large set of frames. The training videos are always uniformly sampled at a specified interval for temporal compression. Previous methods promote the frame rate by either training a video interpolation model in pixel space as a postprocessing stage or training an interpolation model in latent space for a specific base video model. In this paper, we propose a training-free video interpolation method for generative video diffusion models, which is generalizable to different models in a plug-and-play manner. We investigate the non-linearity in the feature space of video diffusion models and transform a video model into a self-cascaded video diffusion model with incorporating the designed hidden state correction modules. The self-cascaded architecture and the correction module are proposed to retain the temporal consistency between key frames and the interpolated frames. Extensive evaluations are preformed on multiple popular video models to demonstrate the effectiveness of the propose method, especially that our training-free method is even comparable to trained interpolation models supported by huge compute resources and large-scale datasets.","sentences":["Video generation has made remarkable progress in recent years, especially since the advent of the video diffusion models.","Many video generation models can produce plausible synthetic videos, e.g., Stable Video Diffusion (SVD).","However, most video models can only generate low frame rate videos due to the limited GPU memory as well as the difficulty of modeling a large set of frames.","The training videos are always uniformly sampled at a specified interval for temporal compression.","Previous methods promote the frame rate by either training a video interpolation model in pixel space as a postprocessing stage or training an interpolation model in latent space for a specific base video model.","In this paper, we propose a training-free video interpolation method for generative video diffusion models, which is generalizable to different models in a plug-and-play manner.","We investigate the non-linearity in the feature space of video diffusion models and transform a video model into a self-cascaded video diffusion model with incorporating the designed hidden state correction modules.","The self-cascaded architecture and the correction module are proposed to retain the temporal consistency between key frames and the interpolated frames.","Extensive evaluations are preformed on multiple popular video models to demonstrate the effectiveness of the propose method, especially that our training-free method is even comparable to trained interpolation models supported by huge compute resources and large-scale datasets."],"url":"http://arxiv.org/abs/2406.00908v1","category":"cs.CV"}
{"created":"2024-06-02 23:44:26","title":"Gapless superconductivity in the low-frequency electrodynamic response of two-dimensional granular In/InO$_x$ composites","abstract":"We measured the full complex ac conductance of two-dimensional granular In/InO$_x$ composites using the mutual inductance technique to explore the transition from a \"failed-superconductor-turned anomalous metal\" to a robust superconductor. In this system, room-temperature annealing was adopted to tune the InO$_x$-mediated coupling between In grains, allowing for the observation of both a \"true\" superconductor-to-insulator transition and the emergence of an intervening anomalous metallic state. In this paper, we show that further annealing increases the inter-grain coupling, which eliminates the anomalous metallic phase, but at the same time prevent the emergence of strong Bose-dominated insulating phase. The complex ac conductance revealed a $T\\to0$ saturating dissipative response in a finite magnetic field, coexisting with a robust superfluid density. The anomalous power-law spectra for the dissipative response appear to indicate quantum critical behavior proximate to a quantum superconductor to anomalous-metal transition as probed in the kilo-Hertz range, and point to signatures of gapless superconductivity in our granular superconducting system.","sentences":["We measured the full complex ac conductance of two-dimensional granular In/InO$_x$ composites using the mutual inductance technique to explore the transition from a \"failed-superconductor-turned anomalous metal\" to a robust superconductor.","In this system, room-temperature annealing was adopted to tune the InO$_x$-mediated coupling between In grains, allowing for the observation of both a \"true\" superconductor-to-insulator transition and the emergence of an intervening anomalous metallic state.","In this paper, we show that further annealing increases the inter-grain coupling, which eliminates the anomalous metallic phase, but at the same time prevent the emergence of strong Bose-dominated insulating phase.","The complex ac conductance revealed a $T\\to0$ saturating dissipative response in a finite magnetic field, coexisting with a robust superfluid density.","The anomalous power-law spectra for the dissipative response appear to indicate quantum critical behavior proximate to a quantum superconductor to anomalous-metal transition as probed in the kilo-Hertz range, and point to signatures of gapless superconductivity in our granular superconducting system."],"url":"http://arxiv.org/abs/2406.00900v1","category":"cond-mat.supr-con"}
{"created":"2024-06-02 23:23:46","title":"Description of turbulent dynamics in the interstellar medium: Multifractal microcanonical analysis: II. Sparse filtering of Herschel observation maps and visualization of filamentary structures at different length scales","abstract":"We present significant improvements to our previous work on noise reduction in {\\sl Herschel} observation maps by defining sparse filtering tools capable of handling, in a unified formalism, a significantly improved noise reduction as well as a deconvolution in order to reduce effects introduced by the limited instrumental response (beam). We implement greater flexibility by allowing a wider choice of parsimonious priors in the noise-reduction process. More precisely, we introduce a sparse filtering and deconvolution approach approach of type $l^2$-$l^p$, with $p > 0$ variable and apply it to a larger set of molecular clouds using {\\sl Herschel} 250 $\\mu $m data in order to demonstrate their wide range of application. In the {\\sl Herschel} data, we are able to use this approach to highlight extremely fine filamentary structures and obtain singularity spectra that tend to show a significantly less $\\log$-normal behavior and a filamentary nature in the less dense regions. We also use high-resolution adaptive magneto-hydrodynamic simulation data to assess the quality of deconvolution in such a simulated beaming framework.","sentences":["We present significant improvements to our previous work on noise reduction in {\\sl Herschel} observation maps by defining sparse filtering tools capable of handling, in a unified formalism, a significantly improved noise reduction as well as a deconvolution in order to reduce effects introduced by the limited instrumental response (beam).","We implement greater flexibility by allowing a wider choice of parsimonious priors in the noise-reduction process.","More precisely, we introduce a sparse filtering and deconvolution approach approach of type $l^2$-$l^p$, with $p > 0$ variable and apply it to a larger set of molecular clouds using {\\sl Herschel} 250 $\\mu $m data in order to demonstrate their wide range of application.","In the {\\sl Herschel} data, we are able to use this approach to highlight extremely fine filamentary structures and obtain singularity spectra that tend to show a significantly less $\\log$-normal behavior and a filamentary nature in the less dense regions.","We also use high-resolution adaptive magneto-hydrodynamic simulation data to assess the quality of deconvolution in such a simulated beaming framework."],"url":"http://arxiv.org/abs/2406.00893v2","category":"astro-ph.GA"}
{"created":"2024-06-02 23:18:12","title":"Global High Categorical Resolution Land Cover Mapping via Weak Supervision","abstract":"Land cover information is indispensable for advancing the United Nations' sustainable development goals, and land cover mapping under a more detailed category system would significantly contribute to economic livelihood tracking and environmental degradation measurement. However, the substantial difficulty in acquiring fine-grained training data makes the implementation of this task particularly challenging. Here, we propose to combine fully labeled source domain and weakly labeled target domain for weakly supervised domain adaptation (WSDA). This is beneficial as the utilization of sparse and coarse weak labels can considerably alleviate the labor required for precise and detailed land cover annotation. Specifically, we introduce the Prototype-based pseudo-label Rectification and Expansion (PRE) approach, which leverages the prototypes (i.e., the class-wise feature centroids) as the bridge to connect sparse labels and global feature distributions. According to the feature distances to the prototypes, the confidence of pseudo-labels predicted in the unlabeled regions of the target domain is assessed. This confidence is then utilized to guide the dynamic expansion and rectification of pseudo-labels. Based on PRE, we carry out high categorical resolution land cover mapping for 10 cities in different regions around the world, severally using PlanetScope, Gaofen-1, and Sentinel-2 satellite images. In the study areas, we achieve cross-sensor, cross-category, and cross-continent WSDA, with the overall accuracy exceeding 80%. The promising results indicate that PRE is capable of reducing the dependency of land cover classification on high-quality annotations, thereby improving label efficiency. We expect our work to enable global fine-grained land cover mapping, which in turn promote Earth observation to provide more precise and thorough information for environmental monitoring.","sentences":["Land cover information is indispensable for advancing the United Nations' sustainable development goals, and land cover mapping under a more detailed category system would significantly contribute to economic livelihood tracking and environmental degradation measurement.","However, the substantial difficulty in acquiring fine-grained training data makes the implementation of this task particularly challenging.","Here, we propose to combine fully labeled source domain and weakly labeled target domain for weakly supervised domain adaptation (WSDA).","This is beneficial as the utilization of sparse and coarse weak labels can considerably alleviate the labor required for precise and detailed land cover annotation.","Specifically, we introduce the Prototype-based pseudo-label Rectification and Expansion (PRE) approach, which leverages the prototypes (i.e., the class-wise feature centroids) as the bridge to connect sparse labels and global feature distributions.","According to the feature distances to the prototypes, the confidence of pseudo-labels predicted in the unlabeled regions of the target domain is assessed.","This confidence is then utilized to guide the dynamic expansion and rectification of pseudo-labels.","Based on PRE, we carry out high categorical resolution land cover mapping for 10 cities in different regions around the world, severally using PlanetScope, Gaofen-1, and Sentinel-2 satellite images.","In the study areas, we achieve cross-sensor, cross-category, and cross-continent WSDA, with the overall accuracy exceeding 80%.","The promising results indicate that PRE is capable of reducing the dependency of land cover classification on high-quality annotations, thereby improving label efficiency.","We expect our work to enable global fine-grained land cover mapping, which in turn promote Earth observation to provide more precise and thorough information for environmental monitoring."],"url":"http://arxiv.org/abs/2406.00891v1","category":"cs.CV"}
{"created":"2024-06-02 23:18:11","title":"Applying Fine-Tuned LLMs for Reducing Data Needs in Load Profile Analysis","abstract":"This paper presents a novel method for utilizing fine-tuned Large Language Models (LLMs) to minimize data requirements in load profile analysis, demonstrated through the restoration of missing data in power system load profiles. A two-stage fine-tuning strategy is proposed to adapt a pre-trained LLMs, i.e., GPT-3.5, for missing data restoration tasks. Through empirical evaluation, we demonstrate the effectiveness of the fine-tuned model in accurately restoring missing data, achieving comparable performance to state-of-the-art specifically designed models such as BERT-PIN. Key findings include the importance of prompt engineering and the optimal utilization of fine-tuning samples, highlighting the efficiency of few-shot learning in transferring knowledge from general user cases to specific target users. Furthermore, the proposed approach demonstrates notable cost-effectiveness and time efficiency compared to training models from scratch, making it a practical solution for scenarios with limited data availability and computing resources. This research has significant potential for application to other power system load profile analysis tasks. Consequently, it advances the use of LLMs in power system analytics, offering promising implications for enhancing the resilience and efficiency of power distribution systems.","sentences":["This paper presents a novel method for utilizing fine-tuned Large Language Models (LLMs) to minimize data requirements in load profile analysis, demonstrated through the restoration of missing data in power system load profiles.","A two-stage fine-tuning strategy is proposed to adapt a pre-trained LLMs, i.e., GPT-3.5, for missing data restoration tasks.","Through empirical evaluation, we demonstrate the effectiveness of the fine-tuned model in accurately restoring missing data, achieving comparable performance to state-of-the-art specifically designed models such as BERT-PIN.","Key findings include the importance of prompt engineering and the optimal utilization of fine-tuning samples, highlighting the efficiency of few-shot learning in transferring knowledge from general user cases to specific target users.","Furthermore, the proposed approach demonstrates notable cost-effectiveness and time efficiency compared to training models from scratch, making it a practical solution for scenarios with limited data availability and computing resources.","This research has significant potential for application to other power system load profile analysis tasks.","Consequently, it advances the use of LLMs in power system analytics, offering promising implications for enhancing the resilience and efficiency of power distribution systems."],"url":"http://arxiv.org/abs/2406.02479v1","category":"cs.LG"}
{"created":"2024-06-02 23:16:00","title":"Reservoir History Matching of the Norne field with generative exotic priors and a coupled Mixture of Experts -- Physics Informed Neural Operator Forward Model","abstract":"We developed a novel reservoir characterization workflow that addresses reservoir history matching by coupling a physics-informed neural operator (PINO) forward model with a mixture of experts' approach, termed cluster classify regress (CCR). The inverse modelling is achieved via an adaptive Regularized Ensemble Kalman inversion (aREKI) method, ideal for rapid inverse uncertainty quantification during history matching. We parametrize unknown permeability and porosity fields for non-Gaussian posterior measures using a variational convolution autoencoder and a denoising diffusion implicit model (DDIM) exotic priors. The CCR works as a supervised model with the PINO surrogate to replicate nonlinear Peaceman well equations. The CCR's flexibility allows any independent machine-learning algorithm for each stage. The PINO reservoir surrogate's loss function is derived from supervised data loss and losses from the initial conditions and residual of the governing black oil PDE. The PINO-CCR surrogate outputs pressure, water, and gas saturations, along with oil, water, and gas production rates. The methodology was compared to a standard numerical black oil simulator for a waterflooding case on the Norne field, showing similar outputs. This PINO-CCR surrogate was then used in the aREKI history matching workflow, successfully recovering the unknown permeability, porosity and fault multiplier, with simulations up to 6000 times faster than conventional methods. Training the PINO-CCR surrogate on an NVIDIA H100 with 80G memory takes about 5 hours for 100 samples of the Norne field. This workflow is suitable for ensemble-based approaches, where posterior density sampling, given an expensive likelihood evaluation, is desirable for uncertainty quantification.","sentences":["We developed a novel reservoir characterization workflow that addresses reservoir history matching by coupling a physics-informed neural operator (PINO) forward model with a mixture of experts' approach, termed cluster classify regress (CCR).","The inverse modelling is achieved via an adaptive Regularized Ensemble Kalman inversion (aREKI) method, ideal for rapid inverse uncertainty quantification during history matching.","We parametrize unknown permeability and porosity fields for non-Gaussian posterior measures using a variational convolution autoencoder and a denoising diffusion implicit model (DDIM) exotic priors.","The CCR works as a supervised model with the PINO surrogate to replicate nonlinear Peaceman well equations.","The CCR's flexibility allows any independent machine-learning algorithm for each stage.","The PINO reservoir surrogate's loss function is derived from supervised data loss and losses from the initial conditions and residual of the governing black oil PDE.","The PINO-CCR surrogate outputs pressure, water, and gas saturations, along with oil, water, and gas production rates.","The methodology was compared to a standard numerical black oil simulator for a waterflooding case on the Norne field, showing similar outputs.","This PINO-CCR surrogate was then used in the aREKI history matching workflow, successfully recovering the unknown permeability, porosity and fault multiplier, with simulations up to 6000 times faster than conventional methods.","Training the PINO-CCR surrogate on an NVIDIA H100 with 80G memory takes about 5 hours for 100 samples of the Norne field.","This workflow is suitable for ensemble-based approaches, where posterior density sampling, given an expensive likelihood evaluation, is desirable for uncertainty quantification."],"url":"http://arxiv.org/abs/2406.00889v1","category":"cs.LG"}
{"created":"2024-06-02 22:40:05","title":"Visual place recognition for aerial imagery: A survey","abstract":"Aerial imagery and its direct application to visual localization is an essential problem for many Robotics and Computer Vision tasks. While Global Navigation Satellite Systems (GNSS) are the standard default solution for solving the aerial localization problem, it is subject to a number of limitations, such as, signal instability or solution unreliability that make this option not so desirable. Consequently, visual geolocalization is emerging as a viable alternative. However, adapting Visual Place Recognition (VPR) task to aerial imagery presents significant challenges, including weather variations and repetitive patterns. Current VPR reviews largely neglect the specific context of aerial data. This paper introduces a methodology tailored for evaluating VPR techniques specifically in the domain of aerial imagery, providing a comprehensive assessment of various methods and their performance. However, we not only compare various VPR methods, but also demonstrate the importance of selecting appropriate zoom and overlap levels when constructing map tiles to achieve maximum efficiency of VPR algorithms in the case of aerial imagery. The code is available on our GitHub repository -- https://github.com/prime-slam/aero-vloc.","sentences":["Aerial imagery and its direct application to visual localization is an essential problem for many Robotics and Computer Vision tasks.","While Global Navigation Satellite Systems (GNSS) are the standard default solution for solving the aerial localization problem, it is subject to a number of limitations, such as, signal instability or solution unreliability that make this option not so desirable.","Consequently, visual geolocalization is emerging as a viable alternative.","However, adapting Visual Place Recognition (VPR) task to aerial imagery presents significant challenges, including weather variations and repetitive patterns.","Current VPR reviews largely neglect the specific context of aerial data.","This paper introduces a methodology tailored for evaluating VPR techniques specifically in the domain of aerial imagery, providing a comprehensive assessment of various methods and their performance.","However, we not only compare various VPR methods, but also demonstrate the importance of selecting appropriate zoom and overlap levels when constructing map tiles to achieve maximum efficiency of VPR algorithms in the case of aerial imagery.","The code is available on our GitHub repository -- https://github.com/prime-slam/aero-vloc."],"url":"http://arxiv.org/abs/2406.00885v1","category":"cs.CV"}
{"created":"2024-06-02 21:36:31","title":"OLIVE: Object Level In-Context Visual Embeddings","abstract":"Recent generalist vision-language models (VLMs) have demonstrated impressive reasoning capabilities across diverse multimodal tasks. However, these models still struggle with fine-grained object-level understanding and grounding. In terms of modeling, existing VLMs implicitly align text tokens with image patch tokens, which is ineffective for embedding alignment at the same granularity and inevitably introduces noisy spurious background features. Additionally, these models struggle when generalizing to unseen visual concepts and may not be reliable for domain-specific tasks without further fine-tuning. To address these limitations, we propose a novel method to prompt large language models with in-context visual object vectors, thereby enabling controllable object-level reasoning. This eliminates the necessity of fusing a lengthy array of image patch features and significantly speeds up training. Furthermore, we propose region-level retrieval using our object representations, facilitating rapid adaptation to new objects without additional training. Our experiments reveal that our method achieves competitive referring object classification and captioning performance, while also offering zero-shot generalization and robustness to visually challenging contexts.","sentences":["Recent generalist vision-language models (VLMs) have demonstrated impressive reasoning capabilities across diverse multimodal tasks.","However, these models still struggle with fine-grained object-level understanding and grounding.","In terms of modeling, existing VLMs implicitly align text tokens with image patch tokens, which is ineffective for embedding alignment at the same granularity and inevitably introduces noisy spurious background features.","Additionally, these models struggle when generalizing to unseen visual concepts and may not be reliable for domain-specific tasks without further fine-tuning.","To address these limitations, we propose a novel method to prompt large language models with in-context visual object vectors, thereby enabling controllable object-level reasoning.","This eliminates the necessity of fusing a lengthy array of image patch features and significantly speeds up training.","Furthermore, we propose region-level retrieval using our object representations, facilitating rapid adaptation to new objects without additional training.","Our experiments reveal that our method achieves competitive referring object classification and captioning performance, while also offering zero-shot generalization and robustness to visually challenging contexts."],"url":"http://arxiv.org/abs/2406.00872v1","category":"cs.CV"}
{"created":"2024-06-02 20:35:19","title":"Matrix-Valued Measures and Wishart Statistics for Target Tracking Applications","abstract":"Ensuring sufficiently accurate models is crucial in target tracking systems. If the assumed models deviate too much from the truth, the tracking performance might be severely degraded. While the models are in general defined using multivariate conditions, the measures used to validate them are most often scalar-valued. In this paper, we propose matrix-valued measures for both offline and online assessment of target tracking systems. Recent results from Wishart statistics, and approximations thereof, are adapted and it is shown how these can be incorporated to infer statistical properties for the eigenvalues of the proposed measures. In addition, we relate these results to the statistics of the baseline measures. Finally, the applicability of the proposed measures are demonstrated using two important problems in target tracking: (i) distributed track fusion design; and (ii) filter model mismatch detection.","sentences":["Ensuring sufficiently accurate models is crucial in target tracking systems.","If the assumed models deviate too much from the truth, the tracking performance might be severely degraded.","While the models are in general defined using multivariate conditions, the measures used to validate them are most often scalar-valued.","In this paper, we propose matrix-valued measures for both offline and online assessment of target tracking systems.","Recent results from Wishart statistics, and approximations thereof, are adapted and it is shown how these can be incorporated to infer statistical properties for the eigenvalues of the proposed measures.","In addition, we relate these results to the statistics of the baseline measures.","Finally, the applicability of the proposed measures are demonstrated using two important problems in target tracking: (i) distributed track fusion design; and (ii) filter model mismatch detection."],"url":"http://arxiv.org/abs/2406.00861v1","category":"eess.SY"}
{"created":"2024-06-02 19:50:05","title":"Local Methods with Adaptivity via Scaling","abstract":"The rapid development of machine learning and deep learning has introduced increasingly complex optimization challenges that must be addressed. Indeed, training modern, advanced models has become difficult to implement without leveraging multiple computing nodes in a distributed environment. Distributed optimization is also fundamental to emerging fields such as federated learning. Specifically, there is a need to organize the training process to minimize the time lost due to communication. A widely used and extensively researched technique to mitigate the communication bottleneck involves performing local training before communication. This approach is the focus of our paper. Concurrently, adaptive methods that incorporate scaling, notably led by Adam, have gained significant popularity in recent years. Therefore, this paper aims to merge the local training technique with the adaptive approach to develop efficient distributed learning methods. We consider the classical Local SGD method and enhance it with a scaling feature. A crucial aspect is that the scaling is described generically, allowing us to analyze various approaches, including Adam, RMSProp, and OASIS, in a unified manner. In addition to theoretical analysis, we validate the performance of our methods in practice by training a neural network.","sentences":["The rapid development of machine learning and deep learning has introduced increasingly complex optimization challenges that must be addressed.","Indeed, training modern, advanced models has become difficult to implement without leveraging multiple computing nodes in a distributed environment.","Distributed optimization is also fundamental to emerging fields such as federated learning.","Specifically, there is a need to organize the training process to minimize the time lost due to communication.","A widely used and extensively researched technique to mitigate the communication bottleneck involves performing local training before communication.","This approach is the focus of our paper.","Concurrently, adaptive methods that incorporate scaling, notably led by Adam, have gained significant popularity in recent years.","Therefore, this paper aims to merge the local training technique with the adaptive approach to develop efficient distributed learning methods.","We consider the classical Local SGD method and enhance it with a scaling feature.","A crucial aspect is that the scaling is described generically, allowing us to analyze various approaches, including Adam, RMSProp, and OASIS, in a unified manner.","In addition to theoretical analysis, we validate the performance of our methods in practice by training a neural network."],"url":"http://arxiv.org/abs/2406.00846v1","category":"cs.LG"}
{"created":"2024-06-02 19:43:34","title":"On the equations of compressible fluid dynamics with Cattaneo-type extensions for the heat flux: Symmetrizability and relaxation structure","abstract":"The aim of this work is twofold. From a mathematical point of view, we show the existence of a hyperbolic system of equations that is not symmetrizable in the sense of Friedrichs. Such system appears in the theory of compressible fluid dynamics with Cattaneo-type extensions for the heat flux. In contrast, the linearizations of such system around constant equilibrium solutions have Friedrichs symmetrizers. Then, from a physical perspective, we aim to understand the relaxation term appearing in this system. By noticing the violation of the Kawashima-Shizuta condition, locally and smoothly, with respect to the Fourier frequencies, we construct persistent waves, i.e., solutions preserving the $L^{2}$ norm for all times that are not dissipated by the relaxation terms.","sentences":["The aim of this work is twofold.","From a mathematical point of view, we show the existence of a hyperbolic system of equations that is not symmetrizable in the sense of Friedrichs.","Such system appears in the theory of compressible fluid dynamics with Cattaneo-type extensions for the heat flux.","In contrast, the linearizations of such system around constant equilibrium solutions have Friedrichs symmetrizers.","Then, from a physical perspective, we aim to understand the relaxation term appearing in this system.","By noticing the violation of the Kawashima-Shizuta condition, locally and smoothly, with respect to the Fourier frequencies, we construct persistent waves, i.e., solutions preserving the $L^{2}$ norm for all times that are not dissipated by the relaxation terms."],"url":"http://arxiv.org/abs/2406.00844v1","category":"math.AP"}
{"created":"2024-06-02 18:12:05","title":"A Lazy Abstraction Algorithm for Markov Decision Processes: Theory and Initial Evaluation","abstract":"Analysis of Markov Decision Processes (MDP) is often hindered by state space explosion. Abstraction is a well-established technique in model checking to mitigate this issue. This paper presents a novel lazy abstraction method for MDP analysis based on adaptive simulation graphs. Refinement is performed only when new parts of the state space are explored, which makes partial exploration techniques like Bounded Real-Time Dynamic Programming (BRTDP) retain more merged states. Therefore, we propose a combination of lazy abstraction and BRTDP. To evaluate the performance of our algorithm, we conduct initial experiments using the Quantitative Verification Benchmark Set.","sentences":["Analysis of Markov Decision Processes (MDP) is often hindered by state space explosion.","Abstraction is a well-established technique in model checking to mitigate this issue.","This paper presents a novel lazy abstraction method for MDP analysis based on adaptive simulation graphs.","Refinement is performed only when new parts of the state space are explored, which makes partial exploration techniques like Bounded Real-Time Dynamic Programming (BRTDP) retain more merged states.","Therefore, we propose a combination of lazy abstraction and BRTDP.","To evaluate the performance of our algorithm, we conduct initial experiments using the Quantitative Verification Benchmark Set."],"url":"http://arxiv.org/abs/2406.00824v1","category":"cs.LO"}
{"created":"2024-06-02 18:11:47","title":"Lasso Bandit with Compatibility Condition on Optimal Arm","abstract":"We consider a stochastic sparse linear bandit problem where only a sparse subset of context features affects the expected reward function, i.e., the unknown reward parameter has sparse structure. In the existing Lasso bandit literature, the compatibility conditions together with additional diversity conditions on the context features are imposed to achieve regret bounds that only depend logarithmically on the ambient dimension $d$. In this paper, we demonstrate that even without the additional diversity assumptions, the compatibility condition only on the optimal arm is sufficient to derive a regret bound that depends logarithmically on $d$, and our assumption is strictly weaker than those used in the lasso bandit literature under the single parameter setting. We propose an algorithm that adapts the forced-sampling technique and prove that the proposed algorithm achieves $O(\\text{poly}\\log dT)$ regret under the margin condition. To our knowledge, the proposed algorithm requires the weakest assumptions among Lasso bandit algorithms under a single parameter setting that achieve $O(\\text{poly}\\log dT)$ regret. Through the numerical experiments, we confirm the superior performance of our proposed algorithm.","sentences":["We consider a stochastic sparse linear bandit problem where only a sparse subset of context features affects the expected reward function, i.e., the unknown reward parameter has sparse structure.","In the existing Lasso bandit literature, the compatibility conditions together with additional diversity conditions on the context features are imposed to achieve regret bounds that only depend logarithmically on the ambient dimension $d$.","In this paper, we demonstrate that even without the additional diversity assumptions, the compatibility condition only on the optimal arm is sufficient to derive a regret bound that depends logarithmically on $d$, and our assumption is strictly weaker than those used in the lasso bandit literature under the single parameter setting.","We propose an algorithm that adapts the forced-sampling technique and prove that the proposed algorithm achieves $O(\\text{poly}\\log dT)$ regret under the margin condition.","To our knowledge, the proposed algorithm requires the weakest assumptions among Lasso bandit algorithms under a single parameter setting that achieve $O(\\text{poly}\\log dT)$ regret.","Through the numerical experiments, we confirm the superior performance of our proposed algorithm."],"url":"http://arxiv.org/abs/2406.00823v1","category":"stat.ML"}
{"created":"2024-06-02 17:59:45","title":"Weak convergence of adaptive Markov chain Monte Carlo","abstract":"This article develops general conditions for weak convergence of adaptive Markov chain Monte Carlo processes and is shown to imply a weak law of large numbers for bounded Lipschitz continuous functions. This allows an estimation theory for adaptive Markov chain Monte Carlo where previously developed theory in total variation may fail or be difficult to establish. Extensions of weak convergence to general Wasserstein distances are established along with a weak law of large numbers for possibly unbounded Lipschitz functions. Applications are applied to auto-regressive processes in various settings, unadjusted Langevin processes, and adaptive Metropolis-Hastings.","sentences":["This article develops general conditions for weak convergence of adaptive Markov chain Monte Carlo processes and is shown to imply a weak law of large numbers for bounded Lipschitz continuous functions.","This allows an estimation theory for adaptive Markov chain Monte Carlo where previously developed theory in total variation may fail or be difficult to establish.","Extensions of weak convergence to general Wasserstein distances are established along with a weak law of large numbers for possibly unbounded Lipschitz functions.","Applications are applied to auto-regressive processes in various settings, unadjusted Langevin processes, and adaptive Metropolis-Hastings."],"url":"http://arxiv.org/abs/2406.00820v1","category":"math.ST"}
{"created":"2024-06-02 17:26:27","title":"Covariance-Adaptive Sequential Black-box Optimization for Diffusion Targeted Generation","abstract":"Diffusion models have demonstrated great potential in generating high-quality content for images, natural language, protein domains, etc. However, how to perform user-preferred targeted generation via diffusion models with only black-box target scores of users remains challenging. To address this issue, we first formulate the fine-tuning of the targeted reserve-time stochastic differential equation (SDE) associated with a pre-trained diffusion model as a sequential black-box optimization problem. Furthermore, we propose a novel covariance-adaptive sequential optimization algorithm to optimize cumulative black-box scores under unknown transition dynamics. Theoretically, we prove a $O(\\frac{d^2}{\\sqrt{T}})$ convergence rate for cumulative convex functions without smooth and strongly convex assumptions. Empirically, experiments on both numerical test problems and target-guided 3D-molecule generation tasks show the superior performance of our method in achieving better target scores.","sentences":["Diffusion models have demonstrated great potential in generating high-quality content for images, natural language, protein domains, etc.","However, how to perform user-preferred targeted generation via diffusion models with only black-box target scores of users remains challenging.","To address this issue, we first formulate the fine-tuning of the targeted reserve-time stochastic differential equation (SDE) associated with a pre-trained diffusion model as a sequential black-box optimization problem.","Furthermore, we propose a novel covariance-adaptive sequential optimization algorithm to optimize cumulative black-box scores under unknown transition dynamics.","Theoretically, we prove a $O(\\frac{d^2}{\\sqrt{T}})$ convergence rate for cumulative convex functions without smooth and strongly convex assumptions.","Empirically, experiments on both numerical test problems and target-guided 3D-molecule generation tasks show the superior performance of our method in achieving better target scores."],"url":"http://arxiv.org/abs/2406.00812v1","category":"stat.ML"}
{"created":"2024-06-02 17:09:48","title":"Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection","abstract":"Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE.","sentences":["Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios.","Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP.","Existing methods build a text-based classifier with only closed-set labels.","However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space.","In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data.","Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection.","Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively.","Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset.","The code is publicly available at: https://github.com/tmlr-group/EOE."],"url":"http://arxiv.org/abs/2406.00806v1","category":"cs.LG"}
{"created":"2024-06-02 17:08:56","title":"Extrapolability Improvement of Machine Learning-Based Evapotranspiration Models via Domain-Adversarial Neural Networks","abstract":"Machine learning-based hydrological prediction models, despite their high accuracy, face limitations in extrapolation capabilities when applied globally due to uneven data distribution. This study integrates Domain-Adversarial Neural Networks (DANN) to improve the geographical adaptability of evapotranspiration (ET) models. By employing DANN, we aim to mitigate distributional discrepancies between different sites, significantly enhancing the model's extrapolation capabilities. Our results show that DANN improves ET prediction accuracy with an average increase in the Kling-Gupta Efficiency (KGE) of 0.2 to 0.3 compared to the traditional Leave-One-Out (LOO) method. DANN is particularly effective for isolated sites and transition zones between biomes, reducing data distribution discrepancies and avoiding low-accuracy predictions. By leveraging information from data-rich areas, DANN enhances the reliability of global-scale ET products, especially in ungauged regions. This study highlights the potential of domain adaptation techniques to improve the extrapolation and generalization capabilities of machine learning models in hydrological studies.","sentences":["Machine learning-based hydrological prediction models, despite their high accuracy, face limitations in extrapolation capabilities when applied globally due to uneven data distribution.","This study integrates Domain-Adversarial Neural Networks (DANN) to improve the geographical adaptability of evapotranspiration (ET) models.","By employing DANN, we aim to mitigate distributional discrepancies between different sites, significantly enhancing the model's extrapolation capabilities.","Our results show that DANN improves ET prediction accuracy with an average increase in the Kling-Gupta Efficiency (KGE) of 0.2 to 0.3 compared to the traditional Leave-One-Out (LOO) method.","DANN is particularly effective for isolated sites and transition zones between biomes, reducing data distribution discrepancies and avoiding low-accuracy predictions.","By leveraging information from data-rich areas, DANN enhances the reliability of global-scale ET products, especially in ungauged regions.","This study highlights the potential of domain adaptation techniques to improve the extrapolation and generalization capabilities of machine learning models in hydrological studies."],"url":"http://arxiv.org/abs/2406.00805v1","category":"cs.LG"}
{"created":"2024-06-02 16:19:55","title":"Qudit inspired optimization for graph coloring","abstract":"We introduce a quantum-inspired algorithm for Graph Coloring Problems (GCPs) that utilizes qudits in a product state, with each qudit representing a node in the graph and parameterized by d-dimensional spherical coordinates. We propose and benchmark two optimization strategies: qudit gradient descent (QdGD), initiating qudits in random states and employing gradient descent to minimize a cost function, and qudit local quantum annealing (QdLQA), which adapts the local quantum annealing method to optimize an adiabatic transition from a tractable initial function to a problem-specific cost function. Our approaches are benchmarked against established solutions for standard GCPs, showing that our methods not only rival but frequently surpass the performance of recent state-of-the-art algorithms in terms of solution quality and computational efficiency. The adaptability of our algorithm and its high-quality solutions, achieved with minimal computational resources, point to an advancement in the field of quantum-inspired optimization, with potential applications extending to a broad spectrum of optimization problems.","sentences":["We introduce a quantum-inspired algorithm for Graph Coloring Problems (GCPs) that utilizes qudits in a product state, with each qudit representing a node in the graph and parameterized by d-dimensional spherical coordinates.","We propose and benchmark two optimization strategies: qudit gradient descent (QdGD), initiating qudits in random states and employing gradient descent to minimize a cost function, and qudit local quantum annealing (QdLQA), which adapts the local quantum annealing method to optimize an adiabatic transition from a tractable initial function to a problem-specific cost function.","Our approaches are benchmarked against established solutions for standard GCPs, showing that our methods not only rival but frequently surpass the performance of recent state-of-the-art algorithms in terms of solution quality and computational efficiency.","The adaptability of our algorithm and its high-quality solutions, achieved with minimal computational resources, point to an advancement in the field of quantum-inspired optimization, with potential applications extending to a broad spectrum of optimization problems."],"url":"http://arxiv.org/abs/2406.00792v1","category":"quant-ph"}
{"created":"2024-06-02 16:13:57","title":"Towards Point Cloud Compression for Machine Perception: A Simple and Strong Baseline by Learning the Octree Depth Level Predictor","abstract":"Point cloud compression has garnered significant interest in computer vision. However, existing algorithms primarily cater to human vision, while most point cloud data is utilized for machine vision tasks. To address this, we propose a point cloud compression framework that simultaneously handles both human and machine vision tasks. Our framework learns a scalable bit-stream, using only subsets for different machine vision tasks to save bit-rate, while employing the entire bit-stream for human vision tasks. Building on mainstream octree-based frameworks like VoxelContext-Net, OctAttention, and G-PCC, we introduce a new octree depth-level predictor. This predictor adaptively determines the optimal depth level for each octree constructed from a point cloud, controlling the bit-rate for machine vision tasks. For simpler tasks (\\textit{e.g.}, classification) or objects/scenarios, we use fewer depth levels with fewer bits, saving bit-rate. Conversely, for more complex tasks (\\textit{e.g}., segmentation) or objects/scenarios, we use deeper depth levels with more bits to enhance performance. Experimental results on various datasets (\\textit{e.g}., ModelNet10, ModelNet40, ShapeNet, ScanNet, and KITTI) show that our point cloud compression approach improves performance for machine vision tasks without compromising human vision quality.","sentences":["Point cloud compression has garnered significant interest in computer vision.","However, existing algorithms primarily cater to human vision, while most point cloud data is utilized for machine vision tasks.","To address this, we propose a point cloud compression framework that simultaneously handles both human and machine vision tasks.","Our framework learns a scalable bit-stream, using only subsets for different machine vision tasks to save bit-rate, while employing the entire bit-stream for human vision tasks.","Building on mainstream octree-based frameworks like VoxelContext-Net, OctAttention, and G-PCC, we introduce a new octree depth-level predictor.","This predictor adaptively determines the optimal depth level for each octree constructed from a point cloud, controlling the bit-rate for machine vision tasks.","For simpler tasks (\\textit{e.g.}, classification) or objects/scenarios, we use fewer depth levels with fewer bits, saving bit-rate.","Conversely, for more complex tasks (\\textit{e.g}., segmentation) or objects/scenarios, we use deeper depth levels with more bits to enhance performance.","Experimental results on various datasets (\\textit{e.g}., ModelNet10, ModelNet40, ShapeNet, ScanNet, and KITTI) show that our point cloud compression approach improves performance for machine vision tasks without compromising human vision quality."],"url":"http://arxiv.org/abs/2406.00791v1","category":"cs.CV"}
{"created":"2024-06-02 15:26:52","title":"Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data","abstract":"State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints. To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms. This new attack does not require parameter tuning and further degrades the accuracy, up to 81% points compared to the previous gradient attacks. Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack. We demonstrate the effectiveness of our attacks on five architectures and four critical use cases. Our empirical study demonstrates that CAA outperforms all existing attacks in 17 over the 20 settings, and leads to a drop in the accuracy by up to 96.1% points and 21.9% points compared to CAPGD and MOEVA respectively while being up to five times faster than MOEVA. Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning.","sentences":["State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings.","However, the robustness of these models remains scarcely explored.","Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints.","To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms.","This new attack does not require parameter tuning and further degrades the accuracy, up to 81% points compared to the previous gradient attacks.","Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack.","We demonstrate the effectiveness of our attacks on five architectures and four critical use cases.","Our empirical study demonstrates that CAA outperforms all existing attacks in 17 over the 20 settings, and leads to a drop in the accuracy by up to 96.1% points and 21.9% points compared to CAPGD and MOEVA respectively while being up to five times faster than MOEVA.","Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning."],"url":"http://arxiv.org/abs/2406.00775v1","category":"cs.LG"}
{"created":"2024-06-02 15:20:59","title":"Diffusion Tuning: Transferring Diffusion Models via Chain of Forgetting","abstract":"Diffusion models have significantly advanced the field of generative modeling. However, training a diffusion model is computationally expensive, creating a pressing need to adapt off-the-shelf diffusion models for downstream generation tasks. Current fine-tuning methods focus on parameter-efficient transfer learning but overlook the fundamental transfer characteristics of diffusion models. In this paper, we investigate the transferability of diffusion models and observe a monotonous chain of forgetting trend of transferability along the reverse process. Based on this observation and novel theoretical insights, we present Diff-Tuning, a frustratingly simple transfer approach that leverages the chain of forgetting tendency. Diff-Tuning encourages the fine-tuned model to retain the pre-trained knowledge at the end of the denoising chain close to the generated data while discarding the other noise side. We conduct comprehensive experiments to evaluate Diff-Tuning, including the transfer of pre-trained Diffusion Transformer models to eight downstream generations and the adaptation of Stable Diffusion to five control conditions with ControlNet. Diff-Tuning achieves a 26% improvement over standard fine-tuning and enhances the convergence speed of ControlNet by 24%. Notably, parameter-efficient transfer learning techniques for diffusion models can also benefit from Diff-Tuning.","sentences":["Diffusion models have significantly advanced the field of generative modeling.","However, training a diffusion model is computationally expensive, creating a pressing need to adapt off-the-shelf diffusion models for downstream generation tasks.","Current fine-tuning methods focus on parameter-efficient transfer learning but overlook the fundamental transfer characteristics of diffusion models.","In this paper, we investigate the transferability of diffusion models and observe a monotonous chain of forgetting trend of transferability along the reverse process.","Based on this observation and novel theoretical insights, we present Diff-Tuning, a frustratingly simple transfer approach that leverages the chain of forgetting tendency.","Diff-Tuning encourages the fine-tuned model to retain the pre-trained knowledge at the end of the denoising chain close to the generated data while discarding the other noise side.","We conduct comprehensive experiments to evaluate Diff-Tuning, including the transfer of pre-trained Diffusion Transformer models to eight downstream generations and the adaptation of Stable Diffusion to five control conditions with ControlNet.","Diff-Tuning achieves a 26% improvement over standard fine-tuning and enhances the convergence speed of ControlNet by 24%.","Notably, parameter-efficient transfer learning techniques for diffusion models can also benefit from Diff-Tuning."],"url":"http://arxiv.org/abs/2406.00773v1","category":"cs.LG"}
{"created":"2024-06-02 14:22:09","title":"Once-for-All: Controllable Generative Image Compression with Dynamic Granularity Adaption","abstract":"Although recent generative image compression methods have demonstrated impressive potential in optimizing the rate-distortion-perception trade-off, they still face the critical challenge of flexible rate adaption to diverse compression necessities and scenarios. To overcome this challenge, this paper proposes a Controllable Generative Image Compression framework, Control-GIC, the first capable of fine-grained bitrate adaption across a broad spectrum while ensuring high-fidelity and generality compression. We base Control-GIC on a VQGAN framework representing an image as a sequence of variable-length codes (i.e. VQ-indices), which can be losslessly compressed and exhibits a direct positive correlation with the bitrates. Therefore, drawing inspiration from the classical coding principle, we naturally correlate the information density of local image patches with their granular representations, to achieve dynamic adjustment of the code quantity following different granularity decisions. This implies we can flexibly determine a proper allocation of granularity for the patches to acquire desirable compression rates. We further develop a probabilistic conditional decoder that can trace back to historic encoded multi-granularity representations according to transmitted codes, and then reconstruct hierarchical granular features in the formalization of conditional probability, enabling more informative aggregation to improve reconstruction realism. Our experiments show that Control-GIC allows highly flexible and controllable bitrate adaption and even once compression on an entire dataset to fulfill constrained bitrate conditions. Experimental results demonstrate its superior performance over recent state-of-the-art methods.","sentences":["Although recent generative image compression methods have demonstrated impressive potential in optimizing the rate-distortion-perception trade-off, they still face the critical challenge of flexible rate adaption to diverse compression necessities and scenarios.","To overcome this challenge, this paper proposes a Controllable Generative Image Compression framework, Control-GIC, the first capable of fine-grained bitrate adaption across a broad spectrum while ensuring high-fidelity and generality compression.","We base Control-GIC on a VQGAN framework representing an image as a sequence of variable-length codes (i.e. VQ-indices), which can be losslessly compressed and exhibits a direct positive correlation with the bitrates.","Therefore, drawing inspiration from the classical coding principle, we naturally correlate the information density of local image patches with their granular representations, to achieve dynamic adjustment of the code quantity following different granularity decisions.","This implies we can flexibly determine a proper allocation of granularity for the patches to acquire desirable compression rates.","We further develop a probabilistic conditional decoder that can trace back to historic encoded multi-granularity representations according to transmitted codes, and then reconstruct hierarchical granular features in the formalization of conditional probability, enabling more informative aggregation to improve reconstruction realism.","Our experiments show that Control-GIC allows highly flexible and controllable bitrate adaption and even once compression on an entire dataset to fulfill constrained bitrate conditions.","Experimental results demonstrate its superior performance over recent state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.00758v1","category":"eess.IV"}
{"created":"2024-06-02 13:13:46","title":"Global Rewards in Restless Multi-Armed Bandits","abstract":"Restless multi-armed bandits (RMAB) extend multi-armed bandits so pulling an arm impacts future states. Despite the success of RMABs, a key limiting assumption is the separability of rewards into a sum across arms. We address this deficiency by proposing restless-multi-armed bandit with global rewards (RMAB-G), a generalization of RMABs to global non-separable rewards. To solve RMAB-G, we develop the Linear- and Shapley-Whittle indices, which extend Whittle indices from RMABs to RMAB-Gs. We prove approximation bounds but also point out how these indices could fail when reward functions are highly non-linear. To overcome this, we propose two sets of adaptive policies: the first computes indices iteratively, and the second combines indices with Monte-Carlo Tree Search (MCTS). Empirically, we demonstrate that our proposed policies outperform baselines and index-based policies with synthetic data and real-world data from food rescue.","sentences":["Restless multi-armed bandits (RMAB) extend multi-armed bandits so pulling an arm impacts future states.","Despite the success of RMABs, a key limiting assumption is the separability of rewards into a sum across arms.","We address this deficiency by proposing restless-multi-armed bandit with global rewards (RMAB-G), a generalization of RMABs to global non-separable rewards.","To solve RMAB-G, we develop the Linear- and Shapley-Whittle indices, which extend Whittle indices from RMABs to RMAB-Gs.","We prove approximation bounds but also point out how these indices could fail when reward functions are highly non-linear.","To overcome this, we propose two sets of adaptive policies: the first computes indices iteratively, and the second combines indices with Monte-Carlo Tree Search (MCTS).","Empirically, we demonstrate that our proposed policies outperform baselines and index-based policies with synthetic data and real-world data from food rescue."],"url":"http://arxiv.org/abs/2406.00738v1","category":"cs.LG"}
{"created":"2024-06-02 12:36:43","title":"Role of particle diffusion in shaping the gravitational wave signal from neutron star inspirals","abstract":"It is commonly believed that the dissipative properties of superdense matter play a negligible role in modeling gravitational waveforms from neutron star inspirals. This study aims to investigate whether this presumption holds true for the often neglected dissipative process associated with particle diffusion in superconducting neutron stars. As we demonstrate, diffusion effects can significantly impact the phase of the gravitational wave from the inspiral, manifesting at a magnitude of a few tens of milliradians at large orbit separations, equivalent to orbital frequencies of a few hertz. We also find that dissipation resulting from particle diffusion might increase the neutron star's temperature to approximately $10^7\\rm K$ during the inspiral.","sentences":["It is commonly believed that the dissipative properties of superdense matter play a negligible role in modeling gravitational waveforms from neutron star inspirals.","This study aims to investigate whether this presumption holds true for the often neglected dissipative process associated with particle diffusion in superconducting neutron stars.","As we demonstrate, diffusion effects can significantly impact the phase of the gravitational wave from the inspiral, manifesting at a magnitude of a few tens of milliradians at large orbit separations, equivalent to orbital frequencies of a few hertz.","We also find that dissipation resulting from particle diffusion might increase the neutron star's temperature to approximately $10^7\\rm K$ during the inspiral."],"url":"http://arxiv.org/abs/2406.00729v1","category":"astro-ph.HE"}
{"created":"2024-06-02 12:00:24","title":"Linear Degeneracy in a Class of Nonlinear Second-Order Hyperbolic Systems","abstract":"For a class of nonlinear hyperbolic systems of second order the paper shows that all Lax modes associated with their first-order formulations are linearly degenerate. This property holds for recently considered models of dissipative relativistic fluid dynamics, supporting the possibility that solutions to these models generally avoid singularity formation.","sentences":["For a class of nonlinear hyperbolic systems of second order the paper shows that all Lax modes associated with their first-order formulations are linearly degenerate.","This property holds for recently considered models of dissipative relativistic fluid dynamics, supporting the possibility that solutions to these models generally avoid singularity formation."],"url":"http://arxiv.org/abs/2406.00719v1","category":"math.AP"}
{"created":"2024-06-02 10:52:48","title":"An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine Composites","abstract":"The Tsetlin Machine (TM) has achieved competitive results on several image classification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2. However, color image classification is arguably still in its infancy for TMs, with CIFAR-10 being a focal point for tracking progress. Over the past few years, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in 2023 with the introduction of Drop Clause. In this paper, we leverage the recently proposed TM Composites architecture and introduce a range of TM Specialists that use various image processing techniques. These include Canny edge detection, Histogram of Oriented Gradients, adaptive mean thresholding, adaptive Gaussian thresholding, Otsu's thresholding, color thermometers, and adaptive color thermometers. In addition, we conduct a rigorous hyperparameter search, where we uncover optimal hyperparameters for several of the TM Specialists. The result is a toolbox that provides new state-of-the-art results on CIFAR-10 for TMs with an accuracy of 82.8%. In conclusion, our toolbox of TM Specialists forms a foundation for new TM applications and a landmark for further research on TM Composites in image analysis.","sentences":["The Tsetlin Machine (TM) has achieved competitive results on several image classification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2.","However, color image classification is arguably still in its infancy for TMs, with CIFAR-10 being a focal point for tracking progress.","Over the past few years, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in 2023 with the introduction of Drop Clause.","In this paper, we leverage the recently proposed TM Composites architecture and introduce a range of TM Specialists that use various image processing techniques.","These include Canny edge detection, Histogram of Oriented Gradients, adaptive mean thresholding, adaptive Gaussian thresholding, Otsu's thresholding, color thermometers, and adaptive color thermometers.","In addition, we conduct a rigorous hyperparameter search, where we uncover optimal hyperparameters for several of the TM Specialists.","The result is a toolbox that provides new state-of-the-art results on CIFAR-10 for TMs with an accuracy of 82.8%.","In conclusion, our toolbox of TM Specialists forms a foundation for new TM applications and a landmark for further research on TM Composites in image analysis."],"url":"http://arxiv.org/abs/2406.00704v1","category":"cs.CV"}
{"created":"2024-06-02 10:51:40","title":"A Partition-insensitive Parallel Framework for Distributed Model Fitting","abstract":"Distributed model fitting refers to the process of fitting a mathematical or statistical model to the data using distributed computing resources, such that computing tasks are divided among multiple interconnected computers or nodes, often organized in a cluster or network. Most of the existing methods for distributed model fitting are to formulate it in a consensus optimization problem, and then build up algorithms based on the alternating direction method of multipliers (ADMM). This paper introduces a novel parallel framework for achieving a distributed model fitting. In contrast to previous consensus frameworks, the introduced parallel framework offers two notable advantages. Firstly, it exhibits insensitivity to sample partitioning, meaning that the solution of the algorithm remains unaffected by variations in the number of slave nodes or/and the amount of data each node carries. Secondly, fewer variables are required to be updated at each iteration, so that the proposed parallel framework performs in a more succinct and efficient way, and adapts to high-dimensional data. In addition, we prove that the algorithms under the new parallel framework have a worst-case linear convergence rate in theory. Numerical experiments confirm the generality, robustness, and accuracy of our proposed parallel framework.","sentences":["Distributed model fitting refers to the process of fitting a mathematical or statistical model to the data using distributed computing resources, such that computing tasks are divided among multiple interconnected computers or nodes, often organized in a cluster or network.","Most of the existing methods for distributed model fitting are to formulate it in a consensus optimization problem, and then build up algorithms based on the alternating direction method of multipliers (ADMM).","This paper introduces a novel parallel framework for achieving a distributed model fitting.","In contrast to previous consensus frameworks, the introduced parallel framework offers two notable advantages.","Firstly, it exhibits insensitivity to sample partitioning, meaning that the solution of the algorithm remains unaffected by variations in the number of slave nodes or/and the amount of data each node carries.","Secondly, fewer variables are required to be updated at each iteration, so that the proposed parallel framework performs in a more succinct and efficient way, and adapts to high-dimensional data.","In addition, we prove that the algorithms under the new parallel framework have a worst-case linear convergence rate in theory.","Numerical experiments confirm the generality, robustness, and accuracy of our proposed parallel framework."],"url":"http://arxiv.org/abs/2406.00703v1","category":"stat.CO"}
{"created":"2024-06-02 09:36:37","title":"Exploiting Frequency Correlation for Hyperspectral Image Reconstruction","abstract":"Deep priors have emerged as potent methods in hyperspectral image (HSI) reconstruction. While most methods emphasize space-domain learning using image space priors like non-local similarity, frequency-domain learning using image frequency priors remains neglected, limiting the reconstruction capability of networks. In this paper, we first propose a Hyperspectral Frequency Correlation (HFC) prior rooted in in-depth statistical frequency analyses of existent HSI datasets. Leveraging the HFC prior, we subsequently establish the frequency domain learning composed of a Spectral-wise self-Attention of Frequency (SAF) and a Spectral-spatial Interaction of Frequency (SIF) targeting low-frequency and high-frequency components, respectively. The outputs of SAF and SIF are adaptively merged by a learnable gating filter, thus achieving a thorough exploitation of image frequency priors. Integrating the frequency domain learning and the existing space domain learning, we finally develop the Correlation-driven Mixing Domains Transformer (CMDT) for HSI reconstruction. Extensive experiments highlight that our method surpasses various state-of-the-art (SOTA) methods in reconstruction quality and computational efficiency.","sentences":["Deep priors have emerged as potent methods in hyperspectral image (HSI) reconstruction.","While most methods emphasize space-domain learning using image space priors like non-local similarity, frequency-domain learning using image frequency priors remains neglected, limiting the reconstruction capability of networks.","In this paper, we first propose a Hyperspectral Frequency Correlation (HFC) prior rooted in in-depth statistical frequency analyses of existent HSI datasets.","Leveraging the HFC prior, we subsequently establish the frequency domain learning composed of a Spectral-wise self-Attention of Frequency (SAF) and a Spectral-spatial Interaction of Frequency (SIF) targeting low-frequency and high-frequency components, respectively.","The outputs of SAF and SIF are adaptively merged by a learnable gating filter, thus achieving a thorough exploitation of image frequency priors.","Integrating the frequency domain learning and the existing space domain learning, we finally develop the Correlation-driven Mixing Domains Transformer (CMDT) for HSI reconstruction.","Extensive experiments highlight that our method surpasses various state-of-the-art (SOTA) methods in reconstruction quality and computational efficiency."],"url":"http://arxiv.org/abs/2406.00683v1","category":"eess.IV"}
{"created":"2024-06-02 08:55:44","title":"Nonlinear Stability of First-Order Relativistic Viscous Hydrodynamics","abstract":"This paper shows nonlinear stability of homogeneous states in second-order hyperbolic systems of partial differential equations that model the dynamics of dissipative relativistic fluids, by checking a dissipativity criterion formulated earlier by the authors and invoking a recent general result by the second author on long-time existence and time-asymptotic stability of small-data solutions to nonlinear hyperbolic systems.","sentences":["This paper shows nonlinear stability of homogeneous states in second-order hyperbolic systems of partial differential equations that model the dynamics of dissipative relativistic fluids, by checking a dissipativity criterion formulated earlier by the authors and invoking a recent general result by the second author on long-time existence and time-asymptotic stability of small-data solutions to nonlinear hyperbolic systems."],"url":"http://arxiv.org/abs/2406.00673v1","category":"math.AP"}
{"created":"2024-06-02 08:40:30","title":"An Efficient Trajectory Generation for Bi-copter Flight in Tight Space","abstract":"Unlike squared (or alike) quadrotors, elongated bi-copters leverage natural superiority in crossing tight spaces. To date, extensive works have focused on the design, modeling, and control of bi-copters. Besides, a proper motion planner utilizing bi-copters' shape characteristics is essential to efficiently and safely traverse tight spaces, yet it has rarely been studied. Current motion planning methods will significantly compromise their ability to traverse narrow spaces if the map is inflated based on the long dimension of the bi-copter. In this paper, we propose an efficient motion planning method that enables the safe navigation of bi-copters through narrow spaces. We first adapt a dynamic, feasible path-finding algorithm with whole-body collision checks to generate a collision-free path. Subsequently, we jointly optimize the position and rotation of the bi-copter to produce a trajectory that is safe, dynamically feasible, and smooth. Extensive simulations and real-world experiments have been conducted to verify the reliability and robustness of the proposed method.","sentences":["Unlike squared (or alike) quadrotors, elongated bi-copters leverage natural superiority in crossing tight spaces.","To date, extensive works have focused on the design, modeling, and control of bi-copters.","Besides, a proper motion planner utilizing bi-copters' shape characteristics is essential to efficiently and safely traverse tight spaces, yet it has rarely been studied.","Current motion planning methods will significantly compromise their ability to traverse narrow spaces if the map is inflated based on the long dimension of the bi-copter.","In this paper, we propose an efficient motion planning method that enables the safe navigation of bi-copters through narrow spaces.","We first adapt a dynamic, feasible path-finding algorithm with whole-body collision checks to generate a collision-free path.","Subsequently, we jointly optimize the position and rotation of the bi-copter to produce a trajectory that is safe, dynamically feasible, and smooth.","Extensive simulations and real-world experiments have been conducted to verify the reliability and robustness of the proposed method."],"url":"http://arxiv.org/abs/2406.00671v1","category":"cs.RO"}
{"created":"2024-06-04 17:39:15","title":"Bottom spectrum of three-dimensional manifolds with scalar curvature lower bound","abstract":"A classical result of Cheng states that the bottom spectrum of complete manifolds of fixed dimension and Ricci curvature lower bound achieves its maximal value on the corresponding hyperbolic space. The paper establishes an analogous result for three-dimensional complete manifolds with scalar curvature lower bound subject to some necessary topological assumptions. The rigidity issue is also addressed and a splitting theorem is obtained for such manifolds with the maximal bottom spectrum.","sentences":["A classical result of Cheng states that the bottom spectrum of complete manifolds of fixed dimension and Ricci curvature lower bound achieves its maximal value on the corresponding hyperbolic space.","The paper establishes an analogous result for three-dimensional complete manifolds with scalar curvature lower bound subject to some necessary topological assumptions.","The rigidity issue is also addressed and a splitting theorem is obtained for such manifolds with the maximal bottom spectrum."],"url":"http://arxiv.org/abs/2406.02516v1","category":"math.DG"}
{"created":"2024-06-04 17:24:10","title":"Tensor Network Space-Time Spectral Collocation Method for Solving the Nonlinear Convection Diffusion Equation","abstract":"Spectral methods provide highly accurate numerical solutions for partial differential equations, exhibiting exponential convergence with the number of spectral nodes. Traditionally, in addressing time-dependent nonlinear problems, attention has been on low-order finite difference schemes for time discretization and spectral element schemes for spatial variables. However, our recent developments have resulted in the application of spectral methods to both space and time variables, preserving spectral convergence in both domains. Leveraging Tensor Train techniques, our approach tackles the curse of dimensionality inherent in space-time methods. Here, we extend this methodology to the nonlinear time-dependent convection-diffusion equation. Our discretization scheme exhibits a low-rank structure, facilitating translation to tensor-train (TT) format. Nevertheless, controlling the TT-rank across Newton's iterations, needed to deal with the nonlinearity, poses a challenge, leading us to devise the \"Step Truncation TT-Newton\" method. We demonstrate the exponential convergence of our methods through various benchmark examples. Importantly, our scheme offers significantly reduced memory requirement compared to the full-grid scheme.","sentences":["Spectral methods provide highly accurate numerical solutions for partial differential equations, exhibiting exponential convergence with the number of spectral nodes.","Traditionally, in addressing time-dependent nonlinear problems, attention has been on low-order finite difference schemes for time discretization and spectral element schemes for spatial variables.","However, our recent developments have resulted in the application of spectral methods to both space and time variables, preserving spectral convergence in both domains.","Leveraging Tensor Train techniques, our approach tackles the curse of dimensionality inherent in space-time methods.","Here, we extend this methodology to the nonlinear time-dependent convection-diffusion equation.","Our discretization scheme exhibits a low-rank structure, facilitating translation to tensor-train (TT) format.","Nevertheless, controlling the TT-rank across Newton's iterations, needed to deal with the nonlinearity, poses a challenge, leading us to devise the \"Step Truncation TT-Newton\" method.","We demonstrate the exponential convergence of our methods through various benchmark examples.","Importantly, our scheme offers significantly reduced memory requirement compared to the full-grid scheme."],"url":"http://arxiv.org/abs/2406.02505v1","category":"math.NA"}
{"created":"2024-06-04 17:20:42","title":"Singular Subspace Perturbation Bounds via Rectangular Random Matrix Diffusions","abstract":"Given a matrix $A \\in \\mathbb{R}^{m\\times d}$ with singular values $\\sigma_1\\geq \\cdots \\geq \\sigma_d$, and a random matrix $G \\in \\mathbb{R}^{m\\times d}$ with iid $N(0,T)$ entries for some $T>0$, we derive new bounds on the Frobenius distance between subspaces spanned by the top-$k$ (right) singular vectors of $A$ and $A+G$. This problem arises in numerous applications in statistics where a data matrix may be corrupted by Gaussian noise, and in the analysis of the Gaussian mechanism in differential privacy, where Gaussian noise is added to data to preserve private information. We show that, for matrices $A$ where the gaps in the top-$k$ singular values are roughly $\\Omega(\\sigma_k-\\sigma_{k+1})$ the expected Frobenius distance between the subspaces is $\\tilde{O}(\\frac{\\sqrt{d}}{\\sigma_k-\\sigma_{k+1}} \\times \\sqrt{T})$, improving on previous bounds by a factor of $\\frac{\\sqrt{m}}{\\sqrt{d}} \\sqrt{k}$. To obtain our bounds we view the perturbation to the singular vectors as a diffusion process -- the Dyson-Bessel process -- and use tools from stochastic calculus to track the evolution of the subspace spanned by the top-$k$ singular vectors.","sentences":["Given a matrix $A \\in \\mathbb{R}^{m\\times d}$ with singular values $\\sigma_1\\geq \\cdots \\geq \\sigma_d$, and a random matrix $G \\in \\mathbb{R}^{m\\times d}$ with iid $N(0,T)$ entries for some $T>0$, we derive new bounds on the Frobenius distance between subspaces spanned by the top-$k$ (right) singular vectors of $A$ and $A+G$. This problem arises in numerous applications in statistics where a data matrix may be corrupted by Gaussian noise, and in the analysis of the Gaussian mechanism in differential privacy, where Gaussian noise is added to data to preserve private information.","We show that, for matrices $A$ where the gaps in the top-$k$ singular values are roughly $\\Omega(\\sigma_k-\\sigma_{k+1})$ the expected Frobenius distance between the subspaces is $\\tilde{O}(\\frac{\\sqrt{d}}{\\sigma_k-\\sigma_{k+1}} \\times \\sqrt{T})$, improving on previous bounds by a factor of $\\frac{\\sqrt{m}}{\\sqrt{d}} \\sqrt{k}$. To obtain our bounds we view the perturbation to the singular vectors as a diffusion process -- the Dyson-Bessel process -- and use tools from stochastic calculus to track the evolution of the subspace spanned by the top-$k$ singular vectors."],"url":"http://arxiv.org/abs/2406.02502v1","category":"math.ST"}
{"created":"2024-06-04 17:04:15","title":"Neural network sampling of Bethe-Heitler process in particle-in-cell codes","abstract":"This study uses neural networks to improve Monte Carlo (MC) implementations of the Bethe-Heitler process in Particle-In-Cell (PIC) codes. We provide a neural network that is as accurate as pre-calculated tables, and requires a hundred times less memory to store. It is trained to predict Bethe-Heitler pair production cross-sections for atomic numbers 1-50 and photon energies between 1 MeV and 10 GeV in the PIC code OSIRIS. We first validate our approach against a theoretical estimate in a simplified context. We later prove that both approaches have similar performance in a typical relativistic laser-plasma interaction scenario. The large memory decrease accessible with neural networks will enable introducing more advanced cross-section models for Bethe-Heitler pair production and other QED mechanisms in the MC modules of PIC codes.","sentences":["This study uses neural networks to improve Monte Carlo (MC) implementations of the Bethe-Heitler process in Particle-In-Cell (PIC) codes.","We provide a neural network that is as accurate as pre-calculated tables, and requires a hundred times less memory to store.","It is trained to predict Bethe-Heitler pair production cross-sections for atomic numbers 1-50 and photon energies between 1 MeV and 10 GeV in the PIC code OSIRIS.","We first validate our approach against a theoretical estimate in a simplified context.","We later prove that both approaches have similar performance in a typical relativistic laser-plasma interaction scenario.","The large memory decrease accessible with neural networks will enable introducing more advanced cross-section models for Bethe-Heitler pair production and other QED mechanisms in the MC modules of PIC codes."],"url":"http://arxiv.org/abs/2406.02491v1","category":"physics.comp-ph"}
{"created":"2024-06-04 17:00:14","title":"Ai-Sampler: Adversarial Learning of Markov kernels with involutive maps","abstract":"Markov chain Monte Carlo methods have become popular in statistics as versatile techniques to sample from complicated probability distributions. In this work, we propose a method to parameterize and train transition kernels of Markov chains to achieve efficient sampling and good mixing. This training procedure minimizes the total variation distance between the stationary distribution of the chain and the empirical distribution of the data. Our approach leverages involutive Metropolis-Hastings kernels constructed from reversible neural networks that ensure detailed balance by construction. We find that reversibility also implies $C_2$-equivariance of the discriminator function which can be used to restrict its function space.","sentences":["Markov chain Monte Carlo methods have become popular in statistics as versatile techniques to sample from complicated probability distributions.","In this work, we propose a method to parameterize and train transition kernels of Markov chains to achieve efficient sampling and good mixing.","This training procedure minimizes the total variation distance between the stationary distribution of the chain and the empirical distribution of the data.","Our approach leverages involutive Metropolis-Hastings kernels constructed from reversible neural networks that ensure detailed balance by construction.","We find that reversibility also implies $C_2$-equivariance of the discriminator function which can be used to restrict its function space."],"url":"http://arxiv.org/abs/2406.02490v1","category":"cs.LG"}
{"created":"2024-06-04 16:44:59","title":"A Lazard-type correspondence and applications to post-Lie rings and skew braces","abstract":"Through a Hopf algebraic approach, we develop a framework for formal differentiation and integration and prove a correspondence theorem that comprises the Lazard correspondence, the Malcev correspondence and the formal integration of complete connected Lie algebras with characteristic 0. We then apply this theory to obtain a correspondence theorem for classes of left nilpotent skew braces on a nilpotent group and left nilpotent post-Lie rings on a nilpotent Lie ring.","sentences":["Through a Hopf algebraic approach, we develop a framework for formal differentiation and integration and prove a correspondence theorem that comprises the Lazard correspondence, the Malcev correspondence and the formal integration of complete connected Lie algebras with characteristic 0.","We then apply this theory to obtain a correspondence theorem for classes of left nilpotent skew braces on a nilpotent group and left nilpotent post-Lie rings on a nilpotent Lie ring."],"url":"http://arxiv.org/abs/2406.02475v1","category":"math.RA"}
{"created":"2024-06-04 16:43:53","title":"An Observation About Weak Solutions of Linear Differential Equations in Hilbert Spaces","abstract":"In this note we propose a definition of weak solution for an abstract Cauchy problem in a Hilbert space, and we discuss existence and uniqueness results.","sentences":["In this note we propose a definition of weak solution for an abstract Cauchy problem in a Hilbert space, and we discuss existence and uniqueness results."],"url":"http://arxiv.org/abs/2406.02474v1","category":"math.AP"}
{"created":"2024-06-04 15:37:12","title":"Quasi-two-body decays $B\\to P f_0(500)\\to P\u03c0^+\u03c0^-$ in the perturbative QCD approach","abstract":"In this paper, we study the quasi-two-body decays $B\\to P f_0(500)\\to P\\pi^+\\pi^-$ (with $P=(\\pi, K, \\eta, \\eta^{\\prime})$) within framework of perturbative QCD (PQCD) factorization approach. With the help of $\\pi$-$\\pi$ distribution amplitude and scalar form factor $F_{\\pi\\pi}(\\omega^2)$, we calculate the CP averaged branching fraction and the CP asymmetry for the quasi-two-body decays $B\\to P f_0(500)\\to P\\pi^+\\pi^-$. Taking the quasi-two-body decay $B^+ \\to \\pi^+ f_0(500) \\to \\pi^+ \\pi^+ \\pi^-$ as an explicit example, we present the behaviour of differential branching fraction and direct CP violation versus the $\\pi$-$\\pi$ invariant mass. The total branching fraction and direct CP violation are $\\mathcal{B}(B^+\\to \\pi^+ [\\sigma\\to]\\pi^+\\pi^-) = (1.78 \\pm 0.41\\pm 0.51) \\times 10^{-6}$ and $\\mathcal{A}_{\\rm CP}(B^+\\to \\pi^+ [\\sigma\\to]\\pi^+\\pi^-) = (29.8\\pm 11.1\\pm 13.0)\\%$ respectively. Our results could be tested by further experiments.","sentences":["In this paper, we study the quasi-two-body decays $B\\to P f_0(500)\\to P\\pi^+\\pi^-$ (with $P=(\\pi, K, \\eta, \\eta^{\\prime})$) within framework of perturbative QCD (PQCD) factorization approach.","With the help of $\\pi$-$\\pi$ distribution amplitude and scalar form factor $F_{\\pi\\pi}(\\omega^2)$, we calculate the CP averaged branching fraction and the CP asymmetry for the quasi-two-body decays $B\\to P f_0(500)\\to P\\pi^+\\pi^-$.","Taking the quasi-two-body decay $B^+ \\to \\pi^+ f_0(500)","\\to \\pi^+ \\pi^+ \\pi^-$ as an explicit example, we present the behaviour of differential branching fraction and direct CP violation versus the $\\pi$-$\\pi$ invariant mass.","The total branching fraction and direct CP violation are $\\mathcal{B}(B^+\\to \\pi^+ [\\sigma\\to]\\pi^+\\pi^-) = (1.78 \\pm 0.41\\pm 0.51)","\\times 10^{-6}$ and $\\mathcal{A}_{\\rm CP}(B^+\\to \\pi^+ [\\sigma\\to]\\pi^+\\pi^-) = (29.8\\pm 11.1\\pm 13.0)\\%$ respectively.","Our results could be tested by further experiments."],"url":"http://arxiv.org/abs/2406.02419v1","category":"hep-ph"}
{"created":"2024-06-04 14:50:26","title":"WST -- Widefield Spectroscopic Telescope: design of a new 12m class telescope dedicated to widefield Multi-object and Integral Field Spectroscopy","abstract":"The Wide-Field Spectroscopic Telescope (WST) is a concept for a 12-m class seeing-limited telescope providing two concentric fields of view for simultaneous Multi-Object Spectroscopy and Integral Field Spectroscopy. The specified wavelength range is 0.35-1.6 microns. The baseline optical design relies on a corrected Cassegrain solution feeding Multi-Object spectrographs through fibres, while the central area of the field is propagated down to a gravity-stable Integral Field Station housing 144 spectrographs. The Cassegrain corrector also provides for atmospheric dispersion compensation. All optical components are within commercially available dimensions. With a view to minimizing risks and costs, to the maximum possible extent the telescope relies on proven subsystem solutions. An exception is the tip-tilt secondary mirror, which would likely have to provide some rejection of wind shake. An iteration of the optical design is ongoing, with a view to mitigating the weaknesses of the first baseline design. The telescope would be wavefront-controlled on-sky at the common-path MOS focus. Controls in the IFS path will need to compensate for the effect of subsequent differentials - wavefront and line of sight. There is no shortage of degrees of freedom and metrology solution to do so. The size of the dome is driven by the Nasmyth footprint and the height of the pier, which houses the IFS station. The baseline assumption is that a VLT-like enclosure would provide suitable shielding and ventilation.","sentences":["The Wide-Field Spectroscopic Telescope (WST) is a concept for a 12-m class seeing-limited telescope providing two concentric fields of view for simultaneous Multi-Object Spectroscopy and Integral Field Spectroscopy.","The specified wavelength range is 0.35-1.6 microns.","The baseline optical design relies on a corrected Cassegrain solution feeding Multi-Object spectrographs through fibres, while the central area of the field is propagated down to a gravity-stable Integral Field Station housing 144 spectrographs.","The Cassegrain corrector also provides for atmospheric dispersion compensation.","All optical components are within commercially available dimensions.","With a view to minimizing risks and costs, to the maximum possible extent the telescope relies on proven subsystem solutions.","An exception is the tip-tilt secondary mirror, which would likely have to provide some rejection of wind shake.","An iteration of the optical design is ongoing, with a view to mitigating the weaknesses of the first baseline design.","The telescope would be wavefront-controlled on-sky at the common-path MOS focus.","Controls in the IFS path will need to compensate for the effect of subsequent differentials - wavefront and line of sight.","There is no shortage of degrees of freedom and metrology solution to do so.","The size of the dome is driven by the Nasmyth footprint and the height of the pier, which houses the IFS station.","The baseline assumption is that a VLT-like enclosure would provide suitable shielding and ventilation."],"url":"http://arxiv.org/abs/2406.02373v1","category":"astro-ph.IM"}
{"created":"2024-06-04 14:49:59","title":"Braided scalar quantum field theory","abstract":"We formulate scalar field theories in a curved braided $L_\\infty$-algebra formalism and analyse their correlation functions using Batalin-Vilkovisky quantization. We perform detailed calculations in cubic braided scalar field theory up to two-loop order and three-point multiplicity. The divergent tadpole contributions are eliminated by a suitable choice of central curvature for the $L_\\infty$-structure, and we confirm the absence of UV/IR mixing. The calculations of higher loop and higher multiplicity correlators in homological perturbation theory are facilitated by the introduction of a novel diagrammatic calculus. We derive an algebraic version of the Schwinger-Dyson equations based on the homological perturbation lemma, and use them to prove the braided Wick theorem.","sentences":["We formulate scalar field theories in a curved braided $L_\\infty$-algebra formalism and analyse their correlation functions using Batalin-Vilkovisky quantization.","We perform detailed calculations in cubic braided scalar field theory up to two-loop order and three-point multiplicity.","The divergent tadpole contributions are eliminated by a suitable choice of central curvature for the $L_\\infty$-structure, and we confirm the absence of UV/IR mixing.","The calculations of higher loop and higher multiplicity correlators in homological perturbation theory are facilitated by the introduction of a novel diagrammatic calculus.","We derive an algebraic version of the Schwinger-Dyson equations based on the homological perturbation lemma, and use them to prove the braided Wick theorem."],"url":"http://arxiv.org/abs/2406.02372v1","category":"hep-th"}
{"created":"2024-06-04 14:25:28","title":"Integral curvature estimates for solutions to Ricci flow with $L^p$ bounded scalar curvature","abstract":"In this paper we prove $\\textit{localised, weighted}$ curvature integral estimates for solutions to the Ricci flow in the setting of a smooth four dimensional Ricci flow or a closed $n$-dimensional K\\\"ahler Ricci flow. These integral estimates improve and extend the integral curvature estimates shown by the second author in an earlier paper. If the scalar curvature is uniformly bounded in the spatial $L^p$ sense for some $p>2,$ then the estimates imply a uniform bound on the spatial $L^2$ norm of the Riemannian curvature tensor. Stronger integral estimates are shown to hold if one further assumes a weak non-inflating condition. In a sequel paper, we show that in many natural settings, a non-inflating condition holds.","sentences":["In this paper we prove $\\textit{localised, weighted}$ curvature integral estimates for solutions to the Ricci flow in the setting of a smooth four dimensional Ricci flow or a closed $n$-dimensional K\\\"ahler Ricci flow.","These integral estimates improve and extend the integral curvature estimates shown by the second author in an earlier paper.","If the scalar curvature is uniformly bounded in the spatial $L^p$ sense for some $p>2,$ then the estimates imply a uniform bound on the spatial $L^2$ norm of the Riemannian curvature tensor.","Stronger integral estimates are shown to hold if one further assumes a weak non-inflating condition.","In a sequel paper, we show that in many natural settings, a non-inflating condition holds."],"url":"http://arxiv.org/abs/2406.02351v1","category":"math.DG"}
{"created":"2024-06-04 14:17:44","title":"Geometric interpretation of the vanishing Lie Bracket for two-dimensional rough vector fields","abstract":"In this paper, we prove that if $X,Y$ are continuous, Sobolev vector fields with bounded divergence on the real plane and $[X,Y]=0$, then their flows commute. In particular, we improve the previous result of Colombo-Tione (2021), where the authors require the additional assumption of the weak Lie differentiability on one of the two flows. We also discuss possible extensions to the $\\text{BV}$ setting.","sentences":["In this paper, we prove that if $X,Y$ are continuous, Sobolev vector fields with bounded divergence on the real plane and $[X,Y]=0$, then their flows commute.","In particular, we improve the previous result of Colombo-Tione (2021), where the authors require the additional assumption of the weak Lie differentiability on one of the two flows.","We also discuss possible extensions to the $\\text{BV}$ setting."],"url":"http://arxiv.org/abs/2406.02340v1","category":"math.AP"}
{"created":"2024-06-04 13:54:57","title":"The geometric Toda equations for noncompact symmetric spaces","abstract":"This paper has two purposes. The first is to classify all those versions of the Toda equations which govern the existence of $\\tau$-primitive harmonic maps from a surface into a homogeneous space $G/T$ for which $G$ is a noncomplex noncompact simple real Lie group and $T$ is a maximal compact torus, i.e., a maximal torus inside a maximal compact subgroup $H < G$. Here $\\tau$ is the Coxeter automorphism which Drinfel'd & Sokolov assigned to each affine Dynkin diagram. This allows $\\tau$ to be either an inner or an outer automorphism. We show that, up to equivalence, the real forms $G<G^\\mathbb{C}$ which are compatible with $\\tau$ can be classified using a simple labelling of the corresponding affine diagram.   The second purpose is to establish when stability criteria can be used to prove the existence of solutions. We interpret the Toda equations over a compact Riemann surface $\\Sigma$ as equations for a metric on a holomorphic principal $T^\\mathbb{C}$-bundle $Q^\\mathbb{C}$ over $\\Sigma$. The corresponding Chern connection, when combined with a holomorphic field $\\varphi$, produces a $G$-connection which is flat precisely when the Toda equations hold. We call the pair $(Q^\\mathbb{C},\\varphi)$ a Toda pair. We classify those real forms of the Toda equations for which the Toda pair is a principal pair (in the sense of Bradlow et al.) and we call these totally noncompact Toda pairs. Using the stability theory for principal pairs we prove that for totally noncompact cyclic Toda pairs $(Q^\\mathbb{C},\\varphi)$ the corresponding Toda equations always admit solutions. Every solution to the geometric Toda equations has a corresponding $G$-Higgs bundle. We explain how to construct this $G$-Higgs bundle directly from the Toda pair and show that Baraglia's cyclic Higgs bundles arise from a very special case of totally noncompact cyclic Toda pairs.","sentences":["This paper has two purposes.","The first is to classify all those versions of the Toda equations which govern the existence of $\\tau$-primitive harmonic maps from a surface into a homogeneous space $G/T$ for which $G$ is a noncomplex noncompact simple real Lie group and $T$ is a maximal compact torus, i.e., a maximal torus inside a maximal compact subgroup $H < G$.","Here $\\tau$ is the Coxeter automorphism which Drinfel'd & Sokolov assigned to each affine Dynkin diagram.","This allows $\\tau$ to be either an inner or an outer automorphism.","We show that, up to equivalence, the real forms $G<G^\\mathbb{C}$ which are compatible with $\\tau$ can be classified using a simple labelling of the corresponding affine diagram.   ","The second purpose is to establish when stability criteria can be used to prove the existence of solutions.","We interpret the Toda equations over a compact Riemann surface $\\Sigma$ as equations for a metric on a holomorphic principal $T^\\mathbb{C}$-bundle $Q^\\mathbb{C}$ over $\\Sigma$. The corresponding Chern connection, when combined with a holomorphic field $\\varphi$, produces a $G$-connection which is flat precisely when the Toda equations hold.","We call the pair $(Q^\\mathbb{C},\\varphi)$ a Toda pair.","We classify those real forms of the Toda equations for which the Toda pair is a principal pair (in the sense of Bradlow et al.)","and we call these totally noncompact Toda pairs.","Using the stability theory for principal pairs we prove that for totally noncompact cyclic Toda pairs $(Q^\\mathbb{C},\\varphi)$ the corresponding Toda equations always admit solutions.","Every solution to the geometric Toda equations has a corresponding $G$-Higgs bundle.","We explain how to construct this $G$-Higgs bundle directly from the Toda pair and show that Baraglia's cyclic Higgs bundles arise from a very special case of totally noncompact cyclic Toda pairs."],"url":"http://arxiv.org/abs/2406.02323v1","category":"math.DG"}
{"created":"2024-06-04 13:51:11","title":"Pricing and calibration in the 4-factor path-dependent volatility model","abstract":"We consider the path-dependent volatility (PDV) model of Guyon and Lekeufack (2023), where the instantaneous volatility is a linear combination of a weighted sum of past returns and the square root of a weighted sum of past squared returns. We discuss the influence of an additional parameter that unlocks enough volatility on the upside to reproduce the implied volatility smiles of S&P 500 and VIX options. This PDV model, motivated by empirical studies, comes with computational challenges, especially in relation to VIX options pricing and calibration. We propose an accurate neural network approximation of the VIX which leverages on the Markovianity of the 4-factor version of the model. The VIX is learned as a function of the Markovian factors and the model parameters. We use this approximation to tackle the joint calibration of S&P 500 and VIX options.","sentences":["We consider the path-dependent volatility (PDV) model of Guyon and Lekeufack (2023), where the instantaneous volatility is a linear combination of a weighted sum of past returns and the square root of a weighted sum of past squared returns.","We discuss the influence of an additional parameter that unlocks enough volatility on the upside to reproduce the implied volatility smiles of S&P 500 and VIX options.","This PDV model, motivated by empirical studies, comes with computational challenges, especially in relation to VIX options pricing and calibration.","We propose an accurate neural network approximation of the VIX which leverages on the Markovianity of the 4-factor version of the model.","The VIX is learned as a function of the Markovian factors and the model parameters.","We use this approximation to tackle the joint calibration of S&P 500 and VIX options."],"url":"http://arxiv.org/abs/2406.02319v1","category":"q-fin.CP"}
{"created":"2024-06-04 13:38:45","title":"Traffic Response Functions: Patterns, Propagation and Congestion","abstract":"Using empirical data gathered on motorways in Germany, we follow a new approach by further exploring response functions as a possible tool to study traffic dynamics in motorway networks. We uncover the basic characteristics of responses of flow and density to given signals and the capability of responses to capture the correlation between these fundamental observables. Furthermore, we uncover the potential use of responses to characterize traffic patterns. We are able to demonstrate the differentiation of congestion patterns and the determination of the propagation velocity of moving congestion.","sentences":["Using empirical data gathered on motorways in Germany, we follow a new approach by further exploring response functions as a possible tool to study traffic dynamics in motorway networks.","We uncover the basic characteristics of responses of flow and density to given signals and the capability of responses to capture the correlation between these fundamental observables.","Furthermore, we uncover the potential use of responses to characterize traffic patterns.","We are able to demonstrate the differentiation of congestion patterns and the determination of the propagation velocity of moving congestion."],"url":"http://arxiv.org/abs/2406.02307v1","category":"physics.soc-ph"}
{"created":"2024-06-04 13:19:06","title":"Solving Partial Differential Equations in Different Domains by Operator Learning method Based on Boundary Integral Equations","abstract":"This article explores operator learning models that can deduce solutions to partial differential equations (PDEs) on arbitrary domains without requiring retraining. We introduce two innovative models rooted in boundary integral equations (BIEs): the Boundary Integral Type Deep Operator Network (BI-DeepONet) and the Boundary Integral Trigonometric Deep Operator Neural Network (BI-TDONet), which are crafted to address PDEs across diverse domains. Once fully trained, these BIE-based models adeptly predict the solutions of PDEs in any domain without the need for additional training. BI-TDONet notably enhances its performance by employing the singular value decomposition (SVD) of bounded linear operators, allowing for the efficient distribution of input functions across its modules. Furthermore, to tackle the issue of function sampling values that do not effectively capture oscillatory and impulse signal characteristics, trigonometric coefficients are utilized as both inputs and outputs in BI-TDONet. Our numerical experiments robustly support and confirm the efficacy of this theoretical framework.","sentences":["This article explores operator learning models that can deduce solutions to partial differential equations (PDEs) on arbitrary domains without requiring retraining.","We introduce two innovative models rooted in boundary integral equations (BIEs): the Boundary Integral Type Deep Operator Network (BI-DeepONet) and the Boundary Integral Trigonometric Deep Operator Neural Network (BI-TDONet), which are crafted to address PDEs across diverse domains.","Once fully trained, these BIE-based models adeptly predict the solutions of PDEs in any domain without the need for additional training.","BI-TDONet notably enhances its performance by employing the singular value decomposition (SVD) of bounded linear operators, allowing for the efficient distribution of input functions across its modules.","Furthermore, to tackle the issue of function sampling values that do not effectively capture oscillatory and impulse signal characteristics, trigonometric coefficients are utilized as both inputs and outputs in BI-TDONet.","Our numerical experiments robustly support and confirm the efficacy of this theoretical framework."],"url":"http://arxiv.org/abs/2406.02298v1","category":"math-ph"}
{"created":"2024-06-04 12:52:50","title":"A family of $4$-manifolds with nonnegative Ricci curvature and prescribed asymptotic cone","abstract":"In this paper, we show that for any finite subgroup $\\Gamma < O(4)$ acting freely on $\\mathbb{S}^3$, there exists a $4$-dimensional complete Riemannian manifold $(M,g)$ with ${\\rm Ric}_g \\geq 0 $, such that the asymptotic cone of $(M,g)$ is $C(\\mathbb{S}_\\delta^3 /\\Gamma )$ for some $\\delta = \\delta (\\Gamma ) >0$. This answers a question of Bru\\`e-Pigati-Semola [arXiv:2405.03839] about the topological obstructions of $4$-dimensional non-collapsed tangent cones. Combining this result with a recent work of Bru\\`e-Pigati-Semola [arXiv:2405.03839], one can classify the $4$-dimensional non-collapsed tangent cone in the topological sense.","sentences":["In this paper, we show that for any finite subgroup $\\Gamma < O(4)$ acting freely on $\\mathbb{S}^3$, there exists a $4$-dimensional complete Riemannian manifold $(M,g)$ with ${\\rm Ric}_g \\geq 0 $, such that the asymptotic cone of $(M,g)$ is $C(\\mathbb{S}_\\delta^3 /\\Gamma )$ for some $\\delta = \\delta (\\Gamma ) >0$.","This answers a question of Bru\\`e-Pigati-Semola [arXiv:2405.03839] about the topological obstructions of $4$-dimensional non-collapsed tangent cones.","Combining this result with a recent work of Bru\\`e-Pigati-Semola [arXiv:2405.03839], one can classify the $4$-dimensional non-collapsed tangent cone in the topological sense."],"url":"http://arxiv.org/abs/2406.02279v1","category":"math.DG"}
{"created":"2024-06-04 10:27:16","title":"Numerical scheme for the solution of the \"bad\" Boussinesq equation","abstract":"We present a numerical scheme for the solution of the initial-value problem for the ``bad'' Boussinesq equation. The accuracy of the scheme is tested by comparison with exact soliton solutions as well as with recently obtained asymptotic formulas for the solution.","sentences":["We present a numerical scheme for the solution of the initial-value problem for the ``bad'' Boussinesq equation.","The accuracy of the scheme is tested by comparison with exact soliton solutions as well as with recently obtained asymptotic formulas for the solution."],"url":"http://arxiv.org/abs/2406.02183v1","category":"math.NA"}
{"created":"2024-06-04 09:50:41","title":"Word equations, constraints, and formal languages","abstract":"In this short survey we describe recent advances on word equations with non-rational constraints in groups and monoids, highlighting the important role that formal languages play in this area.","sentences":["In this short survey we describe recent advances on word equations with non-rational constraints in groups and monoids, highlighting the important role that formal languages play in this area."],"url":"http://arxiv.org/abs/2406.02160v1","category":"math.GR"}
{"created":"2024-06-04 09:44:24","title":"Mean field equilibrium asset pricing model with habit formation","abstract":"This paper presents an asset pricing model in an incomplete market involving a large number of heterogeneous agents based on the mean field game theory. In the model, we incorporate habit formation in consumption preferences, which has been widely used to explain various phenomena in financial economics. In order to characterize the market-clearing equilibrium, we derive a quadratic-growth mean field backward stochastic differential equation (BSDE) and study its well-posedness and asymptotic behavior in the large population limit. Additionally, we introduce an exponential quadratic Gaussian reformulation of the asset pricing model, in which the solution is obtained in a semi-analytic form.","sentences":["This paper presents an asset pricing model in an incomplete market involving a large number of heterogeneous agents based on the mean field game theory.","In the model, we incorporate habit formation in consumption preferences, which has been widely used to explain various phenomena in financial economics.","In order to characterize the market-clearing equilibrium, we derive a quadratic-growth mean field backward stochastic differential equation (BSDE) and study its well-posedness and asymptotic behavior in the large population limit.","Additionally, we introduce an exponential quadratic Gaussian reformulation of the asset pricing model, in which the solution is obtained in a semi-analytic form."],"url":"http://arxiv.org/abs/2406.02155v1","category":"q-fin.MF"}
{"created":"2024-06-04 09:34:08","title":"Activation Bottleneck: Sigmoidal Neural Networks Cannot Forecast a Straight Line","abstract":"A neural network has an activation bottleneck if one of its hidden layers has a bounded image. We show that networks with an activation bottleneck cannot forecast unbounded sequences such as straight lines, random walks, or any sequence with a trend: The difference between prediction and ground truth becomes arbitrary large, regardless of the training procedure. Widely-used neural network architectures such as LSTM and GRU suffer from this limitation. In our analysis, we characterize activation bottlenecks and explain why they prevent sigmoidal networks from learning unbounded sequences. We experimentally validate our findings and discuss modifications to network architectures which mitigate the effects of activation bottlenecks.","sentences":["A neural network has an activation bottleneck if one of its hidden layers has a bounded image.","We show that networks with an activation bottleneck cannot forecast unbounded sequences such as straight lines, random walks, or any sequence with a trend: The difference between prediction and ground truth becomes arbitrary large, regardless of the training procedure.","Widely-used neural network architectures such as LSTM and GRU suffer from this limitation.","In our analysis, we characterize activation bottlenecks and explain why they prevent sigmoidal networks from learning unbounded sequences.","We experimentally validate our findings and discuss modifications to network architectures which mitigate the effects of activation bottlenecks."],"url":"http://arxiv.org/abs/2406.02146v1","category":"cs.LG"}
{"created":"2024-06-04 08:59:16","title":"An Archive Can Bring Provable Speed-ups in Multi-Objective Evolutionary Algorithms","abstract":"In the area of multi-objective evolutionary algorithms (MOEAs), there is a trend of using an archive to store non-dominated solutions generated during the search. This is because 1) MOEAs may easily end up with the final population containing inferior solutions that are dominated by other solutions discarded during the search process and 2) the population that has a commensurable size of the problem's Pareto front is often not practical. In this paper, we theoretically show, for the first time, that using an archive can guarantee speed-ups for MOEAs. Specifically, we prove that for two well-established MOEAs (NSGA-II and SMS-EMOA) on two commonly studied problems (OneMinMax and LeadingOnesTrailingZeroes), using an archive brings a polynomial acceleration on the expected running time. The reason is that with an archive, the size of the population can reduce to a small constant; there is no need for the population to keep all the Pareto optimal solutions found. This contrasts existing theoretical studies for MOEAs where a population with a commensurable size of the problem's Pareto front is needed. The findings in this paper not only provide a theoretical confirmation for an increasingly popular practice in the design of MOEAs, but can also be beneficial to the theory community towards studying more practical MOEAs.","sentences":["In the area of multi-objective evolutionary algorithms (MOEAs), there is a trend of using an archive to store non-dominated solutions generated during the search.","This is because 1) MOEAs may easily end up with the final population containing inferior solutions that are dominated by other solutions discarded during the search process and 2) the population that has a commensurable size of the problem's Pareto front is often not practical.","In this paper, we theoretically show, for the first time, that using an archive can guarantee speed-ups for MOEAs.","Specifically, we prove that for two well-established MOEAs (NSGA-II and SMS-EMOA) on two commonly studied problems (OneMinMax and LeadingOnesTrailingZeroes), using an archive brings a polynomial acceleration on the expected running time.","The reason is that with an archive, the size of the population can reduce to a small constant; there is no need for the population to keep all the Pareto optimal solutions found.","This contrasts existing theoretical studies for MOEAs where a population with a commensurable size of the problem's Pareto front is needed.","The findings in this paper not only provide a theoretical confirmation for an increasingly popular practice in the design of MOEAs, but can also be beneficial to the theory community towards studying more practical MOEAs."],"url":"http://arxiv.org/abs/2406.02118v1","category":"cs.NE"}
{"created":"2024-06-04 08:11:28","title":"Boundedness of variation, oscillation and maximal differential transform on BMO space","abstract":"In this paper, we prove that the oscillation operator, variation operator and maximal differential transform associated with the approximate identities are bounded from ${\\rm BMO}({\\mathbb R}^n)$ to its subspace ${\\rm BLO}({\\mathbb R}^n)$.","sentences":["In this paper, we prove that the oscillation operator, variation operator and maximal differential transform associated with the approximate identities are bounded from ${\\rm BMO}({\\mathbb R}^n)$ to its subspace ${\\rm BLO}({\\mathbb R}^n)$."],"url":"http://arxiv.org/abs/2406.02087v1","category":"math.AP"}
{"created":"2024-06-04 07:52:04","title":"Optimization of a fiber Fabry-Perot resonator for low-threshold modulation instability Kerr frequency combs","abstract":"We report a theoretical and experimental investigation of fiber Fabry-Perot cavities aimed at enhancing Kerr frequency comb generation. The modulation instability (MI) power threshold is derived from the linear stability analysis of a generalized Lugiato-Lefever equation. By combining this analysis with the concepts of power enhancement factor (PEF) and optimal coupling, we predict the ideal manufacturing parameters of fiber Fabry-Perot (FFP) cavities for the MI Kerr frequency comb generation. Our findings reveal a distinction between the optimal coupling for modulation instability and that of the cold cavity. Consequently, mirror reflectivity must be adjusted to suit the specific application. We verified the predictions of our theory by measuring the MI power threshold as a function of detuning for three different cavities.","sentences":["We report a theoretical and experimental investigation of fiber Fabry-Perot cavities aimed at enhancing Kerr frequency comb generation.","The modulation instability (MI) power threshold is derived from the linear stability analysis of a generalized Lugiato-Lefever equation.","By combining this analysis with the concepts of power enhancement factor (PEF) and optimal coupling, we predict the ideal manufacturing parameters of fiber Fabry-Perot (FFP) cavities for the MI Kerr frequency comb generation.","Our findings reveal a distinction between the optimal coupling for modulation instability and that of the cold cavity.","Consequently, mirror reflectivity must be adjusted to suit the specific application.","We verified the predictions of our theory by measuring the MI power threshold as a function of detuning for three different cavities."],"url":"http://arxiv.org/abs/2406.02070v1","category":"physics.optics"}
{"created":"2024-06-04 07:49:55","title":"SYZ and optimal transport stability of Weyl polytopes","abstract":"We prove optimal transport stability (in the sense of Andreasson and the second author) for reflexive Weyl polytopes: reflexive polytopes which are convex hulls of an orbit of a Weyl group. When the reflexive Weyl polytope is Delzant, it follows from work of Li, Andreasson, Hultgren, Jonsson, Mazzon, that the weak metric SYZ conjecture holds for the Dwork family in the corresponding toric Fano manifold. In particular, we show that the weak metric SYZ conjecture holds for centrally symmetric smooth Fano toric manifolds.","sentences":["We prove optimal transport stability (in the sense of Andreasson and the second author) for reflexive Weyl polytopes: reflexive polytopes which are convex hulls of an orbit of a Weyl group.","When the reflexive Weyl polytope is Delzant, it follows from work of Li, Andreasson, Hultgren, Jonsson, Mazzon, that the weak metric SYZ conjecture holds for the Dwork family in the corresponding toric Fano manifold.","In particular, we show that the weak metric SYZ conjecture holds for centrally symmetric smooth Fano toric manifolds."],"url":"http://arxiv.org/abs/2406.02068v1","category":"math.DG"}
{"created":"2024-06-04 07:37:47","title":"CAP: A Context-Aware Neural Predictor for NAS","abstract":"Neural predictors are effective in boosting the time-consuming performance evaluation stage in neural architecture search (NAS), owing to their direct estimation of unseen architectures. Despite the effectiveness, training a powerful neural predictor with fewer annotated architectures remains a huge challenge. In this paper, we propose a context-aware neural predictor (CAP) which only needs a few annotated architectures for training based on the contextual information from the architectures. Specifically, the input architectures are encoded into graphs and the predictor infers the contextual structure around the nodes inside each graph. Then, enhanced by the proposed context-aware self-supervised task, the pre-trained predictor can obtain expressive and generalizable representations of architectures. Therefore, only a few annotated architectures are sufficient for training. Experimental results in different search spaces demonstrate the superior performance of CAP compared with state-of-the-art neural predictors. In particular, CAP can rank architectures precisely at the budget of only 172 annotated architectures in NAS-Bench-101. Moreover, CAP can help find promising architectures in both NAS-Bench-101 and DARTS search spaces on the CIFAR-10 dataset, serving as a useful navigator for NAS to explore the search space efficiently.","sentences":["Neural predictors are effective in boosting the time-consuming performance evaluation stage in neural architecture search (NAS), owing to their direct estimation of unseen architectures.","Despite the effectiveness, training a powerful neural predictor with fewer annotated architectures remains a huge challenge.","In this paper, we propose a context-aware neural predictor (CAP) which only needs a few annotated architectures for training based on the contextual information from the architectures.","Specifically, the input architectures are encoded into graphs and the predictor infers the contextual structure around the nodes inside each graph.","Then, enhanced by the proposed context-aware self-supervised task, the pre-trained predictor can obtain expressive and generalizable representations of architectures.","Therefore, only a few annotated architectures are sufficient for training.","Experimental results in different search spaces demonstrate the superior performance of CAP compared with state-of-the-art neural predictors.","In particular, CAP can rank architectures precisely at the budget of only 172 annotated architectures in NAS-Bench-101.","Moreover, CAP can help find promising architectures in both NAS-Bench-101 and DARTS search spaces on the CIFAR-10 dataset, serving as a useful navigator for NAS to explore the search space efficiently."],"url":"http://arxiv.org/abs/2406.02056v1","category":"cs.LG"}
{"created":"2024-06-04 07:23:09","title":"Multi-Scale Direction-Aware Network for Infrared Small Target Detection","abstract":"Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target. Existing deep learning-based methods focus on appearance features and ignore high-frequency directional features. Therefore, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks. Specifically, an innovative multi-directional feature awareness (MDFA) module is constructed, which fully utilizes the prior knowledge of targets and emphasizes the focus on high-frequency directional features. On this basis, combined with the multi-scale local relation learning (MLRL) module, a multi-scale direction-aware (MSDA) module is further constructed. The MSDA module promotes the full extraction of local relations at different scales and the full perception of key features in different directions. Meanwhile, a high-frequency direction injection (HFDI) module without training parameters is constructed to inject the high-frequency directional information of the original image into the network. This helps guide the network to pay attention to detailed information such as target edges and shapes. In addition, we propose a feature aggregation (FA) structure that aggregates multi-level features to solve the problem of small targets disappearing in deep feature maps. Furthermore, a lightweight feature alignment fusion (FAF) module is constructed, which can effectively alleviate the pixel offset existing in multi-level feature map fusion. Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on the public NUDT-SIRST, SIRST and IRSTD-1k datasets.","sentences":["Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target.","Existing deep learning-based methods focus on appearance features and ignore high-frequency directional features.","Therefore, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks.","Specifically, an innovative multi-directional feature awareness (MDFA) module is constructed, which fully utilizes the prior knowledge of targets and emphasizes the focus on high-frequency directional features.","On this basis, combined with the multi-scale local relation learning (MLRL) module, a multi-scale direction-aware (MSDA) module is further constructed.","The MSDA module promotes the full extraction of local relations at different scales and the full perception of key features in different directions.","Meanwhile, a high-frequency direction injection (HFDI) module without training parameters is constructed to inject the high-frequency directional information of the original image into the network.","This helps guide the network to pay attention to detailed information such as target edges and shapes.","In addition, we propose a feature aggregation (FA) structure that aggregates multi-level features to solve the problem of small targets disappearing in deep feature maps.","Furthermore, a lightweight feature alignment fusion (FAF) module is constructed, which can effectively alleviate the pixel offset existing in multi-level feature map fusion.","Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on the public NUDT-SIRST, SIRST and IRSTD-1k datasets."],"url":"http://arxiv.org/abs/2406.02037v1","category":"cs.CV"}
{"created":"2024-06-04 06:33:16","title":"Laplace Meets Moreau: Smooth Approximation to Infimal Convolutions Using Laplace's Method","abstract":"We study approximations to the Moreau envelope -- and infimal convolutions more broadly -- based on Laplace's method, a classical tool in analysis which ties certain integrals to suprema of their integrands. We believe the connection between Laplace's method and infimal convolutions is generally deserving of more attention in the study of optimization and partial differential equations, since it bears numerous potentially important applications, from proximal-type algorithms to solving Halmiton-Jacobi equations.","sentences":["We study approximations to the Moreau envelope -- and infimal convolutions more broadly -- based on Laplace's method, a classical tool in analysis which ties certain integrals to suprema of their integrands.","We believe the connection between Laplace's method and infimal convolutions is generally deserving of more attention in the study of optimization and partial differential equations, since it bears numerous potentially important applications, from proximal-type algorithms to solving Halmiton-Jacobi equations."],"url":"http://arxiv.org/abs/2406.02003v1","category":"math.OC"}
{"created":"2024-06-04 06:32:48","title":"Higher-order Common Information","abstract":"We present a new notion $R_\\ell$ of higher-order common information, which quantifies the information that $\\ell\\geq 2$ arbitrarily distributed random variables have in common. We provide analytical lower bounds on $R_3$ and $R_4$ for jointly Gaussian distributed sources and provide computable lower bounds for $R_\\ell$ for any $\\ell$ and any sources. We also provide a practical method to estimate the lower bounds on, e.g., real-world time-series data. As an example, we consider EEG data acquired in a setup with competing acoustic stimuli. We demonstrate that $R_3$ has descriptive properties that is not in $R_2$. Moreover, we observe a linear relationship between the amount of common information $R_3$ communicated from the acoustic stimuli and to the brain and the corresponding cortical activity in terms of neural tracking of the envelopes of the stimuli.","sentences":["We present a new notion $R_\\ell$ of higher-order common information, which quantifies the information that $\\ell\\geq 2$ arbitrarily distributed random variables have in common.","We provide analytical lower bounds on $R_3$ and $R_4$ for jointly Gaussian distributed sources and provide computable lower bounds for $R_\\ell$ for any $\\ell$ and any sources.","We also provide a practical method to estimate the lower bounds on, e.g., real-world time-series data.","As an example, we consider EEG data acquired in a setup with competing acoustic stimuli.","We demonstrate that $R_3$ has descriptive properties that is not in $R_2$. Moreover, we observe a linear relationship between the amount of common information $R_3$ communicated from the acoustic stimuli and to the brain and the corresponding cortical activity in terms of neural tracking of the envelopes of the stimuli."],"url":"http://arxiv.org/abs/2406.02001v1","category":"cs.IT"}
{"created":"2024-06-04 06:28:15","title":"The Common Solution Space of General Relativity","abstract":"We review the solution space for the field equations of Einstein's General Relativity for various static, spherically symmetric spacetimes. We consider the vacuum case, represented by the Schwarzschild black hole; the de Sitter-Schwarzschild geometry, which includes a cosmological constant; the Reissner-Nordstr\\\"{o}m geometry, which accounts for the presence of charge. Additionally we consider the homogenenous and anisotropic locally rotational Bianchi II spacetime in the vacuum. Our analysis reveals that the field equations for these scenarios share a common three-dimensional group of point transformations, with the generators being the elements of the $D\\otimes_{s}T_{2}$ Lie algebra, known as the semidirect product of dilations and translations in the plane. Due to this algebraic property the field equations for the aforementioned gravitational models can be expressed in the equivalent form of the null geodesic equations for conformally flat geometries. Consequently, the solution space for the field equations is common, and it is the solution space for the free particle in a flat space. This appoach open new directions on the construction of analytic solutions in gravitational physics and cosmology.","sentences":["We review the solution space for the field equations of Einstein's General Relativity for various static, spherically symmetric spacetimes.","We consider the vacuum case, represented by the Schwarzschild black hole; the de Sitter-Schwarzschild geometry, which includes a cosmological constant; the Reissner-Nordstr\\\"{o}m geometry, which accounts for the presence of charge.","Additionally we consider the homogenenous and anisotropic locally rotational Bianchi II spacetime in the vacuum.","Our analysis reveals that the field equations for these scenarios share a common three-dimensional group of point transformations, with the generators being the elements of the $D\\otimes_{s}T_{2}$ Lie algebra, known as the semidirect product of dilations and translations in the plane.","Due to this algebraic property the field equations for the aforementioned gravitational models can be expressed in the equivalent form of the null geodesic equations for conformally flat geometries.","Consequently, the solution space for the field equations is common, and it is the solution space for the free particle in a flat space.","This appoach open new directions on the construction of analytic solutions in gravitational physics and cosmology."],"url":"http://arxiv.org/abs/2406.01998v1","category":"gr-qc"}
{"created":"2024-06-04 06:10:28","title":"Life beyond Fritz: On the detachment of electrolytic bubbles","abstract":"We present an experimental study on detachment characteristics of hydrogen bubbles during electrolysis. Using a transparent (Pt or Ni) electrode enables us to directly observe the bubble contact line and bubble size. Based on these quantities we determine other parameters such as the contact angle and volume through solutions of the Young-Laplace equation. We observe bubbles without ('pinned bubbles') and with ('spreading bubbles') contact line spreading, and find that the latter mode becomes more prevalent if the concentration of HClO4 is greater than or equal to 0.1 M. The departure radius for spreading bubbles is found to drastically exceed the value predicted by the well-known formula of W. Fritz (Physik. Zeitschr. 1935, 36, 379-384) for this case. We show that this is related to the contact line hysteresis, which leads to pinning of the contact line after an initial spreading phase at the receding contact angle. The departure mode is then similar to a pinned bubble and occurs once the contact angle reaches the advancing contact angle of the surface. A prediction for the departure radius based on these findings is found to be consistent with the experimental data.","sentences":["We present an experimental study on detachment characteristics of hydrogen bubbles during electrolysis.","Using a transparent (Pt or Ni) electrode enables us to directly observe the bubble contact line and bubble size.","Based on these quantities we determine other parameters such as the contact angle and volume through solutions of the Young-Laplace equation.","We observe bubbles without ('pinned bubbles') and with ('spreading bubbles') contact line spreading, and find that the latter mode becomes more prevalent if the concentration of HClO4 is greater than or equal to 0.1 M. The departure radius for spreading bubbles is found to drastically exceed the value predicted by the well-known formula of W. Fritz (Physik.","Zeitschr.","1935, 36, 379-384) for this case.","We show that this is related to the contact line hysteresis, which leads to pinning of the contact line after an initial spreading phase at the receding contact angle.","The departure mode is then similar to a pinned bubble and occurs once the contact angle reaches the advancing contact angle of the surface.","A prediction for the departure radius based on these findings is found to be consistent with the experimental data."],"url":"http://arxiv.org/abs/2406.01989v1","category":"physics.flu-dyn"}
{"created":"2024-06-04 05:51:52","title":"Unified one-parameter scaling function for Anderson localization transitions in non-reciprocal non-Hermitian systems","abstract":"By using dimensionless conductances as scaling variables, the conventional one-parameter scaling theory of localization fails for non-reciprocal non-Hermitian systems such as the Hanato-Nelson model. Here, we propose a one-parameter scaling function using the participation ratio as the scaling variable. Employing a highly accurate numerical procedure based on exact diagonalization, we demonstrate that this one-parameter scaling function can describe Anderson localization transitions of non-reciprocal non-Hermitian systems in one and two dimensions of symmetry classes AI and A. The critical exponents of correlation lengths depend on symmetries and dimensionality only, a typical feature of universality. Moreover, we derive a complex-gap equation based on the self-consistent Born approximation that can determine the disorder at which the point gap closes. The obtained disorders match perfectly the critical disorders of Anderson localization transitions from the one-parameter scaling function. Finally, we show that the one-parameter scaling function is also valid for Anderson localization transitions in reciprocal non-Hermitian systems such as two-dimensional class AII$^\\dagger$ and can, thus, serve as a unified scaling function for disordered non-Hermitian systems.","sentences":["By using dimensionless conductances as scaling variables, the conventional one-parameter scaling theory of localization fails for non-reciprocal non-Hermitian systems such as the Hanato-Nelson model.","Here, we propose a one-parameter scaling function using the participation ratio as the scaling variable.","Employing a highly accurate numerical procedure based on exact diagonalization, we demonstrate that this one-parameter scaling function can describe Anderson localization transitions of non-reciprocal non-Hermitian systems in one and two dimensions of symmetry classes AI and A.","The critical exponents of correlation lengths depend on symmetries and dimensionality only, a typical feature of universality.","Moreover, we derive a complex-gap equation based on the self-consistent Born approximation that can determine the disorder at which the point gap closes.","The obtained disorders match perfectly the critical disorders of Anderson localization transitions from the one-parameter scaling function.","Finally, we show that the one-parameter scaling function is also valid for Anderson localization transitions in reciprocal non-Hermitian systems such as two-dimensional class AII$^\\dagger$ and can, thus, serve as a unified scaling function for disordered non-Hermitian systems."],"url":"http://arxiv.org/abs/2406.01984v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-04 04:51:59","title":"Creativity, Generative AI, and Software Development: A Research Agenda","abstract":"Creativity has always been considered a major differentiator to separate the good from the great, and we believe the importance of creativity for software development will only increase as GenAI becomes embedded in developer tool-chains and working practices. This paper uses the McLuhan tetrad alongside scenarios of how GenAI may disrupt software development more broadly, to identify potential impacts GenAI may have on creativity within software development. The impacts are discussed along with a future research agenda comprising six connected themes that consider how individual capabilities, team capabilities, the product, unintended consequences, society, and human aspects can be affected.","sentences":["Creativity has always been considered a major differentiator to separate the good from the great, and we believe the importance of creativity for software development will only increase as GenAI becomes embedded in developer tool-chains and working practices.","This paper uses the McLuhan tetrad alongside scenarios of how GenAI may disrupt software development more broadly, to identify potential impacts GenAI may have on creativity within software development.","The impacts are discussed along with a future research agenda comprising six connected themes that consider how individual capabilities, team capabilities, the product, unintended consequences, society, and human aspects can be affected."],"url":"http://arxiv.org/abs/2406.01966v1","category":"cs.SE"}
{"created":"2024-06-04 03:58:14","title":"Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature","abstract":"Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.","sentences":["Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content.","While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer.","To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy.","Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs.","The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability."],"url":"http://arxiv.org/abs/2406.01946v1","category":"cs.CR"}
{"created":"2024-06-04 03:39:56","title":"Fluid Implicit Particles on Coadjoint Orbits","abstract":"We propose Coadjoint Orbit FLIP (CO-FLIP), a high order accurate, structure preserving fluid simulation method in the hybrid Eulerian-Lagrangian framework. We start with a Hamiltonian formulation of the incompressible Euler Equations, and then, using a local, explicit, and high order divergence free interpolation, construct a modified Hamiltonian system that governs our discrete Euler flow. The resulting discretization, when paired with a geometric time integration scheme, is energy and circulation preserving (formally the flow evolves on a coadjoint orbit) and is similar to the Fluid Implicit Particle (FLIP) method. CO-FLIP enjoys multiple additional properties including that the pressure projection is exact in the weak sense, and the particle-to-grid transfer is an exact inverse of the grid-to-particle interpolation. The method is demonstrated numerically with outstanding stability, energy, and Casimir preservation. We show that the method produces benchmarks and turbulent visual effects even at low grid resolutions.","sentences":["We propose Coadjoint Orbit FLIP (CO-FLIP), a high order accurate, structure preserving fluid simulation method in the hybrid Eulerian-Lagrangian framework.","We start with a Hamiltonian formulation of the incompressible Euler Equations, and then, using a local, explicit, and high order divergence free interpolation, construct a modified Hamiltonian system that governs our discrete Euler flow.","The resulting discretization, when paired with a geometric time integration scheme, is energy and circulation preserving (formally the flow evolves on a coadjoint orbit) and is similar to the Fluid Implicit Particle (FLIP) method.","CO-FLIP enjoys multiple additional properties including that the pressure projection is exact in the weak sense, and the particle-to-grid transfer is an exact inverse of the grid-to-particle interpolation.","The method is demonstrated numerically with outstanding stability, energy, and Casimir preservation.","We show that the method produces benchmarks and turbulent visual effects even at low grid resolutions."],"url":"http://arxiv.org/abs/2406.01936v1","category":"cs.GR"}
{"created":"2024-06-04 03:00:47","title":"Image steganography based on generative implicit neural representation","abstract":"In the realm of advanced steganography, the scale of the model typically correlates directly with the resolution of the fundamental grid, necessitating the training of a distinct neural network for message extraction. This paper proposes an image steganography based on generative implicit neural representation. This approach transcends the constraints of image resolution by portraying data as continuous functional expressions. Notably, this method permits the utilization of a diverse array of multimedia data as cover images, thereby broadening the spectrum of potential carriers. Additionally, by fixing a neural network as the message extractor, we effectively redirect the training burden to the image itself, resulting in both a reduction in computational overhead and an enhancement in steganographic speed. This approach also circumvents potential transmission challenges associated with the message extractor. Experimental findings reveal that this methodology achieves a commendable optimization efficiency, achieving a completion time of just 3 seconds for 64x64 dimensional images, while concealing only 1 bpp of information. Furthermore, the accuracy of message extraction attains an impressive mark of 100%.","sentences":["In the realm of advanced steganography, the scale of the model typically correlates directly with the resolution of the fundamental grid, necessitating the training of a distinct neural network for message extraction.","This paper proposes an image steganography based on generative implicit neural representation.","This approach transcends the constraints of image resolution by portraying data as continuous functional expressions.","Notably, this method permits the utilization of a diverse array of multimedia data as cover images, thereby broadening the spectrum of potential carriers.","Additionally, by fixing a neural network as the message extractor, we effectively redirect the training burden to the image itself, resulting in both a reduction in computational overhead and an enhancement in steganographic speed.","This approach also circumvents potential transmission challenges associated with the message extractor.","Experimental findings reveal that this methodology achieves a commendable optimization efficiency, achieving a completion time of just 3 seconds for 64x64 dimensional images, while concealing only 1 bpp of information.","Furthermore, the accuracy of message extraction attains an impressive mark of 100%."],"url":"http://arxiv.org/abs/2406.01918v1","category":"cs.CR"}
{"created":"2024-06-04 02:47:05","title":"Influence Maximization in Hypergraphs by Stratified Sampling for Efficient Generation of Reverse Reachable Sets","abstract":"Given a hypergraph, influence maximization (IM) is to discover a seed set containing $k$ vertices that have the maximal influence. Although the existing vertex-based IM algorithms perform better than the hyperedge-based algorithms by generating random reverse researchable (RR) sets, they are inefficient because (i) they ignore important structural information associated with hyperedges and thus obtain inferior results, (ii) the frequently-used sampling methods for generating RR sets have low efficiency because of a large number of required samplings along with high sampling variances, and (iii) the vertex-based IM algorithms have large overheads in terms of running time and memory costs. To overcome these shortcomings, this paper proposes a novel approach, called \\emph{HyperIM}. The key idea behind \\emph{HyperIM} is to differentiate structural information of vertices for developing stratified sampling combined with highly-efficient strategies to generate the RR sets. With theoretical guarantees, \\emph{HyperIM} is able to accelerate the influence spread, improve the sampling efficiency, and cut down the expected running time. To further reduce the running time and memory costs, we optimize \\emph{HyperIM} by inferring the bound of the required number of RR sets in conjunction with stratified sampling. Experimental results on real-world hypergraphs show that \\emph{HyperIM} is able to reduce the number of required RR sets and running time by orders of magnitude while increasing the influence spread by up to $2.73X$ on average, compared to the state-of-the-art IM algorithms.","sentences":["Given a hypergraph, influence maximization (IM) is to discover a seed set containing $k$ vertices that have the maximal influence.","Although the existing vertex-based IM algorithms perform better than the hyperedge-based algorithms by generating random reverse researchable (RR) sets, they are inefficient because (i) they ignore important structural information associated with hyperedges and thus obtain inferior results, (ii) the frequently-used sampling methods for generating RR sets have low efficiency because of a large number of required samplings along with high sampling variances, and (iii) the vertex-based IM algorithms have large overheads in terms of running time and memory costs.","To overcome these shortcomings, this paper proposes a novel approach, called \\emph{HyperIM}.","The key idea behind \\emph{HyperIM} is to differentiate structural information of vertices for developing stratified sampling combined with highly-efficient strategies to generate the RR sets.","With theoretical guarantees, \\emph{HyperIM} is able to accelerate the influence spread, improve the sampling efficiency, and cut down the expected running time.","To further reduce the running time and memory costs, we optimize \\emph{HyperIM} by inferring the bound of the required number of RR sets in conjunction with stratified sampling.","Experimental results on real-world hypergraphs show that \\emph{HyperIM} is able to reduce the number of required RR sets and running time by orders of magnitude while increasing the influence spread by up to $2.73X$ on average, compared to the state-of-the-art IM algorithms."],"url":"http://arxiv.org/abs/2406.01911v1","category":"cs.SI"}
{"created":"2024-06-04 02:39:42","title":"PDHG-Unrolled Learning-to-Optimize Method for Large-Scale Linear Programming","abstract":"Solving large-scale linear programming (LP) problems is an important task in various areas such as communication networks, power systems, finance and logistics. Recently, two distinct approaches have emerged to expedite LP solving: (i) First-order methods (FOMs); (ii) Learning to optimize (L2O). In this work, we propose an FOM-unrolled neural network (NN) called PDHG-Net, and propose a two-stage L2O method to solve large-scale LP problems. The new architecture PDHG-Net is designed by unrolling the recently emerged PDHG method into a neural network, combined with channel-expansion techniques borrowed from graph neural networks. We prove that the proposed PDHG-Net can recover PDHG algorithm, thus can approximate optimal solutions of LP instances with a polynomial number of neurons. We propose a two-stage inference approach: first use PDHG-Net to generate an approximate solution, and then apply PDHG algorithm to further improve the solution. Experiments show that our approach can significantly accelerate LP solving, achieving up to a 3$\\times$ speedup compared to FOMs for large-scale LP problems.","sentences":["Solving large-scale linear programming (LP) problems is an important task in various areas such as communication networks, power systems, finance and logistics.","Recently, two distinct approaches have emerged to expedite LP solving: (i) First-order methods (FOMs); (ii) Learning to optimize (L2O).","In this work, we propose an FOM-unrolled neural network (NN) called PDHG-Net, and propose a two-stage L2O method to solve large-scale LP problems.","The new architecture PDHG-Net is designed by unrolling the recently emerged PDHG method into a neural network, combined with channel-expansion techniques borrowed from graph neural networks.","We prove that the proposed PDHG-Net can recover PDHG algorithm, thus can approximate optimal solutions of LP instances with a polynomial number of neurons.","We propose a two-stage inference approach: first use PDHG-Net to generate an approximate solution, and then apply PDHG algorithm to further improve the solution.","Experiments show that our approach can significantly accelerate LP solving, achieving up to a 3$\\times$ speedup compared to FOMs for large-scale LP problems."],"url":"http://arxiv.org/abs/2406.01908v1","category":"cs.LG"}
{"created":"2024-06-04 02:04:06","title":"How Inductive Bias in Machine Learning Aligns with Optimality in Economic Dynamics","abstract":"This paper examines the alignment of inductive biases in machine learning (ML) with structural models of economic dynamics. Unlike dynamical systems found in physical and life sciences, economics models are often specified by differential equations with a mixture of easy-to-enforce initial conditions and hard-to-enforce infinite horizon boundary conditions (e.g. transversality and no-ponzi-scheme conditions). Traditional methods for enforcing these constraints are computationally expensive and unstable. We investigate algorithms where those infinite horizon constraints are ignored, simply training unregularized kernel machines and neural networks to obey the differential equations. Despite the inherent underspecification of this approach, our findings reveal that the inductive biases of these ML models innately enforce the infinite-horizon conditions necessary for the well-posedness. We theoretically demonstrate that (approximate or exact) min-norm ML solutions to interpolation problems are sufficient conditions for these infinite-horizon boundary conditions in a wide class of problems. We then provide empirical evidence that deep learning and ridgeless kernel methods are not only theoretically sound with respect to economic assumptions, but may even dominate classic algorithms in low to medium dimensions. More importantly, these results give confidence that, despite solving seemingly ill-posed problems, there are reasons to trust the plethora of black-box ML algorithms used by economists to solve previously intractable, high-dimensional dynamical systems -- paving the way for future work on estimation of inverse problems with embedded optimal control problems.","sentences":["This paper examines the alignment of inductive biases in machine learning (ML) with structural models of economic dynamics.","Unlike dynamical systems found in physical and life sciences, economics models are often specified by differential equations with a mixture of easy-to-enforce initial conditions and hard-to-enforce infinite horizon boundary conditions (e.g. transversality and no-ponzi-scheme conditions).","Traditional methods for enforcing these constraints are computationally expensive and unstable.","We investigate algorithms where those infinite horizon constraints are ignored, simply training unregularized kernel machines and neural networks to obey the differential equations.","Despite the inherent underspecification of this approach, our findings reveal that the inductive biases of these ML models innately enforce the infinite-horizon conditions necessary for the well-posedness.","We theoretically demonstrate that (approximate or exact) min-norm ML solutions to interpolation problems are sufficient conditions for these infinite-horizon boundary conditions in a wide class of problems.","We then provide empirical evidence that deep learning and ridgeless kernel methods are not only theoretically sound with respect to economic assumptions, but may even dominate classic algorithms in low to medium dimensions.","More importantly, these results give confidence that, despite solving seemingly ill-posed problems, there are reasons to trust the plethora of black-box ML algorithms used by economists to solve previously intractable, high-dimensional dynamical systems -- paving the way for future work on estimation of inverse problems with embedded optimal control problems."],"url":"http://arxiv.org/abs/2406.01898v1","category":"econ.GN"}
{"created":"2024-06-04 02:03:29","title":"The quasilocal energy and thermodynamic first law in accelerating AdS black holes","abstract":"We study the conserved energy of an accelerating AdS black hole by employing the quasilocal formalism, which links the off-shell ADT formalism with the off-shell covariant phase space formalism. In the presence of conical singularities in the accelerating black hole, the expression for the energy depends on the surface term derived from the variation of the action. The essential ingredient of our computations for the quasilocal energy is that the surface contributions come from the conical singularities as well as the ordinary radial boundary. Consequently, the resulting quasilocal energy naturally satisfies the thermodynamic first law for the black hole without any modifications to the thermodynamic variables. Additionally, we obtain the Smarr relation for the black hole using the differential operator method and the scaling argument of the relevant thermodynamic quantities.","sentences":["We study the conserved energy of an accelerating AdS black hole by employing the quasilocal formalism, which links the off-shell ADT formalism with the off-shell covariant phase space formalism.","In the presence of conical singularities in the accelerating black hole, the expression for the energy depends on the surface term derived from the variation of the action.","The essential ingredient of our computations for the quasilocal energy is that the surface contributions come from the conical singularities as well as the ordinary radial boundary.","Consequently, the resulting quasilocal energy naturally satisfies the thermodynamic first law for the black hole without any modifications to the thermodynamic variables.","Additionally, we obtain the Smarr relation for the black hole using the differential operator method and the scaling argument of the relevant thermodynamic quantities."],"url":"http://arxiv.org/abs/2406.01897v1","category":"hep-th"}
{"created":"2024-06-04 01:58:32","title":"SVASTIN: Sparse Video Adversarial Attack via Spatio-Temporal Invertible Neural Networks","abstract":"Robust and imperceptible adversarial video attack is challenging due to the spatial and temporal characteristics of videos. The existing video adversarial attack methods mainly take a gradient-based approach and generate adversarial videos with noticeable perturbations. In this paper, we propose a novel Sparse Adversarial Video Attack via Spatio-Temporal Invertible Neural Networks (SVASTIN) to generate adversarial videos through spatio-temporal feature space information exchanging. It consists of a Guided Target Video Learning (GTVL) module to balance the perturbation budget and optimization speed and a Spatio-Temporal Invertible Neural Network (STIN) module to perform spatio-temporal feature space information exchanging between a source video and the target feature tensor learned by GTVL module. Extensive experiments on UCF-101 and Kinetics-400 demonstrate that our proposed SVASTIN can generate adversarial examples with higher imperceptibility than the state-of-the-art methods with the higher fooling rate. Code is available at \\href{https://github.com/Brittany-Chen/SVASTIN}{https://github.com/Brittany-Chen/SVASTIN}.","sentences":["Robust and imperceptible adversarial video attack is challenging due to the spatial and temporal characteristics of videos.","The existing video adversarial attack methods mainly take a gradient-based approach and generate adversarial videos with noticeable perturbations.","In this paper, we propose a novel Sparse Adversarial Video Attack via Spatio-Temporal Invertible Neural Networks (SVASTIN) to generate adversarial videos through spatio-temporal feature space information exchanging.","It consists of a Guided Target Video Learning (GTVL) module to balance the perturbation budget and optimization speed and a Spatio-Temporal Invertible Neural Network (STIN) module to perform spatio-temporal feature space information exchanging between a source video and the target feature tensor learned by GTVL module.","Extensive experiments on UCF-101 and Kinetics-400 demonstrate that our proposed SVASTIN can generate adversarial examples with higher imperceptibility than the state-of-the-art methods with the higher fooling rate.","Code is available at \\href{https://github.com/Brittany-Chen/SVASTIN}{https://github.com/Brittany-Chen/SVASTIN}."],"url":"http://arxiv.org/abs/2406.01894v1","category":"cs.CV"}
{"created":"2024-06-04 01:35:35","title":"Context Gating in Spiking Neural Networks: Achieving Lifelong Learning through Integration of Local and Global Plasticity","abstract":"Humans learn multiple tasks in succession with minimal mutual interference, through the context gating mechanism in the prefrontal cortex (PFC). The brain-inspired models of spiking neural networks (SNN) have drawn massive attention for their energy efficiency and biological plausibility. To overcome catastrophic forgetting when learning multiple tasks in sequence, current SNN models for lifelong learning focus on memory reserving or regularization-based modification, while lacking SNN to replicate human experimental behavior. Inspired by biological context-dependent gating mechanisms found in PFC, we propose SNN with context gating trained by the local plasticity rule (CG-SNN) for lifelong learning. The iterative training between global and local plasticity for task units is designed to strengthen the connections between task neurons and hidden neurons and preserve the multi-task relevant information. The experiments show that the proposed model is effective in maintaining the past learning experience and has better task-selectivity than other methods during lifelong learning. Our results provide new insights that the CG-SNN model can extend context gating with good scalability on different SNN architectures with different spike-firing mechanisms. Thus, our models have good potential for parallel implementation on neuromorphic hardware and model human's behavior.","sentences":["Humans learn multiple tasks in succession with minimal mutual interference, through the context gating mechanism in the prefrontal cortex (PFC).","The brain-inspired models of spiking neural networks (SNN) have drawn massive attention for their energy efficiency and biological plausibility.","To overcome catastrophic forgetting when learning multiple tasks in sequence, current SNN models for lifelong learning focus on memory reserving or regularization-based modification, while lacking SNN to replicate human experimental behavior.","Inspired by biological context-dependent gating mechanisms found in PFC, we propose SNN with context gating trained by the local plasticity rule (CG-SNN) for lifelong learning.","The iterative training between global and local plasticity for task units is designed to strengthen the connections between task neurons and hidden neurons and preserve the multi-task relevant information.","The experiments show that the proposed model is effective in maintaining the past learning experience and has better task-selectivity than other methods during lifelong learning.","Our results provide new insights that the CG-SNN model can extend context gating with good scalability on different SNN architectures with different spike-firing mechanisms.","Thus, our models have good potential for parallel implementation on neuromorphic hardware and model human's behavior."],"url":"http://arxiv.org/abs/2406.01883v1","category":"cs.NE"}
{"created":"2024-06-04 00:02:52","title":"Neural Green's Operators for Parametric Partial Differential Equations","abstract":"This work introduces neural Green's operators (NGOs), a novel neural operator network architecture that learns the solution operator for a parametric family of linear partial differential equations (PDEs). Our construction of NGOs is derived directly from the Green's formulation of such a solution operator. Similar to deep operator networks (DeepONets) and variationally mimetic operator networks (VarMiONs), NGOs constitutes an expansion of the solution to the PDE in terms of basis functions, that is returned from a sub-network, contracted with coefficients, that are returned from another sub-network. However, in accordance with the Green's formulation, NGOs accept weighted averages of the input functions, rather than sampled values thereof, as is the case in DeepONets and VarMiONs. Application of NGOs to canonical linear parametric PDEs shows that, while they remain competitive with DeepONets, VarMiONs and Fourier neural operators when testing on data that lie within the training distribution, they robustly generalize when testing on finer-scale data generated outside of the training distribution. Furthermore, we show that the explicit representation of the Green's function that is returned by NGOs enables the construction of effective preconditioners for numerical solvers for PDEs.","sentences":["This work introduces neural Green's operators (NGOs), a novel neural operator network architecture that learns the solution operator for a parametric family of linear partial differential equations (PDEs).","Our construction of NGOs is derived directly from the Green's formulation of such a solution operator.","Similar to deep operator networks (DeepONets) and variationally mimetic operator networks (VarMiONs), NGOs constitutes an expansion of the solution to the PDE in terms of basis functions, that is returned from a sub-network, contracted with coefficients, that are returned from another sub-network.","However, in accordance with the Green's formulation, NGOs accept weighted averages of the input functions, rather than sampled values thereof, as is the case in DeepONets and VarMiONs.","Application of NGOs to canonical linear parametric PDEs shows that, while they remain competitive with DeepONets, VarMiONs and Fourier neural operators when testing on data that lie within the training distribution, they robustly generalize when testing on finer-scale data generated outside of the training distribution.","Furthermore, we show that the explicit representation of the Green's function that is returned by NGOs enables the construction of effective preconditioners for numerical solvers for PDEs."],"url":"http://arxiv.org/abs/2406.01857v1","category":"cs.LG"}
{"created":"2024-06-03 23:21:52","title":"Short-range order and local distortions in entropy stabilized oxides","abstract":"An idealized high entropy oxide is characterized by perfect chemical disorder and perfect positional order. In this work, we investigate the extent to which short-range order (SRO) and local structural distortions impede that idealized scenario. Working in the entropy stabilized $\\alpha$-PbO$_2$ structure, we compare a two-component system, (Ti,Zr)O$_2$, with a four-component system, (Ti,Zr,Hf,Sn)O$_2$, using a combination of experimental and computational approaches. Special quasi-random structures are used in conjunction with density functional theory calculations to investigate the local distortions around specific elements revealing significant local distortions that are relatively insensitive to the number of chemical constituents. Using finite temperature Monte Carlo simulations, we are able to reproduce the previously experimentally observed SRO and transition temperature for the two-component system. However, ideal configurational entropy is never reached, so SRO is expected even at synthesis temperatures. On the other hand, the order-disorder transition temperature is dramatically lower and experimentally inaccessible for the four-component system, while the configurational entropy is closer to ideal and less sensitive to temperature. Total scattering measurements and pair distribution function analysis of slow-cooled and quenched samples support this view. In general, we demonstrate that SRO effects in high entropy materials are less prevalent as more components are added in, provided the pairwise interaction strengths remain comparable, while local distortions are less affected by the number of components.","sentences":["An idealized high entropy oxide is characterized by perfect chemical disorder and perfect positional order.","In this work, we investigate the extent to which short-range order (SRO) and local structural distortions impede that idealized scenario.","Working in the entropy stabilized $\\alpha$-PbO$_2$ structure, we compare a two-component system, (Ti,Zr)O$_2$, with a four-component system, (Ti,Zr,Hf,Sn)O$_2$, using a combination of experimental and computational approaches.","Special quasi-random structures are used in conjunction with density functional theory calculations to investigate the local distortions around specific elements revealing significant local distortions that are relatively insensitive to the number of chemical constituents.","Using finite temperature Monte Carlo simulations, we are able to reproduce the previously experimentally observed SRO and transition temperature for the two-component system.","However, ideal configurational entropy is never reached, so SRO is expected even at synthesis temperatures.","On the other hand, the order-disorder transition temperature is dramatically lower and experimentally inaccessible for the four-component system, while the configurational entropy is closer to ideal and less sensitive to temperature.","Total scattering measurements and pair distribution function analysis of slow-cooled and quenched samples support this view.","In general, we demonstrate that SRO effects in high entropy materials are less prevalent as more components are added in, provided the pairwise interaction strengths remain comparable, while local distortions are less affected by the number of components."],"url":"http://arxiv.org/abs/2406.01841v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-03 22:59:11","title":"Affine vertex operator superalgebra $L_{\\widehat{osp(1|2)}}(\\mathcal{l},0)$ at admissible level","abstract":"Let $L_{\\widehat{osp(1|2)}}(\\mathcal{l},0)$ be the simple affine vertex operator superalgebra with admissible level $\\mathcal{l}$. We prove that the category of weak $L_{\\widehat{osp(1|2)}}(\\mathcal{l},0)$-modules on which the positive part of $\\widehat{osp(1|2)}$ acts locally nilpotent is semisimple. Then we prove that $\\mathbb{Q}$-graded vertex operator superalgebras $(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0),\\omega_\\xi)$ with new Virasoro elements $\\omega_\\xi$ are rational and the irreducible modules are exactly the admissible modules for $\\widehat{osp(1|2)}$, where $0<\\xi<1$ is a rational number. Furthermore, we determine the Zhu's algebras $A(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0))$ and their bimodules $A(L(\\mathcal{l},\\mathcal{j}))$ for $(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0),\\omega_\\xi)$, where $\\mathcal{j}$ is the admissible weight. As an application, we calculate the fusion rules among the irreducible ordinary modules of $(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0),\\omega_\\xi)$.","sentences":["Let $L_{\\widehat{osp(1|2)}}(\\mathcal{l},0)$ be the simple affine vertex operator superalgebra with admissible level $\\mathcal{l}$. We prove that the category of weak $L_{\\widehat{osp(1|2)}}(\\mathcal{l},0)$-modules on which the positive part of $\\widehat{osp(1|2)}$ acts locally nilpotent is semisimple.","Then we prove that $\\mathbb{Q}$-graded vertex operator superalgebras $(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0),\\omega_\\xi)$ with new Virasoro elements $\\omega_\\xi$ are rational and the irreducible modules are exactly the admissible modules for $\\widehat{osp(1|2)}$, where $0<\\xi<1$ is a rational number.","Furthermore, we determine the Zhu's algebras $A(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0))$ and their bimodules $A(L(\\mathcal{l},\\mathcal{j}))$ for $(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0),\\omega_\\xi)$, where $\\mathcal{j}$ is the admissible weight.","As an application, we calculate the fusion rules among the irreducible ordinary modules of $(L_{\\widehat{osp(1|2)}}(\\mathcal{l},0),\\omega_\\xi)$."],"url":"http://arxiv.org/abs/2406.01830v1","category":"math.QA"}
{"created":"2024-06-03 22:56:28","title":"Spectral Flow for the Riemann zeros","abstract":"Recently, with Mussardo we defined a quantum mechanical problem of a single particle scattering with impurities wherein the quantized energy levels $E_n (\\sigma)$ are exactly equal to the zeros of the Riemann $\\zeta (s)$ where $\\sigma = \\Re (s)$ in the limit $\\sigma \\to 1/2$. The S-matrix is based on the Euler product and is unitary by construction, thus the underlying hamiltonian is hermitian and all eigenvalues must be real. Motivated by the Hilbert-P\\'olya idea we study the spectral flows for $\\{ E_n (\\sigma) \\}$. This leads to a simple criterion for the validity of the Riemann Hypothesis. The spectral flow arguments are simple enough that we present analogous results for the Generalized and Grand Riemann Hypotheses. We also illustrate our results for a counter example where the Riemann Hypothesis is violated since there is no underlying unitary S-matrix due to the lack of an Euler product.","sentences":["Recently, with Mussardo we defined a quantum mechanical problem of a single particle scattering with impurities wherein the quantized energy levels $E_n (\\sigma)$ are exactly equal to the zeros of the Riemann $\\zeta (s)$ where $\\sigma = \\Re (s)$ in the limit $\\sigma \\to 1/2$. The S-matrix is based on the Euler product and is unitary by construction, thus the underlying hamiltonian is hermitian and all eigenvalues must be real.","Motivated by the Hilbert-P\\'olya idea we study the spectral flows for $\\{ E_n (\\sigma) \\}$.","This leads to a simple criterion for the validity of the Riemann Hypothesis.","The spectral flow arguments are simple enough that we present analogous results for the Generalized and Grand Riemann Hypotheses.","We also illustrate our results for a counter example where the Riemann Hypothesis is violated since there is no underlying unitary S-matrix due to the lack of an Euler product."],"url":"http://arxiv.org/abs/2406.01828v1","category":"math-ph"}
{"created":"2024-06-03 21:56:03","title":"Magnetic field evolution for crystallization-driven dynamos in C/O white dwarfs","abstract":"We investigate the evolution of magnetic fields generated by the crystallization-driven dynamo in carbon-oxygen white dwarfs (WDs) with masses $\\lesssim1.05\\ M_{\\odot}$. We use scalings for the dynamo to demonstrate that the initial magnetic field strength ($B_{0}$) has an upper limit that depends on the initial convection zone size ($R_{\\mathrm{out},0}$) and the WD mass. We solve the induction equation to follow the magnetic field evolution after the dynamo phase ends. We show that the predicted surface magnetic field strength ($B_{\\mathrm{surf}}$) differs from $B_{0}$ by at least a factor of $\\sim$0.3. This reduction depends on $R_{\\mathrm{out},0}$, where values smaller than half of the star radius give $B_{\\mathrm{surf}}\\lesssim0.01\\ B_{0}$. We implement electrical conductivities that account for the solid phase effect on the Ohmic diffusion. We observe that the conductivity increases as the solid core grows, freezing in the magnetic field at a certain point of the evolution and slowing its outwards transport. We study the effect of turbulent magnetic diffusivity induced by the convection and find that for a small $R_{\\mathrm{out},0}$, $B_{\\mathrm{surf}}$ is stronger than the non-turbulent diffusion cases because of the more rapid transport, but still orders of magnitude smaller than $B_{0}$. Given these limitations, the crystallization-driven dynamo theory could explain only magnetic C/O WDs with field strengths less than a few MG for the mass range 0.45-1.05 $M_{\\odot}$. Our results also suggest that a buried fossil field must be at least 100 times stronger than observed surface fields if crystallization-driven convection is responsible for its transport to the surface.","sentences":["We investigate the evolution of magnetic fields generated by the crystallization-driven dynamo in carbon-oxygen white dwarfs (WDs) with masses $\\lesssim1.05\\ M_{\\odot}$.","We use scalings for the dynamo to demonstrate that the initial magnetic field strength ($B_{0}$) has an upper limit that depends on the initial convection zone size ($R_{\\mathrm{out},0}$) and the WD mass.","We solve the induction equation to follow the magnetic field evolution after the dynamo phase ends.","We show that the predicted surface magnetic field strength ($B_{\\mathrm{surf}}$) differs from $B_{0}$ by at least a factor of $\\sim$0.3.","This reduction depends on $R_{\\mathrm{out},0}$, where values smaller than half of the star radius give $B_{\\mathrm{surf}}\\lesssim0.01\\ B_{0}$. We implement electrical conductivities that account for the solid phase effect on the Ohmic diffusion.","We observe that the conductivity increases as the solid core grows, freezing in the magnetic field at a certain point of the evolution and slowing its outwards transport.","We study the effect of turbulent magnetic diffusivity induced by the convection and find that for a small $R_{\\mathrm{out},0}$, $B_{\\mathrm{surf}}$ is stronger than the non-turbulent diffusion cases because of the more rapid transport, but still orders of magnitude smaller than $B_{0}$. Given these limitations, the crystallization-driven dynamo theory could explain only magnetic C/O WDs with field strengths less than a few MG for the mass range 0.45-1.05 $M_{\\odot}$. Our results also suggest that a buried fossil field must be at least 100 times stronger than observed surface fields if crystallization-driven convection is responsible for its transport to the surface."],"url":"http://arxiv.org/abs/2406.01807v1","category":"astro-ph.SR"}
{"created":"2024-06-03 21:42:00","title":"Projective and Carrollian geometry at time/space-like infinity on projectively compact Ricci flat Einstein manifolds","abstract":"In this article we discuss how to construct canonical \\emph{strong} Carrollian geometries at time/space like infinity of projectively compact Ricci flat Einstein manifolds $(M,g)$ and discuss the links between the underlying projective structure of the metric $g$. The obtained Carrollian geometries are determined by the data of the projective compactification. The key idea to achieve this is to consider a new type of Cartan geometry based on a non-effective homogeneous model for projective geometry. We prove that this structure is a general feature of projectively compact Ricci flat Einstein manifolds.","sentences":["In this article we discuss how to construct canonical \\emph{strong} Carrollian geometries at time/space like infinity of projectively compact Ricci flat Einstein manifolds $(M,g)$ and discuss the links between the underlying projective structure of the metric $g$. The obtained Carrollian geometries are determined by the data of the projective compactification.","The key idea to achieve this is to consider a new type of Cartan geometry based on a non-effective homogeneous model for projective geometry.","We prove that this structure is a general feature of projectively compact Ricci flat Einstein manifolds."],"url":"http://arxiv.org/abs/2406.01800v1","category":"math.DG"}
{"created":"2024-06-03 21:03:16","title":"Dependence of the Reconstructed Core-Collapse Supernova Gravitational Wave High-Frequency Feature on the Nuclear Equation of State, in Real Interferometric Data","abstract":"We present an analysis of gravitational wave (GW) predictions from five two-dimensional Core Collapse Supernova (CCSN) simulations that varied only in the Equation of State (EOS) implemented. The GW signals from these simulations are used to produce spectrograms in the absence of noise, and the emergent high-frequency feature (HFF) is found to differ quantitatively between simulations. Below 1 kHz, the HFF is well approximated by a first-order polynomial in time. The resulting slope was found to vary between 10-50% across all models. Further, using real interferometric noise we investigated the current capabilities of GW detectors to resolve these differences in HFF slope for a Galactic CCSN. We find that for distances up to 1 kpc, current detectors can resolve HFF slopes that vary by at least 30%. For further Galactic distances, current detectors are capable of distinguishing the upper and lower bounds of the HFF slope for groupings of our models that varied in EOS. With the higher sensitivity of future GW detectors, and with improved analysis of the HFF, our ability to resolve properties of the HFF will improve for all Galactic distances. This study shows the potential of using the HFF of CCSN produced GWs to provide insight into the physical processes occurring deep within CCSN during collapse, and in particular its potential to further constrain the EOS through GW detection.","sentences":["We present an analysis of gravitational wave (GW) predictions from five two-dimensional Core Collapse Supernova (CCSN) simulations that varied only in the Equation of State (EOS) implemented.","The GW signals from these simulations are used to produce spectrograms in the absence of noise, and the emergent high-frequency feature (HFF) is found to differ quantitatively between simulations.","Below 1 kHz, the HFF is well approximated by a first-order polynomial in time.","The resulting slope was found to vary between 10-50% across all models.","Further, using real interferometric noise we investigated the current capabilities of GW detectors to resolve these differences in HFF slope for a Galactic CCSN.","We find that for distances up to 1 kpc, current detectors can resolve HFF slopes that vary by at least 30%.","For further Galactic distances, current detectors are capable of distinguishing the upper and lower bounds of the HFF slope for groupings of our models that varied in EOS.","With the higher sensitivity of future GW detectors, and with improved analysis of the HFF, our ability to resolve properties of the HFF will improve for all Galactic distances.","This study shows the potential of using the HFF of CCSN produced GWs to provide insight into the physical processes occurring deep within CCSN during collapse, and in particular its potential to further constrain the EOS through GW detection."],"url":"http://arxiv.org/abs/2406.01784v1","category":"astro-ph.HE"}
{"created":"2024-06-03 20:41:34","title":"Higher order asymptotic expansions for the convection-diffusion equation in the Fujita-subcritical case","abstract":"This paper is devoted to the asymptotic expansions of global solutions to the convection-diffusion equation in the Fujita-subcritical case. We improve the result by Zuazua (1993) and establish higher order asymptotic expansions with decay estimates of the remainders. We also discuss the optimality for the decay rate of the remainder.","sentences":["This paper is devoted to the asymptotic expansions of global solutions to the convection-diffusion equation in the Fujita-subcritical case.","We improve the result by Zuazua (1993) and establish higher order asymptotic expansions with decay estimates of the remainders.","We also discuss the optimality for the decay rate of the remainder."],"url":"http://arxiv.org/abs/2406.01777v1","category":"math.AP"}
{"created":"2024-06-03 20:29:09","title":"Concurrent normals problem for convex polytopes and Euclidean distance degree","abstract":"It is conjectured since long that for any convex body $P\\subset \\mathbb{R}^n$ there exists a point in its interior which belongs to at least $2n$ normals from different points on the boundary of $P$. The conjecture is known to be true for $n=2,3,4$.   We treat the same problem for convex polytopes in $\\mathbb{R}^3$. It turns out that the PL concurrent normals problem differs a lot from the smooth one. One almost immediately proves that a convex polytope in $\\mathbb{R}^3$ has $8$ normals to its boundary emanating from some point in its interior. Moreover, we conjecture that each simple polytope has a point in its interior with $10$ normals to the boundary. We confirm the conjecture for all tetrahedra and triangular prisms and give a sufficient condition for a simple polytope to have a point with $10$ normals.   Other related topics (average number of normals, minimal number of normals from an interior point, other dimensions) are discussed.","sentences":["It is conjectured since long that for any convex body $P\\subset \\mathbb{R}^n$ there exists a point in its interior which belongs to at least $2n$ normals from different points on the boundary of $P$. The conjecture is known to be true for $n=2,3,4$.   We treat the same problem for convex polytopes in $\\mathbb{R}^3$. It turns out that the PL concurrent normals problem differs a lot from the smooth one.","One almost immediately proves that a convex polytope in $\\mathbb{R}^3$ has $8$ normals to its boundary emanating from some point in its interior.","Moreover, we conjecture that each simple polytope has a point in its interior with $10$ normals to the boundary.","We confirm the conjecture for all tetrahedra and triangular prisms and give a sufficient condition for a simple polytope to have a point with $10$ normals.   ","Other related topics (average number of normals, minimal number of normals from an interior point, other dimensions) are discussed."],"url":"http://arxiv.org/abs/2406.01773v1","category":"math.MG"}
{"created":"2024-06-03 20:25:57","title":"Homoclinic Solution to Zero of a Non-autonomous, Nonlinear, Second Order Differential Equation with Quadratic Growth on the Derivative","abstract":"This work aims to obtain a positive, smooth, even, and homoclinic to zero (i.e zero at infinity) solution to a non-autonomous, second-order, nonlinear differential equation involving quadratic growth on the derivative. We apply Galerkin's method combined with Strauss' approximation on the term involving the first derivative to obtain weak solutions. We also study the regularity of the solutions and the dependence on their existence with a parameter","sentences":["This work aims to obtain a positive, smooth, even, and homoclinic to zero (i.e zero at infinity) solution to a non-autonomous, second-order, nonlinear differential equation involving quadratic growth on the derivative.","We apply Galerkin's method combined with Strauss' approximation on the term involving the first derivative to obtain weak solutions.","We also study the regularity of the solutions and the dependence on their existence with a parameter"],"url":"http://arxiv.org/abs/2406.01772v1","category":"math.CA"}
{"created":"2024-06-03 20:15:28","title":"How Does Gradient Descent Learn Features -- A Local Analysis for Regularized Two-Layer Neural Networks","abstract":"The ability of learning useful features is one of the major advantages of neural networks. Although recent works show that neural network can operate in a neural tangent kernel (NTK) regime that does not allow feature learning, many works also demonstrate the potential for neural networks to go beyond NTK regime and perform feature learning. Recently, a line of work highlighted the feature learning capabilities of the early stages of gradient-based training. In this paper we consider another mechanism for feature learning via gradient descent through a local convergence analysis. We show that once the loss is below a certain threshold, gradient descent with a carefully regularized objective will capture ground-truth directions. Our results demonstrate that feature learning not only happens at the initial gradient steps, but can also occur towards the end of training.","sentences":["The ability of learning useful features is one of the major advantages of neural networks.","Although recent works show that neural network can operate in a neural tangent kernel (NTK) regime that does not allow feature learning, many works also demonstrate the potential for neural networks to go beyond NTK regime and perform feature learning.","Recently, a line of work highlighted the feature learning capabilities of the early stages of gradient-based training.","In this paper we consider another mechanism for feature learning via gradient descent through a local convergence analysis.","We show that once the loss is below a certain threshold, gradient descent with a carefully regularized objective will capture ground-truth directions.","Our results demonstrate that feature learning not only happens at the initial gradient steps, but can also occur towards the end of training."],"url":"http://arxiv.org/abs/2406.01766v1","category":"cs.LG"}
{"created":"2024-06-03 19:39:15","title":"Stable Boundary Layers with Subsidence: Scaling and Similarity of the Truly Steady State","abstract":"The stable boundary layer (SBL) subjected to large-scale subsidence is studied through large-eddy simulations (LESs) with fixed surface temperature and a linear subsidence velocity profile. These boundary layers reach a truly steady state, where thermal equilibrium is established by a balance between surface cooling and subsidence-induced heating. We identify three governing dimensionless groups by scaling the governing equations with the geostrophic wind and Coriolis frequency, and systematically investigate the impact of these external parameters on global flow properties and mean profiles in the steady state. The SBL depth, low-level jet, and the magnitude of the turbulent momentum flux are reduced when the subsidence rate or Buoyancy number increases, while surface heat flux is enhanced. The shape of normalized mean profiles of temperature and heat flux is mainly determined by the subsidence rate, while they collapse for varying buoyancy and surface Rossby numbers. We develop empirical correlations for the stability parameter $h_{\\theta}/L_O$ and a thermal shape factor, and propose a new unidirectional geostrophic drag law, to form a closed set of equations that estimates relevant flow properties from external parameters. The estimation errors compared to the LES data are less than 5% for friction velocity and surface heat flux, and at most 10% for the SBL depth $h_{\\theta}$. Within the surface layer, dimensionless velocity and temperature gradients in the steady SBL with subsidence show acceptable agreement to Monin-Obukhov similarity theory, while the collapse is improved when a recently proposed mixed scaling parameter, that includes $h_{\\theta}/L_O$, is used.","sentences":["The stable boundary layer (SBL) subjected to large-scale subsidence is studied through large-eddy simulations (LESs) with fixed surface temperature and a linear subsidence velocity profile.","These boundary layers reach a truly steady state, where thermal equilibrium is established by a balance between surface cooling and subsidence-induced heating.","We identify three governing dimensionless groups by scaling the governing equations with the geostrophic wind and Coriolis frequency, and systematically investigate the impact of these external parameters on global flow properties and mean profiles in the steady state.","The SBL depth, low-level jet, and the magnitude of the turbulent momentum flux are reduced when the subsidence rate or Buoyancy number increases, while surface heat flux is enhanced.","The shape of normalized mean profiles of temperature and heat flux is mainly determined by the subsidence rate, while they collapse for varying buoyancy and surface Rossby numbers.","We develop empirical correlations for the stability parameter $h_{\\theta}/L_O$ and a thermal shape factor, and propose a new unidirectional geostrophic drag law, to form a closed set of equations that estimates relevant flow properties from external parameters.","The estimation errors compared to the LES data are less than 5% for friction velocity and surface heat flux, and at most 10% for the SBL depth $h_{\\theta}$. Within the surface layer, dimensionless velocity and temperature gradients in the steady SBL with subsidence show acceptable agreement to Monin-Obukhov similarity theory, while the collapse is improved when a recently proposed mixed scaling parameter, that includes $h_{\\theta}/L_O$, is used."],"url":"http://arxiv.org/abs/2406.01751v1","category":"physics.flu-dyn"}
{"created":"2024-06-03 19:28:12","title":"A universal reduced basis for the calibration of covariant energy density functionals","abstract":"The reduced basis method is used to construct a \"universal\" basis of Dirac orbitals that may be applicable throughout the nuclear chart to calibrate covariant energy density functionals. Relative to our earlier work using the non-relativistic Schr\\\"odinger equation, the Dirac equation adds an extra layer of complexity due to the existence of negative energy states. However, once this problem is mitigated, the resulting reduced basis is able to accurately and efficiently reproduce the high-fidelity model at a fraction of the computational cost. We are confident that the resulting reduced basis will serve as a foundational element in developing rapid and accurate emulators. In turn, these emulators will play a critical role in the Bayesian optimization of covariant energy density functionals.","sentences":["The reduced basis method is used to construct a \"universal\" basis of Dirac orbitals that may be applicable throughout the nuclear chart to calibrate covariant energy density functionals.","Relative to our earlier work using the non-relativistic Schr\\\"odinger equation, the Dirac equation adds an extra layer of complexity due to the existence of negative energy states.","However, once this problem is mitigated, the resulting reduced basis is able to accurately and efficiently reproduce the high-fidelity model at a fraction of the computational cost.","We are confident that the resulting reduced basis will serve as a foundational element in developing rapid and accurate emulators.","In turn, these emulators will play a critical role in the Bayesian optimization of covariant energy density functionals."],"url":"http://arxiv.org/abs/2406.01747v1","category":"nucl-th"}
{"created":"2024-06-03 19:03:24","title":"Time-Spectral Efficiency","abstract":"This study concerns the efficiency of time-spectral methods for numerical solution of differential equations. It is found that the time-spectral method GWRM demonstrates insensitivity to stiffness and chaoticity due to the implicit nature of the solution algorithm. Accuracy is thus determined primarily by numerical resolution of the solution shape. Examples of efficient solution of stiff and chaotic problems, where explicit methods fail or are significantly slower, are given. Non-smooth and partially steep solutions, however, remain challenging for convergence and accuracy. Some, earlier suggested, smoothing algorithms are shown to be ineffective in addressing this issue. Our findings underscore the need for further exploration of time-spectral approaches to enhance convergence and accuracy for steep or non-smooth solutions.","sentences":["This study concerns the efficiency of time-spectral methods for numerical solution of differential equations.","It is found that the time-spectral method GWRM demonstrates insensitivity to stiffness and chaoticity due to the implicit nature of the solution algorithm.","Accuracy is thus determined primarily by numerical resolution of the solution shape.","Examples of efficient solution of stiff and chaotic problems, where explicit methods fail or are significantly slower, are given.","Non-smooth and partially steep solutions, however, remain challenging for convergence and accuracy.","Some, earlier suggested, smoothing algorithms are shown to be ineffective in addressing this issue.","Our findings underscore the need for further exploration of time-spectral approaches to enhance convergence and accuracy for steep or non-smooth solutions."],"url":"http://arxiv.org/abs/2406.01740v1","category":"math.NA"}
{"created":"2024-06-03 19:00:41","title":"A Surprisingly Simple Method for Distributed Euclidean-Minimum Spanning Tree / Single Linkage Dendrogram Construction from High Dimensional Embeddings via Distance Decomposition","abstract":"We introduce a decomposition method for the distributed calculation of exact Euclidean Minimum Spanning Trees in high dimensions (where sub-quadratic algorithms are not effective), or more generalized geometric-minimum spanning trees of complete graphs, where for each vertex $v\\in V$ in the graph $G=(V,E)$ is represented by a vector in $\\vec{v}\\in \\mathbb{R}^n$, and each for any edge, the the weight of the edge in the graph is given by a symmetric binary `distance' function between the representative vectors $w(\\{x,y\\}) = d(\\vec{x},\\vec{y})$. This is motivated by the task of clustering high dimensional embeddings produced by neural networks, where low-dimensional algorithms are ineffective; such geometric-minimum spanning trees find applications as a subroutine in the construction of single linkage dendrograms, as the two structures can be converted between each other efficiently.","sentences":["We introduce a decomposition method for the distributed calculation of exact Euclidean Minimum Spanning Trees in high dimensions (where sub-quadratic algorithms are not effective), or more generalized geometric-minimum spanning trees of complete graphs, where for each vertex $v\\in V$ in the graph $G=(V,E)$ is represented by a vector in $\\vec{v}\\in \\mathbb{R}^n$, and each for any edge, the the weight of the edge in the graph is given by a symmetric binary `distance' function between the representative vectors $w(\\{x,y\\}) = d(\\vec{x},\\vec{y})$.","This is motivated by the task of clustering high dimensional embeddings produced by neural networks, where low-dimensional algorithms are ineffective; such geometric-minimum spanning trees find applications as a subroutine in the construction of single linkage dendrograms, as the two structures can be converted between each other efficiently."],"url":"http://arxiv.org/abs/2406.01739v1","category":"cs.DC"}
{"created":"2024-06-03 18:55:50","title":"Scaling laws of elastic proton-proton scattering differential cross sections","abstract":"We show that elastic scattering $pp \\to pp $ differential cross sections as function of the four-momentum transfer square $|t|$ have a universal property, such that the ratio of bump-to-dip positions is constant from the energies of the ISR to the LHC, from tens of GeV and up to the TeV scale. We explore this property to compare the geometrical scaling observed at the ISR with the recently proposed scaling law at the LHC. We argue that, at the LHC, within present experimental uncertainties, there is in fact a family of scaling laws. We discuss the constraints that the scaling laws impose on the parametrization of the elastic $pp\\to pp$ scattering amplitude.","sentences":["We show that elastic scattering $pp \\to pp $ differential cross sections as function of the four-momentum transfer square $|t|$ have a universal property, such that the ratio of bump-to-dip positions is constant from the energies of the ISR to the LHC, from tens of GeV and up to the TeV scale.","We explore this property to compare the geometrical scaling observed at the ISR with the recently proposed scaling law at the LHC.","We argue that, at the LHC, within present experimental uncertainties, there is in fact a family of scaling laws.","We discuss the constraints that the scaling laws impose on the parametrization of the elastic $pp\\to pp$ scattering amplitude."],"url":"http://arxiv.org/abs/2406.01737v1","category":"hep-ph"}
{"created":"2024-06-03 18:49:57","title":"Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching","abstract":"Diffusion Transformers have recently demonstrated unprecedented generative capabilities for various tasks. The encouraging results, however, come with the cost of slow inference, since each denoising step requires inference on a transformer model with a large scale of parameters. In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68% of the computation in the cache steps (46.84% for all steps), with less than 0.01 drop in FID. To achieve this, we introduce a novel scheme, named Learning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for diffusion transformers. Specifically, by leveraging the identical structure of layers in transformers and the sequential nature of diffusion, we explore redundant computations between timesteps by treating each layer as the fundamental unit for caching. To address the challenge of the exponential search space in deep models for identifying layers to cache and remove, we propose a novel differentiable optimization objective. An input-invariant yet timestep-variant router is then optimized, which can finally produce a static computation graph. Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed.","sentences":["Diffusion Transformers have recently demonstrated unprecedented generative capabilities for various tasks.","The encouraging results, however, come with the cost of slow inference, since each denoising step requires inference on a transformer model with a large scale of parameters.","In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters.","In the case of U-ViT-H/2, for example, we may remove up to 93.68% of the computation in the cache steps (46.84% for all steps), with less than 0.01 drop in FID.","To achieve this, we introduce a novel scheme, named Learning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for diffusion transformers.","Specifically, by leveraging the identical structure of layers in transformers and the sequential nature of diffusion, we explore redundant computations between timesteps by treating each layer as the fundamental unit for caching.","To address the challenge of the exponential search space in deep models for identifying layers to cache and remove, we propose a novel differentiable optimization objective.","An input-invariant yet timestep-variant router is then optimized, which can finally produce a static computation graph.","Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed."],"url":"http://arxiv.org/abs/2406.01733v1","category":"cs.LG"}
{"created":"2024-06-03 18:18:03","title":"Walk on Spheres for PDE-based Path Planning","abstract":"In this paper, we investigate the Walk on Spheres algorithm (WoS) for motion planning in robotics. WoS is a Monte Carlo method to solve the Dirichlet problem developed in the 50s by Muller and has recently been repopularized by Sawhney and Crane, who showed its applicability for geometry processing in volumetric domains. This paper provides a first study into the applicability of WoS for robot motion planning in configuration spaces, with potential fields defined as the solution of screened Poisson equations. The experiments in this paper empirically indicate the method's trivial parallelization, its dimension-independent convergence characteristic of $O(1/N)$ in the number of walks, and a validation experiment on the RR platform.","sentences":["In this paper, we investigate the Walk on Spheres algorithm (WoS) for motion planning in robotics.","WoS is a Monte Carlo method to solve the Dirichlet problem developed in the 50s by Muller and has recently been repopularized by Sawhney and Crane, who showed its applicability for geometry processing in volumetric domains.","This paper provides a first study into the applicability of WoS for robot motion planning in configuration spaces, with potential fields defined as the solution of screened Poisson equations.","The experiments in this paper empirically indicate the method's trivial parallelization, its dimension-independent convergence characteristic of $O(1/N)$ in the number of walks, and a validation experiment on the RR platform."],"url":"http://arxiv.org/abs/2406.01713v1","category":"cs.RO"}
{"created":"2024-06-03 18:08:59","title":"Counting and Hardness-of-Finding Fixed Points in Cellular Automata on Random Graphs","abstract":"We study the fixed points of outer-totalistic cellular automata on sparse random regular graphs. These can be seen as constraint satisfaction problems, where each variable must adhere to the same local constraint, which depends solely on its state and the total number of its neighbors in each possible state. Examples of this setting include classical problems such as independent sets or assortative/dissasortative partitions. We analyse the existence and number of fixed points in the large system limit using the cavity method, under both the replica symmetric (RS) and one-step replica symmetry breaking (1RSB) assumption. This method allows us to characterize the structure of the space of solutions, in particular, if the solutions are clustered and whether the clusters contain frozen variables. This last property is conjectured to be linked to the typical algorithmic hardness of the problem. We bring experimental evidence for this claim by studying the performance of the belief-propagation reinforcement algorithm, a message-passing-based solver for these constraint satisfaction problems.","sentences":["We study the fixed points of outer-totalistic cellular automata on sparse random regular graphs.","These can be seen as constraint satisfaction problems, where each variable must adhere to the same local constraint, which depends solely on its state and the total number of its neighbors in each possible state.","Examples of this setting include classical problems such as independent sets or assortative/dissasortative partitions.","We analyse the existence and number of fixed points in the large system limit using the cavity method, under both the replica symmetric (RS) and one-step replica symmetry breaking (1RSB) assumption.","This method allows us to characterize the structure of the space of solutions, in particular, if the solutions are clustered and whether the clusters contain frozen variables.","This last property is conjectured to be linked to the typical algorithmic hardness of the problem.","We bring experimental evidence for this claim by studying the performance of the belief-propagation reinforcement algorithm, a message-passing-based solver for these constraint satisfaction problems."],"url":"http://arxiv.org/abs/2406.01710v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-03 18:01:01","title":"Anomalous thermal relaxation in Yukawa fluids","abstract":"We perform microscopic many-body simulations of the thermal relaxation of a realistic warm Yukawa fluid by solving the dynamics of an ensemble of N particles in a cubic box acting as a heat reservoir kept at fixed temperature. For specific conditions of density, ion size and potential interaction range we find novel anomalous thermal relaxation for these systems, exhibiting faster cooling from increasingly larger temperature values. Our findings show that the interplay among finite particle size, Yukawa interaction range and density are key to understanding the features of the cooling curve $T(t)$. The non-Newtonian thermal behavior is analyzed in terms of a time-dependent relaxation time $\\tau(t)$ and a generalized Langevin equation. We compare our results to existing Mpemba-like phenomena and interpret temperature oscillation patterns arising from a non-trivial memory function. Further, we consider the impact of composition admixtures in the simulated system. We discuss these results and the more general implications for non-Markovian systems where Yukawa-like potentials are involved, such as astrophysical plasmas.","sentences":["We perform microscopic many-body simulations of the thermal relaxation of a realistic warm Yukawa fluid by solving the dynamics of an ensemble of N particles in a cubic box acting as a heat reservoir kept at fixed temperature.","For specific conditions of density, ion size and potential interaction range we find novel anomalous thermal relaxation for these systems, exhibiting faster cooling from increasingly larger temperature values.","Our findings show that the interplay among finite particle size, Yukawa interaction range and density are key to understanding the features of the cooling curve $T(t)$. The non-Newtonian thermal behavior is analyzed in terms of a time-dependent relaxation time $\\tau(t)$ and a generalized Langevin equation.","We compare our results to existing Mpemba-like phenomena and interpret temperature oscillation patterns arising from a non-trivial memory function.","Further, we consider the impact of composition admixtures in the simulated system.","We discuss these results and the more general implications for non-Markovian systems where Yukawa-like potentials are involved, such as astrophysical plasmas."],"url":"http://arxiv.org/abs/2406.01700v1","category":"physics.plasm-ph"}
{"created":"2024-06-03 18:00:02","title":"Prethermal Time-Crystalline Corner Modes","abstract":"We demonstrate the existence of prethermal discrete time crystals whose sub-harmonic response is entirely localized to zero-dimensional corner modes. Within the exponentially long prethermal regime, we show that the robustness of these corner modes arises from two related, yet distinct mechanisms: the presence of a higher-order symmetry-protected topological phase in the effective Hamiltonian, or the emergence of a dynamical constraint that prevents the decay of the corner mode. While the first mechanism ensures the stability of the sub-harmonic response throughout the entirety of the prethermal regime, it is restricted to initial states in the ground state manifold of the effective Hamiltonian. By contrast, the second mechanism enables the observation of the prethermal time-crystalline order for arbitrary initial states, albeit with a time scale that is not only determined by the frequency of the drive, but also the relative energy scale across the system's sublattices. We characterize these two mechanisms by simulating the dynamics of a periodically driven two-dimensional spin model, and discuss natural extensions of our model to all other dimensions.","sentences":["We demonstrate the existence of prethermal discrete time crystals whose sub-harmonic response is entirely localized to zero-dimensional corner modes.","Within the exponentially long prethermal regime, we show that the robustness of these corner modes arises from two related, yet distinct mechanisms: the presence of a higher-order symmetry-protected topological phase in the effective Hamiltonian, or the emergence of a dynamical constraint that prevents the decay of the corner mode.","While the first mechanism ensures the stability of the sub-harmonic response throughout the entirety of the prethermal regime, it is restricted to initial states in the ground state manifold of the effective Hamiltonian.","By contrast, the second mechanism enables the observation of the prethermal time-crystalline order for arbitrary initial states, albeit with a time scale that is not only determined by the frequency of the drive, but also the relative energy scale across the system's sublattices.","We characterize these two mechanisms by simulating the dynamics of a periodically driven two-dimensional spin model, and discuss natural extensions of our model to all other dimensions."],"url":"http://arxiv.org/abs/2406.01686v1","category":"quant-ph"}
{"created":"2024-06-03 18:00:01","title":"Emergence of a second law of thermodynamics in isolated quantum systems","abstract":"The second law of thermodynamics states that the entropy of an isolated system can only increase over time. This appears to conflict with the reversible evolution of isolated quantum systems under the Schr\\\"odinger equation, which preserves the von Neumann entropy. Nonetheless, one finds that with respect to many observables, expectation values approach a fixed value -- their equilibrium value. This ultimately raises the question: in what sense does the entropy of an isolated quantum system increase over time? For classical systems, one introduces the assumption of a low entropy initial state along with the concept of ignorance about the microscopic details of the physical system, leading to a statistical interpretation of the second law. By considering the observables through which we examine quantum systems, both these assumptions can be incorporated, building upon recent studies of the equilibration on average of observables. While the statistical behavior of observable expectation values is well-established, a quantitative connection to entropy increase has been lacking so far. In deriving novel bounds for the equilibration of observables, and considering the entropy of the system relative to observables, we recover a variant of the second law: the entropy with respect to a given observable tends towards its equilibrium value in the course of the system's unitary evolution. These results also support recent findings which question the necessity of non-integrability for equilibration in quantum systems. We further illustrate our bounds using numerical results from the paradigmatic example of a quantum Ising model on a chain of spins. There, we observe entropy increasing up to equilibrium values, as well as fluctuations which expose the underlying reversible evolution in accordance with the derived bounds.","sentences":["The second law of thermodynamics states that the entropy of an isolated system can only increase over time.","This appears to conflict with the reversible evolution of isolated quantum systems under the Schr\\\"odinger equation, which preserves the von Neumann entropy.","Nonetheless, one finds that with respect to many observables, expectation values approach a fixed value -- their equilibrium value.","This ultimately raises the question: in what sense does the entropy of an isolated quantum system increase over time?","For classical systems, one introduces the assumption of a low entropy initial state along with the concept of ignorance about the microscopic details of the physical system, leading to a statistical interpretation of the second law.","By considering the observables through which we examine quantum systems, both these assumptions can be incorporated, building upon recent studies of the equilibration on average of observables.","While the statistical behavior of observable expectation values is well-established, a quantitative connection to entropy increase has been lacking so far.","In deriving novel bounds for the equilibration of observables, and considering the entropy of the system relative to observables, we recover a variant of the second law: the entropy with respect to a given observable tends towards its equilibrium value in the course of the system's unitary evolution.","These results also support recent findings which question the necessity of non-integrability for equilibration in quantum systems.","We further illustrate our bounds using numerical results from the paradigmatic example of a quantum Ising model on a chain of spins.","There, we observe entropy increasing up to equilibrium values, as well as fluctuations which expose the underlying reversible evolution in accordance with the derived bounds."],"url":"http://arxiv.org/abs/2406.01677v1","category":"quant-ph"}
{"created":"2024-06-03 18:00:01","title":"New insights into axion freeze-in","abstract":"Freeze-in via the axion-photon coupling, $g_{\\phi\\gamma}$, can produce axions in the early Universe. At low reheating temperatures close to the minimum allowed value $T_{\\rm reh}\\approx T_{\\rm BBN}\\approx 10\\,{\\rm MeV}$, the abundance peaks for axion masses $m_\\phi\\approx T_{\\rm reh}$. Such heavy axions are unstable and subsequently decay, leading to strong constraints on $g_{\\phi\\gamma}$ from astrophysics and cosmology. In this work, we revisit the computation of the freeze-in abundance and clarify important issues. We begin with a complete computation of the collision terms for the Primakoff process, electron-positron annihilation, and photon-to-axion (inverse-)decay, while approximately taking into account plasma screening and threshold effects. We then solve the Boltzmann equation for the full axion distribution function. We confirm previous results about the importance of both processes to the effective \"relic abundance\" (defined as density prior to decay), and provide useful fitting formulae to estimate the freeze-in abundance from the equilibrium interaction rate. For the distribution function, we find an out-of-equilibrium population of axions and introduce an effective temperature for them. We follow the evolution right up until decay, and find that the average axion kinetic energy is larger than a thermal relic by between 20\\% and 80\\%, which may have implications for limits on decaying axions from X-ray spectra. We extend our study to a two-axion system with quartic cross-coupling, and find that for typical/expected couplings, freeze-in of a second axion flavour by annihilations leads to a negligibly small contribution to the relic density.","sentences":["Freeze-in via the axion-photon coupling, $g_{\\phi\\gamma}$, can produce axions in the early Universe.","At low reheating temperatures close to the minimum allowed value $T_{\\rm reh}\\approx T_{\\rm BBN}\\approx","10\\,{\\rm MeV}$, the abundance peaks for axion masses $m_\\phi\\approx T_{\\rm reh}$. Such heavy axions are unstable and subsequently decay, leading to strong constraints on $g_{\\phi\\gamma}$ from astrophysics and cosmology.","In this work, we revisit the computation of the freeze-in abundance and clarify important issues.","We begin with a complete computation of the collision terms for the Primakoff process, electron-positron annihilation, and photon-to-axion (inverse-)decay, while approximately taking into account plasma screening and threshold effects.","We then solve the Boltzmann equation for the full axion distribution function.","We confirm previous results about the importance of both processes to the effective \"relic abundance\" (defined as density prior to decay), and provide useful fitting formulae to estimate the freeze-in abundance from the equilibrium interaction rate.","For the distribution function, we find an out-of-equilibrium population of axions and introduce an effective temperature for them.","We follow the evolution right up until decay, and find that the average axion kinetic energy is larger than a thermal relic by between 20\\% and 80\\%, which may have implications for limits on decaying axions from X-ray spectra.","We extend our study to a two-axion system with quartic cross-coupling, and find that for typical/expected couplings, freeze-in of a second axion flavour by annihilations leads to a negligibly small contribution to the relic density."],"url":"http://arxiv.org/abs/2406.01678v1","category":"hep-ph"}
{"created":"2024-06-03 17:59:57","title":"MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild","abstract":"We present MultiPly, a novel framework to reconstruct multiple people in 3D from monocular in-the-wild videos. Reconstructing multiple individuals moving and interacting naturally from monocular in-the-wild videos poses a challenging task. Addressing it necessitates precise pixel-level disentanglement of individuals without any prior knowledge about the subjects. Moreover, it requires recovering intricate and complete 3D human shapes from short video sequences, intensifying the level of difficulty. To tackle these challenges, we first define a layered neural representation for the entire scene, composited by individual human and background models. We learn the layered neural representation from videos via our layer-wise differentiable volume rendering. This learning process is further enhanced by our hybrid instance segmentation approach which combines the self-supervised 3D segmentation and the promptable 2D segmentation module, yielding reliable instance segmentation supervision even under close human interaction. A confidence-guided optimization formulation is introduced to optimize the human poses and shape/appearance alternately. We incorporate effective objectives to refine human poses via photometric information and impose physically plausible constraints on human dynamics, leading to temporally consistent 3D reconstructions with high fidelity. The evaluation of our method shows the superiority over prior art on publicly available datasets and in-the-wild videos.","sentences":["We present MultiPly, a novel framework to reconstruct multiple people in 3D from monocular in-the-wild videos.","Reconstructing multiple individuals moving and interacting naturally from monocular in-the-wild videos poses a challenging task.","Addressing it necessitates precise pixel-level disentanglement of individuals without any prior knowledge about the subjects.","Moreover, it requires recovering intricate and complete 3D human shapes from short video sequences, intensifying the level of difficulty.","To tackle these challenges, we first define a layered neural representation for the entire scene, composited by individual human and background models.","We learn the layered neural representation from videos via our layer-wise differentiable volume rendering.","This learning process is further enhanced by our hybrid instance segmentation approach which combines the self-supervised 3D segmentation and the promptable 2D segmentation module, yielding reliable instance segmentation supervision even under close human interaction.","A confidence-guided optimization formulation is introduced to optimize the human poses and shape/appearance alternately.","We incorporate effective objectives to refine human poses via photometric information and impose physically plausible constraints on human dynamics, leading to temporally consistent 3D reconstructions with high fidelity.","The evaluation of our method shows the superiority over prior art on publicly available datasets and in-the-wild videos."],"url":"http://arxiv.org/abs/2406.01595v1","category":"cs.CV"}
{"created":"2024-06-03 17:59:33","title":"Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks","abstract":"A wide range of empirical and theoretical works have shown that overparameterisation can amplify the performance of neural networks. According to the lottery ticket hypothesis, overparameterised networks have an increased chance of containing a sub-network that is well-initialised to solve the task at hand. A more parsimonious approach, inspired by animal learning, consists in guiding the learner towards solving the task by curating the order of the examples, i.e. providing a curriculum. However, this learning strategy seems to be hardly beneficial in deep learning applications. In this work, we undertake an analytical study that connects curriculum learning and overparameterisation. In particular, we investigate their interplay in the online learning setting for a 2-layer network in the XOR-like Gaussian Mixture problem. Our results show that a high degree of overparameterisation -- while simplifying the problem -- can limit the benefit from curricula, providing a theoretical account of the ineffectiveness of curricula in deep learning.","sentences":["A wide range of empirical and theoretical works have shown that overparameterisation can amplify the performance of neural networks.","According to the lottery ticket hypothesis, overparameterised networks have an increased chance of containing a sub-network that is well-initialised to solve the task at hand.","A more parsimonious approach, inspired by animal learning, consists in guiding the learner towards solving the task by curating the order of the examples, i.e. providing a curriculum.","However, this learning strategy seems to be hardly beneficial in deep learning applications.","In this work, we undertake an analytical study that connects curriculum learning and overparameterisation.","In particular, we investigate their interplay in the online learning setting for a 2-layer network in the XOR-like Gaussian Mixture problem.","Our results show that a high degree of overparameterisation -- while simplifying the problem -- can limit the benefit from curricula, providing a theoretical account of the ineffectiveness of curricula in deep learning."],"url":"http://arxiv.org/abs/2406.01589v1","category":"stat.ML"}
{"created":"2024-06-03 17:56:58","title":"Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit","abstract":"We study the problem of gradient descent learning of a single-index target function $f_*(\\boldsymbol{x}) = \\textstyle\\sigma_*\\left(\\langle\\boldsymbol{x},\\boldsymbol{\\theta}\\rangle\\right)$ under isotropic Gaussian data in $\\mathbb{R}^d$, where the link function $\\sigma_*:\\mathbb{R}\\to\\mathbb{R}$ is an unknown degree $q$ polynomial with information exponent $p$ (defined as the lowest degree in the Hermite expansion). Prior works showed that gradient-based training of neural networks can learn this target with $n\\gtrsim d^{\\Theta(p)}$ samples, and such statistical complexity is predicted to be necessary by the correlational statistical query lower bound. Surprisingly, we prove that a two-layer neural network optimized by an SGD-based algorithm learns $f_*$ of arbitrary polynomial link function with a sample and runtime complexity of $n \\asymp T \\asymp C(q) \\cdot d\\mathrm{polylog} d$, where constant $C(q)$ only depends on the degree of $\\sigma_*$, regardless of information exponent; this dimension dependence matches the information theoretic limit up to polylogarithmic factors. Core to our analysis is the reuse of minibatch in the gradient computation, which gives rise to higher-order information beyond correlational queries.","sentences":["We study the problem of gradient descent learning of a single-index target function $f_*(\\boldsymbol{x}) = \\textstyle\\sigma_*\\left(\\langle\\boldsymbol{x},\\boldsymbol{\\theta}\\rangle\\right)$ under isotropic Gaussian data in $\\mathbb{R}^d$, where the link function $\\sigma_*:\\mathbb{R}\\to\\mathbb{R}$ is an unknown degree $q$ polynomial with information exponent $p$ (defined as the lowest degree in the Hermite expansion).","Prior works showed that gradient-based training of neural networks can learn this target with $n\\gtrsim d^{\\Theta(p)}$ samples, and such statistical complexity is predicted to be necessary by the correlational statistical query lower bound.","Surprisingly, we prove that a two-layer neural network optimized by an SGD-based algorithm learns $f_*$ of arbitrary polynomial link function with a sample and runtime complexity of $n \\asymp T \\asymp C(q)","\\cdot d\\mathrm{polylog} d$, where constant $C(q)$ only depends on the degree of $\\sigma_*$, regardless of information exponent; this dimension dependence matches the information theoretic limit up to polylogarithmic factors.","Core to our analysis is the reuse of minibatch in the gradient computation, which gives rise to higher-order information beyond correlational queries."],"url":"http://arxiv.org/abs/2406.01581v1","category":"cs.LG"}
{"created":"2024-06-03 17:56:36","title":"Tetrahedron Splatting for 3D Generation","abstract":"3D representation is essential to the significant advance of 3D generation with 2D diffusion priors. As a flexible representation, NeRF has been first adopted for 3D representation. With density-based volumetric rendering, it however suffers both intensive computational overhead and inaccurate mesh extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows for precise mesh extraction and real-time rendering but is limited in handling large topological changes in meshes, leading to optimization challenges. Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and rendering efficiency while falling short in mesh extraction. In this work, we introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting), that supports easy convergence during optimization, precise mesh extraction, and real-time rendering simultaneously. This is achieved by integrating surface-based volumetric rendering within a structured tetrahedral grid while preserving the desired ability of precise mesh extraction, and a tile-based differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and normal consistency regularization terms for the signed distance field to improve generation quality and stability. Critically, our representation can be trained without mesh extraction, making the optimization process easier to converge. Our TeT-Splatting can be readily integrated in existing 3D generation pipelines, along with polygonal mesh for texture optimization. Extensive experiments show that our TeT-Splatting strikes a superior tradeoff among convergence speed, render efficiency, and mesh quality as compared to previous alternatives under varying 3D generation settings.","sentences":["3D representation is essential to the significant advance of 3D generation with 2D diffusion priors.","As a flexible representation, NeRF has been first adopted for 3D representation.","With density-based volumetric rendering, it however suffers both intensive computational overhead and inaccurate mesh extraction.","Using a signed distance field and Marching Tetrahedra, DMTet allows for precise mesh extraction and real-time rendering but is limited in handling large topological changes in meshes, leading to optimization challenges.","Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and rendering efficiency while falling short in mesh extraction.","In this work, we introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting), that supports easy convergence during optimization, precise mesh extraction, and real-time rendering simultaneously.","This is achieved by integrating surface-based volumetric rendering within a structured tetrahedral grid while preserving the desired ability of precise mesh extraction, and a tile-based differentiable tetrahedron rasterizer.","Furthermore, we incorporate eikonal and normal consistency regularization terms for the signed distance field to improve generation quality and stability.","Critically, our representation can be trained without mesh extraction, making the optimization process easier to converge.","Our TeT-Splatting can be readily integrated in existing 3D generation pipelines, along with polygonal mesh for texture optimization.","Extensive experiments show that our TeT-Splatting strikes a superior tradeoff among convergence speed, render efficiency, and mesh quality as compared to previous alternatives under varying 3D generation settings."],"url":"http://arxiv.org/abs/2406.01579v1","category":"cs.CV"}
{"created":"2024-06-03 17:51:36","title":"Quantum many-body spin ratchets","abstract":"Introducing a class of SU(2) invariant quantum unitary circuits generating chiral transport, we examine the role of broken space-reflection and time-reversal symmetries on spin transport properties. Upon adjusting parameters of local unitary gates, the dynamics can be either chaotic or integrable. The latter corresponds to a generalization of the space-time discretized (Trotterized) higher-spin quantum Heisenberg chain. We demonstrate that breaking of space-reflection symmetry results in a drift in the dynamical spin susceptibility. Remarkably, we find a universal drift velocity given by a simple formula which, at zero average magnetization, depends only on the values of SU(2) Casimir invariants associated with local spins. In the integrable case, the drift velocity formula is confirmed analytically based on the exact solution of thermodynamic Bethe ansatz equations. Finally, by inspecting the large fluctuations of the time-integrated current between two halves of the system in stationary maximum-entropy states, we demonstrate violation of the Gallavotti-Cohen symmetry, implying that such states cannot be regarded as equilibrium ones. We show that the scaled cumulant generating function of the time-integrated current instead obeys a generalized fluctuation relation.","sentences":["Introducing a class of SU(2) invariant quantum unitary circuits generating chiral transport, we examine the role of broken space-reflection and time-reversal symmetries on spin transport properties.","Upon adjusting parameters of local unitary gates, the dynamics can be either chaotic or integrable.","The latter corresponds to a generalization of the space-time discretized (Trotterized) higher-spin quantum Heisenberg chain.","We demonstrate that breaking of space-reflection symmetry results in a drift in the dynamical spin susceptibility.","Remarkably, we find a universal drift velocity given by a simple formula which, at zero average magnetization, depends only on the values of SU(2) Casimir invariants associated with local spins.","In the integrable case, the drift velocity formula is confirmed analytically based on the exact solution of thermodynamic Bethe ansatz equations.","Finally, by inspecting the large fluctuations of the time-integrated current between two halves of the system in stationary maximum-entropy states, we demonstrate violation of the Gallavotti-Cohen symmetry, implying that such states cannot be regarded as equilibrium ones.","We show that the scaled cumulant generating function of the time-integrated current instead obeys a generalized fluctuation relation."],"url":"http://arxiv.org/abs/2406.01571v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-03 17:51:02","title":"Regularity for the fractional $p$-Laplace equation","abstract":"Higher Sobolev and H\\\"older regularity is studied for local weak solutions of the fractional $p$-Laplace equation of order $s$ in the case $p\\ge 2$. Depending on the regime considered, i.e. $$0<s\\le\\tfrac{p-2}{p}\\quad \\text{or} \\quad\\tfrac{p-2}{p}<s<1,$$ precise local estimates are proven. The relevant estimates are stable if the fractional order $s$ reaches $1$; the known Sobolev regularity estimates for the local $p$-Laplace are recovered. The case $p=2$ reproduces the almost $W^{1+s,2}_{\\rm loc}$-regularity for the fractional Laplace equation of any order $s\\in(0,1)$.","sentences":["Higher Sobolev and H\\\"older regularity is studied for local weak solutions of the fractional $p$-Laplace equation of order $s$ in the case $p\\ge 2$.","Depending on the regime considered, i.e. $$0<s\\le\\tfrac{p-2}{p}\\quad \\text{or} \\quad\\tfrac{p-2}{p}<s<1,$$ precise local estimates are proven.","The relevant estimates are stable if the fractional order $s$ reaches $1$; the known Sobolev regularity estimates for the local $p$-Laplace are recovered.","The case $p=2$ reproduces the almost $W^{1+s,2}_{\\rm loc}$-regularity for the fractional Laplace equation of any order $s\\in(0,1)$."],"url":"http://arxiv.org/abs/2406.01568v1","category":"math.AP"}
{"created":"2024-06-03 17:46:33","title":"Extremum Seeking Control for Scalar Maps with Distributed Diffusion PDEs","abstract":"This paper deals with the gradient extremum seeking control for static scalar maps with actuators governed by distributed diffusion partial differential equations (PDEs). To achieve the real-time optimization objective, we design a compensation controller for the distributed diffusion PDE via backstepping transformation in infinite dimensions. A further contribution of this paper is the appropriate motion planning design of the so-called probing (or perturbation) signal, which is more involved than in the non-distributed counterpart. Hence, with these two design ingredients, we provide an averaging-based methodology that can be implemented using the gradient and Hessian estimates. Local exponential stability for the closed-loop equilibrium of the average error dynamics is guaranteed through a Lyapunov-based analysis. By employing the averaging theory for infinite-dimensional systems, we prove that the trajectory converges to a small neighborhood surrounding the optimal point. The effectiveness of the proposed extremum seeking controller for distributed diffusion PDEs in cascade of nonlinear maps to be optimized is illustrated by means of numerical simulations.","sentences":["This paper deals with the gradient extremum seeking control for static scalar maps with actuators governed by distributed diffusion partial differential equations (PDEs).","To achieve the real-time optimization objective, we design a compensation controller for the distributed diffusion PDE via backstepping transformation in infinite dimensions.","A further contribution of this paper is the appropriate motion planning design of the so-called probing (or perturbation) signal, which is more involved than in the non-distributed counterpart.","Hence, with these two design ingredients, we provide an averaging-based methodology that can be implemented using the gradient and Hessian estimates.","Local exponential stability for the closed-loop equilibrium of the average error dynamics is guaranteed through a Lyapunov-based analysis.","By employing the averaging theory for infinite-dimensional systems, we prove that the trajectory converges to a small neighborhood surrounding the optimal point.","The effectiveness of the proposed extremum seeking controller for distributed diffusion PDEs in cascade of nonlinear maps to be optimized is illustrated by means of numerical simulations."],"url":"http://arxiv.org/abs/2406.01564v1","category":"math.OC"}
{"created":"2024-06-04 17:59:25","title":"Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning","abstract":"Training models with longer in-context lengths is a significant challenge for multimodal model due to substantial GPU memory and computational costs. This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently. We present Visualized In-Context Text Processing (VisInContext), which processes long in-context text using visual tokens. This technique significantly reduces GPU memory usage and floating point operations (FLOPs) for both training and inferenceing stage. For instance, our method expands the pre-training in-context text length from 256 to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model. Experimental results demonstrate that model trained with VisInContext delivers superior performance on common downstream benchmarks for in-context few-shot evaluation. Additionally, VisInContext is complementary to existing methods for increasing in-context text length and enhances document understanding capabilities, showing great potential in document QA tasks and sequential document retrieval.","sentences":["Training models with longer in-context lengths is a significant challenge for multimodal model due to substantial GPU memory and computational costs.","This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently.","We present Visualized In-Context Text Processing (VisInContext), which processes long in-context text using visual tokens.","This technique significantly reduces GPU memory usage and floating point operations (FLOPs) for both training and inferenceing stage.","For instance, our method expands the pre-training in-context text length from 256 to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model.","Experimental results demonstrate that model trained with VisInContext delivers superior performance on common downstream benchmarks for in-context few-shot evaluation.","Additionally, VisInContext is complementary to existing methods for increasing in-context text length and enhances document understanding capabilities, showing great potential in document QA tasks and sequential document retrieval."],"url":"http://arxiv.org/abs/2406.02547v1","category":"cs.CV"}
{"created":"2024-06-04 17:58:03","title":"Loki: Low-Rank Keys for Efficient Sparse Attention","abstract":"Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.","sentences":["Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used.","In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference.","In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block.","Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models.","Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space.","Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs."],"url":"http://arxiv.org/abs/2406.02542v1","category":"cs.LG"}
{"created":"2024-06-04 17:24:19","title":"An Open-Source Tool for Mapping War Destruction at Scale in Ukraine using Sentinel-1 Time Series","abstract":"Access to detailed war impact assessments is crucial for humanitarian organizations to effectively assist populations most affected by armed conflicts. However, maintaining a comprehensive understanding of the situation on the ground is challenging, especially in conflicts that cover vast territories and extend over long periods. This study presents a scalable and transferable method for estimating war-induced damage to buildings. We first train a machine learning model to output pixel-wise probability of destruction from Synthetic Aperture Radar (SAR) satellite image time series, leveraging existing, manual damage assessments as ground truth and cloud-based geospatial analysis tools for large-scale inference. We further post-process these assessments using open building footprints to obtain a final damage estimate per building. We introduce an accessible, open-source tool that allows users to adjust the confidence interval based on their specific requirements and use cases. Our approach enables humanitarian organizations and other actors to rapidly screen large geographic regions for war impacts. We provide two publicly accessible dashboards: a Ukraine Damage Explorer to dynamically view our pre-computed estimates, and a Rapid Damage Mapping Tool to easily run our method and produce custom maps.","sentences":["Access to detailed war impact assessments is crucial for humanitarian organizations to effectively assist populations most affected by armed conflicts.","However, maintaining a comprehensive understanding of the situation on the ground is challenging, especially in conflicts that cover vast territories and extend over long periods.","This study presents a scalable and transferable method for estimating war-induced damage to buildings.","We first train a machine learning model to output pixel-wise probability of destruction from Synthetic Aperture Radar (SAR) satellite image time series, leveraging existing, manual damage assessments as ground truth and cloud-based geospatial analysis tools for large-scale inference.","We further post-process these assessments using open building footprints to obtain a final damage estimate per building.","We introduce an accessible, open-source tool that allows users to adjust the confidence interval based on their specific requirements and use cases.","Our approach enables humanitarian organizations and other actors to rapidly screen large geographic regions for war impacts.","We provide two publicly accessible dashboards: a Ukraine Damage Explorer to dynamically view our pre-computed estimates, and a Rapid Damage Mapping Tool to easily run our method and produce custom maps."],"url":"http://arxiv.org/abs/2406.02506v1","category":"cs.CV"}
{"created":"2024-06-04 15:50:42","title":"Coresets for Multiple $\\ell_p$ Regression","abstract":"A coreset of a dataset with $n$ examples and $d$ features is a weighted subset of examples that is sufficient for solving downstream data analytic tasks. Nearly optimal constructions of coresets for least squares and $\\ell_p$ linear regression with a single response are known in prior work. However, for multiple $\\ell_p$ regression where there can be $m$ responses, there are no known constructions with size sublinear in $m$. In this work, we construct coresets of size $\\tilde O(\\varepsilon^{-2}d)$ for $p<2$ and $\\tilde O(\\varepsilon^{-p}d^{p/2})$ for $p>2$ independently of $m$ (i.e., dimension-free) that approximate the multiple $\\ell_p$ regression objective at every point in the domain up to $(1\\pm\\varepsilon)$ relative error. If we only need to preserve the minimizer subject to a subspace constraint, we improve these bounds by an $\\varepsilon$ factor for all $p>1$. All of our bounds are nearly tight.   We give two application of our results. First, we settle the number of uniform samples needed to approximate $\\ell_p$ Euclidean power means up to a $(1+\\varepsilon)$ factor, showing that $\\tilde\\Theta(\\varepsilon^{-2})$ samples for $p = 1$, $\\tilde\\Theta(\\varepsilon^{-1})$ samples for $1 < p < 2$, and $\\tilde\\Theta(\\varepsilon^{1-p})$ samples for $p>2$ is tight, answering a question of Cohen-Addad, Saulpic, and Schwiegelshohn. Second, we show that for $1<p<2$, every matrix has a subset of $\\tilde O(\\varepsilon^{-1}k)$ rows which spans a $(1+\\varepsilon)$-approximately optimal $k$-dimensional subspace for $\\ell_p$ subspace approximation, which is also nearly optimal.","sentences":["A coreset of a dataset with $n$ examples and $d$ features is a weighted subset of examples that is sufficient for solving downstream data analytic tasks.","Nearly optimal constructions of coresets for least squares and $\\ell_p$ linear regression with a single response are known in prior work.","However, for multiple $\\ell_p$ regression where there can be $m$ responses, there are no known constructions with size sublinear in $m$. In this work, we construct coresets of size $\\tilde O(\\varepsilon^{-2}d)$ for $p<2$ and $\\tilde O(\\varepsilon^{-p}d^{p/2})$ for $p>2$ independently of $m$ (i.e., dimension-free) that approximate the multiple $\\ell_p$ regression objective at every point in the domain up to $(1\\pm\\varepsilon)$ relative error.","If we only need to preserve the minimizer subject to a subspace constraint, we improve these bounds by an $\\varepsilon$ factor for all $p>1$. All of our bounds are nearly tight.   ","We give two application of our results.","First, we settle the number of uniform samples needed to approximate $\\ell_p$ Euclidean power means up to a $(1+\\varepsilon)$ factor, showing that $\\tilde\\Theta(\\varepsilon^{-2})$ samples for $p = 1$, $\\tilde\\Theta(\\varepsilon^{-1})$ samples for $1 < p < 2$, and $\\tilde\\Theta(\\varepsilon^{1-p})$ samples for $p>2$ is tight, answering a question of Cohen-Addad, Saulpic, and Schwiegelshohn.","Second, we show that for $1<p<2$, every matrix has a subset of $\\tilde O(\\varepsilon^{-1}k)$ rows which spans a $(1+\\varepsilon)$-approximately optimal $k$-dimensional subspace for $\\ell_p$ subspace approximation, which is also nearly optimal."],"url":"http://arxiv.org/abs/2406.02432v1","category":"cs.DS"}
{"created":"2024-06-04 15:39:08","title":"Representing Piecewise-Linear Functions by Functions with Minimal Arity","abstract":"Any continuous piecewise-linear function $F\\colon \\mathbb{R}^{n}\\to \\mathbb{R}$ can be represented as a linear combination of $\\max$ functions of at most $n+1$ affine-linear functions. In our previous paper [``Representing piecewise linear functions by functions with small arity'', AAECC, 2023], we showed that this upper bound of $n+1$ arguments is tight. In the present paper, we extend this result by establishing a correspondence between the function $F$ and the minimal number of arguments that are needed in any such decomposition. We show that the tessellation of the input space $\\mathbb{R}^{n}$ induced by the function $F$ has a direct connection to the number of arguments in the $\\max$ functions.","sentences":["Any continuous piecewise-linear function $F\\colon \\mathbb{R}^{n}\\to \\mathbb{R}$ can be represented as a linear combination of $\\max$ functions of at most $n+1$ affine-linear functions.","In our previous paper [``Representing piecewise linear functions by functions with small arity'', AAECC, 2023], we showed that this upper bound of $n+1$ arguments is tight.","In the present paper, we extend this result by establishing a correspondence between the function $F$ and the minimal number of arguments that are needed in any such decomposition.","We show that the tessellation of the input space $\\mathbb{R}^{n}$ induced by the function $F$ has a direct connection to the number of arguments in the $\\max$ functions."],"url":"http://arxiv.org/abs/2406.02421v1","category":"cs.DM"}
{"created":"2024-06-04 14:49:07","title":"Query-based Semantic Gaussian Field for Scene Representation in Reinforcement Learning","abstract":"Latent scene representation plays a significant role in training reinforcement learning (RL) agents. To obtain good latent vectors describing the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF pipeline into scene representation learning. However, these NeRF-related methods struggle to perceive 3D structural information due to the inefficient dense sampling in volumetric rendering. Moreover, they lack fine-grained semantic information included in their scene representation vectors because they evenly consider free and occupied spaces. Both of them can destroy the performance of downstream RL tasks. To address the above challenges, we propose a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to learn 3D scene representation for the first time. In brief, we present the Query-based Generalizable 3DGS to bridge the 3DGS technique and scene representations with more geometrical awareness than those in NeRFs. Moreover, we present the Hierarchical Semantics Encoding to ground the fine-grained semantic features to 3D Gaussians and further distilled to the scene representation vectors. We conduct extensive experiments on two RL platforms including Maniskill2 and Robomimic across 10 different tasks. The results show that our method outperforms the other 5 baselines by a large margin. We achieve the best success rates on 8 tasks and the second-best on the other two tasks.","sentences":["Latent scene representation plays a significant role in training reinforcement learning (RL) agents.","To obtain good latent vectors describing the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF pipeline into scene representation learning.","However, these NeRF-related methods struggle to perceive 3D structural information due to the inefficient dense sampling in volumetric rendering.","Moreover, they lack fine-grained semantic information included in their scene representation vectors because they evenly consider free and occupied spaces.","Both of them can destroy the performance of downstream RL tasks.","To address the above challenges, we propose a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to learn 3D scene representation for the first time.","In brief, we present the Query-based Generalizable 3DGS to bridge the 3DGS technique and scene representations with more geometrical awareness than those in NeRFs.","Moreover, we present the Hierarchical Semantics Encoding to ground the fine-grained semantic features to 3D Gaussians and further distilled to the scene representation vectors.","We conduct extensive experiments on two RL platforms including Maniskill2 and Robomimic across 10 different tasks.","The results show that our method outperforms the other 5 baselines by a large margin.","We achieve the best success rates on 8 tasks and the second-best on the other two tasks."],"url":"http://arxiv.org/abs/2406.02370v1","category":"cs.RO"}
{"created":"2024-06-04 14:19:50","title":"Cluster-Aware Similarity Diffusion for Instance Retrieval","abstract":"Diffusion-based re-ranking is a common method used for retrieving instances by performing similarity propagation in a nearest neighbor graph. However, existing techniques that construct the affinity graph based on pairwise instances can lead to the propagation of misinformation from outliers and other manifolds, resulting in inaccurate results. To overcome this issue, we propose a novel Cluster-Aware Similarity (CAS) diffusion for instance retrieval. The primary concept of CAS is to conduct similarity diffusion within local clusters, which can reduce the influence from other manifolds explicitly. To obtain a symmetrical and smooth similarity matrix, our Bidirectional Similarity Diffusion strategy introduces an inverse constraint term to the optimization objective of local cluster diffusion. Additionally, we have optimized a Neighbor-guided Similarity Smoothing approach to ensure similarity consistency among the local neighbors of each instance. Evaluations in instance retrieval and object re-identification validate the effectiveness of the proposed CAS, our code is publicly available.","sentences":["Diffusion-based re-ranking is a common method used for retrieving instances by performing similarity propagation in a nearest neighbor graph.","However, existing techniques that construct the affinity graph based on pairwise instances can lead to the propagation of misinformation from outliers and other manifolds, resulting in inaccurate results.","To overcome this issue, we propose a novel Cluster-Aware Similarity (CAS) diffusion for instance retrieval.","The primary concept of CAS is to conduct similarity diffusion within local clusters, which can reduce the influence from other manifolds explicitly.","To obtain a symmetrical and smooth similarity matrix, our Bidirectional Similarity Diffusion strategy introduces an inverse constraint term to the optimization objective of local cluster diffusion.","Additionally, we have optimized a Neighbor-guided Similarity Smoothing approach to ensure similarity consistency among the local neighbors of each instance.","Evaluations in instance retrieval and object re-identification validate the effectiveness of the proposed CAS, our code is publicly available."],"url":"http://arxiv.org/abs/2406.02343v1","category":"cs.LG"}
{"created":"2024-06-04 14:18:01","title":"QCDGE database, Quantum Chemistry Database with Ground- and Excited-state Properties of 450 Kilo Molecules","abstract":"Due to rapid advancements in deep learning techniques, the demand for large-volume high-quality databases grows significantly in chemical research. We developed a quantum-chemistry database that includes 443,106 small organic molecules with sizes up to 10 heavy atoms including carbon (C), nitrogen (N), oxygen (O), and fluorine (F). Ground-state geometry optimizations and frequency calculations of all compounds were performed at the B3LYP/6-31G* level with the BJD3 dispersion correction, while the excited-state single-point calculations were conducted at the $\\omega$B97X-D/6-31G* level. Totally twenty seven molecular properties, such as geometric, thermodynamic, electronic and energetic properties, were gathered from these calculations. Meanwhile, we also established a comprehensive protocol for the construction of a high-volume quantum-chemistry database. Our QCDGE (Quantum Chemistry Database with Ground- and Excited-State Properties) database contains a substantial volume of data, exhibits high chemical diversity, and most importantly includes excited-state information. This database, along with its construction protocol, is expected to have a significant impact on the broad applications of machine learning studies across different fields of chemistry, especially in the area of excited-state research.","sentences":["Due to rapid advancements in deep learning techniques, the demand for large-volume high-quality databases grows significantly in chemical research.","We developed a quantum-chemistry database that includes 443,106 small organic molecules with sizes up to 10 heavy atoms including carbon (C), nitrogen (N), oxygen (O), and fluorine (F).","Ground-state geometry optimizations and frequency calculations of all compounds were performed at the B3LYP/6-31G* level with the BJD3 dispersion correction, while the excited-state single-point calculations were conducted at the $\\omega$B97X-D/6-31G* level.","Totally twenty seven molecular properties, such as geometric, thermodynamic, electronic and energetic properties, were gathered from these calculations.","Meanwhile, we also established a comprehensive protocol for the construction of a high-volume quantum-chemistry database.","Our QCDGE (Quantum Chemistry Database with Ground- and Excited-State Properties) database contains a substantial volume of data, exhibits high chemical diversity, and most importantly includes excited-state information.","This database, along with its construction protocol, is expected to have a significant impact on the broad applications of machine learning studies across different fields of chemistry, especially in the area of excited-state research."],"url":"http://arxiv.org/abs/2406.02341v1","category":"physics.chem-ph"}
{"created":"2024-06-04 13:58:28","title":"On Affine Homotopy between Language Encoders","abstract":"Pre-trained language encoders -- functions that represent text as vectors -- are an integral component of many NLP tasks. We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that a faithful measure of similarity needs to be \\emph{intrinsic}, that is, task-independent, yet still be informative of \\emph{extrinsic} similarity -- the performance on downstream tasks. It is common to consider two encoders similar if they are \\emph{homotopic}, i.e., if they can be aligned through some transformation. In this spirit, we study the properties of \\emph{affine} alignment of language encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them.","sentences":["Pre-trained language encoders -- functions that represent text as vectors -- are an integral component of many NLP tasks.","We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar?","We contend that a faithful measure of similarity needs to be \\emph{intrinsic}, that is, task-independent, yet still be informative of \\emph{extrinsic} similarity -- the performance on downstream tasks.","It is common to consider two encoders similar if they are \\emph{homotopic}, i.e., if they can be aligned through some transformation.","In this spirit, we study the properties of \\emph{affine} alignment of language encoders and its implications on extrinsic similarity.","We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity.","We confirm this on datasets of natural language representations.","Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them."],"url":"http://arxiv.org/abs/2406.02329v1","category":"cs.CL"}
{"created":"2024-06-04 13:57:34","title":"Continual Unsupervised Out-of-Distribution Detection","abstract":"Deep learning models excel when the data distribution during training aligns with testing data. Yet, their performance diminishes when faced with out-of-distribution (OOD) samples, leading to great interest in the field of OOD detection. Current approaches typically assume that OOD samples originate from an unconcentrated distribution complementary to the training distribution. While this assumption is appropriate in the traditional unsupervised OOD (U-OOD) setting, it proves inadequate when considering the place of deployment of the underlying deep learning model. To better reflect this real-world scenario, we introduce the novel setting of continual U-OOD detection. To tackle this new setting, we propose a method that starts from a U-OOD detector, which is agnostic to the OOD distribution, and slowly updates during deployment to account for the actual OOD distribution. Our method uses a new U-OOD scoring function that combines the Mahalanobis distance with a nearest-neighbor approach. Furthermore, we design a confidence-scaled few-shot OOD detector that outperforms previous methods. We show our method greatly improves upon strong baselines from related fields.","sentences":["Deep learning models excel when the data distribution during training aligns with testing data.","Yet, their performance diminishes when faced with out-of-distribution (OOD) samples, leading to great interest in the field of OOD detection.","Current approaches typically assume that OOD samples originate from an unconcentrated distribution complementary to the training distribution.","While this assumption is appropriate in the traditional unsupervised OOD (U-OOD) setting, it proves inadequate when considering the place of deployment of the underlying deep learning model.","To better reflect this real-world scenario, we introduce the novel setting of continual U-OOD detection.","To tackle this new setting, we propose a method that starts from a U-OOD detector, which is agnostic to the OOD distribution, and slowly updates during deployment to account for the actual OOD distribution.","Our method uses a new U-OOD scoring function that combines the Mahalanobis distance with a nearest-neighbor approach.","Furthermore, we design a confidence-scaled few-shot OOD detector that outperforms previous methods.","We show our method greatly improves upon strong baselines from related fields."],"url":"http://arxiv.org/abs/2406.02327v1","category":"cs.CV"}
{"created":"2024-06-04 13:17:24","title":"Learning-Rate-Free Stochastic Optimization over Riemannian Manifolds","abstract":"In recent years, interest in gradient-based optimization over Riemannian manifolds has surged. However, a significant challenge lies in the reliance on hyperparameters, especially the learning rate, which requires meticulous tuning by practitioners to ensure convergence at a suitable rate. In this work, we introduce innovative learning-rate-free algorithms for stochastic optimization over Riemannian manifolds, eliminating the need for hand-tuning and providing a more robust and user-friendly approach. We establish high probability convergence guarantees that are optimal, up to logarithmic factors, compared to the best-known optimally tuned rate in the deterministic setting. Our approach is validated through numerical experiments, demonstrating competitive performance against learning-rate-dependent algorithms.","sentences":["In recent years, interest in gradient-based optimization over Riemannian manifolds has surged.","However, a significant challenge lies in the reliance on hyperparameters, especially the learning rate, which requires meticulous tuning by practitioners to ensure convergence at a suitable rate.","In this work, we introduce innovative learning-rate-free algorithms for stochastic optimization over Riemannian manifolds, eliminating the need for hand-tuning and providing a more robust and user-friendly approach.","We establish high probability convergence guarantees that are optimal, up to logarithmic factors, compared to the best-known optimally tuned rate in the deterministic setting.","Our approach is validated through numerical experiments, demonstrating competitive performance against learning-rate-dependent algorithms."],"url":"http://arxiv.org/abs/2406.02296v1","category":"cs.LG"}
{"created":"2024-06-04 12:47:11","title":"Analyzing the Benefits of Prototypes for Semi-Supervised Category Learning","abstract":"Categories can be represented at different levels of abstraction, from prototypes focused on the most typical members to remembering all observed exemplars of the category. These representations have been explored in the context of supervised learning, where stimuli are presented with known category labels. We examine the benefits of prototype-based representations in a less-studied domain: semi-supervised learning, where agents must form unsupervised representations of stimuli before receiving category labels. We study this problem in a Bayesian unsupervised learning model called a variational auto-encoder, and we draw on recent advances in machine learning to implement a prior that encourages the model to use abstract prototypes to represent data. We apply this approach to image datasets and show that forming prototypes can improve semi-supervised category learning. Additionally, we study the latent embeddings of the models and show that these prototypes allow the models to form clustered representations without supervision, contributing to their success in downstream categorization performance.","sentences":["Categories can be represented at different levels of abstraction, from prototypes focused on the most typical members to remembering all observed exemplars of the category.","These representations have been explored in the context of supervised learning, where stimuli are presented with known category labels.","We examine the benefits of prototype-based representations in a less-studied domain: semi-supervised learning, where agents must form unsupervised representations of stimuli before receiving category labels.","We study this problem in a Bayesian unsupervised learning model called a variational auto-encoder, and we draw on recent advances in machine learning to implement a prior that encourages the model to use abstract prototypes to represent data.","We apply this approach to image datasets and show that forming prototypes can improve semi-supervised category learning.","Additionally, we study the latent embeddings of the models and show that these prototypes allow the models to form clustered representations without supervision, contributing to their success in downstream categorization performance."],"url":"http://arxiv.org/abs/2406.02268v1","category":"cs.LG"}
{"created":"2024-06-04 12:18:14","title":"Exploring the Efficiency of Renewable Energy-based Modular Data Centers at Scale","abstract":"Modular data centers (MDCs) that can be placed right at the energy farms and powered mostly by renewable energy, are proven to be a flexible and effective approach to lowering the carbon footprint of data centers. However, the main challenge of using renewable energy is the high variability of power produced, which implies large volatility in powering computing resources at MDCs, and degraded application performance due to the task evictions and migrations. This causes challenges for platform operators to decide the MDC deployment. To this end, we present SkyBox, a framework that employs a holistic and learning-based approach for platform operators to explore the efficient use of renewable energy with MDC deployment across geographical regions. SkyBox is driven by the insights based on our study of real-world power traces from a variety of renewable energy farms -- the predictable production of renewable energy and the complementary nature of energy production patterns across different renewable energy sources and locations. With these insights, SkyBox first uses the coefficient of variation metric to select the qualified renewable farms, and proposes a subgraph identification algorithm to identify a set of farms with complementary energy production patterns. After that, SkyBox enables smart workload placement and migrations to further tolerate the power variability. Our experiments with real power traces and datacenter workloads show that SkyBox has the lowest carbon emissions in comparison with current MDC deployment approaches. SkyBox also minimizes the impact of the power variability on cloud virtual machines, enabling rMDCs a practical solution of efficiently using renewable energy.","sentences":["Modular data centers (MDCs) that can be placed right at the energy farms and powered mostly by renewable energy, are proven to be a flexible and effective approach to lowering the carbon footprint of data centers.","However, the main challenge of using renewable energy is the high variability of power produced, which implies large volatility in powering computing resources at MDCs, and degraded application performance due to the task evictions and migrations.","This causes challenges for platform operators to decide the MDC deployment.","To this end, we present SkyBox, a framework that employs a holistic and learning-based approach for platform operators to explore the efficient use of renewable energy with MDC deployment across geographical regions.","SkyBox is driven by the insights based on our study of real-world power traces from a variety of renewable energy farms -- the predictable production of renewable energy and the complementary nature of energy production patterns across different renewable energy sources and locations.","With these insights, SkyBox first uses the coefficient of variation metric to select the qualified renewable farms, and proposes a subgraph identification algorithm to identify a set of farms with complementary energy production patterns.","After that, SkyBox enables smart workload placement and migrations to further tolerate the power variability.","Our experiments with real power traces and datacenter workloads show that SkyBox has the lowest carbon emissions in comparison with current MDC deployment approaches.","SkyBox also minimizes the impact of the power variability on cloud virtual machines, enabling rMDCs a practical solution of efficiently using renewable energy."],"url":"http://arxiv.org/abs/2406.02252v1","category":"cs.DC"}
{"created":"2024-06-04 11:33:40","title":"SMCL: Saliency Masked Contrastive Learning for Long-tailed Recognition","abstract":"Real-world data often follow a long-tailed distribution with a high imbalance in the number of samples between classes. The problem with training from imbalanced data is that some background features, common to all classes, can be unobserved in classes with scarce samples. As a result, this background correlates to biased predictions into ``major\" classes. In this paper, we propose saliency masked contrastive learning, a new method that uses saliency masking and contrastive learning to mitigate the problem and improve the generalizability of a model. Our key idea is to mask the important part of an image using saliency detection and use contrastive learning to move the masked image towards minor classes in the feature space, so that background features present in the masked image are no longer correlated with the original class. Experiment results show that our method achieves state-of-the-art level performance on benchmark long-tailed datasets.","sentences":["Real-world data often follow a long-tailed distribution with a high imbalance in the number of samples between classes.","The problem with training from imbalanced data is that some background features, common to all classes, can be unobserved in classes with scarce samples.","As a result, this background correlates to biased predictions into ``major\" classes.","In this paper, we propose saliency masked contrastive learning, a new method that uses saliency masking and contrastive learning to mitigate the problem and improve the generalizability of a model.","Our key idea is to mask the important part of an image using saliency detection and use contrastive learning to move the masked image towards minor classes in the feature space, so that background features present in the masked image are no longer correlated with the original class.","Experiment results show that our method achieves state-of-the-art level performance on benchmark long-tailed datasets."],"url":"http://arxiv.org/abs/2406.02223v1","category":"cs.CV"}
{"created":"2024-06-04 09:45:04","title":"Radar Spectra-Language Model for Automotive Scene Parsing","abstract":"Radar sensors are low cost, long-range, and weather-resilient. Therefore, they are widely used for driver assistance functions, and are expected to be crucial for the success of autonomous driving in the future. In many perception tasks only pre-processed radar point clouds are considered. In contrast, radar spectra are a raw form of radar measurements and contain more information than radar point clouds. However, radar spectra are rather difficult to interpret. In this work, we aim to explore the semantic information contained in spectra in the context of automated driving, thereby moving towards better interpretability of radar spectra. To this end, we create a radar spectra-language model, allowing us to query radar spectra measurements for the presence of scene elements using free text. We overcome the scarcity of radar spectra data by matching the embedding space of an existing vision-language model (VLM). Finally, we explore the benefit of the learned representation for scene parsing, and obtain improvements in free space segmentation and object detection merely by injecting the spectra embedding into a baseline model.","sentences":["Radar sensors are low cost, long-range, and weather-resilient.","Therefore, they are widely used for driver assistance functions, and are expected to be crucial for the success of autonomous driving in the future.","In many perception tasks only pre-processed radar point clouds are considered.","In contrast, radar spectra are a raw form of radar measurements and contain more information than radar point clouds.","However, radar spectra are rather difficult to interpret.","In this work, we aim to explore the semantic information contained in spectra in the context of automated driving, thereby moving towards better interpretability of radar spectra.","To this end, we create a radar spectra-language model, allowing us to query radar spectra measurements for the presence of scene elements using free text.","We overcome the scarcity of radar spectra data by matching the embedding space of an existing vision-language model (VLM).","Finally, we explore the benefit of the learned representation for scene parsing, and obtain improvements in free space segmentation and object detection merely by injecting the spectra embedding into a baseline model."],"url":"http://arxiv.org/abs/2406.02158v1","category":"cs.CV"}
{"created":"2024-06-04 09:21:31","title":"SimulTron: On-Device Simultaneous Speech to Speech Translation","abstract":"Simultaneous speech-to-speech translation (S2ST) holds the promise of breaking down communication barriers and enabling fluid conversations across languages. However, achieving accurate, real-time translation through mobile devices remains a major challenge. We introduce SimulTron, a novel S2ST architecture designed to tackle this task. SimulTron is a lightweight direct S2ST model that uses the strengths of the Translatotron framework while incorporating key modifications for streaming operation, and an adjustable fixed delay. Our experiments show that SimulTron surpasses Translatotron 2 in offline evaluations. Furthermore, real-time evaluations reveal that SimulTron improves upon the performance achieved by Translatotron 1. Additionally, SimulTron achieves superior BLEU scores and latency compared to previous real-time S2ST method on the MuST-C dataset. Significantly, we have successfully deployed SimulTron on a Pixel 7 Pro device, show its potential for simultaneous S2ST on-device.","sentences":["Simultaneous speech-to-speech translation (S2ST) holds the promise of breaking down communication barriers and enabling fluid conversations across languages.","However, achieving accurate, real-time translation through mobile devices remains a major challenge.","We introduce SimulTron, a novel S2ST architecture designed to tackle this task.","SimulTron is a lightweight direct S2ST model that uses the strengths of the Translatotron framework while incorporating key modifications for streaming operation, and an adjustable fixed delay.","Our experiments show that SimulTron surpasses Translatotron 2 in offline evaluations.","Furthermore, real-time evaluations reveal that SimulTron improves upon the performance achieved by Translatotron 1.","Additionally, SimulTron achieves superior BLEU scores and latency compared to previous real-time S2ST method on the MuST-C dataset.","Significantly, we have successfully deployed SimulTron on a Pixel 7 Pro device, show its potential for simultaneous S2ST on-device."],"url":"http://arxiv.org/abs/2406.02133v1","category":"eess.AS"}
{"created":"2024-06-04 07:49:30","title":"Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models","abstract":"Molecule synthesis through machine learning is one of the fundamental problems in drug discovery. Current data-driven strategies employ one-step retrosynthesis models and search algorithms to predict synthetic routes in a top-bottom manner. Despite their effective performance, these strategies face limitations in the molecule synthetic route generation due to a greedy selection of the next molecule set without any lookahead. Furthermore, existing strategies cannot control the generation of synthetic routes based on possible criteria such as material costs, yields, and step count. In this work, we propose a general and principled framework via conditional residual energy-based models (EBMs), that focus on the quality of the entire synthetic route based on the specific criteria. By incorporating an additional energy-based function into our probabilistic model, our proposed algorithm can enhance the quality of the most probable synthetic routes (with higher probabilities) generated by various strategies in a plug-and-play fashion. Extensive experiments demonstrate that our framework can consistently boost performance across various strategies and outperforms previous state-of-the-art top-1 accuracy by a margin of 2.5%. Code is available at https://github.com/SongtaoLiu0823/CREBM.","sentences":["Molecule synthesis through machine learning is one of the fundamental problems in drug discovery.","Current data-driven strategies employ one-step retrosynthesis models and search algorithms to predict synthetic routes in a top-bottom manner.","Despite their effective performance, these strategies face limitations in the molecule synthetic route generation due to a greedy selection of the next molecule set without any lookahead.","Furthermore, existing strategies cannot control the generation of synthetic routes based on possible criteria such as material costs, yields, and step count.","In this work, we propose a general and principled framework via conditional residual energy-based models (EBMs), that focus on the quality of the entire synthetic route based on the specific criteria.","By incorporating an additional energy-based function into our probabilistic model, our proposed algorithm can enhance the quality of the most probable synthetic routes (with higher probabilities) generated by various strategies in a plug-and-play fashion.","Extensive experiments demonstrate that our framework can consistently boost performance across various strategies and outperforms previous state-of-the-art top-1 accuracy by a margin of 2.5%.","Code is available at https://github.com/SongtaoLiu0823/CREBM."],"url":"http://arxiv.org/abs/2406.02066v1","category":"cs.LG"}
{"created":"2024-06-04 07:45:27","title":"Advancing Generalized Transfer Attack with Initialization Derived Bilevel Optimization and Dynamic Sequence Truncation","abstract":"Transfer attacks generate significant interest for real-world black-box applications by crafting transferable adversarial examples through surrogate models. Whereas, existing works essentially directly optimize the single-level objective w.r.t. the surrogate model, which always leads to poor interpretability of attack mechanism and limited generalization performance over unknown victim models. In this work, we propose the \\textbf{B}il\\textbf{E}vel \\textbf{T}ransfer \\textbf{A}ttac\\textbf{K} (BETAK) framework by establishing an initialization derived bilevel optimization paradigm, which explicitly reformulates the nested constraint relationship between the Upper-Level (UL) pseudo-victim attacker and the Lower-Level (LL) surrogate attacker. Algorithmically, we introduce the Hyper Gradient Response (HGR) estimation as an effective feedback for the transferability over pseudo-victim attackers, and propose the Dynamic Sequence Truncation (DST) technique to dynamically adjust the back-propagation path for HGR and reduce computational overhead simultaneously. Meanwhile, we conduct detailed algorithmic analysis and provide convergence guarantee to support non-convexity of the LL surrogate attacker. Extensive evaluations demonstrate substantial improvement of BETAK (e.g., $\\mathbf{53.41}$\\% increase of attack success rates against IncRes-v$2_{ens}$) against different victims and defense methods in targeted and untargeted attack scenarios. The source code is available at https://github.com/callous-youth/BETAK.","sentences":["Transfer attacks generate significant interest for real-world black-box applications by crafting transferable adversarial examples through surrogate models.","Whereas, existing works essentially directly optimize the single-level objective w.r.t.","the surrogate model, which always leads to poor interpretability of attack mechanism and limited generalization performance over unknown victim models.","In this work, we propose the \\textbf{B}il\\textbf{E}vel \\textbf{T}ransfer \\textbf{A}ttac\\textbf{K} (BETAK) framework by establishing an initialization derived bilevel optimization paradigm, which explicitly reformulates the nested constraint relationship between the Upper-Level (UL) pseudo-victim attacker and the Lower-Level (LL) surrogate attacker.","Algorithmically, we introduce the Hyper Gradient Response (HGR) estimation as an effective feedback for the transferability over pseudo-victim attackers, and propose the Dynamic Sequence Truncation (DST) technique to dynamically adjust the back-propagation path for HGR and reduce computational overhead simultaneously.","Meanwhile, we conduct detailed algorithmic analysis and provide convergence guarantee to support non-convexity of the LL surrogate attacker.","Extensive evaluations demonstrate substantial improvement of BETAK (e.g., $\\mathbf{53.41}$\\% increase of attack success rates against IncRes-v$2_{ens}$) against different victims and defense methods in targeted and untargeted attack scenarios.","The source code is available at https://github.com/callous-youth/BETAK."],"url":"http://arxiv.org/abs/2406.02064v1","category":"cs.LG"}
{"created":"2024-06-04 07:43:04","title":"Graph Adversarial Diffusion Convolution","abstract":"This paper introduces a min-max optimization formulation for the Graph Signal Denoising (GSD) problem. In this formulation, we first maximize the second term of GSD by introducing perturbations to the graph structure based on Laplacian distance and then minimize the overall loss of the GSD. By solving the min-max optimization problem, we derive a new variant of the Graph Diffusion Convolution (GDC) architecture, called Graph Adversarial Diffusion Convolution (GADC). GADC differs from GDC by incorporating an additional term that enhances robustness against adversarial attacks on the graph structure and noise in node features. Moreover, GADC improves the performance of GDC on heterophilic graphs. Extensive experiments demonstrate the effectiveness of GADC across various datasets. Code is available at https://github.com/SongtaoLiu0823/GADC.","sentences":["This paper introduces a min-max optimization formulation for the Graph Signal Denoising (GSD) problem.","In this formulation, we first maximize the second term of GSD by introducing perturbations to the graph structure based on Laplacian distance and then minimize the overall loss of the GSD.","By solving the min-max optimization problem, we derive a new variant of the Graph Diffusion Convolution (GDC) architecture, called Graph Adversarial Diffusion Convolution (GADC).","GADC differs from GDC by incorporating an additional term that enhances robustness against adversarial attacks on the graph structure and noise in node features.","Moreover, GADC improves the performance of GDC on heterophilic graphs.","Extensive experiments demonstrate the effectiveness of GADC across various datasets.","Code is available at https://github.com/SongtaoLiu0823/GADC."],"url":"http://arxiv.org/abs/2406.02059v1","category":"cs.LG"}
{"created":"2024-06-04 07:35:23","title":"PETRA: Parallel End-to-end Training with Reversible Architectures","abstract":"Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling. In this work, we show how reversible architectures can solve challenges in parallelizing deep model training. We introduce PETRA, a novel alternative to backpropagation for parallelizing gradient computations. PETRA facilitates effective model parallelism by enabling stages (i.e., a set of layers) to compute independently on different devices, while only needing to communicate activations and gradients between each other. By decoupling the forward and backward passes and keeping a single updated version of the parameters, the need for weight stashing is also removed. We develop a custom autograd-like training framework for PETRA, and we demonstrate its effectiveness on CIFAR-10, ImageNet32, and ImageNet, achieving competitive accuracies comparable to backpropagation using ResNet-18, ResNet-34, and ResNet-50 models.","sentences":["Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling.","In this work, we show how reversible architectures can solve challenges in parallelizing deep model training.","We introduce PETRA, a novel alternative to backpropagation for parallelizing gradient computations.","PETRA facilitates effective model parallelism by enabling stages (i.e., a set of layers) to compute independently on different devices, while only needing to communicate activations and gradients between each other.","By decoupling the forward and backward passes and keeping a single updated version of the parameters, the need for weight stashing is also removed.","We develop a custom autograd-like training framework for PETRA, and we demonstrate its effectiveness on CIFAR-10, ImageNet32, and ImageNet, achieving competitive accuracies comparable to backpropagation using ResNet-18, ResNet-34, and ResNet-50 models."],"url":"http://arxiv.org/abs/2406.02052v1","category":"cs.LG"}
{"created":"2024-06-04 07:27:36","title":"QROA: A Black-Box Query-Response Optimization Attack on LLMs","abstract":"Large Language Models (LLMs) have surged in popularity in recent months, yet they possess concerning capabilities for generating harmful content when manipulated. This study introduces the Query-Response Optimization Attack (QROA), an optimization-based strategy designed to exploit LLMs through a black-box, query-only interaction. QROA adds an optimized trigger to a malicious instruction to compel the LLM to generate harmful content. Unlike previous approaches, QROA does not require access to the model's logit information or any other internal data and operates solely through the standard query-response interface of LLMs. Inspired by deep Q-learning and Greedy coordinate descent, the method iteratively updates tokens to maximize a designed reward function. We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\\%. We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed. This study demonstrates the feasibility of generating jailbreak attacks against deployed LLMs in the public domain using black-box optimization methods, enabling more comprehensive safety testing of LLMs.","sentences":["Large Language Models (LLMs) have surged in popularity in recent months, yet they possess concerning capabilities for generating harmful content when manipulated.","This study introduces the Query-Response Optimization Attack (QROA), an optimization-based strategy designed to exploit LLMs through a black-box, query-only interaction.","QROA adds an optimized trigger to a malicious instruction to compel the LLM to generate harmful content.","Unlike previous approaches, QROA does not require access to the model's logit information or any other internal data and operates solely through the standard query-response interface of LLMs.","Inspired by deep Q-learning and Greedy coordinate descent, the method iteratively updates tokens to maximize a designed reward function.","We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\\%.","We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed.","This study demonstrates the feasibility of generating jailbreak attacks against deployed LLMs in the public domain using black-box optimization methods, enabling more comprehensive safety testing of LLMs."],"url":"http://arxiv.org/abs/2406.02044v1","category":"cs.CL"}
{"created":"2024-06-04 07:23:41","title":"Leveraging Predicate and Triplet Learning for Scene Graph Generation","abstract":"Scene Graph Generation (SGG) aims to identify entities and predict the relationship triplets \\textit{\\textless subject, predicate, object\\textgreater } in visual scenes. Given the prevalence of large visual variations of subject-object pairs even in the same predicate, it can be quite challenging to model and refine predicate representations directly across such pairs, which is however a common strategy adopted by most existing SGG methods. We observe that visual variations within the identical triplet are relatively small and certain relation cues are shared in the same type of triplet, which can potentially facilitate the relation learning in SGG. Moreover, for the long-tail problem widely studied in SGG task, it is also crucial to deal with the limited types and quantity of triplets in tail predicates. Accordingly, in this paper, we propose a Dual-granularity Relation Modeling (DRM) network to leverage fine-grained triplet cues besides the coarse-grained predicate ones. DRM utilizes contexts and semantics of predicate and triplet with Dual-granularity Constraints, generating compact and balanced representations from two perspectives to facilitate relation recognition. Furthermore, a Dual-granularity Knowledge Transfer (DKT) strategy is introduced to transfer variation from head predicates/triplets to tail ones, aiming to enrich the pattern diversity of tail classes to alleviate the long-tail problem. Extensive experiments demonstrate the effectiveness of our method, which establishes new state-of-the-art performance on Visual Genome, Open Image, and GQA datasets. Our code is available at \\url{https://github.com/jkli1998/DRM}","sentences":["Scene Graph Generation (SGG) aims to identify entities and predict the relationship triplets \\textit{\\textless subject, predicate, object\\textgreater } in visual scenes.","Given the prevalence of large visual variations of subject-object pairs even in the same predicate, it can be quite challenging to model and refine predicate representations directly across such pairs, which is however a common strategy adopted by most existing SGG methods.","We observe that visual variations within the identical triplet are relatively small and certain relation cues are shared in the same type of triplet, which can potentially facilitate the relation learning in SGG.","Moreover, for the long-tail problem widely studied in SGG task, it is also crucial to deal with the limited types and quantity of triplets in tail predicates.","Accordingly, in this paper, we propose a Dual-granularity Relation Modeling (DRM) network to leverage fine-grained triplet cues besides the coarse-grained predicate ones.","DRM utilizes contexts and semantics of predicate and triplet with Dual-granularity Constraints, generating compact and balanced representations from two perspectives to facilitate relation recognition.","Furthermore, a Dual-granularity Knowledge Transfer (DKT) strategy is introduced to transfer variation from head predicates/triplets to tail ones, aiming to enrich the pattern diversity of tail classes to alleviate the long-tail problem.","Extensive experiments demonstrate the effectiveness of our method, which establishes new state-of-the-art performance on Visual Genome, Open Image, and GQA datasets.","Our code is available at \\url{https://github.com/jkli1998/DRM}"],"url":"http://arxiv.org/abs/2406.02038v1","category":"cs.CV"}
{"created":"2024-06-04 07:17:44","title":"M2D-CLAP: Masked Modeling Duo Meets CLAP for Learning General-purpose Audio-Language Representation","abstract":"Contrastive language-audio pre-training (CLAP) enables zero-shot (ZS) inference of audio and exhibits promising performance in several classification tasks. However, conventional audio representations are still crucial for many tasks where ZS is not applicable (e.g., regression problems). Here, we explore a new representation, a general-purpose audio-language representation, that performs well in both ZS and transfer learning. To do so, we propose a new method, M2D-CLAP, which combines self-supervised learning Masked Modeling Duo (M2D) and CLAP. M2D learns an effective representation to model audio signals, and CLAP aligns the representation with text embedding. As a result, M2D-CLAP learns a versatile representation that allows for both ZS and transfer learning. Experiments show that M2D-CLAP performs well on linear evaluation, fine-tuning, and ZS classification with a GTZAN state-of-the-art of 75.17%, thus achieving a general-purpose audio-language representation.","sentences":["Contrastive language-audio pre-training (CLAP) enables zero-shot (ZS) inference of audio and exhibits promising performance in several classification tasks.","However, conventional audio representations are still crucial for many tasks where ZS is not applicable (e.g., regression problems).","Here, we explore a new representation, a general-purpose audio-language representation, that performs well in both ZS and transfer learning.","To do so, we propose a new method, M2D-CLAP, which combines self-supervised learning Masked Modeling Duo (M2D) and CLAP.","M2D learns an effective representation to model audio signals, and CLAP aligns the representation with text embedding.","As a result, M2D-CLAP learns a versatile representation that allows for both ZS and transfer learning.","Experiments show that M2D-CLAP performs well on linear evaluation, fine-tuning, and ZS classification with a GTZAN state-of-the-art of 75.17%, thus achieving a general-purpose audio-language representation."],"url":"http://arxiv.org/abs/2406.02032v1","category":"eess.AS"}
{"created":"2024-06-04 06:43:34","title":"Phonetic Enhanced Language Modeling for Text-to-Speech Synthesis","abstract":"Recent language model-based text-to-speech (TTS) frameworks demonstrate scalability and in-context learning capabilities. However, they suffer from robustness issues due to the accumulation of errors in speech unit predictions during autoregressive language modeling. In this paper, we propose a phonetic enhanced language modeling method to improve the performance of TTS models. We leverage self-supervised representations that are phonetically rich as the training target for the autoregressive language model. Subsequently, a non-autoregressive model is employed to predict discrete acoustic codecs that contain fine-grained acoustic details. The TTS model focuses solely on linguistic modeling during autoregressive training, thereby reducing the error propagation that occurs in non-autoregressive training. Both objective and subjective evaluations validate the effectiveness of our proposed method.","sentences":["Recent language model-based text-to-speech (TTS) frameworks demonstrate scalability and in-context learning capabilities.","However, they suffer from robustness issues due to the accumulation of errors in speech unit predictions during autoregressive language modeling.","In this paper, we propose a phonetic enhanced language modeling method to improve the performance of TTS models.","We leverage self-supervised representations that are phonetically rich as the training target for the autoregressive language model.","Subsequently, a non-autoregressive model is employed to predict discrete acoustic codecs that contain fine-grained acoustic details.","The TTS model focuses solely on linguistic modeling during autoregressive training, thereby reducing the error propagation that occurs in non-autoregressive training.","Both objective and subjective evaluations validate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2406.02009v1","category":"eess.AS"}
{"created":"2024-06-04 06:28:05","title":"Machine learning framework for predicting the entangling capability of parameterized quantum circuits","abstract":"In the noisy intermediate-scale quantum (NISQ) era, quantum devices face significant limitations. Variational quantum algorithms (VQAs) are promising solutions, but their performance heavily depends on the parameterized quantum circuits (PQCs) they utilize. The entanglement of PQCs is an important metric for constructing PQCs. This is because entanglement is not only a key property that distinguishes quantum from classical computing, but it also affects the computational performance of VQAs. However, due to the extensive quantum state sampling required, its computational cost is very high. To address this challenge, we propose a machine learning framework that utilizes a long short-term memory (LSTM) model and gate encoding technology to predict the entangling capability of PQCs. By encoding PQCs into matrix sequences via gate encoding technology and feeding them into an LSTM model at different time steps, our method effectively simulates quantum dynamic evolution. We trained the LSTM model on a dataset of random PQCs. For testing scenarios, our model achieved a pearson correlation coefficient (Pc) of 0.9791 and an root mean square error (RMSE) of 0.05, demonstrating high prediction accuracy and validating the framework's effectiveness. This approach significantly reduces the entanglement computational cost associated with sampling quantum states and provides a practical tool for designing PQC structures and theoretically analyzing the role of entanglement in PQCs.","sentences":["In the noisy intermediate-scale quantum (NISQ) era, quantum devices face significant limitations.","Variational quantum algorithms (VQAs) are promising solutions, but their performance heavily depends on the parameterized quantum circuits (PQCs) they utilize.","The entanglement of PQCs is an important metric for constructing PQCs.","This is because entanglement is not only a key property that distinguishes quantum from classical computing, but it also affects the computational performance of VQAs.","However, due to the extensive quantum state sampling required, its computational cost is very high.","To address this challenge, we propose a machine learning framework that utilizes a long short-term memory (LSTM) model and gate encoding technology to predict the entangling capability of PQCs.","By encoding PQCs into matrix sequences via gate encoding technology and feeding them into an LSTM model at different time steps, our method effectively simulates quantum dynamic evolution.","We trained the LSTM model on a dataset of random PQCs.","For testing scenarios, our model achieved a pearson correlation coefficient (Pc) of 0.9791 and an root mean square error (RMSE) of 0.05, demonstrating high prediction accuracy and validating the framework's effectiveness.","This approach significantly reduces the entanglement computational cost associated with sampling quantum states and provides a practical tool for designing PQC structures and theoretically analyzing the role of entanglement in PQCs."],"url":"http://arxiv.org/abs/2406.01997v1","category":"quant-ph"}
{"created":"2024-06-04 14:30:44","title":"A compact stellarator-tokamak hybrid","abstract":"Tokamaks and stellarators are the leading two magnetic confinement devices for producing fusion energy, begging the question of whether the strengths of the two could be merged into a single concept. To meet this challenge, we propose a first-of-its kind optimized stellarator-tokamak hybrid. Compared to a typical tokamak coil set, only a single simple type of stellarator coil has to be added which leads to a compact, volume- and transport-preserving magnetic field, with an added rotational transform that reaches levels thought to enhance stability.","sentences":["Tokamaks and stellarators are the leading two magnetic confinement devices for producing fusion energy, begging the question of whether the strengths of the two could be merged into a single concept.","To meet this challenge, we propose a first-of-its kind optimized stellarator-tokamak hybrid.","Compared to a typical tokamak coil set, only a single simple type of stellarator coil has to be added which leads to a compact, volume- and transport-preserving magnetic field, with an added rotational transform that reaches levels thought to enhance stability."],"url":"http://arxiv.org/abs/2406.02353v1","category":"physics.plasm-ph"}
{"created":"2024-06-04 12:37:11","title":"Image contrast enhancement based on the Schr\u00f6dinger operator spectrum","abstract":"This study proposes a novel image contrast enhancement method based on image projection onto the squared eigenfunctions of the two dimensional Schr\\\"odinger operator. This projection depends on a design parameter \\texorpdfstring{\\(\\gamma\\)}{gamma} which is proposed to control the pixel intensity during image reconstruction. The performance of the proposed method is investigated through its application to color images. The selection of \\texorpdfstring{\\(\\gamma\\)}{gamma} values is performed using k-means, which helps preserve the image spatial adjacency information. Furthermore, multi-objective optimization using the Non dominated Sorting Genetic Algorithm II (NSAG2) algorithm is proposed to select the optimal values of \\texorpdfstring{\\(\\gamma\\)}{gamma} and the semi-classical parameter h from the 2DSCSA. The results demonstrate the effectiveness of the proposed method for enhancing image contrast while preserving the inherent characteristics of the original image, producing the desired enhancement with almost no artifacts.","sentences":["This study proposes a novel image contrast enhancement method based on image projection onto the squared eigenfunctions of the two dimensional Schr\\\"odinger operator.","This projection depends on a design parameter \\texorpdfstring{\\(\\gamma\\)}{gamma} which is proposed to control the pixel intensity during image reconstruction.","The performance of the proposed method is investigated through its application to color images.","The selection of \\texorpdfstring{\\(\\gamma\\)}{gamma} values is performed using k-means, which helps preserve the image spatial adjacency information.","Furthermore, multi-objective optimization using the Non dominated Sorting Genetic Algorithm II (NSAG2) algorithm is proposed to select the optimal values of \\texorpdfstring{\\(\\gamma\\)}{gamma} and the semi-classical parameter h from the 2DSCSA.","The results demonstrate the effectiveness of the proposed method for enhancing image contrast while preserving the inherent characteristics of the original image, producing the desired enhancement with almost no artifacts."],"url":"http://arxiv.org/abs/2406.02264v1","category":"cs.CV"}
{"created":"2024-06-04 11:07:11","title":"Automatic nonstationary anisotropic Tikhonov regularization through bilevel optimization","abstract":"Regularization techniques are necessary to compute meaningful solutions to discrete ill-posed inverse problems. The well-known 2-norm Tikhonov regularization method equipped with a discretization of the gradient operator as regularization operator penalizes large gradient components of the solution to overcome instabilities. However, this method is homogeneous, i.e., it does not take into account the orientation of the regularized solution and therefore tends to smooth the desired structures, textures and discontinuities, which often contain important information. If the local orientation field of the solution is known, a possible way to overcome this issue is to implement local anisotropic regularization by penalizing weighted directional derivatives. In this paper, considering problems that are inherently two-dimensional, we propose to automatically and simultaneously recover the regularized solution and the local orientation parameters (used to define the anisotropic regularization term) by solving a bilevel optimization problem. Specifically, the lower level problem is Tikhonov regularization equipped with local anisotropic regularization, while the objective function of the upper level problem encodes some natural assumptions about the local orientation parameters and the Tikhonov regularization parameter. Application of the proposed algorithm to a variety of inverse problems in imaging (such as denoising, deblurring, tomography and Dix inversion), with both real and synthetic data, shows its effectiveness and robustness.","sentences":["Regularization techniques are necessary to compute meaningful solutions to discrete ill-posed inverse problems.","The well-known 2-norm Tikhonov regularization method equipped with a discretization of the gradient operator as regularization operator penalizes large gradient components of the solution to overcome instabilities.","However, this method is homogeneous, i.e., it does not take into account the orientation of the regularized solution and therefore tends to smooth the desired structures, textures and discontinuities, which often contain important information.","If the local orientation field of the solution is known, a possible way to overcome this issue is to implement local anisotropic regularization by penalizing weighted directional derivatives.","In this paper, considering problems that are inherently two-dimensional, we propose to automatically and simultaneously recover the regularized solution and the local orientation parameters (used to define the anisotropic regularization term) by solving a bilevel optimization problem.","Specifically, the lower level problem is Tikhonov regularization equipped with local anisotropic regularization, while the objective function of the upper level problem encodes some natural assumptions about the local orientation parameters and the Tikhonov regularization parameter.","Application of the proposed algorithm to a variety of inverse problems in imaging (such as denoising, deblurring, tomography and Dix inversion), with both real and synthetic data, shows its effectiveness and robustness."],"url":"http://arxiv.org/abs/2406.02209v1","category":"math.NA"}
{"created":"2024-06-04 11:03:12","title":"Diffusion Distribution Model for Damage Mitigation in Scanning Transmission Electron Microscopy","abstract":"Despite the widespread use of Scanning Transmission Electron Microscopy (STEM) for observing the structure of materials at the atomic scale, a detailed understanding of some relevant electron beam damage mechanisms is limited. Recent reports suggest that certain types of damage can be modeled as a diffusion process and that the accumulation effects of this process must be kept low in order to reduce damage. We therefore develop an explicit mathematical formulation of spatiotemporal diffusion processes in STEM that take into account both instrument and sample parameters. Furthermore, our framework can aid the design of Diffusion Controlled Sampling (DCS) strategies using optimally selected probe positions in STEM, that constrain the cumulative diffusion distribution. Numerical simulations highlight the variability of the cumulative diffusion distribution for different experimental STEM configurations. These analytical and numerical frameworks can subsequently be used for careful design of 2- and 4-dimensional STEM experiments where beam damage is minimised.","sentences":["Despite the widespread use of Scanning Transmission Electron Microscopy (STEM) for observing the structure of materials at the atomic scale, a detailed understanding of some relevant electron beam damage mechanisms is limited.","Recent reports suggest that certain types of damage can be modeled as a diffusion process and that the accumulation effects of this process must be kept low in order to reduce damage.","We therefore develop an explicit mathematical formulation of spatiotemporal diffusion processes in STEM that take into account both instrument and sample parameters.","Furthermore, our framework can aid the design of Diffusion Controlled Sampling (DCS) strategies using optimally selected probe positions in STEM, that constrain the cumulative diffusion distribution.","Numerical simulations highlight the variability of the cumulative diffusion distribution for different experimental STEM configurations.","These analytical and numerical frameworks can subsequently be used for careful design of 2- and 4-dimensional STEM experiments where beam damage is minimised."],"url":"http://arxiv.org/abs/2406.02207v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 10:58:15","title":"Minimum-norm solutions of the non-symmetric semidefinite Procrustes problem","abstract":"Given two matrices $X,B\\in \\mathbb{R}^{n\\times m}$ and a set $\\mathcal{A}\\subseteq \\mathbb{R}^{n\\times n}$, a Procrustes problem consists in finding a matrix $A \\in \\mathcal{A}$ such that the Frobenius norm of $AX-B$ is minimized. When $\\mathcal{A}$ is the set of the matrices whose symmetric part is positive semidefinite, we obtain the so-called non-symmetric positive semidefinite Procrustes (NSPDSP) problem. The NSPDSP problem arises in the estimation of compliance or stiffness matrix in solid and elastic structures. If $X$ has rank $r$, Baghel et al. (Lin. Alg. Appl., 2022) proposed a three-step semi-analytical approach: (1) construct a reduced NSPDSP problem in dimension $r\\times r$, (2) solve the reduced problem by means of a fast gradient method with a linear rate of convergence, and (3) post-process the solution of the reduced problem to construct a solution of the larger original NSPDSP problem. In this paper, we revisit this approach of Baghel et al. and identify an unnecessary assumption used by the authors leading to cases where their algorithm cannot attain a minimum and produces solutions with unbounded norm. In fact, revising the post-processing phase of their semi-analytical approach, we show that the infimum of the NSPDSP problem is always attained, and we show how to compute a minimum-norm solution. We also prove that the symmetric part of the computed solution has minimum rank bounded by $r$, and that the skew-symmetric part has rank bounded by $2r$. Several numerical examples show the efficiency of this algorithm, both in terms of computational speed and of finding optimal minimum-norm solutions.","sentences":["Given two matrices $X,B\\in \\mathbb{R}^{n\\times m}$ and a set $\\mathcal{A}\\subseteq \\mathbb{R}^{n\\times n}$, a Procrustes problem consists in finding a matrix $A \\in \\mathcal{A}$ such that the Frobenius norm of $AX-B$ is minimized.","When $\\mathcal{A}$ is the set of the matrices whose symmetric part is positive semidefinite, we obtain the so-called non-symmetric positive semidefinite Procrustes (NSPDSP) problem.","The NSPDSP problem arises in the estimation of compliance or stiffness matrix in solid and elastic structures.","If $X$ has rank $r$, Baghel et al.","(Lin.","Alg.","Appl., 2022) proposed a three-step semi-analytical approach: (1) construct a reduced NSPDSP problem in dimension $r\\times r$, (2) solve the reduced problem by means of a fast gradient method with a linear rate of convergence, and (3) post-process the solution of the reduced problem to construct a solution of the larger original NSPDSP problem.","In this paper, we revisit this approach of Baghel et al. and identify an unnecessary assumption used by the authors leading to cases where their algorithm cannot attain a minimum and produces solutions with unbounded norm.","In fact, revising the post-processing phase of their semi-analytical approach, we show that the infimum of the NSPDSP problem is always attained, and we show how to compute a minimum-norm solution.","We also prove that the symmetric part of the computed solution has minimum rank bounded by $r$, and that the skew-symmetric part has rank bounded by $2r$. Several numerical examples show the efficiency of this algorithm, both in terms of computational speed and of finding optimal minimum-norm solutions."],"url":"http://arxiv.org/abs/2406.02203v1","category":"math.NA"}
{"created":"2024-06-04 10:03:23","title":"Layer-2 Arbitrage: An Empirical Analysis of Swap Dynamics and Price Disparities on Rollups","abstract":"This paper explores the dynamics of Decentralized Finance (DeFi) within the Layer-2 ecosystem, focusing on Automated Market Makers (AMM) and arbitrage on Ethereum rollups. We observe significant shifts in trading activity from Ethereum to rollups, with swaps on rollups happening 2-3 times more often, though, with lower trade volume. By examining the price differences between AMMs and centralized exchanges, we discover over 0.5 million unexploited arbitrage opportunities on rollups. Remarkably, we observe that these opportunities last, on average, 10 to 20 blocks, requiring adjustments to the LVR metrics to avoid double-counting arbitrage. Our results show that arbitrage in Arbitrum, Base, and Optimism pools ranges from 0.03% to 0.05% of trading volume, while in zkSync Era it oscillates around 0.25%, with the LVR metric overestimating arbitrage by a factor of five. Rollups offer not only lower gas fees, but also provide faster block production, leading to significant differences compared to the trading and arbitrage dynamics of Ethereum.","sentences":["This paper explores the dynamics of Decentralized Finance (DeFi) within the Layer-2 ecosystem, focusing on Automated Market Makers (AMM) and arbitrage on Ethereum rollups.","We observe significant shifts in trading activity from Ethereum to rollups, with swaps on rollups happening 2-3 times more often, though, with lower trade volume.","By examining the price differences between AMMs and centralized exchanges, we discover over 0.5 million unexploited arbitrage opportunities on rollups.","Remarkably, we observe that these opportunities last, on average, 10 to 20 blocks, requiring adjustments to the LVR metrics to avoid double-counting arbitrage.","Our results show that arbitrage in Arbitrum, Base, and Optimism pools ranges from 0.03% to 0.05% of trading volume, while in zkSync Era it oscillates around 0.25%, with the LVR metric overestimating arbitrage by a factor of five.","Rollups offer not only lower gas fees, but also provide faster block production, leading to significant differences compared to the trading and arbitrage dynamics of Ethereum."],"url":"http://arxiv.org/abs/2406.02172v1","category":"cs.CR"}
{"created":"2024-06-04 08:44:25","title":"Deriving the size and shape of the ALBA electron beam with optical synchrotron radiation interferometry using aperture masks: technical choices","abstract":"We explore non-redundant aperture masking to derive the size and shape of the ALBA synchrotron light source at optical wavelengths using synchrotron radiation interferometry. We show that non-redundant masks are required due to phase fluctuations arising within the experimental set-up. We also show, using closure phase, that the phase fluctuations are factorizable into element-based errors. We employ multiple masks, including 2, 3, 5, and 6 hole configurations. We develop a process for self-calibration of the element-based amplitudes (square root of flux through the aperture), which corrects for non-uniform illumination over the mask, in order to derive visibility coherences and phases, from which the source size and shape can be derived. We explore the optimal procedures to obtain the most reliable results with the 5-hole mask, based on the temporal scatter in measured coherences and closure phases. We find that the closure phases are very stable, and close to zero (within $2^o$). Through uv-modeling, we consider the noise properties of the experiment and conclude that our visibility measurements per frame are likely accurate to an rms scatter of $\\sim 1\\%$.","sentences":["We explore non-redundant aperture masking to derive the size and shape of the ALBA synchrotron light source at optical wavelengths using synchrotron radiation interferometry.","We show that non-redundant masks are required due to phase fluctuations arising within the experimental set-up.","We also show, using closure phase, that the phase fluctuations are factorizable into element-based errors.","We employ multiple masks, including 2, 3, 5, and 6 hole configurations.","We develop a process for self-calibration of the element-based amplitudes (square root of flux through the aperture), which corrects for non-uniform illumination over the mask, in order to derive visibility coherences and phases, from which the source size and shape can be derived.","We explore the optimal procedures to obtain the most reliable results with the 5-hole mask, based on the temporal scatter in measured coherences and closure phases.","We find that the closure phases are very stable, and close to zero (within $2^o$).","Through uv-modeling, we consider the noise properties of the experiment and conclude that our visibility measurements per frame are likely accurate to an rms scatter of $\\sim 1\\%$."],"url":"http://arxiv.org/abs/2406.02114v1","category":"physics.acc-ph"}
{"created":"2024-06-04 07:54:10","title":"ZTF SN Ia DR2: Study of Type Ia Supernova lightcurve fits","abstract":"Type Ia supernova (SN Ia) cosmology relies on the estimation of lightcurve parameters to derive precision distances that leads to the estimation of cosmological parameters. The empirical SALT2 lightcurve modeling that relies on only two parameters, a stretch x1, and a color c, has been used by the community for almost two decades. In this paper we study the ability of the SALT2 model to fit the nearly 3000 cosmology-grade SN Ia lightcurves from the second release of the Zwicky Transient Facility (ZTF) cosmology science working group. While the ZTF data was not used to train SALT2, the algorithm is modeling the ZTF SN Ia optical lightcurves remarkably well, except for lightcurve points prior to -10 d from maximum, where the training critically lacks statistics. We find that the lightcurve fitting is robust against the considered choice of phase-range, but we show the [-10; +40] d range to be optimal in terms of statistics and accuracy. We do not detect any significant features in the lightcurve fit residuals that could be connected to the host environment. Potential systematic population differences related to the SN Ia host properties might thus not be accountable for by the addition of extra lightcurve parameters. However, a small but significant inconsistency between residuals of blue- and red-SN Ia strongly suggests the existence of a phase-dependent color term, with potential implications for the use of SNe Ia in precision cosmology. We thus encourage modellers to explore this avenue and we emphasize the importance that SN Ia cosmology must include a SALT2 retraining to accurately model the lightcurves and avoid biasing the derivation of cosmological parameters.","sentences":["Type Ia supernova (SN Ia) cosmology relies on the estimation of lightcurve parameters to derive precision distances that leads to the estimation of cosmological parameters.","The empirical SALT2 lightcurve modeling that relies on only two parameters, a stretch x1, and a color c, has been used by the community for almost two decades.","In this paper we study the ability of the SALT2 model to fit the nearly 3000 cosmology-grade SN Ia lightcurves from the second release of the Zwicky Transient Facility (ZTF) cosmology science working group.","While the ZTF data was not used to train SALT2, the algorithm is modeling the ZTF SN Ia optical lightcurves remarkably well, except for lightcurve points prior to -10 d from maximum, where the training critically lacks statistics.","We find that the lightcurve fitting is robust against the considered choice of phase-range, but we show the [-10; +40] d range to be optimal in terms of statistics and accuracy.","We do not detect any significant features in the lightcurve fit residuals that could be connected to the host environment.","Potential systematic population differences related to the SN Ia host properties might thus not be accountable for by the addition of extra lightcurve parameters.","However, a small but significant inconsistency between residuals of blue- and red-SN Ia strongly suggests the existence of a phase-dependent color term, with potential implications for the use of SNe Ia in precision cosmology.","We thus encourage modellers to explore this avenue and we emphasize the importance that SN Ia cosmology must include a SALT2 retraining to accurately model the lightcurves and avoid biasing the derivation of cosmological parameters."],"url":"http://arxiv.org/abs/2406.02073v1","category":"astro-ph.CO"}
{"created":"2024-06-04 07:45:59","title":"On the largest minimum distances of [n,6] LCD codes","abstract":"Linear complementary dual (LCD) codes can be used to against side-channel attacks and fault noninvasive attacks. Let $d_{a}(n,6)$ and $d_{l}(n,6)$ be the minimum weights of all binary optimal linear codes and LCD codes with length $n$ and dimension 6, respectively.In this article, we aim to obtain the values of $d_{l}(n,6)$ for $n\\geq 51$ by investigating the nonexistence and constructions of LCD codes with given parameters. Suppose that $s \\ge 0$ and $0\\leq t\\leq 62$ are two integers and $n=63s+t$. Using the theories of defining vectors, generalized anti-codes, reduced codes and nested codes, we exactly determine $d_{l}(n,6)$ for $t \\notin\\{21,22,25,26,33,34,37,38,45,46\\}$, while we show that $d_{l}(n,6)\\in$$\\{d_{a}(n,6)$ $-1,d_{a}(n,6)\\}$ for $t\\in\\{21,22,26,34,37,38,46\\}$ and $ d_{l}(n,6)\\in$$ \\{d_{a}(n,6)-2,$ $d_{a}(n,6)-1\\}$ for$t\\in{25,33,45\\}$.","sentences":["Linear complementary dual (LCD) codes can be used to against side-channel attacks and fault noninvasive attacks.","Let $d_{a}(n,6)$ and $d_{l}(n,6)$ be the minimum weights of all binary optimal linear codes and LCD codes with length $n$ and dimension 6, respectively.","In this article, we aim to obtain the values of $d_{l}(n,6)$ for $n\\geq 51$ by investigating the nonexistence and constructions of LCD codes with given parameters.","Suppose that $s \\ge 0$ and $0\\leq t\\leq 62$ are two integers and $n=63s+t$. Using the theories of defining vectors, generalized anti-codes, reduced codes and nested codes, we exactly determine $d_{l}(n,6)$ for $t \\notin\\{21,22,25,26,33,34,37,38,45,46\\}$, while we show that $d_{l}(n,6)\\in$$\\{d_{a}(n,6)$ $-1,d_{a}(n,6)\\}$ for $t\\in\\{21,22,26,34,37,38,46\\}$ and $ d_{l}(n,6)\\in$$ \\{d_{a}(n,6)-2,$ $d_{a}(n,6)-1\\}$ for$t\\in{25,33,45\\}$."],"url":"http://arxiv.org/abs/2406.02065v1","category":"cs.IT"}
{"created":"2024-06-04 07:17:08","title":"An Axiomatisation of Error Intolerant Estimation","abstract":"Point estimation is a fundamental statistical task. Given the wide selection of available point estimators, it is unclear, however, what, if any, would be universally-agreed theoretical reasons to generally prefer one such estimator over another. In this paper, we define a class of estimation scenarios which includes commonly-encountered problem situations such as both ``high stakes'' estimation and scientific inference, and introduce a new class of estimators, Error Intolerance Candidates (EIC) estimators, which we prove is optimal for it.   EIC estimators are parameterised by an externally-given loss function. We prove, however, that even without such a loss function if one accepts a small number of incontrovertible-seeming assumptions regarding what constitutes a reasonable loss function, the optimal EIC estimator can be characterised uniquely.   The optimal estimator derived in this second case is a previously-studied combination of maximum a posteriori (MAP) estimation and Wallace-Freeman (WF) estimation which has long been advocated among Minimum Message Length (MML) researchers, where it is derived as an approximation to the information-theoretic Strict MML estimator. Our results provide a novel justification for it that is purely Bayesian and requires neither approximations nor coding, placing both MAP and WF as special cases in the larger class of EIC estimators.","sentences":["Point estimation is a fundamental statistical task.","Given the wide selection of available point estimators, it is unclear, however, what, if any, would be universally-agreed theoretical reasons to generally prefer one such estimator over another.","In this paper, we define a class of estimation scenarios which includes commonly-encountered problem situations such as both ``high stakes'' estimation and scientific inference, and introduce a new class of estimators, Error Intolerance Candidates (EIC) estimators, which we prove is optimal for it.   ","EIC estimators are parameterised by an externally-given loss function.","We prove, however, that even without such a loss function if one accepts a small number of incontrovertible-seeming assumptions regarding what constitutes a reasonable loss function, the optimal EIC estimator can be characterised uniquely.   ","The optimal estimator derived in this second case is a previously-studied combination of maximum a posteriori (MAP) estimation and Wallace-Freeman (WF) estimation which has long been advocated among Minimum Message Length (MML) researchers, where it is derived as an approximation to the information-theoretic Strict MML estimator.","Our results provide a novel justification for it that is purely Bayesian and requires neither approximations nor coding, placing both MAP and WF as special cases in the larger class of EIC estimators."],"url":"http://arxiv.org/abs/2406.02031v1","category":"math.ST"}
{"created":"2024-06-04 06:17:00","title":"Data-driven optimal prediction with control","abstract":"This study presents the extension of the data-driven optimal prediction approach to the dynamical system with control. The optimal prediction is used to analyze dynamical systems in which the states consist of resolved and unresolved variables. The latter variables can not be measured explicitly. They may have smaller amplitudes and affect the resolved variables that can be measured. The optimal prediction approach recovers the averaged trajectories of the resolved variables by computing conditional expectations, while the distribution of the unresolved variables is assumed to be known. We consider such dynamical systems and introduce their additional control functions. To predict the targeted trajectories numerically, we develop a data-driven method based on the dynamic mode decomposition. The proposed approach takes the $\\mathit{measured}$ trajectories of the resolved variables, constructs an approximate linear operator from the Mori-Zwanzig decomposition, and reconstructs the $\\mathit{averaged}$ trajectories of the same variables. It is demonstrated that the method is much faster than the Monte Carlo simulations and it provides a reliable prediction. We experimentally confirm the efficacy of the proposed method for two Hamiltonian dynamical systems.","sentences":["This study presents the extension of the data-driven optimal prediction approach to the dynamical system with control.","The optimal prediction is used to analyze dynamical systems in which the states consist of resolved and unresolved variables.","The latter variables can not be measured explicitly.","They may have smaller amplitudes and affect the resolved variables that can be measured.","The optimal prediction approach recovers the averaged trajectories of the resolved variables by computing conditional expectations, while the distribution of the unresolved variables is assumed to be known.","We consider such dynamical systems and introduce their additional control functions.","To predict the targeted trajectories numerically, we develop a data-driven method based on the dynamic mode decomposition.","The proposed approach takes the $\\mathit{measured}$ trajectories of the resolved variables, constructs an approximate linear operator from the Mori-Zwanzig decomposition, and reconstructs the $\\mathit{averaged}$ trajectories of the same variables.","It is demonstrated that the method is much faster than the Monte Carlo simulations and it provides a reliable prediction.","We experimentally confirm the efficacy of the proposed method for two Hamiltonian dynamical systems."],"url":"http://arxiv.org/abs/2406.01991v1","category":"math.DS"}
{"created":"2024-06-04 04:51:24","title":"Subspace Quasi-Newton Method with Gradient Approximation","abstract":"In recent years, various subspace algorithms have been developed to handle large-scale optimization problems. Although existing subspace Newton methods require fewer iterations to converge in practice, the matrix operations and full gradient computation are bottlenecks when dealing with large-scale problems. %In this study, We propose a subspace quasi-Newton method that is restricted to a deterministic-subspace together with a gradient approximation based on random matrix theory. Our method does not require full gradients, let alone Hessian matrices. Yet, it achieves the same order of the worst-case iteration complexities in average for convex and nonconvex cases, compared to existing subspace methods. In numerical experiments, we confirm the superiority of our algorithm in terms of computation time.","sentences":["In recent years, various subspace algorithms have been developed to handle large-scale optimization problems.","Although existing subspace Newton methods require fewer iterations to converge in practice, the matrix operations and full gradient computation are bottlenecks when dealing with large-scale problems.","%In this study, We propose a subspace quasi-Newton method that is restricted to a deterministic-subspace together with a gradient approximation based on random matrix theory.","Our method does not require full gradients, let alone Hessian matrices.","Yet, it achieves the same order of the worst-case iteration complexities in average for convex and nonconvex cases, compared to existing subspace methods.","In numerical experiments, we confirm the superiority of our algorithm in terms of computation time."],"url":"http://arxiv.org/abs/2406.01965v1","category":"math.OC"}
{"created":"2024-06-04 04:02:35","title":"Recursive Polynomial Method for Fast Collision Avoidance Maneuver Design","abstract":"A simple and reliable algorithm for collision avoidance maneuvers (CAMs), capable of computing impulsive, multi-impulsive, and low-thrust maneuvers, is proposed. The probability of collision (PoC) is approximated by a polynomial of arbitrary order as a function of the control, transforming the CAM designinto a polynomial program. The solution procedure is initiated by computing the CAM via a first-order greedy optimization approach, wherein the control action is applied in the direction of the gradient of PoC to maximize its change. Successively, the polynomial is truncated at higher orders, and the solution of the previous order is used to linearize the constraint. This enables achieving accurate solutions even for highly nonlinear safety metrics and dynamics. Since the optimization process comprises only polynomial evaluations, the method is computationally efficient, with run times typically below 1 s. Moreover, no restrictions on the considered dynamics are necessary; therefore, results are shown for Keplerian, J2, and circular restricted three-body problem dynamics.","sentences":["A simple and reliable algorithm for collision avoidance maneuvers (CAMs), capable of computing impulsive, multi-impulsive, and low-thrust maneuvers, is proposed.","The probability of collision (PoC) is approximated by a polynomial of arbitrary order as a function of the control, transforming the CAM designinto a polynomial program.","The solution procedure is initiated by computing the CAM via a first-order greedy optimization approach, wherein the control action is applied in the direction of the gradient of PoC to maximize its change.","Successively, the polynomial is truncated at higher orders, and the solution of the previous order is used to linearize the constraint.","This enables achieving accurate solutions even for highly nonlinear safety metrics and dynamics.","Since the optimization process comprises only polynomial evaluations, the method is computationally efficient, with run times typically below 1 s.","Moreover, no restrictions on the considered dynamics are necessary; therefore, results are shown for Keplerian, J2, and circular restricted three-body problem dynamics."],"url":"http://arxiv.org/abs/2406.01949v1","category":"math.OC"}
{"created":"2024-06-04 03:59:28","title":"Can Entanglement-enhanced Quantum Kernels Improve Data Classification?","abstract":"Classical machine learning, extensively utilized across diverse domains, faces limitations in speed, efficiency, parallelism, and processing of complex datasets. In contrast, quantum machine learning algorithms offer significant advantages, including exponentially faster computations, enhanced data handling capabilities, inherent parallelism, and improved optimization for complex problems. In this study, we used the entanglement-enhanced quantum kernel in quantum support vector machine to train complex respiratory data sets. Compared to classical algorithms, our findings reveal that QSVM performs better with 45% higher accuracy for complex respiratory data sets while maintaining comparable performance with linear datasets in contrast to their classical counterparts executed on a 2-qubit system. Through our study, we investigate the efficacy of the QSVM-Kernel algorithm in harnessing the enhanced dimensionality of the quantum Hilbert space for effectively training complex datasets.","sentences":["Classical machine learning, extensively utilized across diverse domains, faces limitations in speed, efficiency, parallelism, and processing of complex datasets.","In contrast, quantum machine learning algorithms offer significant advantages, including exponentially faster computations, enhanced data handling capabilities, inherent parallelism, and improved optimization for complex problems.","In this study, we used the entanglement-enhanced quantum kernel in quantum support vector machine to train complex respiratory data sets.","Compared to classical algorithms, our findings reveal that QSVM performs better with 45% higher accuracy for complex respiratory data sets while maintaining comparable performance with linear datasets in contrast to their classical counterparts executed on a 2-qubit system.","Through our study, we investigate the efficacy of the QSVM-Kernel algorithm in harnessing the enhanced dimensionality of the quantum Hilbert space for effectively training complex datasets."],"url":"http://arxiv.org/abs/2406.01948v1","category":"quant-ph"}
{"created":"2024-06-04 03:57:17","title":"Pareto Optimal Hybrid Beamforming for Short-Packet Millimeter-Wave Integrated Sensing and Communication","abstract":"Pareto optimal solutions are conceived for radar beamforming error (RBE) and sum rate maximization in short-packet (SP) millimeter-wave (mmWave) integrated sensing and communication (ISAC). Our ultimate goal is to realize ultra-reliable low-latency communication (uRLLC) and real-time sensing capabilities for 6G applications. The ISAC base station (BS) transmits short packets in the downlink (DL) to serve multiple communication users (CUs) and detect multiple radar targets (RTs). We investigate the performance trade-off between the sensing and communication capabilities by optimizing both the radio frequency (RF) and the baseband (BB) transmit precoder (TPC), together with the block lengths. The optimization problem considers the minimum rate requirements of the CUs, the maximum tolerable radar beamforming error (RBE) for the RTs, the unit modulus (UM) elements of the RF TPC, and the finite transmit power as the constraints for SP transmission. The resultant problem is highly non-convex due to the intractable rate expression of the SP regime coupled with the non-convex rate and UM constraints. To solve this problem, we propose an innovative two-layer bisection search (TLBS) algorithm, wherein the RF and BB TPCs are optimized in the inner layer, followed by the block length in the outer layer. Furthermore, a pair of novel methods, namely a bisection search-based majorizer and minimizer (BMM) as well as exact penalty-based manifold optimization (EPMO) are harnessed for optimizing the RF TPC in the inner layer. Subsequently, the BB TPC and the block length are derived via second-order cone programming (SOCP) and mixed integer programming methods, respectively. Finally, our exhaustive simulation results reveal the effect of system parameters for various settings on the RBE-rate region of the SP mmWave ISAC system and demonstrate a significantly enhanced performance compared to the benchmarks.","sentences":["Pareto optimal solutions are conceived for radar beamforming error (RBE) and sum rate maximization in short-packet (SP) millimeter-wave (mmWave) integrated sensing and communication (ISAC).","Our ultimate goal is to realize ultra-reliable low-latency communication (uRLLC) and real-time sensing capabilities for 6G applications.","The ISAC base station (BS) transmits short packets in the downlink (DL) to serve multiple communication users (CUs) and detect multiple radar targets (RTs).","We investigate the performance trade-off between the sensing and communication capabilities by optimizing both the radio frequency (RF) and the baseband (BB) transmit precoder (TPC), together with the block lengths.","The optimization problem considers the minimum rate requirements of the CUs, the maximum tolerable radar beamforming error (RBE) for the RTs, the unit modulus (UM) elements of the RF TPC, and the finite transmit power as the constraints for SP transmission.","The resultant problem is highly non-convex due to the intractable rate expression of the SP regime coupled with the non-convex rate and UM constraints.","To solve this problem, we propose an innovative two-layer bisection search (TLBS) algorithm, wherein the RF and BB TPCs are optimized in the inner layer, followed by the block length in the outer layer.","Furthermore, a pair of novel methods, namely a bisection search-based majorizer and minimizer (BMM) as well as exact penalty-based manifold optimization (EPMO) are harnessed for optimizing the RF TPC in the inner layer.","Subsequently, the BB TPC and the block length are derived via second-order cone programming (SOCP) and mixed integer programming methods, respectively.","Finally, our exhaustive simulation results reveal the effect of system parameters for various settings on the RBE-rate region of the SP mmWave ISAC system and demonstrate a significantly enhanced performance compared to the benchmarks."],"url":"http://arxiv.org/abs/2406.01945v1","category":"eess.SP"}
{"created":"2024-06-04 03:50:12","title":"The Role of Level-Set Geometry on the Performance of PDHG for Conic Linear Optimization","abstract":"We consider solving huge-scale instances of (convex) conic linear optimization problems, at the scale where matrix-factorization-free methods are attractive or necessary. The restarted primal-dual hybrid gradient method (rPDHG) -- with heuristic enhancements and GPU implementation -- has been very successful in solving huge-scale linear programming (LP) problems; however its application to more general conic convex optimization problems is not so well-studied. We analyze the theoretical and practical performance of rPDHG for general (convex) conic linear optimization, and LP as a special case thereof. We show a relationship between the geometry of the primal-dual (sub-)level sets $W_\\varepsilon$ and the convergence rate of rPDHG. Specifically, we prove a bound on the convergence rate of rPDHG that improves when there is a primal-dual (sub-)level set $W_\\varepsilon$ for which (i) $W_\\varepsilon$ is close to the optimal solution set (in Hausdorff distance), and (ii) the ratio of the diameter to the \"conic radius\" of $W_\\varepsilon$ is small. And in the special case of LP problems, the performance of rPDHG is bounded only by this ratio applied to the (sub-)level set corresponding to the best non-optimal extreme point. Depending on the problem instance, this ratio can take on extreme values and can result in poor performance of rPDHG both in theory and in practice. To address this issue, we show how central-path-based linear transformations -- including conic rescaling -- can markedly enhance the convergence rate of rPDHG. Furthermore, we present computational results that demonstrate how such rescalings can accelerate convergence to high-accuracy solutions, and lead to more efficient methods for huge-scale linear optimization problems.","sentences":["We consider solving huge-scale instances of (convex) conic linear optimization problems, at the scale where matrix-factorization-free methods are attractive or necessary.","The restarted primal-dual hybrid gradient method (rPDHG) -- with heuristic enhancements and GPU implementation -- has been very successful in solving huge-scale linear programming (LP) problems; however its application to more general conic convex optimization problems is not so well-studied.","We analyze the theoretical and practical performance of rPDHG for general (convex) conic linear optimization, and LP as a special case thereof.","We show a relationship between the geometry of the primal-dual (sub-)level sets $W_\\varepsilon$ and the convergence rate of rPDHG.","Specifically, we prove a bound on the convergence rate of rPDHG that improves when there is a primal-dual (sub-)level set $W_\\varepsilon$ for which (i) $W_\\varepsilon$ is close to the optimal solution set (in Hausdorff distance), and (ii) the ratio of the diameter to the \"conic radius\" of $W_\\varepsilon$ is small.","And in the special case of LP problems, the performance of rPDHG is bounded only by this ratio applied to the (sub-)level set corresponding to the best non-optimal extreme point.","Depending on the problem instance, this ratio can take on extreme values and can result in poor performance of rPDHG both in theory and in practice.","To address this issue, we show how central-path-based linear transformations -- including conic rescaling -- can markedly enhance the convergence rate of rPDHG.","Furthermore, we present computational results that demonstrate how such rescalings can accelerate convergence to high-accuracy solutions, and lead to more efficient methods for huge-scale linear optimization problems."],"url":"http://arxiv.org/abs/2406.01942v1","category":"math.OC"}
{"created":"2024-06-04 03:45:08","title":"Nutrition Estimation for Dietary Management: A Transformer Approach with Depth Sensing","abstract":"Nutrition estimation is crucial for effective dietary management and overall health and well-being. Existing methods often struggle with sub-optimal accuracy and can be time-consuming. In this paper, we propose NuNet, a transformer-based network designed for nutrition estimation that utilizes both RGB and depth information from food images. We have designed and implemented a multi-scale encoder and decoder, along with two types of feature fusion modules, specialized for estimating five nutritional factors. These modules effectively balance the efficiency and effectiveness of feature extraction with flexible usage of our customized attention mechanisms and fusion strategies. Our experimental study shows that NuNet outperforms its variants and existing solutions significantly for nutrition estimation. It achieves an error rate of 15.65%, the lowest known to us, largely due to our multi-scale architecture and fusion modules. This research holds practical values for dietary management with huge potential for transnational research and deployment and could inspire other applications involving multiple data types with varying degrees of importance.","sentences":["Nutrition estimation is crucial for effective dietary management and overall health and well-being.","Existing methods often struggle with sub-optimal accuracy and can be time-consuming.","In this paper, we propose NuNet, a transformer-based network designed for nutrition estimation that utilizes both RGB and depth information from food images.","We have designed and implemented a multi-scale encoder and decoder, along with two types of feature fusion modules, specialized for estimating five nutritional factors.","These modules effectively balance the efficiency and effectiveness of feature extraction with flexible usage of our customized attention mechanisms and fusion strategies.","Our experimental study shows that NuNet outperforms its variants and existing solutions significantly for nutrition estimation.","It achieves an error rate of 15.65%, the lowest known to us, largely due to our multi-scale architecture and fusion modules.","This research holds practical values for dietary management with huge potential for transnational research and deployment and could inspire other applications involving multiple data types with varying degrees of importance."],"url":"http://arxiv.org/abs/2406.01938v1","category":"cs.CV"}
{"created":"2024-06-04 03:42:59","title":"Cram\u00e9r-Rao Bound Analysis and Beamforming Design for Integrated Sensing and Communication with Extended Targets","abstract":"This paper studies an integrated sensing and communication (ISAC) system, where a multi-antenna base station transmits beamformed signals for joint downlink multi-user communication and radar sensing of an extended target (ET). By considering echo signals as reflections from valid elements on the ET contour, a set of novel Cram\\'er-Rao bounds (CRBs) is derived for parameter estimation of the ET, including central range, direction, and orientation. The ISAC transmit beamforming design is then formulated as an optimization problem, aiming to minimize the CRB associated with radar sensing, while satisfying a minimum signal-to-interference-pulse-noise ratio requirement for each communication user, along with a 3-dB beam coverage constraint tailored for the ET. To solve this non-convex problem, we utilize semidefinite relaxation (SDR) and propose a rank-one solution extraction scheme for non-tight relaxation circumstances. To reduce the computation complexity, we further employ an efficient zero-forcing (ZF) based beamforming design, where the sensing task is performed in the null space of communication channels. Numerical results validate the effectiveness of the obtained CRB, revealing the diverse features of CRB for differently shaped ETs. The proposed SDR beamforming design outperforms benchmark designs with lower estimation error and CRB, while the ZF beamforming design greatly improves computation efficiency with minor sensing performance loss.","sentences":["This paper studies an integrated sensing and communication (ISAC) system, where a multi-antenna base station transmits beamformed signals for joint downlink multi-user communication and radar sensing of an extended target (ET).","By considering echo signals as reflections from valid elements on the ET contour, a set of novel Cram\\'er-Rao bounds (CRBs) is derived for parameter estimation of the ET, including central range, direction, and orientation.","The ISAC transmit beamforming design is then formulated as an optimization problem, aiming to minimize the CRB associated with radar sensing, while satisfying a minimum signal-to-interference-pulse-noise ratio requirement for each communication user, along with a 3-dB beam coverage constraint tailored for the ET.","To solve this non-convex problem, we utilize semidefinite relaxation (SDR) and propose a rank-one solution extraction scheme for non-tight relaxation circumstances.","To reduce the computation complexity, we further employ an efficient zero-forcing (ZF) based beamforming design, where the sensing task is performed in the null space of communication channels.","Numerical results validate the effectiveness of the obtained CRB, revealing the diverse features of CRB for differently shaped ETs.","The proposed SDR beamforming design outperforms benchmark designs with lower estimation error and CRB, while the ZF beamforming design greatly improves computation efficiency with minor sensing performance loss."],"url":"http://arxiv.org/abs/2406.01937v1","category":"cs.IT"}
{"created":"2024-06-04 03:35:25","title":"Optimal Transport Guided Correlation Assignment for Multimodal Entity Linking","abstract":"Multimodal Entity Linking (MEL) aims to link ambiguous mentions in multimodal contexts to entities in a multimodal knowledge graph. A pivotal challenge is to fully leverage multi-element correlations between mentions and entities to bridge modality gap and enable fine-grained semantic matching. Existing methods attempt several local correlative mechanisms, relying heavily on the automatically learned attention weights, which may over-concentrate on partial correlations. To mitigate this issue, we formulate the correlation assignment problem as an optimal transport (OT) problem, and propose a novel MEL framework, namely OT-MEL, with OT-guided correlation assignment. Thereby, we exploit the correlation between multimodal features to enhance multimodal fusion, and the correlation between mentions and entities to enhance fine-grained matching. To accelerate model prediction, we further leverage knowledge distillation to transfer OT assignment knowledge to attention mechanism. Experimental results show that our model significantly outperforms previous state-of-the-art baselines and confirm the effectiveness of the OT-guided correlation assignment.","sentences":["Multimodal Entity Linking (MEL) aims to link ambiguous mentions in multimodal contexts to entities in a multimodal knowledge graph.","A pivotal challenge is to fully leverage multi-element correlations between mentions and entities to bridge modality gap and enable fine-grained semantic matching.","Existing methods attempt several local correlative mechanisms, relying heavily on the automatically learned attention weights, which may over-concentrate on partial correlations.","To mitigate this issue, we formulate the correlation assignment problem as an optimal transport (OT) problem, and propose a novel MEL framework, namely OT-MEL, with OT-guided correlation assignment.","Thereby, we exploit the correlation between multimodal features to enhance multimodal fusion, and the correlation between mentions and entities to enhance fine-grained matching.","To accelerate model prediction, we further leverage knowledge distillation to transfer OT assignment knowledge to attention mechanism.","Experimental results show that our model significantly outperforms previous state-of-the-art baselines and confirm the effectiveness of the OT-guided correlation assignment."],"url":"http://arxiv.org/abs/2406.01934v1","category":"cs.CL"}
{"created":"2024-06-04 02:02:23","title":"Towards accurate nuclear mass tables in covariant density functional theory","abstract":"The current investigation focuses on detailed analysis of the anchor based optimization approach (ABOA), its comparison with alternative global fitting protocols and on the global analysis of the truncation of basis effects in the calculation of binding energies. It is shown that ABOA provides a solution which is close to that obtained in alternative approaches but at small portion of their computational time. The application of softer correction function after few initial iterations of ABOA stabilizes and speeds up its convergence. For the first time, the numerical errors in the calculation of binding energies related to the truncation of bosonic and fermionic bases have been globally investigated with respect of asymptotic values corresponding to the infinite basis in the framework of covariant density functional theory (CDFT). These errors typically grow up with the increase of the mass and deformation of the nuclei. To reduce such errors in bosonic sector below 10 keV for almost all nuclei with proton number $Z<120$ one should truncate the bosonic basis at $N_B=28$ instead of presently used $N_B=20$. The reduction of the errors in binding energies due to the truncation of the fermionic basis in CDFT is significantly more numerically costly. For the first time it is shown that the pattern and the speed of the convergence of binding energies as a function of the size of fermionic basis given by $N_F$ depend on the type of covariant energy density functional. The use of explicit density dependence of the meson-nucleon coupling constants or point couplings slows down substantially the speed of convergence of binding energies as a function of $N_F$. A new procedure for finding the asymptotic values of binding energies is suggested in the present paper: it allows better control of numerical errors.","sentences":["The current investigation focuses on detailed analysis of the anchor based optimization approach (ABOA), its comparison with alternative global fitting protocols and on the global analysis of the truncation of basis effects in the calculation of binding energies.","It is shown that ABOA provides a solution which is close to that obtained in alternative approaches but at small portion of their computational time.","The application of softer correction function after few initial iterations of ABOA stabilizes and speeds up its convergence.","For the first time, the numerical errors in the calculation of binding energies related to the truncation of bosonic and fermionic bases have been globally investigated with respect of asymptotic values corresponding to the infinite basis in the framework of covariant density functional theory (CDFT).","These errors typically grow up with the increase of the mass and deformation of the nuclei.","To reduce such errors in bosonic sector below 10 keV for almost all nuclei with proton number $Z<120$ one should truncate the bosonic basis at $N_B=28$ instead of presently used $N_B=20$. The reduction of the errors in binding energies due to the truncation of the fermionic basis in CDFT is significantly more numerically costly.","For the first time it is shown that the pattern and the speed of the convergence of binding energies as a function of the size of fermionic basis given by $N_F$ depend on the type of covariant energy density functional.","The use of explicit density dependence of the meson-nucleon coupling constants or point couplings slows down substantially the speed of convergence of binding energies as a function of $N_F$. A new procedure for finding the asymptotic values of binding energies is suggested in the present paper: it allows better control of numerical errors."],"url":"http://arxiv.org/abs/2406.01896v1","category":"nucl-th"}
{"created":"2024-06-04 01:41:42","title":"Annealing-Assisted Column Generation for Inequality-Constrained Combinatorial Optimization Problems","abstract":"Ising machines are expected to solve combinatorial optimization problems faster than the existing integer programming solvers. These problems, particularly those encountered in practical situations, typically involve inequality constraints. However, owing to the hardware limitations of the current Ising machines, solving combinatorial optimization problems with inequality constraints remains challenging. The Capacitated Vehicle Routing Problem (CVRP) is a typical example of a problem with inequality constraints. The objective function of the CVRP is to minimize the total distance traveled by each vehicle while limiting the total demand of customers served by a single vehicle to the vehicle's capacity. The CVRP is classified as NP-hard and, thus, is commonly solved using heuristic algorithms, such as column generation. Column generation attempts to iteratively generate only the promising routes, as the number of feasible routes increases exponentially. Within this framework, the CVRP is formulated as a set cover problem. The corresponding dual solutions are used to define the pricing subproblem, which is intended to create a new route. By applying Ising machines to this pricing subproblem, the overall computation time can be reduced. This study aims to solve combinatorial optimization problems with inequality constraints using a hybrid algorithm that combines column generation and Ising machines, thereby extending the applications of the latter. We parameterize the difficulty of the inequality constraints and demonstrate that our annealing-assisted column generation can converge to a better lower bound.","sentences":["Ising machines are expected to solve combinatorial optimization problems faster than the existing integer programming solvers.","These problems, particularly those encountered in practical situations, typically involve inequality constraints.","However, owing to the hardware limitations of the current Ising machines, solving combinatorial optimization problems with inequality constraints remains challenging.","The Capacitated Vehicle Routing Problem (CVRP) is a typical example of a problem with inequality constraints.","The objective function of the CVRP is to minimize the total distance traveled by each vehicle while limiting the total demand of customers served by a single vehicle to the vehicle's capacity.","The CVRP is classified as NP-hard and, thus, is commonly solved using heuristic algorithms, such as column generation.","Column generation attempts to iteratively generate only the promising routes, as the number of feasible routes increases exponentially.","Within this framework, the CVRP is formulated as a set cover problem.","The corresponding dual solutions are used to define the pricing subproblem, which is intended to create a new route.","By applying Ising machines to this pricing subproblem, the overall computation time can be reduced.","This study aims to solve combinatorial optimization problems with inequality constraints using a hybrid algorithm that combines column generation and Ising machines, thereby extending the applications of the latter.","We parameterize the difficulty of the inequality constraints and demonstrate that our annealing-assisted column generation can converge to a better lower bound."],"url":"http://arxiv.org/abs/2406.01887v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-04 01:40:00","title":"Monotone Equilibrium Design for Matching Markets with Signaling","abstract":"We study monotone equilibrium design by a planner who chooses an interval of reactions that receivers take before senders and receivers move in matching markets with signaling. In our nonlinear settings, surplus efficiency frontier is convex with decreasing-returns-to-scale information technology. The optimal reaction interval crucially depends on the ripple effect of its lower bound and the trade-off between matching inefficiency and signaling cost savings in the top pooling region generated by its upper bound. Our analysis generates cohesive market design results that integrate the literature on minimum wage, firm size distribution, and relative risk aversion.","sentences":["We study monotone equilibrium design by a planner who chooses an interval of reactions that receivers take before senders and receivers move in matching markets with signaling.","In our nonlinear settings, surplus efficiency frontier is convex with decreasing-returns-to-scale information technology.","The optimal reaction interval crucially depends on the ripple effect of its lower bound and the trade-off between matching inefficiency and signaling cost savings in the top pooling region generated by its upper bound.","Our analysis generates cohesive market design results that integrate the literature on minimum wage, firm size distribution, and relative risk aversion."],"url":"http://arxiv.org/abs/2406.01886v1","category":"econ.TH"}
{"created":"2024-06-04 01:04:01","title":"Resource Optimized Quantum Squaring Circuit","abstract":"Quantum squaring operation is a useful building block in implementing quantum algorithms such as linear regression, regularized least squares algorithm, order-finding algorithm, quantum search algorithm, Newton Raphson division, Euclidean distance calculation, cryptography, and in finding roots and reciprocals. Quantum circuits could be made fault-tolerant by using error correcting codes and fault-tolerant quantum gates (such as the Clifford + T-gates). However, the T-gate is very costly to implement. Two qubit gates (such as the CNOT-gate) are more prone to noise errors than single qubit gates. Consequently, in order to realize reliable quantum algorithms, the quantum circuits should have a low T-count and CNOT-count. In this paper, we present a novel quantum integer squaring architecture optimized for T-count, CNOT-count, T-depth, CNOT-depth, and $KQ_T$ that produces no garbage outputs. To reduce costs, we use a novel approach for arranging the generated partial products that allows us to reduce the number of adders by 50%. We also use the resource efficient logical-AND gate and uncomputation gate shown in [1] to further save resources. The proposed quantum squaring circuit sees an asymptotic reduction of 66.67% in T-count, 50% in T-depth, 29.41% in CNOT-count, 42.86% in CNOT-depth, and 25% in KQ T with respect to Thapliyal et al. [2]. With respect to Nagamani et al. [3] the design sees an asymptotic reduction of 77.27% in T-count, 68.75% in T-depth, 50% in CNOT-count, 61.90% in CNOT-depth, and 6.25% in the $KQ_T$.","sentences":["Quantum squaring operation is a useful building block in implementing quantum algorithms such as linear regression, regularized least squares algorithm, order-finding algorithm, quantum search algorithm, Newton Raphson division, Euclidean distance calculation, cryptography, and in finding roots and reciprocals.","Quantum circuits could be made fault-tolerant by using error correcting codes and fault-tolerant quantum gates (such as the Clifford + T-gates).","However, the T-gate is very costly to implement.","Two qubit gates (such as the CNOT-gate) are more prone to noise errors than single qubit gates.","Consequently, in order to realize reliable quantum algorithms, the quantum circuits should have a low T-count and CNOT-count.","In this paper, we present a novel quantum integer squaring architecture optimized for T-count, CNOT-count, T-depth, CNOT-depth, and $KQ_T$ that produces no garbage outputs.","To reduce costs, we use a novel approach for arranging the generated partial products that allows us to reduce the number of adders by 50%.","We also use the resource efficient logical-AND gate and uncomputation gate shown in [1] to further save resources.","The proposed quantum squaring circuit sees an asymptotic reduction of 66.67% in T-count, 50% in T-depth, 29.41% in CNOT-count, 42.86% in CNOT-depth, and 25% in KQ T with respect to Thapliyal et al.","[2].","With respect to Nagamani et al.","[3] the design sees an asymptotic reduction of 77.27% in T-count, 68.75% in T-depth, 50% in CNOT-count, 61.90% in CNOT-depth, and 6.25% in the $KQ_T$."],"url":"http://arxiv.org/abs/2406.01875v1","category":"quant-ph"}
{"created":"2024-06-04 00:45:37","title":"Understanding Stochastic Natural Gradient Variational Inference","abstract":"Stochastic natural gradient variational inference (NGVI) is a popular posterior inference method with applications in various probabilistic models. Despite its wide usage, little is known about the non-asymptotic convergence rate in the \\emph{stochastic} setting. We aim to lessen this gap and provide a better understanding. For conjugate likelihoods, we prove the first $\\mathcal{O}(\\frac{1}{T})$ non-asymptotic convergence rate of stochastic NGVI. The complexity is no worse than stochastic gradient descent (\\aka black-box variational inference) and the rate likely has better constant dependency that leads to faster convergence in practice. For non-conjugate likelihoods, we show that stochastic NGVI with the canonical parameterization implicitly optimizes a non-convex objective. Thus, a global convergence rate of $\\mathcal{O}(\\frac{1}{T})$ is unlikely without some significant new understanding of optimizing the ELBO using natural gradients.","sentences":["Stochastic natural gradient variational inference (NGVI) is a popular posterior inference method with applications in various probabilistic models.","Despite its wide usage, little is known about the non-asymptotic convergence rate in the \\emph{stochastic} setting.","We aim to lessen this gap and provide a better understanding.","For conjugate likelihoods, we prove the first $\\mathcal{O}(\\frac{1}{T})$ non-asymptotic convergence rate of stochastic NGVI.","The complexity is no worse than stochastic gradient descent (\\aka black-box variational inference) and the rate likely has better constant dependency that leads to faster convergence in practice.","For non-conjugate likelihoods, we show that stochastic NGVI with the canonical parameterization implicitly optimizes a non-convex objective.","Thus, a global convergence rate of $\\mathcal{O}(\\frac{1}{T})$ is unlikely without some significant new understanding of optimizing the ELBO using natural gradients."],"url":"http://arxiv.org/abs/2406.01870v1","category":"cs.LG"}
{"created":"2024-06-04 00:09:05","title":"Variational quantum state preparation for quantum-enhanced metrology in noisy systems","abstract":"We investigate optimized quantum state preparation for quantum metrology applications in noisy environments. We simulate a low-depth variational quantum circuit (VQC) composed of a sequence of global rotations and entangling operations applied to a chain of qubits that are subject to dephasing noise. The parameters controlling the VQC are numerically optimized to maximize the quantum Fisher information, which characterizes the ultimate metrological sensitivity of a quantum state, with respect to a global rotation. We find that regardless of the details of the entangling operation implemented in the VQC, the optimal quantum states can be broadly classified into a trio of qualitative regimes -- cat-like, squeezed-like, and product states -- associated with different dephasing rates. Our findings are relevant for designing optimal state-preparation strategies for next-generation quantum sensors exploiting entanglement, such as time and frequency standards and magnetometers, aimed at achieving state-of-the-art performance in the presence of noise and decoherence.","sentences":["We investigate optimized quantum state preparation for quantum metrology applications in noisy environments.","We simulate a low-depth variational quantum circuit (VQC) composed of a sequence of global rotations and entangling operations applied to a chain of qubits that are subject to dephasing noise.","The parameters controlling the VQC are numerically optimized to maximize the quantum Fisher information, which characterizes the ultimate metrological sensitivity of a quantum state, with respect to a global rotation.","We find that regardless of the details of the entangling operation implemented in the VQC, the optimal quantum states can be broadly classified into a trio of qualitative regimes -- cat-like, squeezed-like, and product states -- associated with different dephasing rates.","Our findings are relevant for designing optimal state-preparation strategies for next-generation quantum sensors exploiting entanglement, such as time and frequency standards and magnetometers, aimed at achieving state-of-the-art performance in the presence of noise and decoherence."],"url":"http://arxiv.org/abs/2406.01859v1","category":"quant-ph"}
{"created":"2024-06-04 00:03:16","title":"CCAT: FYST Prime-Cam Readout Software: A framework for massively scalable KID arrays","abstract":"We outline the development of the readout software for the Prime-Cam and Mod-Cam instruments on the CCAT Fred Young Submillimeter Telescope (FYST), primecam_readout. The instruments feature lumped-element kinetic inductance detector (LEKID) arrays driven by Xilinx ZCU111 RFSoC boards. In the current configuration, each board can drive up to 4000 KIDs, and Prime-Cam is implementing approximately 25 boards. The software runs on a centralized control computer connected to the boards via dedicated ethernet, and facilitates such tasks as frequency-multiplexed tone comb driving, comb calibration and optimization, and detector timestream establishment. The control computer utilizes dynamically generated control channels for each board, allowing for simultaneous parallel control over all, while uniquely tracking diagnostics for each. This work demonstrates a scalable RFSoC readout architecture where computational demands increase linearly with the number of detectors, enabling control of tens-of-thousands of KIDs with modest hardware, and opening the door to the next generation of KID arrays housing millions of detectors.","sentences":["We outline the development of the readout software for the Prime-Cam and Mod-Cam instruments on the CCAT Fred Young Submillimeter Telescope (FYST), primecam_readout.","The instruments feature lumped-element kinetic inductance detector (LEKID) arrays driven by Xilinx ZCU111 RFSoC boards.","In the current configuration, each board can drive up to 4000 KIDs, and Prime-Cam is implementing approximately 25 boards.","The software runs on a centralized control computer connected to the boards via dedicated ethernet, and facilitates such tasks as frequency-multiplexed tone comb driving, comb calibration and optimization, and detector timestream establishment.","The control computer utilizes dynamically generated control channels for each board, allowing for simultaneous parallel control over all, while uniquely tracking diagnostics for each.","This work demonstrates a scalable RFSoC readout architecture where computational demands increase linearly with the number of detectors, enabling control of tens-of-thousands of KIDs with modest hardware, and opening the door to the next generation of KID arrays housing millions of detectors."],"url":"http://arxiv.org/abs/2406.01858v1","category":"astro-ph.IM"}
{"created":"2024-06-04 00:01:48","title":"On Approximation of Robust Max-Cut and Related Problems using Randomized Rounding Algorithms","abstract":"Goemans and Williamson proposed a randomized rounding algorithm for the MAX-CUT problem with a 0.878 approximation bound in expectation. The 0.878 approximation bound remains the best-known approximation bound for this APX-hard problem. Their approach was subsequently applied to other related problems such as Max-DiCut, MAX-SAT, and Max-2SAT, etc. We show that the randomized rounding algorithm can also be used to achieve a 0.878 approximation bound for the robust and distributionally robust counterparts of the max-cut problem. We also show that the approximation bounds for the other problems are maintained for their robust and distributionally robust counterparts if the randomization projection framework is used.","sentences":["Goemans and Williamson proposed a randomized rounding algorithm for the MAX-CUT problem with a 0.878 approximation bound in expectation.","The 0.878 approximation bound remains the best-known approximation bound for this APX-hard problem.","Their approach was subsequently applied to other related problems such as Max-DiCut, MAX-SAT, and Max-2SAT, etc.","We show that the randomized rounding algorithm can also be used to achieve a 0.878 approximation bound for the robust and distributionally robust counterparts of the max-cut problem.","We also show that the approximation bounds for the other problems are maintained for their robust and distributionally robust counterparts if the randomization projection framework is used."],"url":"http://arxiv.org/abs/2406.01856v1","category":"cs.DS"}
{"created":"2024-06-03 23:53:04","title":"Cell-free massive MIMO Channels in an Urban Environment -- Measurements and Channel Statistics","abstract":"Cell-free massive MIMO (CF-mMIMO), where each user equipment (UE) is connected to multiple access points (APs), is emerging as an important component for 5G and 6G cellular systems. Accurate channel models based on measurements are required to optimize their design and deployment. This paper presents an extensive measurement campaign for CF-mMIMO in an urban environment. A new \"virtual AP\" technique measures channels between 80 UE locations and more than 20,000 possible microcellular AP locations. Measurements are done at 3.5 GHz carrier frequency with 350 MHz bandwidth (BW). The paper describes the measurement setup and data processing, shows sample results and their physical interpretation, and provides statistics for key quantities such as pathloss, shadowing, delay spread (DS), and delay window. We find pathloss coefficients of 2.9 and 10.4 for line-of-sight (LOS) and non line-of-sight (NLOS), respectively, where the high LOS coefficient is mainly because larger distance leads to more grazing angle of incidence and thus lower antenna gain in our setup. Shadowing standard deviations are 5.1/16.6 dB, and root mean squared (RMS) DSs of -80.6/-72.6 dBs. The measurements can also be used for parameterizing a CUNEC-type model, which will be reported in future work.","sentences":["Cell-free massive MIMO (CF-mMIMO), where each user equipment (UE) is connected to multiple access points (APs), is emerging as an important component for 5G and 6G cellular systems.","Accurate channel models based on measurements are required to optimize their design and deployment.","This paper presents an extensive measurement campaign for CF-mMIMO in an urban environment.","A new \"virtual AP\" technique measures channels between 80 UE locations and more than 20,000 possible microcellular AP locations.","Measurements are done at 3.5 GHz carrier frequency with 350 MHz bandwidth (BW).","The paper describes the measurement setup and data processing, shows sample results and their physical interpretation, and provides statistics for key quantities such as pathloss, shadowing, delay spread (DS), and delay window.","We find pathloss coefficients of 2.9 and 10.4 for line-of-sight (LOS) and","non line-of-sight (NLOS), respectively, where the high LOS coefficient is mainly because larger distance leads to more grazing angle of incidence and thus lower antenna gain in our setup.","Shadowing standard deviations are 5.1/16.6 dB, and root mean squared (RMS) DSs of -80.6/-72.6 dBs.","The measurements can also be used for parameterizing a CUNEC-type model, which will be reported in future work."],"url":"http://arxiv.org/abs/2406.01850v1","category":"eess.SP"}
{"created":"2024-06-03 23:47:42","title":"Optimal Control Synthesis with Relaxed Global Temporal Logic Specifications for Homogeneous Multi-robot Teams","abstract":"In this work, we address the problem of control synthesis for a homogeneous team of robots given a global temporal logic specification and formal user preferences for relaxation in case of infeasibility. The relaxation preferences are represented as a Weighted Finite-state Edit System and are used to compute a relaxed specification automaton that captures all allowable relaxations of the mission specification and their costs. For synthesis, we introduce a Mixed Integer Linear Programming (MILP) formulation that combines the motion of the team of robots with the relaxed specification automaton. Our approach combines automata-based and MILP-based methods and leverages the strengths of both approaches while avoiding their shortcomings. Specifically, the relaxed specification automaton explicitly accounts for the progress towards satisfaction, and the MILP-based optimization approach avoids the state-space explosion associated with explicit product-automata construction, thereby efficiently solving the problem. The case studies highlight the efficiency of the proposed approach.","sentences":["In this work, we address the problem of control synthesis for a homogeneous team of robots given a global temporal logic specification and formal user preferences for relaxation in case of infeasibility.","The relaxation preferences are represented as a Weighted Finite-state Edit System and are used to compute a relaxed specification automaton that captures all allowable relaxations of the mission specification and their costs.","For synthesis, we introduce a Mixed Integer Linear Programming (MILP) formulation that combines the motion of the team of robots with the relaxed specification automaton.","Our approach combines automata-based and MILP-based methods and leverages the strengths of both approaches while avoiding their shortcomings.","Specifically, the relaxed specification automaton explicitly accounts for the progress towards satisfaction, and the MILP-based optimization approach avoids the state-space explosion associated with explicit product-automata construction, thereby efficiently solving the problem.","The case studies highlight the efficiency of the proposed approach."],"url":"http://arxiv.org/abs/2406.01848v1","category":"cs.RO"}
{"created":"2024-06-03 23:09:30","title":"Boosting Vision-Language Models with Transduction","abstract":"Transduction is a powerful paradigm that leverages the structure of unlabeled data to boost predictive accuracy. We present TransCLIP, a novel and computationally efficient transductive approach designed for Vision-Language Models (VLMs). TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances. Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a KL divergence penalty that integrates the text-encoder knowledge and guides the transductive learning process. We further derive an iterative Block Majorize-Minimize (BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-assignment updates, yielding computationally efficient transduction for large-scale datasets. We report comprehensive evaluations, comparisons, and ablation studies that demonstrate: (i) Transduction can greatly enhance the generalization capabilities of inductive pretrained zero- and few-shot VLMs; (ii) TransCLIP substantially outperforms standard transductive few-shot learning methods relying solely on vision features, notably due to the KL-based language constraint.","sentences":["Transduction is a powerful paradigm that leverages the structure of unlabeled data to boost predictive accuracy.","We present TransCLIP, a novel and computationally efficient transductive approach designed for Vision-Language Models (VLMs).","TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances.","Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a KL divergence penalty that integrates the text-encoder knowledge and guides the transductive learning process.","We further derive an iterative Block Majorize-Minimize (BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-assignment updates, yielding computationally efficient transduction for large-scale datasets.","We report comprehensive evaluations, comparisons, and ablation studies that demonstrate: (i) Transduction can greatly enhance the generalization capabilities of inductive pretrained zero-","and few-shot VLMs; (ii) TransCLIP substantially outperforms standard transductive few-shot learning methods relying solely on vision features, notably due to the KL-based language constraint."],"url":"http://arxiv.org/abs/2406.01837v1","category":"cs.CV"}
{"created":"2024-06-03 22:30:05","title":"Error Field Predictability and Consequences for ITER","abstract":"ITER coil tolerances are re-evaluated using the modern understanding of coupling to least-stable plasma modes and an updated center-line-traced model of ITER's coil windings. This reassessment finds the tolerances to be conservative through a statistical, linear study of $n=1$ error fields (EFs) due to tilted, shifted misplacements and nominal windings of central solenoid and poloidal field coils within tolerance. We also show that a model-based correction scheme remains effective even when metrology quality is sub-optimal, and compare this to projected empirical correction schemes. We begin with an analysis of the necessity of error field correction (EFC) for daily operation in ITER using scaling laws for the EF penetration threshold. We then consider the predictability of EF dominant mode overlap across early planned ITER scenarios and, as measuring EFs in high power scenarios can pose risks to the device, the potential for extrapolation to the ITER Baseline Scenario (IBS). We find that carefully designing a scenario matching currents proportionally to those of the IBS is far more important than plasma shape or profiles in accurately measuring an optimal correction current set.","sentences":["ITER coil tolerances are re-evaluated using the modern understanding of coupling to least-stable plasma modes and an updated center-line-traced model of ITER's coil windings.","This reassessment finds the tolerances to be conservative through a statistical, linear study of $n=1$ error fields (EFs) due to tilted, shifted misplacements and nominal windings of central solenoid and poloidal field coils within tolerance.","We also show that a model-based correction scheme remains effective even when metrology quality is sub-optimal, and compare this to projected empirical correction schemes.","We begin with an analysis of the necessity of error field correction (EFC) for daily operation in ITER using scaling laws for the EF penetration threshold.","We then consider the predictability of EF dominant mode overlap across early planned ITER scenarios and, as measuring EFs in high power scenarios can pose risks to the device, the potential for extrapolation to the ITER Baseline Scenario (IBS).","We find that carefully designing a scenario matching currents proportionally to those of the IBS is far more important than plasma shape or profiles in accurately measuring an optimal correction current set."],"url":"http://arxiv.org/abs/2406.01824v1","category":"physics.plasm-ph"}
{"created":"2024-06-03 22:12:22","title":"Deep asymmetric mixture model for unsupervised cell segmentation","abstract":"Automated cell segmentation has become increasingly crucial for disease diagnosis and drug discovery, as manual delineation is excessively laborious and subjective. To address this issue with limited manual annotation, researchers have developed semi/unsupervised segmentation approaches. Among these approaches, the Deep Gaussian mixture model plays a vital role due to its capacity to facilitate complex data distributions. However, these models assume that the data follows symmetric normal distributions, which is inapplicable for data that is asymmetrically distributed. These models also obstacles weak generalization capacity and are sensitive to outliers. To address these issues, this paper presents a novel asymmetric mixture model for unsupervised cell segmentation. This asymmetric mixture model is built by aggregating certain multivariate Gaussian mixture models with log-likelihood and self-supervised-based optimization functions. The proposed asymmetric mixture model outperforms (nearly 2-30% gain in dice coefficient, p<0.05) the existing state-of-the-art unsupervised models on cell segmentation including the segment anything.","sentences":["Automated cell segmentation has become increasingly crucial for disease diagnosis and drug discovery, as manual delineation is excessively laborious and subjective.","To address this issue with limited manual annotation, researchers have developed semi/unsupervised segmentation approaches.","Among these approaches, the Deep Gaussian mixture model plays a vital role due to its capacity to facilitate complex data distributions.","However, these models assume that the data follows symmetric normal distributions, which is inapplicable for data that is asymmetrically distributed.","These models also obstacles weak generalization capacity and are sensitive to outliers.","To address these issues, this paper presents a novel asymmetric mixture model for unsupervised cell segmentation.","This asymmetric mixture model is built by aggregating certain multivariate Gaussian mixture models with log-likelihood and self-supervised-based optimization functions.","The proposed asymmetric mixture model outperforms (nearly 2-30% gain in dice coefficient, p<0.05) the existing state-of-the-art unsupervised models on cell segmentation including the segment anything."],"url":"http://arxiv.org/abs/2406.01815v1","category":"cs.CV"}
{"created":"2024-06-03 21:42:36","title":"The clustering of Lyman Alpha Emitting galaxies at z=2-3","abstract":"We measure the clustering of Lyman Alpha Emitting galaxies (LAEs) selected from the One-hundred-square-degree DECam Imaging in Narrowbands (ODIN) survey, with spectroscopic follow-up from Dark Energy Spectroscopic Instrument (DESI). We use DESI spectroscopy to optimize our selection and to constrain the interloper fraction and redshift distribution of our narrow-band selected sources. We select samples at z=2.45 and 3.1 in the COSMOS field with median Ly-alpha fluxes of 10^{-16}erg/s/cm2. Covariances and cosmological inferences are obtained from a series of mock catalogs built upon high-resolution N-body simulations that match the footprint, number density, redshift distribution and observed clustering of the sample. We find that both samples have a correlation length of r_0=3.0+/-0.2 Mpc/h. Within our fiducial cosmology these correspond to 3D number densities of 10^{-3}h3/Mpc3 and, from our mock catalogs, biases of 1.7 and 2.0 at z=2.45 and 3.1, respectively. We discuss the implications of these measurements for the use of LAEs as large-scale structure tracers for high-redshift cosmology.","sentences":["We measure the clustering of Lyman Alpha Emitting galaxies (LAEs) selected from the One-hundred-square-degree DECam Imaging in Narrowbands (ODIN) survey, with spectroscopic follow-up from Dark Energy Spectroscopic Instrument (DESI).","We use DESI spectroscopy to optimize our selection and to constrain the interloper fraction and redshift distribution of our narrow-band selected sources.","We select samples at z=2.45 and 3.1 in the COSMOS field with median Ly-alpha fluxes of 10^{-16}erg/s/cm2.","Covariances and cosmological inferences are obtained from a series of mock catalogs built upon high-resolution N-body simulations that match the footprint, number density, redshift distribution and observed clustering of the sample.","We find that both samples have a correlation length of r_0=3.0+/-0.2 Mpc/h. Within our fiducial cosmology these correspond to 3D number densities of 10^{-3}h3/Mpc3 and, from our mock catalogs, biases of 1.7 and 2.0 at z=2.45 and 3.1, respectively.","We discuss the implications of these measurements for the use of LAEs as large-scale structure tracers for high-redshift cosmology."],"url":"http://arxiv.org/abs/2406.01803v1","category":"astro-ph.CO"}
{"created":"2024-06-03 21:40:59","title":"Online Control in Population Dynamics","abstract":"The study of population dynamics originated with early sociological works (Malthus, 1872) but has since extended into many fields, including biology, epidemiology, evolutionary game theory, and economics. Most studies on population dynamics focus on the problem of prediction rather than control. Existing mathematical models for population control are often restricted to specific, noise-free dynamics, while real-world population changes can be complex and adversarial.   To address this gap, we propose a new framework based on the paradigm of online control. We first characterize a set of linear dynamical systems that can naturally model evolving populations. We then give an efficient gradient-based controller for these systems, with near-optimal regret bounds with respect to a broad class of linear policies. Our empirical evaluations demonstrate the effectiveness of the proposed algorithm for population control even in non-linear models such as SIR and replicator dynamics.","sentences":["The study of population dynamics originated with early sociological works (Malthus, 1872) but has since extended into many fields, including biology, epidemiology, evolutionary game theory, and economics.","Most studies on population dynamics focus on the problem of prediction rather than control.","Existing mathematical models for population control are often restricted to specific, noise-free dynamics, while real-world population changes can be complex and adversarial.   ","To address this gap, we propose a new framework based on the paradigm of online control.","We first characterize a set of linear dynamical systems that can naturally model evolving populations.","We then give an efficient gradient-based controller for these systems, with near-optimal regret bounds with respect to a broad class of linear policies.","Our empirical evaluations demonstrate the effectiveness of the proposed algorithm for population control even in non-linear models such as SIR and replicator dynamics."],"url":"http://arxiv.org/abs/2406.01799v1","category":"cs.LG"}
{"created":"2024-06-03 21:04:45","title":"Tumbling elimination induced by permeability: an experimental approach","abstract":"Archetypal falling behaviors of impervious objects are classified into four modes: fluttering, tumbling, steady descent and chaotic motion. The classical scenario predicts these behaviors to be affected by two dimensionless quantities: dimensionless inertia and Reynolds number. In this article we explore experimentally the effect of permeability and porosity on the falling regimes of porous plates. By drilling several hole distributions in rectangular plates, both permeability and porosity are varied systematically. We discover that the introduction of porosity affects the stability of the falling regimes eliminating tumbling. Using a phenomenological model we show that a decrease in circulation induced by the introduction of holes is the primary mechanism for stabilizing the plates' trajectories.","sentences":["Archetypal falling behaviors of impervious objects are classified into four modes: fluttering, tumbling, steady descent and chaotic motion.","The classical scenario predicts these behaviors to be affected by two dimensionless quantities: dimensionless inertia and Reynolds number.","In this article we explore experimentally the effect of permeability and porosity on the falling regimes of porous plates.","By drilling several hole distributions in rectangular plates, both permeability and porosity are varied systematically.","We discover that the introduction of porosity affects the stability of the falling regimes eliminating tumbling.","Using a phenomenological model we show that a decrease in circulation induced by the introduction of holes is the primary mechanism for stabilizing the plates' trajectories."],"url":"http://arxiv.org/abs/2406.01785v1","category":"physics.flu-dyn"}
{"created":"2024-06-03 20:23:38","title":"Provable Optimality of the Square-Tooth Atomic Frequency Comb Quantum Memory","abstract":"Atomic frequency comb (AFC) quantum memories are a promising technology for quantum repeater networks because they enable multi-mode, high-fidelity storage of photons with on-demand retrieval. The optimization of the retrieval efficiency of an AFC memory is important because it strongly impacts the entanglement generation rate in quantum networks. Despite initial theoretical analyses and recent experimental demonstrations, a rigorous proof of the universally optimal configuration for the highest AFC retrieval efficiency has not been presented. In this paper we offer a simple analytical proof which shows that the optimized square-tooth AFC provides the highest retrieval efficiency among all possible comb tooth shapes, under the physical constraint of maximal optical depth of an atomic ensemble. The optimality still holds even when the non-zero background absorption and the finite homogeneous broadening of atoms are considered. Our proof provides experimentalists with rigorous arguments how to create optimal AFC under realistic experimental conditions. Finally, we also identify other functional optimization problems where our proof technique is applicable, thus proving the optimality of the square function in more general scenarios.","sentences":["Atomic frequency comb (AFC) quantum memories are a promising technology for quantum repeater networks because they enable multi-mode, high-fidelity storage of photons with on-demand retrieval.","The optimization of the retrieval efficiency of an AFC memory is important because it strongly impacts the entanglement generation rate in quantum networks.","Despite initial theoretical analyses and recent experimental demonstrations, a rigorous proof of the universally optimal configuration for the highest AFC retrieval efficiency has not been presented.","In this paper we offer a simple analytical proof which shows that the optimized square-tooth AFC provides the highest retrieval efficiency among all possible comb tooth shapes, under the physical constraint of maximal optical depth of an atomic ensemble.","The optimality still holds even when the non-zero background absorption and the finite homogeneous broadening of atoms are considered.","Our proof provides experimentalists with rigorous arguments how to create optimal AFC under realistic experimental conditions.","Finally, we also identify other functional optimization problems where our proof technique is applicable, thus proving the optimality of the square function in more general scenarios."],"url":"http://arxiv.org/abs/2406.01769v1","category":"quant-ph"}
{"created":"2024-06-03 20:09:46","title":"Provably Feasible and Stable White-Box Trajectory Optimization","abstract":"We study the problem of Trajectory Optimization (TO) for a general class of stiff and constrained dynamic systems. We establish a set of mild assumptions, under which we show that TO converges numerically stably to a locally optimal and feasible solution up to arbitrary user-specified error tolerance. Our key observation is that all prior works use SQP as a black-box solver, where a TO problem is formulated as a Nonlinear Program (NLP) and the underlying SQP solver is not allowed to modify the NLP. Instead, we propose a white-box TO solver, where the SQP solver is informed with characteristics of the objective function and the dynamic system. It then uses these characteristics to derive approximate dynamic systems and customize the discretization schemes.","sentences":["We study the problem of Trajectory Optimization (TO) for a general class of stiff and constrained dynamic systems.","We establish a set of mild assumptions, under which we show that TO converges numerically stably to a locally optimal and feasible solution up to arbitrary user-specified error tolerance.","Our key observation is that all prior works use SQP as a black-box solver, where a TO problem is formulated as a Nonlinear Program (NLP) and the underlying SQP solver is not allowed to modify the NLP.","Instead, we propose a white-box TO solver, where the SQP solver is informed with characteristics of the objective function and the dynamic system.","It then uses these characteristics to derive approximate dynamic systems and customize the discretization schemes."],"url":"http://arxiv.org/abs/2406.01763v1","category":"math.OC"}
{"created":"2024-06-03 19:49:33","title":"On the completeness of several fortification-interdiction games in the Polynomial Hierarchy","abstract":"Fortification-interdiction games are tri-level adversarial games where two opponents act in succession to protect, disrupt and simply use an infrastructure for a specific purpose. Many such games have been formulated and tackled in the literature through specific algorithmic methods, however very few investigations exist on the completeness of such fortification problems in order to locate them rigorously in the polynomial hierarchy. We clarify the completeness status of several well-known fortification problems, such as the Tri-level Interdiction Knapsack Problem with unit fortification and attack weights, the Max-flow Interdiction Problem and Shortest Path Interdiction Problem with Fortification, the Multi-level Critical Node Problem with unit weights, as well as a well-studied electric grid defence planning problem. For all of these problems, we prove their completeness either for the $\\Sigma^p_2$ or the $\\Sigma^p_3$ class of the polynomial hierarchy. We also prove that the Multi-level Fortification-Interdiction Knapsack Problem with an arbitrary number of protection and interdiction rounds and unit fortification and attack weights is complete for any level of the polynomial hierarchy, therefore providing a useful basis for further attempts at proving the completeness of protection-interdiction games at any level of said hierarchy.","sentences":["Fortification-interdiction games are tri-level adversarial games where two opponents act in succession to protect, disrupt and simply use an infrastructure for a specific purpose.","Many such games have been formulated and tackled in the literature through specific algorithmic methods, however very few investigations exist on the completeness of such fortification problems in order to locate them rigorously in the polynomial hierarchy.","We clarify the completeness status of several well-known fortification problems, such as the Tri-level Interdiction Knapsack Problem with unit fortification and attack weights, the Max-flow Interdiction Problem and Shortest Path Interdiction Problem with Fortification, the Multi-level Critical Node Problem with unit weights, as well as a well-studied electric grid defence planning problem.","For all of these problems, we prove their completeness either for the $\\Sigma^p_2$ or the $\\Sigma^p_3$ class of the polynomial hierarchy.","We also prove that the Multi-level Fortification-Interdiction Knapsack Problem with an arbitrary number of protection and interdiction rounds and unit fortification and attack weights is complete for any level of the polynomial hierarchy, therefore providing a useful basis for further attempts at proving the completeness of protection-interdiction games at any level of said hierarchy."],"url":"http://arxiv.org/abs/2406.01756v1","category":"cs.CC"}
{"created":"2024-06-03 19:44:16","title":"Validating Automated Resonance Evaluation with Synthetic Data","abstract":"The integrity and precision of nuclear data are crucial for a broad spectrum of applications, from national security and nuclear reactor design to medical diagnostics, where the associated uncertainties can significantly impact outcomes. A substantial portion of uncertainty in nuclear data originates from the subjective biases in the evaluation process, a crucial phase in the nuclear data production pipeline. Recent advancements indicate that automation of certain routines can mitigate these biases, thereby standardizing the evaluation process, reducing uncertainty and enhancing reproducibility. This article contributes to developing a framework for automated evaluation techniques testing, emphasizing automated fitting methods that do not require the user to provide any prior information. This approach simplifies the process and reduces the manual effort needed in the initial evaluation stage. It highlights the capability of the framework to validate and optimize subroutines, targeting the performance analysis and optimization of the fitting procedure using high-fidelity synthetic data (labeled experimental data) and the concept of a fully controlled computational experiment. An error metric is introduced to provide a clear and intuitive measure of the fitting quality by quantifying the accuracy and performance across the specified energy. This metric sets a scale for comparison and optimization of routines or hyperparameter selection, improving the entire evaluation process methodology and increasing reproducibility and objectivity.","sentences":["The integrity and precision of nuclear data are crucial for a broad spectrum of applications, from national security and nuclear reactor design to medical diagnostics, where the associated uncertainties can significantly impact outcomes.","A substantial portion of uncertainty in nuclear data originates from the subjective biases in the evaluation process, a crucial phase in the nuclear data production pipeline.","Recent advancements indicate that automation of certain routines can mitigate these biases, thereby standardizing the evaluation process, reducing uncertainty and enhancing reproducibility.","This article contributes to developing a framework for automated evaluation techniques testing, emphasizing automated fitting methods that do not require the user to provide any prior information.","This approach simplifies the process and reduces the manual effort needed in the initial evaluation stage.","It highlights the capability of the framework to validate and optimize subroutines, targeting the performance analysis and optimization of the fitting procedure using high-fidelity synthetic data (labeled experimental data) and the concept of a fully controlled computational experiment.","An error metric is introduced to provide a clear and intuitive measure of the fitting quality by quantifying the accuracy and performance across the specified energy.","This metric sets a scale for comparison and optimization of routines or hyperparameter selection, improving the entire evaluation process methodology and increasing reproducibility and objectivity."],"url":"http://arxiv.org/abs/2406.01754v1","category":"physics.comp-ph"}
{"created":"2024-06-03 19:43:06","title":"Optimizing the Optimal Weighted Average: Efficient Distributed Sparse Classification","abstract":"While distributed training is often viewed as a solution to optimizing linear models on increasingly large datasets, inter-machine communication costs of popular distributed approaches can dominate as data dimensionality increases. Recent work on non-interactive algorithms shows that approximate solutions for linear models can be obtained efficiently with only a single round of communication among machines. However, this approximation often degenerates as the number of machines increases. In this paper, building on the recent optimal weighted average method, we introduce a new technique, ACOWA, that allows an extra round of communication to achieve noticeably better approximation quality with minor runtime increases. Results show that for sparse distributed logistic regression, ACOWA obtains solutions that are more faithful to the empirical risk minimizer and attain substantially higher accuracy than other distributed algorithms.","sentences":["While distributed training is often viewed as a solution to optimizing linear models on increasingly large datasets, inter-machine communication costs of popular distributed approaches can dominate as data dimensionality increases.","Recent work on non-interactive algorithms shows that approximate solutions for linear models can be obtained efficiently with only a single round of communication among machines.","However, this approximation often degenerates as the number of machines increases.","In this paper, building on the recent optimal weighted average method, we introduce a new technique, ACOWA, that allows an extra round of communication to achieve noticeably better approximation quality with minor runtime increases.","Results show that for sparse distributed logistic regression, ACOWA obtains solutions that are more faithful to the empirical risk minimizer and attain substantially higher accuracy than other distributed algorithms."],"url":"http://arxiv.org/abs/2406.01753v1","category":"cs.LG"}
{"created":"2024-06-03 19:31:58","title":"Shape matters: Understanding the effect of electrode geometry on cell resistance and chemo-mechanical stress","abstract":"Rechargeable batteries that incorporate shaped three-dimensional electrodes have been shown to have increased power and energy densities for a given footprint area when compared to a conventional geometry, i.e., a planar cathode and anode that sandwich an electrolyte. Electrodes can be shaped to enable a higher loading of active material, while keeping the ion transport distance small, however, the relationship between electrical and mechanical performance remains poorly understood. A variety of electrode shapes have been explored, where the electrodes are individually shaped or intertwined with one another. Advances in manufacturing and shape and topology optimization have made such designs a reality. In this paper, we explore sinusoidal half cells and interdigitated full cells. First, we use a simple electrostatics model to understand the cell resistance as a function of shape. We focus on low-temperature conditions, where the electrolyte conductivity decreases and the governing dimensionless parameters change. Next, we use a chemo-mechanics model to examine the stress concentrations that arise due to intercalation-driven volume expansion. We show that shaped electrodes provide a significant reduction in resistance, however, they result in unfavorable stress concentrations. Overall, we find that the fully interdigitated electrodes may provide the best balance with respect to this trade-off.","sentences":["Rechargeable batteries that incorporate shaped three-dimensional electrodes have been shown to have increased power and energy densities for a given footprint area when compared to a conventional geometry, i.e., a planar cathode and anode that sandwich an electrolyte.","Electrodes can be shaped to enable a higher loading of active material, while keeping the ion transport distance small, however, the relationship between electrical and mechanical performance remains poorly understood.","A variety of electrode shapes have been explored, where the electrodes are individually shaped or intertwined with one another.","Advances in manufacturing and shape and topology optimization have made such designs a reality.","In this paper, we explore sinusoidal half cells and interdigitated full cells.","First, we use a simple electrostatics model to understand the cell resistance as a function of shape.","We focus on low-temperature conditions, where the electrolyte conductivity decreases and the governing dimensionless parameters change.","Next, we use a chemo-mechanics model to examine the stress concentrations that arise due to intercalation-driven volume expansion.","We show that shaped electrodes provide a significant reduction in resistance, however, they result in unfavorable stress concentrations.","Overall, we find that the fully interdigitated electrodes may provide the best balance with respect to this trade-off."],"url":"http://arxiv.org/abs/2406.01748v1","category":"physics.chem-ph"}
{"created":"2024-06-03 19:08:01","title":"Quantum optimization using a 127-qubit gate-model IBM quantum computer can outperform quantum annealers for nontrivial binary optimization problems","abstract":"We introduce a comprehensive quantum solver for binary combinatorial optimization problems on gate-model quantum computers that outperforms any published alternative and consistently delivers correct solutions for problems with up to 127 qubits. We provide an overview of the internal workflow, describing the integration of a customized ansatz and variational parameter update strategy, efficient error suppression in hardware execution, and overhead-free post-processing to correct for bit-flip errors. We benchmark this solver on IBM quantum computers for several classically nontrivial unconstrained binary optimization problems -- the entire optimization is conducted on hardware with no use of classical simulation or prior knowledge of the solution. First, we demonstrate the ability to correctly solve Max-Cut instances for random regular graphs with a variety of densities using up to 120 qubits, where the graph topologies are not matched to device connectivity. Next, we apply the solver to higher-order binary optimization and successfully search for the ground state energy of a 127-qubit spin-glass model with linear, quadratic, and cubic interaction terms. Use of this new quantum solver increases the likelihood of finding the minimum energy by up to $\\sim1,500\\times$ relative to published results using a DWave annealer, and it can find the correct solution when the annealer fails. Furthermore, for both problem types, the Q-CTRL solver outperforms a heuristic local solver used to indicate the relative difficulty of the problems pursued. Overall, these results represent the largest quantum optimizations successfully solved on hardware to date, and demonstrate the first time a gate-model quantum computer has been able to outperform an annealer for a class of binary optimization problems.","sentences":["We introduce a comprehensive quantum solver for binary combinatorial optimization problems on gate-model quantum computers that outperforms any published alternative and consistently delivers correct solutions for problems with up to 127 qubits.","We provide an overview of the internal workflow, describing the integration of a customized ansatz and variational parameter update strategy, efficient error suppression in hardware execution, and overhead-free post-processing to correct for bit-flip errors.","We benchmark this solver on IBM quantum computers for several classically nontrivial unconstrained binary optimization problems -- the entire optimization is conducted on hardware with no use of classical simulation or prior knowledge of the solution.","First, we demonstrate the ability to correctly solve Max-Cut instances for random regular graphs with a variety of densities using up to 120 qubits, where the graph topologies are not matched to device connectivity.","Next, we apply the solver to higher-order binary optimization and successfully search for the ground state energy of a 127-qubit spin-glass model with linear, quadratic, and cubic interaction terms.","Use of this new quantum solver increases the likelihood of finding the minimum energy by up to $\\sim1,500\\times$ relative to published results using a DWave annealer, and it can find the correct solution when the annealer fails.","Furthermore, for both problem types, the Q-CTRL solver outperforms a heuristic local solver used to indicate the relative difficulty of the problems pursued.","Overall, these results represent the largest quantum optimizations successfully solved on hardware to date, and demonstrate the first time a gate-model quantum computer has been able to outperform an annealer for a class of binary optimization problems."],"url":"http://arxiv.org/abs/2406.01743v1","category":"quant-ph"}
{"created":"2024-06-03 18:53:28","title":"Dip-Bump Structure in Proton's Single Diffractive Dissociation at the Large Hadron Collider","abstract":"By extending the dipole Pomeron (DP) model, successful in describing elastic nucleon-nucleon scattering, to proton single diffractive dissociation (SD), we predict a dip-bump structure in the squared four-momentum transfer ($t$) distribution of proton's SD. Structures in the $t$ distribution of single diffractive dissociation are predicted around $t=-4$ GeV$^2$ at LHC energies in the range of 3 GeV$^2$ $\\lesssim|t|\\lesssim$ 7 GeV$^2$. Apart from the dependence on $s$ (total energy squared) and $t$ (squared momentum transfer), we predict also a dependence on missing masses. We include the minimum set of Regge trajectories, namely the Pomeron and the Odderon, indispensable at the LHC. Further generalization, e.g., by the inclusion of non-leading Regge trajectories, is straightforward. The present model contains two types of Regge trajectories: those connected with $t$-channel exchanges (the Pomeron, the Odderon, and non-leading (secondary) reggeons) appearing at small and moderate $-t$, where they are real and nearly linear, as well as direct-channel trajectories $\\alpha(M^2)$ related to missing masses. In this paper, we concentrate on structures in $t$ neglecting (for the time being) resonances in $M^2$.","sentences":["By extending the dipole Pomeron (DP) model, successful in describing elastic nucleon-nucleon scattering, to proton single diffractive dissociation (SD), we predict a dip-bump structure in the squared four-momentum transfer ($t$) distribution of proton's SD.","Structures in the $t$ distribution of single diffractive dissociation are predicted around $t=-4$ GeV$^2$ at LHC energies in the range of 3 GeV$^2$ $\\lesssim|t|\\lesssim$ 7 GeV$^2$. Apart from the dependence on $s$ (total energy squared) and $t$ (squared momentum transfer), we predict also a dependence on missing masses.","We include the minimum set of Regge trajectories, namely the Pomeron and the Odderon, indispensable at the LHC.","Further generalization, e.g., by the inclusion of non-leading Regge trajectories, is straightforward.","The present model contains two types of Regge trajectories: those connected with $t$-channel exchanges (the Pomeron, the Odderon, and non-leading (secondary) reggeons) appearing at small and moderate $-t$, where they are real and nearly linear, as well as direct-channel trajectories $\\alpha(M^2)$ related to missing masses.","In this paper, we concentrate on structures in $t$ neglecting (for the time being) resonances in $M^2$."],"url":"http://arxiv.org/abs/2406.01735v1","category":"hep-ph"}
{"created":"2024-06-03 18:41:51","title":"Predictive Model and Optimization of Micromixers Geometry using Gaussian Process with Uncertainty Quantification and Genetic Algorithm","abstract":"Microfluidic devices are gaining attention for their small size and ability to handle tiny fluid volumes. Mixing fluids efficiently at this scale, known as micromixing, is crucial. This article builds upon previous research by introducing a novel optimization approach in microfluidics, combining Computational Fluid Dynamics (CFD) with Machine Learning (ML) techniques. The research focuses on improving global optimization while reducing computational expenses. It draws inspiration from a Y-type micromixer, initially featuring cylindrical grooves on the main channel's surface and internal obstructions. Simulations, conducted using OpenFOAM software, evaluate the impact of circular obstructions on mixing percentage and pressure drop, considering variations in obstruction diameter and offset. A Gaussian Process (GP) was utilized to model the data, providing model uncertainty. Thus, this study optimizes geometries by using genetic algorithm (GA) and least-square optimization based on the reduced order model provided by GP. Results align with previous research, showing that medium-sized obstructions (137 mm diameter, 10 mm offset) near the channel wall are optimal. This approach not only provides efficient microfluidic optimization with uncertainty quantification but also highlights the effectiveness of combining CFD and ML techniques in achieving desired outcomes.","sentences":["Microfluidic devices are gaining attention for their small size and ability to handle tiny fluid volumes.","Mixing fluids efficiently at this scale, known as micromixing, is crucial.","This article builds upon previous research by introducing a novel optimization approach in microfluidics, combining Computational Fluid Dynamics (CFD) with Machine Learning (ML) techniques.","The research focuses on improving global optimization while reducing computational expenses.","It draws inspiration from a Y-type micromixer, initially featuring cylindrical grooves on the main channel's surface and internal obstructions.","Simulations, conducted using OpenFOAM software, evaluate the impact of circular obstructions on mixing percentage and pressure drop, considering variations in obstruction diameter and offset.","A Gaussian Process (GP) was utilized to model the data, providing model uncertainty.","Thus, this study optimizes geometries by using genetic algorithm (GA) and least-square optimization based on the reduced order model provided by GP.","Results align with previous research, showing that medium-sized obstructions (137 mm diameter, 10 mm offset) near the channel wall are optimal.","This approach not only provides efficient microfluidic optimization with uncertainty quantification but also highlights the effectiveness of combining CFD and ML techniques in achieving desired outcomes."],"url":"http://arxiv.org/abs/2406.01728v1","category":"physics.flu-dyn"}
{"created":"2024-06-03 18:37:38","title":"A General 3D Road Model for Motorcycle Racing","abstract":"We present a novel control-oriented motorcycle model and use it for computing racing lines on a nonplanar racetrack. The proposed model combines recent advances in nonplanar road models with the dynamics of motorcycles. Our approach considers the additional camber degree of freedom of the motorcycle body with a simplified model of the rider and front steering fork bodies. We demonstrate the effectiveness of our model by computing minimum-time racing trajectories on a nonplanar racetrack.","sentences":["We present a novel control-oriented motorcycle model and use it for computing racing lines on a nonplanar racetrack.","The proposed model combines recent advances in nonplanar road models with the dynamics of motorcycles.","Our approach considers the additional camber degree of freedom of the motorcycle body with a simplified model of the rider and front steering fork bodies.","We demonstrate the effectiveness of our model by computing minimum-time racing trajectories on a nonplanar racetrack."],"url":"http://arxiv.org/abs/2406.01726v1","category":"cs.RO"}
{"created":"2024-06-03 18:29:27","title":"Wasserstein Distributionally Robust Control and State Estimation for Partially Observable Linear Systems","abstract":"This paper presents a novel Wasserstein distributionally robust control and state estimation algorithm for partially observable linear stochastic systems, where the probability distributions of disturbances and measurement noises are unknown. Our method consists of the control and state estimation phases to handle distributional ambiguities of system disturbances and measurement noises, respectively. Leveraging tools from modern distributionally robust optimization, we consider an approximation of the control problem with an arbitrary nominal distribution and derive its closed-form optimal solution. We show that the separation principle holds, thereby allowing the state estimator to be designed separately. A novel distributionally robust Kalman filter is then proposed as an optimal solution to the state estimation problem with Gaussian nominal distributions. Our key contribution is the combination of distributionally robust control and state estimation into a unified algorithm. This is achieved by formulating a tractable semidefinite programming problem that iteratively determines the worst-case covariance matrices of all uncertainties, leading to a scalable and efficient algorithm. Our method is also shown to enjoy a guaranteed cost property as well as a probabilistic out-of-sample performance guarantee. The results of our numerical experiments demonstrate the performance and computational efficiency of the proposed method.","sentences":["This paper presents a novel Wasserstein distributionally robust control and state estimation algorithm for partially observable linear stochastic systems, where the probability distributions of disturbances and measurement noises are unknown.","Our method consists of the control and state estimation phases to handle distributional ambiguities of system disturbances and measurement noises, respectively.","Leveraging tools from modern distributionally robust optimization, we consider an approximation of the control problem with an arbitrary nominal distribution and derive its closed-form optimal solution.","We show that the separation principle holds, thereby allowing the state estimator to be designed separately.","A novel distributionally robust Kalman filter is then proposed as an optimal solution to the state estimation problem with Gaussian nominal distributions.","Our key contribution is the combination of distributionally robust control and state estimation into a unified algorithm.","This is achieved by formulating a tractable semidefinite programming problem that iteratively determines the worst-case covariance matrices of all uncertainties, leading to a scalable and efficient algorithm.","Our method is also shown to enjoy a guaranteed cost property as well as a probabilistic out-of-sample performance guarantee.","The results of our numerical experiments demonstrate the performance and computational efficiency of the proposed method."],"url":"http://arxiv.org/abs/2406.01723v1","category":"eess.SY"}
{"created":"2024-06-03 18:25:30","title":"Quantifying nonclassicality of mixed Fock states","abstract":"Nonclassical states of bosonic modes are important resources for quantum-enhanced technologies. Yet, quantifying nonclassicality of these states, in particular mixed states, can be a challenge. Here we present results of quantifying the nonclassicality of a bosonic mode in a mixed Fock state via the operational resource theory (ORT) measure [W. Ge, K. Jacobs, S. Asiri, M. Foss-Feig, and M. S. Zubairy, Phys. Rev. Res. 2, 023400 (2020)], which relates nonclassicality to metrological advantage. Generally speaking, evaluating the ORT measure for mixed states is challenging, since it involves finding a convex roof. However, we show that our problem can be reduced to a linear programming problem. By analyzing the results of numerical optimization, we are able to extract exact, analytical results for the case where three or four neighboring Fock states have nonzero population. Interestingly, we find that such a mode can be in distinct phases, depending on the populations. Lastly, we demonstrate how our method is generalizable to density matrices of higher ranks. Our findings suggests a viable method for evaluating nonclassicality of arbitrary mixed bosonic states and potentially for solving other convex roof optimization problems.","sentences":["Nonclassical states of bosonic modes are important resources for quantum-enhanced technologies.","Yet, quantifying nonclassicality of these states, in particular mixed states, can be a challenge.","Here we present results of quantifying the nonclassicality of a bosonic mode in a mixed Fock state via the operational resource theory (ORT) measure [W. Ge, K. Jacobs, S. Asiri, M. Foss-Feig, and M. S. Zubairy, Phys.","Rev. Res. 2, 023400 (2020)], which relates nonclassicality to metrological advantage.","Generally speaking, evaluating the ORT measure for mixed states is challenging, since it involves finding a convex roof.","However, we show that our problem can be reduced to a linear programming problem.","By analyzing the results of numerical optimization, we are able to extract exact, analytical results for the case where three or four neighboring Fock states have nonzero population.","Interestingly, we find that such a mode can be in distinct phases, depending on the populations.","Lastly, we demonstrate how our method is generalizable to density matrices of higher ranks.","Our findings suggests a viable method for evaluating nonclassicality of arbitrary mixed bosonic states and potentially for solving other convex roof optimization problems."],"url":"http://arxiv.org/abs/2406.01717v1","category":"quant-ph"}
{"created":"2024-06-03 18:02:42","title":"Asymptotic synchronization of Kuramoto oscillators with time delay and non-universal interaction","abstract":"We study the emergence of synchronization in the Kuramoto model on a digraph in the presence of time delays. Assuming the digraph is strongly connected, we first establish a uniform bound on the phase diameter and subsequently prove the asymptotic frequency synchronization of the oscillators under suitable assumptions on the initial configurations. In the case of an all-to-all connection, we obtain an exponential synchronization estimate. Additionally, we present numerical simulations, providing further insights into the synchronization and oscillatory behaviors of the oscillator frequencies depending on the network structure and the magnitude of the time delay.","sentences":["We study the emergence of synchronization in the Kuramoto model on a digraph in the presence of time delays.","Assuming the digraph is strongly connected, we first establish a uniform bound on the phase diameter and subsequently prove the asymptotic frequency synchronization of the oscillators under suitable assumptions on the initial configurations.","In the case of an all-to-all connection, we obtain an exponential synchronization estimate.","Additionally, we present numerical simulations, providing further insights into the synchronization and oscillatory behaviors of the oscillator frequencies depending on the network structure and the magnitude of the time delay."],"url":"http://arxiv.org/abs/2406.01703v1","category":"math.OC"}
{"created":"2024-06-03 18:00:02","title":"Intermediate-mass black hole binary parameter estimation with next-generation ground-based detector networks","abstract":"Astrophysical scenarios for the formation and evolution of intermediate-mass black holes (IMBHs) in the mass range $10^2 M_\\odot \\lesssim M \\lesssim 10^6 M_\\odot$ remain uncertain, but future ground-based gravitational-wave (GW) interferometers will probe the lower end of the IMBH mass range. We study the detectability of IMBH binary mergers and the measurability of their parameters with next-generation ground-based detector networks consisting of various combinations of Cosmic Explorer (CE) and Einstein Telescope (ET) interferometers. We find that, for binaries with component masses $m_{1,2}\\sim 1000\\,M_\\odot$, an optimal 3-detector network can constrain the masses with errors $\\lesssim 0.1\\%$ ($\\lesssim 1\\%$) at $z=0.5$ ($z=2$), and the source redshift can be measured with percent-level accuracy or better at $z\\lesssim 2$. The redshift of lighter binaries ($m_{1,2}\\lesssim 300\\,M_\\odot$) can still be measured with $O(10)\\%$ accuracy even at $z=10$. Binaries with $z\\lesssim 0.5$ can be localized within $1\\,\\rm{deg}^2$ for $m_{1,2}\\lesssim 1000\\,M_\\odot$, and within $0.1\\,\\rm{deg}^2$ for comparable mass systems. The sky localization is good enough that it may be possible to cross-correlate GW searches with galaxy catalogs and to search for electromagnetic counterparts to IMBH mergers. We also point out that the low-frequency sensitivity of the detectors is crucial for IMBH detection and parameter estimation. It will be interesting to use our results in conjunction with population synthesis codes to constrain astrophysical IMBH formation models.","sentences":["Astrophysical scenarios for the formation and evolution of intermediate-mass black holes (IMBHs) in the mass range $10^2 M_\\odot \\lesssim M \\lesssim 10^6 M_\\odot$ remain uncertain, but future ground-based gravitational-wave (GW) interferometers will probe the lower end of the IMBH mass range.","We study the detectability of IMBH binary mergers and the measurability of their parameters with next-generation ground-based detector networks consisting of various combinations of Cosmic Explorer (CE) and Einstein Telescope (ET) interferometers.","We find that, for binaries with component masses $m_{1,2}\\sim 1000\\,M_\\odot$, an optimal 3-detector network can constrain the masses with errors $\\lesssim 0.1\\%$ ($\\lesssim 1\\%$) at $z=0.5$ ($z=2$), and the source redshift can be measured with percent-level accuracy or better at $z\\lesssim 2$.","The redshift of lighter binaries ($m_{1,2}\\lesssim 300\\,M_\\odot$) can still be measured with $O(10)\\%$ accuracy even at $z=10$. Binaries with $z\\lesssim 0.5$ can be localized within $1\\,\\rm{deg}^2$ for $m_{1,2}\\lesssim 1000\\,M_\\odot$, and within $0.1\\,\\rm{deg}^2$ for comparable mass systems.","The sky localization is good enough that it may be possible to cross-correlate GW searches with galaxy catalogs and to search for electromagnetic counterparts to IMBH mergers.","We also point out that the low-frequency sensitivity of the detectors is crucial for IMBH detection and parameter estimation.","It will be interesting to use our results in conjunction with population synthesis codes to constrain astrophysical IMBH formation models."],"url":"http://arxiv.org/abs/2406.01687v1","category":"gr-qc"}
{"created":"2024-06-03 18:00:00","title":"A generalized statistical model for fits to parton distributions","abstract":"Parton distribution functions (PDFs) form an essential part of particle physics calculations. Currently, the most precise predictions for these non-perturbative functions are generated through fits to global data. A problem that several PDF fitting groups encounter is the presence of tension in data sets that appear to pull the fits in different directions. In other words, the best fit depends on the choice of data set. Several methods to capture the uncertainty in PDFs in presence of seemingly inconsistent fits have been proposed and are currently in use. These methods are important to ensure that uncertainty in PDFs are not underestimated. Here we propose a novel method for estimating the uncertainty by introducing a generalized statistical model inspired by unsupervised machine learning techniques, namely the Gaussian Mixture Model (GMM). Using a toy model of PDFs, we demonstrate how the GMM can be used to faithfully reconstruct the likelihood associated with PDF fits, which can in turn be used to accurately determine the uncertainty on PDFs, especially in presence of tension in the fitted data sets. We further show how this statistical model reduces to the usual chi-squared likelihood function for a consistent data set and provide measures to optimize the number of Gaussians in the GMM.","sentences":["Parton distribution functions (PDFs) form an essential part of particle physics calculations.","Currently, the most precise predictions for these non-perturbative functions are generated through fits to global data.","A problem that several PDF fitting groups encounter is the presence of tension in data sets that appear to pull the fits in different directions.","In other words, the best fit depends on the choice of data set.","Several methods to capture the uncertainty in PDFs in presence of seemingly inconsistent fits have been proposed and are currently in use.","These methods are important to ensure that uncertainty in PDFs are not underestimated.","Here we propose a novel method for estimating the uncertainty by introducing a generalized statistical model inspired by unsupervised machine learning techniques, namely the Gaussian Mixture Model (GMM).","Using a toy model of PDFs, we demonstrate how the GMM can be used to faithfully reconstruct the likelihood associated with PDF fits, which can in turn be used to accurately determine the uncertainty on PDFs, especially in presence of tension in the fitted data sets.","We further show how this statistical model reduces to the usual chi-squared likelihood function for a consistent data set and provide measures to optimize the number of Gaussians in the GMM."],"url":"http://arxiv.org/abs/2406.01664v1","category":"hep-ph"}
{"created":"2024-06-03 17:59:22","title":"Stochastic Control with Signatures","abstract":"This paper proposes to parameterize open loop controls in stochastic optimal control problems via suitable classes of functionals depending on the driver's path signature, a concept adopted from rough path integration theory. We rigorously prove that these controls are dense in the class of progressively measurable controls and use rough path methods to establish suitable conditions for stability of the controlled dynamics and target functional. These results pave the way for Monte Carlo methods to stochastic optimal control for generic target functionals and dynamics. We discuss the rather versatile numerical algorithms for computing approximately optimal controls and verify their accurateness in benchmark problems from Mathematical Finance.","sentences":["This paper proposes to parameterize open loop controls in stochastic optimal control problems via suitable classes of functionals depending on the driver's path signature, a concept adopted from rough path integration theory.","We rigorously prove that these controls are dense in the class of progressively measurable controls and use rough path methods to establish suitable conditions for stability of the controlled dynamics and target functional.","These results pave the way for Monte Carlo methods to stochastic optimal control for generic target functionals and dynamics.","We discuss the rather versatile numerical algorithms for computing approximately optimal controls and verify their accurateness in benchmark problems from Mathematical Finance."],"url":"http://arxiv.org/abs/2406.01585v1","category":"math.OC"}
{"created":"2024-06-04 17:59:04","title":"Dark photon limits from patchy dark screening of the cosmic microwave background","abstract":"Dark photons that kinetically mix with the Standard Model photon give rise to new spectral anisotropies (patchy dark screening) in the cosmic microwave background (CMB) due to conversion of photons to dark photons within large-scale structure. We utilize predictions for this patchy dark screening signal to provide the tightest constraints to date on the dark photon kinetic mixing parameter ($\\varepsilon \\lesssim 4\\times 10^{-8}$ (95\\% confidence level)) over the mass range $10^{-13} \\,\\, {\\rm eV} \\lesssim m_{{A^\\prime}} \\lesssim 10^{-11}$ eV, almost an order of magnitude stronger than previous limits, by applying state-of-the-art component separation techniques to the cross-correlation of $\\textit{Planck}$ CMB and $\\textit{unWISE}$ galaxy survey data.","sentences":["Dark photons that kinetically mix with the Standard Model photon give rise to new spectral anisotropies (patchy dark screening) in the cosmic microwave background (CMB) due to conversion of photons to dark photons within large-scale structure.","We utilize predictions for this patchy dark screening signal to provide the tightest constraints to date on the dark photon kinetic mixing parameter ($\\varepsilon \\lesssim 4\\times 10^{-8}$ (95\\% confidence level)) over the mass range $10^{-13} \\,\\, {\\rm eV} \\lesssim m_{{A^\\prime}} \\lesssim 10^{-11}$ eV, almost an order of magnitude stronger than previous limits, by applying state-of-the-art component separation techniques to the cross-correlation of $\\textit{Planck}$ CMB and $\\textit{unWISE}$ galaxy survey data."],"url":"http://arxiv.org/abs/2406.02546v1","category":"hep-ph"}
{"created":"2024-06-04 17:18:20","title":"Electronic properties of magnetic semiconductor $\\textrm{CuMnO}_{2}$ : a first principles study","abstract":"Geometrically frustrated magnetic semiconductor $\\textrm{CuMnO}_{2}$ has potential applications as photo-catalyst, in photochemical cells and multi-ferroic devices. Electronic band structure in the antiferromagnetic and ferromagnetic phases of $\\textrm{CuMnO}_{2}$ were calculated using first principle density functional theory (DFT) as implemented in VASP. Electronic band structure in the antiferromagnetic state shows indirect band gap ($\\sim 0.53$ eV) where as in the ferromagnetic state it shows half-metallic state with 100\\% spin polarization. The half-metallic state arises due to \\textit{double exchange} mechanism. In the half-metallic state the density of states for the up spin channel shows asymmetric power law behaviour near the Fermi level while the down spin channel shows fully gapped behaviour. The calculated magnetic moment of Mn atom in the ferromagnetic (3.70 $\\mu_{B}$) and antiferromagnetic (3.57 $\\mu_{B}$) states are consistent with experimental values. Our calculation predicts potential application of $\\textrm{CuMnO}_{2}$ in spintronic devices especially in the ferromagnetic state, as a spin injector for spin valves in spintronic devices.","sentences":["Geometrically frustrated magnetic semiconductor $\\textrm{CuMnO}_{2}$ has potential applications as photo-catalyst, in photochemical cells and multi-ferroic devices.","Electronic band structure in the antiferromagnetic and ferromagnetic phases of $\\textrm{CuMnO}_{2}$ were calculated using first principle density functional theory (DFT) as implemented in VASP.","Electronic band structure in the antiferromagnetic state shows indirect band gap ($\\sim 0.53$ eV) where as in the ferromagnetic state it shows half-metallic state with 100\\% spin polarization.","The half-metallic state arises due to \\textit{double exchange} mechanism.","In the half-metallic state the density of states for the up spin channel shows asymmetric power law behaviour near the Fermi level while the down spin channel shows fully gapped behaviour.","The calculated magnetic moment of Mn atom in the ferromagnetic (3.70 $\\mu_{B}$) and antiferromagnetic (3.57 $\\mu_{B}$) states are consistent with experimental values.","Our calculation predicts potential application of $\\textrm{CuMnO}_{2}$ in spintronic devices especially in the ferromagnetic state, as a spin injector for spin valves in spintronic devices."],"url":"http://arxiv.org/abs/2406.02499v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 17:16:32","title":"The story of SN 2021aatd -- a peculiar 1987A-like supernova with an early-phase luminosity excess","abstract":"There is a growing number of peculiar events that cannot be assigned to any of the main supernova (SN) classes. SN 1987A and a handful of similar objects, thought to be explosive outcomes of blue supergiant stars, belong to them: while their spectra closely resemble those of H-rich (IIP) SNe, their light-curve (LC) evolution is very different. Here we present the detailed photometric and spectroscopic analysis of SN 2021aatd, a peculiar Type II explosion: while its early-time evolution resembles that of the slowly evolving, double-peaked SN 2020faa (however, at a lower luminosity scale), after $\\sim$40 days, its LC shape becomes similar to that of SN 1987A-like explosions. Beyond comparing LCs, color curves, and spectra of SN 2021aatd to that of SNe 2020faa, 1987A, and of other objects, we compare the observed spectra with our own SYN++ models and with the outputs of published radiative transfer models. We also modeled the pseudo-bolometric LCs of SNe 2021aatd and 1987A assuming a two-component (core+shell) ejecta, and involving the rotational energy of a newborn magnetar in addition to radioactive decay. We find that both the photometric and spectroscopic evolution of SN 2021aatd can be well described with the explosion of a $\\sim$15 $M_\\odot$ blue supergiant star. Nevertheless, SN 2021aatd shows higher temperatures and weaker Na ID and Ba II 6142 A lines than SN 1987A, which is reminiscent of rather to IIP-like atmospheres. With the applied two-component ejecta model (counting with both decay and magnetar energy), we can successfully describe the bolometric LC of SN 2021aatd, including the first $\\sim$40-day long phase showing an excess compared to 87A-like SNe but being strikingly similar to that of the long-lived SN 2020faa. Nevertheless, finding a unified model that also explains the LCs of more luminous events (like SN 2020faa) is still a matter of concern.","sentences":["There is a growing number of peculiar events that cannot be assigned to any of the main supernova (SN) classes.","SN 1987A and a handful of similar objects, thought to be explosive outcomes of blue supergiant stars, belong to them: while their spectra closely resemble those of H-rich (IIP) SNe, their light-curve (LC) evolution is very different.","Here we present the detailed photometric and spectroscopic analysis of SN 2021aatd, a peculiar Type II explosion: while its early-time evolution resembles that of the slowly evolving, double-peaked SN 2020faa (however, at a lower luminosity scale), after $\\sim$40 days, its LC shape becomes similar to that of SN 1987A-like explosions.","Beyond comparing LCs, color curves, and spectra of SN 2021aatd to that of SNe 2020faa, 1987A, and of other objects, we compare the observed spectra with our own SYN++ models and with the outputs of published radiative transfer models.","We also modeled the pseudo-bolometric LCs of SNe 2021aatd and 1987A assuming a two-component (core+shell) ejecta, and involving the rotational energy of a newborn magnetar in addition to radioactive decay.","We find that both the photometric and spectroscopic evolution of SN 2021aatd can be well described with the explosion of a $\\sim$15 $M_\\odot$ blue supergiant star.","Nevertheless, SN 2021aatd shows higher temperatures and weaker Na ID and Ba II 6142","A lines than SN 1987A, which is reminiscent of rather to IIP-like atmospheres.","With the applied two-component ejecta model (counting with both decay and magnetar energy), we can successfully describe the bolometric LC of SN 2021aatd, including the first $\\sim$40-day long phase showing an excess compared to 87A-like SNe but being strikingly similar to that of the long-lived SN 2020faa.","Nevertheless, finding a unified model that also explains the LCs of more luminous events (like SN 2020faa) is still a matter of concern."],"url":"http://arxiv.org/abs/2406.02498v1","category":"astro-ph.SR"}
{"created":"2024-06-04 17:08:30","title":"Velocity Scanning Tomography for Room-Temperature Quantum Simulation","abstract":"Quantum simulation offers an analog approach for exploring exotic quantum phenomena using controllable platforms, typically necessitating ultracold temperatures to maintain the quantum coherence. Superradiance lattices (SLs) have been harnessed to simulate coherent topological physics at room temperature, but the thermal motion of atoms remains a notable challenge in accurately measuring the physical quantities. To overcome this obstacle, we invent and validate a velocity scanning tomography technique to discern the responses of atoms with different velocities, allowing cold-atom spectroscopic resolution within room-temperature SLs. By comparing absorption spectra with and without atoms moving at specific velocities, we can derive the Wannier-Stark ladders of the SL across various effective static electric fields, their strengths being proportional to the atomic velocities. We extract the Zak phase of the SL by monitoring the ladder frequency shift as a function of the atomic velocity, effectively demonstrating the topological winding of the energy bands. Our research signifies the feasibility of room-temperature quantum simulation and facilitates their applications in quantum information processing.","sentences":["Quantum simulation offers an analog approach for exploring exotic quantum phenomena using controllable platforms, typically necessitating ultracold temperatures to maintain the quantum coherence.","Superradiance lattices (SLs) have been harnessed to simulate coherent topological physics at room temperature, but the thermal motion of atoms remains a notable challenge in accurately measuring the physical quantities.","To overcome this obstacle, we invent and validate a velocity scanning tomography technique to discern the responses of atoms with different velocities, allowing cold-atom spectroscopic resolution within room-temperature SLs.","By comparing absorption spectra with and without atoms moving at specific velocities, we can derive the Wannier-Stark ladders of the SL across various effective static electric fields, their strengths being proportional to the atomic velocities.","We extract the Zak phase of the SL by monitoring the ladder frequency shift as a function of the atomic velocity, effectively demonstrating the topological winding of the energy bands.","Our research signifies the feasibility of room-temperature quantum simulation and facilitates their applications in quantum information processing."],"url":"http://arxiv.org/abs/2406.02494v1","category":"quant-ph"}
{"created":"2024-06-04 16:42:14","title":"Electromagnetic corrections in hadronic tau decays","abstract":"We briefly review electromagnetic radiative corrections in semileptonic tau decays and their main applications.","sentences":["We briefly review electromagnetic radiative corrections in semileptonic tau decays and their main applications."],"url":"http://arxiv.org/abs/2406.02471v1","category":"hep-ph"}
{"created":"2024-06-04 16:07:37","title":"Spinor-helicity calculation of the $g^* g^* \\to q\\overline{q} V^*$ amplitude at the tree level","abstract":"We compute amplitudes for the process $g^* g^* \\to q \\overline q V^*$ (two virtual gluons into a quark, antiquark and a boson) at the tree level using the spinor-helicity formalism. The resulting analytic expressions are much shorter than squared amplitudes obtained using trace methods. Our results can be used to expedite numerical calculations in phenomenological studies of the Drell-Yan process in high energy factorization framework.","sentences":["We compute amplitudes for the process $g^* g^* \\to q \\overline q V^*$ (two virtual gluons into a quark, antiquark and a boson) at the tree level using the spinor-helicity formalism.","The resulting analytic expressions are much shorter than squared amplitudes obtained using trace methods.","Our results can be used to expedite numerical calculations in phenomenological studies of the Drell-Yan process in high energy factorization framework."],"url":"http://arxiv.org/abs/2406.02445v1","category":"hep-ph"}
{"created":"2024-06-04 16:03:10","title":"Probing the Scalar WIMP-Pion Coupling with the first LUX-ZEPLIN data","abstract":"Weakly interacting massive particles (WIMPs) may interact with a virtual pion that is exchanged between nucleons. This interaction channel is important to consider in models where the spin-independent isoscalar channel is suppressed. Using data from the first science run of the LUX-ZEPLIN dark matter experiment, containing 60 live days of data in a 5.5~tonne fiducial mass of liquid xenon, we report the results on a search for WIMP-pion interactions. We observe no significant excess and set an upper limit of $1.5\\times10^{-46}$~cm$^2$ at a 90\\% confidence level for a WIMP mass of 33~GeV/c$^2$ for this interaction.","sentences":["Weakly interacting massive particles (WIMPs) may interact with a virtual pion that is exchanged between nucleons.","This interaction channel is important to consider in models where the spin-independent isoscalar channel is suppressed.","Using data from the first science run of the LUX-ZEPLIN dark matter experiment, containing 60 live days of data in a 5.5~tonne fiducial mass of liquid xenon, we report the results on a search for WIMP-pion interactions.","We observe no significant excess and set an upper limit of $1.5\\times10^{-46}$~cm$^2$ at a 90\\% confidence level for a WIMP mass of 33~GeV/c$^2$ for this interaction."],"url":"http://arxiv.org/abs/2406.02441v1","category":"hep-ex"}
{"created":"2024-06-04 15:26:06","title":"21cmSense v2: A modular, open-source 21cm sensitivity calculator","abstract":"The 21cm line of neutral hydrogen is a powerful probe of the high-redshift universe (Cosmic Dawn and the Epoch of Reionization), with an unprecedented potential to inform us about key processes of early galaxy formation, the first stars and even cosmology and structure formation, via intensity mapping. It is the subject of a number of current and upcoming low-frequency radio experiments. This paper presents 21cmSense v2.0, which is a Python package that provides a modular framework for calculating the sensitivity of these experiments, in order to enhance the process of their design and forecasting their power for parameter inference. Version 2.0 of 21cmSense has been re-written from the ground up to be more modular and extensible than its venerable predecessor (Pober et al., 2013, 2014), and to provide a more user-friendly interface. The package is freely available both to use and contribute towards at https://github.com/rasg-affiliates/21cmSense.","sentences":["The 21cm line of neutral hydrogen is a powerful probe of the high-redshift universe (Cosmic Dawn and the Epoch of Reionization), with an unprecedented potential to inform us about key processes of early galaxy formation, the first stars and even cosmology and structure formation, via intensity mapping.","It is the subject of a number of current and upcoming low-frequency radio experiments.","This paper presents 21cmSense v2.0, which is a Python package that provides a modular framework for calculating the sensitivity of these experiments, in order to enhance the process of their design and forecasting their power for parameter inference.","Version 2.0 of 21cmSense has been re-written from the ground up to be more modular and extensible than its venerable predecessor (Pober et al., 2013, 2014), and to provide a more user-friendly interface.","The package is freely available both to use and contribute towards at https://github.com/rasg-affiliates/21cmSense."],"url":"http://arxiv.org/abs/2406.02415v1","category":"astro-ph.CO"}
{"created":"2024-06-04 15:11:49","title":"One-arm Probabilities for Metric Graph Gaussian Free Fields below and at the Critical Dimension","abstract":"For the critical level-set of the Gaussian free field on the metric graph of $\\mathbb Z^d$, we consider the one-arm probability $\\theta_d(N)$, i.e., the probability that the boundary of a box of side length $2N$ is connected to the center. We prove that $\\theta_d(N)$ is $O(N^{-\\frac{d}{2}+1})$ for $3\\le d\\le 5$, and is $N^{-2+o(1)}$ for $d=6$. Our upper bounds match the lower bounds in a previous work by Ding and Wirth up to a constant factor for $3\\le d\\le 5$, and match the exponent therein for $d=6$. Combined with our previous result that $\\theta_d(N) \\asymp N^{-2}$ for $d>6$, this seems to present the first percolation model whose one-arm probabilities are essentially completely understood in all dimensions. In particular, these results fully confirm Werner's conjectures (2021) on the one-arm exponents:   \\begin{equation*}   \\text{(1) for}\\ 3\\le d<d_c=6,\\ \\theta_d(N)=N^{-\\frac{d}{2}+o(1)};\\ \\text{(2) for}\\ d>d_c,\\ \\theta_d(N)=N^{-2+o(1)}.   \\end{equation*}   Prior to our work, Drewitz, Pr\\'evost and Rodriguez obtained upper bounds for $d\\in \\{3, 4\\}$, which are very sharp although lose some diverging factors. In the same work, they conjectured that $\\theta_{d_c}(N) = N^{-2+o(1)}$, which is now established. In addition, in a recent concurrent work, Drewitz, Pr\\'evost and Rodriguez independently obtained the up-to-constant upper bound for $d=3$.","sentences":["For the critical level-set of the Gaussian free field on the metric graph of $\\mathbb Z^d$, we consider the one-arm probability $\\theta_d(N)$, i.e., the probability that the boundary of a box of side length $2N$ is connected to the center.","We prove that $\\theta_d(N)$ is $O(N^{-\\frac{d}{2}+1})$ for $3\\le d\\le 5$, and is $N^{-2+o(1)}$ for $d=6$. Our upper bounds match the lower bounds in a previous work by Ding and Wirth up to a constant factor for $3\\le d\\le 5$, and match the exponent therein for $d=6$. Combined with our previous result that $\\theta_d(N) \\asymp N^{-2}$ for $d>6$, this seems to present the first percolation model whose one-arm probabilities are essentially completely understood in all dimensions.","In particular, these results fully confirm Werner's conjectures (2021) on the one-arm exponents:   \\begin{equation*}   \\text{(1)","for}\\ 3\\le d<d_c=6,\\ \\theta_d(N)=N^{-\\frac{d}{2}+o(1)};\\ \\text{(2)","for}\\ d>d_c,\\ \\theta_d(N)=N^{-2+o(1)}.   ","\\end{equation*}   Prior to our work, Drewitz, Pr\\'evost and Rodriguez obtained upper bounds for $d\\in \\{3, 4\\}$, which are very sharp although lose some diverging factors.","In the same work, they conjectured that $\\theta_{d_c}(N) = N^{-2+o(1)}$, which is now established.","In addition, in a recent concurrent work, Drewitz, Pr\\'evost and Rodriguez independently obtained the up-to-constant upper bound for $d=3$."],"url":"http://arxiv.org/abs/2406.02397v1","category":"math.PR"}
{"created":"2024-06-04 14:43:00","title":"Probing the zero energy shell wave functions of triangular graphene quantum dots with broken sublattice symmetry using a localized impurity","abstract":"We present here a method of probing the wave functions of a degenerate shell in a triangular graphene quantum dot, triangulene, using a localized substitutional impurity. We demonstrate its applicability to the example of aza-triangulenes. Using the analytical solution for degenerate states of an all-carbon triangulene as a basis for a triangulene containing a nitrogen impurity, we predict the structure of the zero energy shell in the presence of this impurity. We show that the impurity allows probing of the wave functions of a degenerate shell on a carbon site where it is located. We confirm our predictions by a comparison with the tight-binding and ab-initio calculation as well as with experiment.","sentences":["We present here a method of probing the wave functions of a degenerate shell in a triangular graphene quantum dot, triangulene, using a localized substitutional impurity.","We demonstrate its applicability to the example of aza-triangulenes.","Using the analytical solution for degenerate states of an all-carbon triangulene as a basis for a triangulene containing a nitrogen impurity, we predict the structure of the zero energy shell in the presence of this impurity.","We show that the impurity allows probing of the wave functions of a degenerate shell on a carbon site where it is located.","We confirm our predictions by a comparison with the tight-binding and ab-initio calculation as well as with experiment."],"url":"http://arxiv.org/abs/2406.02364v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-04 14:19:28","title":"Spectral representation in Klein space: simplifying celestial leaf amplitudes","abstract":"In this paper, we explore the spectral representation in Klein space which is the split $(2,2)$ signature flat spacetime. The Klein space can be foliated into Lorentzian $\\mathrm{AdS}_3 /\\mathbb{Z}$ slices, and its identity resolution has both continuous and discrete parts. We calculate the identity resolution, and the plancherel measure in these slices. Using the foliation of Klein space into the slices, and the identity resolution, and the plancherel measure in each slices, we compute the spectral representation of the massive bulk-to-bulk propagator in Klein space. It can be expressed as the sum of product of two massive (or tachyonic) conformal primary wavefunctions, with both continuous and discrete parts, and sharing a common boundary coordinate. An interesting point in Klein space is that since the identity resolution has both discrete and continuous parts, a new type of conformal primary wavefunction naturally arises for the massive (or tachyonic) case. For the conformal primary wavefunctions, both the discrete and continuous parts involve integrating over the common boundary coordinate and the real (or imaginary) mass. In the discrete part, the conformal dimension is summed, whereas in the continuous part, it is integrated. The spectral representation in Klein space is a computational tool to derive conformal block expansions for celestial amplitude in Klein space and its building blocks called celestial leaf amplitudes, by integrating the particle interaction vertex over a single slice of foliation.","sentences":["In this paper, we explore the spectral representation in Klein space which is the split $(2,2)$ signature flat spacetime.","The Klein space can be foliated into Lorentzian $\\mathrm{AdS}_3 /\\mathbb{Z}$ slices, and its identity resolution has both continuous and discrete parts.","We calculate the identity resolution, and the plancherel measure in these slices.","Using the foliation of Klein space into the slices, and the identity resolution, and the plancherel measure in each slices, we compute the spectral representation of the massive bulk-to-bulk propagator in Klein space.","It can be expressed as the sum of product of two massive (or tachyonic) conformal primary wavefunctions, with both continuous and discrete parts, and sharing a common boundary coordinate.","An interesting point in Klein space is that since the identity resolution has both discrete and continuous parts, a new type of conformal primary wavefunction naturally arises for the massive (or tachyonic) case.","For the conformal primary wavefunctions, both the discrete and continuous parts involve integrating over the common boundary coordinate and the real (or imaginary) mass.","In the discrete part, the conformal dimension is summed, whereas in the continuous part, it is integrated.","The spectral representation in Klein space is a computational tool to derive conformal block expansions for celestial amplitude in Klein space and its building blocks called celestial leaf amplitudes, by integrating the particle interaction vertex over a single slice of foliation."],"url":"http://arxiv.org/abs/2406.02342v1","category":"hep-th"}
{"created":"2024-06-04 12:54:34","title":"Constraining P and T Violating Forces with Chiral Molecules","abstract":"New sources of parity and time reversal violation are predicted by well motivated extensions of the Standard Model and can be effectively probed by precision spectroscopy of atoms and molecules. Chiral molecules have distinguished enantiomers which are related by parity transformation. Thus, they are promising candidates to search for parity violation at molecular scales, yet to be observed. In this work, we show that precision spectroscopy of the hyperfine structure of chiral molecules is sensitive to new physics sources of parity and time reversal violation. In particular, such a study can be sensitive to regions unexplored by terrestial experiments of a new chiral spin-1 particle that couples to nucleons. We explore the potential to hunt for time reversal violation in chiral molecules and show that it can be a complementary measurement to other probes. We assess the feasibility of such hyperfine metrology and project the sensitivity in CHDBrI$^+$.","sentences":["New sources of parity and time reversal violation are predicted by well motivated extensions of the Standard Model and can be effectively probed by precision spectroscopy of atoms and molecules.","Chiral molecules have distinguished enantiomers which are related by parity transformation.","Thus, they are promising candidates to search for parity violation at molecular scales, yet to be observed.","In this work, we show that precision spectroscopy of the hyperfine structure of chiral molecules is sensitive to new physics sources of parity and time reversal violation.","In particular, such a study can be sensitive to regions unexplored by terrestial experiments of a new chiral spin-1 particle that couples to nucleons.","We explore the potential to hunt for time reversal violation in chiral molecules and show that it can be a complementary measurement to other probes.","We assess the feasibility of such hyperfine metrology and project the sensitivity in CHDBrI$^+$."],"url":"http://arxiv.org/abs/2406.02281v1","category":"hep-ph"}
{"created":"2024-06-04 12:53:43","title":"From Chromospheric Evaporation to Coronal Rain: An Investigation of the Mass and Energy Cycle of a Flare","abstract":"Chromospheric evaporation (CE) and coronal rain (CR) represent two crucial phenomena encompassing the circulation of mass and energy during solar flares. While CE marks the start of the hot inflow into the flaring loop, CR marks the end, indicating the outflow in the form of cool and dense condensations. With \\textit{IRIS} and \\textit{AIA/SDO}, we examine and compare the evolution, dynamics, morphology, and energetics of the CR and CE during a C2.1 flare. The CE is directly observed in imaging and spectra in the \\ion{Fe}{XXI} line with \\textit{IRIS} and in the \\ion{Fe}{XVIII} line of AIA, with upward average total speeds of $138\\pm[35]~$km~s$^{-1}$ and a temperature of $[9.03\\pm3.28]\\times10^{6}$~K. An explosive to gentle CE transition is observed, with an apparent reduction in turbulence. From quiescent to gradual flare phase, the amount and density of CR increases by a factor of $\\approx4.4$ and 6, respectively. The rain's velocity increases by a 1.4, in agreement with gas pressure drag. In contrast, the clump widths variation is negligible. The location and morphology of CE match closely those of the rain showers, with similar CE sub-structure to the rain strands, reflecting fundamental scales of mass and energy transport. We obtain a CR outflow mass three times larger than the CE inflow mass, suggesting the presence of unresolved CE, perhaps at higher temperatures. The CR energy corresponds to half that of the CE. These results suggest an essential role of coronal rain in the mass-energy cycle of a flare.","sentences":["Chromospheric evaporation (CE) and coronal rain (CR) represent two crucial phenomena encompassing the circulation of mass and energy during solar flares.","While CE marks the start of the hot inflow into the flaring loop, CR marks the end, indicating the outflow in the form of cool and dense condensations.","With \\textit{IRIS} and \\textit{AIA/SDO}, we examine and compare the evolution, dynamics, morphology, and energetics of the CR and CE during a C2.1 flare.","The CE is directly observed in imaging and spectra in the \\ion{Fe}{XXI} line with \\textit{IRIS} and in the \\ion{Fe}{XVIII} line of AIA, with upward average total speeds of $138\\pm[35]~$km~s$^{-1}$ and a temperature of $[9.03\\pm3.28]\\times10^{6}$~K. An explosive to gentle CE transition is observed, with an apparent reduction in turbulence.","From quiescent to gradual flare phase, the amount and density of CR increases by a factor of $\\approx4.4$ and 6, respectively.","The rain's velocity increases by a 1.4, in agreement with gas pressure drag.","In contrast, the clump widths variation is negligible.","The location and morphology of CE match closely those of the rain showers, with similar CE sub-structure to the rain strands, reflecting fundamental scales of mass and energy transport.","We obtain a CR outflow mass three times larger than the CE inflow mass, suggesting the presence of unresolved CE, perhaps at higher temperatures.","The CR energy corresponds to half that of the CE.","These results suggest an essential role of coronal rain in the mass-energy cycle of a flare."],"url":"http://arxiv.org/abs/2406.02280v1","category":"astro-ph.SR"}
{"created":"2024-06-04 12:48:19","title":"Hydrodynamic simulations of cool stellar atmospheres with \\texttt{MANCHA}","abstract":"Aims. Our aim is to test how different binning strategies previously studied in one-dimensional models perform in three-dimensional radiative hydrodynamic simulations of stellar atmospheres. Methods. Realistic box-in-a-star simulations of the near-surface convection and photosphere of three spectral types (G2V, K0V, and M2V) were run with the MANCHA code with grey opacity. After reaching the stationary state, one snapshot of each of the three stellar simulations was used to compute the radiative energy exchange rate with grey opacity, opacity binned in four $\\tau$-bins, and opacity binned in 18 $\\{ \\tau, \\lambda \\}$-bins. These rates were compared with the ones computed with opacity distribution functions. Then, stellar simulations were run with grey, four-bin, and 18-bin opacities to see the impact of the opacity setup on the mean stratification of the temperature and its gradient after time evolution. Results. The simulations of main sequence cool stars with the MANCHA code are consistent with those in the literature. For the three stars, the radiative energy exchange rates computed with 18 bins are remarkably close to the ones computed with the opacity distribution functions. The rates computed with four bins are similar to the rates computed with 18 bins, and present a significant improvement with respect to the rates computed with the Rosseland opacity, especially above the stellar surface. The Rosseland mean can reproduce the proper rates in sub-surface layers, but produces large errors for the atmospheric layers of the G2V and K0V stars. In the case of the M2V star, the Rosseland mean fails even in sub-surface layers, owing to the importance of the contribution from molecular lines in the opacity, underestimated by the harmonic mean. Similar conclusions are reached studying the mean stratification of the temperature and its gradient after time evolution.","sentences":["Aims.","Our aim is to test how different binning strategies previously studied in one-dimensional models perform in three-dimensional radiative hydrodynamic simulations of stellar atmospheres.","Methods.","Realistic box-in-a-star simulations of the near-surface convection and photosphere of three spectral types (G2V, K0V, and M2V) were run with the MANCHA code with grey opacity.","After reaching the stationary state, one snapshot of each of the three stellar simulations was used to compute the radiative energy exchange rate with grey opacity, opacity binned in four $\\tau$-bins, and opacity binned in 18 $\\{ \\tau, \\lambda \\}$-bins.","These rates were compared with the ones computed with opacity distribution functions.","Then, stellar simulations were run with grey, four-bin, and 18-bin opacities to see the impact of the opacity setup on the mean stratification of the temperature and its gradient after time evolution.","Results.","The simulations of main sequence cool stars with the MANCHA code are consistent with those in the literature.","For the three stars, the radiative energy exchange rates computed with 18 bins are remarkably close to the ones computed with the opacity distribution functions.","The rates computed with four bins are similar to the rates computed with 18 bins, and present a significant improvement with respect to the rates computed with the Rosseland opacity, especially above the stellar surface.","The Rosseland mean can reproduce the proper rates in sub-surface layers, but produces large errors for the atmospheric layers of the G2V and K0V stars.","In the case of the M2V star, the Rosseland mean fails even in sub-surface layers, owing to the importance of the contribution from molecular lines in the opacity, underestimated by the harmonic mean.","Similar conclusions are reached studying the mean stratification of the temperature and its gradient after time evolution."],"url":"http://arxiv.org/abs/2406.02271v1","category":"astro-ph.SR"}
{"created":"2024-06-04 12:00:34","title":"Decentralized Physical Infrastructure Network (DePIN): Challenges and Opportunities","abstract":"The widespread use of the Internet has posed challenges to existing centralized physical infrastructure networks. Issues such as data privacy risks, service disruptions, and substantial expansion costs have emerged. To address these challenges, an innovative network architecture called Decentralized Physical Infrastructure Network (DePIN) has emerged. DePIN leverages blockchain technology to decentralize the control and management of physical devices, addressing limitations of traditional infrastructure network. This article provides a comprehensive exploration of DePIN, presenting its five-layer architecture, key design principles. Furthermore, it presents a detailed survey of the extant applications, operating mechanisms, and provides an in-depth analysis of market data pertaining to DePIN. Finally, it discusses a wide range of the open challenges faced by DePIN.","sentences":["The widespread use of the Internet has posed challenges to existing centralized physical infrastructure networks.","Issues such as data privacy risks, service disruptions, and substantial expansion costs have emerged.","To address these challenges, an innovative network architecture called Decentralized Physical Infrastructure Network (DePIN) has emerged.","DePIN leverages blockchain technology to decentralize the control and management of physical devices, addressing limitations of traditional infrastructure network.","This article provides a comprehensive exploration of DePIN, presenting its five-layer architecture, key design principles.","Furthermore, it presents a detailed survey of the extant applications, operating mechanisms, and provides an in-depth analysis of market data pertaining to DePIN.","Finally, it discusses a wide range of the open challenges faced by DePIN."],"url":"http://arxiv.org/abs/2406.02239v1","category":"cs.NI"}
{"created":"2024-06-04 11:45:30","title":"Constrained cosmological simulations of the Local Group using Bayesian hierarchical field-level inference","abstract":"We present a novel approach based on Bayesian field-level inference capable of resolving individual galaxies within the Local Group (LG), enabling detailed studies of its structure and formation via posterior simulations. We extend the Bayesian Origin Reconstruction from Galaxies (BORG) algorithm with a multi-resolution approach, allowing us to reach smaller mass scales and apply observational constraints based on LG galaxies. Our updated data model simultaneously accounts for observations of mass tracers within the dark haloes of the Milky Way (MW) and M31, their observed separation and relative velocity, and the quiet surrounding Hubble flow represented through the positions and velocities of galaxies at distances from one to four Mpc. Our approach delivers representative posterior samples of $\\Lambda$CDM realisations that are statistically and simultaneously consistent with all these observations, leading to significantly tighter mass constraints than found if the individual datasets are considered separately. In particular, we estimate the virial masses of the MW and M31 to be $\\log_{10}(M_{200c}/M_\\odot) = 12.07\\pm0.08$ and $12.33\\pm0.10$, respectively, their sum to be $\\log_{10}(\\Sigma M_{200c}/M_\\odot)= 12.52\\pm0.07$, and the enclosed mass within spheres of radius $R$ to be $\\log_{10}(M(R)/M_\\odot)= 12.71\\pm0.06$ and $12.96\\pm0.08$ for $R=1$ Mpc and 3 Mpc, respectively. The M31-MW orbit is nearly radial for most of our $\\Lambda$CDM LG's, and most lie in a dark matter sheet that aligns approximately with the Supergalactic Plane, even though the surrounding density field was not used explicitly as a constraint. The approximate simulations employed in our inference are accurately reproduced by high-fidelity structure formation simulations, demonstrating the potential for future high-resolution, full-physics $\\Lambda$CDM posterior simulations of LG look-alikes.","sentences":["We present a novel approach based on Bayesian field-level inference capable of resolving individual galaxies within the Local Group (LG), enabling detailed studies of its structure and formation via posterior simulations.","We extend the Bayesian Origin Reconstruction from Galaxies (BORG) algorithm with a multi-resolution approach, allowing us to reach smaller mass scales and apply observational constraints based on LG galaxies.","Our updated data model simultaneously accounts for observations of mass tracers within the dark haloes of the Milky Way (MW) and M31, their observed separation and relative velocity, and the quiet surrounding Hubble flow represented through the positions and velocities of galaxies at distances from one to four Mpc.","Our approach delivers representative posterior samples of $\\Lambda$CDM realisations that are statistically and simultaneously consistent with all these observations, leading to significantly tighter mass constraints than found if the individual datasets are considered separately.","In particular, we estimate the virial masses of the MW and M31 to be $\\log_{10}(M_{200c}/M_\\odot) = 12.07\\pm0.08$ and $12.33\\pm0.10$, respectively, their sum to be $\\log_{10}(\\Sigma M_{200c}/M_\\odot)= 12.52\\pm0.07$, and the enclosed mass within spheres of radius $R$ to be $\\log_{10}(M(R)/M_\\odot)= 12.71\\pm0.06$ and $12.96\\pm0.08$ for $R=1$ Mpc and 3 Mpc, respectively.","The M31-MW orbit is nearly radial for most of our $\\Lambda$CDM LG's, and most lie in a dark matter sheet that aligns approximately with the Supergalactic Plane, even though the surrounding density field was not used explicitly as a constraint.","The approximate simulations employed in our inference are accurately reproduced by high-fidelity structure formation simulations, demonstrating the potential for future high-resolution, full-physics $\\Lambda$CDM posterior simulations of LG look-alikes."],"url":"http://arxiv.org/abs/2406.02228v1","category":"astro-ph.GA"}
{"created":"2024-06-04 11:41:34","title":"Extracting the partonic structure of colorless exchanges at the Electron Ion Collider","abstract":"We investigate the determination of the partonic structure of colorless exchanges in deep inelastic diffractive $ep$ scattering at the Electron Ion Collider, using the standard decomposition into Pomeron and Reggeon contributions. We perform fits to simulated diffractive cross section pseudodata in four variables, including the momentum transfer $t$, to estimate the achievable precision on the Pomeron and Reggeon quark and gluon distributions. We analyze the influence of different cuts in the kinematic variables, beam energy configurations and luminosities, including a `first year' scenario. We conclude that the EIC will be able to constrain the partonic structure of the sub-leading Reggeon exchange with a precision comparable to that of the leading Pomeron exchange.","sentences":["We investigate the determination of the partonic structure of colorless exchanges in deep inelastic diffractive $ep$ scattering at the Electron Ion Collider, using the standard decomposition into Pomeron and Reggeon contributions.","We perform fits to simulated diffractive cross section pseudodata in four variables, including the momentum transfer $t$, to estimate the achievable precision on the Pomeron and Reggeon quark and gluon distributions.","We analyze the influence of different cuts in the kinematic variables, beam energy configurations and luminosities, including a `first year' scenario.","We conclude that the EIC will be able to constrain the partonic structure of the sub-leading Reggeon exchange with a precision comparable to that of the leading Pomeron exchange."],"url":"http://arxiv.org/abs/2406.02227v1","category":"hep-ph"}
{"created":"2024-06-04 09:46:38","title":"Quantum Statistical Effects on Warm Dark Matter and the Mass Constraint from the Cosmic Large Scale Structure","abstract":"The suppression of small-scale matter power spectrum is a distinct feature of Warm Dark Matter (WDM), which permits a constraint on the WDM mass from galaxy surveys. In the thermal relic WDM scenario, quantum statistical effects are not manifest. In a unified framework, we investigate the quantum statistical effects for a fermion case with a degenerate pressure and a boson case with a Bose-Einstein condensation (BEC). Compared to the thermal relic case, the degenerate fermion case only slightly lowers the mass bound while the boson case with a high initial BEC fraction ($\\gtrsim90\\%$) significantly lowers it. On the other hand, the BEC fraction drops during the relativistic-to-nonrelativistic transition and completely disappears if the initial fraction is below $\\sim64\\%$. Given the rising interest in resolving the late-time galaxy-scale problems with boson condensation, a question is posed on how a high initial BEC fraction can be dynamically created so that a DM condensed component remains today.","sentences":["The suppression of small-scale matter power spectrum is a distinct feature of Warm Dark Matter (WDM), which permits a constraint on the WDM mass from galaxy surveys.","In the thermal relic WDM scenario, quantum statistical effects are not manifest.","In a unified framework, we investigate the quantum statistical effects for a fermion case with a degenerate pressure and a boson case with a Bose-Einstein condensation (BEC).","Compared to the thermal relic case, the degenerate fermion case only slightly lowers the mass bound while the boson case with a high initial BEC fraction ($\\gtrsim90\\%$) significantly lowers it.","On the other hand, the BEC fraction drops during the relativistic-to-nonrelativistic transition and completely disappears if the initial fraction is below $\\sim64\\%$. Given the rising interest in resolving the late-time galaxy-scale problems with boson condensation, a question is posed on how a high initial BEC fraction can be dynamically created so that a DM condensed component remains today."],"url":"http://arxiv.org/abs/2406.02159v1","category":"astro-ph.CO"}
{"created":"2024-06-04 09:25:25","title":"Eikonal calculation of (p,3p) cross sections for neutron-rich nuclei","abstract":"In this work, we present the first, to our knowledge, theoretical description of two-proton removal reactions with proton target $(p,3p)$ for medium-mass nuclei at intermediate energies and present cross sections for the different bound states of the residual nucleus with two fewer protons. The description of the reaction assumes two sequential ``quasifree'' collisions between the target and removed protons and considers eikonal propagation in between. The formalism is applied to the reactions $^{12}\\mathrm{C}(p,3p)^{10}\\mathrm{Be}$, $^{28}\\mathrm{Mg}(p,3p)^{26}\\mathrm{Ne}$ and $^{54}\\mathrm{Ca}(p,3p)^{52}\\mathrm{Ar}$, finding reasonable agreement to experimental data for the $^{12}$C target and an overestimation of a factor $\\sim3$ for the more neutron-rich and $^{54}$Ca, which is similar to the results found in two-proton knockout experiments with $^9$Be and $^{12}$C targets.","sentences":["In this work, we present the first, to our knowledge, theoretical description of two-proton removal reactions with proton target $(p,3p)$ for medium-mass nuclei at intermediate energies and present cross sections for the different bound states of the residual nucleus with two fewer protons.","The description of the reaction assumes two sequential ``quasifree'' collisions between the target and removed protons and considers eikonal propagation in between.","The formalism is applied to the reactions $^{12}\\mathrm{C}(p,3p)^{10}\\mathrm{Be}$, $^{28}\\mathrm{Mg}(p,3p)^{26}\\mathrm{Ne}$ and $^{54}\\mathrm{Ca}(p,3p)^{52}\\mathrm{Ar}$, finding reasonable agreement to experimental data for the $^{12}$C target and an overestimation of a factor $\\sim3$ for the more neutron-rich and $^{54}$Ca, which is similar to the results found in two-proton knockout experiments with $^9$Be and $^{12}$C targets."],"url":"http://arxiv.org/abs/2406.02138v1","category":"nucl-th"}
{"created":"2024-06-04 09:18:35","title":"Canonical coordinates for Yang-Mills-Chern-Simons theory","abstract":"We consider the classical field theory of 2+1-dimensional Yang-Mills-Chern-Simons theory on an arbitrary spatial manifold. We first define a gauge covariant transverse electric field strength, which together with the gauge covariant scalar magnetic field strength can be taken as coordinates on the classical phase space. We then determine the Poisson-Dirac bracket and find that these coordinates are canonically conjugate to each other. The Hamiltonian is non-polynomial when expressed in terms of these coordinates, but can be expanded in a power series in the coupling constant with polynomial coefficients.","sentences":["We consider the classical field theory of 2+1-dimensional Yang-Mills-Chern-Simons theory on an arbitrary spatial manifold.","We first define a gauge covariant transverse electric field strength, which together with the gauge covariant scalar magnetic field strength can be taken as coordinates on the classical phase space.","We then determine the Poisson-Dirac bracket and find that these coordinates are canonically conjugate to each other.","The Hamiltonian is non-polynomial when expressed in terms of these coordinates, but can be expanded in a power series in the coupling constant with polynomial coefficients."],"url":"http://arxiv.org/abs/2406.02132v1","category":"hep-th"}
{"created":"2024-06-04 09:11:01","title":"On the non-uniqueness of the energy-momentum and spin currents","abstract":"The macroscopic energy-momentum and spin densities of relativistic spin hydrodynamics are obtained from the ensemble average of their respective microscopic definitions (quantum operators). These microscopic definitions suffer from ambiguities, meaning that, one may obtain different forms of symmetric energy-momentum tensor and spin tensor through pseudogauge transformations (or in other words Belinfante improvement procedure). However, this ambiguity can be removed if we obtain these currents using Noether's second theorem instead of widely used Noether's first theorem. In this article, we use Noether's second theorem to derive (symmetric) energy-momentum tensor and (antisymmetric) spin tensor without the need of pseudogauge transformations for free Dirac massive particles with spin one-half.","sentences":["The macroscopic energy-momentum and spin densities of relativistic spin hydrodynamics are obtained from the ensemble average of their respective microscopic definitions (quantum operators).","These microscopic definitions suffer from ambiguities, meaning that, one may obtain different forms of symmetric energy-momentum tensor and spin tensor through pseudogauge transformations (or in other words Belinfante improvement procedure).","However, this ambiguity can be removed if we obtain these currents using Noether's second theorem instead of widely used Noether's first theorem.","In this article, we use Noether's second theorem to derive (symmetric) energy-momentum tensor and (antisymmetric) spin tensor without the need of pseudogauge transformations for free Dirac massive particles with spin one-half."],"url":"http://arxiv.org/abs/2406.02127v1","category":"hep-th"}
{"created":"2024-06-04 08:36:21","title":"Euler-Heisenberg black hole surrounded by quintessence in the background of perfect fluid dark matter: Thermodynamics, Shadows and Quasinormal modes","abstract":"Current observations show that a significant fraction of the Universe is composed of dark energy and dark matter. In this paper, we investigate the simultaneous effects of these dark sectors on the Euler-Heisenberg black hole, using the quintessence matter field and perfect fluid to model them. In particular, we study the black hole's thermodynamics, shadows, and quasinormal modes, and discuss in detail how these properties change with relatively large or small dark sector components.","sentences":["Current observations show that a significant fraction of the Universe is composed of dark energy and dark matter.","In this paper, we investigate the simultaneous effects of these dark sectors on the Euler-Heisenberg black hole, using the quintessence matter field and perfect fluid to model them.","In particular, we study the black hole's thermodynamics, shadows, and quasinormal modes, and discuss in detail how these properties change with relatively large or small dark sector components."],"url":"http://arxiv.org/abs/2406.02109v1","category":"gr-qc"}
{"created":"2024-06-04 08:27:24","title":"The Benchmark Mode $\u03a9_c \\to \u03a9^-\u03c0^+$ and Its Related Processes","abstract":"The benchmark mode $\\Omega_c^0\\to \\Omega^- \\pi^+$, which receives purely factorization contribution, is of great importance among all the decay channels of $\\Omega_c^0$ decays. In this work, within the framework of non-relativistic quark model (NRQM), we calculate all the 6 baryon transition form factors involving $\\frac12 ^+\\to \\frac32 ^+$ decays. The absolute branching fractions of non-leptonic decays $\\Omega_c^0\\to \\Omega^- \\pi^+$, $\\Omega_c^0\\to \\Omega^- \\rho^+$ and $\\Omega_c^0\\to \\Xi^- \\pi^+$ as well as semi-leptonic decays $\\Omega_c^0\\to \\Omega^- \\ell^+ \\nu_{\\ell} \\; (\\ell=e,\\mu)$ are calculated although they cannot be measured directly by current experiment. Based on the prediction $\\mathcal{B}(\\Omega_c^0\\to\\Omega^-\\pi^+)=3.43\\%$ in our work, we further predict the ratios between interested modes and the benchmark mode, giving $R(\\Xi^-\\pi^+)=0.156$, $R(\\Omega^-\\rho^+)=5.33$, $R(\\Omega^- e^+\\nu_e)=1.18$ and $R(\\Omega^- \\mu^+\\nu_\\mu)=1.11$. The predictions on $\\Omega_c^0\\to \\Xi^-\\pi^+$ and $\\Omega_c^0\\to \\Omega^- e^+\\nu$ agree well with recent measured ratios reported by LHCb in 2023 and ALICE in 2024, respectively.","sentences":["The benchmark mode $\\Omega_c^0\\to \\Omega^- \\pi^+$, which receives purely factorization contribution, is of great importance among all the decay channels of $\\Omega_c^0$ decays.","In this work, within the framework of non-relativistic quark model (NRQM), we calculate all the 6 baryon transition form factors involving $\\frac12 ^+\\to \\frac32 ^+$ decays.","The absolute branching fractions of non-leptonic decays $\\Omega_c^0\\to \\Omega^- \\pi^+$, $\\Omega_c^0\\to \\Omega^- \\rho^+$ and $\\Omega_c^0\\to \\Xi^- \\pi^+$ as well as semi-leptonic decays $\\Omega_c^0\\to \\Omega^- \\ell^+ \\nu_{\\ell} \\; (\\ell=e,\\mu)$ are calculated although they cannot be measured directly by current experiment.","Based on the prediction $\\mathcal{B}(\\Omega_c^0\\to\\Omega^-\\pi^+)=3.43\\%$ in our work, we further predict the ratios between interested modes and the benchmark mode, giving $R(\\Xi^-\\pi^+)=0.156$, $R(\\Omega^-\\rho^+)=5.33$, $R(\\Omega^- e^+\\nu_e)=1.18$ and $R(\\Omega^- \\mu^+\\nu_\\mu)=1.11$. The predictions on $\\Omega_c^0\\to \\Xi^-\\pi^+$ and $\\Omega_c^0\\to \\Omega^- e^+\\nu$ agree well with recent measured ratios reported by LHCb in 2023 and ALICE in 2024, respectively."],"url":"http://arxiv.org/abs/2406.02097v1","category":"hep-ph"}
{"created":"2024-06-04 08:06:17","title":"Simultaneous spectropolarimetric observations in the H$\u03b1$ and Ca II 8662 \u00c5 lines of an active region","abstract":"We present spectropolarimetric observations of an active region recorded simultaneously in the H$\\alpha$ Ca II 8662 {\\AA} lines. The sunspot exhibits multiple structures, including a lightbridge and a region where Ca II 8662 {\\AA} line core is in emission. Correspondingly, the H$\\alpha$ line core image displays brightening in the emission region, with the spectral profiles showing elevated line cores. The stratification of the line-of-sight magnetic field is inferred through non-LTE multiline inversions of the Ca II 8662 {\\AA} line and the weak field approximation over the H$\\alpha$ line. The field strength inferred from the H$\\alpha$ line core is consistently smaller than that inferred from inversions at $\\log \\tau_{500}$ = $-$4.5. However, the study finds no correlation between the WFA over the core of the H$\\alpha$ line and that inferred from inversions at $\\log \\tau_{500}$ = $-$4.5. In regions exhibiting emission features, the morphology of the magnetic field at $\\log \\tau_{500}$ = $-$4.5 resembles that at $\\log \\tau_{500}$ = $-$1, with slightly higher or comparable field strengths. The magnetic field morphology inferred from the core of the H$\\alpha$ line is also similar to that inferred from the full spectral range of the H$\\alpha$ line in the emission region. The field strength inferred in the lightbridge at $\\log \\tau_{500}$ = $-$1 is smaller than the surrounding umbral regions and comparable at $\\log \\tau_{500}$ = $-$4.5. Similarly, the field strength inferred in the lightbridge from the WFA over the H$\\alpha$ line appears lower compared to the surrounding umbral regions.","sentences":["We present spectropolarimetric observations of an active region recorded simultaneously in the H$\\alpha$ Ca II 8662 {\\AA} lines.","The sunspot exhibits multiple structures, including a lightbridge and a region where Ca II 8662 {\\AA} line core is in emission.","Correspondingly, the H$\\alpha$ line core image displays brightening in the emission region, with the spectral profiles showing elevated line cores.","The stratification of the line-of-sight magnetic field is inferred through non-LTE multiline inversions of the Ca II 8662 {\\AA} line and the weak field approximation over the H$\\alpha$ line.","The field strength inferred from the H$\\alpha$ line core is consistently smaller than that inferred from inversions at $\\log \\tau_{500}$ = $-$4.5.","However, the study finds no correlation between the WFA over the core of the H$\\alpha$ line and that inferred from inversions at $\\log \\tau_{500}$ = $-$4.5.","In regions exhibiting emission features, the morphology of the magnetic field at $\\log \\tau_{500}$ = $-$4.5 resembles that at $\\log \\tau_{500}$ = $-$1, with slightly higher or comparable field strengths.","The magnetic field morphology inferred from the core of the H$\\alpha$ line is also similar to that inferred from the full spectral range of the H$\\alpha$ line in the emission region.","The field strength inferred in the lightbridge at $\\log \\tau_{500}$ = $-$1 is smaller than the surrounding umbral regions and comparable at $\\log \\tau_{500}$ = $-$4.5.","Similarly, the field strength inferred in the lightbridge from the WFA over the H$\\alpha$ line appears lower compared to the surrounding umbral regions."],"url":"http://arxiv.org/abs/2406.02083v1","category":"astro-ph.SR"}
{"created":"2024-06-04 07:55:37","title":"A Silicon Photonic 32-Input Coherent Combiner for Turbulence Mitigation in Free Space Optics Links","abstract":"A photonic integrated circuit (PIC) for the coherent combination of 32 input optical signals into a single output fiber is reported. The PIC was fabricated using a low-loss thick silicon-on-insulator (SOI) process and packaged with 32 input and 1 output fibers. The basic building block is a 2x2 Mach-Zehnder interferometer (MZI) with an external (to the MZI branches) and an internal thermal phase shifter, and a bandwidth in excess of 80 kHz. The PIC monolithically integrates 31 MZIs and 31 germanium photodetectors, and is suitable in principle for turbulence mitigation in LEO-ground and horizontal free space optics links. Improvements to the device for the coherent combination of 64 inputs and for the reduction of insertion losses are also discussed","sentences":["A photonic integrated circuit (PIC) for the coherent combination of 32 input optical signals into a single output fiber is reported.","The PIC was fabricated using a low-loss thick silicon-on-insulator (SOI) process and packaged with 32 input and 1 output fibers.","The basic building block is a 2x2 Mach-Zehnder interferometer (MZI) with an external (to the MZI branches) and an internal thermal phase shifter, and a bandwidth in excess of 80 kHz.","The PIC monolithically integrates 31 MZIs and 31 germanium photodetectors, and is suitable in principle for turbulence mitigation in LEO-ground and horizontal free space optics links.","Improvements to the device for the coherent combination of 64 inputs and for the reduction of insertion losses are also discussed"],"url":"http://arxiv.org/abs/2406.02076v1","category":"physics.optics"}
{"created":"2024-06-04 07:49:41","title":"Bertran\u010fs Theorem and the Double Copy of Relativistic Field Theories","abstract":"Which relativistic field theories give rise to Kepler dynamics in the two-body problem? We consider a class of Hamiltonians that is the unique relativistic extension of the Kepler problem preserving its so(4) algebra, and have orbits related through time reparametrisation to orbits of the original Kepler problem. For three explicit examples, we give a natural interpretation in terms of spin-0,-1 and -2 interacting field theories in 5D. These are organically connected via the classical double copy, which therefore preserves maximal superintegrability.","sentences":["Which relativistic field theories give rise to Kepler dynamics in the two-body problem?","We consider a class of Hamiltonians that is the unique relativistic extension of the Kepler problem preserving its so(4) algebra, and have orbits related through time reparametrisation to orbits of the original Kepler problem.","For three explicit examples, we give a natural interpretation in terms of spin-0,-1 and -2 interacting field theories in 5D. These are organically connected via the classical double copy, which therefore preserves maximal superintegrability."],"url":"http://arxiv.org/abs/2406.02067v1","category":"hep-th"}
{"created":"2024-06-04 07:33:09","title":"Matrix dissimilarities based on differences in moments and sparsity","abstract":"Generating a dissimilarity matrix is typically the first step in big data analysis. Although numerous methods exist, such as Euclidean distance, Minkowski distance, Manhattan distance, Bray Curtis dissimilarity, Jaccard similarity and Dice dissimilarity, it remains unclear which factors drive dissimilarity between groups. In this paper, we introduce an approach based on differences in moments and sparsity. We show that this method can delineate the key factors underlying group differences. For example, in biology, mean dissimilarity indicates differences driven by up down regulated gene expressions, standard deviation dissimilarity reflects the heterogeneity of response to treatment, and sparsity dissimilarity corresponds to differences prompted by the activation silence of genes. Through extensive reanalysis of genome, transcriptome, proteome, metabolome, immune profiling, microbiome, and social science datasets, we demonstrate insights not captured in previous studies. For instance, it shows that the sparsity dissimilarity is as effective as the mean dissimilarity in predicting the alleviation effects of a COVID 19 drug, suggesting that sparsity dissimilarity is highly meaningful.","sentences":["Generating a dissimilarity matrix is typically the first step in big data analysis.","Although numerous methods exist, such as Euclidean distance, Minkowski distance, Manhattan distance, Bray Curtis dissimilarity, Jaccard similarity and Dice dissimilarity, it remains unclear which factors drive dissimilarity between groups.","In this paper, we introduce an approach based on differences in moments and sparsity.","We show that this method can delineate the key factors underlying group differences.","For example, in biology, mean dissimilarity indicates differences driven by up down regulated gene expressions, standard deviation dissimilarity reflects the heterogeneity of response to treatment, and sparsity dissimilarity corresponds to differences prompted by the activation silence of genes.","Through extensive reanalysis of genome, transcriptome, proteome, metabolome, immune profiling, microbiome, and social science datasets, we demonstrate insights not captured in previous studies.","For instance, it shows that the sparsity dissimilarity is as effective as the mean dissimilarity in predicting the alleviation effects of a COVID 19 drug, suggesting that sparsity dissimilarity is highly meaningful."],"url":"http://arxiv.org/abs/2406.02051v1","category":"q-bio.QM"}
{"created":"2024-06-04 07:29:37","title":"Kinematic analysis of a parallel robot for minimally invasive surgery","abstract":"The paper presents the kinematic modelling for the coupled motion of a 6-DOF surgical parallel robot PARA-SILSROB which guides a mobile platform carrying the surgical instruments, and the actuators of the sub-modules which hold these tools. To increase the surgical procedure safety, a closed form solution for the kinematic model is derived and then, the forward and inverse kinematic models for the mobile orientation platform are obtained. The kinematic models are used in numerical simulations for the reorientation of the endoscopic camera, which imposes an automated compensatory motion from the active instruments' mod-ules.","sentences":["The paper presents the kinematic modelling for the coupled motion of a 6-DOF surgical parallel robot PARA-SILSROB which guides a mobile platform carrying the surgical instruments, and the actuators of the sub-modules which hold these tools.","To increase the surgical procedure safety, a closed form solution for the kinematic model is derived and then, the forward and inverse kinematic models for the mobile orientation platform are obtained.","The kinematic models are used in numerical simulations for the reorientation of the endoscopic camera, which imposes an automated compensatory motion from the active instruments' mod-ules."],"url":"http://arxiv.org/abs/2406.02047v1","category":"cs.RO"}
{"created":"2024-06-04 07:28:15","title":"Experimental single-photon quantum key distribution surpassing the fundamental coherent-state rate limit","abstract":"Single-photon sources are essential for quantum networks, enabling applications ranging from quantum key distribution (QKD) to the burgeoning quantum internet. Despite the remarkable advancements, the current reliance of QKD on attenuated coherent (laser) light sources has imposed a fundamental limit on the secret key rate (SKR). This constraint is primarily attributable to the scarcity of single-photon components within coherent light, confined by an inherent upper bound of 1/e. Here, we report high-rate QKD using a high-efficiency single-photon source, enabling an SKR transcending the fundamental rate limit of coherent light. We developed an on-demand, bright semiconductor quantum-dot single-photon source with an efficiency of 0.71(2), exceeding the inherent bound of coherent light by approximately 2.87 dB. Implementing narrow-bandwidth filtering and random polarization modulation, we conducted a field QKD trial over a 14.6(1.1)-dB-loss free-space urban channel, achieving an SKR of 0.00108 bits per pulse. This surpasses the practical limit of coherent-light-based QKD by 2.53 dB. Our findings conclusively demonstrate the superior performance of nanotechnology-based single-photon sources over coherent light for QKD applications, marking a pivotal stride towards the realization of a global quantum internet.","sentences":["Single-photon sources are essential for quantum networks, enabling applications ranging from quantum key distribution (QKD) to the burgeoning quantum internet.","Despite the remarkable advancements, the current reliance of QKD on attenuated coherent (laser) light sources has imposed a fundamental limit on the secret key rate (SKR).","This constraint is primarily attributable to the scarcity of single-photon components within coherent light, confined by an inherent upper bound of 1/e. Here, we report high-rate QKD using a high-efficiency single-photon source, enabling an SKR transcending the fundamental rate limit of coherent light.","We developed an on-demand, bright semiconductor quantum-dot single-photon source with an efficiency of 0.71(2), exceeding the inherent bound of coherent light by approximately 2.87 dB. Implementing narrow-bandwidth filtering and random polarization modulation, we conducted a field QKD trial over a 14.6(1.1)-dB-loss free-space urban channel, achieving an SKR of 0.00108 bits per pulse.","This surpasses the practical limit of coherent-light-based QKD by 2.53 dB. Our findings conclusively demonstrate the superior performance of nanotechnology-based single-photon sources over coherent light for QKD applications, marking a pivotal stride towards the realization of a global quantum internet."],"url":"http://arxiv.org/abs/2406.02045v1","category":"quant-ph"}
{"created":"2024-06-04 07:03:15","title":"First demonstration of a TES based cryogenic Li$_2$MoO$_4$detector for neutrinoless double beta decay search","abstract":"Cryogenic calorimetric experiments to search for neutrinoless double-beta decay ($0\\nu\\beta\\beta$) are highly competitive, scalable and versatile in isotope. The largest planned detector array, CUPID, is comprised of about 1500 individual Li$_2^{100}$MoO$_{4}$ detector modules with a further scale up envisioned for a follow up experiment (CUPID-1T). In this article, we present a novel detector concept targeting this second stage with a low impedance TES based readout for the Li$_2$MoO$_{4}$ absorber that is easily mass-produced and lends itself to a multiplexed readout. We present the detector design and results from a first prototype detector operated at the NEXUS shallow underground facility at Fermilab. The detector is a 2-cm-side cube with 21$\\,$g mass that is strongly thermally coupled to its readout chip to allow rise-times of $\\sim$0.5$\\,$ms. This design is more than one order of magnitude faster than present NTD based detectors and is hence expected to effectively mitigate backgrounds generated through the pile-up of two independent two neutrino decay events coinciding close in time. Together with a baseline resolution of 1.95$\\,$keV (FWHM) these performance parameters extrapolate to a background index from pile-up as low as $5\\cdot 10^{-6}\\,$counts/keV/kg/yr in CUPID size crystals. The detector was calibrated up to the MeV region showing sufficient dynamic range for $0\\nu\\beta\\beta$ searches. In combination with a SuperCDMS HVeV detector this setup also allowed us to perform a precision measurement of the scintillation time constants of Li$_2$MoO$_{4}$. The crystal showed a significant fast scintillation emission with O(10$\\,\\mu$s) time-scale, more than an order below the detector response of presently considered light detectors suggesting the possibility of further progress in pile-up rejection through better light detectors in the future.","sentences":["Cryogenic calorimetric experiments to search for neutrinoless double-beta decay ($0\\nu\\beta\\beta$) are highly competitive, scalable and versatile in isotope.","The largest planned detector array, CUPID, is comprised of about 1500 individual Li$_2^{100}$MoO$_{4}$ detector modules with a further scale up envisioned for a follow up experiment (CUPID-1T).","In this article, we present a novel detector concept targeting this second stage with a low impedance TES based readout for the Li$_2$MoO$_{4}$ absorber that is easily mass-produced and lends itself to a multiplexed readout.","We present the detector design and results from a first prototype detector operated at the NEXUS shallow underground facility at Fermilab.","The detector is a 2-cm-side cube with 21$\\,$g mass that is strongly thermally coupled to its readout chip to allow rise-times of $\\sim$0.5$\\,$ms.","This design is more than one order of magnitude faster than present NTD based detectors and is hence expected to effectively mitigate backgrounds generated through the pile-up of two independent two neutrino decay events coinciding close in time.","Together with a baseline resolution of 1.95$\\,$keV (FWHM) these performance parameters extrapolate to a background index from pile-up as low as $5\\cdot 10^{-6}\\,$counts/keV/kg/yr in CUPID size crystals.","The detector was calibrated up to the MeV region showing sufficient dynamic range for $0\\nu\\beta\\beta$ searches.","In combination with a SuperCDMS HVeV detector this setup also allowed us to perform a precision measurement of the scintillation time constants of Li$_2$MoO$_{4}$. The crystal showed a significant fast scintillation emission with O(10$\\,\\mu$s) time-scale, more than an order below the detector response of presently considered light detectors suggesting the possibility of further progress in pile-up rejection through better light detectors in the future."],"url":"http://arxiv.org/abs/2406.02025v1","category":"hep-ex"}
{"created":"2024-06-04 07:01:23","title":"Density functional Bogoliubov-de Gennes theory for superconductors implemented in the SIESTA code","abstract":"We present SIESTA-BdG, an implementation of the simultaneous solution of the Bogoliubov-de Gennes (BdG) and Density Functional Theory (DFT) problem in SIESTA, a first-principles method and code for material simulations which uses pseudopotentials and a localized basis set. This unified approach describes both conventional and unconventional superconducting states, and enables a description of inhomogeneous superconductors and heterostructures. We demonstrate the validity, accuracy, and efficiency of SIESTA-BdG by computing physically relevant quantities (superconducting charge density, band structure, superconducting gap features, density of states) for conventional singlet (Nb, Pb) and unconventional (FeSe) superconductors. We find excellent agreement with experiments and results obtained within the KKR-BdG computational framework. SIESTA-BdG forms the basis for modelling quantum transport in superconducting devices and including - in an approximate fashion - the superconducting DFT potential of Oliveira, Gross, and Kohn.","sentences":["We present SIESTA-BdG, an implementation of the simultaneous solution of the Bogoliubov-de Gennes (BdG) and Density Functional Theory (DFT) problem in SIESTA, a first-principles method and code for material simulations which uses pseudopotentials and a localized basis set.","This unified approach describes both conventional and unconventional superconducting states, and enables a description of inhomogeneous superconductors and heterostructures.","We demonstrate the validity, accuracy, and efficiency of SIESTA-BdG by computing physically relevant quantities (superconducting charge density, band structure, superconducting gap features, density of states) for conventional singlet (Nb, Pb) and unconventional (FeSe) superconductors.","We find excellent agreement with experiments and results obtained within the KKR-BdG computational framework.","SIESTA-BdG forms the basis for modelling quantum transport in superconducting devices and including - in an approximate fashion - the superconducting DFT potential of Oliveira, Gross, and Kohn."],"url":"http://arxiv.org/abs/2406.02022v1","category":"cond-mat.supr-con"}
{"created":"2024-06-04 06:58:58","title":"Uncorrelated estimations of $H_0$ redshift evolution from DESI baryon acoustic oscillation observations","abstract":"The Dark Energy Spectroscopic Instrumnet (DESI) collaboration recently released the first year data of baryon acoustic oscillations (BAOs). Basing on the five different tracers, the cosmological constraint shows a hint of deviation from the standard $\\Lambda$CDM model. In this letter, We combine the DESI BAOs with other cosmic probes to constrain the evolution of Hubble constant as a function of redshift in flat $\\Lambda$CDM model. The non-parametric method is used to estimate the value of Hubble constant at different redshift bins. The correlation among different bins are removed by diagonalizing the covariance matrix. The joint data sample demonstrate a decreasing trend of Hubble constant with a significance of $8.6 \\sigma$, which can naturally resolve the Hubble tension. It may be due to dynamical dark energy or modified gravity.","sentences":["The Dark Energy Spectroscopic Instrumnet (DESI) collaboration recently released the first year data of baryon acoustic oscillations (BAOs).","Basing on the five different tracers, the cosmological constraint shows a hint of deviation from the standard $\\Lambda$CDM model.","In this letter, We combine the DESI BAOs with other cosmic probes to constrain the evolution of Hubble constant as a function of redshift in flat $\\Lambda$CDM model.","The non-parametric method is used to estimate the value of Hubble constant at different redshift bins.","The correlation among different bins are removed by diagonalizing the covariance matrix.","The joint data sample demonstrate a decreasing trend of Hubble constant with a significance of $8.6 \\sigma$, which can naturally resolve the Hubble tension.","It may be due to dynamical dark energy or modified gravity."],"url":"http://arxiv.org/abs/2406.02019v1","category":"astro-ph.CO"}
{"created":"2024-06-04 06:42:54","title":"On Ramsey degrees, compactness and approximability","abstract":"One of the consequences of the Compactness Principle in structural Ramsey theory is that the small Ramsey degrees cannot exceed the corresponding big Ramsey degrees, thereby justifying the choice of adjectives. However, it is unclear what happens in the realm of dual Ramsey degrees due to the lack of the compactness argument that applies to that setting. In this paper we present a framework within which both ``direct'' and dual Ramsey statements can be stated and reasoned about in a uniform fashion. We introduce the notion of approximability which yields a general compactness argument powerful enough to prove statements about both ``direct'' and dual Ramsey phenomena. We conclude the paper with an application of the new strategies by generalizing Voigt's $\\star$-version of the Infinite Ramsey Theorem to a large class of relational structures and deriving a Ramsey statement for ``loose colorings'' of enumerated Fra\\\"{\\i}ss\\'{e} limits.","sentences":["One of the consequences of the Compactness Principle in structural Ramsey theory is that the small Ramsey degrees cannot exceed the corresponding big Ramsey degrees, thereby justifying the choice of adjectives.","However, it is unclear what happens in the realm of dual Ramsey degrees due to the lack of the compactness argument that applies to that setting.","In this paper we present a framework within which both ``direct'' and dual Ramsey statements can be stated and reasoned about in a uniform fashion.","We introduce the notion of approximability which yields a general compactness argument powerful enough to prove statements about both ``direct'' and dual Ramsey phenomena.","We conclude the paper with an application of the new strategies by generalizing Voigt's $\\star$-version of the Infinite Ramsey Theorem to a large class of relational structures and deriving a Ramsey statement for ``loose colorings'' of enumerated Fra\\\"{\\i}ss\\'{e} limits."],"url":"http://arxiv.org/abs/2406.02007v1","category":"math.LO"}
{"created":"2024-06-04 06:24:07","title":"3D Imaging of Complex Specular Surfaces by Fusing Polarimetric and Deflectometric Information","abstract":"Accurate and fast 3D imaging of specular surfaces still poses major challenges for state-of-the-art optical measurement principles. Frequently used methods, such as phase-measuring deflectometry (PMD) or shape-from-polarization (SfP), rely on strong assumptions about the measured objects, limiting their generalizability in broader application areas like medical imaging, industrial inspection, virtual reality, or cultural heritage analysis. In this paper, we introduce a measurement principle that utilizes a novel technique to effectively encode and decode the information contained in a light field reflected off a specular surface. We combine polarization cues from SfP with geometric information obtained from PMD to resolve all arising ambiguities in the 3D measurement. Moreover, our approach removes the unrealistic orthographic imaging assumption for SfP, which significantly improves the respective results. We showcase our new technique by demonstrating single-shot and multi-shot measurements on complex-shaped specular surfaces, displaying an evaluated accuracy of surface normals below $0.6^\\circ$.","sentences":["Accurate and fast 3D imaging of specular surfaces still poses major challenges for state-of-the-art optical measurement principles.","Frequently used methods, such as phase-measuring deflectometry (PMD) or shape-from-polarization (SfP), rely on strong assumptions about the measured objects, limiting their generalizability in broader application areas like medical imaging, industrial inspection, virtual reality, or cultural heritage analysis.","In this paper, we introduce a measurement principle that utilizes a novel technique to effectively encode and decode the information contained in a light field reflected off a specular surface.","We combine polarization cues from SfP with geometric information obtained from PMD to resolve all arising ambiguities in the 3D measurement.","Moreover, our approach removes the unrealistic orthographic imaging assumption for SfP, which significantly improves the respective results.","We showcase our new technique by demonstrating single-shot and multi-shot measurements on complex-shaped specular surfaces, displaying an evaluated accuracy of surface normals below $0.6^\\circ$."],"url":"http://arxiv.org/abs/2406.01994v1","category":"cs.CV"}
{"created":"2024-06-04 06:10:52","title":"50 GeV $\u03c0^-$ in, nothing out: a sensitive probe of invisible $\u03b7$ and $\u03b7'$ decays with NA64h","abstract":"We present the first results from a proof-of-concept search for dark sectors via invisible decays of pseudoscalar $\\eta$ and $\\eta'$ mesons in the NA64h experiment at the CERN SPS. Our novel technique uses the charge-exchange reaction of 50 GeV $\\pi^-$ on nuclei of an active target as the source of neutral mesons. The $\\eta, \\eta' \\to invisible$ events would exhibit themselves via a striking signature - the complete disappearance of the incoming beam energy in the detector. No evidence for such events has been found with $2.9\\times10^{9}$ pions on target accumulated during one day of data taking. This allows us to set a stringent limit on the branching ratio ${\\rm Br}(\\eta' \\to invisible) < 2.1 \\times 10^{-4}$ improving the current bound by a factor of $\\simeq3$. We also set a limit on ${\\rm Br}(\\eta \\to invisible) < 1.1 \\times 10^{-4}$ comparable with the existing one. These results demonstrate the great potential of our approach and provide clear guidance on how to enhance and extend the sensitivity for dark sector physics from future searches for invisible neutral meson decays.","sentences":["We present the first results from a proof-of-concept search for dark sectors via invisible decays of pseudoscalar $\\eta$ and $\\eta'$ mesons in the NA64h experiment at the CERN SPS.","Our novel technique uses the charge-exchange reaction of 50 GeV $\\pi^-$ on nuclei of an active target as the source of neutral mesons.","The $\\eta, \\eta' \\to invisible$ events would exhibit themselves via a striking signature - the complete disappearance of the incoming beam energy in the detector.","No evidence for such events has been found with $2.9\\times10^{9}$ pions on target accumulated during one day of data taking.","This allows us to set a stringent limit on the branching ratio ${\\rm Br}(\\eta' \\to invisible) <","2.1 \\times 10^{-4}$ improving the current bound by a factor of $\\simeq3$. We also set a limit on ${\\rm Br}(\\eta \\to invisible) < 1.1 \\times 10^{-4}$ comparable with the existing one.","These results demonstrate the great potential of our approach and provide clear guidance on how to enhance and extend the sensitivity for dark sector physics from future searches for invisible neutral meson decays."],"url":"http://arxiv.org/abs/2406.01990v1","category":"hep-ex"}
{"created":"2024-06-04 05:46:59","title":"Quantum colored strings in the hole-doped $t$-$J_z$ model","abstract":"The stripe phase, an intertwined order observed in high-temperature superconductors, is regarded as playing a key role in elucidating the underlying mechanism of superconductivity, especially in cuprates. Following Jan Zaanen's early scenario, the full-filled charge stripe can be taken as the interactive elastic quantum strings of holes, stabilized by $\\pi$-phase shifts between neighboring magnetic domains. However, this scenario is challenging to explain, particularly in terms of electron pairing, which necessitates hole pairs. In this work, we propose a new effective model for describing the stripe phase in the hole-doped $t$-$J_z$ model. With respect to the antiferromagnetic background, the model comprises three types of color-labeled point-defects coupling to {an effective} spin field. Comparing with numerical results from large-scale density matrix renormalization group (DMRG) simulations, we find semi-quantitative agreement in local hole density, magnetic moment, and the newly proposed spectrum features of the {effective} spin field. By systematically analyzing the hole-density distribution and the scaling of groundstate energy at different system sizes, we determine the effective core radius and the effective hopping amplitude of the quantum string. Furthermore, the local pinning field can be finely adjusted to drag the quantum string, offering a potential method for detecting it in optical lattices.","sentences":["The stripe phase, an intertwined order observed in high-temperature superconductors, is regarded as playing a key role in elucidating the underlying mechanism of superconductivity, especially in cuprates.","Following Jan Zaanen's early scenario, the full-filled charge stripe can be taken as the interactive elastic quantum strings of holes, stabilized by $\\pi$-phase shifts between neighboring magnetic domains.","However, this scenario is challenging to explain, particularly in terms of electron pairing, which necessitates hole pairs.","In this work, we propose a new effective model for describing the stripe phase in the hole-doped $t$-$J_z$ model.","With respect to the antiferromagnetic background, the model comprises three types of color-labeled point-defects coupling to {an effective} spin field.","Comparing with numerical results from large-scale density matrix renormalization group (DMRG) simulations, we find semi-quantitative agreement in local hole density, magnetic moment, and the newly proposed spectrum features of the {effective} spin field.","By systematically analyzing the hole-density distribution and the scaling of groundstate energy at different system sizes, we determine the effective core radius and the effective hopping amplitude of the quantum string.","Furthermore, the local pinning field can be finely adjusted to drag the quantum string, offering a potential method for detecting it in optical lattices."],"url":"http://arxiv.org/abs/2406.01980v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 05:18:44","title":"Gifts from long-range interaction: Emergent gapless topological behaviors in quantum spin chain","abstract":"Topology in condensed matter physics is typically associated with a bulk energy gap. However, recent research has shifted focus to topological phases without a bulk energy gap, exhibiting nontrivial gapless topological behaviors. In this letter, we explore a cluster Ising chain with long-range antiferromagnetic interactions that decay as a power law with the distance. Using complementary numerical and analytical techniques, we demonstrate that long-range interactions can unambiguously induce an algebraic topological phase and a topological Gaussian universality, both of which exhibit nontrivial gapless topological behaviors. Our study not only provides a platform to investigate the fundamental physics of quantum many-body systems but also offers a novel route toward searching for gapless topological phases in realistic quantum simulators.","sentences":["Topology in condensed matter physics is typically associated with a bulk energy gap.","However, recent research has shifted focus to topological phases without a bulk energy gap, exhibiting nontrivial gapless topological behaviors.","In this letter, we explore a cluster Ising chain with long-range antiferromagnetic interactions that decay as a power law with the distance.","Using complementary numerical and analytical techniques, we demonstrate that long-range interactions can unambiguously induce an algebraic topological phase and a topological Gaussian universality, both of which exhibit nontrivial gapless topological behaviors.","Our study not only provides a platform to investigate the fundamental physics of quantum many-body systems but also offers a novel route toward searching for gapless topological phases in realistic quantum simulators."],"url":"http://arxiv.org/abs/2406.01974v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 05:09:42","title":"Evolution of the optical emission lines and the X-ray emission during the super-active stage of T CrB","abstract":"T CrB is a symbiotic star that experiences nova outbursts every $\\sim$ 80~yr. The next, long-anticipated nova outburst should occur during the 2024-2026 period. Here, we present results of high-resolution optical spectroscopy of T CrB in the period 2016 - 2023. In these spectra, we measured the equivalent widths of the H$\\alpha$, H$\\beta$, HeI and HeII emission lines. The maximum equivalent width (EW) was recorded on May 2021, when the EW of $H\\alpha$ reached -44.6 \\AA and H$\\beta$ = -21.5 \\AA. At the other extreme, the minimum of EW($H\\alpha$)= -2.9 \\AA was recorded in October 2023. After October 2023, the B-band emission brightened, suggesting a re-appearance of the orbital modulation. In addition to the optical data, we study the X-ray behaviour in the same period. We find a strong correlation between $EW(H\\alpha)$ and X-ray flux with a correlation coefficient -0.78 and a significance of 2.6$\\times 10^{-5}$.","sentences":["T CrB is a symbiotic star that experiences nova outbursts every $\\sim$ 80~yr.","The next, long-anticipated nova outburst should occur during the 2024-2026 period.","Here, we present results of high-resolution optical spectroscopy of T CrB in the period 2016 - 2023.","In these spectra, we measured the equivalent widths of the H$\\alpha$, H$\\beta$, HeI and HeII emission lines.","The maximum equivalent width (EW) was recorded on May 2021, when the EW of $H\\alpha$ reached -44.6 \\AA and H$\\beta$ = -21.5 \\AA.","At the other extreme, the minimum of EW($H\\alpha$)= -2.9 \\AA was recorded in October 2023.","After October 2023, the B-band emission brightened, suggesting a re-appearance of the orbital modulation.","In addition to the optical data, we study the X-ray behaviour in the same period.","We find a strong correlation between $EW(H\\alpha)$ and X-ray flux with a correlation coefficient -0.78 and a significance of 2.6$\\times 10^{-5}$."],"url":"http://arxiv.org/abs/2406.01971v1","category":"astro-ph.SR"}
{"created":"2024-06-04 04:31:50","title":"Backward bifurcation arising from decline of immunity against emerging infectious diseases","abstract":"Decline of immunity is a phenomenon characterized by immunocompromised host and plays a crucial role in the epidemiology of emerging infectious diseases (EIDs) such as COVID-19. In this paper, we propose an age-structured model with vaccination and reinfection of immune individuals. We prove that the disease-free equilibrium of the model undergoes backward and forward transcritical bifurcations at the critical value of the basic reproduction number for different values of parameters. We illustrate the results by numerical computations, and also find that the endemic equilibrium exhibits a saddle-node bifurcation on the extended branch of the forward transcritical bifurcation. These results allow us to understand the interplay between the decline of immunity and EIDs, and are able to provide strategies for mitigating the impact of EIDs on global health.","sentences":["Decline of immunity is a phenomenon characterized by immunocompromised host and plays a crucial role in the epidemiology of emerging infectious diseases (EIDs) such as COVID-19.","In this paper, we propose an age-structured model with vaccination and reinfection of immune individuals.","We prove that the disease-free equilibrium of the model undergoes backward and forward transcritical bifurcations at the critical value of the basic reproduction number for different values of parameters.","We illustrate the results by numerical computations, and also find that the endemic equilibrium exhibits a saddle-node bifurcation on the extended branch of the forward transcritical bifurcation.","These results allow us to understand the interplay between the decline of immunity and EIDs, and are able to provide strategies for mitigating the impact of EIDs on global health."],"url":"http://arxiv.org/abs/2406.01957v1","category":"math.DS"}
{"created":"2024-06-04 04:26:56","title":"Chemical mapping of temperate sub-Neptune atmospheres: Constraining the deep-interior H2O/H2 using the atmospheric CO2/CH4","abstract":"Understanding the envelope composition of sub-Neptune-type exoplanets is challenging due to the inherent degeneracy in their interior composition scenarios. Particularly, the H2O/H2 ratio, or can be expressed as the O/H ratio, in the planetary envelope provides crucial insights into the origin of these exoplanets relative to the ice line during formation. Using self-consistent radiative transfer modeling and a rate-based automatic chemical network generator combined with 1D photochemical kinetic-transport atmospheric modeling, we investigate atmospheres of temperate sub-Neptunes, ranging from H2-dominated to H2O-dominated scenarios with Teq = 250-400 K, using K2-18 b (Teq = 255 K), LP 791-18 c (Teq = 324 K), and TOI-270 d (Teq = 354 K) as examples. Our models indicate that using the atmospheric CO2/CH4 ratio to infer the deep-interior H2O/H2 ratio. Applying to recent JWST observations, our findings suggest K2-18 b likely has an interior highly enriched in water (approximately 50% H2O), exceeding the amount of water in a 100x solar metallicity scenario and suggesting a formation history that involved substantial accretion of ices. In contrast, TOI-270 d has an interior composition of approximately 25% H2O, which is comparable to the conventional metallicity framework with a metallicity higher than 100x solar metallicity. Furthermore, our models identify carbonyl sulfide (OCS) and sulfur dioxide (SO2) as strong indicators of at least a 10% water-rich envelope in temperate sub-Neptunes. These results provide a method to delineate the internal composition and formation mechanisms of sub-Neptunes with Teq< ~500 K via atmospheric characterization through transmission spectroscopy.","sentences":["Understanding the envelope composition of sub-Neptune-type exoplanets is challenging due to the inherent degeneracy in their interior composition scenarios.","Particularly, the H2O/H2 ratio, or can be expressed as the O/H ratio, in the planetary envelope provides crucial insights into the origin of these exoplanets relative to the ice line during formation.","Using self-consistent radiative transfer modeling and a rate-based automatic chemical network generator combined with 1D photochemical kinetic-transport atmospheric modeling, we investigate atmospheres of temperate sub-Neptunes, ranging from H2-dominated to H2O-dominated scenarios with Teq = 250-400 K, using K2-18 b (Teq = 255 K), LP 791-18 c (Teq = 324 K), and TOI-270 d (Teq = 354 K) as examples.","Our models indicate that using the atmospheric CO2/CH4 ratio to infer the deep-interior H2O/H2 ratio.","Applying to recent JWST observations, our findings suggest K2-18 b likely has an interior highly enriched in water (approximately 50% H2O), exceeding the amount of water in a 100x solar metallicity scenario and suggesting a formation history that involved substantial accretion of ices.","In contrast, TOI-270 d has an interior composition of approximately 25% H2O, which is comparable to the conventional metallicity framework with a metallicity higher than 100x solar metallicity.","Furthermore, our models identify carbonyl sulfide (OCS) and sulfur dioxide (SO2) as strong indicators of at least a 10% water-rich envelope in temperate sub-Neptunes.","These results provide a method to delineate the internal composition and formation mechanisms of sub-Neptunes with Teq< ~500 K via atmospheric characterization through transmission spectroscopy."],"url":"http://arxiv.org/abs/2406.01955v1","category":"astro-ph.EP"}
{"created":"2024-06-04 04:22:47","title":"Plug-and-Play Diffusion Distillation","abstract":"Diffusion models have shown tremendous results in image generation. However, due to the iterative nature of the diffusion process and its reliance on classifier-free guidance, inference times are slow. In this paper, we propose a new distillation approach for guided diffusion models in which an external lightweight guide model is trained while the original text-to-image model remains frozen. We show that our method reduces the inference computation of classifier-free guided latent-space diffusion models by almost half, and only requires 1\\% trainable parameters of the base model. Furthermore, once trained, our guide model can be applied to various fine-tuned, domain-specific versions of the base diffusion model without the need for additional training: this \"plug-and-play\" functionality drastically improves inference computation while maintaining the visual fidelity of generated images. Empirically, we show that our approach is able to produce visually appealing results and achieve a comparable FID score to the teacher with as few as 8 to 16 steps.","sentences":["Diffusion models have shown tremendous results in image generation.","However, due to the iterative nature of the diffusion process and its reliance on classifier-free guidance, inference times are slow.","In this paper, we propose a new distillation approach for guided diffusion models in which an external lightweight guide model is trained while the original text-to-image model remains frozen.","We show that our method reduces the inference computation of classifier-free guided latent-space diffusion models by almost half, and only requires 1\\% trainable parameters of the base model.","Furthermore, once trained, our guide model can be applied to various fine-tuned, domain-specific versions of the base diffusion model without the need for additional training: this \"plug-and-play\" functionality drastically improves inference computation while maintaining the visual fidelity of generated images.","Empirically, we show that our approach is able to produce visually appealing results and achieve a comparable FID score to the teacher with as few as 8 to 16 steps."],"url":"http://arxiv.org/abs/2406.01954v1","category":"cs.CV"}
{"created":"2024-06-04 04:03:51","title":"Experimental Validation of Enhanced Information Capacity by Quantum Switch in Accordance with Thermodynamic Laws","abstract":"We experimentally probe the interplay of the quantum switch with the laws of thermodynamics. The quantum switch places two channels in a superposition of orders and may be applied to thermalizing channels. Quantum-switching thermal channels has been shown to give apparent violations of the second law. Central to these apparent violations is how quantum switching channels can increase the capacity to communicate information. We experimentally show this increase and how it is consistent with the laws of thermodynamics, demonstrating how thermodynamic resources are consumed. We use a nuclear magnetic resonance approach with coherently controlled interactions of nuclear spin qubits. We verify an analytical upper bound on the increase in capacity for channels that preserve energy and thermal states, and demonstrate that the bound can be exceeded for an energy-altering channel. We show that the switch can be used to take a thermal state to a state that is not thermal, whilst consuming free energy associated with the coherence of a control system. The results show how the switch can be incorporated into quantum thermodynamics experiments as an additional resource.","sentences":["We experimentally probe the interplay of the quantum switch with the laws of thermodynamics.","The quantum switch places two channels in a superposition of orders and may be applied to thermalizing channels.","Quantum-switching thermal channels has been shown to give apparent violations of the second law.","Central to these apparent violations is how quantum switching channels can increase the capacity to communicate information.","We experimentally show this increase and how it is consistent with the laws of thermodynamics, demonstrating how thermodynamic resources are consumed.","We use a nuclear magnetic resonance approach with coherently controlled interactions of nuclear spin qubits.","We verify an analytical upper bound on the increase in capacity for channels that preserve energy and thermal states, and demonstrate that the bound can be exceeded for an energy-altering channel.","We show that the switch can be used to take a thermal state to a state that is not thermal, whilst consuming free energy associated with the coherence of a control system.","The results show how the switch can be incorporated into quantum thermodynamics experiments as an additional resource."],"url":"http://arxiv.org/abs/2406.01951v1","category":"quant-ph"}
{"created":"2024-06-04 03:38:50","title":"Lepton Flavor Violation at FCC-ee and CEPC","abstract":"Lepton flavor violating processes are highly suppressed in the Standard Model. Therefore, if observed, lepton flavor violation would be a clear indication of new physics beyond the Standard Model. We study the process $e^+e^-\\to\\tau\\mu$ at tree-level in the Standard Model Effective Field Theory both on the $Z$-pole and at higher center-of-mass energies. We show that the constraints derived from the future circular $e^+e^-$ colliders FCC-ee and CEPC, are complementary to those obtained from low-energy tau decays at BaBar and Belle, as well as projections from Belle-II.","sentences":["Lepton flavor violating processes are highly suppressed in the Standard Model.","Therefore, if observed, lepton flavor violation would be a clear indication of new physics beyond the Standard Model.","We study the process $e^+e^-\\to\\tau\\mu$ at tree-level in the Standard Model Effective Field Theory both on the $Z$-pole and at higher center-of-mass energies.","We show that the constraints derived from the future circular $e^+e^-$ colliders FCC-ee and CEPC, are complementary to those obtained from low-energy tau decays at BaBar and Belle, as well as projections from Belle-II."],"url":"http://arxiv.org/abs/2406.01935v1","category":"hep-ph"}
{"created":"2024-06-04 03:24:03","title":"History-Aware Planning for Risk-free Autonomous Navigation on Unknown Uneven Terrain","abstract":"It is challenging for the mobile robot to achieve autonomous and mapless navigation in the unknown environment with uneven terrain. In this study, we present a layered and systematic pipeline. At the local level, we maintain a tree structure that is dynamically extended with the navigation. This structure unifies the planning with the terrain identification. Besides, it contributes to explicitly identifying the hazardous areas on uneven terrain. In particular, certain nodes of the tree are consistently kept to form a sparse graph at the global level, which records the history of the exploration. A series of subgoals that can be obtained in the tree and the graph are utilized for leading the navigation. To determine a subgoal, we develop an evaluation method whose input elements can be efficiently obtained on the layered structure. We conduct both simulation and real-world experiments to evaluate the developed method and its key modules. The experimental results demonstrate the effectiveness and efficiency of our method. The robot can travel through the unknown uneven region safely and reach the target rapidly without a preconstructed map.","sentences":["It is challenging for the mobile robot to achieve autonomous and mapless navigation in the unknown environment with uneven terrain.","In this study, we present a layered and systematic pipeline.","At the local level, we maintain a tree structure that is dynamically extended with the navigation.","This structure unifies the planning with the terrain identification.","Besides, it contributes to explicitly identifying the hazardous areas on uneven terrain.","In particular, certain nodes of the tree are consistently kept to form a sparse graph at the global level, which records the history of the exploration.","A series of subgoals that can be obtained in the tree and the graph are utilized for leading the navigation.","To determine a subgoal, we develop an evaluation method whose input elements can be efficiently obtained on the layered structure.","We conduct both simulation and real-world experiments to evaluate the developed method and its key modules.","The experimental results demonstrate the effectiveness and efficiency of our method.","The robot can travel through the unknown uneven region safely and reach the target rapidly without a preconstructed map."],"url":"http://arxiv.org/abs/2406.01928v1","category":"cs.RO"}
{"created":"2024-06-04 03:19:28","title":"Explaining the possible 95 GeV excesses in the $B-L$ symmetric SSM","abstract":"This study investigates the excesses observed in the diphoton and $b\\bar b$ data around $95\\;{\\rm GeV}$ within the framework of the $B-L$ supersymmetric model (B-LSSM). Comparing with the minimal supersymmetric standard model, the B-LSSM incorporates two singlet chiral Higgs bosons which mix with the SM-like Higgs boson due to the gauge kinetic mixing effect. The richer Higgs sector indicates that designating the B-LSSM specific CP-even Higgs state as the lightest Higgs boson has great potential to explain the excesses at around $95\\;{\\rm GeV}$. Considering the two-loop effective potential corrections to the squared Higgs mass matrix, it is found that the signal strengthes $\\mu(h_{95})_{\\gamma\\gamma}$, $\\mu(h_{95})_{b b}$ can be described simultaneously in the experimental $1\\sigma$ interval. And the B-LSSM specific parameters $\\tan\\beta'$, $B_\\eta$, $g_{YB}$, $M_{Z'}$ affect the theoretical predictions on the light Higgs boson masses and signal strengthes $\\mu(h_{95})_{\\gamma\\gamma}$, $\\mu(h_{95})_{bb}$ significantly.","sentences":["This study investigates the excesses observed in the diphoton and $b\\bar b$ data around $95\\;{\\rm GeV}$ within the framework of the $B-L$ supersymmetric model (B-LSSM).","Comparing with the minimal supersymmetric standard model, the B-LSSM incorporates two singlet chiral Higgs bosons which mix with the SM-like Higgs boson due to the gauge kinetic mixing effect.","The richer Higgs sector indicates that designating the B-LSSM specific CP-even Higgs state as the lightest Higgs boson has great potential to explain the excesses at around $95\\;{\\rm GeV}$. Considering the two-loop effective potential corrections to the squared Higgs mass matrix, it is found that the signal strengthes $\\mu(h_{95})_{\\gamma\\gamma}$, $\\mu(h_{95})_{b b}$ can be described simultaneously in the experimental $1\\sigma$ interval.","And the B-LSSM specific parameters $\\tan\\beta'$, $B_\\eta$, $g_{YB}$, $M_{Z'}$ affect the theoretical predictions on the light Higgs boson masses and signal strengthes $\\mu(h_{95})_{\\gamma\\gamma}$, $\\mu(h_{95})_{bb}$ significantly."],"url":"http://arxiv.org/abs/2406.01926v1","category":"hep-ph"}
{"created":"2024-06-04 03:09:45","title":"Development of Bayesian Component Failure Models in E1 HEMP Grid Analysis","abstract":"Combined electric power system and High-Altitude Electromagnetic Pulse (HEMP) models are being developed to determine the effect of a HEMP on the US power grid. The work relies primarily on deterministic methods; however, it is computationally untenable to evaluate the E1 HEMP response of large numbers of grid components distributed across a large interconnection. Further, the deterministic assessment of these components' failures are largely unachievable. E1 HEMP laboratory testing of the components is accomplished, but is expensive, leaving few data points to construct failure models of grid components exposed to E1 HEMP. The use of Bayesian priors, developed using the subject matter expertise, combined with the minimal test data in a Bayesian inference process, provides the basis for the development of more robust and cost-effective statistical component failure models. These can be used with minimal computational burden in a simulation environment such as sampling of Cumulative Distribution Functions (CDFs).","sentences":["Combined electric power system and High-Altitude Electromagnetic Pulse (HEMP) models are being developed to determine the effect of a HEMP on the US power grid.","The work relies primarily on deterministic methods; however, it is computationally untenable to evaluate the E1 HEMP response of large numbers of grid components distributed across a large interconnection.","Further, the deterministic assessment of these components' failures are largely unachievable.","E1 HEMP laboratory testing of the components is accomplished, but is expensive, leaving few data points to construct failure models of grid components exposed to E1 HEMP.","The use of Bayesian priors, developed using the subject matter expertise, combined with the minimal test data in a Bayesian inference process, provides the basis for the development of more robust and cost-effective statistical component failure models.","These can be used with minimal computational burden in a simulation environment such as sampling of Cumulative Distribution Functions (CDFs)."],"url":"http://arxiv.org/abs/2406.01923v1","category":"eess.SY"}
