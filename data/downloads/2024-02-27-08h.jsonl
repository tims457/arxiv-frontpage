{"created":"2024-02-26 11:42:29","title":"Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision","abstract":"Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \\texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation.","sentences":["Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language.","Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages.","In this paper, we show that CLQA can be addressed using a single encoder-decoder model.","To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia.","We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation.","Together, we show our approach, \\texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot language adaptation settings, including those using machine translation."],"url":"http://arxiv.org/abs/2402.16508v1","category":"cs.CL"}
{"created":"2024-02-26 11:41:38","title":"Note on cosmographic approach to determining parameters of Barrow entropic dark energy model","abstract":"The cosmographic approach is used to determine the parameters of the Barrow entropic dark energy model. The model parameters are expressed through the current kinematic characteristics of Universe expansion.","sentences":["The cosmographic approach is used to determine the parameters of the Barrow entropic dark energy model.","The model parameters are expressed through the current kinematic characteristics of Universe expansion."],"url":"http://arxiv.org/abs/2402.16507v1","category":"gr-qc"}
{"created":"2024-02-26 11:41:28","title":"Stochastic Conditional Diffusion Models for Semantic Image Synthesis","abstract":"Semantic image synthesis (SIS) is a task to generate realistic images corresponding to semantic maps (labels). It can be applied to diverse real-world practices such as photo editing or content creation. However, in real-world applications, SIS often encounters noisy user inputs. To address this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a robust conditional diffusion model that features novel forward and generation processes tailored for SIS with noisy labels. It enhances robustness by stochastically perturbing the semantic label maps through Label Diffusion, which diffuses the labels with discrete diffusion. Through the diffusion of labels, the noisy and clean semantic maps become similar as the timestep increases, eventually becoming identical at $t=T$. This facilitates the generation of an image close to a clean image, enabling robust generation. Furthermore, we propose a class-wise noise schedule to differentially diffuse the labels depending on the class. We demonstrate that the proposed method generates high-quality samples through extensive experiments and analyses on benchmark datasets, including a novel experimental setup simulating human errors during real-world applications.","sentences":["Semantic image synthesis (SIS) is a task to generate realistic images corresponding to semantic maps (labels).","It can be applied to diverse real-world practices such as photo editing or content creation.","However, in real-world applications, SIS often encounters noisy user inputs.","To address this, we propose Stochastic Conditional Diffusion Model (SCDM), which is a robust conditional diffusion model that features novel forward and generation processes tailored for SIS with noisy labels.","It enhances robustness by stochastically perturbing the semantic label maps through Label Diffusion, which diffuses the labels with discrete diffusion.","Through the diffusion of labels, the noisy and clean semantic maps become similar as the timestep increases, eventually becoming identical at $t=T$. This facilitates the generation of an image close to a clean image, enabling robust generation.","Furthermore, we propose a class-wise noise schedule to differentially diffuse the labels depending on the class.","We demonstrate that the proposed method generates high-quality samples through extensive experiments and analyses on benchmark datasets, including a novel experimental setup simulating human errors during real-world applications."],"url":"http://arxiv.org/abs/2402.16506v1","category":"cs.CV"}
{"created":"2024-02-26 11:40:51","title":"Memory GAPS: Would LLM pass the Tulving Test?","abstract":"The Tulving Test was designed to investigate memory performance in recognition and recall tasks. Its results help assess the relevance of the \"Synergistic Ecphory Model\" of memory and similar RK paradigms in human performance. This paper starts investigating whether the more than forty-year-old framework sheds some light on LLMs' acts of remembering.","sentences":["The Tulving Test was designed to investigate memory performance in recognition and recall tasks.","Its results help assess the relevance of the \"Synergistic Ecphory Model\" of memory and similar RK paradigms in human performance.","This paper starts investigating whether the more than forty-year-old framework sheds some light on LLMs' acts of remembering."],"url":"http://arxiv.org/abs/2402.16505v1","category":"cs.AI"}
{"created":"2024-02-26 11:37:16","title":"Gluon generalized parton distributions of the proton at non-zero skewness","abstract":"Using a recently developed light-front spectator model that incorporates gluon, where the light-front wave functions are modeled from the soft-wall AdS/QCD prediction, we examine the leading twist gluon generalized parton distributions (GPDs) inside the proton. We derive the chirally even and odd distributions by using the overlap representation of the light-front wave functions. In terms of GPDs at non-zero skewness, we investigate the entire three-dimensional representation of gluons. We analyse the gluon impact parameter distributions at $\\xi=0$ using the Fourier transform of GPDs. We address the total angular momentum contribution of the gluons by using the Ji's sum rule and also give our predictions for both the canonical and kinetic orbital angular momentum in the light-cone gauge.","sentences":["Using a recently developed light-front spectator model that incorporates gluon, where the light-front wave functions are modeled from the soft-wall AdS/QCD prediction, we examine the leading twist gluon generalized parton distributions (GPDs) inside the proton.","We derive the chirally even and odd distributions by using the overlap representation of the light-front wave functions.","In terms of GPDs at non-zero skewness, we investigate the entire three-dimensional representation of gluons.","We analyse the gluon impact parameter distributions at $\\xi=0$ using the Fourier transform of GPDs.","We address the total angular momentum contribution of the gluons by using the Ji's sum rule and also give our predictions for both the canonical and kinetic orbital angular momentum in the light-cone gauge."],"url":"http://arxiv.org/abs/2402.16503v1","category":"hep-ph"}
{"created":"2024-02-26 11:35:23","title":"Trajectory Prediction for Autonomous Driving Using a Transformer Network","abstract":"Predicting the trajectories of surrounding agents is still considered one of the most challenging tasks for autonomous driving. In this paper, we introduce a multi-modal trajectory prediction framework based on the transformer network. The semantic maps of each agent are used as inputs to convolutional networks to automatically derive relevant contextual information. A novel auxiliary loss that penalizes unfeasible off-road predictions is also proposed in this study. Experiments on the Lyft l5kit dataset show that the proposed model achieves state-of-the-art performance, substantially improving the accuracy and feasibility of the prediction outcomes.","sentences":["Predicting the trajectories of surrounding agents is still considered one of the most challenging tasks for autonomous driving.","In this paper, we introduce a multi-modal trajectory prediction framework based on the transformer network.","The semantic maps of each agent are used as inputs to convolutional networks to automatically derive relevant contextual information.","A novel auxiliary loss that penalizes unfeasible off-road predictions is also proposed in this study.","Experiments on the Lyft l5kit dataset show that the proposed model achieves state-of-the-art performance, substantially improving the accuracy and feasibility of the prediction outcomes."],"url":"http://arxiv.org/abs/2402.16501v1","category":"cs.RO"}
{"created":"2024-02-26 11:31:48","title":"LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments","abstract":"Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.","sentences":["Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence.","However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions.","There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments.","To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments.","LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration.","We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration.","We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings.","The code and data will be available."],"url":"http://arxiv.org/abs/2402.16499v1","category":"cs.CL"}
{"created":"2024-02-26 11:30:39","title":"Results of the follow-up of ANTARES neutrino alerts","abstract":"High-energy neutrinos could be produced in the interaction of charged cosmic rays with matter or radiation surrounding astrophysical sources. To look for transient sources associated with neutrino emission, a follow-up program of neutrino alerts has been operating within the ANTARES Collaboration since 2009. This program, named TAToO, has triggered robotic optical telescopes (MASTER, TAROT, ROTSE and the SVOM ground based telescopes) immediately after the detection of any relevant neutrino candidate and scheduled several observations in the weeks following the detection. A subset of ANTARES events with highest probabilities of being of cosmic origin has also been followed by the Swift and the INTEGRAL satellites, the Murchison Widefield Array radio telescope and the H.E.S.S. high-energy gamma-ray telescope. The results of twelve years of observations are reported. No optical counterpart has been significantly associated with an ANTARES candidate neutrino signal during image analysis. Constraints on transient neutrino emission have been set. In September 2015, ANTARES issued a neutrino alert and during the follow-up, a potential transient counterpart was identified by Swift and MASTER. A multi-wavelength follow-up campaign has allowed to identify the nature of this source and has proven its fortuitous association with the neutrino. The return of experience is particularly important for the design of the alert system of KM3NeT, the next generation neutrino telescope in the Mediterranean Sea.","sentences":["High-energy neutrinos could be produced in the interaction of charged cosmic rays with matter or radiation surrounding astrophysical sources.","To look for transient sources associated with neutrino emission, a follow-up program of neutrino alerts has been operating within the ANTARES Collaboration since 2009.","This program, named TAToO, has triggered robotic optical telescopes (MASTER, TAROT, ROTSE and the SVOM ground based telescopes) immediately after the detection of any relevant neutrino candidate and scheduled several observations in the weeks following the detection.","A subset of ANTARES events with highest probabilities of being of cosmic origin has also been followed by the Swift and the INTEGRAL satellites, the Murchison Widefield Array radio telescope and the H.E.S.S. high-energy gamma-ray telescope.","The results of twelve years of observations are reported.","No optical counterpart has been significantly associated with an ANTARES candidate neutrino signal during image analysis.","Constraints on transient neutrino emission have been set.","In September 2015, ANTARES issued a neutrino alert and during the follow-up, a potential transient counterpart was identified by Swift and MASTER.","A multi-wavelength follow-up campaign has allowed to identify the nature of this source and has proven its fortuitous association with the neutrino.","The return of experience is particularly important for the design of the alert system of KM3NeT, the next generation neutrino telescope in the Mediterranean Sea."],"url":"http://arxiv.org/abs/2402.16498v1","category":"astro-ph.HE"}
{"created":"2024-02-26 11:30:34","title":"SAND: Decoupling Sanitization from Fuzzing for Low Overhead","abstract":"Sanitizers provide robust test oracles for various software vulnerabilities. Fuzzing on sanitizer-enabled programs has been the best practice to find software bugs. Since sanitizers need to heavily instrument a target program to insert run-time checks, sanitizer-enabled programs have much higher overhead compared to normally built programs. In this paper, we present SAND, a new fuzzing framework that decouples sanitization from the fuzzing loop. SAND performs fuzzing on a normally built program and only invokes sanitizer-enabled programs when input is shown to be interesting. Since most of the generated inputs are not interesting, i.e., not bug-triggering, SAND allows most of the fuzzing time to be spent on the normally built program. To identify interesting inputs, we introduce execution pattern for a practical execution analysis on the normally built program. We realize SAND on top of AFL++ and evaluate it on 12 real-world programs. Our extensive evaluation highlights its effectiveness: on a period of 24 hours, compared to fuzzing on ASan/UBSan-enabled and MSan-enabled programs, SAND respectively achieves 2.6x and 15x throughput and detects 51% and 242% more bugs.","sentences":["Sanitizers provide robust test oracles for various software vulnerabilities.","Fuzzing on sanitizer-enabled programs has been the best practice to find software bugs.","Since sanitizers need to heavily instrument a target program to insert run-time checks, sanitizer-enabled programs have much higher overhead compared to normally built programs.","In this paper, we present SAND, a new fuzzing framework that decouples sanitization from the fuzzing loop.","SAND performs fuzzing on a normally built program and only invokes sanitizer-enabled programs when input is shown to be interesting.","Since most of the generated inputs are not interesting, i.e., not bug-triggering, SAND allows most of the fuzzing time to be spent on the normally built program.","To identify interesting inputs, we introduce execution pattern for a practical execution analysis on the normally built program.","We realize SAND on top of AFL++ and evaluate it on 12 real-world programs.","Our extensive evaluation highlights its effectiveness: on a period of 24 hours, compared to fuzzing on ASan/UBSan-enabled and MSan-enabled programs, SAND respectively achieves 2.6x and 15x throughput and detects 51% and 242% more bugs."],"url":"http://arxiv.org/abs/2402.16497v1","category":"cs.CR"}
{"created":"2024-02-26 11:16:02","title":"Low-energy theorem revisited and OPE in massless QCD","abstract":"We revisit a low-energy theorem (LET) of NSVZ type in SU($N$) QCD with $N_f$ massless quarks derived in [1] by implementing it in dimensional regularization. The LET relates $n$-point correlators in the lhs to $n+1$-point correlators with the extra insertion of Tr$F^2$ at zero momentum in the rhs. First, we demonstrate that, for $2$-point correlators of an operator $O$ in the lhs, the LET implies that, in general, the integrated $3$-point correlator in the rhs needs in perturbation theory an infinite additive renormalization in addition to the multiplicative one. Second, we relate the above counterterm -- that is completely fixed by the LET -- to a corresponding divergent contact term in a certain coefficient of the OPE of Tr$F^2$ with $O$ in the momentum representation, thus extending by means of the LET to any operator $O$ an independent argument that first appeared for $O$=Tr$F^2$ in [2]. Third, we verify by direct computation that the latter divergent contact term first computed in [3] to order $g^4$ in perturbation theory and to all orders in [2] actually agrees with the one implied by the LET. Fourth, we evaluate the divergent contact terms for the above OPE coefficient both in the coordinate and momentum representation and discuss their relation. Fifth, we demonstrate that in the asymptotically free phase of QCD the aforementioned counterterm in the LET -- though divergent order by order in perturbation theory -- is actually finite nonperturbatively after resummation to all perturbative orders. Finally, we briefly recall the implications of the LET in the gauge-invariant framework of dimensional regularization for the perturbative and nonperturbative renormalization in large-$N$ QCD. The implications of the LET inside and above the conformal window of SU($N$) QCD with $N_f$ massless quarks will appear in a forthcoming paper.","sentences":["We revisit a low-energy theorem (LET) of NSVZ type in SU($N$) QCD with $N_f$ massless quarks derived in [1] by implementing it in dimensional regularization.","The LET relates $n$-point correlators in the lhs to $n+1$-point correlators with the extra insertion of Tr$F^2$ at zero momentum in the rhs.","First, we demonstrate that, for $2$-point correlators of an operator $O$ in the lhs, the LET implies that, in general, the integrated $3$-point correlator in the rhs needs in perturbation theory an infinite additive renormalization in addition to the multiplicative one.","Second, we relate the above counterterm -- that is completely fixed by the LET -- to a corresponding divergent contact term in a certain coefficient of the OPE of Tr$F^2$ with $O$ in the momentum representation, thus extending by means of the LET to any operator $O$ an independent argument that first appeared for $O$=Tr$F^2$ in [2].","Third, we verify by direct computation that the latter divergent contact term first computed in [3] to order $g^4$ in perturbation theory and to all orders in [2] actually agrees with the one implied by the LET.","Fourth, we evaluate the divergent contact terms for the above OPE coefficient both in the coordinate and momentum representation and discuss their relation.","Fifth, we demonstrate that in the asymptotically free phase of QCD the aforementioned counterterm in the LET -- though divergent order by order in perturbation theory -- is actually finite nonperturbatively after resummation to all perturbative orders.","Finally, we briefly recall the implications of the LET in the gauge-invariant framework of dimensional regularization for the perturbative and nonperturbative renormalization in large-$N$ QCD.","The implications of the LET inside and above the conformal window of SU($N$) QCD with $N_f$ massless quarks will appear in a forthcoming paper."],"url":"http://arxiv.org/abs/2402.16490v1","category":"hep-th"}
{"created":"2024-02-26 11:12:14","title":"Nonlocal gradients: Fundamental theorem of calculus, Poincar\u00e9 inequalities and embeddings","abstract":"We address the study of nonlocal gradients defined through general radial kernels $\\rho$. Our investigation focuses on the properties of the associated function spaces, which depend on the characteristics of the kernel function. Specifically, even with minimal assumptions on $\\rho$, we establish Poincar\\'e inequalities and compact embeddings into Lebesgue spaces. Additionally, we present a fundamental theorem of calculus that enables us to recover a function from its nonlocal gradient through a convolution. This is used to demonstrate embeddings into Orlicz spaces and spaces of continuous functions that mirror the well-known Sobolev and Morrey inequalities for classical gradients. Finally, we establish conditions for inclusions and equality of spaces associated to different kernels.","sentences":["We address the study of nonlocal gradients defined through general radial kernels $\\rho$. Our investigation focuses on the properties of the associated function spaces, which depend on the characteristics of the kernel function.","Specifically, even with minimal assumptions on $\\rho$, we establish Poincar\\'e inequalities and compact embeddings into Lebesgue spaces.","Additionally, we present a fundamental theorem of calculus that enables us to recover a function from its nonlocal gradient through a convolution.","This is used to demonstrate embeddings into Orlicz spaces and spaces of continuous functions that mirror the well-known Sobolev and Morrey inequalities for classical gradients.","Finally, we establish conditions for inclusions and equality of spaces associated to different kernels."],"url":"http://arxiv.org/abs/2402.16487v1","category":"math.AP"}
{"created":"2024-02-26 11:08:26","title":"Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification","abstract":"Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner. Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936). The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality. The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition.","sentences":["Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification.","This research addresses this problem with a novel, scalable, and AI-driven solution.","The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types.","Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes.","Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft.","It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and supervised few-shot learning for aircraft type classification.","To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully supervised manner.","Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936).","The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality.","The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition."],"url":"http://arxiv.org/abs/2402.16486v1","category":"cs.CV"}
{"created":"2024-02-26 11:04:09","title":"On geometrical origin of Kodama vector","abstract":"It has been known that warped product spacetimes such as spherically symmetric ones admit the Kodama vector. This vector provides a locally conserved current made by contraction of the Einstein tensor, even though there is no Killing vector. In addition, a quasilocal mass, Birkhoff's theorem and various properties are closely related to the Kodama vector. Recently, it is shown that the notion of the Kodama vector can be extended to three-dimensional axisymmetric spacetimes even if the spacetimes are not warped product. This implies that warped product may not be a necessary condition for a spacetime to admit the Kodama vector. We show properties of the Kodama vector originate from the conformal Killing-Yano two-form. In particular, the well-known spacetimes that admit the Kodama vector have a closed conformal Killing-Yano two-form. Furthermore, we show the Kodama vector provides local conserved currents for each order of the Lovelock tensor as well as the Einstein tensor.","sentences":["It has been known that warped product spacetimes such as spherically symmetric ones admit the Kodama vector.","This vector provides a locally conserved current made by contraction of the Einstein tensor, even though there is no Killing vector.","In addition, a quasilocal mass, Birkhoff's theorem and various properties are closely related to the Kodama vector.","Recently, it is shown that the notion of the Kodama vector can be extended to three-dimensional axisymmetric spacetimes even if the spacetimes are not warped product.","This implies that warped product may not be a necessary condition for a spacetime to admit the Kodama vector.","We show properties of the Kodama vector originate from the conformal Killing-Yano two-form.","In particular, the well-known spacetimes that admit the Kodama vector have a closed conformal Killing-Yano two-form.","Furthermore, we show the Kodama vector provides local conserved currents for each order of the Lovelock tensor as well as the Einstein tensor."],"url":"http://arxiv.org/abs/2402.16484v1","category":"gr-qc"}
{"created":"2024-02-26 11:01:54","title":"On Languaging a Simulation Engine","abstract":"Language model intelligence is revolutionizing the way we program materials simulations. However, the diversity of simulation scenarios renders it challenging to precisely transform human language into a tailored simulator. Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a simulation engine, by taking a scenario instance of water sorption in porous matrices. Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair. Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and distilling the simulator input into executable format. Importantly, depending on its functionalized type, each language model features a distinct processing of chat history to best balance its memory limit and information completeness, thus leveraging the model intelligence to unstructured nature of human request. Overall, this work establishes language model as an intelligent platform to unlock the era of languaging a simulation engine.","sentences":["Language model intelligence is revolutionizing the way we program materials simulations.","However, the diversity of simulation scenarios renders it challenging to precisely transform human language into a tailored simulator.","Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a simulation engine, by taking a scenario instance of water sorption in porous matrices.","Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair.","Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and distilling the simulator input into executable format.","Importantly, depending on its functionalized type, each language model features a distinct processing of chat history to best balance its memory limit and information completeness, thus leveraging the model intelligence to unstructured nature of human request.","Overall, this work establishes language model as an intelligent platform to unlock the era of languaging a simulation engine."],"url":"http://arxiv.org/abs/2402.16482v1","category":"cs.AI"}
{"created":"2024-02-26 10:49:10","title":"Parity violation in the observed galaxy trispectrum","abstract":"Recent measurements of the 4-point correlation function in large-scale galaxy surveys have found apparent evidence of parity violation in the distribution of galaxies. This cannot happen via dynamical gravitational effects in general relativity. If such a violation arose from physics in the early Universe it could indicate important new physics beyond the standard model, and would be at odds with most models of inflation. It is therefore now timely to consider the galaxy trispectrum in more detail. While the intrinsic 4-point correlation function, or equivalently the trispectrum, its Fourier counterpart, is parity invariant, the observed trispectrum must take redshift-space distortions into account. Although the standard Newtonian correction also respects parity invariance, we show that sub-leading relativistic corrections do not. We demonstrate that these can be significant at intermediate linear scales and are dominant over the Newtonian parity-invariant part around the equality scale and above. Therefore when observing the galaxy 4-point correlation function, we should expect to detect parity violation on large scales.","sentences":["Recent measurements of the 4-point correlation function in large-scale galaxy surveys have found apparent evidence of parity violation in the distribution of galaxies.","This cannot happen via dynamical gravitational effects in general relativity.","If such a violation arose from physics in the early Universe it could indicate important new physics beyond the standard model, and would be at odds with most models of inflation.","It is therefore now timely to consider the galaxy trispectrum in more detail.","While the intrinsic 4-point correlation function, or equivalently the trispectrum, its Fourier counterpart, is parity invariant, the observed trispectrum must take redshift-space distortions into account.","Although the standard Newtonian correction also respects parity invariance, we show that sub-leading relativistic corrections do not.","We demonstrate that these can be significant at intermediate linear scales and are dominant over the Newtonian parity-invariant part around the equality scale and above.","Therefore when observing the galaxy 4-point correlation function, we should expect to detect parity violation on large scales."],"url":"http://arxiv.org/abs/2402.16478v1","category":"astro-ph.CO"}
{"created":"2024-02-26 10:45:47","title":"Covert Communication Over Additive-Noise Channels","abstract":"We study the fundamental limits of covert communications over general memoryless additive-noise channels. Under mild integrability assumptions, we find a general upper bound on the square-root scaling constant, which only involves the variance of the logarithm of the probability density function of the noise. Furthermore, we show that under some additional assumptions, this upper bound is tight. We also provide upper bounds on the length of the secret key required to achieve the optimal scaling.","sentences":["We study the fundamental limits of covert communications over general memoryless additive-noise channels.","Under mild integrability assumptions, we find a general upper bound on the square-root scaling constant, which only involves the variance of the logarithm of the probability density function of the noise.","Furthermore, we show that under some additional assumptions, this upper bound is tight.","We also provide upper bounds on the length of the secret key required to achieve the optimal scaling."],"url":"http://arxiv.org/abs/2402.16475v1","category":"cs.IT"}
{"created":"2024-02-26 10:43:44","title":"Particle Motion in Hamiltonian Formalism","abstract":"The goal of this contribution is to introduce the Hamiltonian formalism of theoretical mechanics for analysing motion in generic linear and non-linear dynamical systems, including particle accelerators. This framework allows the derivation and integration of equations of motion, in order to describe the particle trajectory evolution with respect to time. First, basic concepts are re-visited for describing particle propagation through the resolution of differential equations, applied to linear and non-linear motion. These equations of motion can be obtained by the Lagrangian of the system, which is the natural step leading to Hamiltonian Formalism and its properties. The accelerator ring Hamiltonian is derived, starting with the relativistic Hamiltonian of particles in the influence of E/M fields and a series of canonical (or symplectic) transformations and approximations. Thereby, introductory concepts of beam dynamics such as invariants and transport matrices are revisited and extended towards generic concepts such as action-angle variables and symplectic maps. To this end, the ground is prepared for the advanced methods and tools used for studying non-linear motion in particle accelerators.","sentences":["The goal of this contribution is to introduce the Hamiltonian formalism of theoretical mechanics for analysing motion in generic linear and non-linear dynamical systems, including particle accelerators.","This framework allows the derivation and integration of equations of motion, in order to describe the particle trajectory evolution with respect to time.","First, basic concepts are re-visited for describing particle propagation through the resolution of differential equations, applied to linear and non-linear motion.","These equations of motion can be obtained by the Lagrangian of the system, which is the natural step leading to Hamiltonian Formalism and its properties.","The accelerator ring Hamiltonian is derived, starting with the relativistic Hamiltonian of particles in the influence of E/M fields and a series of canonical (or symplectic) transformations and approximations.","Thereby, introductory concepts of beam dynamics such as invariants and transport matrices are revisited and extended towards generic concepts such as action-angle variables and symplectic maps.","To this end, the ground is prepared for the advanced methods and tools used for studying non-linear motion in particle accelerators."],"url":"http://arxiv.org/abs/2402.16474v1","category":"physics.acc-ph"}
{"created":"2024-02-26 10:42:25","title":"DCVSMNet: Double Cost Volume Stereo Matching Network","abstract":"We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a novel architecture characterised by by two small upper (group-wise) and lower (norm correlation) cost volumes. Each cost volume is processed separately, and a coupling module is proposed to fuse the geometry information extracted from the upper and lower cost volumes. DCVSMNet is a fast stereo matching network with a 67 ms inference time and strong generalization ability which can produce competitive results compared to state-of-the-art methods. The results on several bench mark datasets show that DCVSMNet achieves better accuracy than methods such as CGI-Stereo and BGNet at the cost of greater inference time.","sentences":["We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a novel architecture characterised by by two small upper (group-wise) and lower (norm correlation) cost volumes.","Each cost volume is processed separately, and a coupling module is proposed to fuse the geometry information extracted from the upper and lower cost volumes.","DCVSMNet is a fast stereo matching network with a 67 ms inference time and strong generalization ability which can produce competitive results compared to state-of-the-art methods.","The results on several bench mark datasets show that DCVSMNet achieves better accuracy than methods such as CGI-Stereo and BGNet at the cost of greater inference time."],"url":"http://arxiv.org/abs/2402.16473v1","category":"cs.CV"}
{"created":"2024-02-26 10:33:36","title":"mEdIT: Multilingual Text Editing via Instruction Tuning","abstract":"We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT generalizes effectively to new languages over multilingual baselines. We publicly release our data, code, and trained models at https://github.com/vipulraheja/medit.","sentences":["We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance.","mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning.","They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on (Spanish).","We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families.","We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs.","We also find that mEdIT generalizes effectively to new languages over multilingual baselines.","We publicly release our data, code, and trained models at https://github.com/vipulraheja/medit."],"url":"http://arxiv.org/abs/2402.16472v1","category":"cs.CL"}
{"created":"2024-02-26 10:31:45","title":"Unveiling Vulnerability of Self-Attention","abstract":"Pre-trained language models (PLMs) are shown to be vulnerable to minor word changes, which poses a big threat to real-world systems. While previous studies directly focus on manipulating word inputs, they are limited by their means of generating adversarial samples, lacking generalization to versatile real-world attack. This paper studies the basic structure of transformer-based PLMs, the self-attention (SA) mechanism. (1) We propose a powerful perturbation technique \\textit{HackAttend}, which perturbs the attention scores within the SA matrices via meticulously crafted attention masks. We show that state-of-the-art PLMs fall into heavy vulnerability that minor attention perturbations $(1\\%)$ can produce a very high attack success rate $(98\\%)$. Our paper expands the conventional text attack of word perturbations to more general structural perturbations. (2) We introduce \\textit{S-Attend}, a novel smoothing technique that effectively makes SA robust via structural perturbations. We empirically demonstrate that this simple yet effective technique achieves robust performance on par with adversarial training when facing various text attackers. Code is publicly available at \\url{github.com/liongkj/HackAttend}.","sentences":["Pre-trained language models (PLMs) are shown to be vulnerable to minor word changes, which poses a big threat to real-world systems.","While previous studies directly focus on manipulating word inputs, they are limited by their means of generating adversarial samples, lacking generalization to versatile real-world attack.","This paper studies the basic structure of transformer-based PLMs, the self-attention (SA) mechanism.","(1) We propose a powerful perturbation technique \\textit{HackAttend}, which perturbs the attention scores within the SA matrices via meticulously crafted attention masks.","We show that state-of-the-art PLMs fall into heavy vulnerability that minor attention perturbations $(1\\%)$ can produce a very high attack success rate $(98\\%)$. Our paper expands the conventional text attack of word perturbations to more general structural perturbations.","(2) We introduce \\textit{S-Attend}, a novel smoothing technique that effectively makes SA robust via structural perturbations.","We empirically demonstrate that this simple yet effective technique achieves robust performance on par with adversarial training when facing various text attackers.","Code is publicly available at \\url{github.com/liongkj/HackAttend}."],"url":"http://arxiv.org/abs/2402.16470v1","category":"cs.CL"}
{"created":"2024-02-26 10:26:50","title":"Integrated Sensing and Communications with Affine Frequency Division Multiplexing","abstract":"Integrated sensing and communications (ISAC) is regarded as a key technology in next-generation (6G) mobile communication systems. Affine frequency division multiplexing (AFDM) is a recently proposed waveform that achieves optimal diversity gain in high mobility scenarios and has appealing properties in high-frequency communication. In this letter, we present an AFDM-based ISAC system. We first show that in order to identify all delay and Doppler components associated with the propagation medium, either the full AFDM signal or only its pilot part consisting of one discrete affine Fourier transform (DAFT) domain symbol and its guard interval can be used. Our results show that using one pilot symbol achieves almost the same sensing performance as using the entire AFDM frame. Furthermore, due to the chirp nature of AFDM, sensing with one pilot provides a unique feature allowing for simple self-interference cancellation, thus avoiding the need for expensive full duplex methods.","sentences":["Integrated sensing and communications (ISAC) is regarded as a key technology in next-generation (6G) mobile communication systems.","Affine frequency division multiplexing (AFDM) is a recently proposed waveform that achieves optimal diversity gain in high mobility scenarios and has appealing properties in high-frequency communication.","In this letter, we present an AFDM-based ISAC system.","We first show that in order to identify all delay and Doppler components associated with the propagation medium, either the full AFDM signal or only its pilot part consisting of one discrete affine Fourier transform (DAFT) domain symbol and its guard interval can be used.","Our results show that using one pilot symbol achieves almost the same sensing performance as using the entire AFDM frame.","Furthermore, due to the chirp nature of AFDM, sensing with one pilot provides a unique feature allowing for simple self-interference cancellation, thus avoiding the need for expensive full duplex methods."],"url":"http://arxiv.org/abs/2402.16468v1","category":"cs.IT"}
{"created":"2024-02-26 10:19:23","title":"Exploratory Landscape Analysis for Mixed-Variable Problems","abstract":"Exploratory landscape analysis and fitness landscape analysis in general have been pivotal in facilitating problem understanding, algorithm design and endeavors such as automated algorithm selection and configuration. These techniques have largely been limited to search spaces of a single domain. In this work, we provide the means to compute exploratory landscape features for mixed-variable problems where the decision space is a mixture of continuous, binary, integer, and categorical variables. This is achieved by utilizing existing encoding techniques originating from machine learning. We provide a comprehensive juxtaposition of the results based on these different techniques. To further highlight their merit for practical applications, we design and conduct an automated algorithm selection study based on a hyperparameter optimization benchmark suite. We derive a meaningful compartmentalization of these benchmark problems by clustering based on the used landscape features. The identified clusters mimic the behavior the used algorithms exhibit. Meaning, the different clusters have different best performing algorithms. Finally, our trained algorithm selector is able to close the gap between the single best and the virtual best solver by 57.5% over all benchmark problems.","sentences":["Exploratory landscape analysis and fitness landscape analysis in general have been pivotal in facilitating problem understanding, algorithm design and endeavors such as automated algorithm selection and configuration.","These techniques have largely been limited to search spaces of a single domain.","In this work, we provide the means to compute exploratory landscape features for mixed-variable problems where the decision space is a mixture of continuous, binary, integer, and categorical variables.","This is achieved by utilizing existing encoding techniques originating from machine learning.","We provide a comprehensive juxtaposition of the results based on these different techniques.","To further highlight their merit for practical applications, we design and conduct an automated algorithm selection study based on a hyperparameter optimization benchmark suite.","We derive a meaningful compartmentalization of these benchmark problems by clustering based on the used landscape features.","The identified clusters mimic the behavior the used algorithms exhibit.","Meaning, the different clusters have different best performing algorithms.","Finally, our trained algorithm selector is able to close the gap between the single best and the virtual best solver by 57.5% over all benchmark problems."],"url":"http://arxiv.org/abs/2402.16467v1","category":"cs.NE"}
{"created":"2024-02-26 10:06:54","title":"Enabling Communication and Control Co-Design in 6G Networks","abstract":"Networked control systems (NCSs), which are feedback control loops closed over a communication network, have been a popular research topic over the past decades. Numerous works in the literature propose novel algorithms and protocols with joint consideration of communication and control. However, the vast majority of the recent research results, which have shown remarkable performance improvements if a cross-layer methodology is followed, have not been widely adopted by the industry. In this work, we review the shortcomings of today's mobile networks that render cross-layer solutions, such as semantic and goal-oriented communications, very challenging in practice. To tackle this, we propose a new framework for 6G user plane design that simplifies the adoption of recent research results in networked control, thereby facilitating the joint communication and control design in next-generation mobile networks.","sentences":["Networked control systems (NCSs), which are feedback control loops closed over a communication network, have been a popular research topic over the past decades.","Numerous works in the literature propose novel algorithms and protocols with joint consideration of communication and control.","However, the vast majority of the recent research results, which have shown remarkable performance improvements if a cross-layer methodology is followed, have not been widely adopted by the industry.","In this work, we review the shortcomings of today's mobile networks that render cross-layer solutions, such as semantic and goal-oriented communications, very challenging in practice.","To tackle this, we propose a new framework for 6G user plane design that simplifies the adoption of recent research results in networked control, thereby facilitating the joint communication and control design in next-generation mobile networks."],"url":"http://arxiv.org/abs/2402.16462v1","category":"cs.NI"}
{"created":"2024-02-26 10:03:33","title":"Defending LLMs against Jailbreaking Attacks via Backtranslation","abstract":"Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiveness and efficiency. We empirically demonstrate that our defense significantly outperforms the baselines, in the cases that are hard for the baselines, and our defense also has little impact on the generation quality for benign input prompts.","sentences":["Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent.","In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''.","Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response.","The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker.","We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt.","We explain that the proposed defense provides several benefits on its effectiveness and efficiency.","We empirically demonstrate that our defense significantly outperforms the baselines, in the cases that are hard for the baselines, and our defense also has little impact on the generation quality for benign input prompts."],"url":"http://arxiv.org/abs/2402.16459v1","category":"cs.CL"}
{"created":"2024-02-26 09:59:04","title":"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering","abstract":"Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at \\url{https://github.com/hyintell/RetrievalQA}","sentences":["Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information.","However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied.","This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge.","The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly.","This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods.","We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions.","Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training.","The dataset and code will be available at \\url{https://github.com/hyintell/RetrievalQA}"],"url":"http://arxiv.org/abs/2402.16457v1","category":"cs.CL"}
{"created":"2024-02-26 09:58:58","title":"Formal Degrees and Parabolic Induction: the Maximal Generic Case","abstract":"We study the compatibility of the formal degree conjecture and the parabolic induction process in the simplest nontrivial case for quasi-split $p$-adic groups. For a generic discrete series $\\pi$ induced from an irreducible supercuspidal $\\sigma$ of a maximal Levi subgroup, we compute the quotient $d(\\pi)/d(\\sigma)$ of formal degrees under some assumptions. As an application, we verify the conjecture for discrete series of split $\\mathrm{G}_2$ supported on maximal Levi subgroups.","sentences":["We study the compatibility of the formal degree conjecture and the parabolic induction process in the simplest nontrivial case for quasi-split $p$-adic groups.","For a generic discrete series $\\pi$ induced from an irreducible supercuspidal $\\sigma$ of a maximal Levi subgroup, we compute the quotient $d(\\pi)/d(\\sigma)$ of formal degrees under some assumptions.","As an application, we verify the conjecture for discrete series of split $\\mathrm{G}_2$ supported on maximal Levi subgroups."],"url":"http://arxiv.org/abs/2402.16456v1","category":"math.NT"}
{"created":"2024-02-26 09:58:36","title":"Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on Computational Fluid Dynamics Problems","abstract":"Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the most widely studied methods for their capability to solve expensive real-world optimization problems. However, the development of new methods and benchmarking with other techniques still relies almost exclusively on artificially created problems. In this paper, we use two real-world computational fluid dynamics problems to compare the performance of eleven state-of-the-art single-objective SAEAs. We analyze the performance by investigating the quality and robustness of the obtained solutions and the convergence properties of the selected methods. Our findings suggest that the more recently published methods, as well as the techniques that utilize differential evolution as one of their optimization mechanisms, perform significantly better than the other considered methods.","sentences":["Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the most widely studied methods for their capability to solve expensive real-world optimization problems.","However, the development of new methods and benchmarking with other techniques still relies almost exclusively on artificially created problems.","In this paper, we use two real-world computational fluid dynamics problems to compare the performance of eleven state-of-the-art single-objective SAEAs.","We analyze the performance by investigating the quality and robustness of the obtained solutions and the convergence properties of the selected methods.","Our findings suggest that the more recently published methods, as well as the techniques that utilize differential evolution as one of their optimization mechanisms, perform significantly better than the other considered methods."],"url":"http://arxiv.org/abs/2402.16455v1","category":"cs.NE"}
{"created":"2024-02-26 09:57:13","title":"Intelligent Reflecting Surfaces and Next Generation Wireless Systems","abstract":"Intelligent reflecting surface (IRS) is a potential candidate for massive multiple-input multiple-output (MIMO) 2.0 technology due to its low cost, ease of deployment, energy efficiency and extended coverage. This chapter investigates the slot-by-slot IRS reflection pattern design and two-timescale reflection pattern design schemes, respectively. For the slot-by-slot reflection optimization, we propose exploiting an IRS to improve the propagation channel rank in mmWave massive MIMO systems without need to increase the transmit power budget. Then, we analyze the impact of the distributed IRS on the channel rank. To further reduce the heavy overhead of channel training, channel state information (CSI) estimation, and feedback in time-varying MIMO channels, we present a two-timescale reflection optimization scheme, where the IRS is configured relatively infrequently based on statistical CSI (S-CSI) and the active beamformers and power allocation are updated based on quickly outdated instantaneous CSI (I-CSI) per slot. The achievable average sum-rate (AASR) of the system is maximized without excessive overhead of cascaded channel estimation. A recursive sampling particle swarm optimization (PSO) algorithm is developed to optimize the large-timescale IRS reflection pattern efficiently with reduced samplings of channel samples.","sentences":["Intelligent reflecting surface (IRS) is a potential candidate for massive multiple-input multiple-output (MIMO) 2.0 technology due to its low cost, ease of deployment, energy efficiency and extended coverage.","This chapter investigates the slot-by-slot IRS reflection pattern design and two-timescale reflection pattern design schemes, respectively.","For the slot-by-slot reflection optimization, we propose exploiting an IRS to improve the propagation channel rank in mmWave massive MIMO systems without need to increase the transmit power budget.","Then, we analyze the impact of the distributed IRS on the channel rank.","To further reduce the heavy overhead of channel training, channel state information (CSI) estimation, and feedback in time-varying MIMO channels, we present a two-timescale reflection optimization scheme, where the IRS is configured relatively infrequently based on statistical CSI (S-CSI) and the active beamformers and power allocation are updated based on quickly outdated instantaneous CSI (I-CSI) per slot.","The achievable average sum-rate (AASR) of the system is maximized without excessive overhead of cascaded channel estimation.","A recursive sampling particle swarm optimization (PSO) algorithm is developed to optimize the large-timescale IRS reflection pattern efficiently with reduced samplings of channel samples."],"url":"http://arxiv.org/abs/2402.16453v1","category":"eess.SP"}
{"created":"2024-02-26 09:56:17","title":"The Neumann sieve problem revisited","abstract":"Let $\\Omega$ be a domain in $\\mathbb{R}^n$, $\\Gamma$ be a hyperplane intersecting $\\Omega$, $\\varepsilon>0$ be a small parameter, and $\\Omega_\\varepsilon=\\Omega\\setminus\\overline{\\Sigma_\\varepsilon}$, where the set $\\Sigma_\\varepsilon$ has a geometry of a thin \"sieve\" - a layer of thickness $2\\varepsilon$ centered on $\\Gamma$ with a lot of drilled passages in it; when $\\varepsilon \\to 0$, the number of passages (per finite volume) tends to infinity, while the diameters of their cross-sections tend to zero. For the case of identical straight periodically distributed passages T. Del Vecchio [Ann. Mat. Pura Appl., 1987] proved that the Neumann Laplacian on $\\Omega_\\varepsilon$ converges in a kind of strong resolvent sense to the Laplacian on $\\Omega\\setminus\\Gamma$ subject to the so-called $\\delta'$-conditions on $\\Gamma$ provided the passages are appropriately scaled. In the current work we refine this result deriving estimates on the rate of convergence in terms of $L^2\\to L^2$ and $L^2\\to H^1$ operator norms; also we provide the estimate for the distance between the spectra of these operators in the weighted Hausdorff metrics. The assumptions we impose on the geometry and distribution of the passages are rather general; several examples obeying these assumptions are presented. For $n=2$ the results of T. Del Vecchio are not complete and some cases remain as open problems; we fill these gaps in the current work.","sentences":["Let $\\Omega$ be a domain in $\\mathbb{R}^n$, $\\Gamma$ be a hyperplane intersecting $\\Omega$, $\\varepsilon>0$ be a small parameter, and $\\Omega_\\varepsilon=\\Omega\\setminus\\overline{\\Sigma_\\varepsilon}$, where the set $\\Sigma_\\varepsilon$ has a geometry of a thin \"sieve\" - a layer of thickness $2\\varepsilon$ centered on $\\Gamma$ with a lot of drilled passages in it; when $\\varepsilon \\to 0$, the number of passages (per finite volume) tends to infinity, while the diameters of their cross-sections tend to zero.","For the case of identical straight periodically distributed passages T. Del Vecchio [Ann.","Mat.","Pura Appl., 1987] proved that the Neumann Laplacian on $\\Omega_\\varepsilon$ converges in a kind of strong resolvent sense to the Laplacian on $\\Omega\\setminus\\Gamma$ subject to the so-called $\\delta'$-conditions on $\\Gamma$ provided the passages are appropriately scaled.","In the current work we refine this result deriving estimates on the rate of convergence in terms of $L^2\\to L^2$ and $L^2\\to H^1$ operator norms; also we provide the estimate for the distance between the spectra of these operators in the weighted Hausdorff metrics.","The assumptions we impose on the geometry and distribution of the passages are rather general; several examples obeying these assumptions are presented.","For $n=2$ the results of T. Del Vecchio are not complete and some cases remain as open problems; we fill these gaps in the current work."],"url":"http://arxiv.org/abs/2402.16451v1","category":"math.AP"}
{"created":"2024-02-26 09:53:37","title":"Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments","abstract":"This paper proposes a LiDAR-based goal-seeking and exploration framework, addressing the efficiency of online obstacle avoidance in unstructured environments populated with static and moving obstacles. This framework addresses two significant challenges associated with traditional dynamic control barrier functions (D-CBFs): their online construction and the diminished real-time performance caused by utilizing multiple D-CBFs. To tackle the first challenge, the framework's perception component begins with clustering point clouds via the DBSCAN algorithm, followed by encapsulating these clusters with the minimum bounding ellipses (MBEs) algorithm to create elliptical representations. By comparing the current state of MBEs with those stored from previous moments, the differentiation between static and dynamic obstacles is realized, and the Kalman filter is utilized to predict the movements of the latter. Such analysis facilitates the D-CBF's online construction for each MBE. To tackle the second challenge, we introduce buffer zones, generating Type-II D-CBFs online for each identified obstacle. Utilizing these buffer zones as activation areas substantially reduces the number of D-CBFs that need to be activated. Upon entering these buffer zones, the system prioritizes safety, autonomously navigating safe paths, and hence referred to as the exploration mode. Exiting these buffer zones triggers the system's transition to goal-seeking mode. We demonstrate that the system's states under this framework achieve safety and asymptotic stabilization. Experimental results in simulated and real-world environments have validated our framework's capability, allowing a LiDAR-equipped mobile robot to efficiently and safely reach the desired location within dynamic environments containing multiple obstacles.","sentences":["This paper proposes a LiDAR-based goal-seeking and exploration framework, addressing the efficiency of online obstacle avoidance in unstructured environments populated with static and moving obstacles.","This framework addresses two significant challenges associated with traditional dynamic control barrier functions (D-CBFs): their online construction and the diminished real-time performance caused by utilizing multiple D-CBFs.","To tackle the first challenge, the framework's perception component begins with clustering point clouds via the DBSCAN algorithm, followed by encapsulating these clusters with the minimum bounding ellipses (MBEs) algorithm to create elliptical representations.","By comparing the current state of MBEs with those stored from previous moments, the differentiation between static and dynamic obstacles is realized, and the Kalman filter is utilized to predict the movements of the latter.","Such analysis facilitates the D-CBF's online construction for each MBE.","To tackle the second challenge, we introduce buffer zones, generating Type-II D-CBFs online for each identified obstacle.","Utilizing these buffer zones as activation areas substantially reduces the number of D-CBFs that need to be activated.","Upon entering these buffer zones, the system prioritizes safety, autonomously navigating safe paths, and hence referred to as the exploration mode.","Exiting these buffer zones triggers the system's transition to goal-seeking mode.","We demonstrate that the system's states under this framework achieve safety and asymptotic stabilization.","Experimental results in simulated and real-world environments have validated our framework's capability, allowing a LiDAR-equipped mobile robot to efficiently and safely reach the desired location within dynamic environments containing multiple obstacles."],"url":"http://arxiv.org/abs/2402.16449v1","category":"cs.RO"}
{"created":"2024-02-26 09:43:52","title":"ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing","abstract":"Large Language Models (LLMs), including GPT-x and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks. Under the premise that protein sequences constitute the protein language, Protein Large Language Models (ProLLMs) trained on protein corpora excel at de novo protein sequence generation. However, as of now, unlike LLMs in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field. This prompts us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands. To address these challenges, we introduce a training framework to transform any general LLM into a ProLLM capable of handling multiple PLP tasks. Specifically, our framework utilizes low-rank adaptation and employs a two-stage training approach, and it is distinguished by its universality, low overhead, and scalability. Through training under this framework, we propose the ProLLaMA model, the first known ProLLM to handle multiple PLP tasks simultaneously. Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task. In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities. In the protein property prediction task, ProLLaMA achieves nearly 100\\% accuracy across many categories. The latter two tasks are beyond the reach of other ProLLMs. Code is available at \\url{https://github.com/Lyu6PosHao/ProLLaMA}.","sentences":["Large Language Models (LLMs), including GPT-x and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks.","Under the premise that protein sequences constitute the protein language, Protein Large Language Models (ProLLMs) trained on protein corpora excel at de novo protein sequence generation.","However, as of now, unlike LLMs in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field.","This prompts us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands.","To address these challenges, we introduce a training framework to transform any general LLM into a ProLLM capable of handling multiple PLP tasks.","Specifically, our framework utilizes low-rank adaptation and employs a two-stage training approach, and it is distinguished by its universality, low overhead, and scalability.","Through training under this framework, we propose the ProLLaMA model, the first known ProLLM to handle multiple PLP tasks simultaneously.","Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task.","In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities.","In the protein property prediction task, ProLLaMA achieves nearly 100\\% accuracy across many categories.","The latter two tasks are beyond the reach of other ProLLMs.","Code is available at \\url{https://github.com/Lyu6PosHao/ProLLaMA}."],"url":"http://arxiv.org/abs/2402.16445v1","category":"cs.CE"}
{"created":"2024-02-26 09:43:02","title":"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors","abstract":"The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We release ShieldLM at \\url{https://github.com/thu-coai/ShieldLM} to support accurate and explainable safety detection under various safety standards, contributing to the ongoing efforts to enhance the safety of LLMs.","sentences":["The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner.","In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions.","To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards.","Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability.","Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs.","We release ShieldLM at \\url{https://github.com/thu-coai/ShieldLM} to support accurate and explainable safety detection under various safety standards, contributing to the ongoing efforts to enhance the safety of LLMs."],"url":"http://arxiv.org/abs/2402.16444v1","category":"cs.CL"}
{"created":"2024-02-26 09:38:39","title":"On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions","abstract":"Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, partition-based distributed greedy algorithm to identify the remaining subset. We show that these algorithms find high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in quality compared to centralized methods, and scale to a dataset with 13 billion points.","sentences":["Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points.","For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality.","Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems.","However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset.","In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees.","The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones.","When bounding does not find the complete subset, we use a multi-round, partition-based distributed greedy algorithm to identify the remaining subset.","We show that these algorithms find high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in quality compared to centralized methods, and scale to a dataset with 13 billion points."],"url":"http://arxiv.org/abs/2402.16442v1","category":"cs.LG"}
{"created":"2024-02-26 09:37:24","title":"Retrouver l'inventeur-auteur : la lev{\u00e9}e d'homonymies d'autorat entre les brevets et les publications scientifiques","abstract":"Patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes. Authors' and inventors' names are the key identifiers to carry out these analyses, which however, run up against the issue of disambiguation. By extension identifying inventors who are also academic authors is a non-trivial challenge. We propose a method using the International Patent Classification (IPC) and the IPCCAT API to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents. The method is developed and manually qualified based on three corpora of patents extracted from the international EPO database Espacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501 authors. The proposed algorithm solves the general problem of disambiguation with an error rate lower than 5%.","sentences":["Patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes.","Authors' and inventors' names are the key identifiers to carry out these analyses, which however, run up against the issue of disambiguation.","By extension identifying inventors who are also academic authors is a non-trivial challenge.","We propose a method using the International Patent Classification (IPC) and the IPCCAT API to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents.","The method is developed and manually qualified based on three corpora of patents extracted from the international EPO database Espacenet.","Among a set of 4679 patents and 7720 inventors, we obtain 2501 authors.","The proposed algorithm solves the general problem of disambiguation with an error rate lower than 5%."],"url":"http://arxiv.org/abs/2402.16440v1","category":"cs.IR"}
{"created":"2024-02-26 09:32:28","title":"Training Implicit Generative Models via an Invariant Statistical Loss","abstract":"Implicit generative models have the capability to learn arbitrary complex data distributions. On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal. In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases. Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization of arbitrary complex distributions. Then, we consider the temporal setting (both univariate and multivariate), in which we model the conditional distribution of each sample given the history of the process. We demonstrate through numerical simulations that this new method yields promising results, successfully learning true distributions in a variety of scenarios and mitigating some of the well-known problems that state-of-the-art implicit methods present.","sentences":["Implicit generative models have the capability to learn arbitrary complex data distributions.","On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues.","As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal.","In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases.","Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data.","We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization of arbitrary complex distributions.","Then, we consider the temporal setting (both univariate and multivariate), in which we model the conditional distribution of each sample given the history of the process.","We demonstrate through numerical simulations that this new method yields promising results, successfully learning true distributions in a variety of scenarios and mitigating some of the well-known problems that state-of-the-art implicit methods present."],"url":"http://arxiv.org/abs/2402.16435v1","category":"cs.LG"}
{"created":"2024-02-26 09:31:46","title":"Optimization of the Downlink Spectral- and Energy-Efficiency of RIS-aided Multi-user URLLC MIMO Systems","abstract":"Modern wireless communication systems are expected to provide improved latency and reliability. To meet these expectations, a short packet length is needed, which makes the first-order Shannon rate an inaccurate performance metric for such communication systems. A more accurate approximation of the achievable rates of finite-block-length (FBL) coding regimes is known as the normal approximation (NA). It is therefore of substantial interest to study the optimization of the FBL rate in multi-user multiple-input multiple-output (MIMO) systems, in which each user may transmit and/or receive multiple data streams. Hence, we formulate a general optimization problem for improving the spectral and energy efficiency of multi-user MIMO-aided ultra-reliable low-latency communication (URLLC) systems, which are assisted by reconfigurable intelligent surfaces (RISs). We show that a RIS is capable of substantially improving the performance of multi-user MIMO-aided URLLC systems. Moreover, the benefits of RIS increase as the packet length and/or the tolerable bit error rate are reduced. This reveals that RISs can be even more beneficial in URLLC systems for improving the FBL rates than in conventional systems approaching Shannon rates.","sentences":["Modern wireless communication systems are expected to provide improved latency and reliability.","To meet these expectations, a short packet length is needed, which makes the first-order Shannon rate an inaccurate performance metric for such communication systems.","A more accurate approximation of the achievable rates of finite-block-length (FBL) coding regimes is known as the normal approximation (NA).","It is therefore of substantial interest to study the optimization of the FBL rate in multi-user multiple-input multiple-output (MIMO) systems, in which each user may transmit and/or receive multiple data streams.","Hence, we formulate a general optimization problem for improving the spectral and energy efficiency of multi-user MIMO-aided ultra-reliable low-latency communication (URLLC) systems, which are assisted by reconfigurable intelligent surfaces (RISs).","We show that a RIS is capable of substantially improving the performance of multi-user MIMO-aided URLLC systems.","Moreover, the benefits of RIS increase as the packet length and/or the tolerable bit error rate are reduced.","This reveals that RISs can be even more beneficial in URLLC systems for improving the FBL rates than in conventional systems approaching Shannon rates."],"url":"http://arxiv.org/abs/2402.16434v1","category":"cs.IT"}
{"created":"2024-02-26 09:29:05","title":"Improving behavior based authentication against adversarial attack using XAI","abstract":"In recent years, machine learning models, especially deep neural networks, have been widely used for classification tasks in the security domain. However, these models have been shown to be vulnerable to adversarial manipulation: small changes learned by an adversarial attack model, when applied to the input, can cause significant changes in the output. Most research on adversarial attacks and corresponding defense methods focuses only on scenarios where adversarial samples are directly generated by the attack model. In this study, we explore a more practical scenario in behavior-based authentication, where adversarial samples are collected from the attacker. The generated adversarial samples from the model are replicated by attackers with a certain level of discrepancy. We propose an eXplainable AI (XAI) based defense strategy against adversarial attacks in such scenarios. A feature selector, trained with our method, can be used as a filter in front of the original authenticator. It filters out features that are more vulnerable to adversarial attacks or irrelevant to authentication, while retaining features that are more robust. Through comprehensive experiments, we demonstrate that our XAI based defense strategy is effective against adversarial attacks and outperforms other defense strategies, such as adversarial training and defensive distillation.","sentences":["In recent years, machine learning models, especially deep neural networks, have been widely used for classification tasks in the security domain.","However, these models have been shown to be vulnerable to adversarial manipulation: small changes learned by an adversarial attack model, when applied to the input, can cause significant changes in the output.","Most research on adversarial attacks and corresponding defense methods focuses only on scenarios where adversarial samples are directly generated by the attack model.","In this study, we explore a more practical scenario in behavior-based authentication, where adversarial samples are collected from the attacker.","The generated adversarial samples from the model are replicated by attackers with a certain level of discrepancy.","We propose an eXplainable AI (XAI) based defense strategy against adversarial attacks in such scenarios.","A feature selector, trained with our method, can be used as a filter in front of the original authenticator.","It filters out features that are more vulnerable to adversarial attacks or irrelevant to authentication, while retaining features that are more robust.","Through comprehensive experiments, we demonstrate that our XAI based defense strategy is effective against adversarial attacks and outperforms other defense strategies, such as adversarial training and defensive distillation."],"url":"http://arxiv.org/abs/2402.16430v1","category":"cs.CR"}
{"created":"2024-02-26 09:28:16","title":"Effect of utterance duration and phonetic content on speaker identification using second-order statistical methods","abstract":"Second-order statistical methods show very good results for automatic speaker identification in controlled recording conditions. These approaches are generally used on the entire speech material available. In this paper, we study the influence of the content of the test speech material on the performances of such methods, i.e. under a more analytical approach. The goal is to investigate on the kind of information which is used by these methods, and where it is located in the speech signal. Liquids and glides together, vowels, and more particularly nasal vowels and nasal consonants, are found to be particularly speaker specific: test utterances of 1 second, composed in majority of acoustic material from one of these classes provide better speaker identification results than phonetically balanced test utterances, even though the training is done, in both cases, with 15 seconds of phonetically balanced speech. Nevertheless, results with other phoneme classes are never dramatically poor. These results tend to show that the speaker-dependent information captured by long-term second-order statistics is consistently common to all phonetic classes, and that the homogeneity of the test material may improve the quality of the estimates.","sentences":["Second-order statistical methods show very good results for automatic speaker identification in controlled recording conditions.","These approaches are generally used on the entire speech material available.","In this paper, we study the influence of the content of the test speech material on the performances of such methods, i.e. under a more analytical approach.","The goal is to investigate on the kind of information which is used by these methods, and where it is located in the speech signal.","Liquids and glides together, vowels, and more particularly nasal vowels and nasal consonants, are found to be particularly speaker specific: test utterances of 1 second, composed in majority of acoustic material from one of these classes provide better speaker identification results than phonetically balanced test utterances, even though the training is done, in both cases, with 15 seconds of phonetically balanced speech.","Nevertheless, results with other phoneme classes are never dramatically poor.","These results tend to show that the speaker-dependent information captured by long-term second-order statistics is consistently common to all phonetic classes, and that the homogeneity of the test material may improve the quality of the estimates."],"url":"http://arxiv.org/abs/2402.16429v1","category":"cs.IR"}
{"created":"2024-02-26 09:22:57","title":"COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing","abstract":"Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency and generalization in large-scale retrieval scenarios. While considerable success has been achieved, there still exist urgent limitations. Existing works ignore the locality relationships of representations and attributes, which have effective transferability between seeable classes and unseeable classes. Also, the continuous-value attributes are not fully harnessed. In response, we conduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which depicts the relationships from seen classes to unseen ones through three meticulously designed explorations, i.e., point-wise, pair-wise and class-wise consistency constraints. By regressing attributes from the proposed attribute prototype network, COMAE learns the local features that are relevant to the visual attributes. Then COMAE utilizes contrastive learning to comprehensively depict the context of attributes, rather than instance-independent optimization. Finally, the class-wise constraint is designed to cohesively learn the hash code, image representation, and visual attributes more effectively. Experimental results on the popular ZSH datasets demonstrate that COMAE outperforms state-of-the-art hashing techniques, especially in scenarios with a larger number of unseen label classes.","sentences":["Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency and generalization in large-scale retrieval scenarios.","While considerable success has been achieved, there still exist urgent limitations.","Existing works ignore the locality relationships of representations and attributes, which have effective transferability between seeable classes and unseeable classes.","Also, the continuous-value attributes are not fully harnessed.","In response, we conduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which depicts the relationships from seen classes to unseen ones through three meticulously designed explorations, i.e., point-wise, pair-wise and class-wise consistency constraints.","By regressing attributes from the proposed attribute prototype network, COMAE learns the local features that are relevant to the visual attributes.","Then COMAE utilizes contrastive learning to comprehensively depict the context of attributes, rather than instance-independent optimization.","Finally, the class-wise constraint is designed to cohesively learn the hash code, image representation, and visual attributes more effectively.","Experimental results on the popular ZSH datasets demonstrate that COMAE outperforms state-of-the-art hashing techniques, especially in scenarios with a larger number of unseen label classes."],"url":"http://arxiv.org/abs/2402.16424v1","category":"cs.CV"}
{"created":"2024-02-26 09:21:17","title":"Outline-Guided Object Inpainting with Diffusion Models","abstract":"Instance segmentation datasets play a crucial role in training accurate and robust computer vision models. However, obtaining accurate mask annotations to produce high-quality segmentation datasets is a costly and labor-intensive process. In this work, we show how this issue can be mitigated by starting with small annotated instance segmentation datasets and augmenting them to effectively obtain a sizeable annotated dataset. We achieve that by creating variations of the available annotated object instances in a way that preserves the provided mask annotations, thereby resulting in new image-mask pairs to be added to the set of annotated images. Specifically, we generate new images using a diffusion-based inpainting model to fill out the masked area with a desired object class by guiding the diffusion through the object outline. We show that the object outline provides a simple, but also reliable and convenient training-free guidance signal for the underlying inpainting model that is often sufficient to fill out the mask with an object of the correct class without further text guidance and preserve the correspondence between generated images and the mask annotations with high precision. Our experimental results reveal that our method successfully generates realistic variations of object instances, preserving their shape characteristics while introducing diversity within the augmented area. We also show that the proposed method can naturally be combined with text guidance and other image augmentation techniques.","sentences":["Instance segmentation datasets play a crucial role in training accurate and robust computer vision models.","However, obtaining accurate mask annotations to produce high-quality segmentation datasets is a costly and labor-intensive process.","In this work, we show how this issue can be mitigated by starting with small annotated instance segmentation datasets and augmenting them to effectively obtain a sizeable annotated dataset.","We achieve that by creating variations of the available annotated object instances in a way that preserves the provided mask annotations, thereby resulting in new image-mask pairs to be added to the set of annotated images.","Specifically, we generate new images using a diffusion-based inpainting model to fill out the masked area with a desired object class by guiding the diffusion through the object outline.","We show that the object outline provides a simple, but also reliable and convenient training-free guidance signal for the underlying inpainting model that is often sufficient to fill out the mask with an object of the correct class without further text guidance and preserve the correspondence between generated images and the mask annotations with high precision.","Our experimental results reveal that our method successfully generates realistic variations of object instances, preserving their shape characteristics while introducing diversity within the augmented area.","We also show that the proposed method can naturally be combined with text guidance and other image augmentation techniques."],"url":"http://arxiv.org/abs/2402.16421v1","category":"cs.CV"}
{"created":"2024-02-26 09:19:46","title":"Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models","abstract":"We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was BART with an F1-score of 0.786.","sentences":["We present our work on predicting United Nations sustainable development goals (SDG) for university courses.","We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input.","We use this data to train several different smaller language models to predict SDGs for university courses.","This work contributes to better university level adaptation of SDGs.","The best performing model in our experiments was BART with an F1-score of 0.786."],"url":"http://arxiv.org/abs/2402.16420v1","category":"cs.CL"}
{"created":"2024-02-26 09:12:27","title":"Achievable Rate Optimization for Stacked Intelligent Metasurface-Assisted Holographic MIMO Communications","abstract":"Stacked intelligent metasurfaces (SIM) is a revolutionary technology, which can outperform its single-layer counterparts by performing advanced signal processing relying on wave propagation. In this work, we exploit SIM to enable transmit precoding and receiver combining in holographic multiple-input multiple-output (HMIMO) communications, and we study the achievable rate by formulating a joint optimization problem of the SIM phase shifts at both sides of the transceiver and the covariance matrix of the transmitted signal. Notably, we propose its solution by means of an iterative optimization algorithm that relies on the projected gradient method, and accounts for all optimization parameters simultaneously. We also obtain the step size guaranteeing the convergence of the proposed algorithm. Simulation results provide fundamental insights such the performance improvements compared to the single-RIS counterpart and conventional MIMO system. Remarkably, the proposed algorithm results in the same achievable rate as the alternating optimization (AO) benchmark but with a less number of iterations.","sentences":["Stacked intelligent metasurfaces (SIM) is a revolutionary technology, which can outperform its single-layer counterparts by performing advanced signal processing relying on wave propagation.","In this work, we exploit SIM to enable transmit precoding and receiver combining in holographic multiple-input multiple-output (HMIMO) communications, and we study the achievable rate by formulating a joint optimization problem of the SIM phase shifts at both sides of the transceiver and the covariance matrix of the transmitted signal.","Notably, we propose its solution by means of an iterative optimization algorithm that relies on the projected gradient method, and accounts for all optimization parameters simultaneously.","We also obtain the step size guaranteeing the convergence of the proposed algorithm.","Simulation results provide fundamental insights such the performance improvements compared to the single-RIS counterpart and conventional MIMO system.","Remarkably, the proposed algorithm results in the same achievable rate as the alternating optimization (AO) benchmark but with a less number of iterations."],"url":"http://arxiv.org/abs/2402.16415v1","category":"cs.IT"}
{"created":"2024-02-26 09:11:16","title":"AI-enabled STAR-RIS aided MISO ISAC Secure Communications","abstract":"A simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) aided integrated sensing and communication (ISAC) dual-secure communication system is studied in this paper. The sensed target and legitimate users (LUs) are situated on the opposite sides of the STAR-RIS, and the energy splitting and time switching protocols are applied in the STAR-RIS, respectively. The long-term average security rate for LUs is maximized by the joint design of the base station (BS) transmit beamforming and receive filter, along with the STAR-RIS transmitting and reflecting coefficients, under guarantying the echo signal-to-noise ratio thresholds and rate constraints for the LUs. Since the channel information changes over time, conventional convex optimization techniques cannot provide the optimal performance for the system, and result in excessively high computational complexity in the exploration of the long-term gains for the system. Taking continuity control decisions into account, the deep deterministic policy gradient and soft actor-critic algorithms based on off-policy are applied to address the complex non-convex problem. Simulation results comprehensively evaluate the performance of the proposed two reinforcement learning algorithms and demonstrate that STAR-RIS is remarkably better than the two benchmarks in the ISAC system.","sentences":["A simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) aided integrated sensing and communication (ISAC) dual-secure communication system is studied in this paper.","The sensed target and legitimate users (LUs) are situated on the opposite sides of the STAR-RIS, and the energy splitting and time switching protocols are applied in the STAR-RIS, respectively.","The long-term average security rate for LUs is maximized by the joint design of the base station (BS) transmit beamforming and receive filter, along with the STAR-RIS transmitting and reflecting coefficients, under guarantying the echo signal-to-noise ratio thresholds and rate constraints for the LUs.","Since the channel information changes over time, conventional convex optimization techniques cannot provide the optimal performance for the system, and result in excessively high computational complexity in the exploration of the long-term gains for the system.","Taking continuity control decisions into account, the deep deterministic policy gradient and soft actor-critic algorithms based on off-policy are applied to address the complex non-convex problem.","Simulation results comprehensively evaluate the performance of the proposed two reinforcement learning algorithms and demonstrate that STAR-RIS is remarkably better than the two benchmarks in the ISAC system."],"url":"http://arxiv.org/abs/2402.16413v1","category":"eess.SP"}
{"created":"2024-02-26 09:11:12","title":"TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis","abstract":"The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., training a single model on many domains) settings, and show that TOTEM matches or outperforms previous best methods on several popular benchmarks. The code can be found at: https://github.com/SaberaTalukder/TOTEM.","sentences":["The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset.","In this work, we approach unification from a complementary vantage point: unification across tasks and domains.","To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training.","Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner.","TOTEM works across multiple tasks and domains with minimal to no tuning.","We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks.","We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., training a single model on many domains) settings, and show that TOTEM matches or outperforms previous best methods on several popular benchmarks.","The code can be found at: https://github.com/SaberaTalukder/TOTEM."],"url":"http://arxiv.org/abs/2402.16412v1","category":"cs.LG"}
{"created":"2024-02-26 08:59:05","title":"From RAGs to riches: Using large language models to write documents for clinical trials","abstract":"Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.","sentences":["Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others.","Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols.","We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology.","However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references.","To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information.","As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing."],"url":"http://arxiv.org/abs/2402.16406v1","category":"cs.CL"}
{"created":"2024-02-26 08:57:57","title":"Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser Massive MIMO Communications in the Wave Domain","abstract":"Although reconfigurable intelligent surface (RIS) is a promising technology for shaping the propagation environment, it consists of a single-layer structure within inherent limitations regarding the number of beam steering patterns. Based on the recently revolutionary technology, denoted as stacked intelligent metasurface (SIM), we propose its implementation not only on the base station (BS) side in a massive multiple-input multiple-output (mMIMO) setup but also in the intermediate space between the base station and the users to adjust the environment further as needed. For the sake of convenience, we call the former BS SIM (BSIM), and the latter channel SIM (CSIM). Hence, we achieve wave-based combining at the BS and wave-based configuration at the intermediate space. Specifically, we propose a channel estimation method with reduced overhead, being crucial for SIMassisted communications. Next, we derive the uplink sum spectral efficiency (SE) in closed form in terms of statistical channel state information (CSI). Notably, we optimize the phase shifts of both BSIM and CSIM simultaneously by using the projected gradient ascent method (PGAM). Compared to previous works on SIMs, we study the uplink transmission, a mMIMO setup, channel estimation in a single phase, a second SIM at the intermediate space, and simultaneous optimization of the two SIMs. Simulation results show the impact of various parameters on the sum SE, and demonstrate the superiority of our optimization approach compared to the alternating optimization (AO) method.","sentences":["Although reconfigurable intelligent surface (RIS) is a promising technology for shaping the propagation environment, it consists of a single-layer structure within inherent limitations regarding the number of beam steering patterns.","Based on the recently revolutionary technology, denoted as stacked intelligent metasurface (SIM), we propose its implementation not only on the base station (BS) side in a massive multiple-input multiple-output (mMIMO) setup but also in the intermediate space between the base station and the users to adjust the environment further as needed.","For the sake of convenience, we call the former BS SIM (BSIM), and the latter channel SIM (CSIM).","Hence, we achieve wave-based combining at the BS and wave-based configuration at the intermediate space.","Specifically, we propose a channel estimation method with reduced overhead, being crucial for SIMassisted communications.","Next, we derive the uplink sum spectral efficiency (SE) in closed form in terms of statistical channel state information (CSI).","Notably, we optimize the phase shifts of both BSIM and CSIM simultaneously by using the projected gradient ascent method (PGAM).","Compared to previous works on SIMs, we study the uplink transmission, a mMIMO setup, channel estimation in a single phase, a second SIM at the intermediate space, and simultaneous optimization of the two SIMs.","Simulation results show the impact of various parameters on the sum SE, and demonstrate the superiority of our optimization approach compared to the alternating optimization (AO) method."],"url":"http://arxiv.org/abs/2402.16405v1","category":"cs.IT"}
{"created":"2024-02-26 08:57:36","title":"Exploring spatial dispersion in helical wired media: An effective field theory approach","abstract":"The propagation of electromagnetic waves in helical media with spatial dispersion is investigated. The general form of the permittivity tensor with spatial dispersion obeying the helical symmetry is derived. Its particular form describing the medium made of conducting spiral wires with pitch $2\\pi/|q|$ is studied in detail. The solution of the corresponding Maxwell equations is obtained in the paraxial limit. The dispersion law of the electromagnetic field modes, their polarization, and the integral curves of the Poynting vector are analyzed. The dispersion law of photons in such a medium possesses polarization dependent forbidden bands. The widths of these gaps and their positions are tunable in a wide range of energies. If the helix angle $\\alpha$ is not close to $\\pi/2$ and the plasma frequency $\\omega_p\\ll|q|$, then there are two chiral forbidden bands. The energies of one chiral forbidden band are near the plasma frequency $\\omega_p$ and the width of this gap is of order $|q|$. The other chiral forbidden band is narrow and is located near the photon energy $|q|$. In the case $\\alpha\\approx\\pi/2$, the first chiral forbidden band becomes a total forbidden band. If, additionally, the plasma frequency $\\omega_p\\gg|q|$, then the second forbidden band turns into a wide polarization dependent forbidden band. For the energies belonging to this interval the photons with only one linear polarization are transmitted through the medium and the polarization plane of transmitted photons is rotated. In the nonparaxial regime, the solution of the Maxwell equations is obtained in the shortwave approximation. The dispersion law of the electromagnetic field modes, their polarization, and the integral curves of the Poynting vector are found. Scattering of the electromagnetic waves by a slab made of the helical wired medium is considered.","sentences":["The propagation of electromagnetic waves in helical media with spatial dispersion is investigated.","The general form of the permittivity tensor with spatial dispersion obeying the helical symmetry is derived.","Its particular form describing the medium made of conducting spiral wires with pitch $2\\pi/|q|$ is studied in detail.","The solution of the corresponding Maxwell equations is obtained in the paraxial limit.","The dispersion law of the electromagnetic field modes, their polarization, and the integral curves of the Poynting vector are analyzed.","The dispersion law of photons in such a medium possesses polarization dependent forbidden bands.","The widths of these gaps and their positions are tunable in a wide range of energies.","If the helix angle $\\alpha$ is not close to $\\pi/2$ and the plasma frequency $\\omega_p\\ll|q|$, then there are two chiral forbidden bands.","The energies of one chiral forbidden band are near the plasma frequency $\\omega_p$ and the width of this gap is of order $|q|$.","The other chiral forbidden band is narrow and is located near the photon energy $|q|$. In the case $\\alpha\\approx\\pi/2$, the first chiral forbidden band becomes a total forbidden band.","If, additionally, the plasma frequency $\\omega_p\\gg|q|$, then the second forbidden band turns into a wide polarization dependent forbidden band.","For the energies belonging to this interval the photons with only one linear polarization are transmitted through the medium and the polarization plane of transmitted photons is rotated.","In the nonparaxial regime, the solution of the Maxwell equations is obtained in the shortwave approximation.","The dispersion law of the electromagnetic field modes, their polarization, and the integral curves of the Poynting vector are found.","Scattering of the electromagnetic waves by a slab made of the helical wired medium is considered."],"url":"http://arxiv.org/abs/2402.16404v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-26 08:55:10","title":"Graph Learning with Distributional Edge Layouts","abstract":"Graph Neural Networks (GNNs) learn from graph-structured data by passing local messages between neighboring nodes along edges on certain topological layouts. Typically, these topological layouts in modern GNNs are deterministically computed (e.g., attention-based GNNs) or locally sampled (e.g., GraphSage) under heuristic assumptions. In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world. We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of GNNs. DEL is a pre-processing strategy independent of subsequent GNN variants, thus being highly flexible. Experimental results demonstrate that DELs consistently and substantially improve a series of GNN baselines, achieving state-of-the-art performance on multiple datasets.","sentences":["Graph Neural Networks (GNNs) learn from graph-structured data by passing local messages between neighboring nodes along edges on certain topological layouts.","Typically, these topological layouts in modern GNNs are deterministically computed (e.g., attention-based GNNs) or locally sampled (e.g., GraphSage) under heuristic assumptions.","In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world.","We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks.","As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of GNNs.","DEL is a pre-processing strategy independent of subsequent GNN variants, thus being highly flexible.","Experimental results demonstrate that DELs consistently and substantially improve a series of GNN baselines, achieving state-of-the-art performance on multiple datasets."],"url":"http://arxiv.org/abs/2402.16402v1","category":"cs.LG"}
{"created":"2024-02-26 08:53:29","title":"A Stationary Equilibrium Model of Green Technology Adoption with Endogenous Carbon Price","abstract":"This paper proposes and analyzes a stationary equilibrium model for a competitive industry which endogenously determines the carbon price necessary to achieve a given emission target. In the model, firms are identified by their level of technology and make production, entry, and abatement decisions. Polluting firms are subject to a carbon price and abatement is formulated as an irreversible investment, which entails a sunk cost and results in the firms switching to a carbon neutral technology. In equilibrium, we identify a carbon price and a stationary distribution of incumbent, polluting firms, that guarantee the compliance with a certain emission target. Our general theoretical framework is complemented with a case study with Brownian technology shocks, in which we discuss some implications of our model. We observe that a carbon pricing system alongside installation subsidies and tax benefits for green firms trigger earlier investment, while higher income taxes for polluting firms may be distorting. Moreover, we discuss the role of a welfare maximizing regulator, who, by optimally setting the emission target, may mitigate or revert some parameters' effects observed in the model with fixed limit.","sentences":["This paper proposes and analyzes a stationary equilibrium model for a competitive industry which endogenously determines the carbon price necessary to achieve a given emission target.","In the model, firms are identified by their level of technology and make production, entry, and abatement decisions.","Polluting firms are subject to a carbon price and abatement is formulated as an irreversible investment, which entails a sunk cost and results in the firms switching to a carbon neutral technology.","In equilibrium, we identify a carbon price and a stationary distribution of incumbent, polluting firms, that guarantee the compliance with a certain emission target.","Our general theoretical framework is complemented with a case study with Brownian technology shocks, in which we discuss some implications of our model.","We observe that a carbon pricing system alongside installation subsidies and tax benefits for green firms trigger earlier investment, while higher income taxes for polluting firms may be distorting.","Moreover, we discuss the role of a welfare maximizing regulator, who, by optimally setting the emission target, may mitigate or revert some parameters' effects observed in the model with fixed limit."],"url":"http://arxiv.org/abs/2402.16401v1","category":"q-fin.MF"}
{"created":"2024-02-26 08:47:35","title":"Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations","abstract":"Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes. The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state. Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras. However, most of the adopted methods have poor real-time performance. To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations. Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3). Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression. Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art. Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively.","sentences":["Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes.","The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state.","Recent works have adopted the continuous-time ego-motion estimation methods to exploit the inherent nature of event cameras.","However, most of the adopted methods have poor real-time performance.","To alleviate it, a lightweight Gaussian Process (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations.","Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3).","Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression.","Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art.","Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively."],"url":"http://arxiv.org/abs/2402.16398v1","category":"cs.RO"}
{"created":"2024-02-26 08:41:14","title":"Investigating Deep Watermark Security: An Adversarial Transferability Perspective","abstract":"The rise of generative neural networks has triggered an increased demand for intellectual property (IP) protection in generated content. Deep watermarking techniques, recognized for their flexibility in IP protection, have garnered significant attention. However, the surge in adversarial transferable attacks poses unprecedented challenges to the security of deep watermarking techniques-an area currently lacking systematic investigation. This study fills this gap by introducing two effective transferable attackers to assess the vulnerability of deep watermarks against erasure and tampering risks. Specifically, we initially define the concept of local sample density, utilizing it to deduce theorems on the consistency of model outputs. Upon discovering that perturbing samples towards high sample density regions (HSDR) of the target class enhances targeted adversarial transferability, we propose the Easy Sample Selection (ESS) mechanism and the Easy Sample Matching Attack (ESMA) method. Additionally, we propose the Bottleneck Enhanced Mixup (BEM) that integrates information bottleneck theory to reduce the generator's dependence on irrelevant noise. Experiments show a significant enhancement in the success rate of targeted transfer attacks for both ESMA and BEM-ESMA methods. We further conduct a comprehensive evaluation using ESMA and BEM-ESMA as measurements, considering model architecture and watermark encoding length, and achieve some impressive findings.","sentences":["The rise of generative neural networks has triggered an increased demand for intellectual property (IP) protection in generated content.","Deep watermarking techniques, recognized for their flexibility in IP protection, have garnered significant attention.","However, the surge in adversarial transferable attacks poses unprecedented challenges to the security of deep watermarking techniques-an area currently lacking systematic investigation.","This study fills this gap by introducing two effective transferable attackers to assess the vulnerability of deep watermarks against erasure and tampering risks.","Specifically, we initially define the concept of local sample density, utilizing it to deduce theorems on the consistency of model outputs.","Upon discovering that perturbing samples towards high sample density regions (HSDR) of the target class enhances targeted adversarial transferability, we propose the Easy Sample Selection (ESS) mechanism and the Easy Sample Matching Attack (ESMA) method.","Additionally, we propose the Bottleneck Enhanced Mixup (BEM) that integrates information bottleneck theory to reduce the generator's dependence on irrelevant noise.","Experiments show a significant enhancement in the success rate of targeted transfer attacks for both ESMA and BEM-ESMA methods.","We further conduct a comprehensive evaluation using ESMA and BEM-ESMA as measurements, considering model architecture and watermark encoding length, and achieve some impressive findings."],"url":"http://arxiv.org/abs/2402.16397v1","category":"cs.CR"}
{"created":"2024-02-26 08:40:15","title":"Recurrence and transience of step-reinforced random walks","abstract":"We solve a conjecture raised by Bertoin (personal communication) on the step-reinforced random walk, a non-Markovian process generalizing the elephant random walk. We show that, under a 2+$\\delta$-th moment condition, it undergoes a phase transition in dimensions $d=1,2$ between recurrence and transience, and that it is transient for all parameters in dimension $d\\geq 3$.","sentences":["We solve a conjecture raised by Bertoin (personal communication) on the step-reinforced random walk, a non-Markovian process generalizing the elephant random walk.","We show that, under a 2+$\\delta$-th moment condition, it undergoes a phase transition in dimensions $d=1,2$ between recurrence and transience, and that it is transient for all parameters in dimension $d\\geq 3$."],"url":"http://arxiv.org/abs/2402.16396v1","category":"math.PR"}
{"created":"2024-02-26 08:38:32","title":"Audio-Visual Speech Enhancement in Noisy Environments via Emotion-Based Contextual Cues","abstract":"In real-world environments, background noise significantly degrades the intelligibility and clarity of human speech. Audio-visual speech enhancement (AVSE) attempts to restore speech quality, but existing methods often fall short, particularly in dynamic noise conditions. This study investigates the inclusion of emotion as a novel contextual cue within AVSE, hypothesizing that incorporating emotional understanding can improve speech enhancement performance. We propose a novel emotion-aware AVSE system that leverages both auditory and visual information. It extracts emotional features from the facial landmarks of the speaker and fuses them with corresponding audio and visual modalities. This enriched data serves as input to a deep UNet-based encoder-decoder network, specifically designed to orchestrate the fusion of multimodal information enhanced with emotion. The network iteratively refines the enhanced speech representation through an encoder-decoder architecture, guided by perceptually-inspired loss functions for joint learning and optimization. We train and evaluate the model on the CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, a rich repository of audio-visual recordings with annotated emotions. Our comprehensive evaluation demonstrates the effectiveness of emotion as a contextual cue for AVSE. By integrating emotional features, the proposed system achieves significant improvements in both objective and subjective assessments of speech quality and intelligibility, especially in challenging noise environments. Compared to baseline AVSE and audio-only speech enhancement systems, our approach exhibits a noticeable increase in PESQ and STOI, indicating higher perceptual quality and intelligibility. Large-scale listening tests corroborate these findings, suggesting improved human understanding of enhanced speech.","sentences":["In real-world environments, background noise significantly degrades the intelligibility and clarity of human speech.","Audio-visual speech enhancement (AVSE) attempts to restore speech quality, but existing methods often fall short, particularly in dynamic noise conditions.","This study investigates the inclusion of emotion as a novel contextual cue within AVSE, hypothesizing that incorporating emotional understanding can improve speech enhancement performance.","We propose a novel emotion-aware AVSE system that leverages both auditory and visual information.","It extracts emotional features from the facial landmarks of the speaker and fuses them with corresponding audio and visual modalities.","This enriched data serves as input to a deep UNet-based encoder-decoder network, specifically designed to orchestrate the fusion of multimodal information enhanced with emotion.","The network iteratively refines the enhanced speech representation through an encoder-decoder architecture, guided by perceptually-inspired loss functions for joint learning and optimization.","We train and evaluate the model on the CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, a rich repository of audio-visual recordings with annotated emotions.","Our comprehensive evaluation demonstrates the effectiveness of emotion as a contextual cue for AVSE.","By integrating emotional features, the proposed system achieves significant improvements in both objective and subjective assessments of speech quality and intelligibility, especially in challenging noise environments.","Compared to baseline AVSE and audio-only speech enhancement systems, our approach exhibits a noticeable increase in PESQ and STOI, indicating higher perceptual quality and intelligibility.","Large-scale listening tests corroborate these findings, suggesting improved human understanding of enhanced speech."],"url":"http://arxiv.org/abs/2402.16394v1","category":"eess.AS"}
{"created":"2024-02-26 08:32:41","title":"Placing Objects in Context via Inpainting for Out-of-distribution Segmentation","abstract":"When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images.","sentences":["When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training.","Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities.","However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous.","Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts.","In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models.","POC can be used to easily extend any dataset with an arbitrary number of objects.","In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks.","POC is also effective to learn new classes.","For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline.","This corroborates the low sim-to-real gap of models trained on POC-generated images."],"url":"http://arxiv.org/abs/2402.16392v1","category":"cs.CV"}
{"created":"2024-02-26 08:31:45","title":"Quality Assurance for Artificial Intelligence: A Study of Industrial Concerns, Challenges and Best Practices","abstract":"Quality Assurance (QA) aims to prevent mistakes and defects in manufactured products and avoid problems when delivering products or services to customers. QA for AI systems, however, poses particular challenges, given their data-driven and non-deterministic nature as well as more complex architectures and algorithms. While there is growing empirical evidence about practices of machine learning in industrial contexts, little is known about the challenges and best practices of quality assurance for AI systems (QA4AI). In this paper, we report on a mixed-method study of QA4AI in industry practice from various countries and companies. Through interviews with fifteen industry practitioners and a validation survey with 50 practitioner responses, we studied the concerns as well as challenges and best practices in ensuring the QA4AI properties reported in the literature, such as correctness, fairness, interpretability and others. Our findings suggest correctness as the most important property, followed by model relevance, efficiency and deployability. In contrast, transferability (applying knowledge learned in one task to another task), security and fairness are not paid much attention by practitioners compared to other properties. Challenges and solutions are identified for each QA4AI property. For example, interviewees highlighted the trade-off challenge among latency, cost and accuracy for efficiency (latency and cost are parts of efficiency concern). Solutions like model compression are proposed. We identified 21 QA4AI practices across each stage of AI development, with 10 practices being well recognized and another 8 practices being marginally agreed by the survey practitioners.","sentences":["Quality Assurance (QA) aims to prevent mistakes and defects in manufactured products and avoid problems when delivering products or services to customers.","QA for AI systems, however, poses particular challenges, given their data-driven and non-deterministic nature as well as more complex architectures and algorithms.","While there is growing empirical evidence about practices of machine learning in industrial contexts, little is known about the challenges and best practices of quality assurance for AI systems (QA4AI).","In this paper, we report on a mixed-method study of QA4AI in industry practice from various countries and companies.","Through interviews with fifteen industry practitioners and a validation survey with 50 practitioner responses, we studied the concerns as well as challenges and best practices in ensuring the QA4AI properties reported in the literature, such as correctness, fairness, interpretability and others.","Our findings suggest correctness as the most important property, followed by model relevance, efficiency and deployability.","In contrast, transferability (applying knowledge learned in one task to another task), security and fairness are not paid much attention by practitioners compared to other properties.","Challenges and solutions are identified for each QA4AI property.","For example, interviewees highlighted the trade-off challenge among latency, cost and accuracy for efficiency (latency and cost are parts of efficiency concern).","Solutions like model compression are proposed.","We identified 21 QA4AI practices across each stage of AI development, with 10 practices being well recognized and another 8 practices being marginally agreed by the survey practitioners."],"url":"http://arxiv.org/abs/2402.16391v1","category":"cs.SE"}
{"created":"2024-02-26 08:27:50","title":"MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property","abstract":"Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain). In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE and ChatGLM by a noticeable margin, while it had lower scores compared with ChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level. Our source code, data, and models are available at \\url{https://github.com/AI-for-Science/MoZi}.","sentences":["Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks.","However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain).","In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain.","The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch).","In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data.","We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark.","Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE and ChatGLM by a noticeable margin, while it had lower scores compared with ChatGPT.","Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level.","Our source code, data, and models are available at \\url{https://github.com/AI-for-Science/MoZi}."],"url":"http://arxiv.org/abs/2402.16389v1","category":"cs.CL"}
{"created":"2024-02-26 08:22:22","title":"On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method","abstract":"Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and \"the number of layers/steps\" in the GNN-/RNN-based TGL methods and \"the feature-label alignment (FLA) score\", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generalization error, improved overall performance, and lower model complexity. Extensive experiments on real-world datasets demonstrate the effectiveness of our method. Our theoretical findings and proposed algorithm offer essential insights into TGL from a theoretical standpoint, laying the groundwork for the designing practical TGL algorithms in future studies.","sentences":["Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time.","Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored.","This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime.","We establish the connection between the generalization error of TGL algorithms and \"the number of layers/steps\" in the GNN-/RNN-based TGL methods and \"the feature-label alignment (FLA) score\", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods.","Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generalization error, improved overall performance, and lower model complexity.","Extensive experiments on real-world datasets demonstrate the effectiveness of our method.","Our theoretical findings and proposed algorithm offer essential insights into TGL from a theoretical standpoint, laying the groundwork for the designing practical TGL algorithms in future studies."],"url":"http://arxiv.org/abs/2402.16387v1","category":"cs.LG"}
{"created":"2024-02-26 08:08:30","title":"Self Supervised Correlation-based Permutations for Multi-View Clustering","abstract":"Fusing information from different modalities can enhance data analysis tasks, including clustering. However, existing multi-view clustering (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering. We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.). Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective. Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views. We demonstrate the effectiveness of our model using ten MVC benchmark datasets. Theoretically, we show that our model approximates the supervised linear discrimination analysis (LDA) representation. Additionally, we provide an error bound induced by false-pseudo label annotations.","sentences":["Fusing information from different modalities can enhance data analysis tasks, including clustering.","However, existing multi-view clustering (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering.","We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.).","Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective.","Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views.","We demonstrate the effectiveness of our model using ten MVC benchmark datasets.","Theoretically, we show that our model approximates the supervised linear discrimination analysis (LDA) representation.","Additionally, we provide an error bound induced by false-pseudo label annotations."],"url":"http://arxiv.org/abs/2402.16383v1","category":"cs.LG"}
{"created":"2024-02-26 07:58:33","title":"An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation","abstract":"Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application aims to streamline the dataset creation process for TTS models through these features, thereby facilitating advancements in voice-based technologies.","sentences":["Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies.","As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools.","Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress.","This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data.","The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats.","The proposed application aims to streamline the dataset creation process for TTS models through these features, thereby facilitating advancements in voice-based technologies."],"url":"http://arxiv.org/abs/2402.16380v1","category":"eess.AS"}
{"created":"2024-02-26 07:58:12","title":"Improving LLM-based Machine Translation with Systematic Self-Correction","abstract":"Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3) different estimation strategies yield varied impacts on AI feedback, directly affecting the effectiveness of the final corrections. We further compare different LLMs and conduct various experiments involving self-correction and cross-model correction to investigate the potential relationship between the translation and evaluation capabilities of LLMs.","sentences":["Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT).","However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors.","Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance.","Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction.","Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3) different estimation strategies yield varied impacts on AI feedback, directly affecting the effectiveness of the final corrections.","We further compare different LLMs and conduct various experiments involving self-correction and cross-model correction to investigate the potential relationship between the translation and evaluation capabilities of LLMs."],"url":"http://arxiv.org/abs/2402.16379v1","category":"cs.CL"}
{"created":"2024-02-26 07:56:15","title":"Accelerated basis-set convergence of coupled-cluster excitation energies using the density-based basis-set correction method","abstract":"We present the first application to real molecular systems of the recently proposed linear-response theory for the density-based basis-set correction method [J. Chem. Phys. 158, 234107 (2023)]. We apply this approach to accelerate the basis-set convergence of excitation energies in the equation-of-motion coupled-cluster singles doubles (EOM-CCSD) method. We use an approximate linear-response framework which neglects the second-order derivative of the basis-set correction density functional and consists in simply adding to the usual Hamiltonian the one-electron potential generated by the first-order derivative of the functional. This additional basis-set correction potential is evaluated at the Hartree-Fock density, leading to a very computationally cheap basis-set correction. We tested this approach over a set of about 30 excitation energies computed for five small molecular systems and found that the excitation energies from the ground state to Rydberg states are the main source of basis-set error. These excitation energies systematically increase when the size of the basis set is increased, suggesting a biased description in favour of the excited state. Despite the simplicity of the present approach, the results obtained with the basis-set corrected EOM-CCSD method are encouraging as they yield to a mean absolute deviation of 0.02 eV for the aug-cc-pVTZ basis set, while it is of 0.04 eV using the standard EOM-CCSD method. This might open the path to an alternative to explicitly correlated approaches to accelerate the basis-set convergence of excitation energies.","sentences":["We present the first application to real molecular systems of the recently proposed linear-response theory for the density-based basis-set correction method [J. Chem.","Phys. 158, 234107 (2023)].","We apply this approach to accelerate the basis-set convergence of excitation energies in the equation-of-motion coupled-cluster singles doubles (EOM-CCSD) method.","We use an approximate linear-response framework which neglects the second-order derivative of the basis-set correction density functional and consists in simply adding to the usual Hamiltonian the one-electron potential generated by the first-order derivative of the functional.","This additional basis-set correction potential is evaluated at the Hartree-Fock density, leading to a very computationally cheap basis-set correction.","We tested this approach over a set of about 30 excitation energies computed for five small molecular systems and found that the excitation energies from the ground state to Rydberg states are the main source of basis-set error.","These excitation energies systematically increase when the size of the basis set is increased, suggesting a biased description in favour of the excited state.","Despite the simplicity of the present approach, the results obtained with the basis-set corrected EOM-CCSD method are encouraging as they yield to a mean absolute deviation of 0.02 eV for the aug-cc-pVTZ basis set, while it is of 0.04 eV using the standard EOM-CCSD method.","This might open the path to an alternative to explicitly correlated approaches to accelerate the basis-set convergence of excitation energies."],"url":"http://arxiv.org/abs/2402.16378v1","category":"physics.chem-ph"}
{"created":"2024-02-26 07:55:28","title":"Approximation and perturbations of stable solutions to a stationary mean field game system","abstract":"This work introduces a new general approach for the numerical analysis of stable equilibria to second order mean field games systems in cases where the uniqueness of solutions may fail. For the sake of simplicity, we focus on a simple stationary case. We propose an abstract framework to study these solutions by reformulating the mean field game system as an abstract equation in a Banach space. In this context, stable equilibria turn out to be regular solutions to this equation, meaning that the linearized system is well-posed. We provide three applications of this property: we study the sensitivity analysis of stable solutions, establish error estimates for their finite element approximations, and prove the local converge of Newton's method in infinite dimensions.","sentences":["This work introduces a new general approach for the numerical analysis of stable equilibria to second order mean field games systems in cases where the uniqueness of solutions may fail.","For the sake of simplicity, we focus on a simple stationary case.","We propose an abstract framework to study these solutions by reformulating the mean field game system as an abstract equation in a Banach space.","In this context, stable equilibria turn out to be regular solutions to this equation, meaning that the linearized system is well-posed.","We provide three applications of this property: we study the sensitivity analysis of stable solutions, establish error estimates for their finite element approximations, and prove the local converge of Newton's method in infinite dimensions."],"url":"http://arxiv.org/abs/2402.16377v1","category":"math.AP"}
{"created":"2024-02-26 07:53:36","title":"A spectral dominance approach to large random matrices: part II","abstract":"This paper is the second of a series devoted to the study of the dynamics of the spectrum of large random matrices. We study general extensions of the partial differential equation arising to characterize the limit spectral measure of the Dyson Brownian motion. We show that several results of part I extend to cases in which there is no spectral dominance property. We also provide several modeling extensions of such models. Finally we establish new regularizing results for the case of the Dyson Brownian motion.","sentences":["This paper is the second of a series devoted to the study of the dynamics of the spectrum of large random matrices.","We study general extensions of the partial differential equation arising to characterize the limit spectral measure of the Dyson Brownian motion.","We show that several results of part I extend to cases in which there is no spectral dominance property.","We also provide several modeling extensions of such models.","Finally we establish new regularizing results for the case of the Dyson Brownian motion."],"url":"http://arxiv.org/abs/2402.16376v1","category":"math.AP"}
{"created":"2024-02-26 07:52:40","title":"Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning","abstract":"Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data. In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue. This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning. Concretely, according to the observability of distributions in the inference stage and the availability of sufficient supervision information in the training stage, we categorize existing graph learning methods into several essential scenarios, including graph domain adaptation learning, graph out-of-distribution learning, and graph continual learning. For each scenario, a detailed taxonomy is proposed, with specific descriptions and discussions of existing progress made in distribution-shifted graph learning. Additionally, we discuss the potential applications and future directions for graph learning under distribution shifts with a systematic analysis of the current state in this field. The survey is positioned to provide general guidance for the development of effective graph learning algorithms in handling graph distribution shifts, and to stimulate future research and advancements in this area.","sentences":["Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data.","In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue.","This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness.","In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning.","Concretely, according to the observability of distributions in the inference stage and the availability of sufficient supervision information in the training stage, we categorize existing graph learning methods into several essential scenarios, including graph domain adaptation learning, graph out-of-distribution learning, and graph continual learning.","For each scenario, a detailed taxonomy is proposed, with specific descriptions and discussions of existing progress made in distribution-shifted graph learning.","Additionally, we discuss the potential applications and future directions for graph learning under distribution shifts with a systematic analysis of the current state in this field.","The survey is positioned to provide general guidance for the development of effective graph learning algorithms in handling graph distribution shifts, and to stimulate future research and advancements in this area."],"url":"http://arxiv.org/abs/2402.16374v1","category":"cs.LG"}
{"created":"2024-02-26 07:52:10","title":"Testing The Weak Cosmic Censorship Conjecture in Short Haired Black Holes","abstract":"The Weak Cosmic Censorship Conjecture is a hypothesis regarding the properties of event horizons and singularities during the formation of black holes, stating that singularities are always encompassed by the event horizons(TEH) of black holes, thus preventing naked singularities from affecting the causal structure of spacetime. In this paper, we explore the Weak Cosmic Censorship Conjecture in the context of rotating hairy black holes, aiming to understand the impact of hairiness on the conjecture for Kerr black holes. We investigate whether TEH of rotating hairy black holes can be disrupted by incoming test particles and scalar fields. When test particles and scalar fields incident on the rotating hairy black hole are found. In extreme cases, when considering a second order approximation, if the parameter $\\kappa$ (The parameter here is a function related to hair strength $\\kappa(Q_m^{2k})$.)falls within the range of $0<\\kappa<\\sqrt{1/3}$, TEH of a hairy black hole can be disrupted. Conversely, in the range of $\\sqrt{1/3}<\\kappa<1$, TEH of a hairy black hole cannot be disrupted. When considering the second order approximation in near extreme cases, the parameter $\\kappa$, within the range of $0<\\kappa<1$, can lead to the disruption of TEH in this spacetime. When an incident scalar field is present, in near extreme conditions, TEH of a rotating short hair black hole cannot be disrupted. Therefore, the value of the parameter $\\kappa$ reveals the connection between rotating short hair black holes and the weak cosmic censorship conjecture, indicating that the presence of short hair significantly affects TEH of black holes. This will aid in further understanding the nature of rotating short hair black holes.","sentences":["The Weak Cosmic Censorship Conjecture is a hypothesis regarding the properties of event horizons and singularities during the formation of black holes, stating that singularities are always encompassed by the event horizons(TEH) of black holes, thus preventing naked singularities from affecting the causal structure of spacetime.","In this paper, we explore the Weak Cosmic Censorship Conjecture in the context of rotating hairy black holes, aiming to understand the impact of hairiness on the conjecture for Kerr black holes.","We investigate whether TEH of rotating hairy black holes can be disrupted by incoming test particles and scalar fields.","When test particles and scalar fields incident on the rotating hairy black hole are found.","In extreme cases, when considering a second order approximation, if the parameter $\\kappa$ (The parameter here is a function related to hair strength $\\kappa(Q_m^{2k})$.)falls within the range of $0<\\kappa<\\sqrt{1/3}$, TEH of a hairy black hole can be disrupted.","Conversely, in the range of $\\sqrt{1/3}<\\kappa<1$, TEH of a hairy black hole cannot be disrupted.","When considering the second order approximation in near extreme cases, the parameter $\\kappa$, within the range of $0<\\kappa<1$, can lead to the disruption of TEH in this spacetime.","When an incident scalar field is present, in near extreme conditions, TEH of a rotating short hair black hole cannot be disrupted.","Therefore, the value of the parameter $\\kappa$ reveals the connection between rotating short hair black holes and the weak cosmic censorship conjecture, indicating that the presence of short hair significantly affects TEH of black holes.","This will aid in further understanding the nature of rotating short hair black holes."],"url":"http://arxiv.org/abs/2402.16373v1","category":"gr-qc"}
{"created":"2024-02-26 07:51:13","title":"Performance Tradeoff Between Overhead and Achievable SNR in RIS Beam Training","abstract":"Efficient beam training is the key challenge in the codebook-based configuration of reconfigurable intelligent surfaces (RISs) because the beam training overhead can have a strong impact on the achievable system performance. In this paper, we study the performance tradeoff between overhead and achievable signal-to-noise ratio (SNR) in RIS beam training while taking into account the size of the targeted coverage area, the RIS response time, and the delay for feedback transmissions. Thereby, we consider three common beam training strategies: full search (FS), hierarchical search (HS), and tracking-based search (TS). Our analysis shows that the codebook-based illumination of a given coverage area can be realized with wide- or narrow-beam designs, which result in two different scaling laws for the achievable SNR. Similarly, there are two regimes for the overhead, where the number of pilot symbols required for reliable beam training is dependent on and independent of the SNR, respectively. Based on these insights, we investigate the impact of the beam training overhead on the effective rate and provide an upper bound on the user velocity for which the overhead is negligible. Moreover, when the overhead is not negligible, we show that TS beam training achieves higher effective rates than HS and FS beam training, while HS beam training may or may not outperform FS beam training, depending on the RIS response time, feedback delay, and codebook size. Finally, we present numerical simulation results that verify our theoretical analysis. In particular, our results confirm the existence of the proposed regimes, reveal that fast RISs can lead to negligible overhead for FS beam training, and show that large feedback delays can significantly reduce the performance for HS beam training.","sentences":["Efficient beam training is the key challenge in the codebook-based configuration of reconfigurable intelligent surfaces (RISs) because the beam training overhead can have a strong impact on the achievable system performance.","In this paper, we study the performance tradeoff between overhead and achievable signal-to-noise ratio (SNR) in RIS beam training while taking into account the size of the targeted coverage area, the RIS response time, and the delay for feedback transmissions.","Thereby, we consider three common beam training strategies: full search (FS), hierarchical search (HS), and tracking-based search (TS).","Our analysis shows that the codebook-based illumination of a given coverage area can be realized with wide- or narrow-beam designs, which result in two different scaling laws for the achievable SNR.","Similarly, there are two regimes for the overhead, where the number of pilot symbols required for reliable beam training is dependent on and independent of the SNR, respectively.","Based on these insights, we investigate the impact of the beam training overhead on the effective rate and provide an upper bound on the user velocity for which the overhead is negligible.","Moreover, when the overhead is not negligible, we show that TS beam training achieves higher effective rates than HS and FS beam training, while HS beam training may or may not outperform FS beam training, depending on the RIS response time, feedback delay, and codebook size.","Finally, we present numerical simulation results that verify our theoretical analysis.","In particular, our results confirm the existence of the proposed regimes, reveal that fast RISs can lead to negligible overhead for FS beam training, and show that large feedback delays can significantly reduce the performance for HS beam training."],"url":"http://arxiv.org/abs/2402.16372v1","category":"cs.IT"}
{"created":"2024-02-26 07:47:12","title":"Generative AI in Vision: A Survey on Models, Metrics and Applications","abstract":"Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples. Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling. Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation. By synthesizing existing research and highlighting critical advancements in this field, this survey aims to provide researchers and practitioners with a comprehensive understanding of generative AI diffusion and legacy models and inspire future innovations in this exciting area of artificial intelligence.","sentences":["Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples.","Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio.","This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges.","We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling.","Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation.","By synthesizing existing research and highlighting critical advancements in this field, this survey aims to provide researchers and practitioners with a comprehensive understanding of generative AI diffusion and legacy models and inspire future innovations in this exciting area of artificial intelligence."],"url":"http://arxiv.org/abs/2402.16369v1","category":"cs.CV"}
{"created":"2024-02-26 07:45:14","title":"SPINEPS -- Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation","abstract":"Purpose. To present SPINEPS, an open-source deep learning approach for semantic and instance segmentation of 14 spinal structures (ten vertebra substructures, intervertebral discs, spinal cord, spinal canal, and sacrum) in whole body T2w MRI.   Methods. During this HIPPA-compliant, retrospective study, we utilized the public SPIDER dataset (218 subjects, 63% female) and a subset of the German National Cohort (1423 subjects, mean age 53, 49% female) for training and evaluation. We combined CT and T2w segmentations to train models that segment 14 spinal structures in T2w sagittal scans both semantically and instance-wise. Performance evaluation metrics included Dice similarity coefficient, average symmetrical surface distance, panoptic quality, segmentation quality, and recognition quality. Statistical significance was assessed using the Wilcoxon signed-rank test. An in-house dataset was used to qualitatively evaluate out-of-distribution samples.   Results. On the public dataset, our approach outperformed the baseline (instance-wise vertebra dice score 0.929 vs. 0.907, p-value<0.001). Training on auto-generated annotations and evaluating on manually corrected test data from the GNC yielded global dice scores of 0.900 for vertebrae, 0.960 for intervertebral discs, and 0.947 for the spinal canal. Incorporating the SPIDER dataset during training increased these scores to 0.920, 0.967, 0.958, respectively.   Conclusions. The proposed segmentation approach offers robust segmentation of 14 spinal structures in T2w sagittal images, including the spinal cord, spinal canal, intervertebral discs, endplate, sacrum, and vertebrae. The approach yields both a semantic and instance mask as output, thus being easy to utilize. This marks the first publicly available algorithm for whole spine segmentation in sagittal T2w MR imaging.","sentences":["Purpose.","To present SPINEPS, an open-source deep learning approach for semantic and instance segmentation of 14 spinal structures (ten vertebra substructures, intervertebral discs, spinal cord, spinal canal, and sacrum) in whole body T2w MRI.   Methods.","During this HIPPA-compliant, retrospective study, we utilized the public SPIDER dataset (218 subjects, 63% female) and a subset of the German National Cohort (1423 subjects, mean age 53, 49% female) for training and evaluation.","We combined CT and T2w segmentations to train models that segment 14 spinal structures in T2w sagittal scans both semantically and instance-wise.","Performance evaluation metrics included Dice similarity coefficient, average symmetrical surface distance, panoptic quality, segmentation quality, and recognition quality.","Statistical significance was assessed using the Wilcoxon signed-rank test.","An in-house dataset was used to qualitatively evaluate out-of-distribution samples.   ","Results.","On the public dataset, our approach outperformed the baseline (instance-wise vertebra dice score 0.929 vs. 0.907, p-value<0.001).","Training on auto-generated annotations and evaluating on manually corrected test data from the GNC yielded global dice scores of 0.900 for vertebrae, 0.960 for intervertebral discs, and 0.947 for the spinal canal.","Incorporating the SPIDER dataset during training increased these scores to 0.920, 0.967, 0.958, respectively.   Conclusions.","The proposed segmentation approach offers robust segmentation of 14 spinal structures in T2w sagittal images, including the spinal cord, spinal canal, intervertebral discs, endplate, sacrum, and vertebrae.","The approach yields both a semantic and instance mask as output, thus being easy to utilize.","This marks the first publicly available algorithm for whole spine segmentation in sagittal T2w MR imaging."],"url":"http://arxiv.org/abs/2402.16368v1","category":"eess.IV"}
{"created":"2024-02-26 07:33:05","title":"LLM Inference Unveiled: Survey and Roofline Model Insights","abstract":"The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantization), decoding algorithm improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and system-level enhancements. Distinguished by the integration of roofline model analysis, our survey provides a comprehensive and nuanced exploration of efficient LLM inference challenges and solutions. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment. The tool LLM-Viewer is open-sourced.","sentences":["The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges.","Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain.","Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques.","This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM.","Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantization), decoding algorithm improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and system-level enhancements.","Distinguished by the integration of roofline model analysis, our survey provides a comprehensive and nuanced exploration of efficient LLM inference challenges and solutions.","This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment.","The tool LLM-Viewer is open-sourced."],"url":"http://arxiv.org/abs/2402.16363v1","category":"cs.CL"}
{"created":"2024-02-26 07:31:35","title":"Layer-wise Regularized Dropout for Neural Language Models","abstract":"Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a \"self-distillation\" framework, in which each sub-model generated by dropout is the other's \"teacher\" model and \"student\" model. Through extensive experiments on 8 natural language understanding datasets, 6 neural machine translation datasets, and 1 abstractive summarization dataset (a total of 15 datasets), we show that LR-Drop achieves superior performances, including state-of-the-art results.","sentences":["Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique.","To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer.","In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models.","Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy.","Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent.","The proposed LR-Drop can be regarded as a \"self-distillation\" framework, in which each sub-model generated by dropout is the other's \"teacher\" model and \"student\" model.","Through extensive experiments on 8 natural language understanding datasets, 6 neural machine translation datasets, and 1 abstractive summarization dataset (a total of 15 datasets), we show that LR-Drop achieves superior performances, including state-of-the-art results."],"url":"http://arxiv.org/abs/2402.16361v1","category":"cs.CL"}
{"created":"2024-02-26 07:24:32","title":"Feedback Efficient Online Fine-Tuning of Diffusion Models","abstract":"Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that efficiently explores on the manifold of feasible samples. We present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules.","sentences":["Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules.","However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity.","It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property.","Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules).","In this work, we propose a novel reinforcement learning procedure that efficiently explores on the manifold of feasible samples.","We present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules."],"url":"http://arxiv.org/abs/2402.16359v1","category":"cs.LG"}
{"created":"2024-02-26 07:19:23","title":"Language-guided Skill Learning with Temporal Variational Inference","abstract":"We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.","sentences":["We present an algorithm for skill discovery from expert demonstrations.","The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories.","Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments.","To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process.","Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment."],"url":"http://arxiv.org/abs/2402.16354v1","category":"cs.LG"}
{"created":"2024-02-26 07:17:25","title":"MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs","abstract":"Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. These models consistently outperform previous open-source models across five representative mathematical reasoning datasets, achieving state-of-the-art performance. In particular, MathGenieLM-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score among open-source language models.","sentences":["Large language models (LLMs) have exhibited great potential in mathematical reasoning.","However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4.","In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data).","We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions.","Subsequently, we generate code-integrated solutions for the new questions.","To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification.","Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM.","These models consistently outperform previous open-source models across five representative mathematical reasoning datasets, achieving state-of-the-art performance.","In particular, MathGenieLM-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score among open-source language models."],"url":"http://arxiv.org/abs/2402.16352v1","category":"cs.CL"}
{"created":"2024-02-26 07:07:00","title":"C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory","abstract":"Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of convergence, reduce the range of oscillation and match the expert's distribution more closely both for vanilla GAIL and GAIL-DAC.","sentences":["Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator.","It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator.","A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL.","This can cause oscillations during training, harming its sample efficiency and final policy performance.","Recent work has shown that control theory can help with the convergence of a GAN's training.","This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting.","Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL).","On MuJoCo tasks, our controlled variant is able to speed up the rate of convergence, reduce the range of oscillation and match the expert's distribution more closely both for vanilla GAIL and GAIL-DAC."],"url":"http://arxiv.org/abs/2402.16349v1","category":"cs.LG"}
{"created":"2024-02-26 07:00:58","title":"CodeS: Towards Building Open-source Language Models for Text-to-SQL","abstract":"Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.","sentences":["Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL).","However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads.","To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task.","CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes.","This paper studies the research challenges in building CodeS.","To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus.","Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique.","We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications.","The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks."],"url":"http://arxiv.org/abs/2402.16347v1","category":"cs.CL"}
{"created":"2024-02-26 06:58:47","title":"Probing new physics with polarization components of the tau lepton in quasielastic $e^- p \\to \u039b_c \u03c4^-$ scattering process","abstract":"Kinematics restrict the ability of rare charm decays to explore the charged Lepton Flavor Violation processes mediated by the quark-level $c\\to u \\ell \\tau$ transition. To fill the gap, we propose exploring new physics (NP) through the quasielastic scattering process $e^-p\\to \\tau^-\\Lambda_c$ and the polarization of the $\\tau$ lepton. As analyzing modes for the $\\tau$ polarization, we consider the decays $\\tau^-\\to \\pi^-\\nu_{\\tau}$, $\\tau^-\\to \\rho^-\\nu_{\\tau}$, and $\\tau^- \\to \\ell^-\\bar{\\nu}_{\\ell}\\nu_{\\tau}$, and show that the $\\tau$ polarization components can be extracted from analyzing the kinematics of the $\\tau$ visible decay products. In the framework of a general low-energy effective Lagrangian, we then perform a detailed analysis of the polarization components in various aspects and scrutinize possible NP signals. With one upcoming experimental setup, we finally demonstrate promising event rate can be expected for the cascade process and, even in the worst-case scenario -- no signals is observed at all -- it can still provide a competitive potential for constraining the NP, compared with those from the high-$p_T$ dilepton invariant mass tails at high-energy colliders.","sentences":["Kinematics restrict the ability of rare charm decays to explore the charged Lepton Flavor Violation processes mediated by the quark-level $c\\to u \\ell \\tau$ transition.","To fill the gap, we propose exploring new physics (NP) through the quasielastic scattering process $e^-p\\to \\tau^-\\Lambda_c$ and the polarization of the $\\tau$ lepton.","As analyzing modes for the $\\tau$ polarization, we consider the decays $\\tau^-\\to \\pi^-\\nu_{\\tau}$, $\\tau^-\\to \\rho^-\\nu_{\\tau}$, and $\\tau^- \\to \\ell^-\\bar{\\nu}_{\\ell}\\nu_{\\tau}$, and show that the $\\tau$ polarization components can be extracted from analyzing the kinematics of the $\\tau$ visible decay products.","In the framework of a general low-energy effective Lagrangian, we then perform a detailed analysis of the polarization components in various aspects and scrutinize possible NP signals.","With one upcoming experimental setup, we finally demonstrate promising event rate can be expected for the cascade process and, even in the worst-case scenario -- no signals is observed at all -- it can still provide a competitive potential for constraining the NP, compared with those from the high-$p_T$ dilepton invariant mass tails at high-energy colliders."],"url":"http://arxiv.org/abs/2402.16344v1","category":"hep-ph"}
{"created":"2024-02-26 06:42:30","title":"Contingency Planning Using Bi-level Markov Decision Processes for Space Missions","abstract":"This work focuses on autonomous contingency planning for scientific missions by enabling rapid policy computation from any off-nominal point in the state space in the event of a delay or deviation from the nominal mission plan. Successful contingency planning involves managing risks and rewards, often probabilistically associated with actions, in stochastic scenarios. Markov Decision Processes (MDPs) are used to mathematically model decision-making in such scenarios. However, in the specific case of planetary rover traverse planning, the vast action space and long planning time horizon pose computational challenges. A bi-level MDP framework is proposed to improve computational tractability, while also aligning with existing mission planning practices and enhancing explainability and trustworthiness of AI-driven solutions. We discuss the conversion of a mission planning MDP into a bi-level MDP, and test the framework on RoverGridWorld, a modified GridWorld environment for rover mission planning. We demonstrate the computational tractability and near-optimal policies achievable with the bi-level MDP approach, highlighting the trade-offs between compute time and policy optimality as the problem's complexity grows. This work facilitates more efficient and flexible contingency planning in the context of scientific missions.","sentences":["This work focuses on autonomous contingency planning for scientific missions by enabling rapid policy computation from any off-nominal point in the state space in the event of a delay or deviation from the nominal mission plan.","Successful contingency planning involves managing risks and rewards, often probabilistically associated with actions, in stochastic scenarios.","Markov Decision Processes (MDPs) are used to mathematically model decision-making in such scenarios.","However, in the specific case of planetary rover traverse planning, the vast action space and long planning time horizon pose computational challenges.","A bi-level MDP framework is proposed to improve computational tractability, while also aligning with existing mission planning practices and enhancing explainability and trustworthiness of AI-driven solutions.","We discuss the conversion of a mission planning MDP into a bi-level MDP, and test the framework on RoverGridWorld, a modified GridWorld environment for rover mission planning.","We demonstrate the computational tractability and near-optimal policies achievable with the bi-level MDP approach, highlighting the trade-offs between compute time and policy optimality as the problem's complexity grows.","This work facilitates more efficient and flexible contingency planning in the context of scientific missions."],"url":"http://arxiv.org/abs/2402.16342v1","category":"cs.AI"}
{"created":"2024-02-26 06:36:32","title":"BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM","abstract":"The Segment Anything Model (SAM), a foundation model pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision. Despite its strengths, SAM encounters two major challenges. Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input prompts like points or bounding boxes to identify targeted objects. Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks. Current solutions to these problems, which involve finetuning SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging. To overcome these limitations, we introduce BLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach allows for automatic image segmentation without the need for manual prompts, by optimizing a learnable prompt embedding. Furthermore, it significantly reduces the risk of overfitting by training the model's weight parameters and the prompt embedding on two separate subsets of the training dataset, each at a different level of optimization. We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains. The results demonstrate BLO-SAM's superior performance over various state-of-the-art image semantic segmentation methods.","sentences":["The Segment Anything Model (SAM), a foundation model pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision.","Despite its strengths, SAM encounters two major challenges.","Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input prompts like points or bounding boxes to identify targeted objects.","Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks.","Current solutions to these problems, which involve finetuning SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging.","To overcome these limitations, we introduce BLO-SAM, which finetunes SAM based on bi-level optimization (BLO).","Our approach allows for automatic image segmentation without the need for manual prompts, by optimizing a learnable prompt embedding.","Furthermore, it significantly reduces the risk of overfitting by training the model's weight parameters and the prompt embedding on two separate subsets of the training dataset, each at a different level of optimization.","We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains.","The results demonstrate BLO-SAM's superior performance over various state-of-the-art image semantic segmentation methods."],"url":"http://arxiv.org/abs/2402.16338v1","category":"cs.CV"}
{"created":"2024-02-26 06:29:46","title":"On the algebra generated by three commuting matrices: combinatorial cases","abstract":"Gerstenhaber proved in 1961 that the unital algebra generated by a pair of commuting $d\\times d$ matrices over a field has dimension at most $d$. It is an open problem whether the analogous statement is true for triples of matrices which pairwise commute. We answer this question for special classes of triples of matrices arising from combinatorial data.","sentences":["Gerstenhaber proved in 1961 that the unital algebra generated by a pair of commuting $d\\times d$ matrices over a field has dimension at most $d$. It is an open problem whether the analogous statement is true for triples of matrices which pairwise commute.","We answer this question for special classes of triples of matrices arising from combinatorial data."],"url":"http://arxiv.org/abs/2402.16334v1","category":"math.AC"}
{"created":"2024-02-26 06:01:38","title":"Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech","abstract":"Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current models rely mainly on supervised learning, which is time-consuming and expensive for label collection. To solve this problem, we propose VQScore, a self-supervised metric for evaluating speech based on the quantization error of a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE relies on clean speech; hence, large quantization errors can be expected when the speech is distorted. To further improve correlation with real quality scores, domain knowledge of speech processing is incorporated into the model design. We found that the vector quantization mechanism could also be used for self-supervised speech enhancement (SE) model training. To improve the robustness of the encoder for SE, a novel self-distillation mechanism combined with adversarial training is introduced. In summary, the proposed speech quality estimation method and enhancement models require only clean speech for training without any label requirements. Experimental results show that the proposed VQScore and enhancement model are competitive with supervised baselines. The code will be released after publication.","sentences":["Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models.","However, current models rely mainly on supervised learning, which is time-consuming and expensive for label collection.","To solve this problem, we propose VQScore, a self-supervised metric for evaluating speech based on the quantization error of a vector-quantized-variational autoencoder (VQ-VAE).","The training of VQ-VAE relies on clean speech; hence, large quantization errors can be expected when the speech is distorted.","To further improve correlation with real quality scores, domain knowledge of speech processing is incorporated into the model design.","We found that the vector quantization mechanism could also be used for self-supervised speech enhancement (SE) model training.","To improve the robustness of the encoder for SE, a novel self-distillation mechanism combined with adversarial training is introduced.","In summary, the proposed speech quality estimation method and enhancement models require only clean speech for training without any label requirements.","Experimental results show that the proposed VQScore and enhancement model are competitive with supervised baselines.","The code will be released after publication."],"url":"http://arxiv.org/abs/2402.16321v1","category":"cs.SD"}
{"created":"2024-02-26 05:51:47","title":"Data-freeWeight Compress and Denoise for Large Language Models","abstract":"Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in conjunction with pruning and quantization methods. We achieve a model pruning of 80% parameters while retaining 93.43% of the original performance without any calibration data. Additionally, we explore the fundamental properties of the weight matrix of LLMs undergone Rank-k Approximation and conduct comprehensive experiments to elucidate our hypothesis.","sentences":["Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains.","Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed.","To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization.","Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise.","In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices.","Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in conjunction with pruning and quantization methods.","We achieve a model pruning of 80% parameters while retaining 93.43% of the original performance without any calibration data.","Additionally, we explore the fundamental properties of the weight matrix of LLMs undergone Rank-k Approximation and conduct comprehensive experiments to elucidate our hypothesis."],"url":"http://arxiv.org/abs/2402.16319v1","category":"cs.CL"}
{"created":"2024-02-26 05:44:24","title":"Polynomial-Time Computation of Exact $\u03a6$-Equilibria in Polyhedral Games","abstract":"It is a well-known fact that correlated equilibria can be computed in polynomial time in a large class of concisely represented games using the celebrated Ellipsoid Against Hope algorithm (Papadimitriou and Roughgarden, 2008; Jiang and Leyton-Brown, 2015). However, the landscape of efficiently computable equilibria in sequential (extensive-form) games remains unknown. The Ellipsoid Against Hope does not apply directly to these games, because they do not have the required \"polynomial type\" property. Despite this barrier, Huang and von Stengel (2008) altered the algorithm to compute exact extensive-form correlated equilibria.   In this paper, we generalize the Ellipsoid Against Hope and develop a simple algorithmic framework for efficiently computing saddle-points in bilinear zero-sum games, even when one of the dimensions is exponentially large. Moreover, the framework only requires a \"good-enough-response\" oracle, which is a weakened notion of a best-response oracle.   Using this machinery, we develop a general algorithmic framework for computing exact linear $\\Phi$-equilibria in any polyhedral game (under mild assumptions), including correlated equilibria in normal-form games, and extensive-form correlated equilibria in extensive-form games. This enables us to give the first polynomial-time algorithm for computing exact linear-deviation correlated equilibria in extensive-form games, thus resolving an open question by Farina and Pipis (2023). Furthermore, even for the cases for which a polynomial time algorithm for exact equilibria was already known, our framework provides a conceptually simpler solution.","sentences":["It is a well-known fact that correlated equilibria can be computed in polynomial time in a large class of concisely represented games using the celebrated Ellipsoid Against Hope algorithm (Papadimitriou and Roughgarden, 2008; Jiang and Leyton-Brown, 2015).","However, the landscape of efficiently computable equilibria in sequential (extensive-form) games remains unknown.","The Ellipsoid Against Hope does not apply directly to these games, because they do not have the required \"polynomial type\" property.","Despite this barrier, Huang and von Stengel (2008) altered the algorithm to compute exact extensive-form correlated equilibria.   ","In this paper, we generalize the Ellipsoid Against Hope and develop a simple algorithmic framework for efficiently computing saddle-points in bilinear zero-sum games, even when one of the dimensions is exponentially large.","Moreover, the framework only requires a \"good-enough-response\" oracle, which is a weakened notion of a best-response oracle.   ","Using this machinery, we develop a general algorithmic framework for computing exact linear $\\Phi$-equilibria in any polyhedral game (under mild assumptions), including correlated equilibria in normal-form games, and extensive-form correlated equilibria in extensive-form games.","This enables us to give the first polynomial-time algorithm for computing exact linear-deviation correlated equilibria in extensive-form games, thus resolving an open question by Farina and Pipis (2023).","Furthermore, even for the cases for which a polynomial time algorithm for exact equilibria was already known, our framework provides a conceptually simpler solution."],"url":"http://arxiv.org/abs/2402.16316v1","category":"cs.GT"}
{"created":"2024-02-26 05:43:51","title":"Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models","abstract":"Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs. In an effort to further the community's endeavor in this direction, we propose a multiple granularity attribute-centric evaluation benchmark, Finer, which aims to establish a ground to evaluate LVLMs' fine-grained visual comprehension ability and provide significantly improved explainability.","sentences":["Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease.","While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings.","Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V","not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions.","In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs.","In an effort to further the community's endeavor in this direction, we propose a multiple granularity attribute-centric evaluation benchmark, Finer, which aims to establish a ground to evaluate LVLMs' fine-grained visual comprehension ability and provide significantly improved explainability."],"url":"http://arxiv.org/abs/2402.16315v1","category":"cs.CV"}
{"created":"2024-02-26 05:43:48","title":"Equational Bit-Vector Solving via Strong Gr\u00f6bner Bases","abstract":"Bit-vectors, which are integers in a finite number of bits, are ubiquitous in software and hardware systems. In this work, we consider the satisfiability modulo theories (SMT) of bit-vectors. Unlike normal integers, the arithmetics of bit-vectors are modular upon integer overflow. Therefore, the SMT solving of bit-vectors needs to resolve the underlying modular arithmetics. In the literature, two prominent approaches for SMT solving are bit-blasting (that transforms the SMT problem into boolean satisfiability) and integer solving (that transforms the SMT problem into integer properties). Both approaches ignore the algebraic properties of the modular arithmetics and hence could not utilize these properties to improve the efficiency of SMT solving.   In this work, we consider the equational theory of bit-vectors and capture the algebraic properties behind them via strong Gr\\\"obner bases. First, we apply strong Gr\\\"obner bases to the quantifier-free equational theory of bit-vectors and propose a novel algorithmic improvement in the key computation of multiplicative inverse modulo a power of two. Second, we resolve the important case of invariant generation in quantified equational bit-vector properties via strong Gr\\\"obner bases and linear congruence solving. Experimental results over an extensive range of benchmarks show that our approach outperforms existing methods in both time efficiency and memory consumption.","sentences":["Bit-vectors, which are integers in a finite number of bits, are ubiquitous in software and hardware systems.","In this work, we consider the satisfiability modulo theories (SMT) of bit-vectors.","Unlike normal integers, the arithmetics of bit-vectors are modular upon integer overflow.","Therefore, the SMT solving of bit-vectors needs to resolve the underlying modular arithmetics.","In the literature, two prominent approaches for SMT solving are bit-blasting (that transforms the SMT problem into boolean satisfiability) and integer solving (that transforms the SMT problem into integer properties).","Both approaches ignore the algebraic properties of the modular arithmetics and hence could not utilize these properties to improve the efficiency of SMT solving.   ","In this work, we consider the equational theory of bit-vectors and capture the algebraic properties behind them via strong Gr\\\"obner bases.","First, we apply strong Gr\\\"obner bases to the quantifier-free equational theory of bit-vectors and propose a novel algorithmic improvement in the key computation of multiplicative inverse modulo a power of two.","Second, we resolve the important case of invariant generation in quantified equational bit-vector properties via strong Gr\\\"obner bases and linear congruence solving.","Experimental results over an extensive range of benchmarks show that our approach outperforms existing methods in both time efficiency and memory consumption."],"url":"http://arxiv.org/abs/2402.16314v1","category":"cs.LO"}
{"created":"2024-02-26 05:31:34","title":"Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering","abstract":"Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.","sentences":["Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers.","In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question.","With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis.","In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually.","Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers.","We release our data and code at \\url{https://github.com/kobayashikanna01/Chain-of-Discussion}."],"url":"http://arxiv.org/abs/2402.16313v1","category":"cs.CL"}
{"created":"2024-02-26 05:31:14","title":"Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users","abstract":"We study the problem of federated contextual combinatorial cascading bandits, where $|\\mathcal{U}|$ agents collaborate under the coordination of a central server to provide tailored recommendations to the $|\\mathcal{U}|$ corresponding users. Existing works consider either a synchronous framework, necessitating full agent participation and global synchronization, or assume user homogeneity with identical behaviors. We overcome these limitations by considering (1) federated agents operating in an asynchronous communication paradigm, where no mandatory synchronization is required and all agents communicate independently with the server, (2) heterogeneous user behaviors, where users can be stratified into $J \\le |\\mathcal{U}|$ latent user clusters, each exhibiting distinct preferences. For this setting, we propose a UCB-type algorithm with delicate communication protocols. Through theoretical analysis, we give sub-linear regret bounds on par with those achieved in the synchronous framework, while incurring only logarithmic communication costs. Empirical evaluation on synthetic and real-world datasets validates our algorithm's superior performance in terms of regrets and communication costs.","sentences":["We study the problem of federated contextual combinatorial cascading bandits, where $|\\mathcal{U}|$ agents collaborate under the coordination of a central server to provide tailored recommendations to the $|\\mathcal{U}|$ corresponding users.","Existing works consider either a synchronous framework, necessitating full agent participation and global synchronization, or assume user homogeneity with identical behaviors.","We overcome these limitations by considering (1) federated agents operating in an asynchronous communication paradigm, where no mandatory synchronization is required and all agents communicate independently with the server, (2) heterogeneous user behaviors, where users can be stratified into $J \\le |\\mathcal{U}|$ latent user clusters, each exhibiting distinct preferences.","For this setting, we propose a UCB-type algorithm with delicate communication protocols.","Through theoretical analysis, we give sub-linear regret bounds on par with those achieved in the synchronous framework, while incurring only logarithmic communication costs.","Empirical evaluation on synthetic and real-world datasets validates our algorithm's superior performance in terms of regrets and communication costs."],"url":"http://arxiv.org/abs/2402.16312v1","category":"cs.LG"}
{"created":"2024-02-26 05:30:48","title":"Cross-domain Chinese Sentence Pattern Parsing","abstract":"Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.","sentences":["Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.","Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.","To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework.","Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.","Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics."],"url":"http://arxiv.org/abs/2402.16311v1","category":"cs.CL"}
{"created":"2024-02-26 05:28:36","title":"REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories","abstract":"Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\\%-10.9\\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities.","sentences":["Location prediction forecasts a user's location based on historical user mobility traces.","To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful.","Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction.","However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances.","Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction.","Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps.","Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets.","Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\\%-10.9\\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities."],"url":"http://arxiv.org/abs/2402.16310v1","category":"cs.LG"}
{"created":"2024-02-26 05:17:06","title":"DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene Understanding and Real-to-Sim Transfer","abstract":"3D scene understanding for robotic applications exhibits a unique set of requirements including real-time inference, object-centric latent representation learning, accurate 6D pose estimation and 3D reconstruction of objects. Current methods for scene understanding typically rely on a combination of trained models paired with either an explicit or learnt volumetric representation, all of which have their own drawbacks and limitations. We introduce DreamUp3D, a novel Object-Centric Generative Model (OCGM) designed explicitly to perform inference on a 3D scene informed only by a single RGB-D image. DreamUp3D is a self-supervised model, trained end-to-end, and is capable of segmenting objects, providing 3D object reconstructions, generating object-centric latent representations and accurate per-object 6D pose estimates. We compare DreamUp3D to baselines including NeRFs, pre-trained CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene reconstruction, object matching and object pose estimation. Our experiments show that our model outperforms all baselines by a significant margin in real-world scenarios displaying its applicability for 3D scene understanding tasks while meeting the strict demands exhibited in robotics applications.","sentences":["3D scene understanding for robotic applications exhibits a unique set of requirements including real-time inference, object-centric latent representation learning, accurate 6D pose estimation and 3D reconstruction of objects.","Current methods for scene understanding typically rely on a combination of trained models paired with either an explicit or learnt volumetric representation, all of which have their own drawbacks and limitations.","We introduce DreamUp3D, a novel Object-Centric Generative Model (OCGM) designed explicitly to perform inference on a 3D scene informed only by a single RGB-D image.","DreamUp3D is a self-supervised model, trained end-to-end, and is capable of segmenting objects, providing 3D object reconstructions, generating object-centric latent representations and accurate per-object 6D pose estimates.","We compare DreamUp3D to baselines including NeRFs, pre-trained CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene reconstruction, object matching and object pose estimation.","Our experiments show that our model outperforms all baselines by a significant margin in real-world scenarios displaying its applicability for 3D scene understanding tasks while meeting the strict demands exhibited in robotics applications."],"url":"http://arxiv.org/abs/2402.16308v1","category":"cs.RO"}
{"created":"2024-02-26 05:08:40","title":"Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion","abstract":"As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Diffusion is incorporated. By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench.","sentences":["As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions.","In this work, we aim to address this alignment challenge for conditional generation tasks.","First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs).","With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs.","By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment.","As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation.","To further enhance the image fidelity, a Score Distillation Sampling module of Stable Diffusion is incorporated.","By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench."],"url":"http://arxiv.org/abs/2402.16305v1","category":"cs.LG"}
{"created":"2024-02-26 05:03:54","title":"Top-Personalized-K Recommendation","abstract":"The conventional top-K recommendation, which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists. However, is this fixed-size top-K recommendation the optimal approach for every user's satisfaction? Not necessarily. We point out that providing fixed-size recommendations without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones. To address this issue, we introduce Top-Personalized-K Recommendation, a new recommendation task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction. As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility. Through extensive experiments on real-world datasets, we demonstrate the superiority of PerK in Top-Personalized-K recommendation task. We expect that Top-Personalized-K recommendation has the potential to offer enhanced solutions for various real-world recommendation scenarios, based on its great compatibility with existing models.","sentences":["The conventional top-K recommendation, which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists.","However, is this fixed-size top-K recommendation the optimal approach for every user's satisfaction?","Not necessarily.","We point out that providing fixed-size recommendations without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones.","To address this issue, we introduce Top-Personalized-K Recommendation, a new recommendation task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction.","As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility.","Through extensive experiments on real-world datasets, we demonstrate the superiority of PerK in Top-Personalized-K recommendation task.","We expect that Top-Personalized-K recommendation has the potential to offer enhanced solutions for various real-world recommendation scenarios, based on its great compatibility with existing models."],"url":"http://arxiv.org/abs/2402.16304v1","category":"cs.IR"}
{"created":"2024-02-26 04:58:42","title":"Graph Diffusion Policy Optimization","abstract":"Recent research has made significant progress in optimizing diffusion models for specific downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph diffusion presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.","sentences":["Recent research has made significant progress in optimizing diffusion models for specific downstream objectives, which is an important pursuit in fields such as graph generation for drug design.","However, directly applying these models to graph diffusion presents challenges, resulting in suboptimal performance.","This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning.","GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance.","Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives.","Code is available at https://github.com/sail-sg/GDPO."],"url":"http://arxiv.org/abs/2402.16302v1","category":"cs.LG"}
{"created":"2024-02-26 04:41:04","title":"MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer","abstract":"Traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis. In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors. Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct convolutional branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations. In this paper, we propose an innovative multi-view network exclusively based on transformers to address challenges in mammographic image classification. Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between views at the spatial feature map level. Furthermore, we conduct a comprehensive comparative analysis of the performance and effectiveness of transformer-based models under diverse settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is publicly available at https://github.com/prithuls/MV-Swin-T","sentences":["Traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis.","In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors.","Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct convolutional branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations.","In this paper, we propose an innovative multi-view network exclusively based on transformers to address challenges in mammographic image classification.","Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between views at the spatial feature map level.","Furthermore, we conduct a comprehensive comparative analysis of the performance and effectiveness of transformer-based models under diverse settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets.","Our code is publicly available at https://github.com/prithuls/MV-Swin-T"],"url":"http://arxiv.org/abs/2402.16298v1","category":"cs.CV"}
{"created":"2024-02-26 04:39:01","title":"Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics","abstract":"Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation. Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices.","sentences":["Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data.","Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences.","However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series.","To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains.","Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation.","Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices."],"url":"http://arxiv.org/abs/2402.16297v1","category":"cs.LG"}
{"created":"2024-02-26 04:31:53","title":"Decentralized Federated Unlearning on Blockchain","abstract":"Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conducted in parallel or serial. We conduct a comprehensive study of two typical unlearning methods, gradient ascent and re-training, demonstrating the efficient unlearning workflow in these two categories with minimal CH and block update operations. Additionally, we compare the computation and communication costs of these methods.","sentences":["Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes.","Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship.","However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved.","To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.","Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conducted in parallel or serial.","We conduct a comprehensive study of two typical unlearning methods, gradient ascent and re-training, demonstrating the efficient unlearning workflow in these two categories with minimal CH and block update operations.","Additionally, we compare the computation and communication costs of these methods."],"url":"http://arxiv.org/abs/2402.16294v1","category":"cs.CR"}
{"created":"2024-02-26 04:30:33","title":"Images and flares of geodesic hotspots around a Kerr black hole","abstract":"In this study, we develop a numerical method to generate images on an observer's screen, formed by radiation from hotspots on any timelike orbits outside a black hole. This method uses the calculation of fractional numbers, enabling us not only to produce the overall image but also to distinguish between primary, secondary, and higher-order images. Building upon this, we compute the images of hotspots from eight potential types of geodesic timelike orbits outside a Kerr black hole, summarizing the properties of both the overall and individual order images. Furthermore, we calculate the centroid motion and lightcurve. Notably, we observe flare phenomena across all orbit types and classify these flares into three categories based on the Doppler and gravitational redshift effects.","sentences":["In this study, we develop a numerical method to generate images on an observer's screen, formed by radiation from hotspots on any timelike orbits outside a black hole.","This method uses the calculation of fractional numbers, enabling us not only to produce the overall image but also to distinguish between primary, secondary, and higher-order images.","Building upon this, we compute the images of hotspots from eight potential types of geodesic timelike orbits outside a Kerr black hole, summarizing the properties of both the overall and individual order images.","Furthermore, we calculate the centroid motion and lightcurve.","Notably, we observe flare phenomena across all orbit types and classify these flares into three categories based on the Doppler and gravitational redshift effects."],"url":"http://arxiv.org/abs/2402.16293v1","category":"gr-qc"}
{"created":"2024-02-26 04:20:16","title":"Marginal Independence and Partial Set Partitions","abstract":"We establish a bijection between marginal independence models on $n$ random variables and split closed order ideals in the poset of partial set partitions. We also establish that every discrete marginal independence model is toric in cdf coordinates. This generalizes results of Boege, Petrovic, and Sturmfels and Drton and Richardson, and provides a unified framework for discussing marginal independence models.","sentences":["We establish a bijection between marginal independence models on $n$ random variables and split closed order ideals in the poset of partial set partitions.","We also establish that every discrete marginal independence model is toric in cdf coordinates.","This generalizes results of Boege, Petrovic, and Sturmfels and Drton and Richardson, and provides a unified framework for discussing marginal independence models."],"url":"http://arxiv.org/abs/2402.16292v1","category":"math.ST"}
{"created":"2024-02-26 04:11:58","title":"Multi-qubit gates and 'Schr\u00f6dinger cat' states in an optical clock","abstract":"Many-particle entanglement is a key resource for achieving the fundamental precision limits of a quantum sensor. Optical atomic clocks, the current state-of-the-art in frequency precision, are a rapidly emerging area of focus for entanglement-enhanced metrology. Augmenting tweezer-based clocks featuring microscopic control and detection with the high-fidelity entangling gates developed for atom-array information processing offers a promising route towards leveraging highly entangled quantum states for improved optical clocks. Here we develop and employ a family of multi-qubit Rydberg gates to generate 'Schr\\\"odinger cat' states of the Greenberger-Horne-Zeilinger (GHZ) type with up to 9 optical clock qubits in a programmable atom array. In an atom-laser comparison at sufficiently short dark times, we demonstrate a fractional frequency instability below the standard quantum limit using GHZ states of up to 4 qubits. A key challenge to improving the optimal achievable clock precision with GHZ states is their reduced dynamic range. Towards overcoming this hurdle, we simultaneously prepare a cascade of varying-size GHZ states to perform unambiguous phase estimation over an extended interval. These results demonstrate key building blocks for approaching Heisenberg-limited scaling of optical atomic clock precision.","sentences":["Many-particle entanglement is a key resource for achieving the fundamental precision limits of a quantum sensor.","Optical atomic clocks, the current state-of-the-art in frequency precision, are a rapidly emerging area of focus for entanglement-enhanced metrology.","Augmenting tweezer-based clocks featuring microscopic control and detection with the high-fidelity entangling gates developed for atom-array information processing offers a promising route towards leveraging highly entangled quantum states for improved optical clocks.","Here we develop and employ a family of multi-qubit Rydberg gates to generate 'Schr\\\"odinger cat' states of the Greenberger-Horne-Zeilinger (GHZ) type with up to 9 optical clock qubits in a programmable atom array.","In an atom-laser comparison at sufficiently short dark times, we demonstrate a fractional frequency instability below the standard quantum limit using GHZ states of up to 4 qubits.","A key challenge to improving the optimal achievable clock precision with GHZ states is their reduced dynamic range.","Towards overcoming this hurdle, we simultaneously prepare a cascade of varying-size GHZ states to perform unambiguous phase estimation over an extended interval.","These results demonstrate key building blocks for approaching Heisenberg-limited scaling of optical atomic clock precision."],"url":"http://arxiv.org/abs/2402.16289v1","category":"quant-ph"}
{"created":"2024-02-26 04:09:53","title":"PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering","abstract":"Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and three retrievers. Experimental results demonstrate that BERT-based classification models significantly outperform LLMs such as ChatGLM3 and ChatGPT in the memory classification task. Furthermore, our study highlights the importance of effective memory integration in the QA task.","sentences":["Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues.","Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues.","This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task.","PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs).","Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis.","We evaluate this framework using five LLMs and three retrievers.","Experimental results demonstrate that BERT-based classification models significantly outperform LLMs such as ChatGLM3 and ChatGPT in the memory classification task.","Furthermore, our study highlights the importance of effective memory integration in the QA task."],"url":"http://arxiv.org/abs/2402.16288v1","category":"cs.CL"}
{"created":"2024-02-26 04:08:19","title":"Highly Accurate Description of Long-Range Interactions through the Combination of Neural Networks and Physical Models","abstract":"We present a simple and general way to accurately describe long-range interactions between atoms and molecules through combining neural networks with physical models. Demonstrations on the H$_3$, Li$_3$ and 2KRb systems illustrate the exceptional extrapolation capabilities of the trained model, supported by underlying physical models. More importantly, the model exhibits high accuracy at energy scales below a few hundred millikelvin, where the reliability of $ab~initio$ methods diminishes.","sentences":["We present a simple and general way to accurately describe long-range interactions between atoms and molecules through combining neural networks with physical models.","Demonstrations on the H$_3$, Li$_3$ and 2KRb systems illustrate the exceptional extrapolation capabilities of the trained model, supported by underlying physical models.","More importantly, the model exhibits high accuracy at energy scales below a few hundred millikelvin, where the reliability of $ab~initio$ methods diminishes."],"url":"http://arxiv.org/abs/2402.16287v1","category":"physics.atom-ph"}
{"created":"2024-02-26 03:55:15","title":"Pre-merger detection of massive black hole binaries using deep learning","abstract":"Coalescing massive black hole binaries (MBHBs) are one of primary sources for space-based gravitational wave (GW) observations. The mergers of these binaries are expected to give rise to detectable electromagnetic (EM) emissions with a narrow time window. The pre-merger detection of GW signals is vital for follow-up EM observations. The conventional approach for searching GW signals involves high computational costs. In this study, we present a deep learning model to search for GW signals from MBHBs. Our model is able to process 4.7 days of simulated data within 0.01 seconds and detect GW signals several hours to days before the final merger. The model provides the possibility of the coincident GW and EM detection of MBHBs.","sentences":["Coalescing massive black hole binaries (MBHBs) are one of primary sources for space-based gravitational wave (GW) observations.","The mergers of these binaries are expected to give rise to detectable electromagnetic (EM) emissions with a narrow time window.","The pre-merger detection of GW signals is vital for follow-up EM observations.","The conventional approach for searching GW signals involves high computational costs.","In this study, we present a deep learning model to search for GW signals from MBHBs.","Our model is able to process 4.7 days of simulated data within 0.01 seconds and detect GW signals several hours to days before the final merger.","The model provides the possibility of the coincident GW and EM detection of MBHBs."],"url":"http://arxiv.org/abs/2402.16282v1","category":"astro-ph.IM"}
{"created":"2024-02-26 03:54:32","title":"Towards Agile Robots: Intuitive Robot Position Speculation with Neural Networks","abstract":"The robot position speculation, which determines where the chassis should move, is one key step to control the mobile manipulators. The target position must ensure the feasibility of chassis movement and manipulability, which is guaranteed by randomized sampling and kinematic checking in traditional methods. Addressing the demands of agile robotics, this paper proposes a robot position speculation network(RPSN), a learning-based approach to enhance the agility of mobile manipulators. The RPSN incorporates a differentiable inverse kinematic algorithm and a neural network. Through end-to-end training, the RPSN can speculate positions with a high success rate. We apply the RPSN to mobile manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs). Extensive experiments on various simulated environments and physical mobile manipulators demonstrate that the probability of the initial position provided by RPSN being the ideal position is 96.67%. From the kinematic constraint perspective, it achieves 100% generation of the ideal position on average within 1.28 attempts. Much lower than that of random sampling, 31.04. Moreover, the proposed method demonstrates superior data efficiency over pure neural network approaches. The proposed RPSN enables the robot to quickly infer feasible target positions by intuition. This work moves towards building agile robots that can act swiftly like humans.","sentences":["The robot position speculation, which determines where the chassis should move, is one key step to control the mobile manipulators.","The target position must ensure the feasibility of chassis movement and manipulability, which is guaranteed by randomized sampling and kinematic checking in traditional methods.","Addressing the demands of agile robotics, this paper proposes a robot position speculation network(RPSN), a learning-based approach to enhance the agility of mobile manipulators.","The RPSN incorporates a differentiable inverse kinematic algorithm and a neural network.","Through end-to-end training, the RPSN can speculate positions with a high success rate.","We apply the RPSN to mobile manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs).","Extensive experiments on various simulated environments and physical mobile manipulators demonstrate that the probability of the initial position provided by RPSN being the ideal position is 96.67%.","From the kinematic constraint perspective, it achieves 100% generation of the ideal position on average within 1.28 attempts.","Much lower than that of random sampling, 31.04.","Moreover, the proposed method demonstrates superior data efficiency over pure neural network approaches.","The proposed RPSN enables the robot to quickly infer feasible target positions by intuition.","This work moves towards building agile robots that can act swiftly like humans."],"url":"http://arxiv.org/abs/2402.16281v1","category":"cs.RO"}
{"created":"2024-02-26 03:49:18","title":"Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation","abstract":"Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances. As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative adversarial learning, etc. In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of few-shot learning (FSL). Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation. To achieve this goal, we adopt the meta-learning based FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task. First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of few-shot instance segmentation (FSIS) to generalized few-shot instance segmentation (GFSIS). Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized Few-Shot Instance Segmentation (SGFSIS) framework. Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including semi-supervised learning, simple transfer learning, etc., with comparable performance to fully supervised learning with less than 5% annotations.","sentences":["Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances.","As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative adversarial learning, etc.","In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of few-shot learning (FSL).","Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation.","To achieve this goal, we adopt the meta-learning based FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task.","First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of few-shot instance segmentation (FSIS) to generalized few-shot instance segmentation (GFSIS).","Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized Few-Shot Instance Segmentation (SGFSIS) framework.","Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including semi-supervised learning, simple transfer learning, etc., with comparable performance to fully supervised learning with less than 5% annotations."],"url":"http://arxiv.org/abs/2402.16280v1","category":"cs.CV"}
{"created":"2024-02-26 03:46:55","title":"Quadratic Message Passing for Generalized Quadratic Equations Model","abstract":"For approximate inference in the generalized quadratic equations model, many state-of-the-art algorithms lack any prior knowledge of the target signal structure, exhibits slow convergence, and can not handle any analytic prior knowledge of the target signal structure. So, this paper proposes a new algorithm, Quadratic Message passing (QMP). QMP has a complexity as low as $O(N^{3})$. The SE derived for QMP can capture precisely the per-iteration behavior of the simulated algorithm. Simulation results confirm QMP outperforms many state-of-the-art algorithms.","sentences":["For approximate inference in the generalized quadratic equations model, many state-of-the-art algorithms lack any prior knowledge of the target signal structure, exhibits slow convergence, and can not handle any analytic prior knowledge of the target signal structure.","So, this paper proposes a new algorithm, Quadratic Message passing (QMP).","QMP has a complexity as low as $O(N^{3})$. The SE derived for QMP can capture precisely the per-iteration behavior of the simulated algorithm.","Simulation results confirm QMP outperforms many state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2402.16279v1","category":"cs.IT"}
{"created":"2024-02-26 03:46:01","title":"A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction","abstract":"Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that the self-matching training method with InME outperforms the existing ontology embeddings for the GO and FoodOn ontologies and that the method with the concatenation of CoME and OWL2Vec* outperforms them for the HeLiS ontology.","sentences":["Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion.","However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology.","In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME).","The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom.","The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology.","Our evaluation experiments show that the self-matching training method with InME outperforms the existing ontology embeddings for the GO and FoodOn ontologies and that the method with the concatenation of CoME and OWL2Vec* outperforms them for the HeLiS ontology."],"url":"http://arxiv.org/abs/2402.16278v1","category":"cs.AI"}
{"created":"2024-02-26 03:44:54","title":"A Panoramic Study of $K$-Factors for 111 Processes at the 14 TeV LHC","abstract":"In this comprehensive study, we investigate $K$-factors ($K=\\sigma_{\\text{NLO}}/\\sigma_{\\text{LO}}\\equiv 1+\\delta K$) for a broad array of Standard Model processes at the 14 TeV LHC, which are pivotal for background assessments in Beyond the Standard Model (BSM) searches. Using MadGraph5_aMC@NLO, we calculate the leading-order and next-to-leading order (NLO) cross-sections and compute the corresponding $K$-factors for 111 processes. Our analysis reveals $K$-factors ranging from 1.005 for $pp \\to jjj$ to 4.221 for $pp\\to W^\\pm \\gamma\\gamma\\gamma$. Key findings include: (i) processes involving photons display significantly high $K$-factors, attributed to gluon-initiated processes at NLO; (ii) processes with multiple particle productions, particularly those involving vector bosons, exhibit elevated $K$-factors due to multiple real emission processes; (iii) there exists an inverse correlation between the number of jets and $\\delta K$, indicating that the addition of jets generally leads to a decrease in $\\delta K$. Additionally, our investigation into differential $K$-factors relative to transverse momentum and invariant mass shows notable increases with higher $p_T$, but minimal changes with invariant mass. This study highlights the indispensable role of precise $K$-factor evaluations for accurate interpretations of BSM search outcomes.","sentences":["In this comprehensive study, we investigate $K$-factors ($K=\\sigma_{\\text{NLO}}/\\sigma_{\\text{LO}}\\equiv 1+\\delta K$) for a broad array of Standard Model processes at the 14 TeV LHC, which are pivotal for background assessments in Beyond the Standard Model (BSM) searches.","Using MadGraph5_aMC@NLO, we calculate the leading-order and next-to-leading order (NLO) cross-sections and compute the corresponding $K$-factors for 111 processes.","Our analysis reveals $K$-factors ranging from 1.005 for $pp \\to jjj$ to 4.221 for $pp\\to W^\\pm \\gamma\\gamma\\gamma$. Key findings include: (i) processes involving photons display significantly high $K$-factors, attributed to gluon-initiated processes at NLO; (ii) processes with multiple particle productions, particularly those involving vector bosons, exhibit elevated $K$-factors due to multiple real emission processes; (iii) there exists an inverse correlation between the number of jets and $\\delta K$, indicating that the addition of jets generally leads to a decrease in $\\delta K$. Additionally, our investigation into differential $K$-factors relative to transverse momentum and invariant mass shows notable increases with higher $p_T$, but minimal changes with invariant mass.","This study highlights the indispensable role of precise $K$-factor evaluations for accurate interpretations of BSM search outcomes."],"url":"http://arxiv.org/abs/2402.16276v1","category":"hep-ph"}
{"created":"2024-02-26 03:33:44","title":"Baryon asymmetry constraints on Extended Symmetric Teleparallel Gravity","abstract":"In this paper, we have explored the observed matter-antimatter asymmetry in the Universe to constrain the model parameters in extended symmetric teleparallel gravity (STG) or $f(Q,T)$ gravity, where $Q$ be the nonmetricity and $T$ be the trace of energy momentum tensor. We have considered two functional forms of $f(Q,T)$ to find the baryon asymmetry to entropy ratio calculated at a decoupling temperature. Two different data sets namely Hubble data set and the Hubble+BAO+Pantheon data sets are used to constrain the scale factor and the constrained model is used to obtain the baryon asymmetry to entropy ratio. It is observed that, model constrained from the Hubble data set favour a narrow range of the $f(Q,T)$ gravity parameters to reproduce the observed baryon asymmetry.","sentences":["In this paper, we have explored the observed matter-antimatter asymmetry in the Universe to constrain the model parameters in extended symmetric teleparallel gravity (STG) or $f(Q,T)$ gravity, where $Q$ be the nonmetricity and $T$ be the trace of energy momentum tensor.","We have considered two functional forms of $f(Q,T)$ to find the baryon asymmetry to entropy ratio calculated at a decoupling temperature.","Two different data sets namely Hubble data set and the Hubble+BAO+Pantheon data sets are used to constrain the scale factor and the constrained model is used to obtain the baryon asymmetry to entropy ratio.","It is observed that, model constrained from the Hubble data set favour a narrow range of the $f(Q,T)$ gravity parameters to reproduce the observed baryon asymmetry."],"url":"http://arxiv.org/abs/2402.16275v1","category":"gr-qc"}
{"created":"2024-02-26 03:31:22","title":"Radar Anti-jamming Strategy Learning via Domain-knowledge Enhanced Online Convex Optimization","abstract":"The dynamic competition between radar and jammer systems presents a significant challenge for modern Electronic Warfare (EW), as current active learning approaches still lack sample efficiency and fail to exploit jammer's characteristics. In this paper, the competition between a frequency agile radar and a Digital Radio Frequency Memory (DRFM)-based intelligent jammer is considered. We introduce an Online Convex Optimization (OCO) framework designed to illustrate this adversarial interaction. Notably, traditional OCO algorithms exhibit suboptimal sample efficiency due to the limited information obtained per round. To address the limitations, two refined algorithms are proposed, utilizing unbiased gradient estimators that leverage the unique attributes of the jammer system. Sub-linear theoretical results on both static regret and universal regret are provided, marking a significant improvement in OCO performance. Furthermore, simulation results reveal that the proposed algorithms outperform common OCO baselines, suggesting the potential for effective deployment in real-world scenarios.","sentences":["The dynamic competition between radar and jammer systems presents a significant challenge for modern Electronic Warfare (EW), as current active learning approaches still lack sample efficiency and fail to exploit jammer's characteristics.","In this paper, the competition between a frequency agile radar and a Digital Radio Frequency Memory (DRFM)-based intelligent jammer is considered.","We introduce an Online Convex Optimization (OCO) framework designed to illustrate this adversarial interaction.","Notably, traditional OCO algorithms exhibit suboptimal sample efficiency due to the limited information obtained per round.","To address the limitations, two refined algorithms are proposed, utilizing unbiased gradient estimators that leverage the unique attributes of the jammer system.","Sub-linear theoretical results on both static regret and universal regret are provided, marking a significant improvement in OCO performance.","Furthermore, simulation results reveal that the proposed algorithms outperform common OCO baselines, suggesting the potential for effective deployment in real-world scenarios."],"url":"http://arxiv.org/abs/2402.16274v1","category":"eess.SP"}
{"created":"2024-02-26 03:10:11","title":"From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto","abstract":"Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using ChatGPT. We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed. We also propose possible research directions to overcome these gaps. We also see this work as a call to action to bring together the LLM and optimization communities to pursue our vision, thereby enabling much more widespread improved decision-making.","sentences":["Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions.","The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal.","Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model.","This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation.","We describe the state of the art through a literature survey and experiments using ChatGPT.","We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed.","We also propose possible research directions to overcome these gaps.","We also see this work as a call to action to bring together the LLM and optimization communities to pursue our vision, thereby enabling much more widespread improved decision-making."],"url":"http://arxiv.org/abs/2402.16269v1","category":"cs.AI"}
{"created":"2024-02-26 03:09:06","title":"Foundation Model Transparency Reports","abstract":"Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g., the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.","sentences":["Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency.","To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media.","While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent.","To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting.","To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index.","Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g., the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI).","Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions.","We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House."],"url":"http://arxiv.org/abs/2402.16268v1","category":"cs.LG"}
{"created":"2024-02-26 03:04:15","title":"Mean values of multiplicative functions and applications to residue-class distribution","abstract":"We provide a uniform bound on the partial sums of multiplicative functions under very general hypotheses. As an application, we give a nearly optimal estimate for the count of $n \\le x$ for which the Alladi-Erd\\H{o}s function $A(n) = \\sum_{p^k \\parallel n} k p$ takes values in a given residue class modulo $q$, where $q$ varies uniformly up to a fixed power of $\\log x$. We establish a similar result for the equidistribution of the Euler totient function $\\phi(n)$ among the coprime residues to the \"correct\" moduli $q$ that vary uniformly in a similar range, and also quantify the failure of equidistribution of the values of $\\phi(n)$ among the coprime residue classes to the \"incorrect\" moduli.","sentences":["We provide a uniform bound on the partial sums of multiplicative functions under very general hypotheses.","As an application, we give a nearly optimal estimate for the count of $n \\le x$ for which the Alladi-Erd\\H{o}s function $A(n) = \\sum_{p^k","\\parallel n} k p$ takes values in a given residue class modulo $q$, where $q$ varies uniformly up to a fixed power of $\\log x$.","We establish a similar result for the equidistribution of the Euler totient function $\\phi(n)$ among the coprime residues to the \"correct\" moduli $q$ that vary uniformly in a similar range, and also quantify the failure of equidistribution of the values of $\\phi(n)$ among the coprime residue classes to the \"incorrect\" moduli."],"url":"http://arxiv.org/abs/2402.16266v1","category":"math.NT"}
{"created":"2024-02-26 03:04:12","title":"Quantum light source, photon pairs, single photon source, flat optics, metasurface","abstract":"Quantum light sources are essential building blocks for many quantum technologies, enabling secure communication, powerful computing, precise sensing and imaging. Recent advancements have witnessed a significant shift towards the utilization of ``flat\" optics with thickness at subwavelength scales for the development of quantum light sources. This approach offers notable advantages over conventional bulky counterparts, including compactness, scalability, and improved efficiency, along with added functionalities. This review focuses on the recent advances in leveraging flat optics to generate quantum light sources. Specifically, we explore the generation of entangled photon pairs through spontaneous parametric down-conversion in nonlinear metasurfaces, as well as single photon emission from quantum emitters including quantum dots and color centers in 3D and 2D materials. The review covers theoretical principles, fabrication techniques, and properties of these sources, with particular emphasis on the enhanced generation and engineering of quantum light sources using optical resonances supported by nanostructures. We discuss the diverse application range of these sources and highlight the current challenges and perspectives in the field.","sentences":["Quantum light sources are essential building blocks for many quantum technologies, enabling secure communication, powerful computing, precise sensing and imaging.","Recent advancements have witnessed a significant shift towards the utilization of ``flat\" optics with thickness at subwavelength scales for the development of quantum light sources.","This approach offers notable advantages over conventional bulky counterparts, including compactness, scalability, and improved efficiency, along with added functionalities.","This review focuses on the recent advances in leveraging flat optics to generate quantum light sources.","Specifically, we explore the generation of entangled photon pairs through spontaneous parametric down-conversion in nonlinear metasurfaces, as well as single photon emission from quantum emitters including quantum dots and color centers in 3D and 2D materials.","The review covers theoretical principles, fabrication techniques, and properties of these sources, with particular emphasis on the enhanced generation and engineering of quantum light sources using optical resonances supported by nanostructures.","We discuss the diverse application range of these sources and highlight the current challenges and perspectives in the field."],"url":"http://arxiv.org/abs/2402.16265v1","category":"physics.optics"}
{"created":"2024-02-26 03:03:44","title":"Intrinsic supercurrent diode effect in NbSe2 nanobridge","abstract":"The significance of the superconducting diode effect lies in its potential application as a fundamental component in the development of next-generation superconducting circuit technology. The stringent operating conditions at low temperatures have posed challenges for the conventional semiconductor diode, primarily due to its exceptionally high resistivity. In response to this limitation, various approaches have emerged to achieve the superconducting diode effect, primarily involving the disruption of inversion symmetry in a two-dimensional superconductor through heterostructure fabrication. In this study, we present a direct observation of the supercurrent diode effect in a NbSe2 nanobridge with a length of approximately 15 nm, created using focused helium ion beam fabrication. Nonreciprocal supercurrents were identified, reaching a peak value of approximately 380 $\\mu$A for each bias polarity at $B_{z}^{max} =\\pm 0.2$ mT. Notably, the nonreciprocal supercurrent can be toggled by altering the bias polarity. This discovery of the superconducting diode effect introduces a novel avenue and mechanism through nanofabrication on a superconducting flake, offering fresh perspectives for the development of superconducting devices and potential circuits.","sentences":["The significance of the superconducting diode effect lies in its potential application as a fundamental component in the development of next-generation superconducting circuit technology.","The stringent operating conditions at low temperatures have posed challenges for the conventional semiconductor diode, primarily due to its exceptionally high resistivity.","In response to this limitation, various approaches have emerged to achieve the superconducting diode effect, primarily involving the disruption of inversion symmetry in a two-dimensional superconductor through heterostructure fabrication.","In this study, we present a direct observation of the supercurrent diode effect in a NbSe2 nanobridge with a length of approximately 15 nm, created using focused helium ion beam fabrication.","Nonreciprocal supercurrents were identified, reaching a peak value of approximately 380 $\\mu$A for each bias polarity at $B_{z}^{max} =\\pm 0.2$ mT. Notably, the nonreciprocal supercurrent can be toggled by altering the bias polarity.","This discovery of the superconducting diode effect introduces a novel avenue and mechanism through nanofabrication on a superconducting flake, offering fresh perspectives for the development of superconducting devices and potential circuits."],"url":"http://arxiv.org/abs/2402.16264v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-26 02:54:04","title":"CoGenT: A Content-oriented Generative-hit Framework for Content Delivery Networks","abstract":"The service provided by content delivery networks (CDNs) may overlook content locality, leaving room for potential performance improvement. In this study, we explore the feasibility of leveraging generated data as a replacement for fetching data in missing scenarios based on content locality. Due to sufficient local computing resources and reliable generation efficiency, we propose a content-oriented generative-hit framework (CoGenT) for CDNs. CoGenT utilizes idle computing resources on edge nodes to generate requested data based on similar or related cached data to achieve hits. Our implementation in a real-world system demonstrates that CoGenT reduces the average access latency by half. Additionally, experiments conducted on a simulator also confirm that CoGenT can enhance existing caching algorithms, resulting in reduced latency and bandwidth usage.","sentences":["The service provided by content delivery networks (CDNs) may overlook content locality, leaving room for potential performance improvement.","In this study, we explore the feasibility of leveraging generated data as a replacement for fetching data in missing scenarios based on content locality.","Due to sufficient local computing resources and reliable generation efficiency, we propose a content-oriented generative-hit framework (CoGenT) for CDNs.","CoGenT utilizes idle computing resources on edge nodes to generate requested data based on similar or related cached data to achieve hits.","Our implementation in a real-world system demonstrates that CoGenT reduces the average access latency by half.","Additionally, experiments conducted on a simulator also confirm that CoGenT can enhance existing caching algorithms, resulting in reduced latency and bandwidth usage."],"url":"http://arxiv.org/abs/2402.16262v1","category":"cs.DC"}
{"created":"2024-02-26 02:48:43","title":"UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval","abstract":"Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture the subtle relationship between dialogue context and different candidates by regarding historically selected candidates as hard negatives. Extensive experiments and analysis establish state-of-the-art retrieval quality both within and outside its training domain, revealing the promising potential and generalization capability of our model to serve as a universal retriever for different candidate selection tasks simultaneously.","sentences":["Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue.","However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency.","Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection.","To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product.","Furthermore, we introduce two loss constraints to capture the subtle relationship between dialogue context and different candidates by regarding historically selected candidates as hard negatives.","Extensive experiments and analysis establish state-of-the-art retrieval quality both within and outside its training domain, revealing the promising potential and generalization capability of our model to serve as a universal retriever for different candidate selection tasks simultaneously."],"url":"http://arxiv.org/abs/2402.16261v1","category":"cs.CL"}
{"created":"2024-02-26 02:44:14","title":"Problems on Group-labeled Matroid Bases","abstract":"Consider a matroid equipped with a labeling of its ground set to an abelian group. We define the label of a subset of the ground set as the sum of the labels of its elements. We study a collection of problems on finding bases and common bases of matroids with restrictions on their labels. For zero bases and zero common bases, the results are mostly negative. While finding a non-zero basis of a matroid is not difficult, it turns out that the complexity of finding a non-zero common basis depends on the group. Namely, we show that the problem is hard for a fixed group if it contains an element of order two, otherwise it is polynomially solvable.   As a generalization of both zero and non-zero constraints, we further study $F$-avoiding constraints where we seek a basis or common basis whose label is not in a given set $F$ of forbidden labels. Using algebraic techniques, we give a randomized algorithm for finding an $F$-avoiding common basis of two matroids represented over the same field for finite groups given as operation tables. The study of $F$-avoiding bases with groups given as oracles leads to a conjecture stating that whenever an $F$-avoiding basis exists, an $F$-avoiding basis can be obtained from an arbitrary basis by exchanging at most $|F|$ elements. We prove the conjecture for the special cases when $|F|\\le 2$ or the group is ordered. By relying on structural observations on matroids representable over fixed, finite fields, we verify a relaxed version of the conjecture for these matroids. As a consequence, we obtain a polynomial-time algorithm in these special cases for finding an $F$-avoiding basis when $|F|$ is fixed.","sentences":["Consider a matroid equipped with a labeling of its ground set to an abelian group.","We define the label of a subset of the ground set as the sum of the labels of its elements.","We study a collection of problems on finding bases and common bases of matroids with restrictions on their labels.","For zero bases and zero common bases, the results are mostly negative.","While finding a non-zero basis of a matroid is not difficult, it turns out that the complexity of finding a non-zero common basis depends on the group.","Namely, we show that the problem is hard for a fixed group if it contains an element of order two, otherwise it is polynomially solvable.   ","As a generalization of both zero and non-zero constraints, we further study $F$-avoiding constraints where we seek a basis or common basis whose label is not in a given set $F$ of forbidden labels.","Using algebraic techniques, we give a randomized algorithm for finding an $F$-avoiding common basis of two matroids represented over the same field for finite groups given as operation tables.","The study of $F$-avoiding bases with groups given as oracles leads to a conjecture stating that whenever an $F$-avoiding basis exists, an $F$-avoiding basis can be obtained from an arbitrary basis by exchanging at most $|F|$ elements.","We prove the conjecture for the special cases when $|F|\\le 2$ or the group is ordered.","By relying on structural observations on matroids representable over fixed, finite fields, we verify a relaxed version of the conjecture for these matroids.","As a consequence, we obtain a polynomial-time algorithm in these special cases for finding an $F$-avoiding basis when $|F|$ is fixed."],"url":"http://arxiv.org/abs/2402.16259v1","category":"cs.DM"}
{"created":"2024-02-26 02:37:39","title":"Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models","abstract":"Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: \\textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the \"Assembled Projection Heads\" (APH) method for enhancing the reliability of federated models. By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted fine-tuning on locally available data under varying learning rates. Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging. We evaluate the effectiveness of the proposed APH method across three prominent federated benchmarks. Experimental results validate the efficacy of APH in model calibration and uncertainty estimation. Notably, APH can be seamlessly integrated into various federated approaches but only requires less than 30\\% additional computation cost for 100$\\times$ inferences within large models.","sentences":["Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues.","While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded.","In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models.","Our exploration uncovers a significant finding: \\textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data.","This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models.","Inspired by this observation, we propose the \"Assembled Projection Heads\" (APH) method for enhancing the reliability of federated models.","By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted fine-tuning on locally available data under varying learning rates.","Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging.","We evaluate the effectiveness of the proposed APH method across three prominent federated benchmarks.","Experimental results validate the efficacy of APH in model calibration and uncertainty estimation.","Notably, APH can be seamlessly integrated into various federated approaches but only requires less than 30\\% additional computation cost for 100$\\times$ inferences within large models."],"url":"http://arxiv.org/abs/2402.16255v1","category":"cs.LG"}
{"created":"2024-02-26 02:34:41","title":"To be, or not to be, that is the Question: Exploring the pseudorandom generation of texts to write Hamlet from the perspective of the Infinite Monkey Theorem","abstract":"This article explores the theoretical and computational aspects of the Infinite Monkey Theorem, investigating the number of attempts and the time required for a set of pseudorandom characters to assemble and recite Hamlets iconic phrase, To be, or not to be, that is the Question. Drawing inspiration from Emile Borels original concept (1913), the study delves into the practical implications of pseudorandomness using Python. Employing Python simulations to generate excerpts from Hamlet, the research navigates historical perspectives and bridges early theoretical foundations with contemporary computational approaches. A set of tests reveals the attempts and time required to generate incremental parts of the target phrase. Utilizing these results, growth factors are calculated, projecting estimated attempts and time for each text part. The findings indicate an astronomical challenge to generate the entire phrase, requiring approximately 2.68x10e69 attempts and 2.95x10e66 seconds - equivalent to 8.18x10e62 hours or 9.32x10e55 years. This temporal scale, exceeding the age of the universe by 6.75x10e45 times, underscores the immense complexity and improbability of random literary creation. The article concludes with reflections on the mathematical intricacies and statistical feasibility within the context of the Infinite Monkey Theorem, emphasizing the theoretical musings surrounding infinite time and the profound limitations inherent in such endeavors. And that only infinity could write Hamlet randomly.","sentences":["This article explores the theoretical and computational aspects of the Infinite Monkey Theorem, investigating the number of attempts and the time required for a set of pseudorandom characters to assemble and recite Hamlets iconic phrase, To be, or not to be, that is the Question.","Drawing inspiration from Emile Borels original concept (1913), the study delves into the practical implications of pseudorandomness using Python.","Employing Python simulations to generate excerpts from Hamlet, the research navigates historical perspectives and bridges early theoretical foundations with contemporary computational approaches.","A set of tests reveals the attempts and time required to generate incremental parts of the target phrase.","Utilizing these results, growth factors are calculated, projecting estimated attempts and time for each text part.","The findings indicate an astronomical challenge to generate the entire phrase, requiring approximately 2.68x10e69 attempts and 2.95x10e66 seconds - equivalent to 8.18x10e62 hours or 9.32x10e55 years.","This temporal scale, exceeding the age of the universe by 6.75x10e45 times, underscores the immense complexity and improbability of random literary creation.","The article concludes with reflections on the mathematical intricacies and statistical feasibility within the context of the Infinite Monkey Theorem, emphasizing the theoretical musings surrounding infinite time and the profound limitations inherent in such endeavors.","And that only infinity could write Hamlet randomly."],"url":"http://arxiv.org/abs/2402.16253v1","category":"math.NA"}
{"created":"2024-02-26 02:30:42","title":"Cyclic sieving on permutations -- an analysis of maps and statistics in the FindStat database","abstract":"We perform a systematic study of permutation statistics and bijective maps on permutations using SageMath to search the FindStat combinatorial statistics database to identify apparent instances of the cyclic sieving phenomenon (CSP). Cyclic sieving occurs on a set of objects, a statistic, and a map of order $n$ when the evaluation of the statistic generating function at the $d$th power of the primitive $n$th root of unity equals the number of fixed points under the $d$th power of the map. Of the apparent instances found in our experiment, we prove 34 new instances of the CSP, and conjecture three more. Furthermore, we prove the equidistribution of some statistics and show that some maps have the same orbit structure, thus cyclic sieving holds for more even more pairs of a map and a statistic. The maps which exhibit the CSP include reverse/complement, rotation, Lehmer code rotation, toric promotion, and conjugation by the long cycle, as well as a map constructed by Corteel to swap the number of nestings and crossings, the invert Laguerre heap map, and a map of Alexandersson and Kebede designed to preserve right-to-left minima. Our results show that, contrary to common expectations, actions that exhibit homomesy are not necessarily the best candidates for the CSP, and vice versa.","sentences":["We perform a systematic study of permutation statistics and bijective maps on permutations using SageMath to search the FindStat combinatorial statistics database to identify apparent instances of the cyclic sieving phenomenon (CSP).","Cyclic sieving occurs on a set of objects, a statistic, and a map of order $n$ when the evaluation of the statistic generating function at the $d$th power of the primitive $n$th root of unity equals the number of fixed points under the $d$th power of the map.","Of the apparent instances found in our experiment, we prove 34 new instances of the CSP, and conjecture three more.","Furthermore, we prove the equidistribution of some statistics and show that some maps have the same orbit structure, thus cyclic sieving holds for more even more pairs of a map and a statistic.","The maps which exhibit the CSP include reverse/complement, rotation, Lehmer code rotation, toric promotion, and conjugation by the long cycle, as well as a map constructed by Corteel to swap the number of nestings and crossings, the invert Laguerre heap map, and a map of Alexandersson and Kebede designed to preserve right-to-left minima.","Our results show that, contrary to common expectations, actions that exhibit homomesy are not necessarily the best candidates for the CSP, and vice versa."],"url":"http://arxiv.org/abs/2402.16251v1","category":"math.CO"}
{"created":"2024-02-26 02:28:36","title":"A Proof of Weak Cosmic Censorship Conjecture for the Spherically Symmetric Einstein-Maxwell-Charged Scalar Field System","abstract":"Under spherical symmetry, we show that the weak cosmic censorship holds for the gravitational collapse of the Einstein-Maxwell-charged scalar field system. Namely, for this system, with generic initial data, the formed spacetime singularities are concealed inside black-hole regions. This generalizes Christodoulou's celebrated results to the charged case. Due to the presence of charge $Q$ and the complexification of the scalar field $\\phi$, multiple delicate features and miraculous monotonic properties of the Einstein-(real) scalar field system are not present. We develop a systematical approach to incorporate $Q$ and the complex-valued $\\phi$ into the integrated arguments. For instance, we discover a new path, employing the reduced mass ratio, to establish the sharp trapped surface formation criterion for the charged case. Due to the complex structure and the absence of translational symmetry of $\\phi$, we also carry out detailed modified scale-critical BV area estimates with renormalized quantities to deal with $Q$ and $\\phi$. We present a new $C^1$ extension criterion by utilizing the Doppler exponent to elucidate the blueshift effect, analogous to the role of integrating vorticity in the Beale-Kato-Madja breakdown criterion for incompressible fluids. Furthermore, by utilizing only double-null foliations, we establish the desired first and second instability theorems for the charged scenarios and identify generic initial conditions for the non-appearance of naked singularities. Our instability argument requires intricate generalizations of the treatment for the uncharged case via analyzing the precise contribution of the charged terms and its connection to the reduced mass ratio.","sentences":["Under spherical symmetry, we show that the weak cosmic censorship holds for the gravitational collapse of the Einstein-Maxwell-charged scalar field system.","Namely, for this system, with generic initial data, the formed spacetime singularities are concealed inside black-hole regions.","This generalizes Christodoulou's celebrated results to the charged case.","Due to the presence of charge $Q$ and the complexification of the scalar field $\\phi$, multiple delicate features and miraculous monotonic properties of the Einstein-(real) scalar field system are not present.","We develop a systematical approach to incorporate $Q$ and the complex-valued $\\phi$ into the integrated arguments.","For instance, we discover a new path, employing the reduced mass ratio, to establish the sharp trapped surface formation criterion for the charged case.","Due to the complex structure and the absence of translational symmetry of $\\phi$, we also carry out detailed modified scale-critical BV area estimates with renormalized quantities to deal with $Q$ and $\\phi$. We present a new $C^1$ extension criterion by utilizing the Doppler exponent to elucidate the blueshift effect, analogous to the role of integrating vorticity in the Beale-Kato-Madja breakdown criterion for incompressible fluids.","Furthermore, by utilizing only double-null foliations, we establish the desired first and second instability theorems for the charged scenarios and identify generic initial conditions for the non-appearance of naked singularities.","Our instability argument requires intricate generalizations of the treatment for the uncharged case via analyzing the precise contribution of the charged terms and its connection to the reduced mass ratio."],"url":"http://arxiv.org/abs/2402.16250v1","category":"gr-qc"}
{"created":"2024-02-26 02:14:42","title":"Topic-to-essay generation with knowledge-based content selection","abstract":"The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.","sentences":["The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words.","Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity.","In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder.","Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities.","In addition, we have contributed a new Chinese dataset for TEG tasks.","Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency."],"url":"http://arxiv.org/abs/2402.16248v1","category":"cs.CL"}
{"created":"2024-02-26 02:09:19","title":"Random Staircase Generator Matrix Codes","abstract":"In this paper, we propose a class of codes, referred to as random staircase generator matrix codes (SGMCs), which have staircase-like generator matrices. In the infinite-length region, we prove that the random SGMC is capacity-achieving over binary-input output-symmetric (BIOS) channels. In the finite-length region, we present the representative ordered statistics decoding with local constraints (LC-ROSD) algorithm for the SGMCs. The most distinguished feature of the SGMCs with LC-ROSD is that the staircase-like matrices enable parallel implementation of the Gaussian elimination (GE), avoiding the serial GE of conventional OSD and supporting a potential low decoding latency, as implied from simulations. To analyze the performance of random SGMCs in the finite-length region, we derive the ensemble weight spectrum and invoke the conventional union bound. We also derive a partially random coding union (RCU) bound, which is tighter than the conventional one and is used as a criterion to design the SGMCs. Staircase-like generator matrices allow us to derive a series of (tighter and tighter) lower bounds based on the second-order Bonferroni inequality with the incremental number of codewords. The numerical results show that the decoding performance can match well with the proposed partially RCU bound for different code rates and different profiles. The numerical results also show that the tailored SGMCs with the LC-ROSD algorithm can approach the finite-length performance bound, outperforming the 5G low-density parity-check (LDPC) codes, 5G polar codes, and Reed-Muller (RM) codes.","sentences":["In this paper, we propose a class of codes, referred to as random staircase generator matrix codes (SGMCs), which have staircase-like generator matrices.","In the infinite-length region, we prove that the random SGMC is capacity-achieving over binary-input output-symmetric (BIOS) channels.","In the finite-length region, we present the representative ordered statistics decoding with local constraints (LC-ROSD) algorithm for the SGMCs.","The most distinguished feature of the SGMCs with LC-ROSD is that the staircase-like matrices enable parallel implementation of the Gaussian elimination (GE), avoiding the serial GE of conventional OSD and supporting a potential low decoding latency, as implied from simulations.","To analyze the performance of random SGMCs in the finite-length region, we derive the ensemble weight spectrum and invoke the conventional union bound.","We also derive a partially random coding union (RCU) bound, which is tighter than the conventional one and is used as a criterion to design the SGMCs.","Staircase-like generator matrices allow us to derive a series of (tighter and tighter) lower bounds based on the second-order Bonferroni inequality with the incremental number of codewords.","The numerical results show that the decoding performance can match well with the proposed partially RCU bound for different code rates and different profiles.","The numerical results also show that the tailored SGMCs with the LC-ROSD algorithm can approach the finite-length performance bound, outperforming the 5G low-density parity-check (LDPC) codes, 5G polar codes, and Reed-Muller (RM) codes."],"url":"http://arxiv.org/abs/2402.16245v1","category":"cs.IT"}
{"created":"2024-02-26 02:09:00","title":"Two mass-imbalanced atoms in a hard-wall trap: Deep learning integrability of many-body systems","abstract":"The study of integrable systems has led to significant advancements in our understanding of many-body physics. We design a series of numerical experiments to analyze the integrability of a mass-imbalanced two-body system through energy level statistics and deep learning of wavefunctions. The level spacing distributions are fitted by a Brody distribution and the fitting parameter $\\omega$ is found to separate the integrable and non-integrable mass ratios by a critical line $\\omega=0$. The convolutional neural network built from the probability density images could identify the transition points between integrable and non-integrable systems with high accuracy, yet in a much shorter computation time. A brilliant example of the network's ability is to identify a new integrable mass ratio $1/3$ by learning from the known integrable case of equal mass, with a remarkable network confidence of $98.78\\%$. The robustness of our neural networks is further enhanced by adversarial learning, where samples are generated by standard and quantum perturbations mixed in the probability density images and the wavefunctions, respectively.","sentences":["The study of integrable systems has led to significant advancements in our understanding of many-body physics.","We design a series of numerical experiments to analyze the integrability of a mass-imbalanced two-body system through energy level statistics and deep learning of wavefunctions.","The level spacing distributions are fitted by a Brody distribution and the fitting parameter $\\omega$ is found to separate the integrable and non-integrable mass ratios by a critical line $\\omega=0$.","The convolutional neural network built from the probability density images could identify the transition points between integrable and non-integrable systems with high accuracy, yet in a much shorter computation time.","A brilliant example of the network's ability is to identify a new integrable mass ratio $1/3$ by learning from the known integrable case of equal mass, with a remarkable network confidence of $98.78\\%$. The robustness of our neural networks is further enhanced by adversarial learning, where samples are generated by standard and quantum perturbations mixed in the probability density images and the wavefunctions, respectively."],"url":"http://arxiv.org/abs/2402.16244v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-26 02:03:08","title":"HSONet:A Siamese foreground association-driven hard case sample optimization network for high-resolution remote sensing image change detection","abstract":"In the later training stages, further improvement of the models ability to determine changes relies on how well the change detection (CD) model learns hard cases; however, there are two additional challenges to learning hard case samples: (1) change labels are limited and tend to pointer only to foreground targets, yet hard case samples are prevalent in the background, which leads to optimizing the loss function focusing on the foreground targets and ignoring the background hard cases, which we call imbalance. (2) Complex situations, such as light shadows, target occlusion, and seasonal changes, induce hard case samples, and in the absence of both supervisory and scene information, it is difficult for the model to learn hard case samples directly to accurately obtain the feature representations of the change information, which we call missingness. We propose a Siamese foreground association-driven hard case sample optimization network (HSONet). To deal with this imbalance, we propose an equilibrium optimization loss function to regulate the optimization focus of the foreground and background, determine the hard case samples through the distribution of the loss values, and introduce dynamic weights in the loss term to gradually shift the optimization focus of the loss from the foreground to the background hard cases as the training progresses. To address this missingness, we understand hard case samples with the help of the scene context, propose the scene-foreground association module, use potential remote sensing spatial scene information to model the association between the target of interest in the foreground and the related context to obtain scene embedding, and apply this information to the feature reinforcement of hard cases. Experiments on four public datasets show that HSONet outperforms current state-of-the-art CD methods, particularly in detecting hard case samples.","sentences":["In the later training stages, further improvement of the models ability to determine changes relies on how well the change detection (CD) model learns hard cases; however, there are two additional challenges to learning hard case samples: (1) change labels are limited and tend to pointer only to foreground targets, yet hard case samples are prevalent in the background, which leads to optimizing the loss function focusing on the foreground targets and ignoring the background hard cases, which we call imbalance.","(2) Complex situations, such as light shadows, target occlusion, and seasonal changes, induce hard case samples, and in the absence of both supervisory and scene information, it is difficult for the model to learn hard case samples directly to accurately obtain the feature representations of the change information, which we call missingness.","We propose a Siamese foreground association-driven hard case sample optimization network (HSONet).","To deal with this imbalance, we propose an equilibrium optimization loss function to regulate the optimization focus of the foreground and background, determine the hard case samples through the distribution of the loss values, and introduce dynamic weights in the loss term to gradually shift the optimization focus of the loss from the foreground to the background hard cases as the training progresses.","To address this missingness, we understand hard case samples with the help of the scene context, propose the scene-foreground association module, use potential remote sensing spatial scene information to model the association between the target of interest in the foreground and the related context to obtain scene embedding, and apply this information to the feature reinforcement of hard cases.","Experiments on four public datasets show that HSONet outperforms current state-of-the-art CD methods, particularly in detecting hard case samples."],"url":"http://arxiv.org/abs/2402.16242v1","category":"cs.CV"}
{"created":"2024-02-26 01:46:56","title":"Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee","abstract":"A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold. A theoretical analysis for the convergence of the algorithm to an accurate solution is provided. On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods.","sentences":["A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold.","When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations.","Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets.","When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time.","While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence.","To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces.","Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold.","A theoretical analysis for the convergence of the algorithm to an accurate solution is provided.","On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.16237v1","category":"cs.LG"}
{"created":"2024-02-26 01:44:24","title":"Human-AI Co-Creation of Worked Examples for Programming Classes","abstract":"Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes. Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code. However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class. In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming. We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary.We also present a study that assesses the quality of explanations created with this approach","sentences":["Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes.","Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code.","However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class.","In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming.","We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary.","We also present a study that assesses the quality of explanations created with this approach"],"url":"http://arxiv.org/abs/2402.16235v1","category":"cs.HC"}
{"created":"2024-02-26 01:31:50","title":"Phase-Space Delaunay Tessellation Field Estimator","abstract":"The reconstruction of density and velocity fields is of central importance to the interpretation of $N$-body simulations. We propose a phase-space extension of the Delaunay tessellation field estimator (DTFE) that tracks the dark matter fluid in phase-space. The new reconstruction scheme removes several artifacts from the conventional DTFE in multi-stream regions, while preserving the adaptive resolution in high-density regions and yielding continuous fields. The estimator also removes tessellation artifacts of a previously proposed phase-space reconstruction scheme.","sentences":["The reconstruction of density and velocity fields is of central importance to the interpretation of $N$-body simulations.","We propose a phase-space extension of the Delaunay tessellation field estimator (DTFE) that tracks the dark matter fluid in phase-space.","The new reconstruction scheme removes several artifacts from the conventional DTFE in multi-stream regions, while preserving the adaptive resolution in high-density regions and yielding continuous fields.","The estimator also removes tessellation artifacts of a previously proposed phase-space reconstruction scheme."],"url":"http://arxiv.org/abs/2402.16234v1","category":"astro-ph.CO"}
{"created":"2024-02-26 01:18:53","title":"GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series","abstract":"Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life. The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS). However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients. In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets, representing diverse clinical scenarios. Upon comparison with twelve well-established baseline methods, GARNNs not only achieve the best prediction accuracy but also provide high-quality temporal interpretability, in particular for postprandial glucose levels as a result of corresponding meal intake and insulin injection. These findings underline the potential of GARNN as a robust tool for improving diabetes care, bridging the gap between deep learning technology and real-world healthcare solutions.","sentences":["Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life.","The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS).","However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients.","In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis.","We evaluate GARNNs on four datasets, representing diverse clinical scenarios.","Upon comparison with twelve well-established baseline methods, GARNNs not only achieve the best prediction accuracy but also provide high-quality temporal interpretability, in particular for postprandial glucose levels as a result of corresponding meal intake and insulin injection.","These findings underline the potential of GARNN as a robust tool for improving diabetes care, bridging the gap between deep learning technology and real-world healthcare solutions."],"url":"http://arxiv.org/abs/2402.16230v1","category":"cs.LG"}
{"created":"2024-02-25 23:49:05","title":"Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging","abstract":"This research presents a machine-learning approach for tumor detection in medical images using convolutional neural networks (CNNs). The study focuses on preprocessing techniques to enhance image features relevant to tumor detection, followed by developing and training a CNN model for accurate classification. Various image processing techniques, including Gaussian smoothing, bilateral filtering, and K-means clustering, are employed to preprocess the input images and highlight tumor regions. The CNN model is trained and evaluated on a dataset of medical images, with augmentation and data generators utilized to enhance model generalization. Experimental results demonstrate the effectiveness of the proposed approach in accurately detecting tumors in medical images, paving the way for improved diagnostic tools in healthcare.","sentences":["This research presents a machine-learning approach for tumor detection in medical images using convolutional neural networks (CNNs).","The study focuses on preprocessing techniques to enhance image features relevant to tumor detection, followed by developing and training a CNN model for accurate classification.","Various image processing techniques, including Gaussian smoothing, bilateral filtering, and K-means clustering, are employed to preprocess the input images and highlight tumor regions.","The CNN model is trained and evaluated on a dataset of medical images, with augmentation and data generators utilized to enhance model generalization.","Experimental results demonstrate the effectiveness of the proposed approach in accurately detecting tumors in medical images, paving the way for improved diagnostic tools in healthcare."],"url":"http://arxiv.org/abs/2402.16221v1","category":"eess.IV"}
{"created":"2024-02-25 23:35:36","title":"Universal quantum operations and ancilla-based readout for tweezer clocks","abstract":"Enhancing the precision of measurements by harnessing entanglement is a long-sought goal in the field of quantum metrology. Yet attaining the best sensitivity allowed by quantum theory in the presence of noise is an outstanding challenge, requiring optimal probe-state generation and readout strategies. Neutral atom optical clocks, leading systems for measuring time, have shown recent progress in terms of entanglement generation, but currently lack the control capabilities to realize such schemes. Here we show universal quantum operations and ancilla-based readout for ultranarrow optical transitions of neutral atoms. Our demonstration in a tweezer clock platform enables a circuit-based approach to quantum metrology with neutral atom optical clocks. To this end, we demonstrate two-qubit entangling gates with >99.35% fidelity via Rydberg interactions and dynamical connectivity for optical clock qubits, which we combine with local addressing to implement universally programmable quantum circuits. Using this approach, we generate a near-optimal entangled probe state, a cascade of Greenberger-Horne-Zeilinger (GHZ) states of different sizes, and perform dual-quadrature GHZ readout. We also show repeated fast phase detection with non-destructive conditional reset of clock qubits and minimal dead time between repetitions by implementing ancilla-based quantum logic spectroscopy (QLS) for neutral atoms. Finally, we extend this to multi-qubit parity checks and measurement-based Bell state preparation. Our work lays the foundation for hybrid processor-clock devices with neutral atoms and more generally points to a future of practical applications for quantum processors linked with quantum sensors.","sentences":["Enhancing the precision of measurements by harnessing entanglement is a long-sought goal in the field of quantum metrology.","Yet attaining the best sensitivity allowed by quantum theory in the presence of noise is an outstanding challenge, requiring optimal probe-state generation and readout strategies.","Neutral atom optical clocks, leading systems for measuring time, have shown recent progress in terms of entanglement generation, but currently lack the control capabilities to realize such schemes.","Here we show universal quantum operations and ancilla-based readout for ultranarrow optical transitions of neutral atoms.","Our demonstration in a tweezer clock platform enables a circuit-based approach to quantum metrology with neutral atom optical clocks.","To this end, we demonstrate two-qubit entangling gates with >99.35% fidelity via Rydberg interactions and dynamical connectivity for optical clock qubits, which we combine with local addressing to implement universally programmable quantum circuits.","Using this approach, we generate a near-optimal entangled probe state, a cascade of Greenberger-Horne-Zeilinger (GHZ) states of different sizes, and perform dual-quadrature GHZ readout.","We also show repeated fast phase detection with non-destructive conditional reset of clock qubits and minimal dead time between repetitions by implementing ancilla-based quantum logic spectroscopy (QLS) for neutral atoms.","Finally, we extend this to multi-qubit parity checks and measurement-based Bell state preparation.","Our work lays the foundation for hybrid processor-clock devices with neutral atoms and more generally points to a future of practical applications for quantum processors linked with quantum sensors."],"url":"http://arxiv.org/abs/2402.16220v1","category":"quant-ph"}
{"created":"2024-02-25 22:57:43","title":"Sparse-Stochastic Fragmented Exchange for Large-Scale Hybrid TDDFT Calculations","abstract":"We extend our recently developed sparse-stochastic fragmented exchange formalism for ground-state hybrid DFT (ngH-DFT) to calculate absorption spectra within linear-response time-dependent Generalized Kohn-Sham DFT (LR-GKS-TDDFT), for systems consisting of thousands of valence electrons within a grid-based/plane-wave representation. A mixed deterministic/fragmented-stochastic compression of the exchange kernel, here using long-range explicit exchange functionals, provides an efficient method for accurate optical spectra. Both real-time propagation as well frequency-resolved Casida-equation-type approaches for spectra are presented, and the method is applied to large molecular dyes.","sentences":["We extend our recently developed sparse-stochastic fragmented exchange formalism for ground-state hybrid DFT (ngH-DFT) to calculate absorption spectra within linear-response time-dependent Generalized Kohn-Sham DFT (LR-GKS-TDDFT), for systems consisting of thousands of valence electrons within a grid-based/plane-wave representation.","A mixed deterministic/fragmented-stochastic compression of the exchange kernel, here using long-range explicit exchange functionals, provides an efficient method for accurate optical spectra.","Both real-time propagation as well frequency-resolved Casida-equation-type approaches for spectra are presented, and the method is applied to large molecular dyes."],"url":"http://arxiv.org/abs/2402.16217v1","category":"physics.chem-ph"}
{"created":"2024-02-25 22:38:47","title":"Products and powers of principal symmetric ideals","abstract":"Principal symmetric ideals were recently introduced by Harada, Seceleanu, and Sega, with a focus on their homological properties. They are ideals generated by the orbit of a single polynomial under permutations of variables in a polynomial ring. In this paper we seek to determine when a product of two principal symmetric ideals is principal symmetric and when all the powers of a principal symmetric ideal are again principal symmetric ideals. We characterize the ideals that have the latter property as being generated by polynomials invariant up to a scalar multiple under permutation of variables. Recognizing principal symmetric ideals is an open question for the purpose of which we produce certain obstructions. We also demonstrate that the Hilbert functions of symmetric monomial ideals are not all given by symmetric monomial ideals, in contrast to the non-symmetric case.","sentences":["Principal symmetric ideals were recently introduced by Harada, Seceleanu, and Sega, with a focus on their homological properties.","They are ideals generated by the orbit of a single polynomial under permutations of variables in a polynomial ring.","In this paper we seek to determine when a product of two principal symmetric ideals is principal symmetric and when all the powers of a principal symmetric ideal are again principal symmetric ideals.","We characterize the ideals that have the latter property as being generated by polynomials invariant up to a scalar multiple under permutation of variables.","Recognizing principal symmetric ideals is an open question for the purpose of which we produce certain obstructions.","We also demonstrate that the Hilbert functions of symmetric monomial ideals are not all given by symmetric monomial ideals, in contrast to the non-symmetric case."],"url":"http://arxiv.org/abs/2402.16214v1","category":"math.AC"}
{"created":"2024-02-25 22:32:01","title":"Sparse gradient bounds for divergence form elliptic equations","abstract":"We provide sparse estimates for gradients of solutions to divergence form elliptic partial differential equations in terms of the source data. We give a general result of Meyers (or Gehring) type, a result for linear equations with VMO coefficients and a result for linear equations with Dini continuous coefficients. In addition, we provide an abstract theorem conditional on PDE estimates available. The linear results have the full range of weighted estimates with Muckenhoupt weights as a consequence.","sentences":["We provide sparse estimates for gradients of solutions to divergence form elliptic partial differential equations in terms of the source data.","We give a general result of Meyers (or Gehring) type, a result for linear equations with VMO coefficients and a result for linear equations with Dini continuous coefficients.","In addition, we provide an abstract theorem conditional on PDE estimates available.","The linear results have the full range of weighted estimates with Muckenhoupt weights as a consequence."],"url":"http://arxiv.org/abs/2402.16213v1","category":"math.AP"}
{"created":"2024-02-25 22:23:37","title":"HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs","abstract":"Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs' hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models' performance ranged between 3% and 11%, and evaluator agents demonstrated a 6% error rate in hallucination prediction. The proposed framework provides opportunities to test and improve LLMs. Additionally, it has the potential to generate benchmarking datasets tailored to specific domains, such as law, health, and finance.","sentences":["Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications.","Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs.","The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations.","This paper introduces an automated scalable framework that combines benchmarking LLMs' hallucination tendencies with efficient hallucination detection.","We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection.","The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain.","We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models' performance ranged between 3% and 11%, and evaluator agents demonstrated a 6% error rate in hallucination prediction.","The proposed framework provides opportunities to test and improve LLMs.","Additionally, it has the potential to generate benchmarking datasets tailored to specific domains, such as law, health, and finance."],"url":"http://arxiv.org/abs/2402.16211v1","category":"cs.CL"}
{"created":"2024-02-25 21:59:53","title":"Exact calculation of degrees for lattice equations: a singularity approach","abstract":"The theory of degree growth and algebraic entropy plays a crucial role in the field of discrete integrable systems. However, a general method for calculating degree growth for lattice equations (partial difference equations) is not yet known. Here we propose a new method to rigorously compute the exact degree of each iterate for lattice equations. Our strategy is to extend Halburd's method, which is a novel approach to computing the exact degree of each iterate for mappings (ordinary difference equations) from the singularity structure, to lattice equations. First, we illustrate, without rigorous discussion, how to calculate degrees for lattice equations using the lattice version of Halburd's method and discuss what problems we need to solve to make the method rigorous. We then provide a framework to ensure that all calculations are accurate and rigorous. We also discuss how to detect the singularity structure of a lattice equation. Our method is not only accurate and rigorous but also can easily be applied to a wide range of lattice equations.","sentences":["The theory of degree growth and algebraic entropy plays a crucial role in the field of discrete integrable systems.","However, a general method for calculating degree growth for lattice equations (partial difference equations) is not yet known.","Here we propose a new method to rigorously compute the exact degree of each iterate for lattice equations.","Our strategy is to extend Halburd's method, which is a novel approach to computing the exact degree of each iterate for mappings (ordinary difference equations) from the singularity structure, to lattice equations.","First, we illustrate, without rigorous discussion, how to calculate degrees for lattice equations using the lattice version of Halburd's method and discuss what problems we need to solve to make the method rigorous.","We then provide a framework to ensure that all calculations are accurate and rigorous.","We also discuss how to detect the singularity structure of a lattice equation.","Our method is not only accurate and rigorous but also can easily be applied to a wide range of lattice equations."],"url":"http://arxiv.org/abs/2402.16206v1","category":"nlin.SI"}
{"created":"2024-02-25 21:42:57","title":"Effective Phonon Dispersion and Low field transport in AlxGa1-xN alloys using supercells: An ab-initio approach","abstract":"To investigate the transport properties in random alloys, it is important to model the alloy disorder using supercells. Though traditional methods like Virtual Crystal Approximation (VCA) are computationally efficient, the local disorder in the system is not accurately captured as artificial translational symmetry is imposed on the system. However, in the case of supercells, the error introduced by self-image interaction between the impurities is reduced and translational symmetry is explicitly imposed over larger length scales. In this work, we have investigated the Effective Phonon Dispersion (EPD) and transport properties, from first principle calculations using supercells in AlxGa1-xN alloy systems. Using our in-house developed code, the EPD of AlGaN is obtained and the individual modes are identified. Next, we discuss our in-house developed method to calculate low-field transport properties in supercells. First to validate our methods we have solved the Boltzmann Transport Equation using Rode method to compare the phonon limited mobility in the 4 atom GaN primitive cell and 12 atom GaN supercell. Using the same technique, we have investigated the low field transport in random AlxGa1-xN alloy systems. Our calculations show that along with alloy scattering, electron-phonon scattering may also play an important role at room temperature and high-temperature device operation. This technique opens up the path for calculating phonon-limited transport properties in random alloy systems.","sentences":["To investigate the transport properties in random alloys, it is important to model the alloy disorder using supercells.","Though traditional methods like Virtual Crystal Approximation (VCA) are computationally efficient, the local disorder in the system is not accurately captured as artificial translational symmetry is imposed on the system.","However, in the case of supercells, the error introduced by self-image interaction between the impurities is reduced and translational symmetry is explicitly imposed over larger length scales.","In this work, we have investigated the Effective Phonon Dispersion (EPD) and transport properties, from first principle calculations using supercells in AlxGa1-xN alloy systems.","Using our in-house developed code, the EPD of AlGaN is obtained and the individual modes are identified.","Next, we discuss our in-house developed method to calculate low-field transport properties in supercells.","First to validate our methods we have solved the Boltzmann Transport Equation using Rode method to compare the phonon limited mobility in the 4 atom GaN primitive cell and 12 atom GaN supercell.","Using the same technique, we have investigated the low field transport in random AlxGa1-xN alloy systems.","Our calculations show that along with alloy scattering, electron-phonon scattering may also play an important role at room temperature and high-temperature device operation.","This technique opens up the path for calculating phonon-limited transport properties in random alloy systems."],"url":"http://arxiv.org/abs/2402.16203v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-25 21:35:00","title":"Probing the physics of star formation (ProPStar): I. First resolved maps of the electron fraction and cosmic-ray ionization rate in NGC 1333","abstract":"Electron fraction and cosmic-ray ionization rates (CRIR) in star-forming regions are important quantities in astrochemical modeling and are critical to the degree of coupling between neutrals, ions, and electrons, which regulates the dynamics of the magnetic field. However, these are difficult quantities to estimate. We aim to derive the electron fraction and CRIR maps of an active star-forming region. We combined observations of the nearby NGC 1333 star-forming region carried out with the NOEMA interferometer and IRAM 30-m single dish to generate high spatial dynamic range maps of different molecular transitions. We used the DCO$^+$ and H$^{13}$CO$^+$ ratio (in addition to complementary data) to estimate the electron fraction and produce cosmic-ray ionization rate maps. We derived the first large-area electron fraction and CRIR resolved maps in a star-forming region, with typical values of $10^{-6.5}$ and $10^{-16.5}$ s$^{-1}$, respectively. The maps present clear evidence of enhanced values around embedded young stellar objects (YSOs). This provides strong evidence for locally accelerated cosmic rays. We also found a strong enhancement toward the northwest region in the map that might be related either to an interaction with a bubble or to locally generated cosmic rays by YSOs. We used the typical electron fraction and derived a MHD turbulence dissipation scale of 0.054 pc, which could be tested with future observations. We found a higher cosmic-ray ionization rate compared to the canonical value for $N({\\rm H_2})=10^{21}-10^{23}$ cm$^{-2}$ of $10^{-17}$ s$^{-1}$ in the region, and it is likely generated by the accreting YSOs. The high value of the electron fraction suggests that new disks will form from gas in the ideal-MHD limit. This indicates that local enhancements of $\\zeta({\\rm H_2})$, due to YSOs, should be taken into account in the analysis of clustered star formation.","sentences":["Electron fraction and cosmic-ray ionization rates (CRIR) in star-forming regions are important quantities in astrochemical modeling and are critical to the degree of coupling between neutrals, ions, and electrons, which regulates the dynamics of the magnetic field.","However, these are difficult quantities to estimate.","We aim to derive the electron fraction and CRIR maps of an active star-forming region.","We combined observations of the nearby NGC 1333 star-forming region carried out with the NOEMA interferometer and IRAM 30-m single dish to generate high spatial dynamic range maps of different molecular transitions.","We used the DCO$^+$ and H$^{13}$CO$^+$ ratio (in addition to complementary data) to estimate the electron fraction and produce cosmic-ray ionization rate maps.","We derived the first large-area electron fraction and CRIR resolved maps in a star-forming region, with typical values of $10^{-6.5}$ and $10^{-16.5}$ s$^{-1}$, respectively.","The maps present clear evidence of enhanced values around embedded young stellar objects (YSOs).","This provides strong evidence for locally accelerated cosmic rays.","We also found a strong enhancement toward the northwest region in the map that might be related either to an interaction with a bubble or to locally generated cosmic rays by YSOs.","We used the typical electron fraction and derived a MHD turbulence dissipation scale of 0.054 pc, which could be tested with future observations.","We found a higher cosmic-ray ionization rate compared to the canonical value for $N({\\rm H_2})=10^{21}-10^{23}$ cm$^{-2}$ of $10^{-17}$ s$^{-1}$ in the region, and it is likely generated by the accreting YSOs.","The high value of the electron fraction suggests that new disks will form from gas in the ideal-MHD limit.","This indicates that local enhancements of $\\zeta({\\rm H_2})$, due to YSOs, should be taken into account in the analysis of clustered star formation."],"url":"http://arxiv.org/abs/2402.16202v1","category":"astro-ph.GA"}
{"created":"2024-02-25 21:25:06","title":"IR2: Information Regularization for Information Retrieval","abstract":"Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regularization is applied. This provides a systematic approach for optimizing synthetic data generation in data-limited, complex-query IR scenarios. All code, prompts and synthetic data are available at https://github.com/Info-Regularization/Information-Regularization.","sentences":["Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task.","This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation.","This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.","Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%.","Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regularization is applied.","This provides a systematic approach for optimizing synthetic data generation in data-limited, complex-query IR scenarios.","All code, prompts and synthetic data are available at https://github.com/Info-Regularization/Information-Regularization."],"url":"http://arxiv.org/abs/2402.16200v1","category":"cs.IR"}
{"created":"2024-02-25 20:53:25","title":"Stable Graded Multiplicities for Harmonics on a Cyclic Quiver","abstract":"We consider Vinberg $\\theta$-groups associated to a cyclic quiver on $k$ nodes. Let $K$ be the product of the general linear groups associated to each node. Then $K$ acts naturally on $\\oplus \\text{Hom}(V_i, V_{i+1})$ and by Vinberg's theory the polynomials are free over the invariants. We therefore consider the harmonics as a representation of $K$, and give a combinatorial formula for the stable graded multiplicity of each $K$-type. A key lemma provides a combinatorial separation of variables that allows us to cancel the invariants and obtain generalized exponents for the harmonics.","sentences":["We consider Vinberg $\\theta$-groups associated to a cyclic quiver on $k$ nodes.","Let $K$ be the product of the general linear groups associated to each node.","Then $K$ acts naturally on $\\oplus \\text{Hom}(V_i, V_{i+1})$ and by Vinberg's theory the polynomials are free over the invariants.","We therefore consider the harmonics as a representation of $K$, and give a combinatorial formula for the stable graded multiplicity of each $K$-type.","A key lemma provides a combinatorial separation of variables that allows us to cancel the invariants and obtain generalized exponents for the harmonics."],"url":"http://arxiv.org/abs/2402.16198v1","category":"math.RT"}
{"created":"2024-02-25 20:36:51","title":"ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling","abstract":"Effective feature representations play a critical role in enhancing the performance of text generation models that rely on deep neural networks. However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated text. In this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user's utterance while simultaneously enhancing performance. We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of sentiment analysis for open-domain chatbots, enabling the generation of empathetic responses that are fluent and relevant. In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely zeroes in on sentiment and emotion nuances within the user's utterance. This ensures the generation of context-rich representations tailored to the underlying emotional tone and sentiment intricacies of the text. Our approach outperforms existing methods for generating empathetic embeddings, providing empathetic and diverse responses. The performance of our proposed model significantly exceeds that of existing models, enhancing emotion detection accuracy by 6.2% and lexical diversity by 1.4%.","sentences":["Effective feature representations play a critical role in enhancing the performance of text generation models that rely on deep neural networks.","However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated text.","In this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user's utterance while simultaneously enhancing performance.","We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of sentiment analysis for open-domain chatbots, enabling the generation of empathetic responses that are fluent and relevant.","In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely zeroes in on sentiment and emotion nuances within the user's utterance.","This ensures the generation of context-rich representations tailored to the underlying emotional tone and sentiment intricacies of the text.","Our approach outperforms existing methods for generating empathetic embeddings, providing empathetic and diverse responses.","The performance of our proposed model significantly exceeds that of existing models, enhancing emotion detection accuracy by 6.2% and lexical diversity by 1.4%."],"url":"http://arxiv.org/abs/2402.16194v1","category":"cs.CL"}
{"created":"2024-02-25 20:36:03","title":"Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing","abstract":"Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.","sentences":["Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content.","While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance.","To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt.","Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval.","The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth."],"url":"http://arxiv.org/abs/2402.16192v1","category":"cs.CL"}
{"created":"2024-02-25 20:30:05","title":"One-stage Prompt-based Continual Learning","abstract":"Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop < 1%. We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power. The QR loss is only applied during training time, so there is no computational overhead at inference from the QR loss. With the QR loss, our approach maintains ~ 50% computational cost reduction during inference as well as outperforms the prior two-stage PCL methods by ~1.4% on public class-incremental continual learning benchmarks including CIFAR-100, ImageNet-R, and DomainNet.","sentences":["Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues.","Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens.","To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query.","This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop < 1%.","We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power.","The QR loss is only applied during training time, so there is no computational overhead at inference from the QR loss.","With the QR loss, our approach maintains ~ 50% computational cost reduction during inference as well as outperforms the prior two-stage PCL methods by ~1.4% on public class-incremental continual learning benchmarks including CIFAR-100, ImageNet-R, and DomainNet."],"url":"http://arxiv.org/abs/2402.16189v1","category":"cs.CV"}
{"created":"2024-02-25 20:24:07","title":"Attacking LLM Watermarks by Exploiting Their Strengths","abstract":"Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.","sentences":["Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications.","Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content.","However, existing watermarking schemes remain surprisingly susceptible to attack.","In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks.","We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks."],"url":"http://arxiv.org/abs/2402.16187v1","category":"cs.CR"}
{"created":"2024-02-25 20:22:09","title":"An Execution-time-certified Riccati-based IPM Algorithm for RTI-based Input-constrained NMPC","abstract":"Establishing an execution time certificate in deploying model predictive control (MPC) is a pressing and challenging requirement. As nonlinear MPC (NMPC) results in nonlinear programs, differing from quadratic programs encountered in linear MPC, deriving an execution time certificate for NMPC seems an impossible task. Our prior work \\cite{wu2023direct} introduced an input-constrained MPC algorithm with the exact and only \\textit{dimension-dependent} (\\textit{data-independent}) number of floating-point operations ([flops]). This paper extends it to input-constrained NMPC problems via the real-time iteration (RTI) scheme, which results in \\textit{data-varying} (but \\textit{dimension-invariant}) input-constrained MPC problems. Therefore, applying our previous algorithm can certify the execution time based on the assumption that processors perform fixed [flops] in constant time. As the RTI-based scheme generally results in MPC with a long prediction horizon, this paper employs the efficient factorized Riccati recursion, whose computational cost scales linearly with the prediction horizon, to solve the Newton system at each iteration. The execution-time certified capability of the algorithm is theoretically and numerically validated through a case study involving nonlinear control of the chaotic Lorenz system.","sentences":["Establishing an execution time certificate in deploying model predictive control (MPC) is a pressing and challenging requirement.","As nonlinear MPC (NMPC) results in nonlinear programs, differing from quadratic programs encountered in linear MPC, deriving an execution time certificate for NMPC seems an impossible task.","Our prior work \\cite{wu2023direct} introduced an input-constrained MPC algorithm with the exact and only \\textit{dimension-dependent} (\\textit{data-independent}) number of floating-point operations ([flops]).","This paper extends it to input-constrained NMPC problems via the real-time iteration (RTI) scheme, which results in \\textit{data-varying} (but \\textit{dimension-invariant}) input-constrained MPC problems.","Therefore, applying our previous algorithm can certify the execution time based on the assumption that processors perform fixed [flops] in constant time.","As the RTI-based scheme generally results in MPC with a long prediction horizon, this paper employs the efficient factorized Riccati recursion, whose computational cost scales linearly with the prediction horizon, to solve the Newton system at each iteration.","The execution-time certified capability of the algorithm is theoretically and numerically validated through a case study involving nonlinear control of the chaotic Lorenz system."],"url":"http://arxiv.org/abs/2402.16186v1","category":"eess.SY"}
{"created":"2024-02-25 20:11:08","title":"Veneziano and Shapiro-Virasoro amplitudes of arbitrarily excited strings","abstract":"We extend the Veneziano and Shapiro-Virasoro amplitudes to four arbitrarily excited states in bosonic string theory. We use the formalism of coherent string states based on the Di Vecchia-Del Giudice-Fubini construction. Within the same formalism, we also analyze the three string scattering finding the covariant version of the three reggeon interaction. Then studying the factorization properties of the extended four string scattering amplitudes we identify the covariant version of the three string interaction. Finally we obtain generalized Kawai-Lewellen-Tye relations connecting the scattering of four open and closed arbitrarily excited states.","sentences":["We extend the Veneziano and Shapiro-Virasoro amplitudes to four arbitrarily excited states in bosonic string theory.","We use the formalism of coherent string states based on the Di Vecchia-Del Giudice-Fubini construction.","Within the same formalism, we also analyze the three string scattering finding the covariant version of the three reggeon interaction.","Then studying the factorization properties of the extended four string scattering amplitudes we identify the covariant version of the three string interaction.","Finally we obtain generalized Kawai-Lewellen-Tye relations connecting the scattering of four open and closed arbitrarily excited states."],"url":"http://arxiv.org/abs/2402.16183v1","category":"hep-th"}
{"created":"2024-02-25 20:07:13","title":"How Can LLM Guide RL? A Value-Based Approach","abstract":"Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the LLM-informed policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration. Additionally, we present a practical algorithm SLINVIT that simplifies the construction of the value function and employs subgoals to reduce the search complexity. Our experiments across three interactive environments ALFWorld, InterCode, and BlocksWorld demonstrate that our method achieves state-of-the-art success rates and also surpasses previous RL and LLM approaches in terms of sample efficiency. Our code is available at https://github.com/agentification/Language-Integrated-VI.","sentences":["Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback.","However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement.","On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback.","Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms.","Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the LLM-informed policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration.","Additionally, we present a practical algorithm SLINVIT that simplifies the construction of the value function and employs subgoals to reduce the search complexity.","Our experiments across three interactive environments ALFWorld, InterCode, and BlocksWorld demonstrate that our method achieves state-of-the-art success rates and also surpasses previous RL and LLM approaches in terms of sample efficiency.","Our code is available at https://github.com/agentification/Language-Integrated-VI."],"url":"http://arxiv.org/abs/2402.16181v1","category":"cs.LG"}
{"created":"2024-02-25 19:13:47","title":"Table-top soft x-ray source for XAS experiments with photon energies up to 350 eV","abstract":"We present a table-top setup for femtosecond x-ray absorption spectroscopy based on high harmonic generation (HHG) in noble gases. Using sub-millijoule pump pulses at a central wavelength of 1550 nm broadband HHG in the range 70 to 350 eV was demonstrated. The HHG coherence lengths of several millimeters were achieved by reaching the nonadiabatic regime of harmonic generation. NEXAFS experiments on the boron K edge of a boron foil and a hexagonal BN (hBN) 2D material demonstrate the capabilities of the setup.","sentences":["We present a table-top setup for femtosecond x-ray absorption spectroscopy based on high harmonic generation (HHG) in noble gases.","Using sub-millijoule pump pulses at a central wavelength of 1550 nm broadband HHG in the range 70 to 350 eV was demonstrated.","The HHG coherence lengths of several millimeters were achieved by reaching the nonadiabatic regime of harmonic generation.","NEXAFS experiments on the boron K edge of a boron foil and a hexagonal BN (hBN) 2D material demonstrate the capabilities of the setup."],"url":"http://arxiv.org/abs/2402.16176v1","category":"physics.optics"}
{"created":"2024-02-25 19:05:10","title":"XAI-based gait analysis of patients walking with Knee-Ankle-Foot orthosis using video cameras","abstract":"Recent technological advancements in artificial intelligence and computer vision have enabled gait analysis on portable devices such as cell phones. However, most state-of-the-art vision-based systems still impose numerous constraints for capturing a patient's video, such as using a static camera and maintaining a specific distance from it. While these constraints are manageable under professional observation, they pose challenges in home settings. Another issue with most vision-based systems is their output, typically a classification label and confidence value, whose reliability is often questioned by medical professionals. This paper addresses these challenges by presenting a novel system for gait analysis robust to camera movements and providing explanations for its output. The study utilizes a dataset comprising videos of subjects wearing two types of Knee Ankle Foot Orthosis (KAFO), namely \"Locked Knee\" and \"Semi-flexion,\" for mobility, along with metadata and ground truth for explanations. The ground truth highlights the statistical significance of seven features captured using motion capture systems to differentiate between the two gaits. To address camera movement challenges, the proposed system employs super-resolution and pose estimation during pre-processing. It then identifies the seven features - Stride Length, Step Length and Duration of single support of orthotic and non-orthotic leg, Cadence, and Speed - using the skeletal output of pose estimation. These features train a multi-layer perceptron, with its output explained by highlighting the features' contribution to classification. While most state-of-the-art systems struggle with processing the video or training on the proposed dataset, our system achieves an average accuracy of 94%. The model's explainability is validated using ground truth and can be considered reliable.","sentences":["Recent technological advancements in artificial intelligence and computer vision have enabled gait analysis on portable devices such as cell phones.","However, most state-of-the-art vision-based systems still impose numerous constraints for capturing a patient's video, such as using a static camera and maintaining a specific distance from it.","While these constraints are manageable under professional observation, they pose challenges in home settings.","Another issue with most vision-based systems is their output, typically a classification label and confidence value, whose reliability is often questioned by medical professionals.","This paper addresses these challenges by presenting a novel system for gait analysis robust to camera movements and providing explanations for its output.","The study utilizes a dataset comprising videos of subjects wearing two types of Knee Ankle Foot Orthosis (KAFO), namely \"Locked Knee\" and \"Semi-flexion,\" for mobility, along with metadata and ground truth for explanations.","The ground truth highlights the statistical significance of seven features captured using motion capture systems to differentiate between the two gaits.","To address camera movement challenges, the proposed system employs super-resolution and pose estimation during pre-processing.","It then identifies the seven features - Stride Length, Step Length and Duration of single support of orthotic and non-orthotic leg, Cadence, and Speed - using the skeletal output of pose estimation.","These features train a multi-layer perceptron, with its output explained by highlighting the features' contribution to classification.","While most state-of-the-art systems struggle with processing the video or training on the proposed dataset, our system achieves an average accuracy of 94%.","The model's explainability is validated using ground truth and can be considered reliable."],"url":"http://arxiv.org/abs/2402.16175v1","category":"cs.CV"}
{"created":"2024-02-25 18:59:29","title":"GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction","abstract":"While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions.","sentences":["While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive.","Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction.","However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations.","These constraints limit their cross-dataset generalizability.","To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy.","Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space.","It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training.","To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations.","We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy.","Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions."],"url":"http://arxiv.org/abs/2402.16174v1","category":"cs.CV"}
{"created":"2024-02-25 18:58:09","title":"Communication Traffic Characteristics Reveal an IoT Devices Identity","abstract":"Internet of Things (IoT) is one of the technological advancements of the twenty-first century which can improve living standards. However, it also imposes new types of security challenges, including device authentication, traffic types classification, and malicious traffic identification, in the network domain. Traditionally, internet protocol (IP) and media access control (MAC) addresses are utilized for identifying network-connected devices in a network, whilst these addressing schemes are prone to be compromised, including spoofing attacks and MAC randomization. Therefore, device identification using only explicit identifiers is a challenging task. Accurate device identification plays a key role in securing a network. In this paper, a supervised machine learning-based device fingerprinting (DFP) model has been proposed for identifying network-connected IoT devices using only communication traffic characteristics (or implicit identifiers). A single transmission control protocol/internet protocol (TCP/IP) packet header features have been utilized for generating unique fingerprints, with the fingerprints represented as a vector of 22 features. Experimental results have shown that the proposed DFP method achieves over 98% in classifying individual IoT devices using the UNSW dataset with 22 smart-home IoT devices. This signifies that the proposed approach is invaluable to network operators in making their networks more secure.","sentences":["Internet of Things (IoT) is one of the technological advancements of the twenty-first century which can improve living standards.","However, it also imposes new types of security challenges, including device authentication, traffic types classification, and malicious traffic identification, in the network domain.","Traditionally, internet protocol (IP) and media access control (MAC) addresses are utilized for identifying network-connected devices in a network, whilst these addressing schemes are prone to be compromised, including spoofing attacks and MAC randomization.","Therefore, device identification using only explicit identifiers is a challenging task.","Accurate device identification plays a key role in securing a network.","In this paper, a supervised machine learning-based device fingerprinting (DFP) model has been proposed for identifying network-connected IoT devices using only communication traffic characteristics (or implicit identifiers).","A single transmission control protocol/internet protocol (TCP/IP) packet header features have been utilized for generating unique fingerprints, with the fingerprints represented as a vector of 22 features.","Experimental results have shown that the proposed DFP method achieves over 98% in classifying individual IoT devices using the UNSW dataset with 22 smart-home IoT devices.","This signifies that the proposed approach is invaluable to network operators in making their networks more secure."],"url":"http://arxiv.org/abs/2402.16173v1","category":"cs.NI"}
{"created":"2024-02-25 18:52:34","title":"Gravito-magnetic Polarization of Schwarzschild Black Hole","abstract":"We determine the gravito-magnetic Love numbers of non-rotating black holes in all spacetime dimensions through a novel and direct derivation. The Ishibashi- Kodama master field and its associated field equation are avoided. The matching to the EFT variables is simple. This method allows us to correct the values in the literature. Moreover, we highlight a parity-based selection rule for nonlinear terms that include both electric-type and magnetic-type gravitational field tensors, enabling us to conclude that many of the nonlinear response coefficients in the Schwarzschild black hole effective action vanish.","sentences":["We determine the gravito-magnetic Love numbers of non-rotating black holes in all spacetime dimensions through a novel and direct derivation.","The Ishibashi- Kodama master field and its associated field equation are avoided.","The matching to the EFT variables is simple.","This method allows us to correct the values in the literature.","Moreover, we highlight a parity-based selection rule for nonlinear terms that include both electric-type and magnetic-type gravitational field tensors, enabling us to conclude that many of the nonlinear response coefficients in the Schwarzschild black hole effective action vanish."],"url":"http://arxiv.org/abs/2402.16172v1","category":"hep-th"}
{"created":"2024-02-25 18:42:26","title":"How to avoid the commuting conversions of IPC","abstract":"Since the observation in 2006 that it is possible to embed IPC into the atomic polymorphic lambda-calculus (a predicative fragment of system F with universal instantiations restricted to atomic formulas) different such embeddings appeared in the literature. All of them comprise the Russell-Prawitz translation of formulas, but have different strategies for the translation of proofs. Although these embeddings preserve proof identity, all fail in delivering preservation of reduction steps. In fact, they translate the commuting conversions of IPC to beta-equality, or to other kinds of reduction or equality generated by new principles added to system F. The cause for this is the generation of redexes by the translation itself. In this paper, we present an embedding of IPC into atomic system F, still based on the same translation of formulas, but which maps commuting conversions to syntactic identity, while simulating the other kinds of reduction steps present in IPC beta\\eta-reduction. In this sense the translation achieves a truly commuting conversion-free image of IPC in atomic system F.","sentences":["Since the observation in 2006 that it is possible to embed IPC into the atomic polymorphic lambda-calculus (a predicative fragment of system F with universal instantiations restricted to atomic formulas) different such embeddings appeared in the literature.","All of them comprise the Russell-Prawitz translation of formulas, but have different strategies for the translation of proofs.","Although these embeddings preserve proof identity, all fail in delivering preservation of reduction steps.","In fact, they translate the commuting conversions of IPC to beta-equality, or to other kinds of reduction or equality generated by new principles added to system F. The cause for this is the generation of redexes by the translation itself.","In this paper, we present an embedding of IPC into atomic system F, still based on the same translation of formulas, but which maps commuting conversions to syntactic identity, while simulating the other kinds of reduction steps present in IPC beta\\eta-reduction.","In this sense the translation achieves a truly commuting conversion-free image of IPC in atomic system F."],"url":"http://arxiv.org/abs/2402.16171v1","category":"cs.LO"}
{"created":"2024-02-25 18:37:39","title":"Nonparametric Steady-state Learning for Robust Output Regulation of Nonlinear Output Feedback Systems","abstract":"This article addresses the nonadaptive and robust output regulation problem of the general nonlinear output feedback system with error output. The global robust output regulation problem for a class of general output feedback nonlinear systems with an uncertain exosystem and high relative degree can be tackled by constructing a linear generic internal model provided that a continuous nonlinear mapping exists. Leveraging the presented nonadaptive framework facilitates the conversion of the nonlinear robust output regulation problem into a robust nonadaptive stabilization endeavour for the augmented system endowed with Input-to-State Stable dynamics, removing the need for constructing a specific Lyapunov function with positive semidefinite derivatives. To ensure the feasibility of the nonlinear mapping, the approach is extended by incorporating the nonparametric learning framework. Moreover, the introduced nonparametric learning framework provides the ability to learn the dynamics of the steady-state/input behaviour from the signal generated from the internal model only using the output error feedback. As a result, the nonadaptive/nonparametric approach can be advantageous by guaranteeing convergence of the estimation and tracking error even when the underlying controlled system dynamics are complex or poorly understood. The effectiveness of the theoretical results is illustrated for a controlled duffing system and a continuously stirred tank reactor","sentences":["This article addresses the nonadaptive and robust output regulation problem of the general nonlinear output feedback system with error output.","The global robust output regulation problem for a class of general output feedback nonlinear systems with an uncertain exosystem and high relative degree can be tackled by constructing a linear generic internal model provided that a continuous nonlinear mapping exists.","Leveraging the presented nonadaptive framework facilitates the conversion of the nonlinear robust output regulation problem into a robust nonadaptive stabilization endeavour for the augmented system endowed with Input-to-State Stable dynamics, removing the need for constructing a specific Lyapunov function with positive semidefinite derivatives.","To ensure the feasibility of the nonlinear mapping, the approach is extended by incorporating the nonparametric learning framework.","Moreover, the introduced nonparametric learning framework provides the ability to learn the dynamics of the steady-state/input behaviour from the signal generated from the internal model only using the output error feedback.","As a result, the nonadaptive/nonparametric approach can be advantageous by guaranteeing convergence of the estimation and tracking error even when the underlying controlled system dynamics are complex or poorly understood.","The effectiveness of the theoretical results is illustrated for a controlled duffing system and a continuously stirred tank reactor"],"url":"http://arxiv.org/abs/2402.16170v1","category":"eess.SY"}
{"created":"2024-02-25 18:33:25","title":"Hitting \"Probe\"rty with Non-Linearity, and More","abstract":"Structural probes learn a linear transformation to find how dependency trees are embedded in the hidden states of language models. This simple design may not allow for full exploitation of the structure of the encoded information. Hence, to investigate the structure of the encoded information to its full extent, we incorporate non-linear structural probes. We reformulate the design of non-linear structural probes introduced by White et al. making its design simpler yet effective. We also design a visualization framework that lets us qualitatively assess how strongly two words in a sentence are connected in the predicted dependency tree. We use this technique to understand which non-linear probe variant is good at encoding syntactical information. Additionally, we also use it to qualitatively investigate the structure of dependency trees that BERT encodes in each of its layers. We find that the radial basis function (RBF) is an effective non-linear probe for the BERT model than the linear probe.","sentences":["Structural probes learn a linear transformation to find how dependency trees are embedded in the hidden states of language models.","This simple design may not allow for full exploitation of the structure of the encoded information.","Hence, to investigate the structure of the encoded information to its full extent, we incorporate non-linear structural probes.","We reformulate the design of non-linear structural probes introduced by White et al. making its design simpler yet effective.","We also design a visualization framework that lets us qualitatively assess how strongly two words in a sentence are connected in the predicted dependency tree.","We use this technique to understand which non-linear probe variant is good at encoding syntactical information.","Additionally, we also use it to qualitatively investigate the structure of dependency trees that BERT encodes in each of its layers.","We find that the radial basis function (RBF) is an effective non-linear probe for the BERT model than the linear probe."],"url":"http://arxiv.org/abs/2402.16168v1","category":"cs.CL"}
{"created":"2024-02-25 18:07:07","title":"On the Feasibility of Deep Learning Classification from Raw Signal Data in Radiology, Ultrasonography and Electrophysiology","abstract":"Medical imaging is a very useful tool in healthcare, various technologies being employed to non-invasively peek inside the human body. Deep learning with neural networks in radiology was welcome - albeit cautiously - by the radiologist community. Most of the currently deployed or researched deep learning solutions are applied on already generated images of medical scans, use the neural networks to aid in the generation of such images, or use them for identifying specific substance markers in spectrographs. This paper's author posits that if the neural networks were trained directly on the raw signals from the scanning machines, they would gain access to more nuanced information than from the already processed images, hence the training - and later, the inferences - would become more accurate. The paper presents the main current applications of deep learning in radiography, ultrasonography, and electrophysiology, and discusses whether the proposed neural network training directly on raw signals is feasible.","sentences":["Medical imaging is a very useful tool in healthcare, various technologies being employed to non-invasively peek inside the human body.","Deep learning with neural networks in radiology was welcome - albeit cautiously - by the radiologist community.","Most of the currently deployed or researched deep learning solutions are applied on already generated images of medical scans, use the neural networks to aid in the generation of such images, or use them for identifying specific substance markers in spectrographs.","This paper's author posits that if the neural networks were trained directly on the raw signals from the scanning machines, they would gain access to more nuanced information than from the already processed images, hence the training - and later, the inferences - would become more accurate.","The paper presents the main current applications of deep learning in radiography, ultrasonography, and electrophysiology, and discusses whether the proposed neural network training directly on raw signals is feasible."],"url":"http://arxiv.org/abs/2402.16165v1","category":"eess.SY"}
{"created":"2024-02-25 18:01:42","title":"Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation","abstract":"In recent years, self-supervision has drawn a lot of attention in remote sensing society due to its ability to reduce the demand of exact labels in supervised deep learning model training. Self-supervision methods generally utilize image-level information to pretrain models in an unsupervised fashion. Though these pretrained encoders show effectiveness in many downstream tasks, their performance on segmentation tasks is often not as good as that on classification tasks. On the other hand, many easily available label sources (e.g., automatic labeling tools and land cover land use products) exist, which can provide a large amount of noisy labels for segmentation model training. In this work, we propose to explore the under-exploited potential of noisy labels for segmentation task specific pretraining, and exam its robustness when confronted with mismatched categories and different decoders during fine-tuning. Specifically, we inspect the impacts of noisy labels on different layers in supervised model training to serve as the basis of our work. Experiments on two datasets indicate the effectiveness of task specific supervised pretraining with noisy labels. The findings are expected to shed light on new avenues for improving the accuracy and versatility of pretraining strategies for remote sensing image segmentation.","sentences":["In recent years, self-supervision has drawn a lot of attention in remote sensing society due to its ability to reduce the demand of exact labels in supervised deep learning model training.","Self-supervision methods generally utilize image-level information to pretrain models in an unsupervised fashion.","Though these pretrained encoders show effectiveness in many downstream tasks, their performance on segmentation tasks is often not as good as that on classification tasks.","On the other hand, many easily available label sources (e.g., automatic labeling tools and land cover land use products) exist, which can provide a large amount of noisy labels for segmentation model training.","In this work, we propose to explore the under-exploited potential of noisy labels for segmentation task specific pretraining, and exam its robustness when confronted with mismatched categories and different decoders during fine-tuning.","Specifically, we inspect the impacts of noisy labels on different layers in supervised model training to serve as the basis of our work.","Experiments on two datasets indicate the effectiveness of task specific supervised pretraining with noisy labels.","The findings are expected to shed light on new avenues for improving the accuracy and versatility of pretraining strategies for remote sensing image segmentation."],"url":"http://arxiv.org/abs/2402.16164v1","category":"cs.CV"}
{"created":"2024-02-25 18:00:08","title":"An overview of field theories of gravity","abstract":"In the general relativity theory the basic ingredient to describe gravity is the geometry, which interacts with all forms of matter and energy, and as such, the metric could be interpreted as a true physical quantity. However the metric is not matter nor energy, but instead it is a new dynamical variable that Einstein introduced to describe gravity. In order to conciliate this approach to the more traditional ones, physicists have tried to describe the main ideas of GR in terms of standard conceptions of field theory. In this sense, curved metrics are seen as a dynamical variable emerging from a more fundamental field which lies upon a flat Minkowski spacetime. This was made by the hypothesis that the metric tensor may be written as $g_{\\mu\\nu} = \\eta_{\\mu\\nu} + h_{\\mu\\nu}$ where the tensor $ h_{\\mu\\nu}$ was interpreted either in terms of a spin-2 or constructed in terms of other fields. We review some proposals that were suggested in the treatment of gravity in terms of scalar, spinor and tensor fields configurations.","sentences":["In the general relativity theory the basic ingredient to describe gravity is the geometry, which interacts with all forms of matter and energy, and as such, the metric could be interpreted as a true physical quantity.","However the metric is not matter nor energy, but instead it is a new dynamical variable that Einstein introduced to describe gravity.","In order to conciliate this approach to the more traditional ones, physicists have tried to describe the main ideas of GR in terms of standard conceptions of field theory.","In this sense, curved metrics are seen as a dynamical variable emerging from a more fundamental field which lies upon a flat Minkowski spacetime.","This was made by the hypothesis that the metric tensor may be written as $g_{\\mu\\nu} = \\eta_{\\mu\\nu} + h_{\\mu\\nu}$ where the tensor $ h_{\\mu\\nu}$ was interpreted either in terms of a spin-2 or constructed in terms of other fields.","We review some proposals that were suggested in the treatment of gravity in terms of scalar, spinor and tensor fields configurations."],"url":"http://arxiv.org/abs/2402.16163v1","category":"gr-qc"}
{"created":"2024-02-25 17:48:21","title":"Catch Me If You Can: Combatting Fraud in Artificial Currency Based Government Benefits Programs","abstract":"Artificial currencies have grown in popularity in many real-world resource allocation settings, gaining traction in government benefits programs like food assistance and transit benefits programs. However, such programs are susceptible to misreporting fraud, wherein users can misreport their private attributes to gain access to more artificial currency (credits) than they are entitled to. To address the problem of misreporting fraud in artificial currency based benefits programs, we introduce an audit mechanism that induces a two-stage game between an administrator and users. In our proposed mechanism, the administrator running the benefits program can audit users at some cost and levy fines against them for misreporting their information. For this audit game, we study the natural solution concept of a signaling game equilibrium and investigate conditions on the administrator budget to establish the existence of equilibria. The computation of equilibria can be done via linear programming in our problem setting through an appropriate design of the audit rules. Our analysis also provides upper bounds that hold in any signaling game equilibrium on the expected excess payments made by the administrator and the probability that users misreport their information. We further show that the decrease in misreporting fraud corresponding to our audit mechanism far outweighs the administrator spending to run it by establishing that its total costs are lower than that of the status quo with no audits. Finally, to highlight the practical viability of our audit mechanism in mitigating misreporting fraud, we present a case study based on the Washington D.C. federal transit benefits program. In this case study, the proposed audit mechanism achieves several orders of magnitude improvement in total cost compared to a no-audit strategy for some parameter ranges.","sentences":["Artificial currencies have grown in popularity in many real-world resource allocation settings, gaining traction in government benefits programs like food assistance and transit benefits programs.","However, such programs are susceptible to misreporting fraud, wherein users can misreport their private attributes to gain access to more artificial currency (credits) than they are entitled to.","To address the problem of misreporting fraud in artificial currency based benefits programs, we introduce an audit mechanism that induces a two-stage game between an administrator and users.","In our proposed mechanism, the administrator running the benefits program can audit users at some cost and levy fines against them for misreporting their information.","For this audit game, we study the natural solution concept of a signaling game equilibrium and investigate conditions on the administrator budget to establish the existence of equilibria.","The computation of equilibria can be done via linear programming in our problem setting through an appropriate design of the audit rules.","Our analysis also provides upper bounds that hold in any signaling game equilibrium on the expected excess payments made by the administrator and the probability that users misreport their information.","We further show that the decrease in misreporting fraud corresponding to our audit mechanism far outweighs the administrator spending to run it by establishing that its total costs are lower than that of the status quo with no audits.","Finally, to highlight the practical viability of our audit mechanism in mitigating misreporting fraud, we present a case study based on the Washington D.C. federal transit benefits program.","In this case study, the proposed audit mechanism achieves several orders of magnitude improvement in total cost compared to a no-audit strategy for some parameter ranges."],"url":"http://arxiv.org/abs/2402.16162v1","category":"eess.SY"}
{"created":"2024-02-25 17:42:58","title":"The Hankel determinants for the generalized derangement polynomials of order r","abstract":"This paper sets out to introduce the generalized derangement polynomials of order $r $. It then proceeds to establish various identities associated with these polynomials, along with providing recurrence relations for derangement polynomials of order $ r$. Additionally, the paper offers a probabilistic approach for the generalized derangement polynomials of order $r $. Furthermore, it furnishes a clear expression for the Hankel determinants pertaining to these generalized derangement polynomials of order $r$, and subsequently infers the Hankel determinants for derangement polynomials of the same order, as well as for the count of cyclic derangements.","sentences":["This paper sets out to introduce the generalized derangement polynomials of order $r $.","It then proceeds to establish various identities associated with these polynomials, along with providing recurrence relations for derangement polynomials of order $ r$. Additionally, the paper offers a probabilistic approach for the generalized derangement polynomials of order $r $.","Furthermore, it furnishes a clear expression for the Hankel determinants pertaining to these generalized derangement polynomials of order $r$, and subsequently infers the Hankel determinants for derangement polynomials of the same order, as well as for the count of cyclic derangements."],"url":"http://arxiv.org/abs/2402.16160v1","category":"math.CO"}
{"created":"2024-02-25 17:37:53","title":"Distribution-Free Fair Federated Learning with Small Samples","abstract":"As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for both fairness and accuracy, and our experimental results further provide robust empirical validation for our proposed method.","sentences":["As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important.","However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees.","To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples.","Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes.","We provide rigorous theoretical guarantees for both fairness and accuracy, and our experimental results further provide robust empirical validation for our proposed method."],"url":"http://arxiv.org/abs/2402.16158v1","category":"stat.ML"}
{"created":"2024-02-25 17:32:11","title":"Radio Maps for Beam Alignment in mmWave Communications with Location Uncertainty","abstract":"Next generation communication systems require accurate beam alignment to counteract the impairments that characterize propagation in high-frequency bands. The overhead of the pilot sequences required to select the best beam pair is prohibitive when codebooks contain a large number of beams, as is the case in practice. To remedy this issue, some schemes exploit information about the user location to predict the best beam pair. However, these schemes (i) involve no measurements whatsoever, which generally results in a highly suboptimal predicted beam, and (ii) are not robust to localization errors. To address these limitations, this paper builds upon the notion of radio map to develop two algorithms that attain a balance between the quality of the obtained beam pair and measurement overhead. The proposed algorithms predict the received power corresponding to each pair and measure just the Q pairs with highest prediction. While the first algorithm targets simplicity, the second one relies on a Bayesian approach to endow the prediction process with robustness to localization error. The performance of both algorithms is shown to widely outperform existing methods using ray-tracing data.","sentences":["Next generation communication systems require accurate beam alignment to counteract the impairments that characterize propagation in high-frequency bands.","The overhead of the pilot sequences required to select the best beam pair is prohibitive when codebooks contain a large number of beams, as is the case in practice.","To remedy this issue, some schemes exploit information about the user location to predict the best beam pair.","However, these schemes (i) involve no measurements whatsoever, which generally results in a highly suboptimal predicted beam, and (ii) are not robust to localization errors.","To address these limitations, this paper builds upon the notion of radio map to develop two algorithms that attain a balance between the quality of the obtained beam pair and measurement overhead.","The proposed algorithms predict the received power corresponding to each pair and measure just the Q pairs with highest prediction.","While the first algorithm targets simplicity, the second one relies on a Bayesian approach to endow the prediction process with robustness to localization error.","The performance of both algorithms is shown to widely outperform existing methods using ray-tracing data."],"url":"http://arxiv.org/abs/2402.16156v1","category":"eess.SP"}
{"created":"2024-02-25 17:31:24","title":"Deformation families of Novikov bialgebras via differential antisymmetric infinitesimal bialgebras","abstract":"Generalizing S. Gelfand's classical construction of a Novikov algebra from a commutative differential algebra, a deformation family $(A,\\circ_q)$, for scalars $q$, of Novikov algebras is constructed from what we call an admissible commutative differential algebra, by adding a second linear operator to the commutative differential algebra with certain admissibility condition. The case of $(A,\\circ_0)$ recovers the construction of S. Gelfand. This admissibility condition also ensures a bialgebra theory of commutative differential algebras, enriching the antisymmetric infinitesimal bialgebra. This way, a deformation family of Novikov bialgebras is obtained, under the further condition that the two operators are bialgebra derivations. As a special case, we obtain a bialgebra variation of S. Gelfand's construction with an interesting twist: every commutative and cocommutative differential antisymmetric infinitesimal bialgebra gives rise to a Novikov bialgebra whose underlying Novikov algebra is $(A,\\circ_{-\\frac{1}{2}})$ instead of $(A,\\circ_0)$. The close relations of the classical bialgebra theories with Manin triples, classical Yang-Baxter type equations, $\\mathcal{O}$-operators, and pre-structures are expanded to the two new bialgebra theories, in a way that is compatible with the just established connection between the two bialgebras. As an application, Novikov bialgebras are obtained from admissible differential Zinbiel algebras.","sentences":["Generalizing S. Gelfand's classical construction of a Novikov algebra from a commutative differential algebra, a deformation family $(A,\\circ_q)$, for scalars $q$, of Novikov algebras is constructed from what we call an admissible commutative differential algebra, by adding a second linear operator to the commutative differential algebra with certain admissibility condition.","The case of $(A,\\circ_0)$ recovers the construction of S. Gelfand.","This admissibility condition also ensures a bialgebra theory of commutative differential algebras, enriching the antisymmetric infinitesimal bialgebra.","This way, a deformation family of Novikov bialgebras is obtained, under the further condition that the two operators are bialgebra derivations.","As a special case, we obtain a bialgebra variation of S. Gelfand's construction with an interesting twist: every commutative and cocommutative differential antisymmetric infinitesimal bialgebra gives rise to a Novikov bialgebra whose underlying Novikov algebra is $(A,\\circ_{-\\frac{1}{2}})$ instead of $(A,\\circ_0)$. The close relations of the classical bialgebra theories with Manin triples, classical Yang-Baxter type equations, $\\mathcal{O}$-operators, and pre-structures are expanded to the two new bialgebra theories, in a way that is compatible with the just established connection between the two bialgebras.","As an application, Novikov bialgebras are obtained from admissible differential Zinbiel algebras."],"url":"http://arxiv.org/abs/2402.16155v1","category":"math.QA"}
{"created":"2024-02-25 17:30:41","title":"IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations","abstract":"Many applications require a robot to accurately track reference end-effector trajectories. Certain trajectories may not be tracked as single, continuous paths due to the robot's kinematic constraints or obstacles elsewhere in the environment. In this situation, it becomes necessary to divide the trajectory into shorter segments. Each such division introduces a reconfiguration, in which the robot deviates from the reference trajectory, repositions itself in configuration space, and then resumes task execution. The occurrence of reconfigurations should be minimized because they increase the time and energy usage. In this paper, we present IKLink, a method for finding joint motions to track reference end-effector trajectories while executing minimal reconfigurations. Our graph-based method generates a diverse set of Inverse Kinematics (IK) solutions for every waypoint on the reference trajectory and utilizes a dynamic programming algorithm to find the globally optimal motion by linking the IK solutions. We demonstrate the effectiveness of IKLink through a simulation experiment and an illustrative demonstration using a physical robot.","sentences":["Many applications require a robot to accurately track reference end-effector trajectories.","Certain trajectories may not be tracked as single, continuous paths due to the robot's kinematic constraints or obstacles elsewhere in the environment.","In this situation, it becomes necessary to divide the trajectory into shorter segments.","Each such division introduces a reconfiguration, in which the robot deviates from the reference trajectory, repositions itself in configuration space, and then resumes task execution.","The occurrence of reconfigurations should be minimized because they increase the time and energy usage.","In this paper, we present IKLink, a method for finding joint motions to track reference end-effector trajectories while executing minimal reconfigurations.","Our graph-based method generates a diverse set of Inverse Kinematics (IK) solutions for every waypoint on the reference trajectory and utilizes a dynamic programming algorithm to find the globally optimal motion by linking the IK solutions.","We demonstrate the effectiveness of IKLink through a simulation experiment and an illustrative demonstration using a physical robot."],"url":"http://arxiv.org/abs/2402.16154v1","category":"cs.RO"}
{"created":"2024-02-25 17:19:41","title":"ChatMusician: Understanding and Generating Music Intrinsically with LLM","abstract":"While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.","sentences":["While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language.","We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities.","It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language.","ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers.","Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score.","Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline.","On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin.","Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered.","We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub."],"url":"http://arxiv.org/abs/2402.16153v1","category":"cs.SD"}
{"created":"2024-02-25 17:01:10","title":"3D active nematic disclinations behave as Majorana quasiparticles","abstract":"Quasiparticles are low-energy excitations with important roles in condensed matter physics. An intriguing example is provided by Majorana fermions, quasiparticles which are identical to their antiparticles. Despite being implicated in neutrino oscillations and topological superconductivity, their experimental realisations remain scarce. Here we propose a purely classical realisation of Majorana fermions, in terms of 3-dimensional disclination lines in active nematics. Activity is required to overcome the elastic cost associated with these excitations, so they can appear in steady state. We combine topology and simulations to show that active nematics under confinement spontaneously create in their interior topologically charged disclination lines and loops, akin to Majorana quasiparticles with finite momentum. Within an elongated channel, we find a phenomenology similar to that of the Kitaev chain, as local Majorana-like excitations appear near surfaces, while a non-local system-spanning helical disclination line can arise along the centre. In unconfined active turbulence, Majorana-like charged loops are instead exceedingly rare, suggesting that boundaries are crucial to generate these quasiparticles, as in quantum condensed matter. We suggest that 3-dimensional active disclinations can be used to probe the physics of Majorana spinors at a much larger scale than traditionally considered, potentially facilitating the experimental observation of their dynamics.","sentences":["Quasiparticles are low-energy excitations with important roles in condensed matter physics.","An intriguing example is provided by Majorana fermions, quasiparticles which are identical to their antiparticles.","Despite being implicated in neutrino oscillations and topological superconductivity, their experimental realisations remain scarce.","Here we propose a purely classical realisation of Majorana fermions, in terms of 3-dimensional disclination lines in active nematics.","Activity is required to overcome the elastic cost associated with these excitations, so they can appear in steady state.","We combine topology and simulations to show that active nematics under confinement spontaneously create in their interior topologically charged disclination lines and loops, akin to Majorana quasiparticles with finite momentum.","Within an elongated channel, we find a phenomenology similar to that of the Kitaev chain, as local Majorana-like excitations appear near surfaces, while a non-local system-spanning helical disclination line can arise along the centre.","In unconfined active turbulence, Majorana-like charged loops are instead exceedingly rare, suggesting that boundaries are crucial to generate these quasiparticles, as in quantum condensed matter.","We suggest that 3-dimensional active disclinations can be used to probe the physics of Majorana spinors at a much larger scale than traditionally considered, potentially facilitating the experimental observation of their dynamics."],"url":"http://arxiv.org/abs/2402.16149v1","category":"cond-mat.soft"}
{"created":"2024-02-25 16:59:37","title":"On Functorial Lindel\u00f6fifiability","abstract":"In the present paper, we prove that a topological space admits a functorial Lindel\\\"ofification if and only if its realcompactification is Lindel\\\"of. To investigate the functorial Lindel\\\"ofifiability of a topological space, for each topological property $\\mathsf{P}$, we introduce the notion of \"functorial $\\mathsf{P}$-ification\" and give an explicit construction of the functorial $\\mathsf{P}$-ification. Moreover, for a discrete space $X$, we discuss the functorial $|X|$-Lindel\\\"ofifiability of $X$ and study relationships with properties of the cardinal $|X|$. Finally, we apply our results concerning functorial $\\kappa$-Lindel\\\"ofifiability (for some cardinal $\\kappa$) to the space of ordinals and construct several functorial $\\kappa$-Lindel\\\"ofifiable spaces.","sentences":["In the present paper, we prove that a topological space admits a functorial Lindel\\\"ofification if and only if its realcompactification is Lindel\\\"of.","To investigate the functorial Lindel\\\"ofifiability of a topological space, for each topological property $\\mathsf{P}$, we introduce the notion of \"functorial $\\mathsf{P}$-ification\" and give an explicit construction of the functorial $\\mathsf{P}$-ification.","Moreover, for a discrete space $X$, we discuss the functorial $|X|$-Lindel\\\"ofifiability of $X$ and study relationships with properties of the cardinal $|X|$.","Finally, we apply our results concerning functorial $\\kappa$-Lindel\\\"ofifiability (for some cardinal $\\kappa$) to the space of ordinals and construct several functorial $\\kappa$-Lindel\\\"ofifiable spaces."],"url":"http://arxiv.org/abs/2402.16148v1","category":"math.GN"}
{"created":"2024-02-25 16:57:51","title":"Towards Efficient Quantum Hybrid Diffusion Models","abstract":"In this paper, we propose a new methodology to design quantum hybrid diffusion models, derived from classical U-Nets with ResNet and Attention layers. Specifically, we propose two possible different hybridization schemes combining quantum computing's superior generalization with classical networks' modularity. In the first one, we acted at the vertex: ResNet convolutional layers are gradually replaced with variational circuits to create Quantum ResNet blocks. In the second proposed architecture, we extend the hybridization to the intermediate level of the encoder, due to its higher sensitivity in the feature extraction process. In order to conduct an in-depth analysis of the potential advantages stemming from the integration of quantum layers, images generated by quantum hybrid diffusion models are compared to those generated by classical models, and evaluated in terms of several quantitative metrics. The results demonstrate an advantage in using a hybrid quantum diffusion models, as they generally synthesize better-quality images and converges faster. Moreover, they show the additional advantage of having a lower number of parameters to train compared to the classical one, with a reduction that depends on the extent to which the vertex is hybridized.","sentences":["In this paper, we propose a new methodology to design quantum hybrid diffusion models, derived from classical U-Nets with ResNet and Attention layers.","Specifically, we propose two possible different hybridization schemes combining quantum computing's superior generalization with classical networks' modularity.","In the first one, we acted at the vertex: ResNet convolutional layers are gradually replaced with variational circuits to create Quantum ResNet blocks.","In the second proposed architecture, we extend the hybridization to the intermediate level of the encoder, due to its higher sensitivity in the feature extraction process.","In order to conduct an in-depth analysis of the potential advantages stemming from the integration of quantum layers, images generated by quantum hybrid diffusion models are compared to those generated by classical models, and evaluated in terms of several quantitative metrics.","The results demonstrate an advantage in using a hybrid quantum diffusion models, as they generally synthesize better-quality images and converges faster.","Moreover, they show the additional advantage of having a lower number of parameters to train compared to the classical one, with a reduction that depends on the extent to which the vertex is hybridized."],"url":"http://arxiv.org/abs/2402.16147v1","category":"quant-ph"}
{"created":"2024-02-25 16:54:51","title":"Egalitarian Price of Fairness for Indivisible Goods","abstract":"In the context of fair division, the concept of price of fairness has been introduced to quantify the loss of welfare when we have to satisfy some fairness condition. In other words, it is the price we have to pay to guarantee fairness. Various settings of fair division have been considered previously; we extend to the setting of indivisible goods by using egalitarian welfare as the welfare measure, instead of the commonly used utilitarian welfare. We provide lower and upper bounds for various fairness and efficiency conditions such as envy-freeness up to one good (EF1) and maximum Nash welfare (MNW).","sentences":["In the context of fair division, the concept of price of fairness has been introduced to quantify the loss of welfare when we have to satisfy some fairness condition.","In other words, it is the price we have to pay to guarantee fairness.","Various settings of fair division have been considered previously; we extend to the setting of indivisible goods by using egalitarian welfare as the welfare measure, instead of the commonly used utilitarian welfare.","We provide lower and upper bounds for various fairness and efficiency conditions such as envy-freeness up to one good (EF1) and maximum Nash welfare (MNW)."],"url":"http://arxiv.org/abs/2402.16145v1","category":"cs.GT"}
{"created":"2024-02-25 16:48:25","title":"Cinematographic Camera Diffusion Model","abstract":"Designing effective camera trajectories in virtual 3D environments is a challenging task even for experienced animators. Despite an elaborate film grammar, forged through years of experience, that enables the specification of camera motions through cinematographic properties (framing, shots sizes, angles, motions), there are endless possibilities in deciding how to place and move cameras with characters. Dealing with these possibilities is part of the complexity of the problem. While numerous techniques have been proposed in the literature (optimization-based solving, encoding of empirical rules, learning from real examples,...), the results either lack variety or ease of control.   In this paper, we propose a cinematographic camera diffusion model using a transformer-based architecture to handle temporality and exploit the stochasticity of diffusion models to generate diverse and qualitative trajectories conditioned by high-level textual descriptions. We extend the work by integrating keyframing constraints and the ability to blend naturally between motions using latent interpolation, in a way to augment the degree of control of the designers. We demonstrate the strengths of this text-to-camera motion approach through qualitative and quantitative experiments and gather feedback from professional artists. The code and data are available at \\URL{https://github.com/jianghd1996/Camera-control}.","sentences":["Designing effective camera trajectories in virtual 3D environments is a challenging task even for experienced animators.","Despite an elaborate film grammar, forged through years of experience, that enables the specification of camera motions through cinematographic properties (framing, shots sizes, angles, motions), there are endless possibilities in deciding how to place and move cameras with characters.","Dealing with these possibilities is part of the complexity of the problem.","While numerous techniques have been proposed in the literature (optimization-based solving, encoding of empirical rules, learning from real examples,...), the results either lack variety or ease of control.   ","In this paper, we propose a cinematographic camera diffusion model using a transformer-based architecture to handle temporality and exploit the stochasticity of diffusion models to generate diverse and qualitative trajectories conditioned by high-level textual descriptions.","We extend the work by integrating keyframing constraints and the ability to blend naturally between motions using latent interpolation, in a way to augment the degree of control of the designers.","We demonstrate the strengths of this text-to-camera motion approach through qualitative and quantitative experiments and gather feedback from professional artists.","The code and data are available at \\URL{https://github.com/jianghd1996/Camera-control}."],"url":"http://arxiv.org/abs/2402.16143v1","category":"cs.GR"}
{"created":"2024-02-25 16:47:59","title":"From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility","abstract":"This groundbreaking study explores the expanse of Large Language Models (LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) across varied domains ranging from technology, finance, healthcare to education. Despite their established prowess in Natural Language Processing (NLP), these LLMs have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management. This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of LLMs' utility in diverse domains, recognizes the research gaps and realms where the potential of LLMs is yet to be harnessed. This study uncovers innovative ways in which LLMs can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire future researches and applications in the said avenues.","sentences":["This groundbreaking study explores the expanse of Large Language Models (LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) across varied domains ranging from technology, finance, healthcare to education.","Despite their established prowess in Natural Language Processing (NLP), these LLMs have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management.","This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of LLMs' utility in diverse domains, recognizes the research gaps and realms where the potential of LLMs is yet to be harnessed.","This study uncovers innovative ways in which LLMs can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire future researches and applications in the said avenues."],"url":"http://arxiv.org/abs/2402.16142v1","category":"cs.CL"}
{"created":"2024-02-25 16:43:27","title":"Multi-access Distributed Computing Models from Map-Reduce Arrays","abstract":"A novel distributed computing model called \"Multi-access Distributed Computing (MADC)\" was recently introduced in http://www.arXiv:2206.12851. In this paper, we represent MADC models via 2-layered bipartite graphs called Map-Reduce Graphs (MRGs) and a set of arrays called Map-Reduce Arrays (MRAs) inspired from the Placement Delivery Arrays (PDAs) used in the coded caching literature. The connection between MRAs and MRGs is established, thereby exploring new topologies and providing coded shuffling schemes for the MADC models with MRGs using the structure of MRAs. A novel \\textit{Nearest Neighbor Connect-MRG (NNC-MRG)} is explored and a coding scheme is provided for MADC models with NNC-MRG, exploiting the connections between MRAs and PDAs. Moreover, CT is generalized to Generalized Combinatorial-MRG (GC-MRG). A set of $g-$regular MRAs is provided which corresponds to the existing scheme for MADC models with CT and extended those to generate another set of MRAs to represent MADC models with GC-MRG. A lower bound on the computation-communication curve for MADC model with GC-MRG under homogeneous setting is derived and certain cases are explored where the existing scheme is optimal under CT. One of the major limitations of the existing scheme for CT is that it requires an exponentially large number of reducer nodes and input files for large $\\Lambda$. This can be overcome by representing CT by MRAs, where coding schemes can be derived even if some of the reducer nodes are not present. Another way of tackling this is by using a different MRG, specifically NNC-MRG, where the number of reducer nodes and files required are significantly smaller compared to CT. Hence, the advantages are two-fold, which is achievable at the expense of a slight increase in the communication load.","sentences":["A novel distributed computing model called \"Multi-access Distributed Computing (MADC)\" was recently introduced in http://www.arXiv:2206.12851.","In this paper, we represent MADC models via 2-layered bipartite graphs called Map-Reduce Graphs (MRGs) and a set of arrays called Map-Reduce Arrays (MRAs) inspired from the Placement Delivery Arrays (PDAs) used in the coded caching literature.","The connection between MRAs and MRGs is established, thereby exploring new topologies and providing coded shuffling schemes for the MADC models with MRGs using the structure of MRAs.","A novel \\textit{Nearest Neighbor Connect-MRG (NNC-MRG)} is explored and a coding scheme is provided for MADC models with NNC-MRG, exploiting the connections between MRAs and PDAs.","Moreover, CT is generalized to Generalized Combinatorial-MRG (GC-MRG).","A set of $g-$regular MRAs is provided which corresponds to the existing scheme for MADC models with CT and extended those to generate another set of MRAs to represent MADC models with GC-MRG.","A lower bound on the computation-communication curve for MADC model with GC-MRG under homogeneous setting is derived and certain cases are explored where the existing scheme is optimal under CT.","One of the major limitations of the existing scheme for CT is that it requires an exponentially large number of reducer nodes and input files for large $\\Lambda$. This can be overcome by representing CT by MRAs, where coding schemes can be derived even if some of the reducer nodes are not present.","Another way of tackling this is by using a different MRG, specifically NNC-MRG, where the number of reducer nodes and files required are significantly smaller compared to CT.","Hence, the advantages are two-fold, which is achievable at the expense of a slight increase in the communication load."],"url":"http://arxiv.org/abs/2402.16140v1","category":"cs.IT"}
{"created":"2024-02-25 16:36:51","title":"What Generative Artificial Intelligence Means for Terminological Definitions","abstract":"This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.","sentences":["This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions.","GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources.","ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy.","Terminological definitions in recognized resources will likely survive because of their reliability.","From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation."],"url":"http://arxiv.org/abs/2402.16139v1","category":"cs.CL"}
{"created":"2024-02-25 16:25:51","title":"Integration of Conventional Surface Science Techniques with Surface-Sensitive Azimuthal and Polarization Dependent Femtosecond-Resolved Sum Frequency Generation Spectroscopy","abstract":"Experimental insight into the elementary processes underlying charge transfer across interfaces has blossomed with the wide-spread availability of ultra-high vacuum set-ups that allow the preparation and characterization of solid surfaces with well-defined molecular adsorbates over a wide ranges of temperatures. Thick layers of molecular adsorbates or heterostructures of 2D materials generally preclude the use of electrons or atoms as probes in such characterization. However with linear photon-in/photon-out techniques it is often challenging to assign the observed optical response to a particular portion of the interface. We and prior workers have demonstrated in work under ambient conditions that by full characterization of the symmetry of the second order nonlinear optical susceptibility, i.e. the $\\chi^{(2)}$, in sum frequency generation (SFG) spectroscopy, this problem can be overcome. Here we describe an ultra-high vacuum system built to allow conventional UHV sample preparation and characterization, femtosecond and polarization resolved SFG spectroscopy, the azimuthal sample rotation necessary to fully describe $\\chi^{(2)}$ symmetry and with sufficient stability to allow scanning SFG microscopy. We demonstrate these capabilities in proof-of-principle measurements on CO adsorbed on Pt(111) and of the clean Ag(111) surface. Because this set-up allows both full characterization of the nonlinear susceptibility and the temperature control and sample preparation/characterization of conventional UHV set-ups we expect it to be of great utility in investigation of both the basic physics and applications of solid, 2D material heterostructures.","sentences":["Experimental insight into the elementary processes underlying charge transfer across interfaces has blossomed with the wide-spread availability of ultra-high vacuum set-ups that allow the preparation and characterization of solid surfaces with well-defined molecular adsorbates over a wide ranges of temperatures.","Thick layers of molecular adsorbates or heterostructures of 2D materials generally preclude the use of electrons or atoms as probes in such characterization.","However with linear photon-in/photon-out techniques it is often challenging to assign the observed optical response to a particular portion of the interface.","We and prior workers have demonstrated in work under ambient conditions that by full characterization of the symmetry of the second order nonlinear optical susceptibility, i.e. the $\\chi^{(2)}$, in sum frequency generation (SFG) spectroscopy, this problem can be overcome.","Here we describe an ultra-high vacuum system built to allow conventional UHV sample preparation and characterization, femtosecond and polarization resolved SFG spectroscopy, the azimuthal sample rotation necessary to fully describe $\\chi^{(2)}$ symmetry and with sufficient stability to allow scanning SFG microscopy.","We demonstrate these capabilities in proof-of-principle measurements on CO adsorbed on Pt(111) and of the clean Ag(111) surface.","Because this set-up allows both full characterization of the nonlinear susceptibility and the temperature control and sample preparation/characterization of conventional UHV set-ups we expect it to be of great utility in investigation of both the basic physics and applications of solid, 2D material heterostructures."],"url":"http://arxiv.org/abs/2402.16138v1","category":"physics.chem-ph"}
{"created":"2024-02-25 16:22:06","title":"Classical acceleration temperature from evaporated black hole remnants and accelerated electron-mirror radiation","abstract":"We investigate the radiation from accelerating electrons with asymptotic constant velocity and their analog signatures as evaporating black holes with left-over remnants. We find high-speed electrons, while having a high temperature, correspond to low-temperature analog remnants.","sentences":["We investigate the radiation from accelerating electrons with asymptotic constant velocity and their analog signatures as evaporating black holes with left-over remnants.","We find high-speed electrons, while having a high temperature, correspond to low-temperature analog remnants."],"url":"http://arxiv.org/abs/2402.16137v1","category":"gr-qc"}
{"created":"2024-02-25 16:21:43","title":"Analogue simulations of quantum gravity with fluids","abstract":"The recent technological advances in controlling and manipulating fluids have enabled the experimental realization of acoustic analogues of gravitational black holes. A flowing fluid provides an effective curved spacetime on which sound waves can propagate, allowing the simulation of gravitational geometries and related phenomena. The last decade has witnessed a variety of hydrodynamic experiments testing disparate aspects of black hole physics culminating in the recent experimental evidence of Hawking radiation and Penrose superradiance. In this Perspective, we discuss the potential use of analogue hydrodynamic systems beyond classical general relativity towards the exploration of quantum gravitational effects. These include possible insights into the information-loss paradox, black hole physics with Planck-scale quantum corrections, emergent gravity scenarios and the regularization of curvature singularities. We aim at bridging the gap between the non-overlapping communities of experimentalists working with classical and quantum fluids and quantum-gravity theorists, illustrating the opportunities made possible by the latest experimental and theoretical developments in these important areas of research","sentences":["The recent technological advances in controlling and manipulating fluids have enabled the experimental realization of acoustic analogues of gravitational black holes.","A flowing fluid provides an effective curved spacetime on which sound waves can propagate, allowing the simulation of gravitational geometries and related phenomena.","The last decade has witnessed a variety of hydrodynamic experiments testing disparate aspects of black hole physics culminating in the recent experimental evidence of Hawking radiation and Penrose superradiance.","In this Perspective, we discuss the potential use of analogue hydrodynamic systems beyond classical general relativity towards the exploration of quantum gravitational effects.","These include possible insights into the information-loss paradox, black hole physics with Planck-scale quantum corrections, emergent gravity scenarios and the regularization of curvature singularities.","We aim at bridging the gap between the non-overlapping communities of experimentalists working with classical and quantum fluids and quantum-gravity theorists, illustrating the opportunities made possible by the latest experimental and theoretical developments in these important areas of research"],"url":"http://arxiv.org/abs/2402.16136v1","category":"gr-qc"}
{"created":"2024-02-25 16:16:35","title":"Variable martingale Hardy-Lorentz-Karamata spaces and their applications in Fourier Analysis","abstract":"In this paper, we introduce a new class of function spaces, which unify and generalize Lorentz-Karamata spaces, variable Lorentz spaces and other several classical function spaces. Based on the new spaces, we develop the theory of variable martingale Hardy-Lorentz-Karamata spaces and apply it to Fourier Analysis. To be precise, we discuss the basic properties of Lorentz-Karamata spaces with variable exponents. We introduce five variable martingale Hardy-Lorentz-Karamata spaces and characterize them via simple atoms as well as via atoms. As applications of the atomic decompositions, dual theorems and the generalized John-Nirenberg theorem for the new framework are presented. Moreover, we obtain the boundedness of $\\sigma$-sublinear operator defined on variable martingale Hardy-Lorentz-Karamata spaces, which leads to martingale inequalities and the relation of the five variable martingale Hardy-Lorentz-Karamata spaces. Also, we investigate the boundedness of fractional integral operators in this new framework. Finally, we deal with the applications of variable martingale Hardy-Lorentz-Karamata spaces in Fourier analysis by using the previous results. More precisely, we show that the partial sums of the Walsh-Fourier series converge to the function in norm if $f\\in L_{p(\\cdot),q,b}$ with $1<p_-\\le p_+<\\infty$. The Fej\\'{e}r summability method is also studied and it is proved that the maximal Fej\\'{e}r operator is bounded from variable martingale Hardy-Lorentz-Karamata spaces to variable Lorentz-Karamata spaces. As a consequence, we get conclusions about almost everywhere and norm convergence of Fej\\'{e}r means. The results obtained in this paper generalize the results for martingale Hardy-Lorentz-Karamata spaces and variable martingale Hardy-Lorentz spaces. Especially, we remove the condition that $b$ is nondecreasing in previous literature.","sentences":["In this paper, we introduce a new class of function spaces, which unify and generalize Lorentz-Karamata spaces, variable Lorentz spaces and other several classical function spaces.","Based on the new spaces, we develop the theory of variable martingale Hardy-Lorentz-Karamata spaces and apply it to Fourier Analysis.","To be precise, we discuss the basic properties of Lorentz-Karamata spaces with variable exponents.","We introduce five variable martingale Hardy-Lorentz-Karamata spaces and characterize them via simple atoms as well as via atoms.","As applications of the atomic decompositions, dual theorems and the generalized John-Nirenberg theorem for the new framework are presented.","Moreover, we obtain the boundedness of $\\sigma$-sublinear operator defined on variable martingale Hardy-Lorentz-Karamata spaces, which leads to martingale inequalities and the relation of the five variable martingale Hardy-Lorentz-Karamata spaces.","Also, we investigate the boundedness of fractional integral operators in this new framework.","Finally, we deal with the applications of variable martingale Hardy-Lorentz-Karamata spaces in Fourier analysis by using the previous results.","More precisely, we show that the partial sums of the Walsh-Fourier series converge to the function in norm if $f\\in L_{p(\\cdot),q,b}$ with $1<p_-\\le p_+<\\infty$.","The Fej\\'{e}r summability method is also studied and it is proved that the maximal Fej\\'{e}r operator is bounded from variable martingale Hardy-Lorentz-Karamata spaces to variable Lorentz-Karamata spaces.","As a consequence, we get conclusions about almost everywhere and norm convergence of Fej\\'{e}r means.","The results obtained in this paper generalize the results for martingale Hardy-Lorentz-Karamata spaces and variable martingale Hardy-Lorentz spaces.","Especially, we remove the condition that $b$ is nondecreasing in previous literature."],"url":"http://arxiv.org/abs/2402.16133v1","category":"math.FA"}
{"created":"2024-02-25 16:14:26","title":"LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting","abstract":"Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.","sentences":["Time-series forecasting (TSF) finds broad applications in real-world scenarios.","Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency.","However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought.","Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks.","LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each.","LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability.","Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models."],"url":"http://arxiv.org/abs/2402.16132v1","category":"cs.CL"}
{"created":"2024-02-25 16:02:03","title":"Localization in Reconfigurable Intelligent Surface Aided mmWave Systems: A Multiple Measurement Vector Based Channel Estimation Method","abstract":"The sparsity of millimeter wave (mmWave) channels in the angular and temporal domains is beneficial to channel estimation, while the associated channel parameters can be utilized for localization. However, line-of-sight (LoS) blockage poses a significant challenge on the localization in mmWave systems, potentially leading to substantial positioning errors. A promising solution is to employ reconfigurable intelligent surface (RIS) to generate the virtual line-of-sight (VLoS) paths to aid localization. Consequently, wireless localization in the RIS-assisted mmWave systems has become the essential research issue. In this paper, a multiple measurement vector (MMV) model is constructed and a two-stage channel estimation based localization scheme is proposed. During the first stage, by exploiting the beamspace sparsity and employing a random RIS phase shift matrix, the channel parameters are estimated, based on which the precoder at base station and combiner at user equipment (UE) are designed. Then, in the second stage, based on the designed precoding and combining matrices, the optimal phase shift matrix for RIS is designed using the proposed modified temporally correlated multiple sparse Bayesian learning (TMSBL) algorithm. Afterwards, the channel parameters, such as angle of reflection, time-of-arrival, etc., embedding location information are estimated for finally deriving the location of UE. We demonstrate the achievable performance of the proposed algorithm and compare it with the state-of-the-art algorithms. Our studies show that the proposed localization scheme is capable of achieving centimeter level localization accuracy, when LoS path is blocked. Furthermore, the proposed algorithm has a low computational complexity and outperforms the legacy algorithms in different perspectives.","sentences":["The sparsity of millimeter wave (mmWave) channels in the angular and temporal domains is beneficial to channel estimation, while the associated channel parameters can be utilized for localization.","However, line-of-sight (LoS) blockage poses a significant challenge on the localization in mmWave systems, potentially leading to substantial positioning errors.","A promising solution is to employ reconfigurable intelligent surface (RIS) to generate the virtual line-of-sight (VLoS) paths to aid localization.","Consequently, wireless localization in the RIS-assisted mmWave systems has become the essential research issue.","In this paper, a multiple measurement vector (MMV) model is constructed and a two-stage channel estimation based localization scheme is proposed.","During the first stage, by exploiting the beamspace sparsity and employing a random RIS phase shift matrix, the channel parameters are estimated, based on which the precoder at base station and combiner at user equipment (UE) are designed.","Then, in the second stage, based on the designed precoding and combining matrices, the optimal phase shift matrix for RIS is designed using the proposed modified temporally correlated multiple sparse Bayesian learning (TMSBL) algorithm.","Afterwards, the channel parameters, such as angle of reflection, time-of-arrival, etc., embedding location information are estimated for finally deriving the location of UE.","We demonstrate the achievable performance of the proposed algorithm and compare it with the state-of-the-art algorithms.","Our studies show that the proposed localization scheme is capable of achieving centimeter level localization accuracy, when LoS path is blocked.","Furthermore, the proposed algorithm has a low computational complexity and outperforms the legacy algorithms in different perspectives."],"url":"http://arxiv.org/abs/2402.16129v1","category":"eess.SP"}
{"created":"2024-02-25 16:01:28","title":"Direct and Inverse Problems in Baumslag-Solitar Group $BS(1,3)$","abstract":"For integers $m$ and $n$, the Baumslag-Solitar groups, denoted as $BS(m,n)$, are groups generated by two elements with a single defining relation: $BS(m,n) = \\langle a, b | a^mb=ba^n\\rangle$. The sum of dilates, denoted as $r \\cdot A + s \\cdot B$ for integers $r$ and $s$, is defined as $\\{ra + sb; a\\in A, b\\in B\\}$. In 2014, Freiman et al. \\cite{freiman} derived direct and inverse results for sums of dilates and applied these findings to address specific direct and inverse problems within Baumslag-Solitar groups, assuming suitable small doubling properties. In 2015, Freiman et al. \\cite{freiman15} tackled the general problem of small doubling types in a monoid, a subset of the Baumslag-Solitar group $BS(1,2)$. This paper extends these investigations to solve the analogous problem for the Baumslag-Solitar group $BS(1,3)$.","sentences":["For integers $m$ and $n$, the Baumslag-Solitar groups, denoted as $BS(m,n)$, are groups generated by two elements with a single defining relation: $BS(m,n) = \\langle a, b | a^mb=ba^n\\rangle$. The sum of dilates, denoted as $r \\cdot A + s \\cdot B$ for integers $r$ and $s$, is defined as $\\{ra + sb; a\\in A, b\\in B\\}$. In 2014, Freiman et al. \\cite{freiman} derived direct and inverse results for sums of dilates and applied these findings to address specific direct and inverse problems within Baumslag-Solitar groups, assuming suitable small doubling properties.","In 2015, Freiman et al. \\cite{freiman15} tackled the general problem of small doubling types in a monoid, a subset of the Baumslag-Solitar group $BS(1,2)$. This paper extends these investigations to solve the analogous problem for the Baumslag-Solitar group $BS(1,3)$."],"url":"http://arxiv.org/abs/2402.16128v1","category":"math.NT"}
{"created":"2024-02-25 15:51:05","title":"AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation","abstract":"While considerable progress has been made in achieving accurate lip synchronization for 3D speech-driven talking face generation, the task of incorporating expressive facial detail synthesis aligned with the speaker's speaking status remains challenging. Our goal is to directly leverage the inherent style information conveyed by human speech for generating an expressive talking face that aligns with the speaking status. In this paper, we propose AVI-Talking, an Audio-Visual Instruction system for expressive Talking face generation. This system harnesses the robust contextual reasoning and hallucination capability offered by Large Language Models (LLMs) to instruct the realistic synthesis of 3D talking faces. Instead of directly learning facial movements from human speech, our two-stage strategy involves the LLMs first comprehending audio information and generating instructions implying expressive facial details seamlessly corresponding to the speech. Subsequently, a diffusion-based generative network executes these instructions. This two-stage process, coupled with the incorporation of LLMs, enhances model interpretability and provides users with flexibility to comprehend instructions and specify desired operations or modifications. Extensive experiments showcase the effectiveness of our approach in producing vivid talking faces with expressive facial movements and consistent emotional status.","sentences":["While considerable progress has been made in achieving accurate lip synchronization for 3D speech-driven talking face generation, the task of incorporating expressive facial detail synthesis aligned with the speaker's speaking status remains challenging.","Our goal is to directly leverage the inherent style information conveyed by human speech for generating an expressive talking face that aligns with the speaking status.","In this paper, we propose AVI-Talking, an Audio-Visual Instruction system for expressive Talking face generation.","This system harnesses the robust contextual reasoning and hallucination capability offered by Large Language Models (LLMs) to instruct the realistic synthesis of 3D talking faces.","Instead of directly learning facial movements from human speech, our two-stage strategy involves the LLMs first comprehending audio information and generating instructions implying expressive facial details seamlessly corresponding to the speech.","Subsequently, a diffusion-based generative network executes these instructions.","This two-stage process, coupled with the incorporation of LLMs, enhances model interpretability and provides users with flexibility to comprehend instructions and specify desired operations or modifications.","Extensive experiments showcase the effectiveness of our approach in producing vivid talking faces with expressive facial movements and consistent emotional status."],"url":"http://arxiv.org/abs/2402.16124v1","category":"cs.CV"}
{"created":"2024-02-25 15:46:33","title":"InstructEdit: Instruction-based Knowledge Editing for Large Language Models","abstract":"Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdit consistently surpass previous strong baselines. To further investigate the underlying mechanisms of instruction-based knowledge editing, we analyze the principal components of the editing gradient directions, which unveils that instructions can help control optimization direction with stronger OOD generalization. Code and datasets will be available in https://github.com/zjunlp/EasyEdit.","sentences":["Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance.","However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications.","To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing.","Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions.","With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting.","Furthermore, experiments involving holdout unseen task illustrate that InstructEdit consistently surpass previous strong baselines.","To further investigate the underlying mechanisms of instruction-based knowledge editing, we analyze the principal components of the editing gradient directions, which unveils that instructions can help control optimization direction with stronger OOD generalization.","Code and datasets will be available in https://github.com/zjunlp/EasyEdit."],"url":"http://arxiv.org/abs/2402.16123v1","category":"cs.CL"}
{"created":"2024-02-25 15:42:12","title":"Towards Accurate Post-training Quantization for Reparameterized Models","abstract":"Model reparameterization is a widely accepted technique for improving inference speed without compromising performance. However, current Post-training Quantization (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models. This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of quantization parameters. To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of quantized reparameterization models. Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters. Our framework comprises two main components: Quantization Protecting Reparameterization and Across-block Calibration. For effective calibration, Quantization Protecting Reparameterization combines multiple branches into a single convolution with an affine layer. During training, the affine layer accelerates convergence and amplifies the output of the convolution to better accommodate samples with outliers. Additionally, Across-block Calibration leverages the measurement of stage output as supervision to address the gradient problem introduced by MAE and enhance the interlayer correlation with quantization parameters. Comprehensive experiments demonstrate the effectiveness of RepAPQ across various models and tasks. Our framework outperforms previous methods by approximately 1\\% for 8-bit PTQ and 2\\% for 6-bit PTQ, showcasing its superior performance. The code is available at \\url{https://github.com/ilur98/DLMC-QUANT}.","sentences":["Model reparameterization is a widely accepted technique for improving inference speed without compromising performance.","However, current Post-training Quantization (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models.","This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of quantization parameters.","To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of quantized reparameterization models.","Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters.","Our framework comprises two main components: Quantization Protecting Reparameterization and Across-block Calibration.","For effective calibration, Quantization Protecting Reparameterization combines multiple branches into a single convolution with an affine layer.","During training, the affine layer accelerates convergence and amplifies the output of the convolution to better accommodate samples with outliers.","Additionally, Across-block Calibration leverages the measurement of stage output as supervision to address the gradient problem introduced by MAE and enhance the interlayer correlation with quantization parameters.","Comprehensive experiments demonstrate the effectiveness of RepAPQ across various models and tasks.","Our framework outperforms previous methods by approximately 1\\% for 8-bit PTQ and 2\\% for 6-bit PTQ, showcasing its superior performance.","The code is available at \\url{https://github.com/ilur98/DLMC-QUANT}."],"url":"http://arxiv.org/abs/2402.16121v1","category":"cs.CV"}
{"created":"2024-02-25 15:38:05","title":"Zhelobenko-Stern formulas and $B_n$ Toda wave functions","abstract":"Using Zhelobenko-Stern formulas for the action of the generators of orthogonal Lie algebra in corresponding Gelfand-Tsetlin basis, we derive Mellin-Barnes presentations for the wave functions of $B_n$ Toda lattice. They are in accordance with Iorgov-Shadura formulas.","sentences":["Using Zhelobenko-Stern formulas for the action of the generators of orthogonal Lie algebra in corresponding Gelfand-Tsetlin basis, we derive Mellin-Barnes presentations for the wave functions of $B_n$ Toda lattice.","They are in accordance with Iorgov-Shadura formulas."],"url":"http://arxiv.org/abs/2402.16120v1","category":"math.RT"}
{"created":"2024-02-25 15:37:14","title":"DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control","abstract":"This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge. DeepForge uses an architecture that combines 1D convolutional neural networks and gated recurrent units. It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging. The paper also details DeepForge's architecture and the finite element simulation model used to generate the data set, using a three-stroke forging process. The results demonstrate DeepForge's ability to predict microstructure with a mean absolute error of 0.4$\\pm$0.3%. In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece. These results are then verified experimentally, demonstrating a significant step towards improved control and quality in forging processes where temperature can be used as an additional degree of freedom in the process.","sentences":["This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge.","DeepForge uses an architecture that combines 1D convolutional neural networks and gated recurrent units.","It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging.","The paper also details DeepForge's architecture and the finite element simulation model used to generate the data set, using a three-stroke forging process.","The results demonstrate DeepForge's ability to predict microstructure with a mean absolute error of 0.4$\\pm$0.3%.","In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece.","These results are then verified experimentally, demonstrating a significant step towards improved control and quality in forging processes where temperature can be used as an additional degree of freedom in the process."],"url":"http://arxiv.org/abs/2402.16119v1","category":"cs.LG"}
{"created":"2024-02-25 15:31:43","title":"RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis","abstract":"Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.","sentences":["Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI.","Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios.","In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms.","To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning.","Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task."],"url":"http://arxiv.org/abs/2402.16117v1","category":"cs.RO"}
{"created":"2024-02-25 15:29:28","title":"On Performance of RIS-Aided Fluid Antenna Systems","abstract":"This letter studies the performance of reconfigurable intelligent surface (RIS)-aided communications for a fluid antenna system (FAS) enabled receiver. Specifically, a fixed singleantenna base station (BS) transmits information through a RIS to a mobile user (MU) which is equipped with a planar fluid antenna in the absence of a direct link.We first analyze the spatial correlation structures among the positions (or ports) in the planar FAS, and then derive the joint distribution of the equivalent channel gain at the user by exploiting the central limit theorem. Furthermore, we obtain compact analytical expressions for the outage probability (OP) and delay outage rate (DOR). Numerical results illustrate that using FAS with only one activated port into the RIS-aided communication network can greatly enhance the performance, when compared to traditional antenna systems (TAS).","sentences":["This letter studies the performance of reconfigurable intelligent surface (RIS)-aided communications for a fluid antenna system (FAS) enabled receiver.","Specifically, a fixed singleantenna base station (BS) transmits information through a RIS to a mobile user (MU) which is equipped with a planar fluid antenna in the absence of a direct link.","We first analyze the spatial correlation structures among the positions (or ports) in the planar FAS, and then derive the joint distribution of the equivalent channel gain at the user by exploiting the central limit theorem.","Furthermore, we obtain compact analytical expressions for the outage probability (OP) and delay outage rate (DOR).","Numerical results illustrate that using FAS with only one activated port into the RIS-aided communication network can greatly enhance the performance, when compared to traditional antenna systems (TAS)."],"url":"http://arxiv.org/abs/2402.16116v1","category":"cs.IT"}
{"created":"2024-02-25 15:28:11","title":"General Shiba mapping for on-site four-point correlation functions","abstract":"By applying the Shiba mapping on the two particle level, we derive the relation between the local four-point correlation functions of bipartite lattice models with on-site electronic repulsion and those of the corresponding models with attractive interaction in the most general setting. In particular, we extend the results of [Phys. Rev. B, 101, 155148 (2020)], which were limited to the rather specific situation of the static limit in strictly particle-hole symmetric models, (i) by explicitly including both magnetic field and different values of the chemical potentials, and (ii) by considering the full dependence of the generalized susceptibilities on the transfer (bosonic) Matsubara frequency. The derived formalism is then applied, as a relevant benchmark, to the Hubbard atom, by investigating the general properties of the divergences of its irreducible vertex functions as a function of chemical potential and applied magnetic field. The resulting phase-diagrams provide an insightful compass for future studies of the breakdown of the self-consistent perturbation expansion beyond high-symmetric regimes.","sentences":["By applying the Shiba mapping on the two particle level, we derive the relation between the local four-point correlation functions of bipartite lattice models with on-site electronic repulsion and those of the corresponding models with attractive interaction in the most general setting.","In particular, we extend the results of [Phys. Rev. B, 101, 155148 (2020)], which were limited to the rather specific situation of the static limit in strictly particle-hole symmetric models, (i) by explicitly including both magnetic field and different values of the chemical potentials, and (ii) by considering the full dependence of the generalized susceptibilities on the transfer (bosonic)","Matsubara frequency.","The derived formalism is then applied, as a relevant benchmark, to the Hubbard atom, by investigating the general properties of the divergences of its irreducible vertex functions as a function of chemical potential and applied magnetic field.","The resulting phase-diagrams provide an insightful compass for future studies of the breakdown of the self-consistent perturbation expansion beyond high-symmetric regimes."],"url":"http://arxiv.org/abs/2402.16115v1","category":"cond-mat.str-el"}
{"created":"2024-02-25 15:26:16","title":"Model-based Manipulation of Deformable Objects with Non-negligible Dynamics as Shape Regulation","abstract":"Model-based manipulation of deformable objects has traditionally dealt with objects in the quasi-static regimes, either because they are extremely lightweight/small or constrained to move very slowly. On the contrary, soft robotic research has made considerable strides toward general modeling and control - despite soft robots and deformable linear objects being very similar from a mechanical standpoint. In this work, we leverage these recent results to develop a fully dynamic framework of slender deformable objects grasped at one of their ends by a robotic manipulator. We introduce a dynamic model of this system using functional strain parameterizations and describe the manipulation challenge as a regulation control problem. This enables us to define a fully model-based control architecture, for which we can prove analytically closed-loop stability and provide sufficient conditions for steady state convergence to the desired manipulation state. The nature of this work is intended to be markedly experimental. We propose an extensive experimental validation of the proposed ideas. For that, we use a 7-DoF robot tasked with the goal of positioning the distal end of six different electric cables, moving on a plane, in a given position and orientation in space.","sentences":["Model-based manipulation of deformable objects has traditionally dealt with objects in the quasi-static regimes, either because they are extremely lightweight/small or constrained to move very slowly.","On the contrary, soft robotic research has made considerable strides toward general modeling and control - despite soft robots and deformable linear objects being very similar from a mechanical standpoint.","In this work, we leverage these recent results to develop a fully dynamic framework of slender deformable objects grasped at one of their ends by a robotic manipulator.","We introduce a dynamic model of this system using functional strain parameterizations and describe the manipulation challenge as a regulation control problem.","This enables us to define a fully model-based control architecture, for which we can prove analytically closed-loop stability and provide sufficient conditions for steady state convergence to the desired manipulation state.","The nature of this work is intended to be markedly experimental.","We propose an extensive experimental validation of the proposed ideas.","For that, we use a 7-DoF robot tasked with the goal of positioning the distal end of six different electric cables, moving on a plane, in a given position and orientation in space."],"url":"http://arxiv.org/abs/2402.16114v1","category":"cs.RO"}
{"created":"2024-02-25 15:25:18","title":"Thermodynamics of the Einstein-Maxwell system","abstract":"At first glance, thermodynamic properties of gravity with asymptotically AdS conditions and those with box boundary conditions, where the spatial section of the boundary is a sphere of finite radius, appear similar. Both exhibit a similar phase structure and Hawking-Page phase transition. However, when we introduce a U(1) gauge field to the system, discrepancies in thermodynamic properties between the two cases has been reported in [7] (JHEP 11 (2016) 041). In this paper, by accepting the assumption that all Euclidean saddles contribute to the partition function, I found that these discrepancies are resolved due to the contribution from the \"bag of gold (BG),\" which is the class of Euclidean geometries whose area of bolt is bigger than that of the boundary. As a result, the Hawking-Page phase structure is restored, with the unexpected properties that the upper bound of thermodynamic entropy is always larger than the boundary area divided by 4G when the chemical potential is non-zero, and that such high entropy states are realized at sufficiently high temperature.","sentences":["At first glance, thermodynamic properties of gravity with asymptotically AdS conditions and those with box boundary conditions, where the spatial section of the boundary is a sphere of finite radius, appear similar.","Both exhibit a similar phase structure and Hawking-Page phase transition.","However, when we introduce a U(1) gauge field to the system, discrepancies in thermodynamic properties between the two cases has been reported in [7] (JHEP 11 (2016) 041).","In this paper, by accepting the assumption that all Euclidean saddles contribute to the partition function, I found that these discrepancies are resolved due to the contribution from the \"bag of gold (BG),\" which is the class of Euclidean geometries whose area of bolt is bigger than that of the boundary.","As a result, the Hawking-Page phase structure is restored, with the unexpected properties that the upper bound of thermodynamic entropy is always larger than the boundary area divided by 4G when the chemical potential is non-zero, and that such high entropy states are realized at sufficiently high temperature."],"url":"http://arxiv.org/abs/2402.16113v1","category":"gr-qc"}
{"created":"2024-02-25 15:23:30","title":"Trees with flowers: A catalog of integer partition and integer composition trees with their asymptotic analysis","abstract":"We present families of combinatorial classes described as trees with nodes that can carry one of two types of ``flowers'': integer partitions or integer compositions. Two parameters on the flowers of trees will be considered: the number of ``petals'' in all the flowers (petals' weight) and the number of edges in the petals of all the flowers (flowers' weight). We give explicit expressions of their generating functions and deduce general formulas for the asymptotic growth of their coefficients and the expectations of their concentrated distributions.","sentences":["We present families of combinatorial classes described as trees with nodes that can carry one of two types of ``flowers'': integer partitions or integer compositions.","Two parameters on the flowers of trees will be considered: the number of ``petals'' in all the flowers (petals' weight) and the number of edges in the petals of all the flowers (flowers' weight).","We give explicit expressions of their generating functions and deduce general formulas for the asymptotic growth of their coefficients and the expectations of their concentrated distributions."],"url":"http://arxiv.org/abs/2402.16111v1","category":"math.CO"}
{"created":"2024-02-25 15:19:33","title":"Holographic Gauss-Bonnet transport","abstract":"We extend the computational framework of \\cite{Buchel:2023fst} to analysis of shear and bulk viscosities in generic strongly coupled holographic Gauss-Bonnet gauge theories. The finite Gauss-Bonnet coupling constant encodes holographic plasma with non-equal central charges $c\\ne a$ at the ultraviolet fixed point. In a simple model we discuss transport coefficients within the causality window $-\\frac12\\le \\frac{c-a}{c}\\le \\frac 12$ of the theory.","sentences":["We extend the computational framework of \\cite{Buchel:2023fst} to analysis of shear and bulk viscosities in generic strongly coupled holographic Gauss-Bonnet gauge theories.","The finite Gauss-Bonnet coupling constant encodes holographic plasma with non-equal central charges $c\\ne a$ at the ultraviolet fixed point.","In a simple model we discuss transport coefficients within the causality window $-\\frac12\\le \\frac{c-a}{c}\\le \\frac 12$ of the theory."],"url":"http://arxiv.org/abs/2402.16109v1","category":"hep-th"}
{"created":"2024-02-25 14:30:44","title":"Molecular Code-Division Multiple-Access: Signaling, Detection, and Performance","abstract":"To accomplish relatively complex tasks, in Internet of Bio-Nano Things (IoBNT), information collected by different nano-machines (NMs) is usually sent via multiple-access channels to fusion centers (FCs) for further processing. Relying on two types of molecules, in this paper, a molecular code-division multiple-access (MoCDMA) scheme is designed for multiple NMs to simultaneously send information to an access-point (AP) in a diffusive molecular communications (DMC) environment. We assume that different NMs may have different distances from AP, which generates `near-far' effect. Correspondingly, the uniform and channel-inverse based molecular emission schemes are proposed for NMs to emit information molecules. To facilitate the design of different signal detection schemes, the received signals by AP are represented in different forms. Specifically, by considering the limited computational power of nano-machines, three low-complexity detectors are designed in the principles of matched-filtering (MF), zero-forcing (ZF), and minimum mean-square error (MMSE). The noise characteristics in MoCDMA systems and the complexity of various detection schemes are analyzed. The error performance of the MoCDMA systems with various molecular emission and detection schemes is demonstrated and compared. Our studies and performance results demonstrate that MoCDMA constitutes a promising scheme for supporting multiple-access transmission in DMC, while the channel-inverse based transmission can ensure the fairness of communication qualities (FoCQ) among different NMs. Furthermore, different detection schemes may be implemented to attain a good trade-off between implementation complexity and communication reliability.","sentences":["To accomplish relatively complex tasks, in Internet of Bio-Nano Things (IoBNT), information collected by different nano-machines (NMs) is usually sent via multiple-access channels to fusion centers (FCs) for further processing.","Relying on two types of molecules, in this paper, a molecular code-division multiple-access (MoCDMA) scheme is designed for multiple NMs to simultaneously send information to an access-point (AP) in a diffusive molecular communications (DMC) environment.","We assume that different NMs may have different distances from AP, which generates `near-far' effect.","Correspondingly, the uniform and channel-inverse based molecular emission schemes are proposed for NMs to emit information molecules.","To facilitate the design of different signal detection schemes, the received signals by AP are represented in different forms.","Specifically, by considering the limited computational power of nano-machines, three low-complexity detectors are designed in the principles of matched-filtering (MF), zero-forcing (ZF), and minimum mean-square error (MMSE).","The noise characteristics in MoCDMA systems and the complexity of various detection schemes are analyzed.","The error performance of the MoCDMA systems with various molecular emission and detection schemes is demonstrated and compared.","Our studies and performance results demonstrate that MoCDMA constitutes a promising scheme for supporting multiple-access transmission in DMC, while the channel-inverse based transmission can ensure the fairness of communication qualities (FoCQ) among different NMs.","Furthermore, different detection schemes may be implemented to attain a good trade-off between implementation complexity and communication reliability."],"url":"http://arxiv.org/abs/2402.16097v1","category":"cs.IT"}
{"created":"2024-02-25 13:53:49","title":"StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention","abstract":"Utilizing large-scale pretrained models is a well-known strategy to enhance performance on various target tasks. It is typically achieved through fine-tuning pretrained models on target tasks. However, na\\\"{\\i}ve fine-tuning may not fully leverage knowledge embedded in pretrained models. In this study, we introduce a novel fine-tuning method, called stochastic cross-attention (StochCA), specific to Transformer architectures. This method modifies the Transformer's self-attention mechanism to selectively utilize knowledge from pretrained models during fine-tuning. Specifically, in each block, instead of self-attention, cross-attention is performed stochastically according to the predefined probability, where keys and values are extracted from the corresponding block of a pretrained model. By doing so, queries and channel-mixing multi-layer perceptron layers of a target model are fine-tuned to target tasks to learn how to effectively exploit rich representations of pretrained models. To verify the effectiveness of StochCA, extensive experiments are conducted on benchmarks in the areas of transfer learning and domain generalization, where the exploitation of pretrained models is critical. Our experimental results show the superiority of StochCA over state-of-the-art approaches in both areas. Furthermore, we demonstrate that StochCA is complementary to existing approaches, i.e., it can be combined with them to further improve performance. Our code is available at https://github.com/daintlab/stochastic_cross_attention","sentences":["Utilizing large-scale pretrained models is a well-known strategy to enhance performance on various target tasks.","It is typically achieved through fine-tuning pretrained models on target tasks.","However, na\\\"{\\i}ve fine-tuning may not fully leverage knowledge embedded in pretrained models.","In this study, we introduce a novel fine-tuning method, called stochastic cross-attention (StochCA), specific to Transformer architectures.","This method modifies the Transformer's self-attention mechanism to selectively utilize knowledge from pretrained models during fine-tuning.","Specifically, in each block, instead of self-attention, cross-attention is performed stochastically according to the predefined probability, where keys and values are extracted from the corresponding block of a pretrained model.","By doing so, queries and channel-mixing multi-layer perceptron layers of a target model are fine-tuned to target tasks to learn how to effectively exploit rich representations of pretrained models.","To verify the effectiveness of StochCA, extensive experiments are conducted on benchmarks in the areas of transfer learning and domain generalization, where the exploitation of pretrained models is critical.","Our experimental results show the superiority of StochCA over state-of-the-art approaches in both areas.","Furthermore, we demonstrate that StochCA is complementary to existing approaches, i.e., it can be combined with them to further improve performance.","Our code is available at https://github.com/daintlab/stochastic_cross_attention"],"url":"http://arxiv.org/abs/2402.16092v1","category":"cs.CV"}
{"created":"2024-02-25 13:37:53","title":"Bayesian Neural Network For Personalized Federated Learning Parameter Selection","abstract":"Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field. Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data. One of such approach involves personalizing specific layers of neural networks. However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting. In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization. To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters. Finally, we validate our algorithm's efficacy on several real-world datasets, demonstrating that our proposed approach outperforms existing baselines.","sentences":["Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field.","Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data.","One of such approach involves personalizing specific layers of neural networks.","However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting.","In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization.","To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters.","Finally, we validate our algorithm's efficacy on several real-world datasets, demonstrating that our proposed approach outperforms existing baselines."],"url":"http://arxiv.org/abs/2402.16091v1","category":"cs.LG"}
{"created":"2024-02-25 13:37:36","title":"Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis","abstract":"This study provides a comprehensive benchmark framework for Source-Free Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to achieve a rigorous empirical understanding of the complex relationships between multiple key design factors in SF-UDA methods. The study empirically examines a diverse set of SF-UDA techniques, assessing their consistency across datasets, sensitivity to specific hyperparameters, and applicability across different families of backbone architectures. Moreover, it exhaustively evaluates pre-training datasets and strategies, particularly focusing on both supervised and self-supervised methods, as well as the impact of fine-tuning on the source domain. Our analysis also highlights gaps in existing benchmark practices, guiding SF-UDA research towards more effective and general approaches. It emphasizes the importance of backbone architecture and pre-training dataset selection on SF-UDA performance, serving as an essential reference and providing key insights. Lastly, we release the source code of our experimental framework. This facilitates the construction, training, and testing of SF-UDA methods, enabling systematic large-scale experimental analysis and supporting further research efforts in this field.","sentences":["This study provides a comprehensive benchmark framework for Source-Free Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to achieve a rigorous empirical understanding of the complex relationships between multiple key design factors in SF-UDA methods.","The study empirically examines a diverse set of SF-UDA techniques, assessing their consistency across datasets, sensitivity to specific hyperparameters, and applicability across different families of backbone architectures.","Moreover, it exhaustively evaluates pre-training datasets and strategies, particularly focusing on both supervised and self-supervised methods, as well as the impact of fine-tuning on the source domain.","Our analysis also highlights gaps in existing benchmark practices, guiding SF-UDA research towards more effective and general approaches.","It emphasizes the importance of backbone architecture and pre-training dataset selection on SF-UDA performance, serving as an essential reference and providing key insights.","Lastly, we release the source code of our experimental framework.","This facilitates the construction, training, and testing of SF-UDA methods, enabling systematic large-scale experimental analysis and supporting further research efforts in this field."],"url":"http://arxiv.org/abs/2402.16090v1","category":"cs.CV"}
{"created":"2024-02-25 13:22:17","title":"Deep Homography Estimation for Visual Place Recognition","abstract":"Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without additional homography labels, which can also be jointly trained with the backbone network to help it extract the features that are more suitable for local matching. Extensive experiments on benchmark datasets show that our method can outperform several state-of-the-art methods. And it is more than one order of magnitude faster than the mainstream hierarchical VPR methods using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.","sentences":["Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality.","Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency.","They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking.","However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable.","This makes existing methods compromise to train the network only in global feature extraction.","Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification.","Moreover, we design a re-projection error of inliers loss to train the DHE network without additional homography labels, which can also be jointly trained with the backbone network to help it extract the features that are more suitable for local matching.","Extensive experiments on benchmark datasets show that our method can outperform several state-of-the-art methods.","And it is more than one order of magnitude faster than the mainstream hierarchical VPR methods using RANSAC.","The code is released at https://github.com/Lu-Feng/DHE-VPR."],"url":"http://arxiv.org/abs/2402.16086v1","category":"cs.CV"}
{"created":"2024-02-25 13:20:28","title":"Online Drone Scheduling for Last-mile Delivery","abstract":"Delivering a parcel from the distribution hub to the customer's doorstep is called the \\textit{last-mile delivery} step in delivery logistics. In this paper, we study a hybrid {\\it truck-drones} model for the last-mile delivery step, in which a truck moves on a predefined path carrying parcels and drones deliver the parcels. We define the \\textsc{online drone scheduling} problem, where the truck moves in a predefined path, and the customer's requests appear online during the truck's movement. The objective is to schedule a drone associated with every request to minimize the number of drones used subject to the battery budget of the drones and compatibility of the schedules. We propose a 3-competitive deterministic algorithm using the next-fit strategy and 2.7-competitive algorithms using the first-fit strategy for the problem with $O(\\log n)$ worst-case time complexity per request, where $n$ is the maximum number of active requests at any time. We also introduce \\textsc{online variable-size drone scheduling} problem (OVDS). Here, we know all the customer's requests in advance; however, the drones with different battery capacities appear online. The objective is to schedule customers' requests for drones to minimize the number of drones used. We propose a $(2\\alpha + 1)$-competitive algorithm for the OVDS problem with total running time $O(n \\log n)$ for $n$ customer requests, where $\\alpha$ is the ratio of the maximum battery capacity to the minimum battery capacity of the drones. Finally, we address how to generate intervals corresponding to each customer request when there are discrete stopping points on the truck's route, from where the drone can fly and meet with the truck.","sentences":["Delivering a parcel from the distribution hub to the customer's doorstep is called the \\textit{last-mile delivery} step in delivery logistics.","In this paper, we study a hybrid {\\it truck-drones} model for the last-mile delivery step, in which a truck moves on a predefined path carrying parcels and drones deliver the parcels.","We define the \\textsc{online drone scheduling} problem, where the truck moves in a predefined path, and the customer's requests appear online during the truck's movement.","The objective is to schedule a drone associated with every request to minimize the number of drones used subject to the battery budget of the drones and compatibility of the schedules.","We propose a 3-competitive deterministic algorithm using the next-fit strategy and 2.7-competitive algorithms using the first-fit strategy for the problem with $O(\\log n)$ worst-case time complexity per request, where $n$ is the maximum number of active requests at any time.","We also introduce \\textsc{online variable-size drone scheduling} problem (OVDS).","Here, we know all the customer's requests in advance; however, the drones with different battery capacities appear online.","The objective is to schedule customers' requests for drones to minimize the number of drones used.","We propose a $(2\\alpha + 1)$-competitive algorithm for the OVDS problem with total running time $O(n \\log n)$ for $n$ customer requests, where $\\alpha$ is the ratio of the maximum battery capacity to the minimum battery capacity of the drones.","Finally, we address how to generate intervals corresponding to each customer request when there are discrete stopping points on the truck's route, from where the drone can fly and meet with the truck."],"url":"http://arxiv.org/abs/2402.16085v1","category":"cs.DS"}
{"created":"2024-02-25 13:13:59","title":"HPE Transformer: Learning to Optimize Multi-Group Multicast Beamforming Under Nonconvex QoS Constraints","abstract":"This paper studies the quality-of-service (QoS) constrained multi-group multicast beamforming design problem, where each multicast group is composed of a number of users requiring the same content. Due to the nonconvex QoS constraints, this problem is nonconvex and NP-hard. While existing optimization-based iterative algorithms can obtain a suboptimal solution, their iterative nature results in large computational complexity and delay. To facilitate real-time implementations, this paper proposes a deep learning-based approach, which consists of a beamforming structure assisted problem transformation and a customized neural network architecture named hierarchical permutation equivariance (HPE) transformer. The proposed HPE transformer is proved to be permutation equivariant with respect to the users within each multicast group, and also permutation equivariant with respect to different multicast groups. Simulation results demonstrate that the proposed HPE transformer outperforms state-of-the-art optimization-based and deep learning-based approaches for multi-group multicast beamforming design in terms of the total transmit power, the constraint violation, and the computational time. In addition, the proposed HPE transformer achieves pretty good generalization performance on different numbers of users, different numbers of multicast groups, and different signal-to-interference-plus-noise ratio targets.","sentences":["This paper studies the quality-of-service (QoS) constrained multi-group multicast beamforming design problem, where each multicast group is composed of a number of users requiring the same content.","Due to the nonconvex QoS constraints, this problem is nonconvex and NP-hard.","While existing optimization-based iterative algorithms can obtain a suboptimal solution, their iterative nature results in large computational complexity and delay.","To facilitate real-time implementations, this paper proposes a deep learning-based approach, which consists of a beamforming structure assisted problem transformation and a customized neural network architecture named hierarchical permutation equivariance (HPE) transformer.","The proposed HPE transformer is proved to be permutation equivariant with respect to the users within each multicast group, and also permutation equivariant with respect to different multicast groups.","Simulation results demonstrate that the proposed HPE transformer outperforms state-of-the-art optimization-based and deep learning-based approaches for multi-group multicast beamforming design in terms of the total transmit power, the constraint violation, and the computational time.","In addition, the proposed HPE transformer achieves pretty good generalization performance on different numbers of users, different numbers of multicast groups, and different signal-to-interference-plus-noise ratio targets."],"url":"http://arxiv.org/abs/2402.16081v1","category":"cs.IT"}
{"created":"2024-02-25 13:13:05","title":"Generalised Soft Finite Element Method for Elliptic Eigenvalue Problems","abstract":"The recently proposed soft finite element method (SoftFEM) reduces the stiffness (condition numbers), consequently improving the overall approximation accuracy. The method subtracts a least-square term that penalizes the gradient jumps across mesh interfaces from the FEM stiffness bilinear form while maintaining the system's coercivity. Herein, we present two generalizations for SoftFEM that aim to improve the approximation accuracy and further reduce the discrete systems' stiffness. Firstly and most naturally, we generalize SoftFEM by adding a least-square term to the mass bilinear form. Superconvergent results of rates $h^6$ and $h^8$ for eigenvalues are established for linear uniform elements; $h^8$ is the highest order of convergence known in the literature. Secondly, we generalize SoftFEM by applying the blended Gaussian-type quadratures. We demonstrate further reductions in stiffness compared to traditional FEM and SoftFEM. The coercivity and analysis of the optimal error convergences follow the work of SoftFEM. Thus, this paper focuses on the numerical study of these generalizations. For linear and uniform elements, analytical eigenpairs, exact eigenvalue errors, and superconvergent error analysis are established. Various numerical examples demonstrate the potential of generalized SoftFEMs for spectral approximation, particularly in high-frequency regimes.","sentences":["The recently proposed soft finite element method (SoftFEM) reduces the stiffness (condition numbers), consequently improving the overall approximation accuracy.","The method subtracts a least-square term that penalizes the gradient jumps across mesh interfaces from the FEM stiffness bilinear form while maintaining the system's coercivity.","Herein, we present two generalizations for SoftFEM that aim to improve the approximation accuracy and further reduce the discrete systems' stiffness.","Firstly and most naturally, we generalize SoftFEM by adding a least-square term to the mass bilinear form.","Superconvergent results of rates $h^6$ and $h^8$ for eigenvalues are established for linear uniform elements; $h^8$ is the highest order of convergence known in the literature.","Secondly, we generalize SoftFEM by applying the blended Gaussian-type quadratures.","We demonstrate further reductions in stiffness compared to traditional FEM and SoftFEM.","The coercivity and analysis of the optimal error convergences follow the work of SoftFEM.","Thus, this paper focuses on the numerical study of these generalizations.","For linear and uniform elements, analytical eigenpairs, exact eigenvalue errors, and superconvergent error analysis are established.","Various numerical examples demonstrate the potential of generalized SoftFEMs for spectral approximation, particularly in high-frequency regimes."],"url":"http://arxiv.org/abs/2402.16080v1","category":"math.NA"}
{"created":"2024-02-25 12:40:42","title":"Equivariant Frames and the Impossibility of Continuous Canonicalization","abstract":"Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures. Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements. In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged. In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function. To address this fundamental robustness problem, we formally define and construct \\emph{weighted} frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the actions of $SO(2)$, $SO(3)$, and $S_n$ on point clouds.","sentences":["Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures.","Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements.","In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged.","In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function.","To address this fundamental robustness problem, we formally define and construct \\emph{weighted} frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the actions of $SO(2)$, $SO(3)$, and $S_n$ on point clouds."],"url":"http://arxiv.org/abs/2402.16077v1","category":"cs.LG"}
{"created":"2024-02-26 10:02:29","title":"D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection","abstract":"Swear words are a common proxy to collect datasets with cyberbullying incidents. Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies. After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance. We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies. We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation. Our quantitative and qualitative analyses demonstrate its generalisability to unseen data.","sentences":["Swear words are a common proxy to collect datasets with cyberbullying incidents.","Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies.","After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance.","We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies.","We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation.","Our quantitative and qualitative analyses demonstrate its generalisability to unseen data."],"url":"http://arxiv.org/abs/2402.16458v1","category":"cs.CL"}
{"created":"2024-02-26 09:53:20","title":"WetLinks: a Large-Scale Longitudinal Starlink Dataset with Contiguous Weather Data","abstract":"Low Orbit Satellite (LEO) networks such as Starlink promise Internet access everywhere around the world. In this paper, we present WetLinks - a large and publicly available trace-based dataset of Starlink measurements. The measurements were concurrently collected from two European vantage points over a span of six months. Consisting of approximately 140,000 measurements, the dataset comprises all relevant network parameters such as the upload and download throughputs, the RTT, packet loss, and traceroutes. We further augment the dataset with concurrent data from professional weather stations placed next to both Starlink terminals. Based on our dataset, we analyse Starlink performance, including its susceptibility to weather conditions. We use this to validate our dataset by replicating the results of earlier smaller-scale studies. We release our datasets and all accompanying tooling as open data. To the best of our knowledge, ours is the largest Starlink dataset to date.","sentences":["Low Orbit Satellite (LEO) networks such as Starlink promise Internet access everywhere around the world.","In this paper, we present WetLinks - a large and publicly available trace-based dataset of Starlink measurements.","The measurements were concurrently collected from two European vantage points over a span of six months.","Consisting of approximately 140,000 measurements, the dataset comprises all relevant network parameters such as the upload and download throughputs, the RTT, packet loss, and traceroutes.","We further augment the dataset with concurrent data from professional weather stations placed next to both Starlink terminals.","Based on our dataset, we analyse Starlink performance, including its susceptibility to weather conditions.","We use this to validate our dataset by replicating the results of earlier smaller-scale studies.","We release our datasets and all accompanying tooling as open data.","To the best of our knowledge, ours is the largest Starlink dataset to date."],"url":"http://arxiv.org/abs/2402.16448v1","category":"cs.NI"}
{"created":"2024-02-26 09:17:22","title":"Measurement of beauty-quark production in pp collisions at $\\sqrt{s}=13$ TeV via non-prompt D mesons","abstract":"The $p_{\\rm T}$-differential production cross sections of non-prompt ${\\rm D^0}$, ${\\rm D^+}$, and ${\\rm D_s^+}$ mesons originating from beauty-hadron decays are measured in proton$-$proton collisions at a centre-of-mass energy $\\sqrt{s}$ of 13 TeV. The measurements are performed at midrapidity, $|y| < 0.5$, with the data sample collected by ALICE from 2016 to 2018. The results are in agreement with predictions from several perturbative QCD calculations. The fragmentation fraction of beauty quarks to strange mesons divided by the one to non-strange mesons, $f_{\\rm{s}}/(f_{\\rm{u}} + f_{\\rm{d}})$, is found to be $0.114 \\pm 0.016~{\\rm (stat.)} \\pm 0.006~{\\rm (syst.)} \\pm 0.003~{\\rm (BR)} \\pm 0.003~{\\rm (extrap.)}$. This value is compatible with previous measurements at lower centre-of-mass energies and in different collision systems in agreement with the assumption of universality of fragmentation functions. In addition, the dependence of the non-prompt D meson production on the centre-of-mass energy is investigated by comparing the results obtained at $\\sqrt{s} = 5.02$ and 13 TeV, showing a hardening of the non-prompt D-meson $p_{\\rm T}$-differential production cross section at higher $\\sqrt{s}$. Finally, the ${\\rm b\\overline{b}}$ production cross section per unit of rapidity at midrapidity is calculated from the non-prompt ${\\rm D^0}$, ${\\rm D^+}$, ${\\rm D_s^+}$, and $\\Lambda_{\\rm c}^+$ hadron measurements, obtaining ${\\rm d}\\sigma/{\\rm d}y = 75.2\\pm 3.2~(\\mathrm{stat.}) \\pm 5.2~(\\mathrm{syst.})^{+12.3}_{-3.2} ~(\\mathrm{extrap.})\\text{ } \\rm \\mu b \\;.$","sentences":["The $p_{\\rm T}$-differential production cross sections of non-prompt ${\\rm D^0}$, ${\\rm D^+}$, and ${\\rm D_s^+}$ mesons originating from beauty-hadron decays are measured in proton$-$proton collisions at a centre-of-mass energy $\\sqrt{s}$ of 13 TeV. The measurements are performed at midrapidity, $|y| < 0.5$, with the data sample collected by ALICE from 2016 to 2018.","The results are in agreement with predictions from several perturbative QCD calculations.","The fragmentation fraction of beauty quarks to strange mesons divided by the one to non-strange mesons, $f_{\\rm{s}}/(f_{\\rm{u}} + f_{\\rm{d}})$, is found to be $0.114 \\pm 0.016~{\\rm (stat.)","} \\pm 0.006~{\\rm (syst.)}","\\pm 0.003~{\\rm (BR)} \\pm 0.003~{\\rm (extrap.)}$.","This value is compatible with previous measurements at lower centre-of-mass energies and in different collision systems in agreement with the assumption of universality of fragmentation functions.","In addition, the dependence of the non-prompt D meson production on the centre-of-mass energy is investigated by comparing the results obtained at $\\sqrt{s} = 5.02$ and 13 TeV, showing a hardening of the non-prompt D-meson $p_{\\rm T}$-differential production cross section at higher $\\sqrt{s}$. Finally, the ${\\rm b\\overline{b}}$ production cross section per unit of rapidity at midrapidity is calculated from the non-prompt ${\\rm D^0}$, ${\\rm D^+}$, ${\\rm D_s^+}$, and $\\Lambda_{\\rm c}^+$ hadron measurements, obtaining ${\\rm d}\\sigma/{\\rm d}y = 75.2\\pm 3.2~(\\mathrm{stat.})","\\pm 5.2~(\\mathrm{syst.})^{+12.3}_{-3.2} ~(\\mathrm{extrap.})\\text{ } \\rm \\mu b \\;.$"],"url":"http://arxiv.org/abs/2402.16417v1","category":"hep-ex"}
{"created":"2024-02-26 07:52:42","title":"Valuing insurance against small probability risks: A meta-analysis","abstract":"The demand for voluntary insurance against low-probability, high-impact risks is lower than expected. To assess the magnitude of the demand, we conduct a meta-analysis of contingent valuation studies using a dataset of experimentally elicited and survey-based estimates. We find that the average stated willingness to pay (WTP) for insurance is 87% of expected losses. We perform a meta-regression analysis to examine the heterogeneity in aggregate WTP across these studies. The meta-regression reveals that information about loss probability and probability levels positively influence relative willingness to pay, whereas respondents' average income and age have a negative effect. Moreover, we identify cultural sub-factors, such as power distance and uncertainty avoidance, that provided additional explanations for differences in WTP across international samples. Methodological factors related to the sampling and data collection process significantly influence the stated WTP. Our results, robust to model specification and publication bias, are relevant to current debates on stated preferences for low-probability risks management.","sentences":["The demand for voluntary insurance against low-probability, high-impact risks is lower than expected.","To assess the magnitude of the demand, we conduct a meta-analysis of contingent valuation studies using a dataset of experimentally elicited and survey-based estimates.","We find that the average stated willingness to pay (WTP) for insurance is 87% of expected losses.","We perform a meta-regression analysis to examine the heterogeneity in aggregate WTP across these studies.","The meta-regression reveals that information about loss probability and probability levels positively influence relative willingness to pay, whereas respondents' average income and age have a negative effect.","Moreover, we identify cultural sub-factors, such as power distance and uncertainty avoidance, that provided additional explanations for differences in WTP across international samples.","Methodological factors related to the sampling and data collection process significantly influence the stated WTP.","Our results, robust to model specification and publication bias, are relevant to current debates on stated preferences for low-probability risks management."],"url":"http://arxiv.org/abs/2402.16375v1","category":"q-fin.RM"}
{"created":"2024-02-26 07:00:24","title":"Boosting Graph Pooling with Persistent Homology","abstract":"Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.","sentences":["Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power.","However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability.","In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner.","In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance.","Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility."],"url":"http://arxiv.org/abs/2402.16346v1","category":"cs.LG"}
{"created":"2024-02-26 06:08:25","title":"Achieving $\\tilde{O}(1/\u03b5)$ Sample Complexity for Constrained Markov Decision Process","abstract":"We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\\frac{\\kappa}{\\epsilon}\\cdot\\log^2(1/\\epsilon))$ sample complexity bound, with $\\kappa$ being a problem-dependent parameter, yet independent of $\\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \\textit{adaptive} remaining resource capacities. The key elements of our algorithm are: i). an eliminating procedure that characterizes one optimal basis of the primal LP, and; ii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.","sentences":["We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making.","In this problem, we are given finite resources and a MDP with unknown transition probabilities.","At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time.","In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems.","We derive a logarithmic regret bound, which translates into a $O(\\frac{\\kappa}{\\epsilon}\\cdot\\log^2(1/\\epsilon))$ sample complexity bound, with $\\kappa$ being a problem-dependent parameter, yet independent of $\\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems.","To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \\textit{adaptive} remaining resource capacities.","The key elements of our algorithm are: i).","an eliminating procedure that characterizes one optimal basis of the primal LP, and; ii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis."],"url":"http://arxiv.org/abs/2402.16324v1","category":"cs.LG"}
{"created":"2024-02-26 05:27:41","title":"Collective Evaluation Problem","abstract":"This study focuses on situations where a finite set of alternatives is evaluated by collecting evaluations from several individuals, some of whom may not evaluate specific alternatives. The collection of subsets of alternatives that individuals (can) evaluate is referred to as an evaluability profile. For a given evaluability profile, we define a collective evaluation function whose inputs are the evaluation orders of individuals on the subsets of alternatives that they evaluate. We investigate the properties of collective evaluation functions, which are modifications of those introduced in previous studies. We identify the necessary and sufficient conditions on the evaluability profile that ensure the existence of collective evaluation functions satisfying four different combinations of these properties.","sentences":["This study focuses on situations where a finite set of alternatives is evaluated by collecting evaluations from several individuals, some of whom may not evaluate specific alternatives.","The collection of subsets of alternatives that individuals (can) evaluate is referred to as an evaluability profile.","For a given evaluability profile, we define a collective evaluation function whose inputs are the evaluation orders of individuals on the subsets of alternatives that they evaluate.","We investigate the properties of collective evaluation functions, which are modifications of those introduced in previous studies.","We identify the necessary and sufficient conditions on the evaluability profile that ensure the existence of collective evaluation functions satisfying four different combinations of these properties."],"url":"http://arxiv.org/abs/2402.16309v1","category":"econ.TH"}
{"created":"2024-02-26 02:09:36","title":"Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices","abstract":"This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.","sentences":["This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video.","By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform.","For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized.","The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds.","This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices.","It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities."],"url":"http://arxiv.org/abs/2402.16246v1","category":"cs.CV"}
{"created":"2024-02-25 20:43:55","title":"Language Models for Code Completion: A Practical Evaluation","abstract":"Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the model's predictions, we found that 66.3% of failures were due to the models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.","sentences":["Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data.","This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code.","We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models.","We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions.","These models were then evaluated using six standard metrics across twelve programming languages.","Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance.","A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies.","Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java.","InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives.","Our study also revealed that offline evaluations do not accurately reflect real-world scenarios.","Upon qualitative analysis of the model's predictions, we found that 66.3% of failures were due to the models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote.","Given these findings, we propose several strategies to overcome the current limitations.","These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability."],"url":"http://arxiv.org/abs/2402.16197v1","category":"cs.SE"}
{"created":"2024-02-25 20:17:18","title":"Exploring gene content with pangenome gene graphs","abstract":"Motivation: The gene content regulates the biology of an organism. It varies between species and between individuals of the same species. Although tools have been developed to identify gene content changes in bacterial genomes, none is applicable to collections of large eukaryotic genomes such as the human pangenome.   Results: We developed pangene, a computational tool to identify gene orientation, gene order and gene copy-number changes in a collection of genomes. Pangene aligns a set of input protein sequences to the genomes, resolves redundancies between protein sequences and constructs a gene graph with each genome represented as a walk in the graph. It additionally finds subgraphs that encodes gene content changes. Applied to the human pangenome, pangene identifies known gene-level variations and reveals complex haplotypes that are not well studied before. Pangene also works with high-quality bacterial pangenome and reports similar numbers of core and accessory genes in comparison to existing tools.   Availability and implementation: Source code at https://github.com/lh3/pangene; pre-built pangene graphs can be downloaded from https://zenodo.org/records/8118576 and visualized at http://pangene.liheng.org.","sentences":["Motivation: The gene content regulates the biology of an organism.","It varies between species and between individuals of the same species.","Although tools have been developed to identify gene content changes in bacterial genomes, none is applicable to collections of large eukaryotic genomes such as the human pangenome.   ","Results:","We developed pangene, a computational tool to identify gene orientation, gene order and gene copy-number changes in a collection of genomes.","Pangene aligns a set of input protein sequences to the genomes, resolves redundancies between protein sequences and constructs a gene graph with each genome represented as a walk in the graph.","It additionally finds subgraphs that encodes gene content changes.","Applied to the human pangenome, pangene identifies known gene-level variations and reveals complex haplotypes that are not well studied before.","Pangene also works with high-quality bacterial pangenome and reports similar numbers of core and accessory genes in comparison to existing tools.   ","Availability and implementation: Source code at https://github.com/lh3/pangene; pre-built pangene graphs can be downloaded from https://zenodo.org/records/8118576 and visualized at http://pangene.liheng.org."],"url":"http://arxiv.org/abs/2402.16185v1","category":"q-bio.GN"}
{"created":"2024-02-25 20:08:58","title":"MoodCapture: Depression Detection Using In-the-Wild Smartphone Images","abstract":"MoodCapture presents a novel approach that assesses depression based on images automatically captured from the front-facing camera of smartphones as people go about their daily lives. We collect over 125,000 photos in the wild from N=177 participants diagnosed with major depressive disorder for 90 days. Images are captured naturalistically while participants respond to the PHQ-8 depression survey question: \\textit{``I have felt down, depressed, or hopeless''}. Our analysis explores important image attributes, such as angle, dominant colors, location, objects, and lighting. We show that a random forest trained with face landmarks can classify samples as depressed or non-depressed and predict raw PHQ-8 scores effectively. Our post-hoc analysis provides several insights through an ablation study, feature importance analysis, and bias assessment. Importantly, we evaluate user concerns about using MoodCapture to detect depression based on sharing photos, providing critical insights into privacy concerns that inform the future design of in-the-wild image-based mental health assessment tools.","sentences":["MoodCapture presents a novel approach that assesses depression based on images automatically captured from the front-facing camera of smartphones as people go about their daily lives.","We collect over 125,000 photos in the wild from N=177 participants diagnosed with major depressive disorder for 90 days.","Images are captured naturalistically while participants respond to the PHQ-8 depression survey question: \\textit{``I have felt down, depressed, or hopeless''}.","Our analysis explores important image attributes, such as angle, dominant colors, location, objects, and lighting.","We show that a random forest trained with face landmarks can classify samples as depressed or non-depressed and predict raw PHQ-8 scores effectively.","Our post-hoc analysis provides several insights through an ablation study, feature importance analysis, and bias assessment.","Importantly, we evaluate user concerns about using MoodCapture to detect depression based on sharing photos, providing critical insights into privacy concerns that inform the future design of in-the-wild image-based mental health assessment tools."],"url":"http://arxiv.org/abs/2402.16182v1","category":"cs.HC"}
{"created":"2024-02-25 17:13:03","title":"Experimental identification of force, velocity, and nematic order relationships in active nematic cell monolayers","abstract":"Cell alignment often forms nematic order, which can lead to anomalous collective cell flow due to the so-called active force. Although it is appreciated that cell migration is driven by traction force, a quantitative evaluation of the relationships between the traction force, the nematic patterning, and the cell flow velocity is still elusive. Here we have found that cellular traction force aligns almost perfectly and is proportional in amplitude to the gradient of the nematic order tensor, not only near the topological defects but also globally. Furthermore, the flow in the monolayer was best described by adding nonlinear forces and a diffusion term derived from symmetry considerations. These nonlinear active forces enhance density instability but suppress bending instability, explaining why cell accumulation and dispersion can occur in neural progenitor cell culture while their ordering pattern is stable.","sentences":["Cell alignment often forms nematic order, which can lead to anomalous collective cell flow due to the so-called active force.","Although it is appreciated that cell migration is driven by traction force, a quantitative evaluation of the relationships between the traction force, the nematic patterning, and the cell flow velocity is still elusive.","Here we have found that cellular traction force aligns almost perfectly and is proportional in amplitude to the gradient of the nematic order tensor, not only near the topological defects but also globally.","Furthermore, the flow in the monolayer was best described by adding nonlinear forces and a diffusion term derived from symmetry considerations.","These nonlinear active forces enhance density instability but suppress bending instability, explaining why cell accumulation and dispersion can occur in neural progenitor cell culture while their ordering pattern is stable."],"url":"http://arxiv.org/abs/2402.16151v1","category":"cond-mat.soft"}
{"created":"2024-02-25 16:11:32","title":"A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity","abstract":"Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results.","sentences":["Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system.","In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones.","This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way.","The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning.","The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results."],"url":"http://arxiv.org/abs/2402.16131v1","category":"cs.LG"}
{"created":"2024-02-25 15:11:58","title":"FuseChat: Knowledge Fusion of Chat Models","abstract":"While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \\textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \\textsc{FuseChat}. \\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning. Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely \\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and \\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains demonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5 (March)} and approaching \\texttt{Mixtral-8x7B-Instruct}. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/FuseLLM}.","sentences":["While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies.","An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training.","However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible.","Recently, \\textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training.","In this report, we extend the scalability and flexibility of the \\textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \\textsc{FuseChat}.","\\textsc{FuseChat} comprises two main stages.","Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning.","Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning.","We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely \\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and \\texttt{OpenChat-3.5-7B}.","Experimental results spanning various chat domains demonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5 (March)} and approaching \\texttt{Mixtral-8x7B-Instruct}.","Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/FuseLLM}."],"url":"http://arxiv.org/abs/2402.16107v1","category":"cs.CL"}
{"created":"2024-02-25 14:19:41","title":"chainBoost: A Secure Performance Booster for Blockchain-based Resource Markets","abstract":"Cryptocurrencies and blockchain technology provide an innovative model for reshaping digital services. Driven by the movement toward Web 3.0, recent systems started to provide distributed services, such as computation outsourcing or file storage, on top of the currency exchange medium. By allowing anyone to join and collect cryptocurrency payments for serving others, these systems create decentralized markets for trading digital resources. Yet, there is still a big gap between the promise of these markets and their practical viability. Existing initiatives are still early-stage and have already encountered security and efficiency obstacles. At the same time, existing work around promising ideas, specifically sidechains, fall short in exploiting their full potential in addressing these problems.   To bridge this gap, we propose chainBoost, a secure performance booster for decentralized resource markets. It expedites service related operations, reduces the blockchain size, and supports flexible service-payment exchange modalities at low overhead. At its core, chainBoost employs a sidechain, that has a (security and semantic) mutual-dependence with the mainchain, to which the system offloads heavy/frequent operations. To enable it, we develop a novel sidechain architecture composed of temporary and permanent blocks, a block suppression mechanism to prune the sidechain, a syncing protocol to permit arbitrary data exchange between the two chains, and an autorecovery protocol to support robustness and resilience. We analyze the security of chainBoost, and implement a proof-of-concept prototype for a distributed file storage market as a use case. For a market handling around 2000 transactions per round, our experiments show up to 11x improvement in throughput and 94\\% reduction in confirmation time. They also show that chainBoost can reduce the main blockchain size by around 90%.","sentences":["Cryptocurrencies and blockchain technology provide an innovative model for reshaping digital services.","Driven by the movement toward Web 3.0, recent systems started to provide distributed services, such as computation outsourcing or file storage, on top of the currency exchange medium.","By allowing anyone to join and collect cryptocurrency payments for serving others, these systems create decentralized markets for trading digital resources.","Yet, there is still a big gap between the promise of these markets and their practical viability.","Existing initiatives are still early-stage and have already encountered security and efficiency obstacles.","At the same time, existing work around promising ideas, specifically sidechains, fall short in exploiting their full potential in addressing these problems.   ","To bridge this gap, we propose chainBoost, a secure performance booster for decentralized resource markets.","It expedites service related operations, reduces the blockchain size, and supports flexible service-payment exchange modalities at low overhead.","At its core, chainBoost employs a sidechain, that has a (security and semantic) mutual-dependence with the mainchain, to which the system offloads heavy/frequent operations.","To enable it, we develop a novel sidechain architecture composed of temporary and permanent blocks, a block suppression mechanism to prune the sidechain, a syncing protocol to permit arbitrary data exchange between the two chains, and an autorecovery protocol to support robustness and resilience.","We analyze the security of chainBoost, and implement a proof-of-concept prototype for a distributed file storage market as a use case.","For a market handling around 2000 transactions per round, our experiments show up to 11x improvement in throughput and 94\\% reduction in confirmation time.","They also show that chainBoost can reduce the main blockchain size by around 90%."],"url":"http://arxiv.org/abs/2402.16095v1","category":"cs.CR"}
{"created":"2024-02-25 14:18:14","title":"Bistochastically private release of data streams with zero delay","abstract":"Although the bulk of the research in privacy and statistical disclosure control is designed for static data, more and more data are often collected as continuous streams, and extensions of popular privacy tools and models have been proposed for this scenario. However, most of these proposals require buffers, where incoming individuals are momentarily stored, anonymized, and then released following a delay, thus considering a data stream as a succession of batches while it is by nature continuous. Having a delay unavoidably alters data freshness but also, more critically, inordinately exerts constraints on what can be achieved in terms of protection and information preservation. By considering randomized response, and specifically its recent bistochastic extension, in the context of dynamic data, this paper proposes a protocol for the anonymization of data streams that achieves zero delay while exhibiting formal privacy guarantees. Using a new tool in the privacy literature that introduces the concept of elementary plausible deniability, we show that it is feasible to achieve an atomic processing of individuals entering a stream, in-stead of proceeding by batches. We illustrate the application of the proposed approach by an empirical example.","sentences":["Although the bulk of the research in privacy and statistical disclosure control is designed for static data, more and more data are often collected as continuous streams, and extensions of popular privacy tools and models have been proposed for this scenario.","However, most of these proposals require buffers, where incoming individuals are momentarily stored, anonymized, and then released following a delay, thus considering a data stream as a succession of batches while it is by nature continuous.","Having a delay unavoidably alters data freshness but also, more critically, inordinately exerts constraints on what can be achieved in terms of protection and information preservation.","By considering randomized response, and specifically its recent bistochastic extension, in the context of dynamic data, this paper proposes a protocol for the anonymization of data streams that achieves zero delay while exhibiting formal privacy guarantees.","Using a new tool in the privacy literature that introduces the concept of elementary plausible deniability, we show that it is feasible to achieve an atomic processing of individuals entering a stream, in-stead of proceeding by batches.","We illustrate the application of the proposed approach by an empirical example."],"url":"http://arxiv.org/abs/2402.16094v1","category":"cs.CR"}
{"created":"2024-02-25 13:15:57","title":"Modeling Point Uncertainty in Radar SLAM","abstract":"While visual and laser-based simultaneous localization and mapping (SLAM) techniques have gained significant attention, radar SLAM remains a robust option for challenging conditions. This paper aims to improve the performance of radar SLAM by modeling point uncertainty. The basic SLAM system is a radar-inertial odometry (RIO) system that leverages velocity-aided radar points and high-frequency inertial measurements. We first propose to model the uncertainty of radar points in polar coordinates by considering the nature of radar sensing. Then in the SLAM system, the uncertainty model is designed into the data association module and is incorporated to weight the motion estimation. Real-world experiments on public and self-collected datasets validate the effectiveness of the proposed models and approaches. The findings highlight the potential of incorporating radar point uncertainty modeling to improve the radar SLAM system in adverse environments.","sentences":["While visual and laser-based simultaneous localization and mapping (SLAM) techniques have gained significant attention, radar SLAM remains a robust option for challenging conditions.","This paper aims to improve the performance of radar SLAM by modeling point uncertainty.","The basic SLAM system is a radar-inertial odometry (RIO) system that leverages velocity-aided radar points and high-frequency inertial measurements.","We first propose to model the uncertainty of radar points in polar coordinates by considering the nature of radar sensing.","Then in the SLAM system, the uncertainty model is designed into the data association module and is incorporated to weight the motion estimation.","Real-world experiments on public and self-collected datasets validate the effectiveness of the proposed models and approaches.","The findings highlight the potential of incorporating radar point uncertainty modeling to improve the radar SLAM system in adverse environments."],"url":"http://arxiv.org/abs/2402.16082v1","category":"cs.RO"}
{"created":"2024-02-25 12:19:21","title":"Behavioral Refinement via Interpolant-based Policy Diffusion","abstract":"Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging benchmarks, BRIDGER outperforms state-of-the-art diffusion policies and we provide further analysis on design considerations when applying BRIDGER.","sentences":["Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations.","Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks.","These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise.","However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data.","The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations.","We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy.","Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning.","It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available.","In experiments on challenging benchmarks, BRIDGER outperforms state-of-the-art diffusion policies and we provide further analysis on design considerations when applying BRIDGER."],"url":"http://arxiv.org/abs/2402.16075v1","category":"cs.LG"}
{"created":"2024-02-25 12:06:33","title":"Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities","abstract":"In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.","sentences":["In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search.","However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure.","In this paper, we propose a method that overcomes these challenges in a practical, industrial setting.","The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities.","We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium.","The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions."],"url":"http://arxiv.org/abs/2402.16073v1","category":"cs.IR"}
{"created":"2024-02-25 11:37:23","title":"ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications","abstract":"Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects. Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions. However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics. To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions. An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection. ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git.","sentences":["Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects.","Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions.","However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics.","To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions.","An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection.","ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git."],"url":"http://arxiv.org/abs/2402.16068v1","category":"cs.RO"}
{"created":"2024-02-25 11:24:41","title":"Citation-Enhanced Generation for LLM-based Chatbot","abstract":"Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \\textbf{C}itation-\\textbf{E}nhanced \\textbf{G}eneration (\\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available.","sentences":["Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots.","However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability.","Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation.","In this paper, we propose a novel post-hoc \\textbf{C}itation-\\textbf{E}nhanced \\textbf{G}eneration (\\textbf{CEG}) approach combined with retrieval argumentation.","Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way.","It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module.","Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations.","Note that our method is a training-free plug-and-play plugin that is capable of various LLMs.","Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks.","Our codes and dataset will be publicly available."],"url":"http://arxiv.org/abs/2402.16063v1","category":"cs.CL"}
{"created":"2024-02-25 10:32:18","title":"Maximizing UAV Fog Deployment Efficiency for Critical Rescue Operations","abstract":"In disaster scenarios and high-stakes rescue operations, integrating Unmanned Aerial Vehicles (UAVs) as fog nodes has become crucial. This integration ensures a smooth connection between affected populations and essential health monitoring devices, supported by the Internet of Things (IoT). Integrating UAVs in such environments is inherently challenging, where the primary objectives involve maximizing network connectivity and coverage while extending the network's lifetime through energy-efficient strategies to serve the maximum number of affected individuals. In this paper, We propose a novel model centred around dynamic UAV-based fog deployment that optimizes the system's adaptability and operational efficacy within the afflicted areas. First, we decomposed the problem into two subproblems. Connectivity and coverage subproblem, and network lifespan optimization subproblem. We shape our UAV fog deployment problem as a uni-objective optimization and introduce a specialized UAV fog deployment algorithm tailored specifically for UAV fog nodes deployed in rescue missions. While the network lifespan optimization subproblem is efficiently solved via a one-dimensional swapping method. Following that, We introduce a novel optimization strategy for UAV fog node placement in dynamic networks during evacuation scenarios, with a primary focus on ensuring robust connectivity and maximal coverage for mobile users, while extending the network's lifespan. Finally, we introduce Adaptive Whale Optimization Algorithm (WOA) for fog node deployment in a dynamic network. Its agility, rapid convergence, and low computational demands make it an ideal fit for high-pressure environments.","sentences":["In disaster scenarios and high-stakes rescue operations, integrating Unmanned Aerial Vehicles (UAVs) as fog nodes has become crucial.","This integration ensures a smooth connection between affected populations and essential health monitoring devices, supported by the Internet of Things (IoT).","Integrating UAVs in such environments is inherently challenging, where the primary objectives involve maximizing network connectivity and coverage while extending the network's lifetime through energy-efficient strategies to serve the maximum number of affected individuals.","In this paper, We propose a novel model centred around dynamic UAV-based fog deployment that optimizes the system's adaptability and operational efficacy within the afflicted areas.","First, we decomposed the problem into two subproblems.","Connectivity and coverage subproblem, and network lifespan optimization subproblem.","We shape our UAV fog deployment problem as a uni-objective optimization and introduce a specialized UAV fog deployment algorithm tailored specifically for UAV fog nodes deployed in rescue missions.","While the network lifespan optimization subproblem is efficiently solved via a one-dimensional swapping method.","Following that, We introduce a novel optimization strategy for UAV fog node placement in dynamic networks during evacuation scenarios, with a primary focus on ensuring robust connectivity and maximal coverage for mobile users, while extending the network's lifespan.","Finally, we introduce Adaptive Whale Optimization Algorithm (WOA) for fog node deployment in a dynamic network.","Its agility, rapid convergence, and low computational demands make it an ideal fit for high-pressure environments."],"url":"http://arxiv.org/abs/2402.16052v1","category":"cs.NI"}
{"created":"2024-02-25 10:13:04","title":"LLMs with Chain-of-Thought Are Non-Causal Reasoners","abstract":"This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.","sentences":["This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning.","Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa.","We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate.","By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes.","We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations.","We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis."],"url":"http://arxiv.org/abs/2402.16048v1","category":"cs.CL"}
{"created":"2024-02-25 09:32:17","title":"Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research","abstract":"In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further exploration and research in the realm of large-scale NLP.","sentences":["In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs.","Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation.","These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence.","NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction.","This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain.","Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further exploration and research in the realm of large-scale NLP."],"url":"http://arxiv.org/abs/2402.16038v1","category":"cs.CL"}
{"created":"2024-02-25 09:19:11","title":"Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations","abstract":"With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks. In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations. Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advantages of pre-trained models in understanding complex user intentions and improving the quality of recommendations. In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy. Ultimately, the paper points out that the application of Transformer structural pre-training models in e-commerce has not only driven technological innovation, but also brought substantial benefits to merchants and consumers, and looking forward, these models will continue to play a key role in e-commerce and beyond.","sentences":["With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks.","In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes.","This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations.","Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advantages of pre-trained models in understanding complex user intentions and improving the quality of recommendations.","In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy.","Ultimately, the paper points out that the application of Transformer structural pre-training models in e-commerce has not only driven technological innovation, but also brought substantial benefits to merchants and consumers, and looking forward, these models will continue to play a key role in e-commerce and beyond."],"url":"http://arxiv.org/abs/2402.16035v1","category":"cs.CL"}
{"created":"2024-02-25 09:17:22","title":"Emotion Classification in Short English Texts using Deep Learning Techniques","abstract":"Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts. Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy. To evaluate these methods, we introduce the \"SmallEnglishEmotions\" dataset, comprising 6372 varied short Persian texts annotated with five primary emotion categories. Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset.","sentences":["Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies.","This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts.","Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy.","To evaluate these methods, we introduce the \"SmallEnglishEmotions\" dataset, comprising 6372 varied short Persian texts annotated with five primary emotion categories.","Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset."],"url":"http://arxiv.org/abs/2402.16034v1","category":"cs.CL"}
{"created":"2024-02-25 08:45:10","title":"Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration","abstract":"While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.","sentences":["While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives.","This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues.","Building upon these findings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration (VCB) method to better align LLMs with human preferences.","Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings."],"url":"http://arxiv.org/abs/2402.16030v1","category":"cs.CL"}
{"created":"2024-02-25 07:46:57","title":"TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages","abstract":"The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.","sentences":["The capability to jointly process multi-modal information is becoming an essential task.","However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development.","We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text.","We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem.","To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost.","In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages.","We evaluate the proposed TMT on all six modality translation tasks.","TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance."],"url":"http://arxiv.org/abs/2402.16021v1","category":"cs.CL"}
{"created":"2024-02-25 07:22:37","title":"Complexity of Manipulation and Bribery in Premise-Based Judgment Aggregation with Simple Formulas","abstract":"Judgment aggregation is a framework to aggregate individual opinions on multiple, logically connected issues into a collective outcome. It is open to manipulative attacks such as \\textsc{Manipulation} where judges cast their judgments strategically. Previous works have shown that most computational problems corresponding to these manipulative attacks are \\NP-hard. This desired computational barrier, however, often relies on formulas that are either of unbounded size or of complex structure.   We revisit the computational complexity for various \\textsc{Manipulation} and \\textsc{Bribery} problems in judgment aggregation, now focusing on simple and realistic formulas. We restrict all formulas to be clauses that are (positive) monotone, Horn-clauses, or have bounded length. For basic variants of \\textsc{Manipulation}, we show that these restrictions make several variants, which were in general known to be \\NP-hard, polynomial-time solvable. Moreover, we provide a P vs.\\ NP dichotomy for a large class of clause restrictions (generalizing monotone and Horn clauses) by showing a close relationship between variants of \\textsc{Manipulation} and variants of \\textsc{Satisfiability}. For Hamming distance based \\textsc{Manipulation}, we show that \\NP-hardness even holds for positive monotone clauses of length three, but the problem becomes polynomial-time solvable for positive monotone clauses of length two. For \\textsc{Bribery}, we show that \\NP-hardness even holds for positive monotone clauses of length two, but it becomes polynomial-time solvable for the same clause set if there is a constant budget.","sentences":["Judgment aggregation is a framework to aggregate individual opinions on multiple, logically connected issues into a collective outcome.","It is open to manipulative attacks such as \\textsc{Manipulation} where judges cast their judgments strategically.","Previous works have shown that most computational problems corresponding to these manipulative attacks are \\NP-hard.","This desired computational barrier, however, often relies on formulas that are either of unbounded size or of complex structure.   ","We revisit the computational complexity for various \\textsc{Manipulation} and \\textsc{Bribery} problems in judgment aggregation, now focusing on simple and realistic formulas.","We restrict all formulas to be clauses that are (positive) monotone, Horn-clauses, or have bounded length.","For basic variants of \\textsc{Manipulation}, we show that these restrictions make several variants, which were in general known to be \\NP-hard, polynomial-time solvable.","Moreover, we provide a P vs.\\ NP dichotomy for a large class of clause restrictions (generalizing monotone and Horn clauses) by showing a close relationship between variants of \\textsc{Manipulation} and variants of \\textsc{Satisfiability}.","For Hamming distance based \\textsc{Manipulation}, we show that \\NP-hardness even holds for positive monotone clauses of length three, but the problem becomes polynomial-time solvable for positive monotone clauses of length two.","For \\textsc{Bribery}, we show that \\NP-hardness even holds for positive monotone clauses of length two, but it becomes polynomial-time solvable for the same clause set if there is a constant budget."],"url":"http://arxiv.org/abs/2402.16016v1","category":"cs.GT"}
{"created":"2024-02-25 07:19:01","title":"Building Flexible Machine Learning Models for Scientific Computing at Scale","abstract":"Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering applications and physics discovery.","sentences":["Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing.","OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws.","Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches.","The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering applications and physics discovery."],"url":"http://arxiv.org/abs/2402.16014v1","category":"cs.LG"}
{"created":"2024-02-25 06:53:35","title":"Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision","abstract":"The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes. However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well. While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data. Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples. In this paper, we approach XAI with an innovative {\\em model debugging} methodology realized through Jacobian Saliency Map (JSM). To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemming from the intricate nature of its progression. We introduce an interpretable, multimodal model for AD classification over its multi-stage progression, incorporating JSM as a modality-agnostic tool that provides insights into volumetric changes indicative of brain abnormalities. Our extensive evaluation including ablation study manifests the efficacy of using JSM for model debugging and interpretation, while significantly enhancing model accuracy as well.","sentences":["The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes.","However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well.","While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data.","Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples.","In this paper, we approach XAI with an innovative {\\em model debugging} methodology realized through Jacobian Saliency Map (JSM).","To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemming from the intricate nature of its progression.","We introduce an interpretable, multimodal model for AD classification over its multi-stage progression, incorporating JSM as a modality-agnostic tool that provides insights into volumetric changes indicative of brain abnormalities.","Our extensive evaluation including ablation study manifests the efficacy of using JSM for model debugging and interpretation, while significantly enhancing model accuracy as well."],"url":"http://arxiv.org/abs/2402.16008v1","category":"cs.CV"}
{"created":"2024-02-25 06:39:15","title":"Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation","abstract":"In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients. Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images. Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues. The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes. Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning. However, a significant {\\em domain discrepancy} exists between natural and medical images, which causes AI models resulting from transfer learning to exhibit heightened {\\em vulnerability} to adversarial attacks. This paper proposes a {\\em domain assimilation} approach that introduces texture and color adaptation into transfer learning, followed by a texture preservation component to suppress undesired distortion. We systematically analyze the performance of transfer learning in the face of various adversarial attacks under different data modalities, with the overarching goal of fortifying the model's robustness and security in medical imaging tasks. The results demonstrate high effectiveness in reducing attack efficacy, contributing toward more trustworthy transfer learning in biomedical applications.","sentences":["In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients.","Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images.","Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues.","The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes.","Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning.","However, a significant {\\em domain discrepancy} exists between natural and medical images, which causes AI models resulting from transfer learning to exhibit heightened {\\em vulnerability} to adversarial attacks.","This paper proposes a {\\em domain assimilation} approach that introduces texture and color adaptation into transfer learning, followed by a texture preservation component to suppress undesired distortion.","We systematically analyze the performance of transfer learning in the face of various adversarial attacks under different data modalities, with the overarching goal of fortifying the model's robustness and security in medical imaging tasks.","The results demonstrate high effectiveness in reducing attack efficacy, contributing toward more trustworthy transfer learning in biomedical applications."],"url":"http://arxiv.org/abs/2402.16005v1","category":"cs.CV"}
{"created":"2024-02-25 06:19:04","title":"Post-Quantum Cryptography Neural Network","abstract":"In recent years, quantum computers and Shor quantum algorithm have posed a threat to current mainstream asymmetric cryptography methods (e.g. RSA and Elliptic Curve Cryptography (ECC)). Therefore, it is necessary to construct a Post-Quantum Cryptography (PQC) method to resist quantum computing attacks. Therefore, this study proposes a PQC-based neural network that maps a code-based PQC method to a neural network structure and enhances the security of ciphertexts with non-linear activation functions, random perturbation of ciphertexts, and uniform distribution of ciphertexts. In practical experiments, this study uses cellular network signals as a case study to demonstrate that encryption and decryption can be performed by the proposed PQC-based neural network with the uniform distribution of ciphertexts. In the future, the proposed PQC-based neural network could be applied to various applications.","sentences":["In recent years, quantum computers and Shor quantum algorithm have posed a threat to current mainstream asymmetric cryptography methods (e.g. RSA and Elliptic Curve Cryptography (ECC)).","Therefore, it is necessary to construct a Post-Quantum Cryptography (PQC) method to resist quantum computing attacks.","Therefore, this study proposes a PQC-based neural network that maps a code-based PQC method to a neural network structure and enhances the security of ciphertexts with non-linear activation functions, random perturbation of ciphertexts, and uniform distribution of ciphertexts.","In practical experiments, this study uses cellular network signals as a case study to demonstrate that encryption and decryption can be performed by the proposed PQC-based neural network with the uniform distribution of ciphertexts.","In the future, the proposed PQC-based neural network could be applied to various applications."],"url":"http://arxiv.org/abs/2402.16002v1","category":"cs.CR"}
{"created":"2024-02-25 05:23:57","title":"Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis","abstract":"Portfolio management issues have been extensively studied in the field of artificial intelligence in recent years, but existing deep learning-based quantitative trading methods have some areas where they could be improved. First of all, the prediction mode of stocks is singular; often, only one trading expert is trained by a model, and the trading decision is solely based on the prediction results of the model. Secondly, the data source used by the model is relatively simple, and only considers the data of the stock itself, ignoring the impact of the whole market risk on the stock. In this paper, the DQN algorithm is introduced into asset management portfolios in a novel and straightforward way, and the performance greatly exceeds the benchmark, which fully proves the effectiveness of the DRL algorithm in portfolio management. This also inspires us to consider the complexity of financial problems, and the use of algorithms should be fully combined with the problems to adapt. Finally, in this paper, the strategy is implemented by selecting the assets and actions with the largest Q value. Since different assets are trained separately as environments, there may be a phenomenon of Q value drift among different assets (different assets have different Q value distribution areas), which may easily lead to incorrect asset selection. Consider adding constraints so that the Q values of different assets share a Q value distribution to improve results.","sentences":["Portfolio management issues have been extensively studied in the field of artificial intelligence in recent years, but existing deep learning-based quantitative trading methods have some areas where they could be improved.","First of all, the prediction mode of stocks is singular; often, only one trading expert is trained by a model, and the trading decision is solely based on the prediction results of the model.","Secondly, the data source used by the model is relatively simple, and only considers the data of the stock itself, ignoring the impact of the whole market risk on the stock.","In this paper, the DQN algorithm is introduced into asset management portfolios in a novel and straightforward way, and the performance greatly exceeds the benchmark, which fully proves the effectiveness of the DRL algorithm in portfolio management.","This also inspires us to consider the complexity of financial problems, and the use of algorithms should be fully combined with the problems to adapt.","Finally, in this paper, the strategy is implemented by selecting the assets and actions with the largest Q value.","Since different assets are trained separately as environments, there may be a phenomenon of Q value drift among different assets (different assets have different Q value distribution areas), which may easily lead to incorrect asset selection.","Consider adding constraints so that the Q values of different assets share a Q value distribution to improve results."],"url":"http://arxiv.org/abs/2402.15994v1","category":"q-fin.CP"}
{"created":"2024-02-25 05:22:45","title":"Learning method for S4 with Diagonal State Space Layers using Balanced Truncation","abstract":"We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggesting that our approach effectively leverages the strengths of the original model.","sentences":["We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics.","This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference.","By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance.","Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics.","Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggesting that our approach effectively leverages the strengths of the original model."],"url":"http://arxiv.org/abs/2402.15993v1","category":"cs.LG"}
{"created":"2024-02-25 05:05:52","title":"An Empirical Study of Challenges in Machine Learning Asset Management","abstract":"In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle. This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs. Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects. Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic clustering. We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the most discussed. We also find 79 solution topics, categorized under 18 macro-topics, highlighting software dependency, feature development, and file management as key solutions. This research underscores the need for further exploration of identified pain points and the importance of collaborative efforts across academia, industry, and the research community.","sentences":["In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle.","This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs.","Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects.","Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic clustering.","We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the most discussed.","We also find 79 solution topics, categorized under 18 macro-topics, highlighting software dependency, feature development, and file management as key solutions.","This research underscores the need for further exploration of identified pain points and the importance of collaborative efforts across academia, industry, and the research community."],"url":"http://arxiv.org/abs/2402.15990v1","category":"cs.SE"}
{"created":"2024-02-25 05:04:51","title":"PIDformer: Transformer Meets Control Theory","abstract":"In this work, we address two main shortcomings of transformer architectures: input corruption and rank collapse in their output representation. We unveil self-attention as an autonomous state-space model that inherently promotes smoothness in its solutions, leading to lower-rank outputs and diminished representation capacity. Moreover, the steady-state solution of the model is sensitive to input perturbations. We incorporate a Proportional-Integral-Derivative (PID) closed-loop feedback control system with a reference point into the model to improve robustness and representation capacity. This integration aims to preserve high-frequency details while bolstering model stability, rendering it more noise-resilient. The resulting controlled state-space model is theoretically proven robust and adept at addressing the rank collapse. Motivated by this control framework, we derive a novel class of transformers, PID-controlled Transformer (PIDformer), aimed at improving robustness and mitigating the rank-collapse issue inherent in softmax transformers. We empirically evaluate the model for advantages and robustness against baseline transformers across various practical tasks, including object classification, image segmentation, and language modeling.","sentences":["In this work, we address two main shortcomings of transformer architectures: input corruption and rank collapse in their output representation.","We unveil self-attention as an autonomous state-space model that inherently promotes smoothness in its solutions, leading to lower-rank outputs and diminished representation capacity.","Moreover, the steady-state solution of the model is sensitive to input perturbations.","We incorporate a Proportional-Integral-Derivative (PID) closed-loop feedback control system with a reference point into the model to improve robustness and representation capacity.","This integration aims to preserve high-frequency details while bolstering model stability, rendering it more noise-resilient.","The resulting controlled state-space model is theoretically proven robust and adept at addressing the rank collapse.","Motivated by this control framework, we derive a novel class of transformers, PID-controlled Transformer (PIDformer), aimed at improving robustness and mitigating the rank-collapse issue inherent in softmax transformers.","We empirically evaluate the model for advantages and robustness against baseline transformers across various practical tasks, including object classification, image segmentation, and language modeling."],"url":"http://arxiv.org/abs/2402.15989v1","category":"cs.AI"}
{"created":"2024-02-25 04:52:02","title":"Likelihood-based Mitigation of Evaluation Bias in Large Language Models","abstract":"Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.","sentences":["Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics.","However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure.","It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods.","In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators.","We also propose a method to mitigate the likelihood bias.","Our method utilizes highly biased instances as few-shot examples for in-context learning.","Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias.","Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly."],"url":"http://arxiv.org/abs/2402.15987v1","category":"cs.CL"}
{"created":"2024-02-25 03:58:37","title":"Signed Graph Representation Learning: A Survey","abstract":"With the prevalence of social media, the connectedness between people has been greatly enhanced. Real-world relations between users on social media are often not limited to expressing positive ties such as friendship, trust, and agreement, but they also reflect negative ties such as enmity, mistrust, and disagreement, which can be well modelled by signed graphs. Signed Graph Representation Learning (SGRL) is an effective approach to analyze the complex patterns in real-world signed graphs with the co-existence of positive and negative links. In recent years, SGRL has witnesses fruitful results. SGRL tries to allocate low-dimensional representations to nodes and edges which could preserve the graph structure, attribute and some collective properties, e.g., balance theory and status theory. To the best of knowledge, there is no survey paper about SGRL up to now. In this paper, we present a broad review of SGRL methods and discuss some future research directions.","sentences":["With the prevalence of social media, the connectedness between people has been greatly enhanced.","Real-world relations between users on social media are often not limited to expressing positive ties such as friendship, trust, and agreement, but they also reflect negative ties such as enmity, mistrust, and disagreement, which can be well modelled by signed graphs.","Signed Graph Representation Learning (SGRL) is an effective approach to analyze the complex patterns in real-world signed graphs with the co-existence of positive and negative links.","In recent years, SGRL has witnesses fruitful results.","SGRL tries to allocate low-dimensional representations to nodes and edges which could preserve the graph structure, attribute and some collective properties, e.g., balance theory and status theory.","To the best of knowledge, there is no survey paper about SGRL up to now.","In this paper, we present a broad review of SGRL methods and discuss some future research directions."],"url":"http://arxiv.org/abs/2402.15980v1","category":"cs.SI"}
{"created":"2024-02-25 03:37:32","title":"Towards Mixed Reality as the Everyday Computing Paradigm: Challenges & Design Recommendations","abstract":"This research presents a proof-of-concept prototype of an all-in-one mixed reality application platform, developed to investigate the needs and expectations of users from mixed reality systems. The study involved an extensive user study with 1,052 participants, including the collection of diaries from 6 users and conducting interviews with 51 participants to gain deeper insights into their experiences. The findings from the interviews revealed that directly porting current user flows into 3D environments was not well-received by the target users. Instead, users expressed a clear preference for alternative 3D interactions along with the continued use of 2D interfaces. This study provides insights for understanding user preferences and interactions in mixed reality systems, and design recommendations to facilitate the mass adoption of MR systems.","sentences":["This research presents a proof-of-concept prototype of an all-in-one mixed reality application platform, developed to investigate the needs and expectations of users from mixed reality systems.","The study involved an extensive user study with 1,052 participants, including the collection of diaries from 6 users and conducting interviews with 51 participants to gain deeper insights into their experiences.","The findings from the interviews revealed that directly porting current user flows into 3D environments was not well-received by the target users.","Instead, users expressed a clear preference for alternative 3D interactions along with the continued use of 2D interfaces.","This study provides insights for understanding user preferences and interactions in mixed reality systems, and design recommendations to facilitate the mass adoption of MR systems."],"url":"http://arxiv.org/abs/2402.15974v1","category":"cs.HC"}
{"created":"2024-02-25 03:07:32","title":"CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models","abstract":"Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating \"knowledge\" derived from models, instead of model parameters. We present a novel framework called \\codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus preserving the privacy benefits of federated learning; (4) allowing of adaptive optimization of knowledge shared for personalized learning. We empirically validate \\codream on standard FL tasks, demonstrating competitive performance despite not sharing model parameters. Our code: https://mitmedialab.github.io/codream.github.io/","sentences":["Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters.","Our approach extends this concept by aggregating \"knowledge\" derived from models, instead of model parameters.","We present a novel framework called \\codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL.","Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution.","Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus preserving the privacy benefits of federated learning; (4) allowing of adaptive optimization of knowledge shared for personalized learning.","We empirically validate \\codream on standard FL tasks, demonstrating competitive performance despite not sharing model parameters.","Our code: https://mitmedialab.github.io/codream.github.io/"],"url":"http://arxiv.org/abs/2402.15968v1","category":"cs.LG"}
{"created":"2024-02-25 02:46:33","title":"Budget-Constrained Tool Learning with Planning","abstract":"Despite intensive efforts devoted to tool learning, the problem of budget-constrained tool learning, which focuses on resolving user queries within a specific budget constraint, has been widely overlooked. This paper proposes a novel method for budget-constrained tool learning. Our approach involves creating a preferable plan under the budget constraint before utilizing the tools. This plan outlines the feasible tools and the maximum number of times they can be employed, offering a comprehensive overview of the tool learning process for large language models. This allows them to allocate the budget from a broader perspective. To devise the plan without incurring significant extra costs, we suggest initially estimating the usefulness of the candidate tools based on past experience. Subsequently, we employ dynamic programming to formulate the plan. Experimental results demonstrate that our method can be integrated with various tool learning methods, significantly enhancing their effectiveness under strict budget constraints.","sentences":["Despite intensive efforts devoted to tool learning, the problem of budget-constrained tool learning, which focuses on resolving user queries within a specific budget constraint, has been widely overlooked.","This paper proposes a novel method for budget-constrained tool learning.","Our approach involves creating a preferable plan under the budget constraint before utilizing the tools.","This plan outlines the feasible tools and the maximum number of times they can be employed, offering a comprehensive overview of the tool learning process for large language models.","This allows them to allocate the budget from a broader perspective.","To devise the plan without incurring significant extra costs, we suggest initially estimating the usefulness of the candidate tools based on past experience.","Subsequently, we employ dynamic programming to formulate the plan.","Experimental results demonstrate that our method can be integrated with various tool learning methods, significantly enhancing their effectiveness under strict budget constraints."],"url":"http://arxiv.org/abs/2402.15960v1","category":"cs.AI"}
{"created":"2024-02-25 01:10:55","title":"Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management","abstract":"This paper proposes an innovative Attention-GAN framework for enhancing cybersecurity, focusing on anomaly detection. In response to the challenges posed by the constantly evolving nature of cyber threats, the proposed approach aims to generate diverse and realistic synthetic attack scenarios, thereby enriching the dataset and improving threat identification. Integrating attention mechanisms with Generative Adversarial Networks (GANs) is a key feature of the proposed method. The attention mechanism enhances the model's ability to focus on relevant features, essential for detecting subtle and complex attack patterns. In addition, GANs address the issue of data scarcity by generating additional varied attack data, encompassing known and emerging threats. This dual approach ensures that the system remains relevant and effective against the continuously evolving cyberattacks. The KDD Cup and CICIDS2017 datasets were used to validate this model, which exhibited significant improvements in anomaly detection. It achieved an accuracy of 99.69% on the KDD dataset and 97.93% on the CICIDS2017 dataset, with precision, recall, and F1-scores above 97%, demonstrating its effectiveness in recognizing complex attack patterns. This study contributes significantly to cybersecurity by providing a scalable and adaptable solution for anomaly detection in the face of sophisticated and dynamic cyber threats. The exploration of GANs for data augmentation highlights a promising direction for future research, particularly in situations where data limitations restrict the development of cybersecurity systems. The attention-GAN framework has emerged as a pioneering approach, setting a new benchmark for advanced cyber-defense strategies.","sentences":["This paper proposes an innovative Attention-GAN framework for enhancing cybersecurity, focusing on anomaly detection.","In response to the challenges posed by the constantly evolving nature of cyber threats, the proposed approach aims to generate diverse and realistic synthetic attack scenarios, thereby enriching the dataset and improving threat identification.","Integrating attention mechanisms with Generative Adversarial Networks (GANs) is a key feature of the proposed method.","The attention mechanism enhances the model's ability to focus on relevant features, essential for detecting subtle and complex attack patterns.","In addition, GANs address the issue of data scarcity by generating additional varied attack data, encompassing known and emerging threats.","This dual approach ensures that the system remains relevant and effective against the continuously evolving cyberattacks.","The KDD Cup and CICIDS2017 datasets were used to validate this model, which exhibited significant improvements in anomaly detection.","It achieved an accuracy of 99.69% on the KDD dataset and 97.93% on the CICIDS2017 dataset, with precision, recall, and F1-scores above 97%, demonstrating its effectiveness in recognizing complex attack patterns.","This study contributes significantly to cybersecurity by providing a scalable and adaptable solution for anomaly detection in the face of sophisticated and dynamic cyber threats.","The exploration of GANs for data augmentation highlights a promising direction for future research, particularly in situations where data limitations restrict the development of cybersecurity systems.","The attention-GAN framework has emerged as a pioneering approach, setting a new benchmark for advanced cyber-defense strategies."],"url":"http://arxiv.org/abs/2402.15945v1","category":"cs.CR"}
{"created":"2024-02-25 00:53:16","title":"Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware","abstract":"Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified 10 key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. In this paper, we discuss these challenges in detail and state the path for innovation that we envision. Next, we present FMArts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we (i) show how the unique properties of FMArts enabled us to design and develop a complex FMware for a large customer in a timely manner and (ii) discuss the lessons that we learned in doing so. We hope that the disclosure of the aforementioned challenges and our associated efforts to tackle them will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions across the software engineering discipline.","sentences":["Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models.","We refer to software built using FMs as FMware.","The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges.","Based on our industrial experience, we identified 10 key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky.","In this paper, we discuss these challenges in detail and state the path for innovation that we envision.","Next, we present FMArts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy FMware.","Finally, we (i) show how the unique properties of FMArts enabled us to design and develop a complex FMware for a large customer in a timely manner and (ii) discuss the lessons that we learned in doing so.","We hope that the disclosure of the aforementioned challenges and our associated efforts to tackle them will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions across the software engineering discipline."],"url":"http://arxiv.org/abs/2402.15943v1","category":"cs.SE"}
{"created":"2024-02-24 23:54:41","title":"Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models","abstract":"Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution. To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect contamination caused by the variants of test data. TED significantly mitigates performance improvements up to 66.9\\% attributed to data contamination across 24 settings and 21 contamination degrees. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.","sentences":["Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks.","Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination.","However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges.","In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs.","CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution.","To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's output distribution.","To facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks.","Extensive experimental results show that CDD achieves the average relative improvements of 21.8\\%-30.2\\% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect contamination caused by the variants of test data.","TED significantly mitigates performance improvements up to 66.9\\% attributed to data contamination across 24 settings and 21 contamination degrees.","In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark."],"url":"http://arxiv.org/abs/2402.15938v1","category":"cs.CL"}
{"created":"2024-02-24 23:31:34","title":"Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA","abstract":"In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at $\\href{https://github.com/matthewdm0816/BridgeQA}{\\text{this URL}}$.","sentences":["In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset).","Current approaches resort supplement 3D reasoning with 2D information.","However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations.","To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues.","We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure.","This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other.","Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA.","Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions.","Code is available at $\\href{https://github.com/matthewdm0816/BridgeQA}{\\text{this URL}}$."],"url":"http://arxiv.org/abs/2402.15933v1","category":"cs.CV"}
{"created":"2024-02-24 23:16:57","title":"QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs","abstract":"Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.","sentences":["Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks.","However, traditional studies do not provide formal guarantees on the performance of LLMs.","In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs.","Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt.","Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation."],"url":"http://arxiv.org/abs/2402.15929v1","category":"cs.AI"}
{"created":"2024-02-24 23:01:21","title":"MultiContrievers: Analysis of Dense Retrieval Representations","abstract":"Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extractability, but extractability usually correlates poorly with benchmark performance 2) gender bias is present, but is not caused by the contriever representations 3) there is high sensitivity to both random initialisation and to data shuffle, suggesting that future retrieval research should test across a wider spread of both.","sentences":["Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks.","We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever).","We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models.","We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents.","We measure this extractability via information theoretic probing.","We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles.","We find that (1) contriever models have significantly increased extractability, but extractability usually correlates poorly with benchmark performance 2) gender bias is present, but is not caused by the contriever representations 3) there is high sensitivity to both random initialisation and to data shuffle, suggesting that future retrieval research should test across a wider spread of both."],"url":"http://arxiv.org/abs/2402.15925v1","category":"cs.CL"}
{"created":"2024-02-24 22:36:23","title":"Predicting Outcomes in Video Games with Long Short Term Memory Networks","abstract":"Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events. However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making. Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs) based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series. As a proof of concept, we evaluate our model's performance within a classic, two-player arcade game, Super Street Fighter II Turbo. We also benchmark our method against state of the art methods for time series forecasting; i.e. Transformer models found in large language models (LLMs). Finally, we open-source our data set and code in hopes of furthering work in predictive analysis for arcade games.","sentences":["Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events.","However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making.","Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins.","Our Long Short Term Memory Network (LSTMs) based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series.","As a proof of concept, we evaluate our model's performance within a classic, two-player arcade game, Super Street Fighter II Turbo.","We also benchmark our method against state of the art methods for time series forecasting; i.e. Transformer models found in large language models (LLMs).","Finally, we open-source our data set and code in hopes of furthering work in predictive analysis for arcade games."],"url":"http://arxiv.org/abs/2402.15923v1","category":"cs.LG"}
{"created":"2024-02-24 21:20:53","title":"Enhanced Droplet Analysis Using Generative Adversarial Networks","abstract":"Precision devices play an important role in enhancing production quality and productivity in agricultural systems. Therefore, the optimization of these devices is essential in precision agriculture. Recently, with the advancements of deep learning, there have been several studies aiming to harness its capabilities for improving spray system performance. However, the effectiveness of these methods heavily depends on the size of the training dataset, which is expensive and time-consuming to collect. To address the challenge of insufficient training samples, this paper proposes an alternative solution by generating artificial images of droplets using generative adversarial networks (GAN). The GAN model is trained by using a small dataset captured by a high-speed camera and capable of generating images with progressively increasing resolution. The results demonstrate that the model can generate high-quality images with the size of $1024\\times1024$. Furthermore, this research leverages recent advancements in computer vision and deep learning to develop a light droplet detector using the synthetic dataset. As a result, the detection model achieves a 16.06\\% increase in mean average precision (mAP) when utilizing the synthetic dataset. To the best of our knowledge, this work stands as the first to employ a generative model for augmenting droplet detection. Its significance lies not only in optimizing nozzle design for constructing efficient spray systems but also in addressing the common challenge of insufficient data in various precision agriculture tasks. This work offers a critical contribution to conserving resources while striving for optimal and sustainable agricultural practices.","sentences":["Precision devices play an important role in enhancing production quality and productivity in agricultural systems.","Therefore, the optimization of these devices is essential in precision agriculture.","Recently, with the advancements of deep learning, there have been several studies aiming to harness its capabilities for improving spray system performance.","However, the effectiveness of these methods heavily depends on the size of the training dataset, which is expensive and time-consuming to collect.","To address the challenge of insufficient training samples, this paper proposes an alternative solution by generating artificial images of droplets using generative adversarial networks (GAN).","The GAN model is trained by using a small dataset captured by a high-speed camera and capable of generating images with progressively increasing resolution.","The results demonstrate that the model can generate high-quality images with the size of $1024\\times1024$. Furthermore, this research leverages recent advancements in computer vision and deep learning to develop a light droplet detector using the synthetic dataset.","As a result, the detection model achieves a 16.06\\% increase in mean average precision (mAP) when utilizing the synthetic dataset.","To the best of our knowledge, this work stands as the first to employ a generative model for augmenting droplet detection.","Its significance lies not only in optimizing nozzle design for constructing efficient spray systems but also in addressing the common challenge of insufficient data in various precision agriculture tasks.","This work offers a critical contribution to conserving resources while striving for optimal and sustainable agricultural practices."],"url":"http://arxiv.org/abs/2402.15909v1","category":"cs.CV"}
{"created":"2024-02-24 21:03:30","title":"Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification","abstract":"This paper proposes an efficient system for classifying cervical cancer cells using pre-trained convolutional neural networks (CNNs). We first fine-tune five pre-trained CNNs and minimize the overall cost of misclassification by prioritizing accuracy for certain classes that have higher associated costs or importance. To further enhance the performance of the models, supervised contrastive learning is included to make the models more adept at capturing important features and patterns. Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset. The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%. To make our system more trustworthy, we have employed several explainable AI techniques to interpret how the models reached a specific decision. The implementation of the system can be found at - https://github.com/isha-67/CervicalCancerStudy.","sentences":["This paper proposes an efficient system for classifying cervical cancer cells using pre-trained convolutional neural networks (CNNs).","We first fine-tune five pre-trained CNNs and minimize the overall cost of misclassification by prioritizing accuracy for certain classes that have higher associated costs or importance.","To further enhance the performance of the models, supervised contrastive learning is included to make the models more adept at capturing important features and patterns.","Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset.","The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%.","To make our system more trustworthy, we have employed several explainable AI techniques to interpret how the models reached a specific decision.","The implementation of the system can be found at - https://github.com/isha-67/CervicalCancerStudy."],"url":"http://arxiv.org/abs/2402.15905v1","category":"cs.CV"}
{"created":"2024-02-24 20:58:04","title":"Optimal Budget Aggregation with Single-Peaked Preferences","abstract":"We study the problem of aggregating distributions, such as budget proposals, into a collective distribution. An ideal aggregation mechanism would be Pareto efficient, strategyproof, and fair. Most previous work assumes that agents evaluate budgets according to the $\\ell_1$ distance to their ideal budget. We investigate and compare different models from the larger class of star-shaped utility functions - a multi-dimensional generalization of single-peaked preferences. For the case of two alternatives, we extend existing results by proving that under very general assumptions, the uniform phantom mechanism is the only strategyproof mechanism that satisfies proportionality - a minimal notion of fairness introduced by Freeman et al. (2021). Moving to the case of more than two alternatives, we establish sweeping impossibilities for $\\ell_1$ and $\\ell_\\infty$ disutilities: no mechanism satisfies efficiency, strategyproofness, and proportionality. We then propose a new kind of star-shaped utilities based on evaluating budgets by the ratios of shares between a given budget and an ideal budget. For these utilities, efficiency, strategyproofness, and fairness become compatible. In particular, we prove that the mechanism that maximizes the Nash product of individual utilities is characterized by group-strategyproofness and a core-based fairness condition.","sentences":["We study the problem of aggregating distributions, such as budget proposals, into a collective distribution.","An ideal aggregation mechanism would be Pareto efficient, strategyproof, and fair.","Most previous work assumes that agents evaluate budgets according to the $\\ell_1$ distance to their ideal budget.","We investigate and compare different models from the larger class of star-shaped utility functions - a multi-dimensional generalization of single-peaked preferences.","For the case of two alternatives, we extend existing results by proving that under very general assumptions, the uniform phantom mechanism is the only strategyproof mechanism that satisfies proportionality - a minimal notion of fairness introduced by Freeman et al. (2021).","Moving to the case of more than two alternatives, we establish sweeping impossibilities for $\\ell_1$ and $\\ell_\\infty$ disutilities: no mechanism satisfies efficiency, strategyproofness, and proportionality.","We then propose a new kind of star-shaped utilities based on evaluating budgets by the ratios of shares between a given budget and an ideal budget.","For these utilities, efficiency, strategyproofness, and fairness become compatible.","In particular, we prove that the mechanism that maximizes the Nash product of individual utilities is characterized by group-strategyproofness and a core-based fairness condition."],"url":"http://arxiv.org/abs/2402.15904v1","category":"econ.TH"}
{"created":"2024-02-24 20:50:29","title":"ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices","abstract":"Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data. How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem. In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs). By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity. We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently. Extensive simulations have been conducted to validate the significantly increased efficiency of our ESFL approach compared with standard federated learning, split learning, and splitfed learning.","sentences":["Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data.","How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem.","In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs).","By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity.","We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently.","Extensive simulations have been conducted to validate the significantly increased efficiency of our ESFL approach compared with standard federated learning, split learning, and splitfed learning."],"url":"http://arxiv.org/abs/2402.15903v1","category":"cs.LG"}
{"created":"2024-02-24 20:48:14","title":"Ab initio description of monopole resonances in light- and medium-mass nuclei: II. Ab initio PGCM calculations in $^{46}$Ti, $^{28}$Si and $^{24}$Mg","abstract":"Giant resonances (GRs) are a striking manifestation of collective motions in atomic nuclei. The present paper is the second in a series of four dedicated to the use of the projected generator coordinate method (PGCM) for the ab initio determination of the isoscalar giant monopole resonance (GMR) in closed- and open-shell mid-mass nuclei.   While the first paper was dedicated to quantifying various uncertainty sources, the present paper focuses on the first applications to three doubly-open shell nuclei, namely $^{46}$Ti, $^{28}$Si and $^{24}$Mg. In particular, the goal is to investigate from an ab initio standpoint (i) the coupling of the GMR with the giant quadrupole resonance (GQR) in intrinsically-deformed nuclei, (ii) the possible impact of shape coexistence and shape mixing on the GMR, (iii) the GMR based on shape isomers and (iv) the impact of anharmonic effects on the monopole response. The latter is studied by comparing PGCM results to those obtained via the quasi-particle random phase approximation (QRPA), the traditional many-body approach to giant resonances, performed in a consistent setting.   Eventually, PGCM results for sd-shell nuclei are in excellent agreement with experimental data, which is attributed to the capacity of the PGCM to capture the important fragmentation of the monopole response in light, intrinsically-deformed systems. Still, the comparison to data in $^{28}$Si and $^{24}$Mg illustrates the challenge (and the potential benefit) of extracting unambiguous experimental information.","sentences":["Giant resonances (GRs) are a striking manifestation of collective motions in atomic nuclei.","The present paper is the second in a series of four dedicated to the use of the projected generator coordinate method (PGCM) for the ab initio determination of the isoscalar giant monopole resonance (GMR) in closed- and open-shell mid-mass nuclei.   ","While the first paper was dedicated to quantifying various uncertainty sources, the present paper focuses on the first applications to three doubly-open shell nuclei, namely $^{46}$Ti, $^{28}$Si and $^{24}$Mg.","In particular, the goal is to investigate from an ab initio standpoint (i) the coupling of the GMR with the giant quadrupole resonance (GQR) in intrinsically-deformed nuclei, (ii) the possible impact of shape coexistence and shape mixing on the GMR, (iii) the GMR based on shape isomers and (iv) the impact of anharmonic effects on the monopole response.","The latter is studied by comparing PGCM results to those obtained via the quasi-particle random phase approximation (QRPA), the traditional many-body approach to giant resonances, performed in a consistent setting.   ","Eventually, PGCM results for sd-shell nuclei are in excellent agreement with experimental data, which is attributed to the capacity of the PGCM to capture the important fragmentation of the monopole response in light, intrinsically-deformed systems.","Still, the comparison to data in $^{28}$Si and $^{24}$Mg illustrates the challenge (and the potential benefit) of extracting unambiguous experimental information."],"url":"http://arxiv.org/abs/2402.15901v1","category":"nucl-th"}
{"created":"2024-02-24 20:20:07","title":"MMW-Carry: Enhancing Carry Object Detection through Millimeter-Wave Radar-Camera Fusion","abstract":"This paper introduces MMW-Carry, a system designed to predict the probability of individuals carrying various objects using millimeter-wave radar signals, complemented by camera input. The primary goal of MMW-Carry is to provide a rapid and cost-effective preliminary screening solution, specifically tailored for non-super-sensitive scenarios. Overall, MMW-Carry achieves significant advancements in two crucial aspects. Firstly, it addresses localization challenges in complex indoor environments caused by multi-path reflections, enhancing the system's overall robustness. This is accomplished by the integration of camera-based human detection, tracking, and the radar-camera plane transformation for obtaining subjects' spatial occupancy region, followed by a zooming-in operation on the radar images. Secondly, the system performance is elevated by leveraging long-term observation of a subject. This is realized through the intelligent fusion of neural network results from multiple different-view radar images of an in-track moving subject and their carried objects, facilitated by a proposed knowledge-transfer module. Our experiment results demonstrate that MMW-Carry detects objects with an average error rate of 25.22\\% false positives and a 21.71\\% missing rate for individuals moving randomly in a large indoor space, carrying the common-in-everyday-life objects, both in open carry or concealed ways. These findings affirm MMW-Carry's potential to extend its capabilities to detect a broader range of objects for diverse applications.","sentences":["This paper introduces MMW-Carry, a system designed to predict the probability of individuals carrying various objects using millimeter-wave radar signals, complemented by camera input.","The primary goal of MMW-Carry is to provide a rapid and cost-effective preliminary screening solution, specifically tailored for non-super-sensitive scenarios.","Overall, MMW-Carry achieves significant advancements in two crucial aspects.","Firstly, it addresses localization challenges in complex indoor environments caused by multi-path reflections, enhancing the system's overall robustness.","This is accomplished by the integration of camera-based human detection, tracking, and the radar-camera plane transformation for obtaining subjects' spatial occupancy region, followed by a zooming-in operation on the radar images.","Secondly, the system performance is elevated by leveraging long-term observation of a subject.","This is realized through the intelligent fusion of neural network results from multiple different-view radar images of an in-track moving subject and their carried objects, facilitated by a proposed knowledge-transfer module.","Our experiment results demonstrate that MMW-Carry detects objects with an average error rate of 25.22\\% false positives and a 21.71\\% missing rate for individuals moving randomly in a large indoor space, carrying the common-in-everyday-life objects, both in open carry or concealed ways.","These findings affirm MMW-Carry's potential to extend its capabilities to detect a broader range of objects for diverse applications."],"url":"http://arxiv.org/abs/2402.15897v1","category":"eess.SP"}
{"created":"2024-02-24 19:59:15","title":"Statistical Games","abstract":"This work contains the mathematical exploration of a few prototypical games in which central concepts from statistics and probability theory naturally emerge. The first two kinds of games are termed Fisher and Bayesian games, which are connected to Frequentist and Bayesian statistics, respectively. Later, a more general type of game is introduced, termed Statistical game, in which a further parameter, the players' relative risk aversion, can be set. In this work, we show that Fisher and Bayesian games can be viewed as limiting cases of Statistical games. Therefore, Statistical games can be viewed as a unified framework, incorporating both Frequentist and Bayesian statistics. Furthermore, a philosophical framework is (re-)presented -- often referred to as minimax regret criterion -- as a general approach to decision making.   The main motivation for this work was to embed Bayesian statistics into a broader decision-making framework, where, based on collected data, actions with consequences have to be made, which can be translated to utilities (or rewards/losses) of the decision-maker. The work starts with the simplest possible toy model, related to hypothesis testing and statistical inference. This choice has two main benefits: i.) it allows us to determine (conjecture) the behaviour of the equilibrium strategies in various limiting cases ii.) this way, we can introduce Statistical games without requiring additional stochastic parameters. The work contains game theoretical methods related to two-player, non-cooperative games to determine and prove equilibrium strategies of Fisher, Bayesian and Statistical games. It also relies on analytical tools for derivations concerning various limiting cases.","sentences":["This work contains the mathematical exploration of a few prototypical games in which central concepts from statistics and probability theory naturally emerge.","The first two kinds of games are termed Fisher and Bayesian games, which are connected to Frequentist and Bayesian statistics, respectively.","Later, a more general type of game is introduced, termed Statistical game, in which a further parameter, the players' relative risk aversion, can be set.","In this work, we show that Fisher and Bayesian games can be viewed as limiting cases of Statistical games.","Therefore, Statistical games can be viewed as a unified framework, incorporating both Frequentist and Bayesian statistics.","Furthermore, a philosophical framework is (re-)presented -- often referred to as minimax regret criterion -- as a general approach to decision making.   ","The main motivation for this work was to embed Bayesian statistics into a broader decision-making framework, where, based on collected data, actions with consequences have to be made, which can be translated to utilities (or rewards/losses) of the decision-maker.","The work starts with the simplest possible toy model, related to hypothesis testing and statistical inference.","This choice has two main benefits: i.) it allows us to determine (conjecture) the behaviour of the equilibrium strategies in various limiting cases ii.)","this way, we can introduce Statistical games without requiring additional stochastic parameters.","The work contains game theoretical methods related to two-player, non-cooperative games to determine and prove equilibrium strategies of Fisher, Bayesian and Statistical games.","It also relies on analytical tools for derivations concerning various limiting cases."],"url":"http://arxiv.org/abs/2402.15892v1","category":"math.ST"}
{"created":"2024-02-24 17:26:55","title":"Differentially Private Bayesian Persuasion","abstract":"The tension between persuasion and privacy preservation is common in real-world settings. Online platforms should protect the privacy of web users whose data they collect, even as they seek to disclose information about these data to selling advertising spaces. Similarly, hospitals may share patient data to attract research investments with the obligation to preserve patients' privacy. To deal with these issues, we develop a framework to study Bayesian persuasion under differential privacy constraints, where the sender must design an optimal signaling scheme for persuasion while guaranteeing the privacy of each agent's private information in the database. To understand how privacy constraints affect information disclosure, we explore two perspectives within Bayesian persuasion: one views the mechanism as releasing a posterior about the private data, while the other views it as sending an action recommendation.   The posterior-based formulation helps consider privacy-utility tradeoffs, quantifying how the tightness of privacy constraints impacts the sender's optimal utility. For any instance in a common utility function family and a wide range of privacy levels, a significant constant utility gap can be found between any two of the three conditions: $\\epsilon$-differential privacy constraint, relaxation $(\\epsilon,\\delta)$-differential privacy constraint, and no privacy constraint. We further geometrically characterize optimal signaling schemes under different types of constraints ($\\epsilon$-differential privacy, $(\\epsilon,\\delta)$-differential privacy and Renyi differential privacy), all of which can be seen as finding concave hulls in constrained posterior regions. Meanwhile, by taking the action-based view of persuasion, we provide polynomial-time algorithms for computing optimal differentially private signaling schemes, as long as a mild homogeneous condition is met.","sentences":["The tension between persuasion and privacy preservation is common in real-world settings.","Online platforms should protect the privacy of web users whose data they collect, even as they seek to disclose information about these data to selling advertising spaces.","Similarly, hospitals may share patient data to attract research investments with the obligation to preserve patients' privacy.","To deal with these issues, we develop a framework to study Bayesian persuasion under differential privacy constraints, where the sender must design an optimal signaling scheme for persuasion while guaranteeing the privacy of each agent's private information in the database.","To understand how privacy constraints affect information disclosure, we explore two perspectives within Bayesian persuasion: one views the mechanism as releasing a posterior about the private data, while the other views it as sending an action recommendation.   ","The posterior-based formulation helps consider privacy-utility tradeoffs, quantifying how the tightness of privacy constraints impacts the sender's optimal utility.","For any instance in a common utility function family and a wide range of privacy levels, a significant constant utility gap can be found between any two of the three conditions: $\\epsilon$-differential privacy constraint, relaxation $(\\epsilon,\\delta)$-differential privacy constraint, and no privacy constraint.","We further geometrically characterize optimal signaling schemes under different types of constraints ($\\epsilon$-differential privacy, $(\\epsilon,\\delta)$-differential privacy and Renyi differential privacy), all of which can be seen as finding concave hulls in constrained posterior regions.","Meanwhile, by taking the action-based view of persuasion, we provide polynomial-time algorithms for computing optimal differentially private signaling schemes, as long as a mild homogeneous condition is met."],"url":"http://arxiv.org/abs/2402.15872v1","category":"cs.GT"}
{"created":"2024-02-24 17:12:10","title":"Communication as a driver of change","abstract":"Chapter 2 explores the pivotal role of communication as a catalyst for change and its profound influence on human behavior. The focus is on understanding diverse forms of communication and human interaction that propel both individual and collective transformations. Emphasizing the potency of effective communication, the chapter unveils its capacity to unlock dormant mechanisms and unleash untapped potential within individuals. The goal is to illuminate the significant impact of communication in shaping our world and fostering positive change. Chapter 3 discusses the challenges encountered during change processes, providing practical strategies and exploring innovative ways to overcome obstacles - by blending psychological insights, communication strategies, and sociological perspectives.","sentences":["Chapter 2 explores the pivotal role of communication as a catalyst for change and its profound influence on human behavior.","The focus is on understanding diverse forms of communication and human interaction that propel both individual and collective transformations.","Emphasizing the potency of effective communication, the chapter unveils its capacity to unlock dormant mechanisms and unleash untapped potential within individuals.","The goal is to illuminate the significant impact of communication in shaping our world and fostering positive change.","Chapter 3 discusses the challenges encountered during change processes, providing practical strategies and exploring innovative ways to overcome obstacles - by blending psychological insights, communication strategies, and sociological perspectives."],"url":"http://arxiv.org/abs/2402.15863v1","category":"physics.soc-ph"}
{"created":"2024-02-24 16:39:16","title":"NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavour to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometer and depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision-making and instruction following. We train NaVid with 550k navigation samples collected from VLN-CE trajectories, including action-planning and instruction-reasoning samples, along with 665k large-scale web data. Extensive experiments show that NaVid achieves SOTA performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.","sentences":["Vision-and-Language Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions.","In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real.","In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap.","NaVid makes the first endeavour to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometer and depth inputs.","Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action.","Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs.","Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision-making and instruction following.","We train NaVid with 550k navigation samples collected from VLN-CE trajectories, including action-planning and instruction-reasoning samples, along with 665k large-scale web data.","Extensive experiments show that NaVid achieves SOTA performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer.","We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field."],"url":"http://arxiv.org/abs/2402.15852v1","category":"cs.CV"}
{"created":"2024-02-24 15:36:48","title":"Intelligent Attractors for Singularly Perturbed Dynamical Systems","abstract":"Singularly perturbed dynamical systems, commonly known as fast-slow systems, play a crucial role in various applications such as plasma physics. They are closely related to reduced order modeling, closures, and structure-preserving numerical algorithms for multiscale modeling. A powerful and well-known tool to address these systems is the Fenichel normal form, which significantly simplifies fast dynamics near slow manifolds through a transformation. However, the Fenichel normal form is difficult to realize in conventional numerical algorithms. In this work, we explore an alternative way of realizing it through structure-preserving machine learning. Specifically, a fast-slow neural network (FSNN) is proposed for learning data-driven models of singularly perturbed dynamical systems with dissipative fast timescale dynamics. Our method enforces the existence of a trainable, attracting invariant slow manifold as a hard constraint. Closed-form representation of the slow manifold enables efficient integration on the slow time scale and significantly improves prediction accuracy beyond the training data. We demonstrate the FSNN on several examples that exhibit multiple timescales, including the Grad moment system from hydrodynamics, two-scale Lorentz96 equations for modeling atmospheric dynamics, and Abraham-Lorentz dynamics modeling radiation reaction of electrons in a magnetic field.","sentences":["Singularly perturbed dynamical systems, commonly known as fast-slow systems, play a crucial role in various applications such as plasma physics.","They are closely related to reduced order modeling, closures, and structure-preserving numerical algorithms for multiscale modeling.","A powerful and well-known tool to address these systems is the Fenichel normal form, which significantly simplifies fast dynamics near slow manifolds through a transformation.","However, the Fenichel normal form is difficult to realize in conventional numerical algorithms.","In this work, we explore an alternative way of realizing it through structure-preserving machine learning.","Specifically, a fast-slow neural network (FSNN) is proposed for learning data-driven models of singularly perturbed dynamical systems with dissipative fast timescale dynamics.","Our method enforces the existence of a trainable, attracting invariant slow manifold as a hard constraint.","Closed-form representation of the slow manifold enables efficient integration on the slow time scale and significantly improves prediction accuracy beyond the training data.","We demonstrate the FSNN on several examples that exhibit multiple timescales, including the Grad moment system from hydrodynamics, two-scale Lorentz96 equations for modeling atmospheric dynamics, and Abraham-Lorentz dynamics modeling radiation reaction of electrons in a magnetic field."],"url":"http://arxiv.org/abs/2402.15839v1","category":"math.DS"}
{"created":"2024-02-24 14:59:19","title":"Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study","abstract":"Brain tumors represent a severe and life-threatening condition, demanding precise diagnosis and tailored treatment strategies. This study advances patient care with findings from rigorous multiple-instance-learning experimentations across various feature extractors and aggregators in brain tumor histopathology. It establishes new performance benchmarks in glioma subtype classification across multiple datasets, including a novel dataset focused on the Indian demographic (IPD-Brain), providing a valuable resource for existing research. Using a ResNet-50, pretrained on histopathology datasets, for feature extraction, combined with DTFD feature aggregator, our approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on TCGA-Brain dataset respectively for three-way glioma subtype classification. Moreover, it establishes new benchmarks in grading and detecting IHC molecular biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) through H&E stained whole slide images for the IPD-Brain dataset. The work also highlights a significant correlation between the model decision-making processes and the diagnostic reasoning of pathologists, underscoring its capability to mimic professional diagnostic procedures.","sentences":["Brain tumors represent a severe and life-threatening condition, demanding precise diagnosis and tailored treatment strategies.","This study advances patient care with findings from rigorous multiple-instance-learning experimentations across various feature extractors and aggregators in brain tumor histopathology.","It establishes new performance benchmarks in glioma subtype classification across multiple datasets, including a novel dataset focused on the Indian demographic (IPD-Brain), providing a valuable resource for existing research.","Using a ResNet-50, pretrained on histopathology datasets, for feature extraction, combined with DTFD feature aggregator, our approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on TCGA-Brain dataset respectively for three-way glioma subtype classification.","Moreover, it establishes new benchmarks in grading and detecting IHC molecular biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) through H&E stained whole slide images for the IPD-Brain dataset.","The work also highlights a significant correlation between the model decision-making processes and the diagnostic reasoning of pathologists, underscoring its capability to mimic professional diagnostic procedures."],"url":"http://arxiv.org/abs/2402.15832v1","category":"cs.CV"}
{"created":"2024-02-24 14:51:11","title":"Swarm Body: Embodied Swarm Robots","abstract":"The human brain's plasticity allows for the integration of artificial body parts into the human body. Leveraging this, embodied systems realize intuitive interactions with the environment. We introduce a novel concept: embodied swarm robots. Swarm robots constitute a collective of robots working in harmony to achieve a common objective, in our case, serving as functional body parts. Embodied swarm robots can dynamically alter their shape, density, and the correspondences between body parts and individual robots. We contribute an investigation of the influence on embodiment of swarm robot-specific factors derived from these characteristics, focusing on a hand. Our paper is the first to examine these factors through virtual reality (VR) and real-world robot studies to provide essential design considerations and applications of embodied swarm robots. Through quantitative and qualitative analysis, we identified a system configuration to achieve the embodiment of swarm robots.","sentences":["The human brain's plasticity allows for the integration of artificial body parts into the human body.","Leveraging this, embodied systems realize intuitive interactions with the environment.","We introduce a novel concept: embodied swarm robots.","Swarm robots constitute a collective of robots working in harmony to achieve a common objective, in our case, serving as functional body parts.","Embodied swarm robots can dynamically alter their shape, density, and the correspondences between body parts and individual robots.","We contribute an investigation of the influence on embodiment of swarm robot-specific factors derived from these characteristics, focusing on a hand.","Our paper is the first to examine these factors through virtual reality (VR) and real-world robot studies to provide essential design considerations and applications of embodied swarm robots.","Through quantitative and qualitative analysis, we identified a system configuration to achieve the embodiment of swarm robots."],"url":"http://arxiv.org/abs/2402.15830v1","category":"cs.HC"}
{"created":"2024-02-24 14:29:30","title":"Reward Design for Justifiable Sequential Decision-Making","abstract":"Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.","sentences":["Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making.","Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare.","In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state.","This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence.","In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions.","Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified.","We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients.","We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance.","Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state.","This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations.","Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences."],"url":"http://arxiv.org/abs/2402.15826v1","category":"cs.LG"}
{"created":"2024-02-24 14:17:41","title":"Cooperation and Control in Delegation Games","abstract":"Many settings of interest involving humans and machines -- from virtual personal assistants to autonomous vehicles -- can naturally be modelled as principals (humans) delegating to agents (machines), which then interact with each other on their principals' behalf. We refer to these multi-principal, multi-agent scenarios as delegation games. In such games, there are two important failure modes: problems of control (where an agent fails to act in line their principal's preferences) and problems of cooperation (where the agents fail to work well together). In this paper we formalise and analyse these problems, further breaking them down into issues of alignment (do the players have similar preferences?) and capabilities (how competent are the players at satisfying those preferences?). We show -- theoretically and empirically -- how these measures determine the principals' welfare, how they can be estimated using limited observations, and thus how they might be used to help us design more aligned and cooperative AI systems.","sentences":["Many settings of interest involving humans and machines -- from virtual personal assistants to autonomous vehicles -- can naturally be modelled as principals (humans) delegating to agents (machines), which then interact with each other on their principals' behalf.","We refer to these multi-principal, multi-agent scenarios as delegation games.","In such games, there are two important failure modes: problems of control (where an agent fails to act in line their principal's preferences) and problems of cooperation (where the agents fail to work well together).","In this paper we formalise and analyse these problems, further breaking them down into issues of alignment (do the players have similar preferences?) and capabilities (how competent are the players at satisfying those preferences?).","We show -- theoretically and empirically -- how these measures determine the principals' welfare, how they can be estimated using limited observations, and thus how they might be used to help us design more aligned and cooperative AI systems."],"url":"http://arxiv.org/abs/2402.15821v1","category":"cs.GT"}
{"created":"2024-02-24 14:10:17","title":"DART: Depth-Enhanced Accurate and Real-Time Background Matting","abstract":"Matting with a static background, often referred to as ``Background Matting\" (BGM), has garnered significant attention within the computer vision community due to its pivotal role in various practical applications like webcasting and photo editing. Nevertheless, achieving highly accurate background matting remains a formidable challenge, primarily owing to the limitations inherent in conventional RGB images. These limitations manifest in the form of susceptibility to varying lighting conditions and unforeseen shadows.   In this paper, we leverage the rich depth information provided by the RGB-Depth (RGB-D) cameras to enhance background matting performance in real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm to incorporate depth information. The resulting model's output undergoes refinement through Bayesian inference, incorporating a background depth prior. The posterior prediction is then translated into a \"trimap,\" which is subsequently fed into a state-of-the-art matting algorithm to generate more precise alpha mattes. To ensure real-time matting capabilities, a critical requirement for many real-world applications, we distill the backbone of our model from a larger and more versatile BGM network. Our experiments demonstrate the superior performance of the proposed method. Moreover, thanks to the distillation operation, our method achieves a remarkable processing speed of 33 frames per second (fps) on a mid-range edge-computing device. This high efficiency underscores DART's immense potential for deployment in mobile applications}","sentences":["Matting with a static background, often referred to as ``Background Matting\" (BGM), has garnered significant attention within the computer vision community due to its pivotal role in various practical applications like webcasting and photo editing.","Nevertheless, achieving highly accurate background matting remains a formidable challenge, primarily owing to the limitations inherent in conventional RGB images.","These limitations manifest in the form of susceptibility to varying lighting conditions and unforeseen shadows.   ","In this paper, we leverage the rich depth information provided by the RGB-Depth (RGB-D) cameras to enhance background matting performance in real-time, dubbed DART.","Firstly, we adapt the original RGB-based BGM algorithm to incorporate depth information.","The resulting model's output undergoes refinement through Bayesian inference, incorporating a background depth prior.","The posterior prediction is then translated into a \"trimap,\" which is subsequently fed into a state-of-the-art matting algorithm to generate more precise alpha mattes.","To ensure real-time matting capabilities, a critical requirement for many real-world applications, we distill the backbone of our model from a larger and more versatile BGM network.","Our experiments demonstrate the superior performance of the proposed method.","Moreover, thanks to the distillation operation, our method achieves a remarkable processing speed of 33 frames per second (fps) on a mid-range edge-computing device.","This high efficiency underscores DART's immense potential for deployment in mobile applications}"],"url":"http://arxiv.org/abs/2402.15820v1","category":"cs.CV"}
{"created":"2024-02-24 14:01:07","title":"Linguistic Intelligence in Large Language Models for Telecommunications","abstract":"Large Language Models (LLMs) have emerged as a significant advancement in the field of Natural Language Processing (NLP), demonstrating remarkable capabilities in language generation and other language-centric tasks. Despite their evaluation across a multitude of analytical and reasoning tasks in various scientific domains, a comprehensive exploration of their knowledge and understanding within the realm of natural language tasks in the telecommunications domain is still needed. This study, therefore, seeks to evaluate the knowledge and understanding capabilities of LLMs within this domain. To achieve this, we conduct an exhaustive zero-shot evaluation of four prominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer resources than ChatGPT, making them suitable for resource-constrained environments. Their performance is compared with state-of-the-art, fine-tuned models. To the best of our knowledge, this is the first work to extensively evaluate and compare the understanding of LLMs across multiple language-centric tasks in this domain. Our evaluation reveals that zero-shot LLMs can achieve performance levels comparable to the current state-of-the-art fine-tuned models. This indicates that pretraining on extensive text corpora equips LLMs with a degree of specialization, even within the telecommunications domain. We also observe that no single LLM consistently outperforms others, and the performance of different LLMs can fluctuate. Although their performance lags behind fine-tuned models, our findings underscore the potential of LLMs as a valuable resource for understanding various aspects of this field that lack large annotated data.","sentences":["Large Language Models (LLMs) have emerged as a significant advancement in the field of Natural Language Processing (NLP), demonstrating remarkable capabilities in language generation and other language-centric tasks.","Despite their evaluation across a multitude of analytical and reasoning tasks in various scientific domains, a comprehensive exploration of their knowledge and understanding within the realm of natural language tasks in the telecommunications domain is still needed.","This study, therefore, seeks to evaluate the knowledge and understanding capabilities of LLMs within this domain.","To achieve this, we conduct an exhaustive zero-shot evaluation of four prominent LLMs-Llama-2, Falcon, Mistral, and Zephyr.","These models require fewer resources than ChatGPT, making them suitable for resource-constrained environments.","Their performance is compared with state-of-the-art, fine-tuned models.","To the best of our knowledge, this is the first work to extensively evaluate and compare the understanding of LLMs across multiple language-centric tasks in this domain.","Our evaluation reveals that zero-shot LLMs can achieve performance levels comparable to the current state-of-the-art fine-tuned models.","This indicates that pretraining on extensive text corpora equips LLMs with a degree of specialization, even within the telecommunications domain.","We also observe that no single LLM consistently outperforms others, and the performance of different LLMs can fluctuate.","Although their performance lags behind fine-tuned models, our findings underscore the potential of LLMs as a valuable resource for understanding various aspects of this field that lack large annotated data."],"url":"http://arxiv.org/abs/2402.15818v1","category":"cs.CL"}
{"created":"2024-02-24 13:36:58","title":"Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method","abstract":"Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natural language sentences for generated offers. Experimental results show that OG-Narrator improves the buyer's deal rates from 26.67% to 88.88% and brings a ten times of multiplication of profits on all baselines, even a model that has not been aligned.","sentences":["Bargaining is an important and unique part of negotiation between humans.","As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem.","For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes.","It allows us to quantitatively assess an agent's performance in the Bargain task.","We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities.","We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance.","To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natural language sentences for generated offers.","Experimental results show that OG-Narrator improves the buyer's deal rates from 26.67% to 88.88% and brings a ten times of multiplication of profits on all baselines, even a model that has not been aligned."],"url":"http://arxiv.org/abs/2402.15813v1","category":"cs.CL"}
{"created":"2024-02-24 13:27:51","title":"Collective excitations in two-dimensional harmonically trapped quantum droplets","abstract":"The collective excitation modes in quantum droplets trapped in a two-dimensional harmonic potential in the context of symmetric weakly interacting binary bosonic mixtures are studied. By utilizing the linearization technique, the time-dependent extended Gross-Pitaevskii equation, and a sum-rule approach with a variational approximation, the ground state properties and collective excitations of such a two-dimensional quantum system are investigated for various system parameters. We present comprehensive analysis and calculations on the effect of the confinement strength and anisotropy of the trapping potential, the number of atoms in the droplet, and the collective excitation modes. The radius of the droplet, as well as the chemical potential, is non-monotonically related to the number of atoms in the droplet, and the confinement tends to shift the minimum values towards the ideal gas limit. The excitation frequency peaks, which are prominent in a self-bounded droplet, become less pronounced and smoother when subjected to a strong trapping potential. The sum-rule approach fails to reproduce the breathing mode frequency for a moderate number of atoms in a weak trapping potential, however, works perfectly well in a strong confinement. It was found that the anisotropy in the trap eliminates the degeneracy between the quadrupole and scissors modes that occurs in an isotropic trap, causing the frequencies of these two modes to immediately diverge from each other for any degree of anisotropy. These findings provide valuable insights into the unique characteristics and behavior of quantum droplets, offering potential implications for future research and applications in the dynamic behaviors of intriguing quantum droplets.","sentences":["The collective excitation modes in quantum droplets trapped in a two-dimensional harmonic potential in the context of symmetric weakly interacting binary bosonic mixtures are studied.","By utilizing the linearization technique, the time-dependent extended Gross-Pitaevskii equation, and a sum-rule approach with a variational approximation, the ground state properties and collective excitations of such a two-dimensional quantum system are investigated for various system parameters.","We present comprehensive analysis and calculations on the effect of the confinement strength and anisotropy of the trapping potential, the number of atoms in the droplet, and the collective excitation modes.","The radius of the droplet, as well as the chemical potential, is non-monotonically related to the number of atoms in the droplet, and the confinement tends to shift the minimum values towards the ideal gas limit.","The excitation frequency peaks, which are prominent in a self-bounded droplet, become less pronounced and smoother when subjected to a strong trapping potential.","The sum-rule approach fails to reproduce the breathing mode frequency for a moderate number of atoms in a weak trapping potential, however, works perfectly well in a strong confinement.","It was found that the anisotropy in the trap eliminates the degeneracy between the quadrupole and scissors modes that occurs in an isotropic trap, causing the frequencies of these two modes to immediately diverge from each other for any degree of anisotropy.","These findings provide valuable insights into the unique characteristics and behavior of quantum droplets, offering potential implications for future research and applications in the dynamic behaviors of intriguing quantum droplets."],"url":"http://arxiv.org/abs/2402.15811v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-24 13:13:04","title":"Empowering Large Language Model Agents through Action Learning","abstract":"Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.","sentences":["Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior.","In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents.","While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth.","To address these challenges, our study explores open-action learning for language agents.","We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions.","In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness.","Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents."],"url":"http://arxiv.org/abs/2402.15809v1","category":"cs.AI"}
{"created":"2024-02-24 13:08:39","title":"Optimal Zero-Shot Detector for Multi-Armed Attacks","abstract":"This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available ``off the shelf''. To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario for empirical evaluation, where the attacker possesses a pre-trained classifier and launches well-known adversarial attacks against it. Our experiments highlight the effectiveness of our proposed solution, even in scenarios that deviate from the optimal setup.","sentences":["This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset.","Our central objective is to protect the data by detecting any alterations to the input.","We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker.","Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel.","Instead, the defender relies exclusively on a set of pre-existing detectors readily available ``off the shelf''.","To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data.","We further explore a practical use-case scenario for empirical evaluation, where the attacker possesses a pre-trained classifier and launches well-known adversarial attacks against it.","Our experiments highlight the effectiveness of our proposed solution, even in scenarios that deviate from the optimal setup."],"url":"http://arxiv.org/abs/2402.15808v1","category":"cs.LG"}
{"created":"2024-02-24 13:00:54","title":"Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition","abstract":"Scene text recognition (STR) is a challenging task that requires large-scale annotated data for training. However, collecting and labeling real text images is expensive and time-consuming, which limits the availability of real data. Therefore, most existing STR methods resort to synthetic data, which may introduce domain discrepancy and degrade the performance of STR models. To alleviate this problem, recent semi-supervised STR methods exploit unlabeled real data by enforcing character-level consistency regularization between weakly and strongly augmented views of the same image. However, these methods neglect word-level consistency, which is crucial for sequence recognition tasks. This paper proposes a novel semi-supervised learning method for STR that incorporates word-level consistency regularization from both visual and semantic aspects. Specifically, we devise a shortest path alignment module to align the sequential visual features of different views and minimize their distance. Moreover, we adopt a reinforcement learning framework to optimize the semantic similarity of the predicted strings in the embedding space. We conduct extensive experiments on several standard and challenging STR benchmarks and demonstrate the superiority of our proposed method over existing semi-supervised STR methods.","sentences":["Scene text recognition (STR) is a challenging task that requires large-scale annotated data for training.","However, collecting and labeling real text images is expensive and time-consuming, which limits the availability of real data.","Therefore, most existing STR methods resort to synthetic data, which may introduce domain discrepancy and degrade the performance of STR models.","To alleviate this problem, recent semi-supervised STR methods exploit unlabeled real data by enforcing character-level consistency regularization between weakly and strongly augmented views of the same image.","However, these methods neglect word-level consistency, which is crucial for sequence recognition tasks.","This paper proposes a novel semi-supervised learning method for STR that incorporates word-level consistency regularization from both visual and semantic aspects.","Specifically, we devise a shortest path alignment module to align the sequential visual features of different views and minimize their distance.","Moreover, we adopt a reinforcement learning framework to optimize the semantic similarity of the predicted strings in the embedding space.","We conduct extensive experiments on several standard and challenging STR benchmarks and demonstrate the superiority of our proposed method over existing semi-supervised STR methods."],"url":"http://arxiv.org/abs/2402.15806v1","category":"cs.CV"}
{"created":"2024-02-24 12:38:35","title":"Search for long-lived particles using displaced vertices and missing transverse momentum in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for the production of long-lived particles in proton-proton collisions at a center-of-mass energy of 13 TeV at the CERN LHC is presented. The search is based on data collected by the CMS experiment in 2016-2018, corresponding to a total integrated luminosity of 137 fb$^{-1}$. This search is designed to be sensitive to long-lived particles with mean proper decay lengths between 0.1 and 1000 $\\mu$m, whose decay products produce a final state with at least one displaced vertex and missing transverse momentum. A machine learning algorithm, which improves the background rejection power by more than an order of magnitude, is applied to improve the sensitivity. The observation is consistent with the standard model background prediction, and the results are used to constrain split supersymmetry (SUSY) and gauge-mediated SUSY breaking models with different gluino mean proper decay lengths and masses. This search is the first CMS search that shows sensitivity to hadronically decaying long-lived particles from signals with mass differences between the gluino and neutralino below 100 GeV. It sets the most stringent limits to date for split-SUSY models and gauge-mediated SUSY breaking models with gluino proper decay length less than 6 $\\mu$m.","sentences":["A search for the production of long-lived particles in proton-proton collisions at a center-of-mass energy of 13 TeV at the CERN LHC is presented.","The search is based on data collected by the CMS experiment in 2016-2018, corresponding to a total integrated luminosity of 137 fb$^{-1}$. This search is designed to be sensitive to long-lived particles with mean proper decay lengths between 0.1 and 1000 $\\mu$m, whose decay products produce a final state with at least one displaced vertex and missing transverse momentum.","A machine learning algorithm, which improves the background rejection power by more than an order of magnitude, is applied to improve the sensitivity.","The observation is consistent with the standard model background prediction, and the results are used to constrain split supersymmetry (SUSY) and gauge-mediated SUSY breaking models with different gluino mean proper decay lengths and masses.","This search is the first CMS search that shows sensitivity to hadronically decaying long-lived particles from signals with mass differences between the gluino and neutralino below 100 GeV.","It sets the most stringent limits to date for split-SUSY models and gauge-mediated SUSY breaking models with gluino proper decay length less than 6 $\\mu$m."],"url":"http://arxiv.org/abs/2402.15804v1","category":"hep-ex"}
{"created":"2024-02-24 12:05:42","title":"Gait-Based Privacy Protection for Smart Wearable Devices","abstract":"Smart wearable devices (SWDs) collect and store sensitive daily information of many people. Its primary method of identification is still the password unlocking method. However, several studies have shown serious security flaws in that method, which makes the privacy and security concerns of SWDs particularly urgent. Gait identification is well suited for SWDs because its built-in sensors can provide data support for identification. However, existing gait identification methods have low accuracy and neglect to protect the privacy of gait features. In addition, the SWD can be used as an internet of things device for users to share data. But few studies have used gait feature-based encryption schemes to protect the privacy of message interactions between SWDs and other devices. In this paper, we propose a gait identification network, a bi-directional long short-term memory network with an attention mechanism (ABLSTM), to improve the identification accuracy and a stochastic orthogonal transformation (SOT) scheme to protect the extracted gait features from leakage. In the experiments, ABLSTM achieves an accuracy of 95.28%, reducing previous error rate by 19.3%. The SOT scheme is proved to be resistant to the chosen plaintext attack (CPA) and is 30% faster than previous methods. A biometric-based encryption scheme is proposed to enable secure message interactions using gait features as keys after the gait identification stage is passed, and offers better protection of the gait features compared to previous schemes.","sentences":["Smart wearable devices (SWDs) collect and store sensitive daily information of many people.","Its primary method of identification is still the password unlocking method.","However, several studies have shown serious security flaws in that method, which makes the privacy and security concerns of SWDs particularly urgent.","Gait identification is well suited for SWDs because its built-in sensors can provide data support for identification.","However, existing gait identification methods have low accuracy and neglect to protect the privacy of gait features.","In addition, the SWD can be used as an internet of things device for users to share data.","But few studies have used gait feature-based encryption schemes to protect the privacy of message interactions between SWDs and other devices.","In this paper, we propose a gait identification network, a bi-directional long short-term memory network with an attention mechanism (ABLSTM), to improve the identification accuracy and a stochastic orthogonal transformation (SOT) scheme to protect the extracted gait features from leakage.","In the experiments, ABLSTM achieves an accuracy of 95.28%, reducing previous error rate by 19.3%.","The SOT scheme is proved to be resistant to the chosen plaintext attack (CPA) and is 30% faster than previous methods.","A biometric-based encryption scheme is proposed to enable secure message interactions using gait features as keys after the gait identification stage is passed, and offers better protection of the gait features compared to previous schemes."],"url":"http://arxiv.org/abs/2402.15797v1","category":"cs.CR"}
{"created":"2024-02-24 11:54:32","title":"Construction and application of artificial intelligence crowdsourcing map based on multi-track GPS data","abstract":"In recent years, the rapid development of high-precision map technology combined with artificial intelligence has ushered in a new development opportunity in the field of intelligent vehicles. High-precision map technology is an important guarantee for intelligent vehicles to achieve autonomous driving. However, due to the lack of research on high-precision map technology, it is difficult to rationally use this technology in the field of intelligent vehicles. Therefore, relevant researchers studied a fast and effective algorithm to generate high-precision GPS data from a large number of low-precision GPS trajectory data fusion, and generated several key data points to simplify the description of GPS trajectory, and realized the \"crowdsourced update\" model based on a large number of social vehicles for map data collection came into being. This kind of algorithm has the important significance to improve the data accuracy, reduce the measurement cost and reduce the data storage space. On this basis, this paper analyzes the implementation form of crowdsourcing map, so as to improve the various information data in the high-precision map according to the actual situation, and promote the high-precision map can be reasonably applied to the intelligent car.","sentences":["In recent years, the rapid development of high-precision map technology combined with artificial intelligence has ushered in a new development opportunity in the field of intelligent vehicles.","High-precision map technology is an important guarantee for intelligent vehicles to achieve autonomous driving.","However, due to the lack of research on high-precision map technology, it is difficult to rationally use this technology in the field of intelligent vehicles.","Therefore, relevant researchers studied a fast and effective algorithm to generate high-precision GPS data from a large number of low-precision GPS trajectory data fusion, and generated several key data points to simplify the description of GPS trajectory, and realized the \"crowdsourced update\" model based on a large number of social vehicles for map data collection came into being.","This kind of algorithm has the important significance to improve the data accuracy, reduce the measurement cost and reduce the data storage space.","On this basis, this paper analyzes the implementation form of crowdsourcing map, so as to improve the various information data in the high-precision map according to the actual situation, and promote the high-precision map can be reasonably applied to the intelligent car."],"url":"http://arxiv.org/abs/2402.15796v1","category":"cs.AI"}
{"created":"2024-02-24 10:02:21","title":"Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system","abstract":"With the rising popularity of the internet and the widespread use of networks and information systems via the cloud and data centers, the privacy and security of individuals and organizations have become extremely crucial. In this perspective, encryption consolidates effective technologies that can effectively fulfill these requirements by protecting public information exchanges. To achieve these aims, the researchers used a wide assortment of encryption algorithms to accommodate the varied requirements of this field, as well as focusing on complex mathematical issues during their work to substantially complicate the encrypted communication mechanism. as much as possible to preserve personal information while significantly reducing the possibility of attacks. Depending on how complex and distinct the requirements established by these various applications are, the potential of trying to break them continues to occur, and systems for evaluating and verifying the cryptographic algorithms implemented continue to be necessary. The best approach to analyzing an encryption algorithm is to identify a practical and efficient technique to break it or to learn ways to detect and repair weak aspects in algorithms, which is known as cryptanalysis. Experts in cryptanalysis have discovered several methods for breaking the cipher, such as discovering a critical vulnerability in mathematical equations to derive the secret key or determining the plaintext from the ciphertext. There are various attacks against secure cryptographic algorithms in the literature, and the strategies and mathematical solutions widely employed empower cryptanalysts to demonstrate their findings, identify weaknesses, and diagnose maintenance failures in algorithms.","sentences":["With the rising popularity of the internet and the widespread use of networks and information systems via the cloud and data centers, the privacy and security of individuals and organizations have become extremely crucial.","In this perspective, encryption consolidates effective technologies that can effectively fulfill these requirements by protecting public information exchanges.","To achieve these aims, the researchers used a wide assortment of encryption algorithms to accommodate the varied requirements of this field, as well as focusing on complex mathematical issues during their work to substantially complicate the encrypted communication mechanism.","as much as possible to preserve personal information while significantly reducing the possibility of attacks.","Depending on how complex and distinct the requirements established by these various applications are, the potential of trying to break them continues to occur, and systems for evaluating and verifying the cryptographic algorithms implemented continue to be necessary.","The best approach to analyzing an encryption algorithm is to identify a practical and efficient technique to break it or to learn ways to detect and repair weak aspects in algorithms, which is known as cryptanalysis.","Experts in cryptanalysis have discovered several methods for breaking the cipher, such as discovering a critical vulnerability in mathematical equations to derive the secret key or determining the plaintext from the ciphertext.","There are various attacks against secure cryptographic algorithms in the literature, and the strategies and mathematical solutions widely employed empower cryptanalysts to demonstrate their findings, identify weaknesses, and diagnose maintenance failures in algorithms."],"url":"http://arxiv.org/abs/2402.15779v1","category":"cs.CR"}
{"created":"2024-02-24 09:06:25","title":"From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models","abstract":"This study investigated the integration readiness of four predominant cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0, COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the opportunities, risks, and regulatory compliance when adopting Large Language Models (LLMs), using qualitative content analysis and expert validation. Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks. Comparative gap analysis has highlighted that the new ISO 42001:2023, specifically designed for Artificial Intelligence (AI) management systems, provided most comprehensive facilitation for LLM opportunities, whereas COBIT 2019 aligned most closely with the impending European Union AI Act. Nonetheless, our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs, indicating a critical and time-sensitive need for their continuous evolution. We propose integrating human-expert-in-the-loop validation processes as crucial for enhancing cybersecurity frameworks to support secure and compliant LLM integration, and discuss implications for the continuous evolution of cybersecurity GRC frameworks to support the secure integration of LLMs.","sentences":["This study investigated the integration readiness of four predominant cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0, COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the opportunities, risks, and regulatory compliance when adopting Large Language Models (LLMs), using qualitative content analysis and expert validation.","Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks.","Comparative gap analysis has highlighted that the new ISO 42001:2023, specifically designed for Artificial Intelligence (AI) management systems, provided most comprehensive facilitation for LLM opportunities, whereas COBIT 2019 aligned most closely with the impending European Union AI Act.","Nonetheless, our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs, indicating a critical and time-sensitive need for their continuous evolution.","We propose integrating human-expert-in-the-loop validation processes as crucial for enhancing cybersecurity frameworks to support secure and compliant LLM integration, and discuss implications for the continuous evolution of cybersecurity GRC frameworks to support the secure integration of LLMs."],"url":"http://arxiv.org/abs/2402.15770v1","category":"cs.CY"}
{"created":"2024-02-24 08:57:12","title":"Importance Guided Data Augmentation for Neural-Based Code Understanding","abstract":"Pre-trained code models lead the era of code intelligence. Many models have been designed with impressive performance recently. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in the field of code learning. In this paper, we introduce a general data augmentation framework, GenCode, to enhance the training of code understanding models. GenCode follows a generation-and-selection paradigm to prepare useful training codes. Specifically, it uses code transformation techniques to generate new code candidates first and then selects important ones as the training data by importance metrics. To evaluate the effectiveness of GenCode with a general importance metric -- loss value, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5). Compared to the state-of-the-art (SOTA) code augmentation method, MixCode, GenCode produces code models with 2.92% higher accuracy and 4.90% robustness on average.","sentences":["Pre-trained code models lead the era of code intelligence.","Many models have been designed with impressive performance recently.","However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in the field of code learning.","In this paper, we introduce a general data augmentation framework, GenCode, to enhance the training of code understanding models.","GenCode follows a generation-and-selection paradigm to prepare useful training codes.","Specifically, it uses code transformation techniques to generate new code candidates first and then selects important ones as the training data by importance metrics.","To evaluate the effectiveness of GenCode with a general importance metric -- loss value, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5).","Compared to the state-of-the-art (SOTA) code augmentation method, MixCode, GenCode produces code models with 2.92% higher accuracy and 4.90% robustness on average."],"url":"http://arxiv.org/abs/2402.15769v1","category":"cs.SE"}
{"created":"2024-02-24 08:55:49","title":"Anomaly Detection for GONG Doppler Imagery Using a Binary Classification Neural Network","abstract":"One of the products of the National Solar Observatory's Integrated Synoptic Program (NISP) is the farside seismic map which shows the magnetic activity on the unobserved side of the Sun. The production of these rudimentary maps began in 2006, and they have since proven to be a valuable tool in tracking solar activity which cannot be directly observed from the earth's surface. The continuous tracking of solar active regions allows space weather forecasters to monitor critical solar events which may have larger economic and societal impacts here on Earth. In an effort to improve these maps, several steps are underway through the Windows on the Universe project (WoU) funded by the NSF. One of these steps is to improve the quality assurance measures for the images collected at individual sites throughout the GONG network and is used to develop the farside maps. To this end, we have designed a binary classification neural network to determine which of these site images should and should not be included in the farside pipeline that produces the end product maps. This convolutional neural network is a highly effective and computationally efficient method of significantly improving the quality of the farside maps currently produced by the NISP program.","sentences":["One of the products of the National Solar Observatory's Integrated Synoptic Program (NISP) is the farside seismic map which shows the magnetic activity on the unobserved side of the Sun.","The production of these rudimentary maps began in 2006, and they have since proven to be a valuable tool in tracking solar activity which cannot be directly observed from the earth's surface.","The continuous tracking of solar active regions allows space weather forecasters to monitor critical solar events which may have larger economic and societal impacts here on Earth.","In an effort to improve these maps, several steps are underway through the Windows on the Universe project (WoU) funded by the NSF.","One of these steps is to improve the quality assurance measures for the images collected at individual sites throughout the GONG network and is used to develop the farside maps.","To this end, we have designed a binary classification neural network to determine which of these site images should and should not be included in the farside pipeline that produces the end product maps.","This convolutional neural network is a highly effective and computationally efficient method of significantly improving the quality of the farside maps currently produced by the NISP program."],"url":"http://arxiv.org/abs/2402.15768v1","category":"astro-ph.SR"}
{"created":"2024-02-24 08:51:03","title":"PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators","abstract":"Given the task of positioning a ball-like object to a goal region beyond direct reach, humans can often throw, slide, or rebound objects against the wall to attain the goal. However, enabling robots to reason similarly is non-trivial. Existing methods for physical reasoning are data-hungry and struggle with complexity and uncertainty inherent in the real world. This paper presents PhyPlan, a novel physics-informed planning framework that combines physics-informed neural networks (PINNs) with modified Monte Carlo Tree Search (MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan leverages PINNs to simulate and predict outcomes of actions in a fast and accurate manner and uses MCTS for planning. It dynamically determines whether to consult a PINN-based simulator (coarse but fast) or engage directly with the actual environment (fine but slow) to determine optimal policy. Evaluation with robots in simulated 3D environments demonstrates the ability of our approach to solve 3D-physical reasoning tasks involving the composition of dynamic skills. Quantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret when learning novel tasks compared to state-of-the-art, (ii) it expedites skill learning and enhances the speed of physical reasoning, (iii) it demonstrates higher data efficiency compared to a physics un-informed approach.","sentences":["Given the task of positioning a ball-like object to a goal region beyond direct reach, humans can often throw, slide, or rebound objects against the wall to attain the goal.","However, enabling robots to reason similarly is non-trivial.","Existing methods for physical reasoning are data-hungry and struggle with complexity and uncertainty inherent in the real world.","This paper presents PhyPlan, a novel physics-informed planning framework that combines physics-informed neural networks (PINNs) with modified Monte Carlo Tree Search (MCTS) to enable embodied agents to perform dynamic physical tasks.","PhyPlan leverages PINNs to simulate and predict outcomes of actions in a fast and accurate manner and uses MCTS for planning.","It dynamically determines whether to consult a PINN-based simulator (coarse but fast) or engage directly with the actual environment (fine but slow) to determine optimal policy.","Evaluation with robots in simulated 3D environments demonstrates the ability of our approach to solve 3D-physical reasoning tasks involving the composition of dynamic skills.","Quantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret when learning novel tasks compared to state-of-the-art, (ii) it expedites skill learning and enhances the speed of physical reasoning, (iii) it demonstrates higher data efficiency compared to a physics un-informed approach."],"url":"http://arxiv.org/abs/2402.15767v1","category":"cs.RO"}
{"created":"2024-02-24 08:40:30","title":"Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models","abstract":"Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\\texttt{text-davinci-003}), we observed a 9.93\\% improvement with greedy decoding and 8.80\\% improvement with self-consistency on GSM8k compared to the standard CoT. With ChatGPT~(\\texttt{turbo}) and PEP, we achieve SOTA performances on SVAMP with 86.2\\% and GSM8k with 90.98\\%.","sentences":["Large language models~(LLMs) have exhibited impressive performance across NLP tasks.","So far they still face challenges in complex reasoning tasks and can be sensitive to input context.","Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked.","In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP).","Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties.","Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems.","For instance, with the GPT-3.5 model~(\\texttt{text-davinci-003}), we observed a 9.93\\% improvement with greedy decoding and 8.80\\% improvement with self-consistency on GSM8k compared to the standard CoT. With ChatGPT~(\\texttt{turbo}) and PEP, we achieve SOTA performances on SVAMP with 86.2\\% and GSM8k with 90.98\\%."],"url":"http://arxiv.org/abs/2402.15764v1","category":"cs.CL"}
{"created":"2024-02-24 08:20:39","title":"Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning","abstract":"Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification. However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art (SOTA) on the ImageNet dataset. In this research, we introduce an academically underestimated food dataset CNFOOD-241, and pioneer the integration of a residual learning framework within the VMamba model to concurrently harness both global and local state features inherent in the original VMamba architectural design. The research results show that VMamba surpasses current SOTA models in fine-grained and food classification. The proposed Res-VMamba further improves the classification accuracy to 79.54\\% without pretrained weight. Our findings elucidate that our proposed methodology establishes a new benchmark for SOTA performance in food recognition on the CNFOOD-241 dataset. The code can be obtained on GitHub: https://github.com/ChiShengChen/ResVMamba.","sentences":["Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition.","Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification.","However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity.","In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture.","The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art (SOTA) on the ImageNet dataset.","In this research, we introduce an academically underestimated food dataset CNFOOD-241, and pioneer the integration of a residual learning framework within the VMamba model to concurrently harness both global and local state features inherent in the original VMamba architectural design.","The research results show that VMamba surpasses current SOTA models in fine-grained and food classification.","The proposed Res-VMamba further improves the classification accuracy to 79.54\\% without pretrained weight.","Our findings elucidate that our proposed methodology establishes a new benchmark for SOTA performance in food recognition on the CNFOOD-241 dataset.","The code can be obtained on GitHub: https://github.com/ChiShengChen/ResVMamba."],"url":"http://arxiv.org/abs/2402.15761v1","category":"cs.CV"}
{"created":"2024-02-26 11:24:47","title":"Density in weighted Bergman spaces and Bergman completeness of Hartogs domains","abstract":"We study the density of functions which are holomorphic in a neighbourhood of the closure $\\overline{\\Omega}$ of a bounded non-smooth pseudoconvex domain $\\Omega$, in the Bergman space $ H^2(\\Omega ,\\varphi)$ with a plurisubharmonic weight $\\varphi$. As an application, we show that the Hartogs domain   $$ \\Omega _\\alpha : = \\{(z,w) \\in D\\times \\C: |w|< \\delta^\\alpha_D(z) \\}, \\ \\ \\ \\alpha>0, $$ where $D\\subset \\subset \\C$ and $\\delta_D$ denotes the boundary distance, is Bergman complete if and only if every boundary point of $D$ is non-isolated.","sentences":["We study the density of functions which are holomorphic in a neighbourhood of the closure $\\overline{\\Omega}$ of a bounded non-smooth pseudoconvex domain $\\Omega$, in the Bergman space $ H^2(\\Omega ,\\varphi)$ with a plurisubharmonic weight $\\varphi$. As an application, we show that the Hartogs domain   $$ \\Omega _\\alpha : = \\{(z,w) \\in D\\times \\C: |w|< \\delta^\\alpha_D(z) \\}, \\ \\ \\ \\alpha>0, $$ where $D\\subset \\subset \\C$ and $\\delta_D$ denotes the boundary distance, is Bergman complete if and only if every boundary point of $D$ is non-isolated."],"url":"http://arxiv.org/abs/2402.16494v1","category":"math.CV"}
{"created":"2024-02-26 11:18:10","title":"An efficient multimode vectorial nonlinear propagation solver beyond the weak guidance approximation","abstract":"In this article, we present an efficient numerical model able to solve the vectorial nonlinear pulse propagation equation in circularly symmetric multimode waveguides. The algorithm takes advantage of the conservation of total angular momentum of light upon propagation and takes into account the vectorial nature of the propagating modes, making it particularly relevant for studies in ring-core fibers. While conventional propagation solvers exhibit a computational complexity scaling as N^4_{mode} where N_{mode} is the number of considered modes, the present solver scales as N^{3/2}_{mode}. As a first example, it is shown that orbital angular momentum modulation instability processes take place in ring-core fibers in realistic conditions. Finally, it is predicted that the modulation instability process is followed by the appearance of breather-like angular structures.","sentences":["In this article, we present an efficient numerical model able to solve the vectorial nonlinear pulse propagation equation in circularly symmetric multimode waveguides.","The algorithm takes advantage of the conservation of total angular momentum of light upon propagation and takes into account the vectorial nature of the propagating modes, making it particularly relevant for studies in ring-core fibers.","While conventional propagation solvers exhibit a computational complexity scaling as N^4_{mode} where N_{mode} is the number of considered modes, the present solver scales as N^{3/2}_{mode}.","As a first example, it is shown that orbital angular momentum modulation instability processes take place in ring-core fibers in realistic conditions.","Finally, it is predicted that the modulation instability process is followed by the appearance of breather-like angular structures."],"url":"http://arxiv.org/abs/2402.16492v1","category":"physics.optics"}
{"created":"2024-02-26 11:13:28","title":"Multiple Boundary Peak Solution for Critical Elliptic System with Neumann Boundary","abstract":"We consider the following elliptic system with Neumann boundary: \\begin{equation} \\begin{cases} -\\Delta u + \\mu u=v^p, &\\hbox{in } \\Omega, \\\\-\\Delta v + \\mu v=u^q, &\\hbox{in } \\Omega, \\\\\\frac{\\partial u}{\\partial n} = \\frac{\\partial v}{\\partial n} = 0, &\\hbox{on } \\partial\\Omega, \\\\u>0,v>0, &\\hbox{in } \\Omega, \\end{cases} \\end{equation} where $\\Omega \\subset \\mathbb{R}^N$ is a smooth bounded domain, $\\mu$ is a positive constant and $(p,q)$ lies in the critical hyperbola: $$ \\dfrac{1}{p+1} + \\dfrac{1}{q+1} =\\dfrac{N-2}{N}. $$ By using the Lyapunov-Schmidt reduction technique, we establish the existence of infinitely many solutions to above system. These solutions have multiple peaks that are located on the boundary $\\partial \\Omega$. Our results show that the geometry of the boundary $\\partial\\Omega,$ especially its mean curvature, plays a crucial role on the existence and the behaviour of the solutions to the problem.","sentences":["We consider the following elliptic system with Neumann boundary: \\begin{equation} \\begin{cases} -\\Delta u + \\mu u=v^p, &\\hbox{in } \\Omega, \\\\-\\Delta v + \\mu v=u^q, &\\hbox{in } \\Omega, \\\\\\frac{\\partial u}{\\partial n} = \\frac{\\partial v}{\\partial n} = 0, &\\hbox{on } \\partial\\Omega, \\\\u>0,v>0, &\\hbox{in } \\Omega, \\end{cases} \\end{equation} where $\\Omega \\subset \\mathbb{R}^N$ is a smooth bounded domain, $\\mu$ is a positive constant and $(p,q)$ lies in the critical hyperbola: $$ \\dfrac{1}{p+1} + \\dfrac{1}{q+1} =\\dfrac{N-2}{N}.","$$ By using the Lyapunov-Schmidt reduction technique, we establish the existence of infinitely many solutions to above system.","These solutions have multiple peaks that are located on the boundary $\\partial \\Omega$.","Our results show that the geometry of the boundary $\\partial\\Omega,$ especially its mean curvature, plays a crucial role on the existence and the behaviour of the solutions to the problem."],"url":"http://arxiv.org/abs/2402.16489v1","category":"math.AP"}
{"created":"2024-02-26 11:12:35","title":"Efficient Quantum Lattice Gas Automata","abstract":"This study presents a novel quantum algorithm for 1D and 2D lattice gas automata simulation, demonstrating logarithmic complexity in terms of $CX$ gates. The algorithm is composed by three main steps: collision, mapping and propagation. A computational complexity analysis and a comparison using different error rates and number of shots are provided. Despite the impact of noise, our findings indicate that accurate simulations could be achieved already on current noisy devices. This suggests potential for efficient simulation of classical fluid dynamics using quantum lattice gas automata, conditional on advancements in time step concatenation and state preparation.","sentences":["This study presents a novel quantum algorithm for 1D and 2D lattice gas automata simulation, demonstrating logarithmic complexity in terms of $CX$ gates.","The algorithm is composed by three main steps: collision, mapping and propagation.","A computational complexity analysis and a comparison using different error rates and number of shots are provided.","Despite the impact of noise, our findings indicate that accurate simulations could be achieved already on current noisy devices.","This suggests potential for efficient simulation of classical fluid dynamics using quantum lattice gas automata, conditional on advancements in time step concatenation and state preparation."],"url":"http://arxiv.org/abs/2402.16488v1","category":"quant-ph"}
{"created":"2024-02-26 11:03:46","title":"Using Spherical Harmonics to solve the Boltzmann equation: an operator based approach","abstract":"The transport of charged particles or photons in a scattering medium can be modelled with a Boltzmann equation. The mathematical treatment for scattering in such scenarios is often simplified if evaluated in a frame where the scattering centres are, on average, at rest. It is common therefore, to use a mixed coordinate system, wherein space and time are measured in a fixed inertial frame, while momenta are measured in a \"co-moving\" frame. To facilitate analytic and numerical solutions, the momentum dependency of the phase-space density may be expanded as a series of spherical harmonics, typically truncated at low order. A method for deriving the system of equations for the expansion coefficients of the spherical harmonics to arbitrary order is presented in the limit of isotropic, small-angle scattering. The method of derivation takes advantage of operators acting on the space of spherical harmonics. The matrix representations of these operators are employed to compute the system of equations. The computation of matrix representations is detailed and subsequently simplified with the aid of rotations of the coordinate system. The eigenvalues and eigenvectors of the matrix representations are investigated to prepare the application of standard numerical techniques, e.g. the finite volume method or the discontinuous Galerkin method, to solve the system.","sentences":["The transport of charged particles or photons in a scattering medium can be modelled with a Boltzmann equation.","The mathematical treatment for scattering in such scenarios is often simplified if evaluated in a frame where the scattering centres are, on average, at rest.","It is common therefore, to use a mixed coordinate system, wherein space and time are measured in a fixed inertial frame, while momenta are measured in a \"co-moving\" frame.","To facilitate analytic and numerical solutions, the momentum dependency of the phase-space density may be expanded as a series of spherical harmonics, typically truncated at low order.","A method for deriving the system of equations for the expansion coefficients of the spherical harmonics to arbitrary order is presented in the limit of isotropic, small-angle scattering.","The method of derivation takes advantage of operators acting on the space of spherical harmonics.","The matrix representations of these operators are employed to compute the system of equations.","The computation of matrix representations is detailed and subsequently simplified with the aid of rotations of the coordinate system.","The eigenvalues and eigenvectors of the matrix representations are investigated to prepare the application of standard numerical techniques, e.g. the finite volume method or the discontinuous Galerkin method, to solve the system."],"url":"http://arxiv.org/abs/2402.16483v1","category":"physics.plasm-ph"}
{"created":"2024-02-26 10:48:46","title":"Order $p$ quantum Wasserstein distances from couplings","abstract":"Optimal transport provides a powerful mathematical framework with applications spanning numerous fields. A cornerstone within this domain is the $p$-Wasserstein distance, which serves to quantify the cost of transporting one probability measure to another. While recent attempts have sought to extend this measure to the realm of quantum states, existing definitions often present certain limitations, such as not being faithful. In this work, we present a new definition of quantum Wasserstein distances. This definition, leveraging the coupling method and a metric applicable to pure states, draws inspiration from a property characterising the classical Wasserstein distance - its determination based on its value on point masses. Subject to certain continuity properties, our definition exhibits numerous attributes expected of an optimal quantum rendition of the Wasserstein distance. Notably, our approach seamlessly integrates metrics familiar to quantum information theory, such as the trace distance. Moreover, it provides an organic extension for metrics, like Nielsen's complexity metric, allowing their application to mixed states with a natural operational interpretation. Furthermore, we analyze this metric's attributes in the context of random quantum states and unveil phase transitions concerning the complexity of subsystems of random states.","sentences":["Optimal transport provides a powerful mathematical framework with applications spanning numerous fields.","A cornerstone within this domain is the $p$-Wasserstein distance, which serves to quantify the cost of transporting one probability measure to another.","While recent attempts have sought to extend this measure to the realm of quantum states, existing definitions often present certain limitations, such as not being faithful.","In this work, we present a new definition of quantum Wasserstein distances.","This definition, leveraging the coupling method and a metric applicable to pure states, draws inspiration from a property characterising the classical Wasserstein distance - its determination based on its value on point masses.","Subject to certain continuity properties, our definition exhibits numerous attributes expected of an optimal quantum rendition of the Wasserstein distance.","Notably, our approach seamlessly integrates metrics familiar to quantum information theory, such as the trace distance.","Moreover, it provides an organic extension for metrics, like Nielsen's complexity metric, allowing their application to mixed states with a natural operational interpretation.","Furthermore, we analyze this metric's attributes in the context of random quantum states and unveil phase transitions concerning the complexity of subsystems of random states."],"url":"http://arxiv.org/abs/2402.16477v1","category":"quant-ph"}
{"created":"2024-02-26 10:32:00","title":"Strong coupling yields abrupt synchronization transitions in coupled oscillators","abstract":"Coupled oscillator networks often display transitions between qualitatively different phase-locked solutions -- such as synchrony and rotating wave solutions -- following perturbation or parameter variation. In the limit of weak coupling, these transitions can be understood in terms of commonly studied phase approximations. As the coupling strength increases, however, predicting the location and criticality of transition, whether continuous or discontinuous, from the phase dynamics may depend on the order of the phase approximation -- or a phase description of the network dynamics that neglects amplitudes may become impossible altogether. Here we analyze synchronization transitions and their criticality systematically for varying coupling strength in theory and experiments with coupled electrochemical oscillators. First, we analyze bifurcations analysis of synchrony and splay states in an abstract phase model and discuss conditions under which synchronization transitions with different criticalities are possible. Second, we illustrate that transitions with different criticality indeed occur in experimental systems. Third, we highlight that the amplitude dynamics observed in the experiments can be captured in a numerical bifurcation analysis of delay-coupled oscillators. Our results showcase that reduced order phase models may miss important features that one would expect in the dynamics of the full system.","sentences":["Coupled oscillator networks often display transitions between qualitatively different phase-locked solutions -- such as synchrony and rotating wave solutions -- following perturbation or parameter variation.","In the limit of weak coupling, these transitions can be understood in terms of commonly studied phase approximations.","As the coupling strength increases, however, predicting the location and criticality of transition, whether continuous or discontinuous, from the phase dynamics may depend on the order of the phase approximation -- or a phase description of the network dynamics that neglects amplitudes may become impossible altogether.","Here we analyze synchronization transitions and their criticality systematically for varying coupling strength in theory and experiments with coupled electrochemical oscillators.","First, we analyze bifurcations analysis of synchrony and splay states in an abstract phase model and discuss conditions under which synchronization transitions with different criticalities are possible.","Second, we illustrate that transitions with different criticality indeed occur in experimental systems.","Third, we highlight that the amplitude dynamics observed in the experiments can be captured in a numerical bifurcation analysis of delay-coupled oscillators.","Our results showcase that reduced order phase models may miss important features that one would expect in the dynamics of the full system."],"url":"http://arxiv.org/abs/2402.16471v1","category":"math.DS"}
{"created":"2024-02-26 10:16:21","title":"Training Classical Neural Networks by Quantum Machine Learning","abstract":"In recent years, advanced deep neural networks have required a large number of parameters for training. Therefore, finding a method to reduce the number of parameters has become crucial for achieving efficient training. This work proposes a training scheme for classical neural networks (NNs) that utilizes the exponentially large Hilbert space of a quantum system. By mapping a classical NN with $M$ parameters to a quantum neural network (QNN) with $O(\\text{polylog} (M))$ rotational gate angles, we can significantly reduce the number of parameters. These gate angles can be updated to train the classical NN. Unlike existing quantum machine learning (QML) methods, the results obtained from quantum computers using our approach can be directly used on classical computers. Numerical results on the MNIST and Iris datasets are presented to demonstrate the effectiveness of our approach. Additionally, we investigate the effects of deeper QNNs and the number of measurement shots for the QNN, followed by the theoretical perspective of the proposed method. This work opens a new branch of QML and offers a practical tool that can greatly enhance the influence of QML, as the trained QML results can benefit classical computing in our daily lives.","sentences":["In recent years, advanced deep neural networks have required a large number of parameters for training.","Therefore, finding a method to reduce the number of parameters has become crucial for achieving efficient training.","This work proposes a training scheme for classical neural networks (NNs) that utilizes the exponentially large Hilbert space of a quantum system.","By mapping a classical NN with $M$ parameters to a quantum neural network (QNN) with $O(\\text{polylog} (M))$ rotational gate angles, we can significantly reduce the number of parameters.","These gate angles can be updated to train the classical NN.","Unlike existing quantum machine learning (QML) methods, the results obtained from quantum computers using our approach can be directly used on classical computers.","Numerical results on the MNIST and Iris datasets are presented to demonstrate the effectiveness of our approach.","Additionally, we investigate the effects of deeper QNNs and the number of measurement shots for the QNN, followed by the theoretical perspective of the proposed method.","This work opens a new branch of QML and offers a practical tool that can greatly enhance the influence of QML, as the trained QML results can benefit classical computing in our daily lives."],"url":"http://arxiv.org/abs/2402.16465v1","category":"quant-ph"}
{"created":"2024-02-26 10:11:28","title":"Learning to Schedule Online Tasks with Bandit Feedback","abstract":"Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing. Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution. On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain black-box in practice. These render reward and cost hard to model thus unknown before decision making. On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail. This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution. Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm. Specifically, DOL-RM integrates a learning module that incorporates optimistic estimation for reward-to-cost ratio and a decision module that utilizes the Robbins-Monro method to implicitly learn task arrival distribution while making scheduling decisions. Theoretically, DOL-RM achieves convergence gap and no regret learning with a sub-linear regret of $O(T^{3/4})$, which is the first result for online task scheduling under uncertain task arrival distribution and unknown reward and cost. Our numerical results in a synthetic experiment and a real-world application demonstrate the effectiveness of DOL-RM in achieving the best cumulative reward-to-cost ratio compared with other state-of-the-art baselines.","sentences":["Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing.","Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution.","On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain black-box in practice.","These render reward and cost hard to model thus unknown before decision making.","On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail.","This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution.","Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm.","Specifically, DOL-RM integrates a learning module that incorporates optimistic estimation for reward-to-cost ratio and a decision module that utilizes the Robbins-Monro method to implicitly learn task arrival distribution while making scheduling decisions.","Theoretically, DOL-RM achieves convergence gap and no regret learning with a sub-linear regret of $O(T^{3/4})$, which is the first result for online task scheduling under uncertain task arrival distribution and unknown reward and cost.","Our numerical results in a synthetic experiment and a real-world application demonstrate the effectiveness of DOL-RM in achieving the best cumulative reward-to-cost ratio compared with other state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.16463v1","category":"cs.LG"}
{"created":"2024-02-26 09:50:34","title":"Distortion-Controlled Dithering with Reduced Recompression Rate","abstract":"Dithering is a technique that can improve human perception of low-resolution data by reducing quantization artifacts. In this work we formalize and analytically justify two metrics for quantization artifact prominence, using them to design a novel dithering method for distortion-controlled data compression. We present theoretical entropy calculations for this dither and experimentally validate its performance on a low-rate image compression task. The result is a drastic improvement in the perceptual quality of quantized images with a lower recompression entropy than any state-of-the-art dither technique, achieving 45 points lower PIQUE at the same rate or 40% lower rate at the same PIQUE. The proposed dither is an adaptable tool applicable for use in any lossy compression system, permitting precise control of rate-distortion characteristics for both compression and recompression.","sentences":["Dithering is a technique that can improve human perception of low-resolution data by reducing quantization artifacts.","In this work we formalize and analytically justify two metrics for quantization artifact prominence, using them to design a novel dithering method for distortion-controlled data compression.","We present theoretical entropy calculations for this dither and experimentally validate its performance on a low-rate image compression task.","The result is a drastic improvement in the perceptual quality of quantized images with a lower recompression entropy than any state-of-the-art dither technique, achieving 45 points lower PIQUE at the same rate or 40% lower rate at the same PIQUE.","The proposed dither is an adaptable tool applicable for use in any lossy compression system, permitting precise control of rate-distortion characteristics for both compression and recompression."],"url":"http://arxiv.org/abs/2402.16447v1","category":"eess.SP"}
{"created":"2024-02-26 09:47:09","title":"Indoor Localization of Smartphones Thanks to Zero-Energy-Devices Beacons","abstract":"In this paper, we present a new ultra-low power method of indoor localization of smartphones (SM) based on zero-energy-devices (ZEDs) beacons instead of active wireless beacons. Each ZED is equipped with a unique identification number coded into a bit-sequence, and its precise position on the map is recorded. An SM inside the building is assumed to have access to the map of ZEDs. The ZED backscatters ambient waves from base stations (BSs) of the cellular network. The SM detects the ZED message in the variations of the received ambient signal from the BS. We accurately simulate the ambient waves from a BS of Orange 4G commercial network, inside an existing large building covered with ZED beacons, thanks to a ray-tracing-based propagation simulation tool. Our first performance evaluation study shows that the proposed localization system enables us to determine in which room a SM is located, in a realistic and challenging propagation scenario.","sentences":["In this paper, we present a new ultra-low power method of indoor localization of smartphones (SM) based on zero-energy-devices (ZEDs) beacons instead of active wireless beacons.","Each ZED is equipped with a unique identification number coded into a bit-sequence, and its precise position on the map is recorded.","An SM inside the building is assumed to have access to the map of ZEDs.","The ZED backscatters ambient waves from base stations (BSs) of the cellular network.","The SM detects the ZED message in the variations of the received ambient signal from the BS.","We accurately simulate the ambient waves from a BS of Orange 4G commercial network, inside an existing large building covered with ZED beacons, thanks to a ray-tracing-based propagation simulation tool.","Our first performance evaluation study shows that the proposed localization system enables us to determine in which room a SM is located, in a realistic and challenging propagation scenario."],"url":"http://arxiv.org/abs/2402.16446v1","category":"eess.SP"}
{"created":"2024-02-26 09:31:29","title":"Data-Driven Acceleration of Multi-Physics Simulations","abstract":"Multi-physics simulations play a crucial role in understanding complex systems. However, their computational demands are often prohibitive due to high dimensionality and complex interactions, such that actual calculations often rely on approximations. To address this, we introduce a data-driven approach to approximate interactions among degrees of freedom of no direct interest and thus significantly reduce computational costs. Focusing on a semiconductor laser as a case study, we demonstrate the superiority of this method over traditional analytical approximations in both accuracy and efficiency. Our approach streamlines simulations, offering promise for complex multi-physics systems, especially for scenarios requiring a large number of individual simulations.","sentences":["Multi-physics simulations play a crucial role in understanding complex systems.","However, their computational demands are often prohibitive due to high dimensionality and complex interactions, such that actual calculations often rely on approximations.","To address this, we introduce a data-driven approach to approximate interactions among degrees of freedom of no direct interest and thus significantly reduce computational costs.","Focusing on a semiconductor laser as a case study, we demonstrate the superiority of this method over traditional analytical approximations in both accuracy and efficiency.","Our approach streamlines simulations, offering promise for complex multi-physics systems, especially for scenarios requiring a large number of individual simulations."],"url":"http://arxiv.org/abs/2402.16433v1","category":"physics.comp-ph"}
{"created":"2024-02-26 09:31:08","title":"On the existence of KKL observers with nonlinear contracting dynamics (Long Version)","abstract":"KKL (Kazantzis-Kravaris/Luenberger) observers are based on the idea of immersing a given nonlinear system into a target system that is a linear stable filter of the measured output. In the present paper, we extend this theory by allowing this target system to be a nonlinear contracting filter of the output. We prove, under a differential observability condition, the existence of these new KKL observers. We motivate their introduction by showing numerically the possibility of combining convergence speed and robustness to noise, unlike what is known for linear filtering.","sentences":["KKL (Kazantzis-Kravaris/Luenberger) observers are based on the idea of immersing a given nonlinear system into a target system that is a linear stable filter of the measured output.","In the present paper, we extend this theory by allowing this target system to be a nonlinear contracting filter of the output.","We prove, under a differential observability condition, the existence of these new KKL observers.","We motivate their introduction by showing numerically the possibility of combining convergence speed and robustness to noise, unlike what is known for linear filtering."],"url":"http://arxiv.org/abs/2402.16432v1","category":"math.OC"}
{"created":"2024-02-26 09:28:07","title":"Closed form solution to zero coupon bond using a linear stochastic delay differential equation","abstract":"We present a short rate model that satisfies a stochastic delay differential equation. The model can be considered a delayed version of the Merton model (Merton 1970, 1973) or the Vasi\\v{c}ek model (Vasi\\v{c}ek 1977). Using the same technique as the one used by Flore and Nappo (2019), we show that the bond price is an affine function of the short rate, whose coefficients satisfy a system of delay differential equations. We give an analytical solution to this system of delay differential equations, obtaining a closed formula for the zero coupon bond price. Under this model, we can show that the distribution of the short rate is a normal distribution whose mean depends on past values of the short rate. Based on the results of K\\\"uchler and Mensch (1992), we prove the existence of stationary and limiting distributions.","sentences":["We present a short rate model that satisfies a stochastic delay differential equation.","The model can be considered a delayed version of the Merton model (Merton 1970, 1973) or the Vasi\\v{c}ek model (Vasi\\v{c}ek 1977).","Using the same technique as the one used by Flore and Nappo (2019), we show that the bond price is an affine function of the short rate, whose coefficients satisfy a system of delay differential equations.","We give an analytical solution to this system of delay differential equations, obtaining a closed formula for the zero coupon bond price.","Under this model, we can show that the distribution of the short rate is a normal distribution whose mean depends on past values of the short rate.","Based on the results of K\\\"uchler and Mensch (1992), we prove the existence of stationary and limiting distributions."],"url":"http://arxiv.org/abs/2402.16428v1","category":"q-fin.MF"}
{"created":"2024-02-26 09:18:00","title":"Higher-dimensional multifractal analysis for the cusp winding process on hyperbolic surfaces","abstract":"We perform a multifractal analysis of the growth rate of the number of cusp windings for the geodesic flow on hyperbolic surfaces with $m \\geq 1$ cusps. Our main theorem establishes a conditional variational principle for the Hausdorff dimension spectrum of the multi-cusp winding process. Moreover, we show that the dimension spectrum defined on $\\mathbb{R}_{>0}^m$ is real analytic. To prove the main theorem we use a countable Markov shift with a finitely primitive transition matrix and thermodynamic formalism.","sentences":["We perform a multifractal analysis of the growth rate of the number of cusp windings for the geodesic flow on hyperbolic surfaces with $m \\geq 1$ cusps.","Our main theorem establishes a conditional variational principle for the Hausdorff dimension spectrum of the multi-cusp winding process.","Moreover, we show that the dimension spectrum defined on $\\mathbb{R}_{>0}^m$ is real analytic.","To prove the main theorem we use a countable Markov shift with a finitely primitive transition matrix and thermodynamic formalism."],"url":"http://arxiv.org/abs/2402.16418v1","category":"math.DS"}
{"created":"2024-02-26 09:11:20","title":"Cumulant Green's function methods for molecules","abstract":"The cumulant expansion of the Green's function is a computationally efficient beyond-$GW$ approach renowned for its significant enhancement of satellite features in materials. In contrast to the ubiquitous $GW$ approximation of many-body perturbation theory, \\textit{ab initio} cumulant expansions performed on top of $GW$ ($GW$+C) have demonstrated the capability to handle multi-particle processes by incorporating higher-order correlation effects or vertex corrections, yielding better agreements between experiment and theory for satellite structures. While widely employed in condensed matter physics, very few applications of $GW$+C have been published on molecular systems. Here, we assess the performance of this scheme on a series of 10-electron molecular systems (\\ce{Ne}, \\ce{HF}, \\ce{H2O}, \\ce{NH3}, and \\ce{CH4}) where full configuration interaction estimates of the outer-valence quasiparticle and satellite energies are available.","sentences":["The cumulant expansion of the Green's function is a computationally efficient beyond-$GW$ approach renowned for its significant enhancement of satellite features in materials.","In contrast to the ubiquitous $GW$ approximation of many-body perturbation theory, \\textit{ab initio} cumulant expansions performed on top of $GW$ ($GW$+C) have demonstrated the capability to handle multi-particle processes by incorporating higher-order correlation effects or vertex corrections, yielding better agreements between experiment and theory for satellite structures.","While widely employed in condensed matter physics, very few applications of $GW$+C have been published on molecular systems.","Here, we assess the performance of this scheme on a series of 10-electron molecular systems (\\ce{Ne}, \\ce{HF}, \\ce{H2O}, \\ce{NH3}, and \\ce{CH4}) where full configuration interaction estimates of the outer-valence quasiparticle and satellite energies are available."],"url":"http://arxiv.org/abs/2402.16414v1","category":"physics.chem-ph"}
{"created":"2024-02-26 09:09:24","title":"Direct excitation of Kelvin waves on quantized vortices","abstract":"Helices and spirals, prevalent across various systems, play a crucial role in characterizing symmetry, describing dynamics, and imparting unique functionalities, attributed to their inherent simplicity and chiral nature. A helical excitation on a quantized vortex, an example of a one-dimensional topological defect, emerges as a Nambu-Goldstone mode following spontaneous symmetry breaking, known as a Kelvin wave. Kelvin waves play a vital role in energy dissipation within inviscid quantum fluids. However, deliberately exciting Kelvin waves has proven to be challenging. Here, we introduce a controlled method for exciting Kelvin waves on a quantized vortex in superfluid helium-4. We used a charged nanoparticle, oscillated by a time-varying electric field, to stimulate Kelvin waves on the vortex. A major breakthrough in our research is the confirmation of the helical nature of Kelvin waves through three-dimensional image reconstruction, providing visual evidence of their complex dynamics. Additionally, we determined the dispersion relation and the phase velocity of the Kelvin wave and identified the vorticity direction, enhancing our understanding of quantum fluid behavior. This work elucidates the dynamics of Kelvin waves and pioneers a novel approach for manipulating and observing quantized vortices in three dimensions, thereby opening new avenues for exploring quantum fluidic systems.","sentences":["Helices and spirals, prevalent across various systems, play a crucial role in characterizing symmetry, describing dynamics, and imparting unique functionalities, attributed to their inherent simplicity and chiral nature.","A helical excitation on a quantized vortex, an example of a one-dimensional topological defect, emerges as a Nambu-Goldstone mode following spontaneous symmetry breaking, known as a Kelvin wave.","Kelvin waves play a vital role in energy dissipation within inviscid quantum fluids.","However, deliberately exciting Kelvin waves has proven to be challenging.","Here, we introduce a controlled method for exciting Kelvin waves on a quantized vortex in superfluid helium-4.","We used a charged nanoparticle, oscillated by a time-varying electric field, to stimulate Kelvin waves on the vortex.","A major breakthrough in our research is the confirmation of the helical nature of Kelvin waves through three-dimensional image reconstruction, providing visual evidence of their complex dynamics.","Additionally, we determined the dispersion relation and the phase velocity of the Kelvin wave and identified the vorticity direction, enhancing our understanding of quantum fluid behavior.","This work elucidates the dynamics of Kelvin waves and pioneers a novel approach for manipulating and observing quantized vortices in three dimensions, thereby opening new avenues for exploring quantum fluidic systems."],"url":"http://arxiv.org/abs/2402.16411v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-26 09:04:04","title":"CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency","abstract":"Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods.","sentences":["Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously.","However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value.","Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge.","Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem.","To achieve this, we build the neural networks on layered representations (\\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes.","Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same.","Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.16407v1","category":"cs.CV"}
{"created":"2024-02-26 08:56:15","title":"Entropy production for diffusion processes across a semipermeable interface","abstract":"The emerging field of stochastic thermodynamics extends classical ideas of entropy, heat and work to non-equilibrium systems. One notable finding is that the second law of thermodynamics typically only holds after taking appropriate averages with respect to an ensemble of stochastic trajectories. The resulting average rate of entropy production then quantifies the degree of departure from thermodynamic equilibrium. In this paper we investigate how the presence of a semipermeable interface increases the average entropy production of a single diffusing particle. Starting from the Gibbs-Shannon entropy for the particle probability density, we show that a semipermeable interface or membrane $\\calS$ increases the average rate of entropy production by an amount that is equal to the product of the flux through the interface and the logarithm of the ratio of the probability density on either side of the interface, integrated along $\\calS$. The entropy production rate thus vanishes at thermodynamic equilibrium, but can be nonzero during the relaxation to equilibrium, or if there exists a nonzero stationary equilibrium state (NESS). We illustrate the latter using the example of diffusion with stochastic resetting on a circle, and show that the average rate of interfacial entropy production is a nonmonotonic function of the resetting rate and the permeability. Finally, we give a probabilistic interpretation of the interfacial entropy production rate using so-called snapping out Brownian motion. This also allows us to construct a stochastic version of entropy production.","sentences":["The emerging field of stochastic thermodynamics extends classical ideas of entropy, heat and work to non-equilibrium systems.","One notable finding is that the second law of thermodynamics typically only holds after taking appropriate averages with respect to an ensemble of stochastic trajectories.","The resulting average rate of entropy production then quantifies the degree of departure from thermodynamic equilibrium.","In this paper we investigate how the presence of a semipermeable interface increases the average entropy production of a single diffusing particle.","Starting from the Gibbs-Shannon entropy for the particle probability density, we show that a semipermeable interface or membrane $\\calS$ increases the average rate of entropy production by an amount that is equal to the product of the flux through the interface and the logarithm of the ratio of the probability density on either side of the interface, integrated along $\\calS$. The entropy production rate thus vanishes at thermodynamic equilibrium, but can be nonzero during the relaxation to equilibrium, or if there exists a nonzero stationary equilibrium state (NESS).","We illustrate the latter using the example of diffusion with stochastic resetting on a circle, and show that the average rate of interfacial entropy production is a nonmonotonic function of the resetting rate and the permeability.","Finally, we give a probabilistic interpretation of the interfacial entropy production rate using so-called snapping out Brownian motion.","This also allows us to construct a stochastic version of entropy production."],"url":"http://arxiv.org/abs/2402.16403v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-26 08:50:51","title":"Quantitative Propagation of Chaos for Mean Field Interacting Particle System","abstract":"In this paper, quantitative propagation of chaos in $L^\\eta$($\\eta\\in(0,1)$)-Wasserstein distance for mean field interacting particle system is derived, where the diffusion coefficient is allowed to be interacting and the initial distribution of interacting particle system converges to that of the limit equation in $L^1$-Wasserstein distance. The non-degenerate and degenerate cases are investigated respectively and the main tool relies on the gradient estimate of the decoupled SDEs.","sentences":["In this paper, quantitative propagation of chaos in $L^\\eta$($\\eta\\in(0,1)$)-Wasserstein distance for mean field interacting particle system is derived, where the diffusion coefficient is allowed to be interacting and the initial distribution of interacting particle system converges to that of the limit equation in $L^1$-Wasserstein distance.","The non-degenerate and degenerate cases are investigated respectively and the main tool relies on the gradient estimate of the decoupled SDEs."],"url":"http://arxiv.org/abs/2402.16400v1","category":"math.PR"}
{"created":"2024-02-26 08:49:17","title":"Analysis of Embeddings Learned by End-to-End Machine Learning Eye Movement-driven Biometrics Pipeline","abstract":"This paper expands on the foundational concept of temporal persistence in biometric systems, specifically focusing on the domain of eye movement biometrics facilitated by machine learning. Unlike previous studies that primarily focused on developing biometric authentication systems, our research delves into the embeddings learned by these systems, particularly examining their temporal persistence, reliability, and biometric efficacy in response to varying input data. Utilizing two publicly available eye-movement datasets, we employed the state-of-the-art Eye Know You Too machine learning pipeline for our analysis. We aim to validate whether the machine learning-derived embeddings in eye movement biometrics mirror the temporal persistence observed in traditional biometrics. Our methodology involved conducting extensive experiments to assess how different lengths and qualities of input data influence the performance of eye movement biometrics more specifically how it impacts the learned embeddings. We also explored the reliability and consistency of the embeddings under varying data conditions. Three key metrics (kendall's coefficient of concordance, intercorrelations, and equal error rate) were employed to quantitatively evaluate our findings. The results reveal while data length significantly impacts the stability of the learned embeddings, however, the intercorrelations among embeddings show minimal effect.","sentences":["This paper expands on the foundational concept of temporal persistence in biometric systems, specifically focusing on the domain of eye movement biometrics facilitated by machine learning.","Unlike previous studies that primarily focused on developing biometric authentication systems, our research delves into the embeddings learned by these systems, particularly examining their temporal persistence, reliability, and biometric efficacy in response to varying input data.","Utilizing two publicly available eye-movement datasets, we employed the state-of-the-art Eye Know You Too machine learning pipeline for our analysis.","We aim to validate whether the machine learning-derived embeddings in eye movement biometrics mirror the temporal persistence observed in traditional biometrics.","Our methodology involved conducting extensive experiments to assess how different lengths and qualities of input data influence the performance of eye movement biometrics more specifically how it impacts the learned embeddings.","We also explored the reliability and consistency of the embeddings under varying data conditions.","Three key metrics (kendall's coefficient of concordance, intercorrelations, and equal error rate) were employed to quantitatively evaluate our findings.","The results reveal while data length significantly impacts the stability of the learned embeddings, however, the intercorrelations among embeddings show minimal effect."],"url":"http://arxiv.org/abs/2402.16399v1","category":"cs.CV"}
{"created":"2024-02-26 08:28:51","title":"Information Theory Unification of Epidemiological and Population Dynamics","abstract":"We reformulate models in epidemiology and population dynamics in terms of probability distributions. This allows us to construct the Fisher information, which we interpret as the metric of a one-dimensional differentiable manifold. For systems that can be effectively described by a single degree of freedom, we show that their time evolution is fully captured by this metric. In this way, we discover universal features across seemingly very different models. This further motivates a reorganisation of the dynamics around zeroes of the Fisher metric, corresponding to extrema of the probability distribution. Concretely, we propose a simple form of the metric for which we can analytically solve the dynamics of the system that well approximates the time evolution of various established models in epidemiology and population dynamics, thus providing a unifying framework.","sentences":["We reformulate models in epidemiology and population dynamics in terms of probability distributions.","This allows us to construct the Fisher information, which we interpret as the metric of a one-dimensional differentiable manifold.","For systems that can be effectively described by a single degree of freedom, we show that their time evolution is fully captured by this metric.","In this way, we discover universal features across seemingly very different models.","This further motivates a reorganisation of the dynamics around zeroes of the Fisher metric, corresponding to extrema of the probability distribution.","Concretely, we propose a simple form of the metric for which we can analytically solve the dynamics of the system that well approximates the time evolution of various established models in epidemiology and population dynamics, thus providing a unifying framework."],"url":"http://arxiv.org/abs/2402.16390v1","category":"q-bio.PE"}
{"created":"2024-02-26 08:22:40","title":"Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values","abstract":"Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\\alpha$) without compromising the statistical power ($1-\\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty. This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, relying on the split-conformal approach for model calibration. Drawing on insights from conformal prediction, we demonstrate that the derived methods for calculating cross-conformal $p$-values strike a practical compromise between statistical efficiency (full-conformal) and computational efficiency (split-conformal) for uncertainty-quantified anomaly detection on benchmark datasets.","sentences":["Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important.","In this context, effectively controlling Type I error rates ($\\alpha$) without compromising the statistical power ($1-\\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive.","Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty.","This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks.","With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, relying on the split-conformal approach for model calibration.","Drawing on insights from conformal prediction, we demonstrate that the derived methods for calculating cross-conformal $p$-values strike a practical compromise between statistical efficiency (full-conformal) and computational efficiency (split-conformal) for uncertainty-quantified anomaly detection on benchmark datasets."],"url":"http://arxiv.org/abs/2402.16388v1","category":"stat.ML"}
{"created":"2024-02-26 07:32:56","title":"Estimation of complex carryover effects in crossover designs with repeated measures","abstract":"It has been argued that the models used to analyze data from crossover designs are not appropriate when simple carryover effects are assumed. In this paper, the estimability conditions of the carryover effects are found, and a theoretical result that supports them, additionally, two simulation examples are developed in a non-linear dose-response for a repeated measures crossover trial in two designs: the traditional AB/BA design and a Williams square. Both show that a semiparametric model can detect complex carryover effects and that this estimation improves the precision of treatment effect estimators. We concluded that when there are at least five replicates in each observation period per individual, semiparametric statistical models provide a good estimator of the treatment effect and reduce bias with respect to models that assume either, the absence of carryover or simplex carryover effects. In addition, an application of the methodology is shown and the richness in analysis that is gained by being able to estimate complex carryover effects is evident.","sentences":["It has been argued that the models used to analyze data from crossover designs are not appropriate when simple carryover effects are assumed.","In this paper, the estimability conditions of the carryover effects are found, and a theoretical result that supports them, additionally, two simulation examples are developed in a non-linear dose-response for a repeated measures crossover trial in two designs: the traditional AB/BA design and a Williams square.","Both show that a semiparametric model can detect complex carryover effects and that this estimation improves the precision of treatment effect estimators.","We concluded that when there are at least five replicates in each observation period per individual, semiparametric statistical models provide a good estimator of the treatment effect and reduce bias with respect to models that assume either, the absence of carryover or simplex carryover effects.","In addition, an application of the methodology is shown and the richness in analysis that is gained by being able to estimate complex carryover effects is evident."],"url":"http://arxiv.org/abs/2402.16362v1","category":"stat.ME"}
{"created":"2024-02-26 07:18:57","title":"An optimal tradeoff between entanglement and copy complexity for state tomography","abstract":"There has been significant interest in understanding how practical constraints on contemporary quantum devices impact the complexity of quantum learning. For the classic question of tomography, recent work tightly characterized the copy complexity for any protocol that can only measure one copy of the unknown state at a time, showing it is polynomially worse than if one can make fully-entangled measurements. While we now have a fairly complete picture of the rates for such tasks in the near-term and fault-tolerant regimes, it remains poorly understood what the landscape in between looks like.   In this work, we study tomography in the natural setting where one can make measurements of $t$ copies at a time. For sufficiently small $\\epsilon$, we show that for any $t \\le d^2$, $\\widetilde{\\Theta}(\\frac{d^3}{\\sqrt{t}\\epsilon^2})$ copies are necessary and sufficient to learn an unknown $d$-dimensional state $\\rho$ to trace distance $\\epsilon$. This gives a smooth and optimal interpolation between the known rates for single-copy and fully-entangled measurements.   To our knowledge, this is the first smooth entanglement-copy tradeoff known for any quantum learning task, and for tomography, no intermediate point on this curve was known, even at $t = 2$. An important obstacle is that unlike the optimal single-copy protocol, the optimal fully-entangled protocol is inherently biased and thus precludes naive batching approaches. Instead, we devise a novel two-stage procedure that uses Keyl's algorithm to refine a crude estimate for $\\rho$ based on single-copy measurements. A key insight is to use Schur-Weyl sampling not to estimate the spectrum of $\\rho$, but to estimate the deviation of $\\rho$ from the maximally mixed state. When $\\rho$ is far from the maximally mixed state, we devise a novel quantum splitting procedure that reduces to the case where $\\rho$ is close to maximally mixed.","sentences":["There has been significant interest in understanding how practical constraints on contemporary quantum devices impact the complexity of quantum learning.","For the classic question of tomography, recent work tightly characterized the copy complexity for any protocol that can only measure one copy of the unknown state at a time, showing it is polynomially worse than if one can make fully-entangled measurements.","While we now have a fairly complete picture of the rates for such tasks in the near-term and fault-tolerant regimes, it remains poorly understood what the landscape in between looks like.   ","In this work, we study tomography in the natural setting where one can make measurements of $t$ copies at a time.","For sufficiently small $\\epsilon$, we show that for any $t \\le d^2$, $\\widetilde{\\Theta}(\\frac{d^3}{\\sqrt{t}\\epsilon^2})$ copies are necessary and sufficient to learn an unknown $d$-dimensional state $\\rho$ to trace distance $\\epsilon$. This gives a smooth and optimal interpolation between the known rates for single-copy and fully-entangled measurements.   ","To our knowledge, this is the first smooth entanglement-copy tradeoff known for any quantum learning task, and for tomography, no intermediate point on this curve was known, even at $t = 2$.","An important obstacle is that unlike the optimal single-copy protocol, the optimal fully-entangled protocol is inherently biased and thus precludes naive batching approaches.","Instead, we devise a novel two-stage procedure that uses Keyl's algorithm to refine a crude estimate for $\\rho$ based on single-copy measurements.","A key insight is to use Schur-Weyl sampling not to estimate the spectrum of $\\rho$, but to estimate the deviation of $\\rho$ from the maximally mixed state.","When $\\rho$ is far from the maximally mixed state, we devise a novel quantum splitting procedure that reduces to the case where $\\rho$ is close to maximally mixed."],"url":"http://arxiv.org/abs/2402.16353v1","category":"quant-ph"}
{"created":"2024-02-26 07:02:05","title":"Star-Searcher: A Complete and Efficient Aerial System for Autonomous Target Search in Complex Unknown Environments","abstract":"This paper tackles the challenge of autonomous target search using unmanned aerial vehicles (UAVs) in complex unknown environments. To fill the gap in systematic approaches for this task, we introduce Star-Searcher, an aerial system featuring specialized sensor suites, mapping, and planning modules to optimize searching. Path planning challenges due to increased inspection requirements are addressed through a hierarchical planner with a visibility-based viewpoint clustering method. This simplifies planning by breaking it into global and local sub-problems, ensuring efficient global and local path coverage in real-time. Furthermore, our global path planning employs a history-aware mechanism to reduce motion inconsistency from frequent map changes, significantly enhancing search efficiency. We conduct comparisons with state-of-the-art methods in both simulation and the real world, demonstrating shorter flight paths, reduced time, and higher target search completeness. Our approach will be open-sourced for community benefit at https://github.com/SYSU-STAR/STAR-Searcher.","sentences":["This paper tackles the challenge of autonomous target search using unmanned aerial vehicles (UAVs) in complex unknown environments.","To fill the gap in systematic approaches for this task, we introduce Star-Searcher, an aerial system featuring specialized sensor suites, mapping, and planning modules to optimize searching.","Path planning challenges due to increased inspection requirements are addressed through a hierarchical planner with a visibility-based viewpoint clustering method.","This simplifies planning by breaking it into global and local sub-problems, ensuring efficient global and local path coverage in real-time.","Furthermore, our global path planning employs a history-aware mechanism to reduce motion inconsistency from frequent map changes, significantly enhancing search efficiency.","We conduct comparisons with state-of-the-art methods in both simulation and the real world, demonstrating shorter flight paths, reduced time, and higher target search completeness.","Our approach will be open-sourced for community benefit at https://github.com/SYSU-STAR/STAR-Searcher."],"url":"http://arxiv.org/abs/2402.16348v1","category":"cs.RO"}
{"created":"2024-02-26 06:55:36","title":"Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems","abstract":"Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative DDR4 + NVM hybrid memory system, Trimma achieves up to 2.4$\\times$ and on average 58.1\\% speedup benefits, compared with a state-of-the-art design that only leverages the unallocated fast memory space for caching. Trimma addresses metadata management overheads and targets future scalable large-scale hybrid memory architectures.","sentences":["Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies.","With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers.","To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design.","Trimma uses a multi-level metadata table to only track truly necessary address remap entries.","The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance.","Trimma also uses separate formats to store the entries with non-identity and identity mappings.","This improves the overall remap cache hit rate, further boosting the performance.","Trimma is transparent to software and compatible with various types of hybrid memory systems.","When evaluated on a representative DDR4 + NVM hybrid memory system, Trimma achieves up to 2.4$\\times$ and on average 58.1\\% speedup benefits, compared with a state-of-the-art design that only leverages the unallocated fast memory space for caching.","Trimma addresses metadata management overheads and targets future scalable large-scale hybrid memory architectures."],"url":"http://arxiv.org/abs/2402.16343v1","category":"cs.AR"}
{"created":"2024-02-26 06:33:19","title":"Event-Triggered Parameterized Control of Nonlinear Systems","abstract":"This paper deals with event-triggered parameterized control (ETPC) of nonlinear systems with external disturbances. In this control method, between two successive events, each control input to the plant is a linear combination of a set of linearly independent scalar functions. At each event, the controller updates the coefficients of the parameterized control input so as to minimize the error in approximating a continuous time control signal and communicates the same to the actuator. We design an event-triggering rule that guarantees global uniform ultimate boundedness of trajectories of the closed loop system. We also ensure the absence of Zeno behavior by showing the existence of a uniform positive lower bound on the inter-event times. We illustrate our results through numerical examples, which indicate that the proposed control method leads to a significant improvement in average inter-event time and minimum inter-event time compared to the event-triggered zero-order-hold control.","sentences":["This paper deals with event-triggered parameterized control (ETPC) of nonlinear systems with external disturbances.","In this control method, between two successive events, each control input to the plant is a linear combination of a set of linearly independent scalar functions.","At each event, the controller updates the coefficients of the parameterized control input so as to minimize the error in approximating a continuous time control signal and communicates the same to the actuator.","We design an event-triggering rule that guarantees global uniform ultimate boundedness of trajectories of the closed loop system.","We also ensure the absence of Zeno behavior by showing the existence of a uniform positive lower bound on the inter-event times.","We illustrate our results through numerical examples, which indicate that the proposed control method leads to a significant improvement in average inter-event time and minimum inter-event time compared to the event-triggered zero-order-hold control."],"url":"http://arxiv.org/abs/2402.16337v1","category":"math.OC"}
{"created":"2024-02-26 06:21:01","title":"Deep Rating Elicitation for New Users in Collaborative Filtering","abstract":"Recent recommender systems started to use rating elicitation, which asks new users to rate a small seed itemset for inferring their preferences, to improve the quality of initial recommendations. The key challenge of the rating elicitation is to choose the seed items which can best infer the new users' preference. This paper proposes a novel end-to-end Deep learning framework for Rating Elicitation (DRE), that chooses all the seed items at a time with consideration of the non-linear interactions. To this end, it first defines categorical distributions to sample seed items from the entire itemset, then it trains both the categorical distributions and a neural reconstruction network to infer users' preferences on the remaining items from CF information of the sampled seed items. Through the end-to-end training, the categorical distributions are learned to select the most representative seed items while reflecting the complex non-linear interactions. Experimental results show that DRE outperforms the state-of-the-art approaches in the recommendation quality by accurately inferring the new users' preferences and its seed itemset better represents the latent space than the seed itemset obtained by the other methods.","sentences":["Recent recommender systems started to use rating elicitation, which asks new users to rate a small seed itemset for inferring their preferences, to improve the quality of initial recommendations.","The key challenge of the rating elicitation is to choose the seed items which can best infer the new users' preference.","This paper proposes a novel end-to-end Deep learning framework for Rating Elicitation (DRE), that chooses all the seed items at a time with consideration of the non-linear interactions.","To this end, it first defines categorical distributions to sample seed items from the entire itemset, then it trains both the categorical distributions and a neural reconstruction network to infer users' preferences on the remaining items from CF information of the sampled seed items.","Through the end-to-end training, the categorical distributions are learned to select the most representative seed items while reflecting the complex non-linear interactions.","Experimental results show that DRE outperforms the state-of-the-art approaches in the recommendation quality by accurately inferring the new users' preferences and its seed itemset better represents the latent space than the seed itemset obtained by the other methods."],"url":"http://arxiv.org/abs/2402.16327v1","category":"cs.IR"}
{"created":"2024-02-26 06:13:24","title":"Confidence Calibration for Recommender Systems and Its Applications","abstract":"Despite the importance of having a measure of confidence in recommendation results, it has been surprisingly overlooked in the literature compared to the accuracy of the recommendation. In this dissertation, I propose a model calibration framework for recommender systems for estimating accurate confidence in recommendation results based on the learned ranking scores. Moreover, I subsequently introduce two real-world applications of confidence on recommendations: (1) Training a small student model by treating the confidence of a big teacher model as additional learning guidance, (2) Adjusting the number of presented items based on the expected user utility estimated with calibrated probability.","sentences":["Despite the importance of having a measure of confidence in recommendation results, it has been surprisingly overlooked in the literature compared to the accuracy of the recommendation.","In this dissertation, I propose a model calibration framework for recommender systems for estimating accurate confidence in recommendation results based on the learned ranking scores.","Moreover, I subsequently introduce two real-world applications of confidence on recommendations: (1) Training a small student model by treating the confidence of a big teacher model as additional learning guidance, (2) Adjusting the number of presented items based on the expected user utility estimated with calibrated probability."],"url":"http://arxiv.org/abs/2402.16325v1","category":"cs.IR"}
{"created":"2024-02-26 05:57:25","title":"Continuous Power Beaming to Lunar Far Side from EMLP-2 Halo Orbit","abstract":"This paper focuses on FSO-based wireless power transmission (WPT) from Earth-Moon Lagrangian Point-2 (EMLP-2) to a receiver optical antenna equipped with solar cells that can be located anywhere on the lunar far side (LFS). Different solar-powered satellite (SPS) configurations which are EMLP-2 located single stable satellite and EMLP-2 halo orbit revolving single, double, and triple satellites are evaluated in terms of 100% LFS surface coverage percentage (SCP) and continuous Earth visibility. It is found that an equidistant triple satellite scheme on EMLP-2 halo orbit with a semi-major axis length of 15,000 km provides full SCP for LFS and it is essential for the continuous LFS wireless power transmission. In our proposed dynamic cislunar space model, geometric and temporal parameters of the Earth-Moon systems are used in affine transformations. Our dynamic model enables us to determine the full coverage time rate of a specific region such as the LFS southern pole. The outcomes show that the equidistant double satellite scheme provides SCP=100% during 88.60% time of these satellites' single revolution around the EMLP-2 halo orbit. Finally, the probability density function (PDF) of the random harvested power $P_H$ is determined and it validates the simulation data extracted from the stable EMLP-2 satellite and revolving satellite around EMLP-2 halo orbit for minimum and maximum LoS distances. Although the pointing devices to mitigate random misalignment errors are considered for the stable and revolving SPSs, better pointing accuracy is considered for the stable satellite. Our simulations show that the probability of $P_H\\le$41.6 W is around 0.5 for the stable satellite whereas the CDF=0.99 for the revolving satellite case for a transmit power of 1 kW.","sentences":["This paper focuses on FSO-based wireless power transmission (WPT) from Earth-Moon Lagrangian Point-2 (EMLP-2) to a receiver optical antenna equipped with solar cells that can be located anywhere on the lunar far side (LFS).","Different solar-powered satellite (SPS) configurations which are EMLP-2 located single stable satellite and EMLP-2 halo orbit revolving single, double, and triple satellites are evaluated in terms of 100% LFS surface coverage percentage (SCP) and continuous Earth visibility.","It is found that an equidistant triple satellite scheme on EMLP-2 halo orbit with a semi-major axis length of 15,000 km provides full SCP for LFS and it is essential for the continuous LFS wireless power transmission.","In our proposed dynamic cislunar space model, geometric and temporal parameters of the Earth-Moon systems are used in affine transformations.","Our dynamic model enables us to determine the full coverage time rate of a specific region such as the LFS southern pole.","The outcomes show that the equidistant double satellite scheme provides SCP=100% during 88.60% time of these satellites' single revolution around the EMLP-2 halo orbit.","Finally, the probability density function (PDF) of the random harvested power $P_H$ is determined and it validates the simulation data extracted from the stable EMLP-2 satellite and revolving satellite around EMLP-2 halo orbit for minimum and maximum LoS distances.","Although the pointing devices to mitigate random misalignment errors are considered for the stable and revolving SPSs, better pointing accuracy is considered for the stable satellite.","Our simulations show that the probability of $P_H\\le$41.6 W is around 0.5 for the stable satellite whereas the CDF=0.99 for the revolving satellite case for a transmit power of 1 kW."],"url":"http://arxiv.org/abs/2402.16320v1","category":"eess.SP"}
{"created":"2024-02-26 05:13:02","title":"Analyzing Downlink Coverage in Clustered Low Earth Orbit Satellite Constellations: A Stochastic Geometry Approach","abstract":"Satellite networks are emerging as vital solutions for global connectivity beyond 5G. As companies such as SpaceX, OneWeb, and Amazon are poised to launch a large number of satellites in low Earth orbit, the heightened inter-satellite interference caused by mega-constellations has become a significant concern. To address this challenge, recent works have introduced the concept of satellite cluster networks where multiple satellites in a cluster collaborate to enhance the network performance. In order to investigate the performance of these networks, we propose mathematical analyses by modeling the locations of satellites and users using Poisson point processes, building on the success of stochastic geometry-based analyses for satellite networks. In particular, we suggest the lower and upper bounds of the coverage probability as functions of the system parameters, including satellite density, satellite altitude, satellite cluster area, path loss exponent, and Nakagami parameter $m$. We validate the analytical expressions by comparing them with simulation results. Our analyses can be used to design reliable satellite cluster networks by effectively estimating the impact of system parameters on the coverage performance.","sentences":["Satellite networks are emerging as vital solutions for global connectivity beyond 5G.","As companies such as SpaceX, OneWeb, and Amazon are poised to launch a large number of satellites in low Earth orbit, the heightened inter-satellite interference caused by mega-constellations has become a significant concern.","To address this challenge, recent works have introduced the concept of satellite cluster networks where multiple satellites in a cluster collaborate to enhance the network performance.","In order to investigate the performance of these networks, we propose mathematical analyses by modeling the locations of satellites and users using Poisson point processes, building on the success of stochastic geometry-based analyses for satellite networks.","In particular, we suggest the lower and upper bounds of the coverage probability as functions of the system parameters, including satellite density, satellite altitude, satellite cluster area, path loss exponent, and Nakagami parameter $m$. We validate the analytical expressions by comparing them with simulation results.","Our analyses can be used to design reliable satellite cluster networks by effectively estimating the impact of system parameters on the coverage performance."],"url":"http://arxiv.org/abs/2402.16307v1","category":"eess.SP"}
{"created":"2024-02-26 04:43:44","title":"Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning","abstract":"Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a \"filter bubble\". Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern. To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users' latent preferences, a hypergraph-based random walk embedding method is applied to the constructed hypergraph. In our investigation, accuracy is gauged by the alignment between the user and the track, whereas the array of recommended track types measures diversity. We rigorously compared DWHRec against seven state-of-the-art recommendation algorithms using two real-world music datasets. The experimental results validate DWHRec as a solution that adeptly harmonizes accuracy and diversity, delivering a more enriched musical experience. Beyond music recommendation, DWHRec can be extended to cater to other scenarios with similar data structures.","sentences":["Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences.","Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences.","Nevertheless, excessive personalization can confine users within a \"filter bubble\".","Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern.","To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec).","In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph.","Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph.","To explore users' latent preferences, a hypergraph-based random walk embedding method is applied to the constructed hypergraph.","In our investigation, accuracy is gauged by the alignment between the user and the track, whereas the array of recommended track types measures diversity.","We rigorously compared DWHRec against seven state-of-the-art recommendation algorithms using two real-world music datasets.","The experimental results validate DWHRec as a solution that adeptly harmonizes accuracy and diversity, delivering a more enriched musical experience.","Beyond music recommendation, DWHRec can be extended to cater to other scenarios with similar data structures."],"url":"http://arxiv.org/abs/2402.16299v1","category":"cs.IR"}
{"created":"2024-02-26 04:36:56","title":"Mean field analysis of interacting network model with jumps","abstract":"This paper considers an $n$-particle jump-diffusion system with mean filed interaction, where the coefficients are locally Lipschitz continuous. We address the convergence as $n\\to\\infty$ of the empirical measure of the jump-diffusions to the solution of a deterministic McKean-Vlasov equation. The strong well-posedness of the associated McKean-Vlasov equation and a corresponding propagation of chaos result are proven. In particular, we provide also precise estimates of the convergence speed with respect to a Wasserstein-like metric.","sentences":["This paper considers an $n$-particle jump-diffusion system with mean filed interaction, where the coefficients are locally Lipschitz continuous.","We address the convergence as $n\\to\\infty$ of the empirical measure of the jump-diffusions to the solution of a deterministic McKean-Vlasov equation.","The strong well-posedness of the associated McKean-Vlasov equation and a corresponding propagation of chaos result are proven.","In particular, we provide also precise estimates of the convergence speed with respect to a Wasserstein-like metric."],"url":"http://arxiv.org/abs/2402.16295v1","category":"math.PR"}
{"created":"2024-02-26 04:05:08","title":"Self-Assembly of Patterns in the abstract Tile Assembly Model","abstract":"In the abstract Tile Assembly Model, self-assembling systems consisting of tiles of different colors can form structures on which colored patterns are ``painted.'' We explore the complexity, in terms of the unique tile types required, of assembling various patterns, proving several upper and lower bounds.","sentences":["In the abstract Tile Assembly Model, self-assembling systems consisting of tiles of different colors can form structures on which colored patterns are ``painted.''","We explore the complexity, in terms of the unique tile types required, of assembling various patterns, proving several upper and lower bounds."],"url":"http://arxiv.org/abs/2402.16284v1","category":"cs.ET"}
{"created":"2024-02-26 03:59:30","title":"Large-Enhancement Nanoscale Dynamic Nuclear Polarization Near a Silicon Nanowire Surface","abstract":"Dynamic nuclear polarization (DNP) has revolutionized the field of NMR spectroscopy, expanding its reach and capabilities to investigate diverse materials, biomolecules, and complex dynamic processes. Bringing high-efficiency DNP to the nanometer scale would open new avenues for studying nanoscale nuclear spin ensembles, such as single biomolecules, virus particles, and condensed matter systems. Combining pulsed DNP with nanoscale force-detected magnetic resonance measurements, we demonstrated a 100-fold enhancement in the Boltzmann polarization of proton spins in nanoscale sugar droplets at 6 K and 0.33 T. Crucially, this enhancement corresponds to a factor of 200 reduction in the averaging time compared to measurements that rely on the detection of statistical fluctuations in nanoscale nuclear spin ensembles. These results significantly advance the capabilities of force-detected magnetic resonance detection as a practical tool for nanoscale imaging.","sentences":["Dynamic nuclear polarization (DNP) has revolutionized the field of NMR spectroscopy, expanding its reach and capabilities to investigate diverse materials, biomolecules, and complex dynamic processes.","Bringing high-efficiency DNP to the nanometer scale would open new avenues for studying nanoscale nuclear spin ensembles, such as single biomolecules, virus particles, and condensed matter systems.","Combining pulsed DNP with nanoscale force-detected magnetic resonance measurements, we demonstrated a 100-fold enhancement in the Boltzmann polarization of proton spins in nanoscale sugar droplets at 6 K and 0.33 T. Crucially, this enhancement corresponds to a factor of 200 reduction in the averaging time compared to measurements that rely on the detection of statistical fluctuations in nanoscale nuclear spin ensembles.","These results significantly advance the capabilities of force-detected magnetic resonance detection as a practical tool for nanoscale imaging."],"url":"http://arxiv.org/abs/2402.16283v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-26 03:22:24","title":"Topological transitions by magnetization rotation in kagome monolayers of ferromagnetic Weyl semimetal Co-based shandite","abstract":"Co-based shandite Co$_3$Sn$_2$S$_2$ is a ferromagnet hosting Weyl fermions in the layered Co kagome structure. The band topology as well as the magnetism is predicted to vary drastically in the atomically thin films depending on the thickness and surface termination, and as an extreme case, the quantum anomalous Hall state is expected in a monolayer of the Co kagome lattice. Given that the bulk Weyl gap depends on the magnetization direction, here we theoretically study how the topological nature and transport properties vary with the magnetization direction in the systems with kagome monolayer with both Sn and S surface terminations. By using $ab \\ initio$ calculations, we find that in the Sn-end monolayer the anomalous Hall conductivity shows successive discrete changes between different quantized values by rotating the magnetization, indicating several topological transitions between the anomalous quantum Hall insulators with different Chern numbers. Notably, when the magnetization is oriented in-plane and perpendicular to the Co-Co bond, the system exhibits a planar quantized anomalous Hall effect. We clarify that these peculiar behaviors are due to topological changes in the band structures associated with gap closing of the Weyl nodes. In contrast, the S-end monolayer shows rather continuous changes in the transport properties since the system is metallic, although the band structure contains many Weyl nodes. Our results pave the way for controlling Weyl fermions in atomically thin films of Co-based shandite, where the topological nature associated with the Weyl nodes appears more clearly than the bulk.","sentences":["Co-based shandite Co$_3$Sn$_2$S$_2$ is a ferromagnet hosting Weyl fermions in the layered Co kagome structure.","The band topology as well as the magnetism is predicted to vary drastically in the atomically thin films depending on the thickness and surface termination, and as an extreme case, the quantum anomalous Hall state is expected in a monolayer of the Co kagome lattice.","Given that the bulk Weyl gap depends on the magnetization direction, here we theoretically study how the topological nature and transport properties vary with the magnetization direction in the systems with kagome monolayer with both Sn and S surface terminations.","By using $ab \\ initio$ calculations, we find that in the Sn-end monolayer the anomalous Hall conductivity shows successive discrete changes between different quantized values by rotating the magnetization, indicating several topological transitions between the anomalous quantum Hall insulators with different Chern numbers.","Notably, when the magnetization is oriented in-plane and perpendicular to the Co-Co bond, the system exhibits a planar quantized anomalous Hall effect.","We clarify that these peculiar behaviors are due to topological changes in the band structures associated with gap closing of the Weyl nodes.","In contrast, the S-end monolayer shows rather continuous changes in the transport properties since the system is metallic, although the band structure contains many Weyl nodes.","Our results pave the way for controlling Weyl fermions in atomically thin films of Co-based shandite, where the topological nature associated with the Weyl nodes appears more clearly than the bulk."],"url":"http://arxiv.org/abs/2402.16273v1","category":"cond-mat.str-el"}
{"created":"2024-02-26 03:21:02","title":"Mass production and performance study on the 20-inch PMT acrylic protection covers in JUNO","abstract":"The Jiangmen Underground Neutrino Observatory is a neutrino experiment that incorporates 20,012 20-inch photomultiplier tubes (PMTs) and 25,600 3-inch PMTs. A dedicated system was designed to protect the PMTs from an implosion chain reaction underwater. As a crucial element of the protection system, over 20,000 acrylic covers were manufactured through injection molding, ensuring high dimensional precision, mechanical strength, and transparency. This paper presents the manufacturing technology, mass production process, and performance characteristics of the acrylic covers.","sentences":["The Jiangmen Underground Neutrino Observatory is a neutrino experiment that incorporates 20,012 20-inch photomultiplier tubes (PMTs) and 25,600 3-inch PMTs.","A dedicated system was designed to protect the PMTs from an implosion chain reaction underwater.","As a crucial element of the protection system, over 20,000 acrylic covers were manufactured through injection molding, ensuring high dimensional precision, mechanical strength, and transparency.","This paper presents the manufacturing technology, mass production process, and performance characteristics of the acrylic covers."],"url":"http://arxiv.org/abs/2402.16272v1","category":"physics.ins-det"}
{"created":"2024-02-26 03:15:13","title":"Rebuildable biochronometer: inferences and hypothesis on eukaryotic timing system","abstract":"The biochronometers used to keep time in eukaryotes include short-period biochronometer (SPB) and long-period biochronometer (LPB). Because the circadian clock reflects the biological time rhythm of a day, it is considered as SPB. Telomere shortening, which reflects the decreasing of telomere DNA length of chromosomes with the increase of cell division times, can be used to time the lifespan of organisms, so it is regarded as LPB. It is confirmed that SPB and LPB exist in most eukaryotes, and it is speculated that SPB and LPB are closely related. In this paper, based on existing studies, it is speculated that SPB and LPB of most eukaryotes can be co-attenuated with cell division in the process of aging. Due to the attenuated phenomenon of key components in the biochronometers during the growth and development of organisms, the biochronometers attenuate with the aging. Based on existing research results, it is preliminarily determined that the biochronometers can be rebuilt in the co-attenuated process. When the key components of biochronometers are reversed and increased in the organism, it can lead to the reversal of biochronometers, which further leads to the phenomenon of biological rejuvenation and makes the organism younger. In addition, the rebuilding of biochronometers can also lead to the acceleration of biochronometers and the shortening of the original timing time of biochronometers, thus shortening the life span of organisms. The rebuilding of biochronometers includes the reversal of biochronometers, the truncation of biochronometers timing and Uncoordinated co-attenuation of biochronometer and so on. The reversal of the biochronometers, which leads to rejuvenation, can give us a whole new understanding of life expectancy to be different from anti-aging.","sentences":["The biochronometers used to keep time in eukaryotes include short-period biochronometer (SPB) and long-period biochronometer (LPB).","Because the circadian clock reflects the biological time rhythm of a day, it is considered as SPB.","Telomere shortening, which reflects the decreasing of telomere DNA length of chromosomes with the increase of cell division times, can be used to time the lifespan of organisms, so it is regarded as LPB.","It is confirmed that SPB and LPB exist in most eukaryotes, and it is speculated that SPB and LPB are closely related.","In this paper, based on existing studies, it is speculated that SPB and LPB of most eukaryotes can be co-attenuated with cell division in the process of aging.","Due to the attenuated phenomenon of key components in the biochronometers during the growth and development of organisms, the biochronometers attenuate with the aging.","Based on existing research results, it is preliminarily determined that the biochronometers can be rebuilt in the co-attenuated process.","When the key components of biochronometers are reversed and increased in the organism, it can lead to the reversal of biochronometers, which further leads to the phenomenon of biological rejuvenation and makes the organism younger.","In addition, the rebuilding of biochronometers can also lead to the acceleration of biochronometers and the shortening of the original timing time of biochronometers, thus shortening the life span of organisms.","The rebuilding of biochronometers includes the reversal of biochronometers, the truncation of biochronometers timing and Uncoordinated co-attenuation of biochronometer and so on.","The reversal of the biochronometers, which leads to rejuvenation, can give us a whole new understanding of life expectancy to be different from anti-aging."],"url":"http://arxiv.org/abs/2402.16271v1","category":"q-bio.BM"}
{"created":"2024-02-26 02:58:18","title":"Giant resonant skew scattering of plasma waves in graphene off a micromagnet","abstract":"Electron skew scattering by impurities is one of the major mechanisms behind the anomalous Hall effect in ferromagnetic nanostructures. It is particularly strong at the surface of topological insulators where the Dirac equation governs electron dynamics. Motivated by recently discovered mappings between hydrodynamics and spin-1 Dirac equations, we consider the scattering of plasma waves -- propagating charge density oscillations -- excited in graphene off a non-uniform magnetic field created by an adjacent circular micromagnet. The calculated scattering amplitude not only exhibits a giant asymmetry, or skewness, but is resonantly enhanced if the frequency of the incoming wave matches the frequency of the trapped mode circulating the micromagnet in only one direction. Furthermore, if the frequency of incoming waves exceeds the Larmor frequency, the angular distribution of scattered plasma waves is indistinguishable from the one of Dirac electrons at the surface of a topological insulator scattering off a magnetic impurity. The micrometer scale of the proposed setup enables direct investigations of individual skew scattering events previously inaccessible in electronic systems.","sentences":["Electron skew scattering by impurities is one of the major mechanisms behind the anomalous Hall effect in ferromagnetic nanostructures.","It is particularly strong at the surface of topological insulators where the Dirac equation governs electron dynamics.","Motivated by recently discovered mappings between hydrodynamics and spin-1 Dirac equations, we consider the scattering of plasma waves -- propagating charge density oscillations -- excited in graphene off a non-uniform magnetic field created by an adjacent circular micromagnet.","The calculated scattering amplitude not only exhibits a giant asymmetry, or skewness, but is resonantly enhanced if the frequency of the incoming wave matches the frequency of the trapped mode circulating the micromagnet in only one direction.","Furthermore, if the frequency of incoming waves exceeds the Larmor frequency, the angular distribution of scattered plasma waves is indistinguishable from the one of Dirac electrons at the surface of a topological insulator scattering off a magnetic impurity.","The micrometer scale of the proposed setup enables direct investigations of individual skew scattering events previously inaccessible in electronic systems."],"url":"http://arxiv.org/abs/2402.16263v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-26 02:45:19","title":"Distributed Finite-time Differentiator for Multi-agent Systems Under Directed Graph","abstract":"This paper proposes a new distributed finite-time differentiator (DFD) for multi-agent systems (MAS) under directed graph, which extends the differentiator algorithm from the centralized case to the distributed case by only using relative/absolute position information. By skillfully constructing a Lyapunov function, the finite-time stability of the closed-loop system under DFD is proved. Inspired by the duality principle of control theory, a distributed continuous finite-time output consensus algorithm extended from DFD for a class of leader-follower MAS is provided, which not only completely suppresses disturbance, but also avoids chattering. Finally, several simulation examples are given to verify the effectiveness of the DFD.","sentences":["This paper proposes a new distributed finite-time differentiator (DFD) for multi-agent systems (MAS) under directed graph, which extends the differentiator algorithm from the centralized case to the distributed case by only using relative/absolute position information.","By skillfully constructing a Lyapunov function, the finite-time stability of the closed-loop system under DFD is proved.","Inspired by the duality principle of control theory, a distributed continuous finite-time output consensus algorithm extended from DFD for a class of leader-follower MAS is provided, which not only completely suppresses disturbance, but also avoids chattering.","Finally, several simulation examples are given to verify the effectiveness of the DFD."],"url":"http://arxiv.org/abs/2402.16260v1","category":"cs.MA"}
{"created":"2024-02-26 02:35:16","title":"Evidence of a Four-Body Force in an Interaction-Tunable Trapped Cold-Atom System","abstract":"A two-body interaction or force between quantum particles is ubiquitous in nature, and the microscopic description in terms of the bare two-body interaction is the basis for quantitatively describing interacting few- and many-body systems. Alternatively, the effective description in terms of an effective two-body interaction successfully captures the essence of the systems. However, for several important observations, the explanation in terms of an effective two-body interaction is not satisfactory, and the effective three-body interaction has played an essential role in understanding the systems. In this study, we investigate a few-body system comprising of ultracold bosons tightly confined in a deep optical lattice site, which is effectively described as zero-dimensional bosons. By combining an occupancy-resolving high-resolution laser spectroscopy with an inter-orbital Feshbach resonance controlling the bare two-body interaction over a wide range, we obtain a clear evidence of an effective four-body force, which has never been observed in any few-body quantum system so far. This will open the door for the study of multi-body forces in various few-body systems.","sentences":["A two-body interaction or force between quantum particles is ubiquitous in nature, and the microscopic description in terms of the bare two-body interaction is the basis for quantitatively describing interacting few- and many-body systems.","Alternatively, the effective description in terms of an effective two-body interaction successfully captures the essence of the systems.","However, for several important observations, the explanation in terms of an effective two-body interaction is not satisfactory, and the effective three-body interaction has played an essential role in understanding the systems.","In this study, we investigate a few-body system comprising of ultracold bosons tightly confined in a deep optical lattice site, which is effectively described as zero-dimensional bosons.","By combining an occupancy-resolving high-resolution laser spectroscopy with an inter-orbital Feshbach resonance controlling the bare two-body interaction over a wide range, we obtain a clear evidence of an effective four-body force, which has never been observed in any few-body quantum system so far.","This will open the door for the study of multi-body forces in various few-body systems."],"url":"http://arxiv.org/abs/2402.16254v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-26 02:13:36","title":"Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition","abstract":"In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learning (ECTL), in which an agent is trained in self-play with EC and then learns from the data to translate between the emergent protocol and the target community's protocol.","sentences":["In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community.","This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training.","However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting.","In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions.","We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community.","We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learning (ECTL), in which an agent is trained in self-play with EC and then learns from the data to translate between the emergent protocol and the target community's protocol."],"url":"http://arxiv.org/abs/2402.16247v1","category":"cs.LG"}
{"created":"2024-02-26 02:02:00","title":"Green functions of mixed boundary value problems for stationary Stokes systems in two dimensions","abstract":"We establish the existence, uniqueness, and various estimates for Green functions of mixed Dirichlet-conormal derivative problems for the stationary Stokes system with measurable coefficients in a two-dimensional Reifenberg flat domain with a rough separation.","sentences":["We establish the existence, uniqueness, and various estimates for Green functions of mixed Dirichlet-conormal derivative problems for the stationary Stokes system with measurable coefficients in a two-dimensional Reifenberg flat domain with a rough separation."],"url":"http://arxiv.org/abs/2402.16241v1","category":"math.AP"}
{"created":"2024-02-26 01:52:39","title":"Growth of groups with incompressible elements, I","abstract":"We define the class of groups of bounded type with finite cycles from tile inflations. These tile inflations also determine some automata describing the groups. In the case when the automata are finite-state, we show that if the set of incompressible elements of a group in this class is finite, then this group has subexponential growth with a bounded power in the exponent. Then we discuss some special cases with certain special structures of orbital graphs and give explicit ways to find the upper bounds for the growth functions of these groups.","sentences":["We define the class of groups of bounded type with finite cycles from tile inflations.","These tile inflations also determine some automata describing the groups.","In the case when the automata are finite-state, we show that if the set of incompressible elements of a group in this class is finite, then this group has subexponential growth with a bounded power in the exponent.","Then we discuss some special cases with certain special structures of orbital graphs and give explicit ways to find the upper bounds for the growth functions of these groups."],"url":"http://arxiv.org/abs/2402.16238v1","category":"math.GR"}
{"created":"2024-02-26 01:30:51","title":"Onsager Relations between Spin Currents and Charge Currents","abstract":"We consider the macroscopic dynamics of systems with charge and spin currents, using the methods of Onsager's irreversible thermodynamics. Applied to systems with spin-orbit interaction (SOI), we derive Onsager relations showing that, if electrical disequilibrium leads to spin currents, then magnetic disequilibrium leads to charge currents. We consider three examples of such SOI. Two of these predicted charge currents have not previously appeared. By measuring these charge currents one can infer the corresponding spin currents.","sentences":["We consider the macroscopic dynamics of systems with charge and spin currents, using the methods of Onsager's irreversible thermodynamics.","Applied to systems with spin-orbit interaction (SOI), we derive Onsager relations showing that, if electrical disequilibrium leads to spin currents, then magnetic disequilibrium leads to charge currents.","We consider three examples of such SOI.","Two of these predicted charge currents have not previously appeared.","By measuring these charge currents one can infer the corresponding spin currents."],"url":"http://arxiv.org/abs/2402.16233v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-26 01:03:27","title":"Scaling Robust Optimization for Multi-Agent Robotic Systems: A Distributed Perspective","abstract":"This paper presents a novel distributed robust optimization scheme for steering distributions of multi-agent systems under stochastic and deterministic uncertainty. Robust optimization is a subfield of optimization which aims in discovering an optimal solution that remains robustly feasible for all possible realizations of the problem parameters within a given uncertainty set. Such approaches would naturally constitute an ideal candidate for multi-robot control, where in addition to stochastic noise, there might be exogenous deterministic disturbances. Nevertheless, as these methods are usually associated with significantly high computational demands, their application to multi-agent robotics has remained limited. The scope of this work is to propose a scalable robust optimization framework that effectively addresses both types of uncertainties, while retaining computational efficiency and scalability. In this direction, we provide tractable approximations for robust constraints that are relevant in multi-robot settings. Subsequently, we demonstrate how computations can be distributed through an Alternating Direction Method of Multipliers (ADMM) approach towards achieving scalability and communication efficiency. Simulation results highlight the performance of the proposed algorithm in effectively handling both stochastic and deterministic uncertainty in multi-robot systems. The scalability of the method is also emphasized by showcasing tasks with up to 100 agents. The results of this work indicate the promise of blending robust optimization, distribution steering and distributed optimization towards achieving scalable, safe and robust multi-robot control.","sentences":["This paper presents a novel distributed robust optimization scheme for steering distributions of multi-agent systems under stochastic and deterministic uncertainty.","Robust optimization is a subfield of optimization which aims in discovering an optimal solution that remains robustly feasible for all possible realizations of the problem parameters within a given uncertainty set.","Such approaches would naturally constitute an ideal candidate for multi-robot control, where in addition to stochastic noise, there might be exogenous deterministic disturbances.","Nevertheless, as these methods are usually associated with significantly high computational demands, their application to multi-agent robotics has remained limited.","The scope of this work is to propose a scalable robust optimization framework that effectively addresses both types of uncertainties, while retaining computational efficiency and scalability.","In this direction, we provide tractable approximations for robust constraints that are relevant in multi-robot settings.","Subsequently, we demonstrate how computations can be distributed through an Alternating Direction Method of Multipliers (ADMM) approach towards achieving scalability and communication efficiency.","Simulation results highlight the performance of the proposed algorithm in effectively handling both stochastic and deterministic uncertainty in multi-robot systems.","The scalability of the method is also emphasized by showcasing tasks with up to 100 agents.","The results of this work indicate the promise of blending robust optimization, distribution steering and distributed optimization towards achieving scalable, safe and robust multi-robot control."],"url":"http://arxiv.org/abs/2402.16227v1","category":"cs.RO"}
{"created":"2024-02-25 23:58:41","title":"Discrete Fourier Transform Approximations Based on the Cooley-Tukey Radix-2 Algorithm","abstract":"This report elaborates on approximations for the discrete Fourier transform by means of replacing the exact Cooley-Tukey algorithm twiddle-factors by low-complexity integers, such as $0, \\pm \\frac{1}{2}, \\pm 1$.","sentences":["This report elaborates on approximations for the discrete Fourier transform by means of replacing the exact Cooley-Tukey algorithm twiddle-factors by low-complexity integers, such as $0, \\pm \\frac{1}{2}, \\pm 1$."],"url":"http://arxiv.org/abs/2402.16225v1","category":"eess.SP"}
{"created":"2024-02-25 22:56:02","title":"System size scaling of triangularity effects on global temperature gradient-driven gyrokinetic simulations","abstract":"In this work, we explore the triangularity effects on turbulent transport employing global gyrokinetic simulations performed with the ORB5 code. Numerous experiments on the Tokamak \\`a Configuration Variable (TCV) and, more recently, on the DIII-D machine, have demonstrated superior confinement properties in L-mode of negative triangularity (NT) over positive triangularity (PT) configuration. This presents a particularly attractive scenario, as L-mode operation eliminates or significantly mitigates the presence of hazardous edge-localized modes (ELMs). However, a full theoretical understanding of all these observations remains elusive. Specifically, questions remain about how NT improvements can extend to the core where triangularity is very small, and whether these improvements can scale to larger devices. This paper addresses these two questions. Our analysis is divided into two parts: we first demonstrate that the confinement enhancement in NT configurations arises from the interdependent edge-core dynamics, and then we present the results of a system size scan. Crucially, we find that the relative turbulent transport reduction of NT over PT appears not to be contingent on machine dimensions or fluctuation scales and is moreover robust with respect to variations in plasma profiles. This insight underscores the fundamental nature of the NT confinement advantage and paves the way for its potential application in future fusion devices, regardless of their size.","sentences":["In this work, we explore the triangularity effects on turbulent transport employing global gyrokinetic simulations performed with the ORB5 code.","Numerous experiments on the Tokamak \\`a Configuration Variable (TCV) and, more recently, on the DIII-D machine, have demonstrated superior confinement properties in L-mode of negative triangularity (NT) over positive triangularity (PT) configuration.","This presents a particularly attractive scenario, as L-mode operation eliminates or significantly mitigates the presence of hazardous edge-localized modes (ELMs).","However, a full theoretical understanding of all these observations remains elusive.","Specifically, questions remain about how NT improvements can extend to the core where triangularity is very small, and whether these improvements can scale to larger devices.","This paper addresses these two questions.","Our analysis is divided into two parts: we first demonstrate that the confinement enhancement in NT configurations arises from the interdependent edge-core dynamics, and then we present the results of a system size scan.","Crucially, we find that the relative turbulent transport reduction of NT over PT appears not to be contingent on machine dimensions or fluctuation scales and is moreover robust with respect to variations in plasma profiles.","This insight underscores the fundamental nature of the NT confinement advantage and paves the way for its potential application in future fusion devices, regardless of their size."],"url":"http://arxiv.org/abs/2402.16216v1","category":"physics.plasm-ph"}
{"created":"2024-02-25 22:17:48","title":"Complexity function of the most significant digits of $2^N^D$","abstract":"We investigate unipotent dynamics on a torus and apply it to the following problem. Let $d$ be a positive integer. Consider the sequence of digits $(\\textsc{\\textbf{w}}_n)$, where $\\textsc{\\textbf{w}}_n$ is the most digit in the decimal representation of $2^{n^d}$. We prove that the complexity function of the sequence $(\\textsc{\\textbf{w}}_n)$ is, up to finitely many values, a polynomial.","sentences":["We investigate unipotent dynamics on a torus and apply it to the following problem.","Let $d$ be a positive integer.","Consider the sequence of digits $(\\textsc{\\textbf{w}}_n)$, where $\\textsc{\\textbf{w}}_n$ is the most digit in the decimal representation of $2^{n^d}$. We prove that the complexity function of the sequence $(\\textsc{\\textbf{w}}_n)$ is, up to finitely many values, a polynomial."],"url":"http://arxiv.org/abs/2402.16210v1","category":"math.DS"}
{"created":"2024-02-25 21:58:52","title":"Enhanced Graph Pattern Matching","abstract":"Pattern matching queries on strings can be solved in linear time by Knuth-Morris-Pratt (KMP) algorithm. In 1973, Weiner introduced the suffix tree of a string [FOCS 1973] and showed that the seemingly more difficult problem of computing matching statistics can also be solved in liner time. Pattern matching queries on graphs are inherently more difficult: under the Orthogonal Vector hypothesis, the graph pattern matching problem cannot be solved in subquadratic time [TALG 2023]. The complexity of graph pattern matching can be parameterized by the topological complexity of the considered graph, which is captured by a parameter $ p $ [JACM 2023].   In this paper, we show that, as in the string setting, computing matching statistics on graph is as difficult as solving standard pattern matching queries. To this end, we introduce a notion of longest common prefix (LCP) array for arbitrary graphs.","sentences":["Pattern matching queries on strings can be solved in linear time by Knuth-Morris-Pratt (KMP) algorithm.","In 1973, Weiner introduced the suffix tree of a string [FOCS 1973] and showed that the seemingly more difficult problem of computing matching statistics can also be solved in liner time.","Pattern matching queries on graphs are inherently more difficult: under the Orthogonal Vector hypothesis, the graph pattern matching problem cannot be solved in subquadratic time [TALG 2023].","The complexity of graph pattern matching can be parameterized by the topological complexity of the considered graph, which is captured by a parameter $ p $","[JACM 2023].   ","In this paper, we show that, as in the string setting, computing matching statistics on graph is as difficult as solving standard pattern matching queries.","To this end, we introduce a notion of longest common prefix (LCP) array for arbitrary graphs."],"url":"http://arxiv.org/abs/2402.16205v1","category":"cs.DS"}
{"created":"2024-02-25 21:29:44","title":"Honeybee: Decentralized Peer Sampling with Verifiable Random Walks for Blockchain Data Sharding","abstract":"Data sharding - in which block data is sharded without sharding compute - is at the present the favored approach for scaling Ethereum. A key challenge toward implementing data sharding is verifying whether the entirety of a block's data is available in the network (across its shards). A central technique proposed to conduct this verification uses erasure coded blocks and is called data availability sampling (DAS). While the high-level protocol details of DAS has been well discussed in the community, discussions around how such a protocol will be implemented at the peer-to-peer layer are lacking. We identify random sampling of nodes as a fundamental primitive necessary to carry out DAS and present Honeybee, a decentralized algorithm for sampling node that uses verifiable random walks. Honeybee is secure against attacks even in the presence of a large number of Byzantine nodes (e.g., 50% of the network). We evaluate Honeybee through experiments and show that the quality of sampling achieved by Honeybee is significantly better compared to the state-of-the-art. Our proposed algorithm has implications for DAS functions in both full nodes and light nodes.","sentences":["Data sharding - in which block data is sharded without sharding compute - is at the present the favored approach for scaling Ethereum.","A key challenge toward implementing data sharding is verifying whether the entirety of a block's data is available in the network (across its shards).","A central technique proposed to conduct this verification uses erasure coded blocks and is called data availability sampling (DAS).","While the high-level protocol details of DAS has been well discussed in the community, discussions around how such a protocol will be implemented at the peer-to-peer layer are lacking.","We identify random sampling of nodes as a fundamental primitive necessary to carry out DAS and present Honeybee, a decentralized algorithm for sampling node that uses verifiable random walks.","Honeybee is secure against attacks even in the presence of a large number of Byzantine nodes (e.g., 50% of the network).","We evaluate Honeybee through experiments and show that the quality of sampling achieved by Honeybee is significantly better compared to the state-of-the-art.","Our proposed algorithm has implications for DAS functions in both full nodes and light nodes."],"url":"http://arxiv.org/abs/2402.16201v1","category":"cs.NI"}
{"created":"2024-02-25 21:21:43","title":"High-resolution numerical-experimental comparison of heterogeneous slip activity in quasi-2D ferrite sheets","abstract":"The role of heterogeneity in the plastic flow of thin ferrite specimens is investigated in this study. This is done through a recently introduced quasi-2D experimental-numerical framework that allows for a quantitative comparison of the deformation fields of metal microstructures between experiments and simulations at a high level of detail and complexity. The method exploits samples that are locally ultra-thin (\"2D\") and hence have a practically uniform microstructure through their thickness. This allows testing more complex loading conditions compared to uniaxial micromechanical experiments while avoiding the complexity of an unknown subsurface microstructure, which limits comparisons between experiments and simulations in traditional integrated approaches at the level of the polycrystalline microstructure. The present approach enables to study the effect of microstructural features such as grain boundaries. To study the role of stochastic fluctuations, a constitutive model is employed which introduces random heterogeneity into a crystal plasticity model. A detailed analysis of the simulations is performed at the level of individual slip systems. Since both experimental and numerical results are susceptible to stochastic fluctuations, the outcomes of many simulations are compared to the experimentally obtained result. This comparison allows us to determine how a single experiment relates to an ensemble of simulations. Additionally, results obtained with a conventional crystal plasticity model are considered. The analysis reveals that the heterogeneity in the plasticity model is essential for accurately capturing the deformation mechanisms.","sentences":["The role of heterogeneity in the plastic flow of thin ferrite specimens is investigated in this study.","This is done through a recently introduced quasi-2D experimental-numerical framework that allows for a quantitative comparison of the deformation fields of metal microstructures between experiments and simulations at a high level of detail and complexity.","The method exploits samples that are locally ultra-thin (\"2D\") and hence have a practically uniform microstructure through their thickness.","This allows testing more complex loading conditions compared to uniaxial micromechanical experiments while avoiding the complexity of an unknown subsurface microstructure, which limits comparisons between experiments and simulations in traditional integrated approaches at the level of the polycrystalline microstructure.","The present approach enables to study the effect of microstructural features such as grain boundaries.","To study the role of stochastic fluctuations, a constitutive model is employed which introduces random heterogeneity into a crystal plasticity model.","A detailed analysis of the simulations is performed at the level of individual slip systems.","Since both experimental and numerical results are susceptible to stochastic fluctuations, the outcomes of many simulations are compared to the experimentally obtained result.","This comparison allows us to determine how a single experiment relates to an ensemble of simulations.","Additionally, results obtained with a conventional crystal plasticity model are considered.","The analysis reveals that the heterogeneity in the plasticity model is essential for accurately capturing the deformation mechanisms."],"url":"http://arxiv.org/abs/2402.16199v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-25 20:39:44","title":"Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim","abstract":"Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems. However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging.   We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim. SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients. We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. We additionally provide an OpenFOAM sub-module with examples that can be used as starting points for real-world applications in CFD+ML.","sentences":["Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems.","However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging.   ","We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim.","SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients.","We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers.","We additionally provide an OpenFOAM sub-module with examples that can be used as starting points for real-world applications in CFD+ML."],"url":"http://arxiv.org/abs/2402.16196v1","category":"cs.LG"}
{"created":"2024-02-25 20:34:03","title":"Classical Poisson structures from deformations of noncommutative algebras","abstract":"It is well-known that a formal deformation of a commutative algebra ${\\mathcal A}$ leads to a Poisson bracket on ${\\mathcal A}$ and that the classical limit of a derivation on the deformation leads to a derivation on ${\\mathcal A}$, which is Hamiltonian with respect to the Poisson bracket. In this paper we present a generalisation of it for formal deformations of an arbitrary noncommutative algebra ${\\mathcal A}$. The deformation leads in this case to a Poisson algebra structure on $\\Pi({\\mathcal A}):=Z({\\mathcal A})\\times({\\mathcal A}/Z({\\mathcal A}))$ and to the structure of a $\\Pi({\\mathcal A})$-Poisson module on ${\\mathcal A}$. The limiting derivations are then still derivations of ${\\mathcal A}$, but with the Hamiltonian belong to $\\Pi({\\mathcal A})$, rather than to ${\\mathcal A}$. We illustrate our construction with several cases of formal deformations, coming from known quantum algebras, such as the ones associated with the nonabelian Volterra chains, Kontsevich integrable map, the quantum plane and the quantised Grassmann algebra.","sentences":["It is well-known that a formal deformation of a commutative algebra ${\\mathcal A}$ leads to a Poisson bracket on ${\\mathcal A}$ and that the classical limit of a derivation on the deformation leads to a derivation on ${\\mathcal A}$, which is Hamiltonian with respect to the Poisson bracket.","In this paper we present a generalisation of it for formal deformations of an arbitrary noncommutative algebra ${\\mathcal A}$. The deformation leads in this case to a Poisson algebra structure on $\\Pi({\\mathcal A}):=Z({\\mathcal A})\\times({\\mathcal A}/Z({\\mathcal A}))$ and to the structure of a $\\Pi({\\mathcal A})$-Poisson module on ${\\mathcal A}$.","The limiting derivations are then still derivations of ${\\mathcal A}$, but with the Hamiltonian belong to $\\Pi({\\mathcal A})$, rather than to ${\\mathcal A}$.","We illustrate our construction with several cases of formal deformations, coming from known quantum algebras, such as the ones associated with the nonabelian Volterra chains, Kontsevich integrable map, the quantum plane and the quantised Grassmann algebra."],"url":"http://arxiv.org/abs/2402.16191v1","category":"nlin.SI"}
{"created":"2024-02-25 17:40:49","title":"DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem","abstract":"This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.","sentences":["This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems.","Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process.","This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach.","By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators.","It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin.","We also show the effectiveness of NER in the downstream task of relation extraction."],"url":"http://arxiv.org/abs/2402.16159v1","category":"cs.CL"}
{"created":"2024-02-25 17:35:31","title":"Consensus learning: A novel decentralised ensemble learning paradigm","abstract":"The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability. This work introduces a novel distributed machine learning paradigm -- \\emph{consensus learning} -- which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems. These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol. Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism. We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithms. The discussion is supplemented by various numerical simulations, which describe the robustness of the algorithms against Byzantine participants.","sentences":["The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability.","This work introduces a novel distributed machine learning paradigm -- \\emph{consensus learning} -- which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems.","These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol.","Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism.","We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithms.","The discussion is supplemented by various numerical simulations, which describe the robustness of the algorithms against Byzantine participants."],"url":"http://arxiv.org/abs/2402.16157v1","category":"cs.LG"}
{"created":"2024-02-25 16:54:09","title":"100 Gbps Indoor Access and 4.8 Gbps Outdoor Point-to-Point LiFi Transmission Systems using Laser-based Light Sources","abstract":"In this paper, we demonstrate the communication capabilities of light-fidelity (LiFi) systems based on highbrightness and high-bandwidth integrated laser-based sources in a surface mount device (SMD) packaging platform. The laserbased source is able to deliver 450 lumens of white light illumination and the resultant light brightness is over 1000 cd mm2. It is demonstrated that a wavelength division multiplexing (WDM) LiFi system with ten parallel channels is able to deliver over 100 Gbps data rate with the assistance of Volterra filter-based nonlinear equalisers. In addition, an aggregated transmission data rate of 4.8 Gbps has been achieved over a link distance of 500 m with the same type of SMD light source. This work demonstrates the scalability of LiFi systems that employ laserbased light sources, particularly in their capacity to enable highspeed short range, as well as long-range data transmission.","sentences":["In this paper, we demonstrate the communication capabilities of light-fidelity (LiFi) systems based on highbrightness and high-bandwidth integrated laser-based sources in a surface mount device (SMD) packaging platform.","The laserbased source is able to deliver 450 lumens of white light illumination and the resultant light brightness is over 1000 cd mm2.","It is demonstrated that a wavelength division multiplexing (WDM) LiFi system with ten parallel channels is able to deliver over 100 Gbps data rate with the assistance of Volterra filter-based nonlinear equalisers.","In addition, an aggregated transmission data rate of 4.8 Gbps has been achieved over a link distance of 500 m with the same type of SMD light source.","This work demonstrates the scalability of LiFi systems that employ laserbased light sources, particularly in their capacity to enable highspeed short range, as well as long-range data transmission."],"url":"http://arxiv.org/abs/2402.16144v1","category":"eess.SY"}
{"created":"2024-02-25 15:55:00","title":"Liquid-liquid transition in a Bose fluid near collapse","abstract":"Discovering novel emergent behavior in quantum many-body systems is a main objective of contemporary research. In this paper, we explore the effects on phases and phase transitions of the proximity to a Ruelle instability. To accomplish this, we study by quantum Monte Carlo simulations a two-dimensional system of finite-ranged attractive potential with soft-core repulsion at short distances, with a parameter $\\eta$ describing the relative strength of the attractive versus the repulsive part. If $\\eta$ exceeds a characteristic value $\\eta_c$, the thermodynamic limit is lost, as the system becomes unstable against collapse. We investigate the phase diagram of the model for $\\eta \\lesssim \\eta_c$, finding -- in addition to a liquid-vapor transition -- a first-order transition between two liquid phases. Upon cooling, the high-density liquid turns superfluid, possibly above the vapor-liquid-liquid triple temperature. As $\\eta$ approaches $\\eta_c$, the stability region of the high-density liquid is shifted to increasingly higher densities, a behavior at variance with distinguishable quantum or classical particles. Finally, for $\\eta$ larger than $\\eta_c$ our simulations yield evidence of collapse of the low-temperature fluid for any density; the collapsed system forms a circular cluster whose radius is insensitive to the number of particles.","sentences":["Discovering novel emergent behavior in quantum many-body systems is a main objective of contemporary research.","In this paper, we explore the effects on phases and phase transitions of the proximity to a Ruelle instability.","To accomplish this, we study by quantum Monte Carlo simulations a two-dimensional system of finite-ranged attractive potential with soft-core repulsion at short distances, with a parameter $\\eta$ describing the relative strength of the attractive versus the repulsive part.","If $\\eta$ exceeds a characteristic value $\\eta_c$, the thermodynamic limit is lost, as the system becomes unstable against collapse.","We investigate the phase diagram of the model for $\\eta \\lesssim \\eta_c$, finding -- in addition to a liquid-vapor transition -- a first-order transition between two liquid phases.","Upon cooling, the high-density liquid turns superfluid, possibly above the vapor-liquid-liquid triple temperature.","As $\\eta$ approaches $\\eta_c$, the stability region of the high-density liquid is shifted to increasingly higher densities, a behavior at variance with distinguishable quantum or classical particles.","Finally, for $\\eta$ larger than $\\eta_c$ our simulations yield evidence of collapse of the low-temperature fluid for any density; the collapsed system forms a circular cluster whose radius is insensitive to the number of particles."],"url":"http://arxiv.org/abs/2402.16125v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-25 15:24:10","title":"Locomotion of Active Sheets Driven by Curvature Modulation","abstract":"The locomotion of flexible membrane-like organisms on top of curved surfaces appears in different contexts and scales. Still, such dynamics have not yet been quantitatively modeled and no realization of such motion in manmade systems has been achieved. We present an experimental and theoretical study of active gel ribbons surfing on a curved fluid-fluid interface via periodic modulation of their reference curvature. We derive a theoretical model, in which forces and torques emerge from curvature mismatch between the ribbon and the substrate. Analytic and numerical solutions of the equations of motion successfully predict the experimentally measured velocity profiles. We conclude by highlighting the relevance of this new, curvature-driven, mode of locomotion for a broad range of mechanical, as well as biological systems.","sentences":["The locomotion of flexible membrane-like organisms on top of curved surfaces appears in different contexts and scales.","Still, such dynamics have not yet been quantitatively modeled and no realization of such motion in manmade systems has been achieved.","We present an experimental and theoretical study of active gel ribbons surfing on a curved fluid-fluid interface via periodic modulation of their reference curvature.","We derive a theoretical model, in which forces and torques emerge from curvature mismatch between the ribbon and the substrate.","Analytic and numerical solutions of the equations of motion successfully predict the experimentally measured velocity profiles.","We conclude by highlighting the relevance of this new, curvature-driven, mode of locomotion for a broad range of mechanical, as well as biological systems."],"url":"http://arxiv.org/abs/2402.16112v1","category":"cond-mat.soft"}
{"created":"2024-02-25 15:20:01","title":"Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability","abstract":"Multimodal recommender systems amalgamate multimodal information (e.g., textual descriptions, images) into a collaborative filtering framework to provide more accurate recommendations. While the incorporation of multimodal information could enhance the interpretability of these systems, current multimodal models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret. To address this, we propose a Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both model and recommendation interpretability. DGVAE initially projects multimodal information into textual contents, such as converting images to text, by harnessing state-of-the-art multimodal pre-training technologies. It then constructs a frozen item-item graph and encodes the contents and interactions into two sets of disentangled representations utilizing a simplified residual graph convolutional network. DGVAE further regularizes these disentangled representations through mutual information maximization, aligning the representations derived from the interactions between users and items with those learned from textual content. This alignment facilitates the interpretation of user binary interactions via text. Our empirical analysis conducted on three real-world datasets demonstrates that DGVAE significantly surpasses the performance of state-of-the-art baselines by a margin of 10.02%. We also furnish a case study from a real-world dataset to illustrate the interpretability of DGVAE. Code is available at: \\url{https://github.com/enoche/DGVAE}.","sentences":["Multimodal recommender systems amalgamate multimodal information (e.g., textual descriptions, images) into a collaborative filtering framework to provide more accurate recommendations.","While the incorporation of multimodal information could enhance the interpretability of these systems, current multimodal models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret.","To address this, we propose a Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both model and recommendation interpretability.","DGVAE initially projects multimodal information into textual contents, such as converting images to text, by harnessing state-of-the-art multimodal pre-training technologies.","It then constructs a frozen item-item graph and encodes the contents and interactions into two sets of disentangled representations utilizing a simplified residual graph convolutional network.","DGVAE further regularizes these disentangled representations through mutual information maximization, aligning the representations derived from the interactions between users and items with those learned from textual content.","This alignment facilitates the interpretation of user binary interactions via text.","Our empirical analysis conducted on three real-world datasets demonstrates that DGVAE significantly surpasses the performance of state-of-the-art baselines by a margin of 10.02%.","We also furnish a case study from a real-world dataset to illustrate the interpretability of DGVAE.","Code is available at: \\url{https://github.com/enoche/DGVAE}."],"url":"http://arxiv.org/abs/2402.16110v1","category":"cs.IR"}
{"created":"2024-02-25 15:07:11","title":"Displaced Drude peak from $\u03c0$-ton vertex corrections","abstract":"Correlated electron systems often show strong bosonic fluctuations, e.g., of antiferromagnetic nature, around a large wave vector such as $\\mathbf{q}=(\\pi,\\pi\\ldots)$. These fluctuations can give rise to vertex corrections to the optical conductivity through the (transversal) particle-hole channel, coined $\\pi$-ton contributions. Previous numerical results differed qualitatively on how such vertex corrections alter the optical conductivity. Here, we clarify that $\\pi$-ton vertex corrections lead to a displaced Drude peak for correlated metals. The proximity and enhancement of the effect when approaching a phase transition of, e.g., antiferromagnetic nature can be utilized for discriminating $\\pi$-tons in experiments from other physics leading to a displaced Drude peak.","sentences":["Correlated electron systems often show strong bosonic fluctuations, e.g., of antiferromagnetic nature, around a large wave vector such as $\\mathbf{q}=(\\pi,\\pi\\ldots)$. These fluctuations can give rise to vertex corrections to the optical conductivity through the (transversal) particle-hole channel, coined $\\pi$-ton contributions.","Previous numerical results differed qualitatively on how such vertex corrections alter the optical conductivity.","Here, we clarify that $\\pi$-ton vertex corrections lead to a displaced Drude peak for correlated metals.","The proximity and enhancement of the effect when approaching a phase transition of, e.g., antiferromagnetic nature can be utilized for discriminating $\\pi$-tons in experiments from other physics leading to a displaced Drude peak."],"url":"http://arxiv.org/abs/2402.16104v1","category":"cond-mat.str-el"}
{"created":"2024-02-25 15:00:13","title":"Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?","abstract":"With the rise of increasingly powerful and user-facing NLP systems, there is growing interest in assessing whether they have a good representation of uncertainty by evaluating the quality of their predictive distribution over outcomes. We identify two main perspectives that drive starkly different evaluation protocols. The first treats predictive probability as an indication of model confidence; the second as an indication of human label variation. We discuss their merits and limitations, and take the position that both are crucial for trustworthy and fair NLP systems, but that exploiting a single predictive distribution is limiting. We recommend tools and highlight exciting directions towards models with disentangled representations of uncertainty about predictions and uncertainty about human labels.","sentences":["With the rise of increasingly powerful and user-facing NLP systems, there is growing interest in assessing whether they have a good representation of uncertainty by evaluating the quality of their predictive distribution over outcomes.","We identify two main perspectives that drive starkly different evaluation protocols.","The first treats predictive probability as an indication of model confidence; the second as an indication of human label variation.","We discuss their merits and limitations, and take the position that both are crucial for trustworthy and fair NLP systems, but that exploiting a single predictive distribution is limiting.","We recommend tools and highlight exciting directions towards models with disentangled representations of uncertainty about predictions and uncertainty about human labels."],"url":"http://arxiv.org/abs/2402.16102v1","category":"cs.CL"}
{"created":"2024-02-25 14:54:50","title":"Inversion-symmetric Electron Gases as New Platforms for Topological Planar Josephson Junctions","abstract":"Intrinsic Rashba spin-orbital coupling (SOC) can exist in centrosymmetric materials with local inversion symmetry breaking. Here we show that such a SOC can induce topological superconductivity together with an in-plane Zeeman field in planar Josephson junctions formed by the centrosymmetric materials. A single Majorana mode can be created at each end of the junction. We demonstrate this result in a model based on iron-based superconductors. We derive the necessary Fermi surface condition for the topological planar junction and calculate the topological phase diagram with respect to the in-plane Zeeman field and the phase difference between the two superconductors. We provide experimental characteristics for the topological superconductivity, including the differential conductance and the Fano factor tomography which can be measured in the scanning tunneling spectroscopy. Our study reveals that the centrosymmetric systems with local-inversion-symmetry breaking can serve as new platforms for the topological planar Josephson junctions, and help to find more experimentally feasible materials for the topological superconductors.","sentences":["Intrinsic Rashba spin-orbital coupling (SOC) can exist in centrosymmetric materials with local inversion symmetry breaking.","Here we show that such a SOC can induce topological superconductivity together with an in-plane Zeeman field in planar Josephson junctions formed by the centrosymmetric materials.","A single Majorana mode can be created at each end of the junction.","We demonstrate this result in a model based on iron-based superconductors.","We derive the necessary Fermi surface condition for the topological planar junction and calculate the topological phase diagram with respect to the in-plane Zeeman field and the phase difference between the two superconductors.","We provide experimental characteristics for the topological superconductivity, including the differential conductance and the Fano factor tomography which can be measured in the scanning tunneling spectroscopy.","Our study reveals that the centrosymmetric systems with local-inversion-symmetry breaking can serve as new platforms for the topological planar Josephson junctions, and help to find more experimentally feasible materials for the topological superconductors."],"url":"http://arxiv.org/abs/2402.16099v1","category":"cond-mat.supr-con"}
{"created":"2024-02-25 14:22:51","title":"Memory loss is contagious in open quantum systems","abstract":"Memoryless (Markovian) system-bath interactions are of fundamental interest in physics. While typically the absence of memory originates from the characteristics of the bath, here we demonstrate that it can result from the system becoming lossy due to the Markovian interaction with a second bath. This suggests that the property of Markovianity is ``contagious'', i.e., it can be transferred from one bath to another through the system with which they both interact. We introduce a Bloch-Redfield-inspired approach that assumes a Markovian interaction due to the system losses and significantly improves the description of the bath-driven population transfer in a lossy system coupled to a non-Markovian bath. Furthermore, it indicates that such interactions are well-described by an effective spectral density that takes into account the broadening of the system's energy levels due to dissipation. Our findings reduce the computational demand in the theoretical description of complex system-bath setups and pave the way for further studies of counterintuitive Markovian interactions.","sentences":["Memoryless (Markovian) system-bath interactions are of fundamental interest in physics.","While typically the absence of memory originates from the characteristics of the bath, here we demonstrate that it can result from the system becoming lossy due to the Markovian interaction with a second bath.","This suggests that the property of Markovianity is ``contagious'', i.e., it can be transferred from one bath to another through the system with which they both interact.","We introduce a Bloch-Redfield-inspired approach that assumes a Markovian interaction due to the system losses and significantly improves the description of the bath-driven population transfer in a lossy system coupled to a non-Markovian bath.","Furthermore, it indicates that such interactions are well-described by an effective spectral density that takes into account the broadening of the system's energy levels due to dissipation.","Our findings reduce the computational demand in the theoretical description of complex system-bath setups and pave the way for further studies of counterintuitive Markovian interactions."],"url":"http://arxiv.org/abs/2402.16096v1","category":"quant-ph"}
{"created":"2024-02-25 13:29:46","title":"Giant Strain Response of Charge Modulation and Singularity in a Kagome Superconductor","abstract":"Tunable quantum materials hold great potential for applications. Of special interest are materials in which small lattice strain induces giant electronic responses. The kagome compounds AV3Sb5 (A = K, Rb, Cs) provide a testbed for such singular electronic states. In this study, through angle-resolved photoemission spectroscopy, we provide comprehensive spectroscopic measurements of the giant responses induced by compressive and tensile strains on the charge-density-wave (CDW) order parameter and high-order van Hove singularity (HO-VHS) in CsV3Sb5. We observe a tripling of the CDW gap magnitudes with ~1% strain, accompanied by the changes of both energy and mass of the saddle-point fermions. Our results reveal an anticorrelation between the unconventional CDW order parameter and the mass of a HO-VHS, and highlight the role of the latter in the superconducting pairing. The giant electronic responses uncover a rich strain tunability of the versatile kagome system in studying quantum interplays under lattice perturbations.","sentences":["Tunable quantum materials hold great potential for applications.","Of special interest are materials in which small lattice strain induces giant electronic responses.","The kagome compounds AV3Sb5","(A = K, Rb, Cs) provide a testbed for such singular electronic states.","In this study, through angle-resolved photoemission spectroscopy, we provide comprehensive spectroscopic measurements of the giant responses induced by compressive and tensile strains on the charge-density-wave (CDW) order parameter and high-order van Hove singularity (HO-VHS) in CsV3Sb5.","We observe a tripling of the CDW gap magnitudes with ~1% strain, accompanied by the changes of both energy and mass of the saddle-point fermions.","Our results reveal an anticorrelation between the unconventional CDW order parameter and the mass of a HO-VHS, and highlight the role of the latter in the superconducting pairing.","The giant electronic responses uncover a rich strain tunability of the versatile kagome system in studying quantum interplays under lattice perturbations."],"url":"http://arxiv.org/abs/2402.16089v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-25 13:19:21","title":"From Concept to Implementation: Streamlining Sensor and Actuator Selection for Collaborative Design and Engineering of Interactive Systems","abstract":"Selecting appropriate sensors and actuators is a pivotal aspect of design and engineering, particularly in projects involving interactive systems. This article introduces the Design Thinking Based Iterative Sensor and Actuator Selection Flow, a structured decision-making approach aimed at streamlining this essential, yet often complex task. Created to accommodate individuals with diverse levels of technical expertise, our approach is uniquely suited for interdisciplinary teams of designers and engineers. Through the application of the flow to four real-world case studies, we highlight its broad applicability and demonstrate its efficacy in expediting project timelines and enhancing resource utilization. Our work lays a foundation for a more streamlined and user-centered process in selecting sensors and actuators, significantly benefiting the practice of interactive system design. This contribution serves as a seminal foundation for future research, offering significant contributions to both academic inquiry and practical applications across various industries. While the focus of the flow is on streamlining the selection process rather than on in-depth technical considerations, which are beyond the scope of this study, it provides a comprehensive guide for efficient and informed decision-making in the realm of interactive system design.","sentences":["Selecting appropriate sensors and actuators is a pivotal aspect of design and engineering, particularly in projects involving interactive systems.","This article introduces the Design Thinking Based Iterative Sensor and Actuator Selection Flow, a structured decision-making approach aimed at streamlining this essential, yet often complex task.","Created to accommodate individuals with diverse levels of technical expertise, our approach is uniquely suited for interdisciplinary teams of designers and engineers.","Through the application of the flow to four real-world case studies, we highlight its broad applicability and demonstrate its efficacy in expediting project timelines and enhancing resource utilization.","Our work lays a foundation for a more streamlined and user-centered process in selecting sensors and actuators, significantly benefiting the practice of interactive system design.","This contribution serves as a seminal foundation for future research, offering significant contributions to both academic inquiry and practical applications across various industries.","While the focus of the flow is on streamlining the selection process rather than on in-depth technical considerations, which are beyond the scope of this study, it provides a comprehensive guide for efficient and informed decision-making in the realm of interactive system design."],"url":"http://arxiv.org/abs/2402.16084v1","category":"cs.HC"}
{"created":"2024-02-25 13:06:40","title":"Homotopy classification of knotted defects in ordered media","abstract":"We give a homotopy classification of the global defects in ordered media, and explain it via the example of biaxial nematic liquid crystals, i.e., systems where the order parameter space is the quotient of the $3$-sphere $S^3$ by the quaternion group $Q$. As our mathematical model we consider continuous maps from complements of spatial graphs to the space $S^3/Q$ modulo a certain equivalence relation, and find that the equivalence classes are enumerated by the six subgroups of $Q$. Through monodromy around meridional loops, the edges of our spatial graphs are marked by conjugacy classes of $Q$; once we pass to planar diagrams, these labels can be refined to elements of $Q$ associated to each arc. The same classification scheme applies not only in the case of $Q$ but also to arbitrary groups.","sentences":["We give a homotopy classification of the global defects in ordered media, and explain it via the example of biaxial nematic liquid crystals, i.e., systems where the order parameter space is the quotient of the $3$-sphere $S^3$ by the quaternion group $Q$. As our mathematical model we consider continuous maps from complements of spatial graphs to the space $S^3/Q$ modulo a certain equivalence relation, and find that the equivalence classes are enumerated by the six subgroups of $Q$. Through monodromy around meridional loops, the edges of our spatial graphs are marked by conjugacy classes of $Q$; once we pass to planar diagrams, these labels can be refined to elements of $Q$ associated to each arc.","The same classification scheme applies not only in the case of $Q$ but also to arbitrary groups."],"url":"http://arxiv.org/abs/2402.16079v1","category":"cond-mat.soft"}
{"created":"2024-02-25 12:21:25","title":"Quasi-intermediate value theorem and outflanking arc theorem for plane maps","abstract":"For a disk $D$ in the plane $\\mathbb R^2$ and a plane map $f$, we give several conditions on the restriction of $f$ to the boundary $\\partial D$ of $D$ which imply the existence of a fixed point of $f$ in some specified domain in $D$. These conditions are similar to those appeared in the intermediate value theorem for maps on the real line. As an application of the main results, we establish a fixed point theorem for plane maps having an outflanking arc, which extends the famous theorem due to Brouwer: if $f$ is an orientation-preserving homeomorphism on the plane and has a periodic point, then it has a fixed point.","sentences":["For a disk $D$ in the plane $\\mathbb R^2$ and a plane map $f$, we give several conditions on the restriction of $f$ to the boundary $\\partial D$ of $D$ which imply the existence of a fixed point of $f$ in some specified domain in $D$. These conditions are similar to those appeared in the intermediate value theorem for maps on the real line.","As an application of the main results, we establish a fixed point theorem for plane maps having an outflanking arc, which extends the famous theorem due to Brouwer: if $f$ is an orientation-preserving homeomorphism on the plane and has a periodic point, then it has a fixed point."],"url":"http://arxiv.org/abs/2402.16076v1","category":"math.DS"}
{"created":"2024-02-25 11:43:02","title":"High-order topological pumping on a superconducting quantum processor","abstract":"High-order topological phases of matter refer to the systems of $n$-dimensional bulk with the topology of $m$-th order, exhibiting $(n-m)$-dimensional boundary modes and can be characterized by topological pumping. Here, we experimentally demonstrate two types of second-order topological pumps, forming four 0-dimensional corner localized states on a 4$\\times$4 square lattice array of 16 superconducting qubits. The initial ground state of the system for half-filling, as a product of four identical entangled 4-qubit states, is prepared using an adiabatic scheme. During the pumping procedure, we adiabatically modulate the superlattice Bose-Hubbard Hamiltonian by precisely controlling both the hopping strengths and on-site potentials. At the half pumping period, the system evolves to a corner-localized state in a quadrupole configuration. The robustness of the second-order topological pump is also investigated by introducing different on-site disorder. Our work studies the topological properties of high-order topological phases from the dynamical transport picture using superconducting qubits, which would inspire further research on high-order topological phases.","sentences":["High-order topological phases of matter refer to the systems of $n$-dimensional bulk with the topology of $m$-th order, exhibiting $(n-m)$-dimensional boundary modes and can be characterized by topological pumping.","Here, we experimentally demonstrate two types of second-order topological pumps, forming four 0-dimensional corner localized states on a 4$\\times$4 square lattice array of 16 superconducting qubits.","The initial ground state of the system for half-filling, as a product of four identical entangled 4-qubit states, is prepared using an adiabatic scheme.","During the pumping procedure, we adiabatically modulate the superlattice Bose-Hubbard Hamiltonian by precisely controlling both the hopping strengths and on-site potentials.","At the half pumping period, the system evolves to a corner-localized state in a quadrupole configuration.","The robustness of the second-order topological pump is also investigated by introducing different on-site disorder.","Our work studies the topological properties of high-order topological phases from the dynamical transport picture using superconducting qubits, which would inspire further research on high-order topological phases."],"url":"http://arxiv.org/abs/2402.16070v1","category":"quant-ph"}
{"created":"2024-02-25 11:18:11","title":"Sharp pointwise estimate of $\u03b1-$harmonic functions","abstract":"Let $\\alpha>-1$ and assume that $f$ is $\\alpha-$harmonic mapping defined in the unit disk that belongs to the Hardy class $h^p$ with $p\\ge 1$. We obtain some sharp estimates of the type $|f(z)|\\le g(|r|) \\|f^\\ast\\|_p$ and $|Df(z)|\\le h(|r|)\\|f^\\ast\\|_p$. We also prove a Schwarz type lemma for the class of $\\alpha-$harmonic mappings of the unit disk onto itself fixing the origin.","sentences":["Let $\\alpha>-1$ and assume that $f$ is $\\alpha-$harmonic mapping defined in the unit disk that belongs to the Hardy class $h^p$ with $p\\ge 1$.","We obtain some sharp estimates of the type $|f(z)|\\le g(|r|) \\|f^\\ast\\|_p$ and $|Df(z)|\\le h(|r|)\\|f^\\ast\\|_p$.","We also prove a Schwarz type lemma for the class of $\\alpha-$harmonic mappings of the unit disk onto itself fixing the origin."],"url":"http://arxiv.org/abs/2402.16062v1","category":"math.CV"}
{"created":"2024-02-25 11:13:31","title":"Effect of Magnetic Anisotropy Constants on Skyrmion Formation in Co/Pt","abstract":"Skyrmions, which are topologically stable magnetic structures, have manifested promising features to be used as an information carrier in new-age, non-volatile data storage devices. In this article, we show how the creation and stability of skyrmion can be manipulated by external stimuli (here, nano-second current pulse). Co/Pt square nano-structure with Co free layer thickness in the range 1 nm to 5 nm and first and second-order anisotropy constants are taken to study the controlled creation of skyrmions. The magnetization dynamics controlled by the current-induced spin transfer torque help to nucleate skyrmions by transformation from perpendicularly magnetized ground state to a stable state of isolated skyrmions via complex transformation of Neel wall following its image inversion. Compared with the first-order anisotropy alone, how the higher-order anisotropy constants (up to second order) impact the relaxed state of a system has been discussed.","sentences":["Skyrmions, which are topologically stable magnetic structures, have manifested promising features to be used as an information carrier in new-age, non-volatile data storage devices.","In this article, we show how the creation and stability of skyrmion can be manipulated by external stimuli (here, nano-second current pulse).","Co/Pt square nano-structure with Co free layer thickness in the range 1 nm to 5 nm and first and second-order anisotropy constants are taken to study the controlled creation of skyrmions.","The magnetization dynamics controlled by the current-induced spin transfer torque help to nucleate skyrmions by transformation from perpendicularly magnetized ground state to a stable state of isolated skyrmions via complex transformation of Neel wall following its image inversion.","Compared with the first-order anisotropy alone, how the higher-order anisotropy constants (up to second order) impact the relaxed state of a system has been discussed."],"url":"http://arxiv.org/abs/2402.16060v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-25 10:42:51","title":"Energetics of Fano coherence generation","abstract":"In a multi-level quantum system Fano coherences stand for the formation of quantum coherences due to the interaction with the continuum of modes characterizing an incoherent process. When the incoherent source vanishes, Fano coherences tend to disappear. In this paper we propose a V-type three-level quantum system on which we certify the presence of genuinely quantum traits underlying the generation of Fano coherences. We do this by determining work conditions that allows for the loss of positivity of the Kirkwood-Dirac quasiprobability distribution of the stochastic energy variations within the discrete system. We also show the existence of nonequilibrium regimes where the generation of Fano coherences leads to a non-negligible amount of extractable work, however provided the initial state of the discrete system is in a superposition of the energy eigenbasis. We conclude the paper by studying the thermodynamic efficiency of the whole process.","sentences":["In a multi-level quantum system Fano coherences stand for the formation of quantum coherences due to the interaction with the continuum of modes characterizing an incoherent process.","When the incoherent source vanishes, Fano coherences tend to disappear.","In this paper we propose a V-type three-level quantum system on which we certify the presence of genuinely quantum traits underlying the generation of Fano coherences.","We do this by determining work conditions that allows for the loss of positivity of the Kirkwood-Dirac quasiprobability distribution of the stochastic energy variations within the discrete system.","We also show the existence of nonequilibrium regimes where the generation of Fano coherences leads to a non-negligible amount of extractable work, however provided the initial state of the discrete system is in a superposition of the energy eigenbasis.","We conclude the paper by studying the thermodynamic efficiency of the whole process."],"url":"http://arxiv.org/abs/2402.16056v1","category":"quant-ph"}
{"created":"2024-02-25 10:27:46","title":"LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding","abstract":"Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm.","sentences":["Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time.","To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP).","This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements.","By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment.","Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm."],"url":"http://arxiv.org/abs/2402.16050v1","category":"cs.CV"}
{"created":"2024-02-25 10:00:21","title":"Decoding Driver Takeover Behaviour in Conditional Automation with Immersive Virtual Reality","abstract":"The safe transition from conditional automation to manual driving control is significantly intertwined with the vehicle's lateral and longitudinal dynamics. The transition may occur as a result of a system-initiated mandatory takeover (MTOR) or as a driver-initiated discretionary takeover (DTOR). In either condition, the takeover process entails differing cognitive demands and may affect the driving behaviour differently. This study analyzes driving stability and perceived mental workload in 304 takeover attempts recorded from 104 participants within virtual and immersive reality environments. Adopting an exploratory approach, this dynamic simulator study employs a mixed factorial design. Utilizing a deep neural network-based survival analysis with SHAP interpretability, the study investigated the influence of covariates on perception-reaction time (PRT), distinguishing between safe and unsafe control transition and offering insights into the temporal dynamics of these shifts. The distributions of key parameters in experimental groups were analyzed and factors influencing the perceived mental workload were estimated using multivariate linear regression. The findings indicate a notable decrease in the risk of unsafe takeovers (described by a longer PRT) when drivers have prior control-transition experience and familiarity with Automated Vehicles (AVs). However, driver's prior familiarity and experience with AVs only decreased the perceived mental workload associated with DTOR, with an insignificant impact on the cognitive demand of MTOR. Furthermore, multitasking during automated driving significantly elevated the cognitive demand linked to DTOR and led to longer PRT in MTOR situations.","sentences":["The safe transition from conditional automation to manual driving control is significantly intertwined with the vehicle's lateral and longitudinal dynamics.","The transition may occur as a result of a system-initiated mandatory takeover (MTOR) or as a driver-initiated discretionary takeover (DTOR).","In either condition, the takeover process entails differing cognitive demands and may affect the driving behaviour differently.","This study analyzes driving stability and perceived mental workload in 304 takeover attempts recorded from 104 participants within virtual and immersive reality environments.","Adopting an exploratory approach, this dynamic simulator study employs a mixed factorial design.","Utilizing a deep neural network-based survival analysis with SHAP interpretability, the study investigated the influence of covariates on perception-reaction time (PRT), distinguishing between safe and unsafe control transition and offering insights into the temporal dynamics of these shifts.","The distributions of key parameters in experimental groups were analyzed and factors influencing the perceived mental workload were estimated using multivariate linear regression.","The findings indicate a notable decrease in the risk of unsafe takeovers (described by a longer PRT) when drivers have prior control-transition experience and familiarity with Automated Vehicles (AVs).","However, driver's prior familiarity and experience with AVs only decreased the perceived mental workload associated with DTOR, with an insignificant impact on the cognitive demand of MTOR.","Furthermore, multitasking during automated driving significantly elevated the cognitive demand linked to DTOR and led to longer PRT in MTOR situations."],"url":"http://arxiv.org/abs/2402.16046v1","category":"cs.HC"}
{"created":"2024-02-25 09:56:56","title":"Continuous-variable quantum passive optical network","abstract":"Building scalable and secure quantum networks with many users has a high application potential but also holds many practical challenges. A significant stride in this pursuit involves extending quantum key distribution, an information-theoretically secure method for establishing cryptographic keys between two distant users, from a point-to-point protocol implemented on direct optical connections to a quantum access network. Yet, realizations of quantum access networks have, so far, relied on probabilistic or time-sharing strategies. Here, we show theoretically and experimentally that a solution without these constraints can come from the exclusive features of continuous-variable systems. Based on coherent states, we propose continuous-variable quantum passive-optical-network (CV-QPON) protocols, enabling deterministic and simultaneous secret key generation among all network users. We achieve this by leveraging the inherent wave-like property of coherent states split at a beam splitter and electric-field quadrature measurements. We show two protocols with different trust levels assigned to the network users and experimentally demonstrate key generation in a quantum access network with 8 users, each with an 11 km span of access link. Depending on the trust assumptions about users, we reach 1.5 Mbits/s and 2.1 Mbits/s of total network key generation. Demonstrating the potential to expand the network's capacity to accommodate tens of users at a high rate, our CV-QPON protocols offer a pathway toward establishing low-cost, high-rate, and scalable quantum access networks using standard telecom technologies and directly exploiting the existing access network infrastructure.","sentences":["Building scalable and secure quantum networks with many users has a high application potential but also holds many practical challenges.","A significant stride in this pursuit involves extending quantum key distribution, an information-theoretically secure method for establishing cryptographic keys between two distant users, from a point-to-point protocol implemented on direct optical connections to a quantum access network.","Yet, realizations of quantum access networks have, so far, relied on probabilistic or time-sharing strategies.","Here, we show theoretically and experimentally that a solution without these constraints can come from the exclusive features of continuous-variable systems.","Based on coherent states, we propose continuous-variable quantum passive-optical-network (CV-QPON) protocols, enabling deterministic and simultaneous secret key generation among all network users.","We achieve this by leveraging the inherent wave-like property of coherent states split at a beam splitter and electric-field quadrature measurements.","We show two protocols with different trust levels assigned to the network users and experimentally demonstrate key generation in a quantum access network with 8 users, each with an 11 km span of access link.","Depending on the trust assumptions about users, we reach 1.5 Mbits/s and 2.1 Mbits/s of total network key generation.","Demonstrating the potential to expand the network's capacity to accommodate tens of users at a high rate, our CV-QPON protocols offer a pathway toward establishing low-cost, high-rate, and scalable quantum access networks using standard telecom technologies and directly exploiting the existing access network infrastructure."],"url":"http://arxiv.org/abs/2402.16044v1","category":"quant-ph"}
{"created":"2024-02-25 09:52:02","title":"LuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices","abstract":"IoT devices are currently facing continuous malicious attacks due to their widespread use. Among these IoT devices, web vulnerabilities are also widely exploited because of their inherent characteristics, such as improper permission controls and insecure interfaces. Recently, the embedded system web interface framework has become highly diverse, and specific vulnerabilities can arise if developers forget to detect user input parameters or if the detection process is not strict enough. Therefore, discovering vulnerabilities in the web interfaces of IoT devices accurately and comprehensively through an automated method is a major challenge. This paper aims to work out the challenge. We have developed an automated vulnerability detection system called LuaTaint for the typical web interface framework, LuCI. The system employs static taint analysis to address web security issues on mobile terminal platforms to ensure detection coverage. It integrates rules pertaining to page handler control logic within the taint detection process to improve its extensibility. We also implemented a post-processing step with the assistance of large language models to enhance accuracy and reduce the need for manual analysis. We have created a prototype of LuaTaint and tested it on 92 IoT firmwares from 8 well-known vendors. LuaTaint has discovered 68 unknown vulnerabilities.","sentences":["IoT devices are currently facing continuous malicious attacks due to their widespread use.","Among these IoT devices, web vulnerabilities are also widely exploited because of their inherent characteristics, such as improper permission controls and insecure interfaces.","Recently, the embedded system web interface framework has become highly diverse, and specific vulnerabilities can arise if developers forget to detect user input parameters or if the detection process is not strict enough.","Therefore, discovering vulnerabilities in the web interfaces of IoT devices accurately and comprehensively through an automated method is a major challenge.","This paper aims to work out the challenge.","We have developed an automated vulnerability detection system called LuaTaint for the typical web interface framework, LuCI.","The system employs static taint analysis to address web security issues on mobile terminal platforms to ensure detection coverage.","It integrates rules pertaining to page handler control logic within the taint detection process to improve its extensibility.","We also implemented a post-processing step with the assistance of large language models to enhance accuracy and reduce the need for manual analysis.","We have created a prototype of LuaTaint and tested it on 92 IoT firmwares from 8 well-known vendors.","LuaTaint has discovered 68 unknown vulnerabilities."],"url":"http://arxiv.org/abs/2402.16043v1","category":"cs.CR"}
{"created":"2024-02-25 09:48:04","title":"Enhancement of Entanglement via Josephson Parametric Amplifier in a Dual Cavity-Magnon System","abstract":"In the two microwave (MW) cross-shaped cavity magnon system, we describe a method to produce multipartite entanglement and quantum steering. To achieve squeezed states of the magnons, a Josephson parametric amplifier (JPA) creates a squeezed vacuum field that drives the two cavities. We theoretically demonstrate that the cavity-cavity entanglement can be generated at the resonance point, however, increasing the cavity and magnon decay rates generate the cavity-magnon entanglement. By changing the squeezing parameter and increasing the decay rates, we can transfer the cavity-cavity entanglement to cavity-magnon entanglement. Furthermore, the cavity-cavity entanglement (survive up to 2.8K) not only found to be much stronger but also more robust as compared to cavity-magnon entanglement (survive up to 0.4K). More importantly, the genuine photon-magnon-photon tripartite entanglement could be achieved, which is robust against the thermal fluctuations and depends strongly on squeezing parameter. Furthermore, for current dual cavity-magnon system, two-way quantum steering is found when the optomagnonical couplings are equal. The current study offers a straightforward and practical method for achieving multipartite quantum correlations.","sentences":["In the two microwave (MW) cross-shaped cavity magnon system, we describe a method to produce multipartite entanglement and quantum steering.","To achieve squeezed states of the magnons, a Josephson parametric amplifier (JPA) creates a squeezed vacuum field that drives the two cavities.","We theoretically demonstrate that the cavity-cavity entanglement can be generated at the resonance point, however, increasing the cavity and magnon decay rates generate the cavity-magnon entanglement.","By changing the squeezing parameter and increasing the decay rates, we can transfer the cavity-cavity entanglement to cavity-magnon entanglement.","Furthermore, the cavity-cavity entanglement (survive up to 2.8K) not only found to be much stronger but also more robust as compared to cavity-magnon entanglement (survive up to 0.4K).","More importantly, the genuine photon-magnon-photon tripartite entanglement could be achieved, which is robust against the thermal fluctuations and depends strongly on squeezing parameter.","Furthermore, for current dual cavity-magnon system, two-way quantum steering is found when the optomagnonical couplings are equal.","The current study offers a straightforward and practical method for achieving multipartite quantum correlations."],"url":"http://arxiv.org/abs/2402.16042v1","category":"quant-ph"}
{"created":"2024-02-25 09:41:50","title":"EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings","abstract":"This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models showed that their scores on EHRNoteQA correlate more closely with their performance in addressing real-world medical questions evaluated by clinicians than their scores from other LLM benchmarks. This underscores the significance of EHRNoteQA in evaluating LLMs for medical applications and highlights its crucial role in facilitating the integration of LLMs into healthcare systems. The dataset will be made available to the public under PhysioNet credential access, promoting further research in this vital field.","sentences":["This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments.","Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes.","What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows:","Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats.","Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories.","Our comprehensive evaluation on various large language models showed that their scores on EHRNoteQA correlate more closely with their performance in addressing real-world medical questions evaluated by clinicians than their scores from other LLM benchmarks.","This underscores the significance of EHRNoteQA in evaluating LLMs for medical applications and highlights its crucial role in facilitating the integration of LLMs into healthcare systems.","The dataset will be made available to the public under PhysioNet credential access, promoting further research in this vital field."],"url":"http://arxiv.org/abs/2402.16040v1","category":"cs.CL"}
{"created":"2024-02-25 09:31:48","title":"Thermodynamically reversible quantum measurements and related work costs","abstract":"Considering a general microscopic model for quantum measurement comprising a measurement apparatus coupled to a thermal bath, we analyze the energetic resources necessary for the realisation of quantum measurements, including the process of switching on and off the coupling between the system and the apparatus, the transition to a statistical mixture, the classical readout, and the apparatus resetting. We show via general thermodynamic arguments that the minimal required work depends on the energy variation of the system being measured plus information-theoretic quantities characterizing the performance of the measurement -- efficiency and completeness. Additionally, providing an explicit protocol, we show that it is possible to perform thermodynamically reversible measurement, thus reaching the minimal work expenditure. Finally, for finite-time measurement protocols, we illustrate the increasing work cost induced by rising entropy production inherent of finite-time thermodynamic processes. This highlights an emerging trade-off between velocity of the measurement and work cost, on top of a trade-off between efficiency of the measurement and work cost.","sentences":["Considering a general microscopic model for quantum measurement comprising a measurement apparatus coupled to a thermal bath, we analyze the energetic resources necessary for the realisation of quantum measurements, including the process of switching on and off the coupling between the system and the apparatus, the transition to a statistical mixture, the classical readout, and the apparatus resetting.","We show via general thermodynamic arguments that the minimal required work depends on the energy variation of the system being measured plus information-theoretic quantities characterizing the performance of the measurement -- efficiency and completeness.","Additionally, providing an explicit protocol, we show that it is possible to perform thermodynamically reversible measurement, thus reaching the minimal work expenditure.","Finally, for finite-time measurement protocols, we illustrate the increasing work cost induced by rising entropy production inherent of finite-time thermodynamic processes.","This highlights an emerging trade-off between velocity of the measurement and work cost, on top of a trade-off between efficiency of the measurement and work cost."],"url":"http://arxiv.org/abs/2402.16037v1","category":"quant-ph"}
{"created":"2024-02-25 08:41:32","title":"GraphWiz: An Instruction-Following Language Model for Graph Problems","abstract":"Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.","sentences":["Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored.","To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths.","Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes.","To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context.","The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%.","Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data.","We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential.","Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving."],"url":"http://arxiv.org/abs/2402.16029v1","category":"cs.CL"}
{"created":"2024-02-25 08:26:12","title":"Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance Analysis and Optimization","abstract":"Massive interconnection has sparked people's envisioning for next-generation ultra-reliable and low-latency communications (xURLLC), prompting the design of customized next-generation advanced transceivers (NGAT). Rate-splitting multiple access (RSMA) has emerged as a pivotal technology for NGAT design, given its robustness to imperfect channel state information (CSI) and resilience to quality of service (QoS). Additionally, xURLLC urgently appeals to large-scale access techniques, thus massive multiple-input multiple-output (mMIMO) is anticipated to integrate with RSMA to enhance xURLLC. In this paper, we develop an innovative RSMA-assisted massive-MIMO xURLLC (RSMA-mMIMO-xURLLC) network architecture tailored to accommodate xURLLC's critical QoS constraints in finite blocklength (FBL) regimes. Leveraging uplink pilot training under imperfect CSI at the transmitter, we estimate channel gains and customize linear precoders for efficient downlink short-packet data transmission. Subsequently, we formulate a joint rate-splitting, beamforming, and transmit antenna selection optimization problem to maximize the total effective transmission rate (ETR). Addressing this multi-variable coupled non-convex problem, we decompose it into three corresponding subproblems and propose a low-complexity joint iterative algorithm for efficient optimization. Extensive simulations substantiate that compared with non-orthogonal multiple access (NOMA) and space division multiple access (SDMA), the developed architecture improves the total ETR by 15.3% and 41.91%, respectively, as well as accommodates larger-scale access.","sentences":["Massive interconnection has sparked people's envisioning for next-generation ultra-reliable and low-latency communications (xURLLC), prompting the design of customized next-generation advanced transceivers (NGAT).","Rate-splitting multiple access (RSMA) has emerged as a pivotal technology for NGAT design, given its robustness to imperfect channel state information (CSI) and resilience to quality of service (QoS).","Additionally, xURLLC urgently appeals to large-scale access techniques, thus massive multiple-input multiple-output (mMIMO) is anticipated to integrate with RSMA to enhance xURLLC.","In this paper, we develop an innovative RSMA-assisted massive-MIMO xURLLC (RSMA-mMIMO-xURLLC) network architecture tailored to accommodate xURLLC's critical QoS constraints in finite blocklength (FBL) regimes.","Leveraging uplink pilot training under imperfect CSI at the transmitter, we estimate channel gains and customize linear precoders for efficient downlink short-packet data transmission.","Subsequently, we formulate a joint rate-splitting, beamforming, and transmit antenna selection optimization problem to maximize the total effective transmission rate (ETR).","Addressing this multi-variable coupled non-convex problem, we decompose it into three corresponding subproblems and propose a low-complexity joint iterative algorithm for efficient optimization.","Extensive simulations substantiate that compared with non-orthogonal multiple access (NOMA) and space division multiple access (SDMA), the developed architecture improves the total ETR by 15.3% and 41.91%, respectively, as well as accommodates larger-scale access."],"url":"http://arxiv.org/abs/2402.16027v1","category":"cs.IT"}
{"created":"2024-02-25 08:12:19","title":"Learning with Semantics: Towards a Semantics-Aware Routing Anomaly Detection System","abstract":"BGP is the de facto inter-domain routing protocol to ensure global connectivity of the Internet. However, various reasons, such as deliberate attacks or misconfigurations, could cause BGP routing anomalies. Traditional methods for BGP routing anomaly detection require significant manual investigation of routes by network operators. Although machine learning has been applied to automate the process, prior arts typically impose significant training overhead (such as large-scale data labeling and feature crafting), and only produce uninterpretable results. To address these limitations, this paper presents a routing anomaly detection system centering around a novel network representation learning model named BEAM. The core design of BEAM is to accurately learn the unique properties (defined as \\emph{routing role}) of each Autonomous System (AS) in the Internet by incorporating BGP semantics. As a result, routing anomaly detection, given BEAM, is reduced to a matter of discovering unexpected routing role churns upon observing new route announcements. We implement a prototype of our routing anomaly detection system and extensively evaluate its performance. The experimental results, based on 18 real-world RouteViews datasets containing over 11 billion route announcement records, demonstrate that our system can detect all previously-confirmed routing anomalies, while only introducing at most five false alarms every 180 million route announcements. We also deploy our system at a large ISP to perform real-world detection for one month. During the course of deployment, our system detects 497 true anomalies in the wild with an average of only 1.65 false alarms per day.","sentences":["BGP is the de facto inter-domain routing protocol to ensure global connectivity of the Internet.","However, various reasons, such as deliberate attacks or misconfigurations, could cause BGP routing anomalies.","Traditional methods for BGP routing anomaly detection require significant manual investigation of routes by network operators.","Although machine learning has been applied to automate the process, prior arts typically impose significant training overhead (such as large-scale data labeling and feature crafting), and only produce uninterpretable results.","To address these limitations, this paper presents a routing anomaly detection system centering around a novel network representation learning model named BEAM.","The core design of BEAM is to accurately learn the unique properties (defined as \\emph{routing role}) of each Autonomous System (AS) in the Internet by incorporating BGP semantics.","As a result, routing anomaly detection, given BEAM, is reduced to a matter of discovering unexpected routing role churns upon observing new route announcements.","We implement a prototype of our routing anomaly detection system and extensively evaluate its performance.","The experimental results, based on 18 real-world RouteViews datasets containing over 11 billion route announcement records, demonstrate that our system can detect all previously-confirmed routing anomalies, while only introducing at most five false alarms every 180 million route announcements.","We also deploy our system at a large ISP to perform real-world detection for one month.","During the course of deployment, our system detects 497 true anomalies in the wild with an average of only 1.65 false alarms per day."],"url":"http://arxiv.org/abs/2402.16025v1","category":"cs.NI"}
{"created":"2024-02-25 08:07:22","title":"HiGPT: Heterogeneous Graph Language Model","abstract":"Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the \"pre-train\" and \"fine-tune\" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: \"Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we propose HiGPT, a general large graph model with Heterogeneous graph instruction-tuning paradigm. Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets. To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation. We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions. Through comprehensive evaluations, our proposed framework demonstrates exceptional performance in terms of generalization performance.","sentences":["Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges.","Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules.","However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets.","Most of these frameworks follow the \"pre-train\" and \"fine-tune\" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data.","This raises the question: \"Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?''","To tackle those challenges, we propose HiGPT, a general large graph model with Heterogeneous graph instruction-tuning paradigm.","Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets.","To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation.","We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens.","Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions.","Through comprehensive evaluations, our proposed framework demonstrates exceptional performance in terms of generalization performance."],"url":"http://arxiv.org/abs/2402.16024v1","category":"cs.CL"}
{"created":"2024-02-25 08:05:41","title":"In-plane Exciton Polaritons vs Plasmon Polaritons: Nonlocal corrections, confinement and loss","abstract":"Polaritons are quasi-particles describing the coupling between a photon and a material excitation, which can carry large momentum and confine electromagnetic fields to small dimensions, enabling strong light-matter interactions. In the visible (VIS) to near-infrared (NIR) spectral ranges, the intraband response of metals gives rise to surface-plasmon-polaritons (SPPs), which have practically governed polaritonic response and its utilization in nanophotonics. Recently, the concept of interband-based VIS/NIR in-plane exciton polaritons has been introduced in two-dimensional materials, such as transition-metal-dichalcogenides (TMDs), thus providing an excitonic alternative to plasmonic systems. Here, we compare the properties of such in-plane exciton polaritons supported by monolayer TMDs to the equivalent configuration of SPPs supported by thin metallic films, known as the short-range-SPPs (SRSPPs). Taking into account both excitonic and plasmonic nonlocal corrections, which play a major role in large momentum modes, we find that in-plane exciton polaritons provide confinement factors that are an order of magnitude larger than those of SRSPPs, and with six times lower propagation losses. In addition, we show that unlike SPPs, in-plane exciton polaritons are coupled to the TMD's valley degree of freedom, leading to directional propagation that depends on the exciton's valley. These properties make in-plane exciton polaritons promising candidates for VIS/NIR nanophotonics and strong light-matter interaction.","sentences":["Polaritons are quasi-particles describing the coupling between a photon and a material excitation, which can carry large momentum and confine electromagnetic fields to small dimensions, enabling strong light-matter interactions.","In the visible (VIS) to near-infrared (NIR) spectral ranges, the intraband response of metals gives rise to surface-plasmon-polaritons (SPPs), which have practically governed polaritonic response and its utilization in nanophotonics.","Recently, the concept of interband-based VIS/NIR in-plane exciton polaritons has been introduced in two-dimensional materials, such as transition-metal-dichalcogenides (TMDs), thus providing an excitonic alternative to plasmonic systems.","Here, we compare the properties of such in-plane exciton polaritons supported by monolayer TMDs to the equivalent configuration of SPPs supported by thin metallic films, known as the short-range-SPPs (SRSPPs).","Taking into account both excitonic and plasmonic nonlocal corrections, which play a major role in large momentum modes, we find that in-plane exciton polaritons provide confinement factors that are an order of magnitude larger than those of SRSPPs, and with six times lower propagation losses.","In addition, we show that unlike SPPs, in-plane exciton polaritons are coupled to the TMD's valley degree of freedom, leading to directional propagation that depends on the exciton's valley.","These properties make in-plane exciton polaritons promising candidates for VIS/NIR nanophotonics and strong light-matter interaction."],"url":"http://arxiv.org/abs/2402.16023v1","category":"physics.optics"}
{"created":"2024-02-25 07:41:08","title":"A Step-by-step Introduction to the Implementation of Automatic Differentiation","abstract":"Automatic differentiation is a key component in deep learning. This topic is well studied and excellent surveys such as Baydin et al. (2018) have been available to clearly describe the basic concepts. Further, sophisticated implementations of automatic differentiation are now an important part of popular deep learning frameworks. However, it is difficult, if not impossible, to directly teach students the implementation of existing systems due to the complexity. On the other hand, if the teaching stops at the basic concept, students fail to sense the realization of an implementation. For example, we often mention the computational graph in teaching automatic differentiation, but students wonder how to implement and use it. In this document, we partially fill the gap by giving a step by step introduction of implementing a simple automatic differentiation system. We streamline the mathematical concepts and the implementation. Further, we give the motivation behind each implementation detail, so the whole setting becomes very natural.","sentences":["Automatic differentiation is a key component in deep learning.","This topic is well studied and excellent surveys such as Baydin et al. (2018) have been available to clearly describe the basic concepts.","Further, sophisticated implementations of automatic differentiation are now an important part of popular deep learning frameworks.","However, it is difficult, if not impossible, to directly teach students the implementation of existing systems due to the complexity.","On the other hand, if the teaching stops at the basic concept, students fail to sense the realization of an implementation.","For example, we often mention the computational graph in teaching automatic differentiation, but students wonder how to implement and use it.","In this document, we partially fill the gap by giving a step by step introduction of implementing a simple automatic differentiation system.","We streamline the mathematical concepts and the implementation.","Further, we give the motivation behind each implementation detail, so the whole setting becomes very natural."],"url":"http://arxiv.org/abs/2402.16020v1","category":"cs.LG"}
{"created":"2024-02-25 07:38:33","title":"Microscopic study of deformation and orientation effects in heavy-ion reactions above Coulomb barrier using the Boltzmann-Uehling-Uhlenbeck model","abstract":"Background: The understanding of the impact of initial deformation and collision orientation on quasi-fission and fusion-fission reactions remains incomplete. Purpose: This article aims to explore how the orientation of deformed nuclei influences quasi-fission and fusion-fission around 1.2 VB, employing a micro dynamical method in systems with diverse shapes, namely 24Mg + 178Hf, 34S + 168Er, and 48Ti + 154Sm. Method: Utilizing the Boltzmann-Uehling-Uhlenbeck model, this study investigates quasi-fission and fusion fission reactions. The model elucidates micro-dynamic processes and microscopic observables through the definition of the window and event-by-event simulations. Results: The findings reveal that the orientation of deformed nuclei significantly influences the nucleus-nucleus interaction potential, thereby impacting the competition between quasi-fission and fusion-fission. Particularly, the orientation of the deformed target nucleus emerges as the primary factor affecting this competition. Notably, a higher proportion of fusion-fission events is observed when the target nucleus is in the belly orientation compared to the tip. The study also observes that the configuration of the dinuclear system contributes to fluctuations and dissipation. Collisions with different orientations result in distinct dinuclear system configurations, with belly-oriented collisions leading to larger fluctuations between events, while tip-oriented collisions exhibit smaller fluctuations. Conclusions: Considering diverse orientations of nuclei with distinct initial deformations, this study concludes that the orientation of the target nucleus is the key factor influencing quasi-fission and fusion-fission reactions around 1.2 VB.","sentences":["Background: The understanding of the impact of initial deformation and collision orientation on quasi-fission and fusion-fission reactions remains incomplete.","Purpose:","This article aims to explore how the orientation of deformed nuclei influences quasi-fission and fusion-fission around 1.2 VB, employing a micro dynamical method in systems with diverse shapes, namely 24Mg + 178Hf, 34S + 168Er, and 48Ti + 154Sm.","Method: Utilizing the Boltzmann-Uehling-Uhlenbeck model, this study investigates quasi-fission and fusion fission reactions.","The model elucidates micro-dynamic processes and microscopic observables through the definition of the window and event-by-event simulations.","Results:","The findings reveal that the orientation of deformed nuclei significantly influences the nucleus-nucleus interaction potential, thereby impacting the competition between quasi-fission and fusion-fission.","Particularly, the orientation of the deformed target nucleus emerges as the primary factor affecting this competition.","Notably, a higher proportion of fusion-fission events is observed when the target nucleus is in the belly orientation compared to the tip.","The study also observes that the configuration of the dinuclear system contributes to fluctuations and dissipation.","Collisions with different orientations result in distinct dinuclear system configurations, with belly-oriented collisions leading to larger fluctuations between events, while tip-oriented collisions exhibit smaller fluctuations.","Conclusions: Considering diverse orientations of nuclei with distinct initial deformations, this study concludes that the orientation of the target nucleus is the key factor influencing quasi-fission and fusion-fission reactions around 1.2 VB."],"url":"http://arxiv.org/abs/2402.16019v1","category":"nucl-th"}
{"created":"2024-02-25 07:20:49","title":"Network analysis of memristive device circuits: dynamics, stability and correlations","abstract":"Networks with memristive devices are a potential basis for the next generation of computing devices. They are also an important model system for basic science, from modeling nanoscale conductivity to providing insight into the information-processing of neurons. The resistance in a memristive device depends on the history of the applied bias and thus displays a type of memory. The interplay of this memory with the dynamic properties of the network can give rise to new behavior, offering many fascinating theoretical challenges. But methods to analyze general memristive circuits are not well described in the literature. In this paper we develop a general circuit analysis for networks that combine memristive devices alongside resistors, capacitors and inductors and under various types of control. We derive equations of motion for the memory parameters of these circuits and describe the conditions for which a network should display properties characteristic of a resonator system. For the case of a purely memresistive network, we derive Lyapunov functions, which can be used to study the stability of the network dynamics. Surprisingly, analysis of the Lyapunov functions show that these circuits do not always have a stable equilibrium in the case of nonlinear resistance and window functions. The Lyapunov function allows us to study circuit invariances, wherein different circuits give rise to similar equations of motion, which manifest through a gauge freedom and node permutations. Finally, we identify the relation between the graph Laplacian and the operators governing the dynamics of memristor networks operators, and we use these tools to study the correlations between distant memristive devices through the effective resistance.","sentences":["Networks with memristive devices are a potential basis for the next generation of computing devices.","They are also an important model system for basic science, from modeling nanoscale conductivity to providing insight into the information-processing of neurons.","The resistance in a memristive device depends on the history of the applied bias and thus displays a type of memory.","The interplay of this memory with the dynamic properties of the network can give rise to new behavior, offering many fascinating theoretical challenges.","But methods to analyze general memristive circuits are not well described in the literature.","In this paper we develop a general circuit analysis for networks that combine memristive devices alongside resistors, capacitors and inductors and under various types of control.","We derive equations of motion for the memory parameters of these circuits and describe the conditions for which a network should display properties characteristic of a resonator system.","For the case of a purely memresistive network, we derive Lyapunov functions, which can be used to study the stability of the network dynamics.","Surprisingly, analysis of the Lyapunov functions show that these circuits do not always have a stable equilibrium in the case of nonlinear resistance and window functions.","The Lyapunov function allows us to study circuit invariances, wherein different circuits give rise to similar equations of motion, which manifest through a gauge freedom and node permutations.","Finally, we identify the relation between the graph Laplacian and the operators governing the dynamics of memristor networks operators, and we use these tools to study the correlations between distant memristive devices through the effective resistance."],"url":"http://arxiv.org/abs/2402.16015v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-25 07:01:38","title":"Ultrafast and precise distance measurement via real-time chirped pulse interferometry","abstract":"Laser frequency combs, which are composed of a series of equally-spaced coherent frequency components, have triggered revolutionary progress for precision spectroscopy and optical metrology. Length/distance is of fundamental importance in both science and technology. In this work, we describe a ranging scheme based on chirped pulse interferometry. In contrast to the traditional spectral interferometry, the local oscillator is strongly chirped which is able to meet the measurement pulses at arbitrary distances, and therefore the dead zones can be removed. The distances can be precisely determined via two measurement steps based on time-of-flight method and synthetic wavelength interferometry, respectively. To overcome the speed limitation of the optical spectrum analyzer, the spectrograms are stretched and detected by a fast photodetector and oscilloscope, and consequently mapped into the time domain in real time. The experimental results indicate that the measurement uncertainty can be well within 2 $\\upmu$m, compared with the reference distance meter. The Allan deviation can reach 0.4 $\\upmu$m at averaging time of 4 ns, 25 nm at 1 $\\upmu$s, and can achieve 2 nm at 100 $\\upmu$s averaging time. We also measure a spinning disk with grooves of different depths to verify the measurement speed, and the results show that the grooves with about 150 m/s line speed can be clearly captured. Our method provides a unique combination of non-dead zones, ultrafast measurement speed, high precision and accuracy, large ambiguity range, and with only one single comb source. This system could offer a powerful solution for the field measurements in practical applications in future.","sentences":["Laser frequency combs, which are composed of a series of equally-spaced coherent frequency components, have triggered revolutionary progress for precision spectroscopy and optical metrology.","Length/distance is of fundamental importance in both science and technology.","In this work, we describe a ranging scheme based on chirped pulse interferometry.","In contrast to the traditional spectral interferometry, the local oscillator is strongly chirped which is able to meet the measurement pulses at arbitrary distances, and therefore the dead zones can be removed.","The distances can be precisely determined via two measurement steps based on time-of-flight method and synthetic wavelength interferometry, respectively.","To overcome the speed limitation of the optical spectrum analyzer, the spectrograms are stretched and detected by a fast photodetector and oscilloscope, and consequently mapped into the time domain in real time.","The experimental results indicate that the measurement uncertainty can be well within 2 $\\upmu$m, compared with the reference distance meter.","The Allan deviation can reach 0.4 $\\upmu$m at averaging time of 4 ns, 25 nm at 1 $\\upmu$s, and can achieve 2 nm at 100","$\\upmu$s averaging time.","We also measure a spinning disk with grooves of different depths to verify the measurement speed, and the results show that the grooves with about 150 m/s line speed can be clearly captured.","Our method provides a unique combination of non-dead zones, ultrafast measurement speed, high precision and accuracy, large ambiguity range, and with only one single comb source.","This system could offer a powerful solution for the field measurements in practical applications in future."],"url":"http://arxiv.org/abs/2402.16011v1","category":"physics.optics"}
{"created":"2024-02-25 06:59:40","title":"Energy-conserving intermittent-contact motion in complex models","abstract":"Some mechanical systems, that are modeled to have inelastic collisions, nonetheless possess energy-conserving intermittent-contact solutions, known as collisionless solutions. Such a solution, representing a persistent hopping or walking across a level ground, may be important for understanding animal locomotion or for designing efficient walking machines. So far, collisionless motion has been analytically studied in simple two degrees of freedom (DOF) systems, or in a system that decouples into 2-DOF subsystems in the harmonic approximation. In this paper we extend the consideration to a N-DOF system, recovering the known solutions as a special N = 2 case of the general formulation. We show that in the harmonic approximation the collisionless solution is determined by the spectrum of the system. We formulate a solution existence condition, which requires the presence of at least one oscillating normal mode in the most constrained phase of the motion. An application of the developed general framework is illustrated by finding a collisionless solution for a rocking motion of a biped with an armed standing torso.","sentences":["Some mechanical systems, that are modeled to have inelastic collisions, nonetheless possess energy-conserving intermittent-contact solutions, known as collisionless solutions.","Such a solution, representing a persistent hopping or walking across a level ground, may be important for understanding animal locomotion or for designing efficient walking machines.","So far, collisionless motion has been analytically studied in simple two degrees of freedom (DOF) systems, or in a system that decouples into 2-DOF subsystems in the harmonic approximation.","In this paper we extend the consideration to a N-DOF system, recovering the known solutions as a special N = 2 case of the general formulation.","We show that in the harmonic approximation the collisionless solution is determined by the spectrum of the system.","We formulate a solution existence condition, which requires the presence of at least one oscillating normal mode in the most constrained phase of the motion.","An application of the developed general framework is illustrated by finding a collisionless solution for a rocking motion of a biped with an armed standing torso."],"url":"http://arxiv.org/abs/2402.16010v1","category":"cs.RO"}
{"created":"2024-02-25 06:18:41","title":"Cross-Resolution Land Cover Classification Using Outdated Products and Transformers","abstract":"Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues. Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas. Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods. In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data. First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention. Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT). Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products. By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained. Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method. The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017. The results show the superiority of the proposed method compared to state-of-the-art methods. The code is available at https://github.com/yu-ni1989/ANLC-Former.","sentences":["Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues.","Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas.","Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods.","In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data.","First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention.","Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT).","Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products.","By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained.","Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method.","The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017.","The results show the superiority of the proposed method compared to state-of-the-art methods.","The code is available at https://github.com/yu-ni1989/ANLC-Former."],"url":"http://arxiv.org/abs/2402.16001v1","category":"cs.CV"}
{"created":"2024-02-25 05:49:32","title":"Optimal Control of Unbounded Functional Stochastic Evolution Systems in Hilbert Spaces: Second-Order Path-dependent HJB Equation","abstract":"Optimal control and the associated second-order path-dependent Hamilton-Jacobi-Bellman (PHJB) equation are studied for unbounded functional stochastic evolution systems in Hilbert spaces. The notion of viscosity solution without B-continuity is introduced in the sense of Crandall and Lions, and is shown to coincide with the classical solutions and to satisfy a stability property. The value functional is proved to be the unique continuous viscosity solution to the associated PHJB equation, without assuming any B-continuity on the coefficients. In particular, in the Markovian case, our result provides a new theory of viscosity solutions to the Hamilton-Jacobi-Bellman equation for optimal control of stochastic evolutionary equations -- driven by a linear unbounded operator -- in a Hilbert space, and removes the B-continuity assumption on the coefficients, which was initially introduced for first-order equations by Crandall and Lions (see J. Func. Anal. 90 (1990), 237-283; 97 (1991), 417-465), and was subsequently used by Swiech (Comm. Partial Differential Equations 19 (1994), 1999-2036) and Fabbri, Gozzi, and Swiech (Probability Theory and Stochastic Modelling 82, 2017, Springer, Berlin).","sentences":["Optimal control and the associated second-order path-dependent Hamilton-Jacobi-Bellman (PHJB) equation are studied for unbounded functional stochastic evolution systems in Hilbert spaces.","The notion of viscosity solution without B-continuity is introduced in the sense of Crandall and Lions, and is shown to coincide with the classical solutions and to satisfy a stability property.","The value functional is proved to be the unique continuous viscosity solution to the associated PHJB equation, without assuming any B-continuity on the coefficients.","In particular, in the Markovian case, our result provides a new theory of viscosity solutions to the Hamilton-Jacobi-Bellman equation for optimal control of stochastic evolutionary equations -- driven by a linear unbounded operator -- in a Hilbert space, and removes the B-continuity assumption on the coefficients, which was initially introduced for first-order equations by Crandall and Lions (see J. Func.","Anal. 90 (1990), 237-283; 97 (1991), 417-465), and was subsequently used by Swiech (Comm.","Partial Differential Equations 19 (1994), 1999-2036) and Fabbri, Gozzi, and Swiech (Probability Theory and Stochastic Modelling 82, 2017, Springer, Berlin)."],"url":"http://arxiv.org/abs/2402.15998v1","category":"math.OC"}
{"created":"2024-02-25 05:45:36","title":"Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning","abstract":"Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks \"just right\" for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preference learning for supporting efficient visualization design optimization.","sentences":["Quality colormaps can help communicate important data patterns.","However, finding an aesthetically pleasing colormap that looks \"just right\" for a given scenario requires significant design and technical expertise.","We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks.","Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context.","We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model.","In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs.","Our work shows the potential of active preference learning for supporting efficient visualization design optimization."],"url":"http://arxiv.org/abs/2402.15997v1","category":"cs.HC"}
{"created":"2024-02-25 05:32:57","title":"Global Existence, Regularity, and Dissipativity of Reaction-diffusion Equations with State-dependent Delay and Supercritical Nonlinearities","abstract":"This work aims to study the initial-boundary value problem of the reaction-diffusion equation $\\pa_{t}u-\\Delta u=f(u)+g(u(t-\\tau(t,u_t)))+h(t,x)$ in a bounded domain with state-dependent delay and supercritical nonlinearities. We establish the global existence and discuss the regularity and dissipativity of the problem under weaker assumptions. In particular, the existence of a global pullback attractor is proved regardless of uniqueness.","sentences":["This work aims to study the initial-boundary value problem of the reaction-diffusion equation $\\pa_{t}u-\\Delta u=f(u)+g(u(t-\\tau(t,u_t)))+h(t,x)$ in a bounded domain with state-dependent delay and supercritical nonlinearities.","We establish the global existence and discuss the regularity and dissipativity of the problem under weaker assumptions.","In particular, the existence of a global pullback attractor is proved regardless of uniqueness."],"url":"http://arxiv.org/abs/2402.15996v1","category":"math.AP"}
{"created":"2024-02-25 05:26:35","title":"Improved Hardness Results for Learning Intersections of Halfspaces","abstract":"We show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting. Strikingly little is known about this problem. For instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces. On the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of Feige's 3SAT hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With intersections of fewer halfspaces being only ruled out under less standard assumptions [DV21] (such as the existence of local pseudo-random generators with large stretch). We significantly narrow this gap by showing that even learning $\\omega(\\log \\log N)$ halfspaces in dimension $N$ takes super-polynomial time under standard assumptions on worst-case lattice problems (namely that SVP and SIVP are hard to approximate within polynomial factors). Further, we give unconditional hardness results in the statistical query framework. Specifically, we show that for any $k$ (even constant), learning $k$ halfspaces in dimension $N$ requires accuracy $N^{-\\Omega(k)}$, or exponentially many queries -- in particular ruling out SQ algorithms with polynomial accuracy for $\\omega(1)$ halfspaces. To the best of our knowledge this is the first unconditional hardness result for learning a super-constant number of halfspaces.   Our lower bounds are obtained in a unified way via a novel connection we make between intersections of halfspaces and the so-called parallel pancakes distribution [DKS17,BLPR19,BRST21] that has been at the heart of many lower bound constructions in (robust) high-dimensional statistics in the past few years.","sentences":["We show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting.","Strikingly little is known about this problem.","For instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces.","On the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of Feige's 3SAT hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16].","With intersections of fewer halfspaces being only ruled out under less standard assumptions [DV21] (such as the existence of local pseudo-random generators with large stretch).","We significantly narrow this gap by showing that even learning $\\omega(\\log \\log N)$ halfspaces in dimension $N$ takes super-polynomial time under standard assumptions on worst-case lattice problems (namely that SVP and SIVP are hard to approximate within polynomial factors).","Further, we give unconditional hardness results in the statistical query framework.","Specifically, we show that for any $k$ (even constant), learning $k$ halfspaces in dimension $N$ requires accuracy $N^{-\\Omega(k)}$, or exponentially many queries -- in particular ruling out SQ algorithms with polynomial accuracy for $\\omega(1)$ halfspaces.","To the best of our knowledge this is the first unconditional hardness result for learning a super-constant number of halfspaces.   ","Our lower bounds are obtained in a unified way via a novel connection we make between intersections of halfspaces and the so-called parallel pancakes distribution [DKS17,BLPR19,BRST21] that has been at the heart of many lower bound constructions in (robust) high-dimensional statistics in the past few years."],"url":"http://arxiv.org/abs/2402.15995v1","category":"cs.CC"}
{"created":"2024-02-25 05:07:56","title":"$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding","abstract":"Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.","sentences":["Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP).","Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks.","However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems.","Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores.","Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages.","This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages.","In this study, we introduce a confidence calibration model cascade ($C^3$) method.","This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions.","Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.15991v1","category":"cs.CL"}
{"created":"2024-02-25 05:00:20","title":"Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation","abstract":"The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels. We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used currently by the research community. These new datasets offer significant values for FairGAD by providing realistic data that captures the intricacies of social networks. Using our datasets, we investigate the performance-fairness trade-off in eleven existing GAD and non-graph AD methods on five state-of-the-art fairness methods, which sheds light on their effectiveness and limitations in addressing the FairGAD problem.","sentences":["The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings.","Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals.","However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD.","To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter.","These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels.","We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used currently by the research community.","These new datasets offer significant values for FairGAD by providing realistic data that captures the intricacies of social networks.","Using our datasets, we investigate the performance-fairness trade-off in eleven existing GAD and non-graph AD methods on five state-of-the-art fairness methods, which sheds light on their effectiveness and limitations in addressing the FairGAD problem."],"url":"http://arxiv.org/abs/2402.15988v1","category":"cs.SI"}
{"created":"2024-02-25 04:35:45","title":"Phonetic and Lexical Discovery of a Canine Language using HuBERT","abstract":"This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.","sentences":["This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization.","We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations.","Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences.","We further develop a web-based dog vocalization labeling system.","This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users."],"url":"http://arxiv.org/abs/2402.15985v1","category":"cs.SD"}
{"created":"2024-02-25 04:18:43","title":"Topological skyrmions in monolayer multiferroic MoPtGe2S6","abstract":"Two-dimensional (2D) multiferroic materials with coexisting ferroelectricity and ferromagnetism have garnered substantial attention for their intriguing physical properties and diverse promising applications in spintronics. For example, multiferroic materials with electronically controlled broken central symmetry provide a versatile platform for designing and manipulating topological skyrmions and diverse spintronic applications. Here, we investigate the complex magnetic properties of room-temerature multiferroic material MoPtGe2S6 and its electrical control of topological skyrmions using first-principles calculations and atomistic micromagnetic simulations. A sizable Dzyaloshinskii-Moriya interaction (DMI) (2.1 meV) is found in the multiferroic material MoPtGe2S6 with an electrically polarized ground state. The magnetic skyrmions can be stabilized in monolayer MoPtGe2S6 under zero magnetic field, and the chirality of skyrmions can be reversed with electric field-induced flipping of electrical polarization due to the reversed chirality of the DMI. Furthermore, an external magnetic fielc can reverse the magnetization direction and topological charge of the skyrmions as well as tune the size of skyrmions. These results demonstrate that the monolayer MoPtGe2S6 can enrich the 2D skyrmion community and pave the way for electronically controlled spintronic devices.","sentences":["Two-dimensional (2D) multiferroic materials with coexisting ferroelectricity and ferromagnetism have garnered substantial attention for their intriguing physical properties and diverse promising applications in spintronics.","For example, multiferroic materials with electronically controlled broken central symmetry provide a versatile platform for designing and manipulating topological skyrmions and diverse spintronic applications.","Here, we investigate the complex magnetic properties of room-temerature multiferroic material MoPtGe2S6 and its electrical control of topological skyrmions using first-principles calculations and atomistic micromagnetic simulations.","A sizable Dzyaloshinskii-Moriya interaction (DMI) (2.1 meV) is found in the multiferroic material MoPtGe2S6 with an electrically polarized ground state.","The magnetic skyrmions can be stabilized in monolayer MoPtGe2S6 under zero magnetic field, and the chirality of skyrmions can be reversed with electric field-induced flipping of electrical polarization due to the reversed chirality of the DMI.","Furthermore, an external magnetic fielc can reverse the magnetization direction and topological charge of the skyrmions as well as tune the size of skyrmions.","These results demonstrate that the monolayer MoPtGe2S6 can enrich the 2D skyrmion community and pave the way for electronically controlled spintronic devices."],"url":"http://arxiv.org/abs/2402.15983v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-25 03:39:17","title":"Nonlinear photodetector based on InSe p-n homojunction for improving spatial imaging resolution","abstract":"We demonstrate an efficient nonlinear photodetector (NLPD) with quadratic response based on a few-layer InSe p-n homojunction, which is beneficial from the strong second harmonic generation (SHG) process in InSe and effective harvest of photocarriers actuated by the high-quality homojunction. The NLPD can sense light with photon energy smaller than InSe electronic bandgap because the SHG process in InSe doubles the frequency of incident light, extending InSe photodetection wavelength range to 1750 nm. The InSe p-n homojunction, which is electrostatically doped by two split back gates, presents a rectification ratio exceeding 106 with a dark current down to 2 pA and a high normalized responsivity of 0.534 A/W2 for the telecom-band pulsed light at 1550 nm. The photocurrents of the SHG-assisted photodetection have a quadratic dependence on the optical powers, making the NLPD highly sensitive to light intensity variation with improved spatial resolution. As examples, the NLPD is employed to precisely determine the localization point of a focused laser beam waist and implement spatial imaging with an improved resolution compared with the linear photodetector. These features highlight the potential of the proposed NLPD in developing advanced optical sensing and imaging systems.","sentences":["We demonstrate an efficient nonlinear photodetector (NLPD) with quadratic response based on a few-layer InSe p-n homojunction, which is beneficial from the strong second harmonic generation (SHG) process in InSe and effective harvest of photocarriers actuated by the high-quality homojunction.","The NLPD can sense light with photon energy smaller than InSe electronic bandgap because the SHG process in InSe doubles the frequency of incident light, extending InSe photodetection wavelength range to 1750 nm.","The InSe p-n homojunction, which is electrostatically doped by two split back gates, presents a rectification ratio exceeding 106 with a dark current down to 2 pA and a high normalized responsivity of 0.534 A/W2 for the telecom-band pulsed light at 1550 nm.","The photocurrents of the SHG-assisted photodetection have a quadratic dependence on the optical powers, making the NLPD highly sensitive to light intensity variation with improved spatial resolution.","As examples, the NLPD is employed to precisely determine the localization point of a focused laser beam waist and implement spatial imaging with an improved resolution compared with the linear photodetector.","These features highlight the potential of the proposed NLPD in developing advanced optical sensing and imaging systems."],"url":"http://arxiv.org/abs/2402.15975v1","category":"physics.optics"}
{"created":"2024-02-25 03:31:59","title":"Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing","abstract":"Task offloading is a potential solution to satisfy the strict requirements of computation-intensive and latency-sensitive vehicular applications due to the limited onboard computing resources. However, the overwhelming upload traffic may lead to unacceptable uploading time. To tackle this issue, for tasks taking environmental data as input, the data perceived by roadside units (RSU) equipped with several sensors can be directly exploited for computation, resulting in a novel task offloading paradigm with integrated communications, sensing and computing (I-CSC). With this paradigm, vehicles can select to upload their sensed data to RSUs or transmit computing instructions to RSUs during the offloading. By optimizing the computation mode and network resources, in this paper, we investigate an I-CSC-based task offloading problem to reduce the cost caused by resource consumption while guaranteeing the latency of each task. Although this non-convex problem can be handled by the alternating minimization (AM) algorithm that alternatively minimizes the divided four sub-problems, it leads to high computational complexity and local optimal solution. To tackle this challenge, we propose a creative structural knowledge-driven meta-learning (SKDML) method, involving both the model-based AM algorithm and neural networks. Specifically, borrowing the iterative structure of the AM algorithm, also referred to as structural knowledge, the proposed SKDML adopts long short-term memory (LSTM) network-based meta-learning to learn an adaptive optimizer for updating variables in each sub-problem, instead of the handcrafted counterpart in the AM algorithm.","sentences":["Task offloading is a potential solution to satisfy the strict requirements of computation-intensive and latency-sensitive vehicular applications due to the limited onboard computing resources.","However, the overwhelming upload traffic may lead to unacceptable uploading time.","To tackle this issue, for tasks taking environmental data as input, the data perceived by roadside units (RSU) equipped with several sensors can be directly exploited for computation, resulting in a novel task offloading paradigm with integrated communications, sensing and computing (I-CSC).","With this paradigm, vehicles can select to upload their sensed data to RSUs or transmit computing instructions to RSUs during the offloading.","By optimizing the computation mode and network resources, in this paper, we investigate an I-CSC-based task offloading problem to reduce the cost caused by resource consumption while guaranteeing the latency of each task.","Although this non-convex problem can be handled by the alternating minimization (AM) algorithm that alternatively minimizes the divided four sub-problems, it leads to high computational complexity and local optimal solution.","To tackle this challenge, we propose a creative structural knowledge-driven meta-learning (SKDML) method, involving both the model-based AM algorithm and neural networks.","Specifically, borrowing the iterative structure of the AM algorithm, also referred to as structural knowledge, the proposed SKDML adopts long short-term memory (LSTM) network-based meta-learning to learn an adaptive optimizer for updating variables in each sub-problem, instead of the handcrafted counterpart in the AM algorithm."],"url":"http://arxiv.org/abs/2402.15972v1","category":"cs.LG"}
{"created":"2024-02-25 03:25:53","title":"Evaporation of acoustically levitated bicomponent droplets: mass and heat transfer characteristics","abstract":"Evaporation of multicomponent droplets is important in a wide range of applications, albeit complex, and requires a careful investigation. We experimentally and numerically investigate the evaporation characteristics of spherical, ethanol-water droplets with different initial concentration ratios in the acoustic levitation field. Imaging techniques and infrared thermometry are used for acquiring volume and surface temperature variations of droplets, reflecting their mass and heat transfer characteristics. Numerical simulations are conducted using modified parameters based on a theoretical model to consider the effect of the acoustic field. The calculation results show good agreement with the experimental data. The concentration and temperature distribution within the droplet is further investigated based on the numerical results.","sentences":["Evaporation of multicomponent droplets is important in a wide range of applications, albeit complex, and requires a careful investigation.","We experimentally and numerically investigate the evaporation characteristics of spherical, ethanol-water droplets with different initial concentration ratios in the acoustic levitation field.","Imaging techniques and infrared thermometry are used for acquiring volume and surface temperature variations of droplets, reflecting their mass and heat transfer characteristics.","Numerical simulations are conducted using modified parameters based on a theoretical model to consider the effect of the acoustic field.","The calculation results show good agreement with the experimental data.","The concentration and temperature distribution within the droplet is further investigated based on the numerical results."],"url":"http://arxiv.org/abs/2402.15971v1","category":"physics.flu-dyn"}
{"created":"2024-02-25 03:17:31","title":"A Markovian regime-switching stochastic SEQIR epidemic model with governmental policy","abstract":"In this paper, a stochastic SEQIR epidemic model with Markovian regime-switching is proposed and investigated. The governmental policy and implement efficiency are concerned by a generalized incidence function of the susceptible class. We have the existence and uniqueness of the globally positive solution to the stochastic model by using the Lyapunov method. In addition, we study the dynamical behaviors of the disease, and the sufficient conditions for the extinction and persistence in mean are obtained. Finally, numerical simulations are introduced to demonstrate the theoretical results.","sentences":["In this paper, a stochastic SEQIR epidemic model with Markovian regime-switching is proposed and investigated.","The governmental policy and implement efficiency are concerned by a generalized incidence function of the susceptible class.","We have the existence and uniqueness of the globally positive solution to the stochastic model by using the Lyapunov method.","In addition, we study the dynamical behaviors of the disease, and the sufficient conditions for the extinction and persistence in mean are obtained.","Finally, numerical simulations are introduced to demonstrate the theoretical results."],"url":"http://arxiv.org/abs/2402.15970v1","category":"math.PR"}
{"created":"2024-02-25 03:03:34","title":"Direct Punjabi to English speech translation using discrete units","abstract":"Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speech-to-Unit Translation (S2UT) model by a 3.69 BLEU score.","sentences":["Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems.","The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences.","With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever.","Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society.","With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English.","Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model.","The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to).","Our results show that the U2UT model performs better than the Speech-to-Unit Translation (S2UT) model by a 3.69 BLEU score."],"url":"http://arxiv.org/abs/2402.15967v1","category":"cs.CL"}
{"created":"2024-02-25 02:57:53","title":"Evolving E-commerce Logistics Planning- Integrating Embedded Technology and Ant Colony Algorithm for Enhanced Efficiency","abstract":"Amidst the era of networking, the e-commerce sector has undergone notable expansion, notably with the advent of Cross-border E-commerce (CBEC) in recent times. This growth trend persists, necessitating robust logistical frameworks to sustainably support operations. However, the current e-commerce logistics paradigm faces challenges in meeting evolving user demands, prompting a quest for innovative solutions. This research endeavors to address these complexities by undertaking a comprehensive analysis of CBEC logistics models and integrating embedded technology into logistical frameworks, resulting in the development of an advanced logistics tracking system. Moreover, employing the ant colony algorithm, the study conducts experimental investigations into optimizing logistics package distribution route planning. Noteworthy enhancements are observed in key metrics such as average delivery time, signaling the efficacy of this approach. In essence, this research offers a promising pathway towards optimizing logistics package distribution routes and bolstering package transportation efficiency within the CBEC domain.","sentences":["Amidst the era of networking, the e-commerce sector has undergone notable expansion, notably with the advent of Cross-border E-commerce (CBEC) in recent times.","This growth trend persists, necessitating robust logistical frameworks to sustainably support operations.","However, the current e-commerce logistics paradigm faces challenges in meeting evolving user demands, prompting a quest for innovative solutions.","This research endeavors to address these complexities by undertaking a comprehensive analysis of CBEC logistics models and integrating embedded technology into logistical frameworks, resulting in the development of an advanced logistics tracking system.","Moreover, employing the ant colony algorithm, the study conducts experimental investigations into optimizing logistics package distribution route planning.","Noteworthy enhancements are observed in key metrics such as average delivery time, signaling the efficacy of this approach.","In essence, this research offers a promising pathway towards optimizing logistics package distribution routes and bolstering package transportation efficiency within the CBEC domain."],"url":"http://arxiv.org/abs/2402.15965v1","category":"econ.GN"}
{"created":"2024-02-25 02:54:43","title":"Transport properties of asymmetric nuclear matter in the spinodal region","abstract":"We have studied the shear and bulk viscosities of asymmetric nuclear matter in the mechanical and chemical instability region based on IBUU transport simulations in a box system. The Green-Kubo method is used to calculate these viscosities with a prepared dynamically equilibrated nuclear system with hot clusters. While the behavior of the shear viscosity is largely affected by energy-dependent nucleon-nucleon cross sections, the bulk viscosity increases significantly in the presence of nuclear clusters compared to that in uniform nuclear matter. Increasing isospin asymmetry generally increases both viscosities, while their behaviors are qualitatively modified once the isospin asymmetry is large enough to affect significantly the spinodal region. Our calculation shows that the bulk viscosity is more sensitive to the nuclear clustering than the shear viscosity, and is thus a robust quantity related to the phase diagram of asymmetric nuclear matter.","sentences":["We have studied the shear and bulk viscosities of asymmetric nuclear matter in the mechanical and chemical instability region based on IBUU transport simulations in a box system.","The Green-Kubo method is used to calculate these viscosities with a prepared dynamically equilibrated nuclear system with hot clusters.","While the behavior of the shear viscosity is largely affected by energy-dependent nucleon-nucleon cross sections, the bulk viscosity increases significantly in the presence of nuclear clusters compared to that in uniform nuclear matter.","Increasing isospin asymmetry generally increases both viscosities, while their behaviors are qualitatively modified once the isospin asymmetry is large enough to affect significantly the spinodal region.","Our calculation shows that the bulk viscosity is more sensitive to the nuclear clustering than the shear viscosity, and is thus a robust quantity related to the phase diagram of asymmetric nuclear matter."],"url":"http://arxiv.org/abs/2402.15963v1","category":"nucl-th"}
